{
    "author": "zucchini-nlp",
    "message": "ðŸš¨ [unbloating] unify `TypedDict` usage in processing (#40931)\n\n* just squash commits into one\n\n* fix style",
    "sha": "5339f72b9bf2433de4692699a98440e842b4f11e",
    "files": [
        {
            "sha": "97a1c7d0ac92cfae5a58bbc28cbb8de35db5079a",
            "filename": "docs/source/en/auto_docstring.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/docs%2Fsource%2Fen%2Fauto_docstring.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/docs%2Fsource%2Fen%2Fauto_docstring.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fauto_docstring.md?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -292,7 +292,7 @@ The `@auto_docstring` decorator automatically generates docstrings by:\n \n 8. Unrolling kwargs typed with the unpack operator. For specific methods (defined in `UNROLL_KWARGS_METHODS`) or classes (defined in `UNROLL_KWARGS_CLASSES`), the decorator processes `**kwargs` parameters that are typed with `Unpack[KwargsTypedDict]`. It extracts the documentations from the `TypedDict` and adds each parameter to the function's docstring.\n \n-    Currently only supported for [`FastImageProcessorKwargs`].\n+    Currently only supported for [`ImagesKwargs`].\n \n ## Best practices\n "
        },
        {
            "sha": "3227b08cf031b28729719b63e09a50b6d549eef2",
            "filename": "src/transformers/image_processing_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fimage_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fimage_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -20,7 +20,8 @@\n \n from .image_processing_base import BatchFeature, ImageProcessingMixin\n from .image_transforms import center_crop, normalize, rescale\n-from .image_utils import ChannelDimension, get_image_size\n+from .image_utils import ChannelDimension, ImageInput, get_image_size\n+from .processing_utils import ImagesKwargs, Unpack\n from .utils import logging\n from .utils.import_utils import requires\n \n@@ -36,6 +37,8 @@\n \n @requires(backends=(\"vision\",))\n class BaseImageProcessor(ImageProcessingMixin):\n+    valid_kwargs = ImagesKwargs\n+\n     def __init__(self, **kwargs):\n         super().__init__(**kwargs)\n \n@@ -46,9 +49,9 @@ def is_fast(self) -> bool:\n         \"\"\"\n         return False\n \n-    def __call__(self, images, **kwargs) -> BatchFeature:\n+    def __call__(self, images: ImageInput, *args, **kwargs: Unpack[ImagesKwargs]) -> BatchFeature:\n         \"\"\"Preprocess an image or a batch of images.\"\"\"\n-        return self.preprocess(images, **kwargs)\n+        return self.preprocess(images, *args, **kwargs)\n \n     def preprocess(self, images, **kwargs) -> BatchFeature:\n         raise NotImplementedError(\"Each image processor must implement its own preprocess method\")"
        },
        {
            "sha": "a9f6900a10463ac3aa01b649a31a2c18ef985e74",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 6,
            "deletions": 31,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -15,7 +15,7 @@\n from collections.abc import Iterable\n from copy import deepcopy\n from functools import lru_cache, partial\n-from typing import Any, Optional, TypedDict, Union\n+from typing import Any, Optional, Union\n \n import numpy as np\n \n@@ -40,7 +40,7 @@\n     validate_kwargs,\n     validate_preprocess_arguments,\n )\n-from .processing_utils import Unpack\n+from .processing_utils import ImagesKwargs, Unpack\n from .utils import (\n     TensorType,\n     auto_docstring,\n@@ -163,28 +163,6 @@ def divide_to_patches(\n     return patches\n \n \n-class DefaultFastImageProcessorKwargs(TypedDict, total=False):\n-    do_resize: Optional[bool]\n-    size: Optional[dict[str, int]]\n-    default_to_square: Optional[bool]\n-    resample: Optional[Union[\"PILImageResampling\", \"F.InterpolationMode\"]]\n-    do_center_crop: Optional[bool]\n-    crop_size: Optional[dict[str, int]]\n-    do_rescale: Optional[bool]\n-    rescale_factor: Optional[Union[int, float]]\n-    do_normalize: Optional[bool]\n-    image_mean: Optional[Union[float, list[float]]]\n-    image_std: Optional[Union[float, list[float]]]\n-    do_pad: Optional[bool]\n-    pad_size: Optional[dict[str, int]]\n-    do_convert_rgb: Optional[bool]\n-    return_tensors: Optional[Union[str, TensorType]]\n-    data_format: Optional[ChannelDimension]\n-    input_data_format: Optional[Union[str, ChannelDimension]]\n-    device: Optional[\"torch.device\"]\n-    disable_grouping: Optional[bool]\n-\n-\n @auto_docstring\n class BaseImageProcessorFast(BaseImageProcessor):\n     resample = None\n@@ -206,10 +184,10 @@ class BaseImageProcessorFast(BaseImageProcessor):\n     input_data_format = None\n     device = None\n     model_input_names = [\"pixel_values\"]\n-    valid_kwargs = DefaultFastImageProcessorKwargs\n+    valid_kwargs = ImagesKwargs\n     unused_kwargs = None\n \n-    def __init__(self, **kwargs: Unpack[DefaultFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[ImagesKwargs]):\n         super().__init__(**kwargs)\n         kwargs = self.filter_out_unused_kwargs(kwargs)\n         size = kwargs.pop(\"size\", self.size)\n@@ -728,11 +706,8 @@ def _validate_preprocess_kwargs(\n             data_format=data_format,\n         )\n \n-    def __call__(self, images: ImageInput, *args, **kwargs: Unpack[DefaultFastImageProcessorKwargs]) -> BatchFeature:\n-        return self.preprocess(images, *args, **kwargs)\n-\n     @auto_docstring\n-    def preprocess(self, images: ImageInput, *args, **kwargs: Unpack[DefaultFastImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, *args, **kwargs: Unpack[ImagesKwargs]) -> BatchFeature:\n         # args are not validated, but their order in the `preprocess` and `_preprocess` signatures must be the same\n         validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self._valid_kwargs_names)\n         # Set default kwargs from self. This ensures that if a kwarg is not provided\n@@ -765,7 +740,7 @@ def _preprocess_image_like_inputs(\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n-        **kwargs: Unpack[DefaultFastImageProcessorKwargs],\n+        **kwargs: Unpack[ImagesKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess image-like inputs."
        },
        {
            "sha": "e0c2b67fcc909a53271cca9c91cff8ace84a8d37",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -959,8 +959,6 @@ def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]],\n         images: Optional[ImageInput] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[AriaProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "976d2b983ee974f08d88f6a795d4c89cded331c2",
            "filename": "src/transformers/models/aria/processing_aria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -85,8 +85,6 @@ def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]],\n         images: Optional[ImageInput] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[AriaProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "882a85d409466e00f977b86e16cf07d2c3fc4720",
            "filename": "src/transformers/models/aya_vision/processing_aya_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 10,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -19,18 +19,11 @@\n \n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput, make_flat_list_of_images\n-from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n \n \n-class AyaVisionImagesKwargs(ImagesKwargs, total=False):\n-    crop_to_patches: Optional[bool]\n-    min_patches: Optional[int]\n-    max_patches: Optional[int]\n-\n-\n class AyaVisionProcessorKwargs(ProcessingKwargs, total=False):\n-    images_kwargs: AyaVisionImagesKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding_side\": \"left\",\n@@ -140,8 +133,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[AyaVisionProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "f657091683796fbb327f1e98c6caa08281f15019",
            "filename": "src/transformers/models/beit/image_processing_beit.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -33,6 +33,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import (\n     TensorType,\n     filter_out_non_signature_kwargs,\n@@ -54,6 +55,17 @@\n logger = logging.get_logger(__name__)\n \n \n+class BeitImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n+        Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n+        is used for background, and background itself is not included in all classes of a dataset (e.g.\n+        ADE20k). The background label will be replaced by 255.\n+    \"\"\"\n+\n+    do_reduce_labels: Optional[bool]\n+\n+\n @requires(backends=(\"vision\",))\n class BeitImageProcessor(BaseImageProcessor):\n     r\"\"\"\n@@ -99,6 +111,7 @@ class BeitImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = BeitImageProcessorKwargs\n \n     @filter_out_non_signature_kwargs(extra=INIT_SERVICE_KWARGS)\n     def __init__("
        },
        {
            "sha": "5d89120283a553f8fc1b45df9b364a8633e72740",
            "filename": "src/transformers/models/beit/image_processing_beit_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 16,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -22,7 +22,6 @@\n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -40,17 +39,7 @@\n     TensorType,\n     auto_docstring,\n )\n-\n-\n-class BeitFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    r\"\"\"\n-    do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n-        Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n-        is used for background, and background itself is not included in all classes of a dataset (e.g.\n-        ADE20k). The background label will be replaced by 255.\n-    \"\"\"\n-\n-    do_reduce_labels: Optional[bool]\n+from .image_processing_beit import BeitImageProcessorKwargs\n \n \n @auto_docstring\n@@ -66,9 +55,9 @@ class BeitImageProcessorFast(BaseImageProcessorFast):\n     do_rescale = True\n     do_normalize = True\n     do_reduce_labels = False\n-    valid_kwargs = BeitFastImageProcessorKwargs\n+    valid_kwargs = BeitImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[BeitFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[BeitImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     def reduce_label(self, labels: list[\"torch.Tensor\"]):\n@@ -86,7 +75,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         segmentation_maps: Optional[ImageInput] = None,\n-        **kwargs: Unpack[BeitFastImageProcessorKwargs],\n+        **kwargs: Unpack[BeitImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n         segmentation_maps (`ImageInput`, *optional*):\n@@ -101,7 +90,7 @@ def _preprocess_image_like_inputs(\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n-        **kwargs: Unpack[BeitFastImageProcessorKwargs],\n+        **kwargs: Unpack[BeitImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess image-like inputs."
        },
        {
            "sha": "7fc154f3483e1ffc0f7845250ca46d212c909e37",
            "filename": "src/transformers/models/blip/processing_blip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -36,7 +36,6 @@ class BlipProcessorKwargs(ProcessingKwargs, total=False):\n             \"return_length\": False,\n             \"verbose\": True,\n         },\n-        \"images_kwargs\": {},\n     }\n \n \n@@ -67,8 +66,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[BlipProcessorKwargs],\n     ) -> BatchEncoding:\n         \"\"\""
        },
        {
            "sha": "abbbeb6ae0a49199dce58dbaa08a9151a96f66a5",
            "filename": "src/transformers/models/blip_2/processing_blip_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -41,7 +41,6 @@ class Blip2ProcessorKwargs(ProcessingKwargs, total=False):\n             \"return_length\": False,\n             \"verbose\": True,\n         },\n-        \"images_kwargs\": {},\n     }\n \n \n@@ -81,8 +80,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[Blip2ProcessorKwargs],\n     ) -> BatchEncoding:\n         \"\"\""
        },
        {
            "sha": "cad23d02893f44870007637172a82809a0a1bf39",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -35,6 +35,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n \n \n@@ -122,6 +123,10 @@ def get_resize_output_image_size(\n     return new_height, new_width\n \n \n+class BridgeTowerImageProcessorKwargs(ImagesKwargs):\n+    size_divisor: Optional[int]\n+\n+\n class BridgeTowerImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a BridgeTower image processor.\n@@ -169,6 +174,7 @@ class BridgeTowerImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n+    valid_kwargs = BridgeTowerImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "76a76b4b0a478c3436cb6a64d07fe1942da9239d",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 15,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -23,7 +23,6 @@\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n     BatchFeature,\n-    DefaultFastImageProcessorKwargs,\n     ImageInput,\n     SizeDict,\n     TensorType,\n@@ -33,6 +32,7 @@\n )\n from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling\n from ...utils import auto_docstring\n+from .image_processing_bridgetower import BridgeTowerImageProcessorKwargs\n \n \n def make_pixel_mask(\n@@ -85,17 +85,6 @@ def get_resize_output_image_size(\n     return new_height, new_width\n \n \n-class BridgeTowerFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    Args:\n-        size_divisor (`int`, *optional*, defaults to 32):\n-            The size by which to make sure both the height and width can be divided. Only has an effect if `do_resize`\n-            is set to `True`. Can be overridden by the `size_divisor` parameter in the `preprocess` method.\n-    \"\"\"\n-\n-    size_divisor: Optional[int]\n-\n-\n @auto_docstring\n class BridgeTowerImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BICUBIC\n@@ -110,14 +99,14 @@ class BridgeTowerImageProcessorFast(BaseImageProcessorFast):\n     do_normalize = True\n     do_pad = True\n     size_divisor = 32\n-    valid_kwargs = BridgeTowerFastImageProcessorKwargs\n+    valid_kwargs = BridgeTowerImageProcessorKwargs\n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n \n-    def __init__(self, **kwargs: Unpack[BridgeTowerFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[BridgeTowerImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[BridgeTowerFastImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[BridgeTowerImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     def resize("
        },
        {
            "sha": "030c578c49cda3fdd630d869898cc49f94d9430d",
            "filename": "src/transformers/models/bridgetower/processing_bridgetower.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -16,17 +16,10 @@\n Processor class for BridgeTower.\n \"\"\"\n \n-from typing import Optional\n-\n-from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin\n-\n-\n-class BridgeTowerImagesKwargs(ImagesKwargs):\n-    size_divisor: Optional[int]\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin\n \n \n class BridgeTowerProcessorKwargs(ProcessingKwargs, total=False):\n-    images_kwargs: BridgeTowerImagesKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"add_special_tokens\": True,"
        },
        {
            "sha": "247f72322a2db050b03482cc54446801137aa9b9",
            "filename": "src/transformers/models/chameleon/processing_chameleon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -92,8 +92,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[ChameleonProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "358d84ac6d7ceea9d0dd451a4f41e321c9378e56",
            "filename": "src/transformers/models/cohere2_vision/image_processing_cohere2_vision_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 8,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fimage_processing_cohere2_vision_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fimage_processing_cohere2_vision_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fimage_processing_cohere2_vision_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -27,18 +27,13 @@\n from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n-from ...image_processing_utils_fast import (\n-    BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n-    group_images_by_shape,\n-    reorder_images,\n-)\n+from ...image_processing_utils_fast import BaseImageProcessorFast, group_images_by_shape, reorder_images\n from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, ImageInput, PILImageResampling, SizeDict\n-from ...processing_utils import Unpack\n+from ...processing_utils import ImagesKwargs, Unpack\n from ...utils import TensorType, auto_docstring\n \n \n-class Cohere2VisionFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+class Cohere2VisionFastImageProcessorKwargs(ImagesKwargs):\n     \"\"\"\n     crop_to_patches (`bool`, *optional*, defaults to `False`):\n         Whether to crop the image to patches. Can be overridden by the `crop_to_patches` parameter in the"
        },
        {
            "sha": "2b7867d0eae32f6f023b12ce3b214f8a5e39c33c",
            "filename": "src/transformers/models/cohere2_vision/modular_cohere2_vision.py",
            "status": "modified",
            "additions": 29,
            "deletions": 1,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodular_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodular_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodular_cohere2_vision.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -30,8 +30,10 @@\n from transformers.models.got_ocr2.image_processing_got_ocr2_fast import GotOcr2ImageProcessorFast\n \n from ...cache_utils import Cache\n+from ...image_processing_utils import BatchFeature\n+from ...image_utils import ImageInput\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...processing_utils import Unpack\n+from ...processing_utils import ImagesKwargs, Unpack\n from ...utils import TransformersKwargs, auto_docstring, logging\n from ...utils.generic import check_model_inputs\n from .configuration_cohere2_vision import Cohere2VisionConfig\n@@ -301,13 +303,39 @@ def get_optimal_tiled_canvas(\n     return best_grid\n \n \n+class Cohere2VisionFastImageProcessorKwargs(ImagesKwargs):\n+    \"\"\"\n+    crop_to_patches (`bool`, *optional*, defaults to `False`):\n+        Whether to crop the image to patches. Can be overridden by the `crop_to_patches` parameter in the\n+        `preprocess` method.\n+    min_patches (`int`, *optional*, defaults to 1):\n+        The minimum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n+        set to `True`. Can be overridden by the `min_patches` parameter in the `preprocess` method.\n+    max_patches (`int`, *optional*, defaults to 12):\n+        The maximum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n+        set to `True`. Can be overridden by the `max_patches` parameter in the `preprocess` method.\n+    \"\"\"\n+\n+    crop_to_patches: Optional[bool]\n+    min_patches: Optional[int]\n+    max_patches: Optional[int]\n+\n+\n @auto_docstring\n class Cohere2VisionImageProcessorFast(GotOcr2ImageProcessorFast):\n     size = {\"height\": 512, \"width\": 512}\n     min_patches = 1\n     max_patches = 12\n     crop_to_patches = True\n     patch_size = 16\n+    valid_kwargs = Cohere2VisionFastImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[Cohere2VisionFastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    @auto_docstring\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[Cohere2VisionFastImageProcessorKwargs]) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n \n \n __all__ = ["
        },
        {
            "sha": "d4fcec4da875542fadd46d4ba1699448b5eef9e0",
            "filename": "src/transformers/models/cohere2_vision/processing_cohere2_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fprocessing_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fprocessing_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fprocessing_cohere2_vision.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -19,16 +19,11 @@\n \n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput\n-from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n \n \n-class Cohere2VisionImagesKwargs(ImagesKwargs, total=False):\n-    max_patches: Optional[int]\n-\n-\n class Cohere2VisionProcessorKwargs(ProcessingKwargs, total=False):\n-    images_kwargs: Cohere2VisionImagesKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding_side\": \"left\","
        },
        {
            "sha": "176b3e6a15eed2cb2777742a9cbce67749664fe2",
            "filename": "src/transformers/models/colpali/modular_colpali.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -90,8 +90,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[ColPaliProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "032cc70d4482a8a7f261789b1e6b24392ac36ee0",
            "filename": "src/transformers/models/colpali/processing_colpali.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -131,8 +131,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[ColPaliProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "adea1617e459da922bf54ff682843a5e1d4773df",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -93,8 +93,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[ColQwen2ProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "2eb9fed873a8a2be67148e2962c9b3f56874de4a",
            "filename": "src/transformers/models/colqwen2/processing_colqwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -94,8 +94,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[ColQwen2ProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "163224edb34f035c2a49c2a86169e97d9dfdd699",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr.py",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -53,6 +53,7 @@\n     validate_kwargs,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import (\n     TensorType,\n     is_scipy_available,\n@@ -774,6 +775,29 @@ def compute_segments(\n     return segmentation, segments\n \n \n+class ConditionalDetrImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n+        Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n+    do_convert_annotations (`bool`, *optional*, defaults to `True`):\n+        Controls whether to convert the annotations to the format expected by the CONDITIONAL_DETR model. Converts the\n+        bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n+        Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n+    return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n+        Whether to return segmentation masks.\n+    annotations (`AnnotationType` or `list[AnnotationType]`, *optional*):\n+        Annotations to transform according to the padding that is applied to the images.\n+    masks_path (`str` or `pathlib.Path`, *optional*):\n+        Path to the directory containing the segmentation masks.\n+    \"\"\"\n+\n+    format: Optional[Union[str, AnnotationFormat]]\n+    do_convert_annotations: Optional[bool]\n+    return_segmentation_masks: Optional[bool]\n+    annotations: Optional[Union[AnnotationType, list[AnnotationType]]]\n+    masks_path: Optional[Union[str, pathlib.Path]]\n+\n+\n @requires(backends=(\"vision\",))\n class ConditionalDetrImageProcessor(BaseImageProcessor):\n     r\"\"\"\n@@ -829,6 +853,7 @@ class ConditionalDetrImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n+    valid_kwargs = ConditionalDetrImageProcessorKwargs\n \n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.__init__\n     def __init__("
        },
        {
            "sha": "4c5b8602c0cc8856c54e281386103f442f1167c6",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 40,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -15,7 +15,6 @@\n from ...image_processing_utils import BatchFeature, get_size_dict\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     SizeDict,\n     get_image_size_for_max_height_width,\n     get_max_height_width,\n@@ -37,6 +36,7 @@\n from ...utils import TensorType, auto_docstring, logging\n from ...utils.import_utils import requires\n from .image_processing_conditional_detr import (\n+    ConditionalDetrImageProcessorKwargs,\n     compute_segments,\n     convert_segmentation_to_rle,\n     get_size_with_aspect_ratio,\n@@ -46,24 +46,6 @@\n \n logger = logging.get_logger(__name__)\n \n-\n-class ConditionalDetrFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    r\"\"\"\n-    format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n-        Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n-    do_convert_annotations (`bool`, *optional*, defaults to `True`):\n-        Controls whether to convert the annotations to the format expected by the CONDITIONAL_DETR model. Converts the\n-        bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n-        Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n-    return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n-        Whether to return segmentation masks.\n-    \"\"\"\n-\n-    format: Optional[Union[str, AnnotationFormat]]\n-    do_convert_annotations: Optional[bool]\n-    return_segmentation_masks: Optional[bool]\n-\n-\n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n \n \n@@ -278,9 +260,9 @@ class ConditionalDetrImageProcessorFast(BaseImageProcessorFast):\n     size = {\"shortest_edge\": 800, \"longest_edge\": 1333}\n     default_to_square = False\n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n-    valid_kwargs = ConditionalDetrFastImageProcessorKwargs\n+    valid_kwargs = ConditionalDetrImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[ConditionalDetrFastImageProcessorKwargs]) -> None:\n+    def __init__(self, **kwargs: Unpack[ConditionalDetrImageProcessorKwargs]) -> None:\n         if \"pad_and_return_pixel_mask\" in kwargs:\n             kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n \n@@ -542,25 +524,8 @@ def pad(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        annotations: Optional[Union[AnnotationType, list[AnnotationType]]] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        **kwargs: Unpack[ConditionalDetrFastImageProcessorKwargs],\n+        **kwargs: Unpack[ConditionalDetrImageProcessorKwargs],\n     ) -> BatchFeature:\n-        r\"\"\"\n-        annotations (`AnnotationType` or `list[AnnotationType]`, *optional*):\n-            List of annotations associated with the image or batch of images. If annotation is for object\n-            detection, the annotations should be a dictionary with the following keys:\n-            - \"image_id\" (`int`): The image id.\n-            - \"annotations\" (`list[Dict]`): List of annotations for an image. Each annotation should be a\n-                dictionary. An image can have no annotations, in which case the list should be empty.\n-            If annotation is for segmentation, the annotations should be a dictionary with the following keys:\n-            - \"image_id\" (`int`): The image id.\n-            - \"segments_info\" (`list[Dict]`): List of segments for an image. Each segment should be a dictionary.\n-                An image can have no segments, in which case the list should be empty.\n-            - \"file_name\" (`str`): The file name of the image.\n-        masks_path (`str` or `pathlib.Path`, *optional*):\n-            Path to the directory containing the segmentation masks.\n-        \"\"\"\n         if \"pad_and_return_pixel_mask\" in kwargs:\n             kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n             logger.warning_once(\n@@ -575,7 +540,7 @@ def preprocess(\n             )\n             kwargs[\"size\"] = kwargs.pop(\"max_size\")\n \n-        return super().preprocess(images, annotations, masks_path, **kwargs)\n+        return super().preprocess(images, **kwargs)\n \n     def _preprocess(\n         self,"
        },
        {
            "sha": "d2e180de246448726e134c63a604ff0f77de7f7b",
            "filename": "src/transformers/models/convnext/image_processing_convnext.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -38,6 +38,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n from ...utils.import_utils import requires\n \n@@ -49,6 +50,16 @@\n logger = logging.get_logger(__name__)\n \n \n+class ConvNextImageProcessorKwargs(ImagesKwargs):\n+    \"\"\"\n+    crop_pct (`float`, *optional*):\n+        Percentage of the image to crop. Only has an effect if size < 384. Can be\n+        overridden by `crop_pct` in the`preprocess` method.\n+    \"\"\"\n+\n+    crop_pct: Optional[float]\n+\n+\n @requires(backends=(\"vision\",))\n class ConvNextImageProcessor(BaseImageProcessor):\n     r\"\"\"\n@@ -87,6 +98,7 @@ class ConvNextImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = ConvNextImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "035b92f8b7d245eda965d51c1758d26dfa76953d",
            "filename": "src/transformers/models/convnext/image_processing_convnext_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 14,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -22,7 +22,6 @@\n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -39,16 +38,7 @@\n     TensorType,\n     auto_docstring,\n )\n-\n-\n-class ConvNextFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    crop_pct (`float`, *optional*):\n-        Percentage of the image to crop. Only has an effect if size < 384. Can be\n-        overridden by `crop_pct` in the`preprocess` method.\n-    \"\"\"\n-\n-    crop_pct: Optional[float]\n+from .image_processing_convnext import ConvNextImageProcessorKwargs\n \n \n @auto_docstring\n@@ -62,13 +52,13 @@ class ConvNextImageProcessorFast(BaseImageProcessorFast):\n     do_rescale = True\n     do_normalize = True\n     crop_pct = 224 / 256\n-    valid_kwargs = ConvNextFastImageProcessorKwargs\n+    valid_kwargs = ConvNextImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[ConvNextFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[ConvNextImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[ConvNextFastImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[ConvNextImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     def resize("
        },
        {
            "sha": "172016f6431d04690014050f1be00200e726a312",
            "filename": "src/transformers/models/csm/processing_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -246,9 +246,7 @@ def __call__(\n \n         text_kwargs = output_kwargs[\"text_kwargs\"]\n         audio_kwargs = output_kwargs[\"audio_kwargs\"]\n-        common_kwargs = output_kwargs[\"common_kwargs\"]\n-\n-        return_tensors = common_kwargs.pop(\"return_tensors\", None)\n+        return_tensors = text_kwargs.get(\"return_tensors\", None)\n         if return_tensors != \"pt\":\n             raise ValueError(f\"{self.__class__.__name__} only supports `return_tensors='pt'`.\")\n "
        },
        {
            "sha": "c41ac586753ece57eb4e4e26192b78e0023599ca",
            "filename": "src/transformers/models/deepseek_vl/image_processing_deepseek_vl.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -38,6 +38,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n \n \n@@ -48,6 +49,16 @@\n logger = logging.get_logger(__name__)\n \n \n+class DeepseekVLImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    min_size (`int`, *optional*, defaults to 14):\n+        The minimum allowed size for the resized image. Ensures that neither the height nor width\n+        falls below this value after resizing.\n+    \"\"\"\n+\n+    min_size: int\n+\n+\n class DeepseekVLImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a DEEPSEEK_VL image processor.\n@@ -90,6 +101,8 @@ class DeepseekVLImageProcessor(BaseImageProcessor):\n \n     model_input_names = [\"pixel_values\"]\n \n+    valid_kwargs = DeepseekVLImageProcessorKwargs\n+\n     def __init__(\n         self,\n         do_resize: bool = True,"
        },
        {
            "sha": "6eaa15d827d9c6a7f6cfdb06e2532beb1cc94b49",
            "filename": "src/transformers/models/deepseek_vl/image_processing_deepseek_vl_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 18,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -24,25 +24,11 @@\n import torch.nn.functional as F\n \n from ...image_processing_utils import BatchFeature\n-from ...image_processing_utils_fast import (\n-    BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n-    group_images_by_shape,\n-    reorder_images,\n-)\n+from ...image_processing_utils_fast import BaseImageProcessorFast, group_images_by_shape, reorder_images\n from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling, SizeDict\n from ...processing_utils import Unpack\n from ...utils import TensorType, auto_docstring\n-\n-\n-class DeepseekVLFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    r\"\"\"\n-    min_size (`int`, *optional*, defaults to 14):\n-        The minimum allowed size for the resized image. Ensures that neither the height nor width\n-        falls below this value after resizing.\n-    \"\"\"\n-\n-    min_size: int\n+from .image_processing_deepseek_vl import DeepseekVLImageProcessorKwargs\n \n \n @auto_docstring\n@@ -56,9 +42,9 @@ class DeepseekVLImageProcessorFast(BaseImageProcessorFast):\n     do_rescale = True\n     do_normalize = True\n     do_pad = True\n-    valid_kwargs = DeepseekVLFastImageProcessorKwargs\n+    valid_kwargs = DeepseekVLImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[DeepseekVLFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[DeepseekVLImageProcessorKwargs]):\n         super().__init__(**kwargs)\n         if kwargs.get(\"image_mean\") is None:\n             background_color = (127, 127, 127)"
        },
        {
            "sha": "8b93f7fa6c945a744f99d30710465c01402761b0",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -39,6 +39,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n \n \n@@ -49,6 +50,32 @@\n logger = logging.get_logger(__name__)\n \n \n+class DeepseekVLHybridImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    min_size (`int`, *optional*, defaults to 14):\n+        The minimum allowed size for the resized image. Ensures that neither the height nor width\n+        falls below this value after resizing.\n+     high_res_size (`dict`, *optional*, defaults to `{\"height\": 1024, \"width\": 1024}`):\n+        Size of the high resolution output image after resizing. Can be overridden by the `high_res_size` parameter in the `preprocess`\n+        method.\n+    high_res_resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n+        Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`. Can be\n+        overridden by the `high_res_resample` parameter in the `preprocess` method.\n+    high_res_image_mean (`float` or `list[float]`, *optional*, defaults to `OPENAI_CLIP_MEAN`):\n+        Mean to use if normalizing the high resolution image. This is a float or list of floats the length of the number of\n+        channels in the image. Can be overridden by the `high_res_image_mean` parameter in the `preprocess` method.\n+    high_res_image_std (`float` or `list[float]`, *optional*, defaults to `OPENAI_CLIP_STD`):\n+        Standard deviation to use if normalizing the high resolution image. This is a float or list of floats the length of the\n+        number of channels in the image. Can be overridden by the `high_res_image_std` parameter in the `preprocess` method.\n+    \"\"\"\n+\n+    min_size: int\n+    high_res_size: dict\n+    high_res_resample: \"PILImageResampling\"\n+    high_res_image_mean: list[float]\n+    high_res_image_std: list[float]\n+\n+\n class DeepseekVLHybridImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a DEEPSEEK_VL_HYBRID image processor.\n@@ -102,6 +129,7 @@ class DeepseekVLHybridImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"high_res_pixel_values\"]\n+    valid_kwargs = DeepseekVLHybridImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "ff5e7f2e3c73df9b16cd526a3cc8fe28c15eca9c",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 29,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -26,7 +26,6 @@\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n     BatchFeature,\n-    DefaultFastImageProcessorKwargs,\n     get_size_dict,\n     group_images_by_shape,\n     reorder_images,\n@@ -41,32 +40,7 @@\n )\n from ...processing_utils import Unpack\n from ...utils import TensorType, auto_docstring\n-\n-\n-class DeepseekVLHybridFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    r\"\"\"\n-    min_size (`int`, *optional*, defaults to 14):\n-        The minimum allowed size for the resized image. Ensures that neither the height nor width\n-        falls below this value after resizing.\n-     high_res_size (`dict`, *optional*, defaults to `{\"height\": 1024, \"width\": 1024}`):\n-        Size of the high resolution output image after resizing. Can be overridden by the `high_res_size` parameter in the `preprocess`\n-        method.\n-    high_res_resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n-        Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`. Can be\n-        overridden by the `high_res_resample` parameter in the `preprocess` method.\n-    high_res_image_mean (`float` or `list[float]`, *optional*, defaults to `OPENAI_CLIP_MEAN`):\n-        Mean to use if normalizing the high resolution image. This is a float or list of floats the length of the number of\n-        channels in the image. Can be overridden by the `high_res_image_mean` parameter in the `preprocess` method.\n-    high_res_image_std (`float` or `list[float]`, *optional*, defaults to `OPENAI_CLIP_STD`):\n-        Standard deviation to use if normalizing the high resolution image. This is a float or list of floats the length of the\n-        number of channels in the image. Can be overridden by the `high_res_image_std` parameter in the `preprocess` method.\n-    \"\"\"\n-\n-    min_size: int\n-    high_res_size: dict\n-    high_res_resample: \"PILImageResampling\"\n-    high_res_image_mean: list[float]\n-    high_res_image_std: list[float]\n+from .image_processing_deepseek_vl_hybrid import DeepseekVLHybridImageProcessorKwargs\n \n \n @auto_docstring\n@@ -80,14 +54,14 @@ class DeepseekVLHybridImageProcessorFast(BaseImageProcessorFast):\n     do_rescale = True\n     do_normalize = True\n     do_pad = True\n-    valid_kwargs = DeepseekVLHybridFastImageProcessorKwargs\n+    valid_kwargs = DeepseekVLHybridImageProcessorKwargs\n     high_res_image_mean = OPENAI_CLIP_MEAN\n     high_res_image_std = OPENAI_CLIP_STD\n     high_res_size = {\"height\": 1024, \"width\": 1024}\n     high_res_resample = PILImageResampling.BICUBIC\n     model_input_names = [\"pixel_values\", \"high_res_pixel_values\"]\n \n-    def __init__(self, **kwargs: Unpack[DeepseekVLHybridFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[DeepseekVLHybridImageProcessorKwargs]):\n         if kwargs.get(\"image_mean\") is None:\n             background_color = (127, 127, 127)\n         else:"
        },
        {
            "sha": "1507c9f3d0283fe319014dd9533e60d2d70b449d",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 29,
            "deletions": 29,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -22,7 +22,6 @@\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n     BatchFeature,\n-    DefaultFastImageProcessorKwargs,\n     get_size_dict,\n     group_images_by_shape,\n     reorder_images,\n@@ -43,7 +42,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n-from ...processing_utils import Unpack\n+from ...processing_utils import ImagesKwargs, Unpack\n from ...tokenization_utils_base import (\n     PreTokenizedInput,\n     TextInput,\n@@ -430,6 +429,32 @@ def prepare_inputs_for_generation(\n         return model_inputs\n \n \n+class DeepseekVLHybridImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    min_size (`int`, *optional*, defaults to 14):\n+        The minimum allowed size for the resized image. Ensures that neither the height nor width\n+        falls below this value after resizing.\n+     high_res_size (`dict`, *optional*, defaults to `{\"height\": 1024, \"width\": 1024}`):\n+        Size of the high resolution output image after resizing. Can be overridden by the `high_res_size` parameter in the `preprocess`\n+        method.\n+    high_res_resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n+        Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`. Can be\n+        overridden by the `high_res_resample` parameter in the `preprocess` method.\n+    high_res_image_mean (`float` or `list[float]`, *optional*, defaults to `OPENAI_CLIP_MEAN`):\n+        Mean to use if normalizing the high resolution image. This is a float or list of floats the length of the number of\n+        channels in the image. Can be overridden by the `high_res_image_mean` parameter in the `preprocess` method.\n+    high_res_image_std (`float` or `list[float]`, *optional*, defaults to `OPENAI_CLIP_STD`):\n+        Standard deviation to use if normalizing the high resolution image. This is a float or list of floats the length of the\n+        number of channels in the image. Can be overridden by the `high_res_image_std` parameter in the `preprocess` method.\n+    \"\"\"\n+\n+    min_size: int\n+    high_res_size: dict\n+    high_res_resample: \"PILImageResampling\"\n+    high_res_image_mean: list[float]\n+    high_res_image_std: list[float]\n+\n+\n class DeepseekVLHybridImageProcessor(DeepseekVLImageProcessor):\n     r\"\"\"\n     Constructs a DEEPSEEK_VL_HYBRID image processor.\n@@ -483,6 +508,7 @@ class DeepseekVLHybridImageProcessor(DeepseekVLImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"high_res_pixel_values\"]\n+    valid_kwargs = DeepseekVLHybridImageProcessorKwargs\n \n     def __init__(\n         self,\n@@ -727,40 +753,14 @@ def preprocess(\n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n \n-class DeepseekVLHybridFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    r\"\"\"\n-    min_size (`int`, *optional*, defaults to 14):\n-        The minimum allowed size for the resized image. Ensures that neither the height nor width\n-        falls below this value after resizing.\n-     high_res_size (`dict`, *optional*, defaults to `{\"height\": 1024, \"width\": 1024}`):\n-        Size of the high resolution output image after resizing. Can be overridden by the `high_res_size` parameter in the `preprocess`\n-        method.\n-    high_res_resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n-        Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`. Can be\n-        overridden by the `high_res_resample` parameter in the `preprocess` method.\n-    high_res_image_mean (`float` or `list[float]`, *optional*, defaults to `OPENAI_CLIP_MEAN`):\n-        Mean to use if normalizing the high resolution image. This is a float or list of floats the length of the number of\n-        channels in the image. Can be overridden by the `high_res_image_mean` parameter in the `preprocess` method.\n-    high_res_image_std (`float` or `list[float]`, *optional*, defaults to `OPENAI_CLIP_STD`):\n-        Standard deviation to use if normalizing the high resolution image. This is a float or list of floats the length of the\n-        number of channels in the image. Can be overridden by the `high_res_image_std` parameter in the `preprocess` method.\n-    \"\"\"\n-\n-    min_size: int\n-    high_res_size: dict\n-    high_res_resample: \"PILImageResampling\"\n-    high_res_image_mean: list[float]\n-    high_res_image_std: list[float]\n-\n-\n class DeepseekVLHybridImageProcessorFast(DeepseekVLImageProcessorFast):\n     high_res_image_mean = OPENAI_CLIP_MEAN\n     high_res_image_std = OPENAI_CLIP_STD\n     high_res_size = {\"height\": 1024, \"width\": 1024}\n     high_res_resample = PILImageResampling.BICUBIC\n     model_input_names = [\"pixel_values\", \"high_res_pixel_values\"]\n \n-    def __init__(self, **kwargs: Unpack[DeepseekVLHybridFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[DeepseekVLHybridImageProcessorKwargs]):\n         if kwargs.get(\"image_mean\") is None:\n             background_color = (127, 127, 127)\n         else:"
        },
        {
            "sha": "8249c079f5fae98b5492c78075dd2b2d8d3282b9",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr.py",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -53,6 +53,7 @@\n     validate_kwargs,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import (\n     TensorType,\n     is_scipy_available,\n@@ -79,6 +80,30 @@\n \n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n \n+\n+class DeformableDetrImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n+        Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n+    do_convert_annotations (`bool`, *optional*, defaults to `True`):\n+        Controls whether to convert the annotations to the format expected by the DEFORMABLE_DETR model. Converts the\n+        bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n+        Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n+    return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n+        Whether to return segmentation masks.\n+    annotations (`AnnotationType` or `list[AnnotationType]`, *optional*):\n+        Annotations to transform according to the padding that is applied to the images.\n+    masks_path (`str` or `pathlib.Path`, *optional*):\n+        Path to the directory containing the segmentation masks.\n+    \"\"\"\n+\n+    format: Optional[Union[str, AnnotationFormat]]\n+    do_convert_annotations: Optional[bool]\n+    return_segmentation_masks: Optional[bool]\n+    annotations: Optional[Union[AnnotationType, list[AnnotationType]]]\n+    masks_path: Optional[Union[str, pathlib.Path]]\n+\n+\n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n \n \n@@ -827,6 +852,7 @@ class DeformableDetrImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n+    valid_kwargs = DeformableDetrImageProcessorKwargs\n \n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.__init__\n     def __init__("
        },
        {
            "sha": "916ad3dee0e61c396486b6499156338903544222",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 41,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -14,7 +14,6 @@\n from ...image_processing_utils import BatchFeature, get_size_dict\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     SizeDict,\n     get_image_size_for_max_height_width,\n     get_max_height_width,\n@@ -35,29 +34,11 @@\n from ...processing_utils import Unpack\n from ...utils import TensorType, auto_docstring, logging\n from ...utils.import_utils import requires\n-from .image_processing_deformable_detr import get_size_with_aspect_ratio\n+from .image_processing_deformable_detr import DeformableDetrImageProcessorKwargs, get_size_with_aspect_ratio\n \n \n logger = logging.get_logger(__name__)\n \n-\n-class DeformableDetrFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    r\"\"\"\n-    format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n-        Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n-    do_convert_annotations (`bool`, *optional*, defaults to `True`):\n-        Controls whether to convert the annotations to the format expected by the DEFORMABLE_DETR model. Converts the\n-        bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n-        Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n-    return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n-        Whether to return segmentation masks.\n-    \"\"\"\n-\n-    format: Optional[Union[str, AnnotationFormat]]\n-    do_convert_annotations: Optional[bool]\n-    return_segmentation_masks: Optional[bool]\n-\n-\n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n \n \n@@ -272,9 +253,9 @@ class DeformableDetrImageProcessorFast(BaseImageProcessorFast):\n     size = {\"shortest_edge\": 800, \"longest_edge\": 1333}\n     default_to_square = False\n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n-    valid_kwargs = DeformableDetrFastImageProcessorKwargs\n+    valid_kwargs = DeformableDetrImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[DeformableDetrFastImageProcessorKwargs]) -> None:\n+    def __init__(self, **kwargs: Unpack[DeformableDetrImageProcessorKwargs]) -> None:\n         if \"pad_and_return_pixel_mask\" in kwargs:\n             kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n \n@@ -536,25 +517,8 @@ def pad(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        annotations: Optional[Union[AnnotationType, list[AnnotationType]]] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        **kwargs: Unpack[DeformableDetrFastImageProcessorKwargs],\n+        **kwargs: Unpack[DeformableDetrImageProcessorKwargs],\n     ) -> BatchFeature:\n-        r\"\"\"\n-        annotations (`AnnotationType` or `list[AnnotationType]`, *optional*):\n-            List of annotations associated with the image or batch of images. If annotation is for object\n-            detection, the annotations should be a dictionary with the following keys:\n-            - \"image_id\" (`int`): The image id.\n-            - \"annotations\" (`list[Dict]`): List of annotations for an image. Each annotation should be a\n-                dictionary. An image can have no annotations, in which case the list should be empty.\n-            If annotation is for segmentation, the annotations should be a dictionary with the following keys:\n-            - \"image_id\" (`int`): The image id.\n-            - \"segments_info\" (`list[Dict]`): List of segments for an image. Each segment should be a dictionary.\n-                An image can have no segments, in which case the list should be empty.\n-            - \"file_name\" (`str`): The file name of the image.\n-        masks_path (`str` or `pathlib.Path`, *optional*):\n-            Path to the directory containing the segmentation masks.\n-        \"\"\"\n         if \"pad_and_return_pixel_mask\" in kwargs:\n             kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n             logger.warning_once(\n@@ -569,7 +533,7 @@ def preprocess(\n             )\n             kwargs[\"size\"] = kwargs.pop(\"max_size\")\n \n-        return super().preprocess(images, annotations, masks_path, **kwargs)\n+        return super().preprocess(images, **kwargs)\n \n     def _preprocess(\n         self,"
        },
        {
            "sha": "5e0622601ac9fe63af953bac827c8e17c8064c06",
            "filename": "src/transformers/models/detr/image_processing_detr.py",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -52,6 +52,7 @@\n     validate_kwargs,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import (\n     TensorType,\n     is_scipy_available,\n@@ -82,6 +83,29 @@\n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n \n \n+class DetrImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n+        Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n+    do_convert_annotations (`bool`, *optional*, defaults to `True`):\n+        Controls whether to convert the annotations to the format expected by the DETR model. Converts the\n+        bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n+        Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n+    return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n+        Whether to return segmentation masks.\n+    annotations (`AnnotationType` or `list[AnnotationType]`, *optional*):\n+        Annotations to transform according to the padding that is applied to the images.\n+    masks_path (`str` or `pathlib.Path`, *optional*):\n+        Path to the directory containing the segmentation masks.\n+    \"\"\"\n+\n+    format: Optional[Union[str, AnnotationFormat]]\n+    do_convert_annotations: Optional[bool]\n+    return_segmentation_masks: Optional[bool]\n+    annotations: Optional[Union[AnnotationType, list[AnnotationType]]]\n+    masks_path: Optional[Union[str, pathlib.Path]]\n+\n+\n # From the original repo: https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/datasets/transforms.py#L76\n def get_size_with_aspect_ratio(image_size, size, max_size=None) -> tuple[int, int]:\n     \"\"\"\n@@ -811,6 +835,7 @@ class DetrImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n+    valid_kwargs = DetrImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "190d01ab55902b3890be7b371bd4e33551fd9196",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 39,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -28,7 +28,6 @@\n from ...image_processing_utils import BatchFeature, get_size_dict\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     SizeDict,\n     get_image_size_for_max_height_width,\n     get_max_height_width,\n@@ -54,6 +53,7 @@\n )\n from ...utils.import_utils import requires\n from .image_processing_detr import (\n+    DetrImageProcessorKwargs,\n     compute_segments,\n     convert_segmentation_to_rle,\n     get_size_with_aspect_ratio,\n@@ -263,23 +263,6 @@ def prepare_coco_panoptic_annotation(\n     return new_target\n \n \n-class DetrFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    r\"\"\"\n-    format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n-        Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n-    do_convert_annotations (`bool`, *optional*, defaults to `True`):\n-        Controls whether to convert the annotations to the format expected by the DETR model. Converts the\n-        bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n-        Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n-    return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n-        Whether to return segmentation masks.\n-    \"\"\"\n-\n-    format: Optional[Union[str, AnnotationFormat]]\n-    do_convert_annotations: Optional[bool]\n-    return_segmentation_masks: Optional[bool]\n-\n-\n @auto_docstring\n @requires(backends=(\"torchvision\", \"torch\"))\n class DetrImageProcessorFast(BaseImageProcessorFast):\n@@ -294,9 +277,9 @@ class DetrImageProcessorFast(BaseImageProcessorFast):\n     size = {\"shortest_edge\": 800, \"longest_edge\": 1333}\n     default_to_square = False\n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n-    valid_kwargs = DetrFastImageProcessorKwargs\n+    valid_kwargs = DetrImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[DetrFastImageProcessorKwargs]) -> None:\n+    def __init__(self, **kwargs: Unpack[DetrImageProcessorKwargs]) -> None:\n         if \"pad_and_return_pixel_mask\" in kwargs:\n             kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n \n@@ -558,25 +541,8 @@ def pad(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        annotations: Optional[Union[AnnotationType, list[AnnotationType]]] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        **kwargs: Unpack[DetrFastImageProcessorKwargs],\n+        **kwargs: Unpack[DetrImageProcessorKwargs],\n     ) -> BatchFeature:\n-        r\"\"\"\n-        annotations (`AnnotationType` or `list[AnnotationType]`, *optional*):\n-            List of annotations associated with the image or batch of images. If annotation is for object\n-            detection, the annotations should be a dictionary with the following keys:\n-            - \"image_id\" (`int`): The image id.\n-            - \"annotations\" (`list[Dict]`): List of annotations for an image. Each annotation should be a\n-                dictionary. An image can have no annotations, in which case the list should be empty.\n-            If annotation is for segmentation, the annotations should be a dictionary with the following keys:\n-            - \"image_id\" (`int`): The image id.\n-            - \"segments_info\" (`list[Dict]`): List of segments for an image. Each segment should be a dictionary.\n-                An image can have no segments, in which case the list should be empty.\n-            - \"file_name\" (`str`): The file name of the image.\n-        masks_path (`str` or `pathlib.Path`, *optional*):\n-            Path to the directory containing the segmentation masks.\n-        \"\"\"\n         if \"pad_and_return_pixel_mask\" in kwargs:\n             kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n             logger.warning_once(\n@@ -591,7 +557,7 @@ def preprocess(\n             )\n             kwargs[\"size\"] = kwargs.pop(\"max_size\")\n \n-        return super().preprocess(images, annotations, masks_path, **kwargs)\n+        return super().preprocess(images, **kwargs)\n \n     def _preprocess(\n         self,"
        },
        {
            "sha": "812a4149cb3f3d52cefffaaaed4f1ee1c4ab9052",
            "filename": "src/transformers/models/dia/processing_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdia%2Fprocessing_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdia%2Fprocessing_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fprocessing_dia.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -111,9 +111,7 @@ def __call__(\n \n         text_kwargs = output_kwargs[\"text_kwargs\"]\n         audio_kwargs = output_kwargs[\"audio_kwargs\"]\n-        common_kwargs = output_kwargs[\"common_kwargs\"]\n-\n-        return_tensors = common_kwargs.pop(\"return_tensors\", None)\n+        return_tensors = text_kwargs.get(\"return_tensors\", None)\n         if return_tensors != \"pt\":\n             raise ValueError(f\"{self.__class__.__name__} only supports `return_tensors='pt'`.\")\n "
        },
        {
            "sha": "5af3650997247abbef1af5437b02b9a09f91a4cf",
            "filename": "src/transformers/models/donut/image_processing_donut.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -40,6 +40,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, logging\n from ...utils.import_utils import is_vision_available, requires\n \n@@ -51,6 +52,18 @@\n     import PIL\n \n \n+class DonutImageProcessorKwargs(ImagesKwargs):\n+    \"\"\"\n+    do_thumbnail (`bool`, *optional*, defaults to `self.do_thumbnail`):\n+        Whether to resize the image using thumbnail method.\n+    do_align_long_axis (`bool`, *optional*, defaults to `self.do_align_long_axis`):\n+        Whether to align the long axis of the image with the long axis of `size` by rotating by 90 degrees.\n+    \"\"\"\n+\n+    do_thumbnail: Optional[bool]\n+    do_align_long_axis: Optional[bool]\n+\n+\n @requires(backends=(\"vision\",))\n class DonutImageProcessor(BaseImageProcessor):\n     r\"\"\"\n@@ -90,6 +103,7 @@ class DonutImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = DonutImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "9a150f4df75f8fcdd4cefafee17da6d45b708881",
            "filename": "src/transformers/models/donut/image_processing_donut_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 17,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -19,7 +19,7 @@\n import torch\n from torchvision.transforms.v2 import functional as F\n \n-from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature, DefaultFastImageProcessorKwargs\n+from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature\n from ...image_transforms import group_images_by_shape, reorder_images\n from ...image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD, ImageInput, PILImageResampling, SizeDict\n from ...processing_utils import Unpack\n@@ -28,24 +28,12 @@\n     auto_docstring,\n     logging,\n )\n+from .image_processing_donut import DonutImageProcessorKwargs\n \n \n logger = logging.get_logger(__name__)\n \n \n-class DonutFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    Args:\n-        do_thumbnail (`bool`, *optional*, defaults to `self.do_thumbnail`):\n-            Whether to resize the image using thumbnail method.\n-        do_align_long_axis (`bool`, *optional*, defaults to `self.do_align_long_axis`):\n-            Whether to align the long axis of the image with the long axis of `size` by rotating by 90 degrees.\n-    \"\"\"\n-\n-    do_thumbnail: Optional[bool]\n-    do_align_long_axis: Optional[bool]\n-\n-\n @auto_docstring\n class DonutImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BILINEAR\n@@ -58,17 +46,17 @@ class DonutImageProcessorFast(BaseImageProcessorFast):\n     do_thumbnail = True\n     do_align_long_axis = False\n     do_pad = True\n-    valid_kwargs = DonutFastImageProcessorKwargs\n+    valid_kwargs = DonutImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[DonutFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[DonutImageProcessorKwargs]):\n         size = kwargs.pop(\"size\", None)\n         if isinstance(size, (tuple, list)):\n             size = size[::-1]\n         kwargs[\"size\"] = size\n         super().__init__(**kwargs)\n \n     @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[DonutFastImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[DonutImageProcessorKwargs]) -> BatchFeature:\n         if \"size\" in kwargs:\n             size = kwargs.pop(\"size\")\n             if isinstance(size, (tuple, list)):"
        },
        {
            "sha": "a545c90539b9e678a13cbbc37523dc601c7a7ce3",
            "filename": "src/transformers/models/donut/processing_donut.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -74,8 +74,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[DonutProcessorKwargs],\n     ):\n         \"\"\""
        },
        {
            "sha": "3ba5a6e30c21869cac9276f839c458077fc29bba",
            "filename": "src/transformers/models/dpt/image_processing_dpt.py",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -44,6 +44,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import (\n     TensorType,\n     filter_out_non_signature_kwargs,\n@@ -63,6 +64,26 @@\n logger = logging.get_logger(__name__)\n \n \n+class DPTImageProcessorKwargs(ImagesKwargs):\n+    \"\"\"\n+    ensure_multiple_of (`int`, *optional*, defaults to 1):\n+        If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Can be overridden\n+        by `ensure_multiple_of` in `preprocess`.\n+    keep_aspect_ratio (`bool`, *optional*, defaults to `False`):\n+        If `True`, the image is resized to the largest possible size such that the aspect ratio is preserved. Can\n+        be overridden by `keep_aspect_ratio` in `preprocess`.\n+    do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n+        Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n+        is used for background, and background itself is not included in all classes of a dataset (e.g.\n+        ADE20k). The background label will be replaced by 255.\n+    \"\"\"\n+\n+    ensure_multiple_of: Optional[int]\n+    size_divisor: Optional[int]\n+    keep_aspect_ratio: Optional[bool]\n+    do_reduce_labels: Optional[bool]\n+\n+\n def get_resize_output_image_size(\n     input_image: np.ndarray,\n     output_size: Union[int, Iterable[int]],\n@@ -151,6 +172,7 @@ class DPTImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = DPTImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "ba0a6d28c56c58a03ad982ccdba007d33dcd1ea7",
            "filename": "src/transformers/models/dpt/image_processing_dpt_fast.py",
            "status": "modified",
            "additions": 6,
            "deletions": 28,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -28,7 +28,7 @@\n from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_base import BatchFeature\n-from ...image_processing_utils_fast import BaseImageProcessorFast, DefaultFastImageProcessorKwargs\n+from ...image_processing_utils_fast import BaseImageProcessorFast\n from ...image_transforms import group_images_by_shape, reorder_images\n from ...image_utils import (\n     IMAGENET_STANDARD_MEAN,\n@@ -41,35 +41,13 @@\n )\n from ...processing_utils import Unpack\n from ...utils import TensorType, auto_docstring, requires_backends\n+from .image_processing_dpt import DPTImageProcessorKwargs\n \n \n if TYPE_CHECKING:\n     from ...modeling_outputs import DepthEstimatorOutput\n \n \n-class DPTFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    ensure_multiple_of (`int`, *optional*, defaults to 1):\n-        If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Can be overridden\n-        by `ensure_multiple_of` in `preprocess`.\n-    size_divisor (`int`, *optional*):\n-        If `do_pad` is `True`, pads the image dimensions to be divisible by this value. This was introduced in the\n-        DINOv2 paper, which uses the model in combination with DPT.\n-    keep_aspect_ratio (`bool`, *optional*, defaults to `False`):\n-        If `True`, the image is resized to the largest possible size such that the aspect ratio is preserved. Can\n-        be overridden by `keep_aspect_ratio` in `preprocess`.\n-    do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n-        Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n-        is used for background, and background itself is not included in all classes of a dataset (e.g.\n-        ADE20k). The background label will be replaced by 255.\n-    \"\"\"\n-\n-    ensure_multiple_of: Optional[int]\n-    size_divisor: Optional[int]\n-    keep_aspect_ratio: Optional[bool]\n-    do_reduce_labels: Optional[bool]\n-\n-\n def get_resize_output_image_size(\n     input_image: \"torch.Tensor\",\n     output_size: Union[int, Iterable[int]],\n@@ -123,13 +101,13 @@ class DPTImageProcessorFast(BaseImageProcessorFast):\n     do_normalize = True\n     do_reduce_labels = None\n \n-    valid_kwargs = DPTFastImageProcessorKwargs\n+    valid_kwargs = DPTImageProcessorKwargs\n     do_pad = False\n     rescale_factor = 1 / 255\n     ensure_multiple_of = 1\n     keep_aspect_ratio = False\n \n-    def __init__(self, **kwargs: Unpack[DPTFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[DPTImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     def reduce_label(self, labels: list[\"torch.Tensor\"]):\n@@ -147,7 +125,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         segmentation_maps: Optional[ImageInput] = None,\n-        **kwargs: Unpack[DPTFastImageProcessorKwargs],\n+        **kwargs: Unpack[DPTImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n         segmentation_maps (`ImageInput`, *optional*):\n@@ -162,7 +140,7 @@ def _preprocess_image_like_inputs(\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n-        **kwargs: Unpack[DPTFastImageProcessorKwargs],\n+        **kwargs: Unpack[DPTImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess image-like inputs."
        },
        {
            "sha": "241d8de122b2617d1daf28211bb3cafec0b34c7f",
            "filename": "src/transformers/models/dpt/modular_dpt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 25,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -21,7 +21,7 @@\n import torch\n \n from ...image_processing_base import BatchFeature\n-from ...image_processing_utils_fast import BaseImageProcessorFast, DefaultFastImageProcessorKwargs\n+from ...image_processing_utils_fast import BaseImageProcessorFast\n from ...image_transforms import group_images_by_shape, reorder_images\n from ...image_utils import (\n     IMAGENET_STANDARD_MEAN,\n@@ -35,6 +35,7 @@\n     requires_backends,\n )\n from ..beit.image_processing_beit_fast import BeitImageProcessorFast\n+from .image_processing_dpt import DPTImageProcessorKwargs\n \n \n if TYPE_CHECKING:\n@@ -82,29 +83,6 @@ def constrain_to_multiple_of(val, multiple, min_val=0, max_val=None):\n     return SizeDict(height=new_height, width=new_width)\n \n \n-class DPTFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    ensure_multiple_of (`int`, *optional*, defaults to 1):\n-        If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Can be overridden\n-        by `ensure_multiple_of` in `preprocess`.\n-    size_divisor (`int`, *optional*):\n-        If `do_pad` is `True`, pads the image dimensions to be divisible by this value. This was introduced in the\n-        DINOv2 paper, which uses the model in combination with DPT.\n-    keep_aspect_ratio (`bool`, *optional*, defaults to `False`):\n-        If `True`, the image is resized to the largest possible size such that the aspect ratio is preserved. Can\n-        be overridden by `keep_aspect_ratio` in `preprocess`.\n-    do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n-        Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n-        is used for background, and background itself is not included in all classes of a dataset (e.g.\n-        ADE20k). The background label will be replaced by 255.\n-    \"\"\"\n-\n-    ensure_multiple_of: Optional[int]\n-    size_divisor: Optional[int]\n-    keep_aspect_ratio: Optional[bool]\n-    do_reduce_labels: Optional[bool]\n-\n-\n @auto_docstring\n class DPTImageProcessorFast(BeitImageProcessorFast):\n     resample = PILImageResampling.BICUBIC\n@@ -123,7 +101,7 @@ class DPTImageProcessorFast(BeitImageProcessorFast):\n     do_center_crop = None\n     do_reduce_labels = None\n \n-    valid_kwargs = DPTFastImageProcessorKwargs\n+    valid_kwargs = DPTImageProcessorKwargs\n \n     def resize(\n         self,"
        },
        {
            "sha": "d1beabb6c2b98d24cc60623ead251ad4e2d3c447",
            "filename": "src/transformers/models/efficientloftr/image_processing_efficientloftr.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -34,6 +34,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, logging, requires_backends\n \n \n@@ -49,6 +50,15 @@\n logger = logging.get_logger(__name__)\n \n \n+class EfficientLoFTRImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    do_grayscale (`bool`, *optional*, defaults to `True`):\n+        Whether to convert the image to grayscale. Can be overridden by `do_grayscale` in the `preprocess` method.\n+    \"\"\"\n+\n+    do_grayscale: Optional[bool] = True\n+\n+\n # Copied from transformers.models.superpoint.image_processing_superpoint.is_grayscale\n def is_grayscale(\n     image: np.ndarray,\n@@ -155,6 +165,7 @@ class EfficientLoFTRImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = EfficientLoFTRImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "994a10f04ee11607b9fe39ae7fba96612fed13e4",
            "filename": "src/transformers/models/efficientloftr/image_processing_efficientloftr_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 13,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -22,7 +22,6 @@\n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -40,6 +39,7 @@\n     TensorType,\n     auto_docstring,\n )\n+from .image_processing_efficientloftr import EfficientLoFTRImageProcessorKwargs\n \n \n if TYPE_CHECKING:\n@@ -108,15 +108,6 @@ def convert_to_grayscale(\n     return F.rgb_to_grayscale(image, num_output_channels=3)\n \n \n-class EfficientLoFTRFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    r\"\"\"\n-    do_grayscale (`bool`, *optional*, defaults to `True`):\n-        Whether to convert the image to grayscale. Can be overridden by `do_grayscale` in the `preprocess` method.\n-    \"\"\"\n-\n-    do_grayscale: Optional[bool] = True\n-\n-\n @auto_docstring\n class EfficientLoFTRImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BILINEAR\n@@ -126,13 +117,13 @@ class EfficientLoFTRImageProcessorFast(BaseImageProcessorFast):\n     do_rescale = True\n     rescale_factor = 1 / 255\n     do_normalize = None\n-    valid_kwargs = EfficientLoFTRFastImageProcessorKwargs\n+    valid_kwargs = EfficientLoFTRImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[EfficientLoFTRFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[EfficientLoFTRImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[EfficientLoFTRFastImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[EfficientLoFTRImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     def _prepare_images_structure("
        },
        {
            "sha": "f5a69eff70e44cd08578e7c6b4e216fd2ba3e669",
            "filename": "src/transformers/models/efficientnet/image_processing_efficientnet.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -33,6 +33,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n \n \n@@ -43,6 +44,18 @@\n logger = logging.get_logger(__name__)\n \n \n+class EfficientNetImageProcessorKwargs(ImagesKwargs):\n+    \"\"\"\n+    rescale_offset (`bool`, *optional*, defaults to `self.rescale_offset`):\n+        Whether to rescale the image between [-max_range/2, scale_range/2] instead of [0, scale_range].\n+    include_top (`bool`, *optional*, defaults to `self.include_top`):\n+        Normalize the image again with the standard deviation only for image classification if set to True.\n+    \"\"\"\n+\n+    rescale_offset: bool\n+    include_top: bool\n+\n+\n class EfficientNetImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a EfficientNet image processor.\n@@ -83,6 +96,7 @@ class EfficientNetImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = EfficientNetImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "5f3439aaa27308b29d804f5ddbb026b9933333fd",
            "filename": "src/transformers/models/efficientnet/image_processing_efficientnet_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 17,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -20,27 +20,15 @@\n import torch\n from torchvision.transforms.v2 import functional as F\n \n-from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature, DefaultFastImageProcessorKwargs\n+from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature\n from ...image_transforms import group_images_by_shape, reorder_images\n from ...image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD, ImageInput, PILImageResampling, SizeDict\n from ...processing_utils import Unpack\n from ...utils import (\n     TensorType,\n     auto_docstring,\n )\n-\n-\n-class EfficientNetFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    Args:\n-        rescale_offset (`bool`, *optional*, defaults to `self.rescale_offset`):\n-            Whether to rescale the image between [-max_range/2, scale_range/2] instead of [0, scale_range].\n-        include_top (`bool`, *optional*, defaults to `self.include_top`):\n-            Normalize the image again with the standard deviation only for image classification if set to True.\n-    \"\"\"\n-\n-    rescale_offset: bool\n-    include_top: bool\n+from .image_processing_efficientnet import EfficientNetImageProcessorKwargs\n \n \n @auto_docstring\n@@ -57,9 +45,9 @@ class EfficientNetImageProcessorFast(BaseImageProcessorFast):\n     rescale_offset = False\n     do_normalize = True\n     include_top = True\n-    valid_kwargs = EfficientNetFastImageProcessorKwargs\n+    valid_kwargs = EfficientNetImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[EfficientNetFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[EfficientNetImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     def rescale(\n@@ -195,7 +183,7 @@ def _preprocess(\n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n \n     @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[EfficientNetFastImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[EfficientNetImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n "
        },
        {
            "sha": "fca5316a3fcafbd99b94d5c174b05b8c1aad55e0",
            "filename": "src/transformers/models/emu3/image_processing_emu3.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -37,6 +37,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, is_vision_available, logging\n \n \n@@ -46,6 +47,11 @@\n logger = logging.get_logger(__name__)\n \n \n+class Emu3ImageProcessorKwargs(ImagesKwargs):\n+    ratio: Optional[str]\n+    image_area: Optional[int]\n+\n+\n def smart_resize(\n     height: int, width: int, factor: int = 28, min_pixels: int = 56 * 56, max_pixels: int = 14 * 14 * 4 * 1280\n ):\n@@ -108,6 +114,7 @@ class Emu3ImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"image_sizes\"]\n+    valid_kwargs = Emu3ImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "b7ed8e9074f09b2f64c29d6fc3643f05721a5ac4",
            "filename": "src/transformers/models/emu3/processing_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -20,7 +20,7 @@\n \n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput\n-from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n+from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import is_vision_available\n \n@@ -33,14 +33,8 @@ class Emu3TextKwargs(TextKwargs, total=False):\n     return_for_image_generation: bool\n \n \n-class Emu3ImagesKwargs(ImagesKwargs, total=False):\n-    ratio: str\n-    image_area: int\n-\n-\n class Emu3ProcessorKwargs(ProcessingKwargs, total=False):\n     text_kwargs: Emu3TextKwargs\n-    images_kwargs: Emu3ImagesKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"return_for_image_generation\": False,\n@@ -95,8 +89,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[Emu3ProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "189aaaf41d4dd550f6a0657b610261c2cf05f628",
            "filename": "src/transformers/models/eomt/image_processing_eomt.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -36,6 +36,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import (\n     IMAGENET_DEFAULT_MEAN,\n     IMAGENET_DEFAULT_STD,\n@@ -53,6 +54,21 @@\n     import torch.nn.functional as F\n \n \n+class EomtImageProcessorKwargs(ImagesKwargs):\n+    \"\"\"\n+    do_split_image (`bool`, *optional*, defaults to `False`):\n+        Whether to split the input images into overlapping patches for semantic segmentation. If set to `True`, the\n+        input images will be split into patches of size `size[\"shortest_edge\"]` with an overlap between patches.\n+        Otherwise, the input images will be padded to the target size.\n+    ignore_index (`int`, *optional*):\n+        Label to be assigned to background pixels in segmentation maps. If provided, segmentation map pixels\n+        denoted with 0 (background) will be replaced with `ignore_index`.\n+    \"\"\"\n+\n+    do_split_image: bool\n+    ignore_index: Optional[int] = None\n+\n+\n # Adapted from transformers.models.maskformer.image_processing_maskformer.convert_segmentation_map_to_binary_masks\n def convert_segmentation_map_to_binary_masks(\n     segmentation_map: np.ndarray,"
        },
        {
            "sha": "68fd7bb007442302153481b9730a4056a9e129ef",
            "filename": "src/transformers/models/eomt/image_processing_eomt_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 24,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -24,7 +24,6 @@\n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -43,32 +42,14 @@\n     filter_out_non_signature_kwargs,\n )\n from .image_processing_eomt import (\n+    EomtImageProcessorKwargs,\n     compute_segments,\n     convert_segmentation_map_to_binary_masks,\n     get_size_with_aspect_ratio,\n     remove_low_and_no_objects,\n )\n \n \n-class EomtImageProcessorFastKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    do_split_image (`bool`, *optional*, defaults to `False`):\n-            Whether to split the input images into overlapping patches for semantic segmentation. If set to `True`, the\n-            input images will be split into patches of size `size[\"shortest_edge\"]` with an overlap between patches.\n-            Otherwise, the input images will be padded to the target size.\n-    do_pad (`bool`, *optional*, defaults to `False`):\n-            Whether to pad the image. If `True`, will pad the patch dimension of the images in the batch to the largest\n-            number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n-    ignore_index (`int`, *optional*):\n-            Label to be assigned to background pixels in segmentation maps. If provided, segmentation map pixels\n-            denoted with 0 (background) will be replaced with `ignore_index`.\n-    \"\"\"\n-\n-    do_split_image: bool\n-    do_pad: bool\n-    ignore_index: Optional[int] = None\n-\n-\n def get_target_size(size_dict: dict[str, int]) -> tuple[int, int]:\n     \"\"\"Returns the height and width from a size dict.\"\"\"\n     target_height = size_dict[\"shortest_edge\"]\n@@ -102,9 +83,9 @@ class EomtImageProcessorFast(BaseImageProcessorFast):\n     do_split_image = False\n     do_pad = False\n     ignore_index = None\n-    valid_kwargs = EomtImageProcessorFastKwargs\n+    valid_kwargs = EomtImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[EomtImageProcessorFastKwargs]):\n+    def __init__(self, **kwargs: Unpack[EomtImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     def _split_image(self, images: torch.Tensor, size: dict, image_indices: int) -> tuple[list, list]:\n@@ -153,7 +134,7 @@ def preprocess(\n         images: ImageInput,\n         segmentation_maps: Optional[list[torch.Tensor]] = None,\n         instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n-        **kwargs: Unpack[EomtImageProcessorFastKwargs],\n+        **kwargs: Unpack[EomtImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n         segmentation_maps (`ImageInput`, *optional*):\n@@ -171,7 +152,7 @@ def _preprocess_image_like_inputs(\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n-        **kwargs: Unpack[EomtImageProcessorFastKwargs],\n+        **kwargs: Unpack[EomtImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess image-like inputs."
        },
        {
            "sha": "3c19a240516924bb4ce2736f3aa9a4b9530f81c5",
            "filename": "src/transformers/models/flava/image_processing_flava.py",
            "status": "modified",
            "additions": 85,
            "deletions": 0,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -37,6 +37,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n from ...utils.import_utils import requires\n \n@@ -56,6 +57,89 @@\n LOGIT_LAPLACE_EPS: float = 0.1\n \n \n+class FlavaImageProcessorKwargs(ImagesKwargs):\n+    \"\"\"\n+    return_image_mask (`bool`, *optional*, defaults to `False`):\n+        Whether to return the image mask. Can be overridden by the `return_image_mask` parameter in `preprocess`.\n+    input_size_patches (`int`, *optional*, defaults to 14):\n+        Number of patches in the image in height and width direction. 14x14 = 196 total patches. Can be overridden\n+        by the `input_size_patches` parameter in `preprocess`.\n+    total_mask_patches (`int`, *optional*, defaults to 75):\n+        Total number of patches that should be masked. Can be overridden by the `total_mask_patches` parameter in\n+        `preprocess`.\n+    mask_group_min_patches (`int`, *optional*, defaults to 16):\n+        Minimum number of patches that should be masked. Can be overridden by the `mask_group_min_patches`\n+        parameter in `preprocess`.\n+    mask_group_max_patches (`int`, *optional*):\n+        Maximum number of patches that should be masked. Can be overridden by the `mask_group_max_patches`\n+        parameter in `preprocess`.\n+    mask_group_min_aspect_ratio (`float`, *optional*, defaults to 0.3):\n+        Minimum aspect ratio of the mask window. Can be overridden by the `mask_group_min_aspect_ratio` parameter\n+        in `preprocess`.\n+    mask_group_max_aspect_ratio (`float`, *optional*):\n+        Maximum aspect ratio of the mask window. Can be overridden by the `mask_group_max_aspect_ratio` parameter\n+        in `preprocess`.\n+    return_codebook_pixels (`bool`, *optional*, defaults to `False`):\n+        Whether to return the codebook pixel values.\n+    codebook_do_resize (`bool`, *optional*, defaults to `True`):\n+        Whether to resize the input for codebook to a certain. Can be overridden by the `codebook_do_resize`\n+        parameter in `preprocess`. `codebook_size`.\n+    codebook_size (`dict[str, int]`, *optional*, defaults to `{\"height\": 224, \"width\": 224}`):\n+        Resize the input for codebook to the given size. Can be overridden by the `codebook_size` parameter in\n+        `preprocess`.\n+    codebook_resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.LANCZOS`):\n+        Resampling filter to use if resizing the codebook image. Can be overridden by the `codebook_resample`\n+        parameter in `preprocess`.\n+    codebook_do_center_crop (`bool`, *optional*, defaults to `True`):\n+        Whether to crop the input for codebook at the center. If the input size is smaller than\n+        `codebook_crop_size` along any edge, the image is padded with 0's and then center cropped. Can be\n+        overridden by the `codebook_do_center_crop` parameter in `preprocess`.\n+    codebook_crop_size (`dict[str, int]`, *optional*, defaults to `{\"height\": 224, \"width\": 224}`):\n+        Desired output size for codebook input when applying center-cropping. Can be overridden by the\n+        `codebook_crop_size` parameter in `preprocess`.\n+    codebook_do_rescale (`bool`, *optional*, defaults to `True`):\n+        Whether to rescale the input for codebook by the specified scale `codebook_rescale_factor`. Can be\n+        overridden by the `codebook_do_rescale` parameter in `preprocess`.\n+    codebook_rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+        Defines the scale factor to use if rescaling the codebook image. Can be overridden by the\n+        `codebook_rescale_factor` parameter in `preprocess`.\n+    codebook_do_map_pixels (`bool`, *optional*, defaults to `True`):\n+        Whether to map the pixel values of the codebook input to (1 - 2e)x + e. Can be overridden by the\n+        `codebook_do_map_pixels` parameter in `preprocess`.\n+    codebook_do_normalize (`bool`, *optional*, defaults to `True`):\n+        Whether or not to normalize the input for codebook with `codebook_image_mean` and `codebook_image_std`. Can\n+        be overridden by the `codebook_do_normalize` parameter in `preprocess`.\n+    codebook_image_mean (`Optional[Union[float, Iterable[float]]]`, *optional*, defaults to `[0, 0, 0]`):\n+        The sequence of means for each channel, to be used when normalizing images for codebook. Can be overridden\n+        by the `codebook_image_mean` parameter in `preprocess`.\n+    codebook_image_std (`Optional[Union[float, Iterable[float]]]`, *optional*, defaults to `[0.5, 0.5, 0.5]`):\n+        The sequence of standard deviations for each channel, to be used when normalizing images for codebook. Can\n+        be overridden by the `codebook_image_std` parameter in `preprocess`.\n+    \"\"\"\n+\n+    # Mask related params\n+    return_image_mask: Optional[bool]\n+    input_size_patches: Optional[int]\n+    total_mask_patches: Optional[int]\n+    mask_group_min_patches: Optional[int]\n+    mask_group_max_patches: Optional[int]\n+    mask_group_min_aspect_ratio: Optional[float]\n+    mask_group_max_aspect_ratio: Optional[float]\n+    # Codebook related params\n+    return_codebook_pixels: Optional[bool]\n+    codebook_do_resize: Optional[bool]\n+    codebook_size: Optional[bool]\n+    codebook_resample: Optional[int]\n+    codebook_do_center_crop: Optional[bool]\n+    codebook_crop_size: Optional[int]\n+    codebook_do_rescale: Optional[bool]\n+    codebook_rescale_factor: Optional[Union[int, float]]\n+    codebook_do_map_pixels: Optional[bool]\n+    codebook_do_normalize: Optional[bool]\n+    codebook_image_mean: Optional[Union[float, Iterable[float]]]\n+    codebook_image_std: Optional[Union[float, Iterable[float]]]\n+\n+\n # Inspired from https://github.com/microsoft/unilm/blob/master/beit/masking_generator.py\n class FlavaMaskingGenerator:\n     def __init__(\n@@ -225,6 +309,7 @@ class FlavaImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = FlavaImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "0dfbd07f17a7425bdf3c99035c5ef5b409f3e86c",
            "filename": "src/transformers/models/flava/image_processing_flava_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 89,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -16,7 +16,6 @@\n \n import math\n import random\n-from collections.abc import Iterable\n from functools import lru_cache\n from typing import Any, Optional, Union\n \n@@ -26,7 +25,6 @@\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n     BatchFeature,\n-    DefaultFastImageProcessorKwargs,\n     get_size_dict,\n )\n from ...image_transforms import ChannelDimension, group_images_by_shape, reorder_images\n@@ -42,6 +40,7 @@\n     FLAVA_IMAGE_MEAN,\n     FLAVA_IMAGE_STD,\n     LOGIT_LAPLACE_EPS,\n+    FlavaImageProcessorKwargs,\n )\n \n \n@@ -121,90 +120,6 @@ def __call__(self):\n         return mask\n \n \n-class FlavaFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    Args:\n-        return_image_mask (`bool`, *optional*, defaults to `False`):\n-            Whether to return the image mask. Can be overridden by the `return_image_mask` parameter in `preprocess`.\n-        input_size_patches (`int`, *optional*, defaults to 14):\n-            Number of patches in the image in height and width direction. 14x14 = 196 total patches. Can be overridden\n-            by the `input_size_patches` parameter in `preprocess`.\n-        total_mask_patches (`int`, *optional*, defaults to 75):\n-            Total number of patches that should be masked. Can be overridden by the `total_mask_patches` parameter in\n-            `preprocess`.\n-        mask_group_min_patches (`int`, *optional*, defaults to 16):\n-            Minimum number of patches that should be masked. Can be overridden by the `mask_group_min_patches`\n-            parameter in `preprocess`.\n-        mask_group_max_patches (`int`, *optional*):\n-            Maximum number of patches that should be masked. Can be overridden by the `mask_group_max_patches`\n-            parameter in `preprocess`.\n-        mask_group_min_aspect_ratio (`float`, *optional*, defaults to 0.3):\n-            Minimum aspect ratio of the mask window. Can be overridden by the `mask_group_min_aspect_ratio` parameter\n-            in `preprocess`.\n-        mask_group_max_aspect_ratio (`float`, *optional*):\n-            Maximum aspect ratio of the mask window. Can be overridden by the `mask_group_max_aspect_ratio` parameter\n-            in `preprocess`.\n-        return_codebook_pixels (`bool`, *optional*, defaults to `False`):\n-            Whether to return the codebook pixel values.\n-        codebook_do_resize (`bool`, *optional*, defaults to `True`):\n-            Whether to resize the input for codebook to a certain. Can be overridden by the `codebook_do_resize`\n-            parameter in `preprocess`. `codebook_size`.\n-        codebook_size (`dict[str, int]`, *optional*, defaults to `{\"height\": 224, \"width\": 224}`):\n-            Resize the input for codebook to the given size. Can be overridden by the `codebook_size` parameter in\n-            `preprocess`.\n-        codebook_resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.LANCZOS`):\n-            Resampling filter to use if resizing the codebook image. Can be overridden by the `codebook_resample`\n-            parameter in `preprocess`.\n-        codebook_do_center_crop (`bool`, *optional*, defaults to `True`):\n-            Whether to crop the input for codebook at the center. If the input size is smaller than\n-            `codebook_crop_size` along any edge, the image is padded with 0's and then center cropped. Can be\n-            overridden by the `codebook_do_center_crop` parameter in `preprocess`.\n-        codebook_crop_size (`dict[str, int]`, *optional*, defaults to `{\"height\": 224, \"width\": 224}`):\n-            Desired output size for codebook input when applying center-cropping. Can be overridden by the\n-            `codebook_crop_size` parameter in `preprocess`.\n-        codebook_do_rescale (`bool`, *optional*, defaults to `True`):\n-            Whether to rescale the input for codebook by the specified scale `codebook_rescale_factor`. Can be\n-            overridden by the `codebook_do_rescale` parameter in `preprocess`.\n-        codebook_rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n-            Defines the scale factor to use if rescaling the codebook image. Can be overridden by the\n-            `codebook_rescale_factor` parameter in `preprocess`.\n-        codebook_do_map_pixels (`bool`, *optional*, defaults to `True`):\n-            Whether to map the pixel values of the codebook input to (1 - 2e)x + e. Can be overridden by the\n-            `codebook_do_map_pixels` parameter in `preprocess`.\n-        codebook_do_normalize (`bool`, *optional*, defaults to `True`):\n-            Whether or not to normalize the input for codebook with `codebook_image_mean` and `codebook_image_std`. Can\n-            be overridden by the `codebook_do_normalize` parameter in `preprocess`.\n-        codebook_image_mean (`Optional[Union[float, Iterable[float]]]`, *optional*, defaults to `[0, 0, 0]`):\n-            The sequence of means for each channel, to be used when normalizing images for codebook. Can be overridden\n-            by the `codebook_image_mean` parameter in `preprocess`.\n-        codebook_image_std (`Optional[Union[float, Iterable[float]]]`, *optional*, defaults to `[0.5, 0.5, 0.5]`):\n-            The sequence of standard deviations for each channel, to be used when normalizing images for codebook. Can\n-            be overridden by the `codebook_image_std` parameter in `preprocess`.\n-    \"\"\"\n-\n-    # Mask related params\n-    return_image_mask: Optional[bool]\n-    input_size_patches: Optional[int]\n-    total_mask_patches: Optional[int]\n-    mask_group_min_patches: Optional[int]\n-    mask_group_max_patches: Optional[int]\n-    mask_group_min_aspect_ratio: Optional[float]\n-    mask_group_max_aspect_ratio: Optional[float]\n-    # Codebook related params\n-    return_codebook_pixels: Optional[bool]\n-    codebook_do_resize: Optional[bool]\n-    codebook_size: Optional[bool]\n-    codebook_resample: Optional[int]\n-    codebook_do_center_crop: Optional[bool]\n-    codebook_crop_size: Optional[int]\n-    codebook_do_rescale: Optional[bool]\n-    codebook_rescale_factor: Optional[Union[int, float]]\n-    codebook_do_map_pixels: Optional[bool]\n-    codebook_do_normalize: Optional[bool]\n-    codebook_image_mean: Optional[Union[float, Iterable[float]]]\n-    codebook_image_std: Optional[Union[float, Iterable[float]]]\n-\n-\n @auto_docstring\n class FlavaImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BICUBIC\n@@ -239,13 +154,13 @@ class FlavaImageProcessorFast(BaseImageProcessorFast):\n     codebook_do_normalize = True\n     codebook_image_mean = FLAVA_CODEBOOK_MEAN\n     codebook_image_std = FLAVA_CODEBOOK_STD\n-    valid_kwargs = FlavaFastImageProcessorKwargs\n+    valid_kwargs = FlavaImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[FlavaFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[FlavaImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[DefaultFastImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[FlavaImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     @classmethod"
        },
        {
            "sha": "8e8a806e86157cb8f803f8b33946284244fb75fa",
            "filename": "src/transformers/models/flava/processing_flava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 33,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fflava%2Fprocessing_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fflava%2Fprocessing_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fprocessing_flava.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -17,39 +17,8 @@\n \"\"\"\n \n import warnings\n-from collections.abc import Iterable\n-from typing import Optional, Union\n \n-from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin\n-\n-\n-class FlavaImagesKwargs(ImagesKwargs):\n-    # Mask related params\n-    return_image_mask: Optional[bool]\n-    input_size_patches: Optional[int]\n-    total_mask_patches: Optional[int]\n-    mask_group_min_patches: Optional[int]\n-    mask_group_max_patches: Optional[int]\n-    mask_group_min_aspect_ratio: Optional[float]\n-    mask_group_max_aspect_ratio: Optional[float]\n-    # Codebook related params\n-    return_codebook_pixels: Optional[bool]\n-    codebook_do_resize: Optional[bool]\n-    codebook_size: Optional[bool]\n-    codebook_resample: Optional[int]\n-    codebook_do_center_crop: Optional[bool]\n-    codebook_crop_size: Optional[int]\n-    codebook_do_rescale: Optional[bool]\n-    codebook_rescale_factor: Optional[Union[int, float]]\n-    codebook_do_map_pixels: Optional[bool]\n-    codebook_do_normalize: Optional[bool]\n-    codebook_image_mean: Optional[Union[float, Iterable[float]]]\n-    codebook_image_std: Optional[Union[float, Iterable[float]]]\n-\n-\n-class FlavaProcessorKwargs(ProcessingKwargs, total=False):\n-    images_kwargs: FlavaImagesKwargs\n-    _defaults = {}\n+from ...processing_utils import ProcessorMixin\n \n \n class FlavaProcessor(ProcessorMixin):\n@@ -67,7 +36,6 @@ class FlavaProcessor(ProcessorMixin):\n     attributes = [\"image_processor\", \"tokenizer\"]\n     image_processor_class = \"FlavaImageProcessor\"\n     tokenizer_class = (\"BertTokenizer\", \"BertTokenizerFast\")\n-    valid_processor_kwargs = FlavaProcessorKwargs\n \n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         feature_extractor = None"
        },
        {
            "sha": "1c25ddceeafc51a0f7950739dfe90bbee24e9ebd",
            "filename": "src/transformers/models/florence2/processing_florence2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fflorence2%2Fprocessing_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fflorence2%2Fprocessing_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fprocessing_florence2.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -39,7 +39,6 @@\n class Florence2ProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {\n         \"text_kwargs\": {\"padding\": False, \"return_mm_token_type_ids\": False},\n-        \"images_kwargs\": {},\n     }\n \n "
        },
        {
            "sha": "75b2bbad926eeb1318b40fbf65eeabd7bd4f56f0",
            "filename": "src/transformers/models/fuyu/processing_fuyu.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -71,7 +71,6 @@ class FuyuProcessorKwargs(ProcessingKwargs, total=False):\n             \"verbose\": True,\n             \"return_mm_token_type_ids\": False,\n         },\n-        \"images_kwargs\": {},\n     }\n \n \n@@ -487,8 +486,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[FuyuProcessorKwargs],\n     ) -> \"FuyuBatchFeature\":\n         \"\"\""
        },
        {
            "sha": "5206a13a04a3a6b07ea25fbe6930aef40544c1a7",
            "filename": "src/transformers/models/gemma3/image_processing_gemma3.py",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -40,6 +40,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n \n \n@@ -50,6 +51,24 @@\n     import PIL\n \n \n+class Gemma3ImageProcessorKwargs(ImagesKwargs):\n+    \"\"\"\n+    do_pan_and_scan (`bool`, *optional*):\n+        Whether to apply `pan_and_scan` to images.\n+    pan_and_scan_min_crop_size (`int`, *optional*):\n+        Minimum size of each crop in pan and scan.\n+    pan_and_scan_max_num_crops (`int`, *optional*):\n+        Maximum number of crops per image in pan and scan.\n+    pan_and_scan_min_ratio_to_activate (`float`, *optional*):\n+        Minimum aspect ratio to activate pan and scan.\n+    \"\"\"\n+\n+    do_pan_and_scan: Optional[bool]\n+    pan_and_scan_min_crop_size: Optional[int]\n+    pan_and_scan_max_num_crops: Optional[int]\n+    pan_and_scan_min_ratio_to_activate: Optional[float]\n+\n+\n class Gemma3ImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a SigLIP image processor.\n@@ -91,6 +110,7 @@ class Gemma3ImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"num_crops\"]\n+    valid_kwargs = Gemma3ImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "bfb58be2a8e1eccd90bb545910f8dff4d82ddcc0",
            "filename": "src/transformers/models/gemma3/image_processing_gemma3_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 22,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -24,7 +24,6 @@\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n     BatchFeature,\n-    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -35,29 +34,12 @@\n     auto_docstring,\n     logging,\n )\n+from .image_processing_gemma3 import Gemma3ImageProcessorKwargs\n \n \n logger = logging.get_logger(__name__)\n \n \n-class Gemma3FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    do_pan_and_scan (`bool`, *optional*):\n-        Whether to apply `pan_and_scan` to images.\n-    pan_and_scan_min_crop_size (`int`, *optional*):\n-        Minimum size of each crop in pan and scan.\n-    pan_and_scan_max_num_crops (`int`, *optional*):\n-        Maximum number of crops per image in pan and scan.\n-    pan_and_scan_min_ratio_to_activate (`float`, *optional*):\n-        Minimum aspect ratio to activate pan and scan.\n-    \"\"\"\n-\n-    do_pan_and_scan: Optional[bool]\n-    pan_and_scan_min_crop_size: Optional[int]\n-    pan_and_scan_max_num_crops: Optional[int]\n-    pan_and_scan_min_ratio_to_activate: Optional[float]\n-\n-\n @auto_docstring\n class Gemma3ImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BILINEAR\n@@ -73,9 +55,9 @@ class Gemma3ImageProcessorFast(BaseImageProcessorFast):\n     pan_and_scan_min_crop_size = None\n     pan_and_scan_max_num_crops = None\n     pan_and_scan_min_ratio_to_activate = None\n-    valid_kwargs = Gemma3FastImageProcessorKwargs\n+    valid_kwargs = Gemma3ImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[Gemma3FastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[Gemma3ImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     def pan_and_scan_batched(\n@@ -167,7 +149,7 @@ def _process_images_for_pan_and_scan(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        **kwargs: Unpack[Gemma3FastImageProcessorKwargs],\n+        **kwargs: Unpack[Gemma3ImageProcessorKwargs],\n     ) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n "
        },
        {
            "sha": "a9bac5b69e47097f849299d668db22911d9eba41",
            "filename": "src/transformers/models/gemma3/processing_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -20,21 +20,12 @@\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, make_nested_list_of_images\n-from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import to_py_obj\n \n \n-class Gemma3ImagesKwargs(ImagesKwargs):\n-    do_pan_and_scan: Optional[bool]\n-    pan_and_scan_min_crop_size: Optional[int]\n-    pan_and_scan_max_num_crops: Optional[int]\n-    pan_and_scan_min_ratio_to_activate: Optional[float]\n-    do_convert_rgb: Optional[bool]\n-\n-\n class Gemma3ProcessorKwargs(ProcessingKwargs, total=False):\n-    images_kwargs: Gemma3ImagesKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,\n@@ -81,8 +72,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        videos=None,\n-        audio=None,\n         **kwargs: Unpack[Gemma3ProcessorKwargs],\n     ) -> BatchFeature:\n         if text is None and images is None:"
        },
        {
            "sha": "105b1983b7c757d2114ce89dccf309906001cf52",
            "filename": "src/transformers/models/gemma3n/processing_gemma3n.py",
            "status": "modified",
            "additions": 2,
            "deletions": 11,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fprocessing_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fprocessing_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fprocessing_gemma3n.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -19,21 +19,13 @@\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, make_nested_list_of_images\n-from ...processing_utils import AudioKwargs, ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n \n \n-class Gemma3nImagesKwargs(ImagesKwargs):\n-    do_convert_rgb: Optional[bool]\n-\n-\n class Gemma3nProcessorKwargs(ProcessingKwargs, total=False):\n-    audio_kwargs: AudioKwargs\n-    images_kwargs: Gemma3nImagesKwargs\n     _defaults = {\n-        \"text_kwargs\": {\n-            \"padding\": False,\n-        },\n+        \"text_kwargs\": {\"padding\": False},\n     }\n \n \n@@ -101,7 +93,6 @@ def __call__(\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio: Optional[Union[np.ndarray, list[float], list[np.ndarray], list[list[float]]]] = None,\n-        videos=None,\n         **kwargs: Unpack[Gemma3nProcessorKwargs],\n     ) -> BatchFeature:\n         if text is None and images is None and audio is None:"
        },
        {
            "sha": "13f4472e61f33e843e254bbe9edfc6f13a078ab2",
            "filename": "src/transformers/models/glm4v/image_processing_glm4v.py",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -39,13 +39,29 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, logging\n from ...video_utils import VideoInput\n \n \n logger = logging.get_logger(__name__)\n \n \n+class Glm4vImageProcessorKwargs(ImagesKwargs):\n+    \"\"\"\n+    patch_size (`int`, *optional*, defaults to 14):\n+        The spatial patch size of the vision encoder.\n+    temporal_patch_size (`int`, *optional*, defaults to 2):\n+        The temporal patch size of the vision encoder.\n+    merge_size (`int`, *optional*, defaults to 2):\n+        The merge size of the vision encoder to llm encoder.\n+    \"\"\"\n+\n+    patch_size: Optional[int]\n+    temporal_patch_size: Optional[int]\n+    merge_size: Optional[int]\n+\n+\n def smart_resize(\n     num_frames: int,\n     height: int,\n@@ -120,6 +136,7 @@ class Glm4vImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"image_grid_thw\"]\n+    valid_kwargs = Glm4vImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "92e8a5df9137dbe41308464798aac03354870bd1",
            "filename": "src/transformers/models/glm4v/image_processing_glm4v_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 20,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -24,7 +24,6 @@\n )\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -41,27 +40,12 @@\n     auto_docstring,\n     logging,\n )\n-from .image_processing_glm4v import smart_resize\n+from .image_processing_glm4v import Glm4vImageProcessorKwargs, smart_resize\n \n \n logger = logging.get_logger(__name__)\n \n \n-class Glm4vFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    patch_size (`int`, *optional*, defaults to 14):\n-        The spatial patch size of the vision encoder.\n-    temporal_patch_size (`int`, *optional*, defaults to 2):\n-        The temporal patch size of the vision encoder.\n-    merge_size (`int`, *optional*, defaults to 2):\n-        The merge size of the vision encoder to llm encoder.\n-    \"\"\"\n-\n-    patch_size: Optional[int]\n-    temporal_patch_size: Optional[int]\n-    merge_size: Optional[int]\n-\n-\n @auto_docstring\n class Glm4vImageProcessorFast(BaseImageProcessorFast):\n     do_resize = True\n@@ -75,10 +59,10 @@ class Glm4vImageProcessorFast(BaseImageProcessorFast):\n     patch_size = 14\n     temporal_patch_size = 2\n     merge_size = 2\n-    valid_kwargs = Glm4vFastImageProcessorKwargs\n+    valid_kwargs = Glm4vImageProcessorKwargs\n     model_input_names = [\"pixel_values\", \"image_grid_thw\"]\n \n-    def __init__(self, **kwargs: Unpack[Glm4vFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[Glm4vImageProcessorKwargs]):\n         super().__init__(**kwargs)\n         if self.size is not None and (\n             self.size.get(\"shortest_edge\", None) is None or self.size.get(\"longest_edge\", None) is None\n@@ -205,7 +189,7 @@ def _preprocess(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        **kwargs: Unpack[Glm4vFastImageProcessorKwargs],\n+        **kwargs: Unpack[Glm4vImageProcessorKwargs],\n     ) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n "
        },
        {
            "sha": "ac2885a4a9f88de3c82e786c6acdb2dec3ea0a52",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 14,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -32,7 +32,7 @@\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_rope_utils import rope_config_validation\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n-from ...processing_utils import ImagesKwargs, Unpack\n+from ...processing_utils import Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n from ...utils.generic import check_model_inputs\n@@ -52,7 +52,6 @@\n     Qwen2_5_VLVisionAttention,\n     Qwen2_5_VLVisionBlock,\n )\n-from ..qwen2_5_vl.processing_qwen2_5_vl import Qwen2_5_VLVideosProcessorKwargs\n from ..qwen2_vl.processing_qwen2_vl import (\n     Qwen2_VLProcessor,\n     Qwen2_VLProcessorKwargs,\n@@ -1508,19 +1507,7 @@ def _get_image_nums_and_video_nums(\n         return image_counts, video_counts\n \n \n-class Glm4vVideosProcessorKwargs(Qwen2_5_VLVideosProcessorKwargs):\n-    pass\n-\n-\n-class Glm4vImagesKwargs(ImagesKwargs):\n-    patch_size: Optional[int]\n-    temporal_patch_size: Optional[int]\n-    merge_size: Optional[int]\n-\n-\n class Glm4vProcessorKwargs(Qwen2_VLProcessorKwargs):\n-    images_kwargs: Glm4vImagesKwargs\n-    videos_kwargs: Glm4vVideosProcessorKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,"
        },
        {
            "sha": "ad97a10efd737ea81b95af3826647fffc08f3214",
            "filename": "src/transformers/models/glm4v/processing_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 13,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -24,7 +24,7 @@\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n-from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack, VideosKwargs\n+from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import logging\n from ...video_utils import VideoInput\n@@ -33,18 +33,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class Glm4vVideosProcessorKwargs(VideosKwargs, total=False):\n-    fps: Union[list[float], float]\n-\n-\n-class Glm4vImagesKwargs(ImagesKwargs):\n-    patch_size: Optional[int]\n-    temporal_patch_size: Optional[int]\n-    merge_size: Optional[int]\n-\n-\n class Glm4vProcessorKwargs(ProcessingKwargs, total=False):\n-    images_kwargs: Glm4vImagesKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,\n@@ -53,7 +42,6 @@ class Glm4vProcessorKwargs(ProcessingKwargs, total=False):\n         },\n         \"videos_kwargs\": {\"return_metadata\": True},\n     }\n-    videos_kwargs: Glm4vVideosProcessorKwargs\n \n \n class Glm4vProcessor(ProcessorMixin):"
        },
        {
            "sha": "8324ad482baa0759dafe40df7c0e9e7c3de78e9d",
            "filename": "src/transformers/models/glm4v/video_processing_glm4v.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -37,12 +37,11 @@\n \n \n class Glm4vVideoProcessorInitKwargs(VideosKwargs):\n-    max_image_size: dict[str, int] = None\n-    patch_size: Optional[int] = None\n-    temporal_patch_size: Optional[int] = None\n-    merge_size: Optional[int] = None\n-    image_mean: Optional[list[float]] = None\n-    image_std: Optional[list[float]] = None\n+    max_image_size: Optional[dict[str, int]]\n+    patch_size: Optional[int]\n+    temporal_patch_size: Optional[int]\n+    merge_size: Optional[int]\n+    max_duration: Optional[int]\n \n \n @add_start_docstrings("
        },
        {
            "sha": "3424020c65b3759485212889e487f5d6db1e2446",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2.py",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -38,6 +38,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n \n \n@@ -48,6 +49,24 @@\n logger = logging.get_logger(__name__)\n \n \n+class GotOcr2ImageProcessorKwargs(ImagesKwargs):\n+    \"\"\"\n+    crop_to_patches (`bool`, *optional*, defaults to `False`):\n+        Whether to crop the image to patches. Can be overridden by the `crop_to_patches` parameter in the\n+        `preprocess` method.\n+    min_patches (`int`, *optional*, defaults to 1):\n+        The minimum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n+        set to `True`. Can be overridden by the `min_patches` parameter in the `preprocess` method.\n+    max_patches (`int`, *optional*, defaults to 12):\n+        The maximum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n+        set to `True`. Can be overridden by the `max_patches` parameter in the `preprocess` method.\n+    \"\"\"\n+\n+    crop_to_patches: Optional[bool]\n+    min_patches: Optional[int]\n+    max_patches: Optional[int]\n+\n+\n # Similar to image_processing_mllama.get_all_supported_aspect_ratios\n @lru_cache(maxsize=10)\n def get_all_supported_aspect_ratios(min_image_tiles: int, max_image_tiles: int) -> list[tuple[int, int]]:\n@@ -168,6 +187,7 @@ class GotOcr2ImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = GotOcr2ImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "210a18a406beb1d762b9d7f655c81aee315d26c4",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 23,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -22,7 +22,6 @@\n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -32,25 +31,7 @@\n     TensorType,\n     auto_docstring,\n )\n-from .image_processing_got_ocr2 import get_optimal_tiled_canvas\n-\n-\n-class GotOcr2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    crop_to_patches (`bool`, *optional*, defaults to `False`):\n-        Whether to crop the image to patches. Can be overridden by the `crop_to_patches` parameter in the\n-        `preprocess` method.\n-    min_patches (`int`, *optional*, defaults to 1):\n-        The minimum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n-        set to `True`. Can be overridden by the `min_patches` parameter in the `preprocess` method.\n-    max_patches (`int`, *optional*, defaults to 12):\n-        The maximum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n-        set to `True`. Can be overridden by the `max_patches` parameter in the `preprocess` method.\n-    \"\"\"\n-\n-    crop_to_patches: Optional[bool]\n-    min_patches: Optional[int]\n-    max_patches: Optional[int]\n+from .image_processing_got_ocr2 import GotOcr2ImageProcessorKwargs, get_optimal_tiled_canvas\n \n \n @auto_docstring\n@@ -66,13 +47,13 @@ class GotOcr2ImageProcessorFast(BaseImageProcessorFast):\n     crop_to_patches = False\n     min_patches = 1\n     max_patches = 12\n-    valid_kwargs = GotOcr2FastImageProcessorKwargs\n+    valid_kwargs = GotOcr2ImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[GotOcr2FastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[GotOcr2ImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[GotOcr2FastImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[GotOcr2ImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     def crop_image_to_patches("
        },
        {
            "sha": "447122e18c22eb41676f8b1bbf1f52d757b62a64",
            "filename": "src/transformers/models/got_ocr2/processing_got_ocr2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -18,11 +18,10 @@\n \n import numpy as np\n \n-from transformers.processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n-from transformers.tokenization_utils_base import PreTokenizedInput, TextInput\n-\n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput\n+from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import is_vision_available, logging\n \n \n@@ -37,13 +36,13 @@ class GotOcr2TextKwargs(TextKwargs, total=False):\n \n \n class GotOcr2ImagesKwargs(ImagesKwargs, total=False):\n+    crop_to_patches: Optional[bool]\n+    min_patches: Optional[int]\n+    max_patches: Optional[int]\n     box: Optional[Union[list, tuple[float, float], tuple[float, float, float, float]]]\n     color: Optional[str]\n     num_image_tokens: Optional[int]\n     multi_page: Optional[bool]\n-    crop_to_patches: Optional[bool]\n-    min_patches: Optional[int]\n-    max_patches: Optional[int]\n \n \n class GotOcr2ProcessorKwargs(ProcessingKwargs, total=False):\n@@ -136,8 +135,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[GotOcr2ProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "0b76ccfe75dbfc43047c6488ee03590ea3255ef7",
            "filename": "src/transformers/models/granite_speech/processing_granite_speech.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fprocessing_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fprocessing_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fprocessing_granite_speech.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -49,8 +49,6 @@ def __call__(\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]],\n         audio: Union[\"torch.Tensor\", list[\"torch.Tensor\"]] = None,\n         device: str = \"cpu\",\n-        images=None,\n-        videos=None,\n         **kwargs,\n     ) -> BatchFeature:\n         requires_backends(self, [\"torch\"])"
        },
        {
            "sha": "e4e17d4d8ddfe2ad57d28bda89347bb7cb5b254c",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino.py",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -51,6 +51,7 @@\n     validate_kwargs,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import (\n     ExplicitEnum,\n     TensorType,\n@@ -91,6 +92,29 @@ class AnnotationFormat(ExplicitEnum):\n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n \n \n+class GroundingDinoImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n+        Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n+    do_convert_annotations (`bool`, *optional*, defaults to `True`):\n+        Controls whether to convert the annotations to the format expected by the GROUNDING_DINO model. Converts the\n+        bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n+        Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n+    return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n+        Whether to return segmentation masks.\n+    annotations (`AnnotationType` or `list[AnnotationType]`, *optional*):\n+        Annotations to transform according to the padding that is applied to the images.\n+    masks_path (`str` or `pathlib.Path`, *optional*):\n+        Path to the directory containing the segmentation masks.\n+    \"\"\"\n+\n+    format: Optional[Union[str, AnnotationFormat]]\n+    do_convert_annotations: Optional[bool]\n+    return_segmentation_masks: Optional[bool]\n+    annotations: Optional[Union[AnnotationType, list[AnnotationType]]]\n+    masks_path: Optional[Union[str, pathlib.Path]]\n+\n+\n # Copied from transformers.models.detr.image_processing_detr.get_size_with_aspect_ratio\n def get_size_with_aspect_ratio(image_size, size, max_size=None) -> tuple[int, int]:\n     \"\"\"\n@@ -865,6 +889,7 @@ class GroundingDinoImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n+    valid_kwargs = GroundingDinoImageProcessorKwargs\n \n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.__init__\n     def __init__("
        },
        {
            "sha": "ee303ec47fc46f04d6719fa4e1976450b188a838",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino_fast.py",
            "status": "modified",
            "additions": 25,
            "deletions": 41,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -4,6 +4,26 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_grounding_dino.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Inc. team. All rights reserved.\n+#\n+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n+# and OPT implementations in this library. It has been modified from its\n+# original forms to accommodate minor architectural differences compared\n+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n import pathlib\n from typing import TYPE_CHECKING, Any, Optional, Union\n \n@@ -14,7 +34,6 @@\n from ...image_processing_utils import BatchFeature, get_size_dict\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     SizeDict,\n     get_image_size_for_max_height_width,\n     get_max_height_width,\n@@ -35,7 +54,7 @@\n from ...processing_utils import Unpack\n from ...utils import TensorType, auto_docstring, logging\n from ...utils.import_utils import requires\n-from .image_processing_grounding_dino import get_size_with_aspect_ratio\n+from .image_processing_grounding_dino import GroundingDinoImageProcessorKwargs, get_size_with_aspect_ratio\n \n \n if TYPE_CHECKING:\n@@ -44,24 +63,6 @@\n \n logger = logging.get_logger(__name__)\n \n-\n-class GroundingDinoFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    r\"\"\"\n-    format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n-        Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n-    do_convert_annotations (`bool`, *optional*, defaults to `True`):\n-        Controls whether to convert the annotations to the format expected by the GROUNDING_DINO model. Converts the\n-        bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n-        Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n-    return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n-        Whether to return segmentation masks.\n-    \"\"\"\n-\n-    format: Optional[Union[str, AnnotationFormat]]\n-    do_convert_annotations: Optional[bool]\n-    return_segmentation_masks: Optional[bool]\n-\n-\n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n \n \n@@ -304,9 +305,9 @@ class GroundingDinoImageProcessorFast(BaseImageProcessorFast):\n     size = {\"shortest_edge\": 800, \"longest_edge\": 1333}\n     default_to_square = False\n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n-    valid_kwargs = GroundingDinoFastImageProcessorKwargs\n+    valid_kwargs = GroundingDinoImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[GroundingDinoFastImageProcessorKwargs]) -> None:\n+    def __init__(self, **kwargs: Unpack[GroundingDinoImageProcessorKwargs]) -> None:\n         if \"pad_and_return_pixel_mask\" in kwargs:\n             kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n \n@@ -568,25 +569,8 @@ def pad(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        annotations: Optional[Union[AnnotationType, list[AnnotationType]]] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        **kwargs: Unpack[GroundingDinoFastImageProcessorKwargs],\n+        **kwargs: Unpack[GroundingDinoImageProcessorKwargs],\n     ) -> BatchFeature:\n-        r\"\"\"\n-        annotations (`AnnotationType` or `list[AnnotationType]`, *optional*):\n-            List of annotations associated with the image or batch of images. If annotation is for object\n-            detection, the annotations should be a dictionary with the following keys:\n-            - \"image_id\" (`int`): The image id.\n-            - \"annotations\" (`list[Dict]`): List of annotations for an image. Each annotation should be a\n-                dictionary. An image can have no annotations, in which case the list should be empty.\n-            If annotation is for segmentation, the annotations should be a dictionary with the following keys:\n-            - \"image_id\" (`int`): The image id.\n-            - \"segments_info\" (`list[Dict]`): List of segments for an image. Each segment should be a dictionary.\n-                An image can have no segments, in which case the list should be empty.\n-            - \"file_name\" (`str`): The file name of the image.\n-        masks_path (`str` or `pathlib.Path`, *optional*):\n-            Path to the directory containing the segmentation masks.\n-        \"\"\"\n         if \"pad_and_return_pixel_mask\" in kwargs:\n             kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n             logger.warning_once(\n@@ -601,7 +585,7 @@ def preprocess(\n             )\n             kwargs[\"size\"] = kwargs.pop(\"max_size\")\n \n-        return super().preprocess(images, annotations, masks_path, **kwargs)\n+        return super().preprocess(images, **kwargs)\n \n     def _preprocess(\n         self,"
        },
        {
            "sha": "ded6435508a5f0943ee5ddeb1fa07f6280799995",
            "filename": "src/transformers/models/grounding_dino/modular_grounding_dino.py",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodular_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodular_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodular_grounding_dino.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -1,3 +1,23 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Inc. team. All rights reserved.\n+#\n+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n+# and OPT implementations in this library. It has been modified from its\n+# original forms to accommodate minor architectural differences compared\n+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n from typing import TYPE_CHECKING, Optional, Union\n \n import torch"
        },
        {
            "sha": "5f2f900451b20e75b892229e6aabdc5d5935f7e0",
            "filename": "src/transformers/models/grounding_dino/processing_grounding_dino.py",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -16,13 +16,12 @@\n Processor class for Grounding DINO.\n \"\"\"\n \n-import pathlib\n import warnings\n from typing import TYPE_CHECKING, Optional, Union\n \n from ...image_transforms import center_to_corners_format\n-from ...image_utils import AnnotationFormat, ImageInput\n-from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...image_utils import ImageInput\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n from ...utils import TensorType, is_torch_available\n \n@@ -99,16 +98,7 @@ def get(self, key, *args, **kwargs):\n         return super().get(key, *args, **kwargs)\n \n \n-class GroundingDinoImagesKwargs(ImagesKwargs, total=False):\n-    annotations: Optional[Union[AnnotationType, list[AnnotationType]]]\n-    return_segmentation_masks: Optional[bool]\n-    masks_path: Optional[Union[str, pathlib.Path]]\n-    do_convert_annotations: Optional[bool]\n-    format: Optional[Union[str, AnnotationFormat]]\n-\n-\n class GroundingDinoProcessorKwargs(ProcessingKwargs, total=False):\n-    images_kwargs: GroundingDinoImagesKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"add_special_tokens\": True,"
        },
        {
            "sha": "7fda46e3a990e3f7fae26e62ce7fb03ffd2056c2",
            "filename": "src/transformers/models/idefics/image_processing_idefics.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fidefics%2Fimage_processing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fidefics%2Fimage_processing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fimage_processing_idefics.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -28,13 +28,28 @@\n     to_numpy_array,\n     valid_images,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, is_torch_available\n \n \n IDEFICS_STANDARD_MEAN = [0.48145466, 0.4578275, 0.40821073]\n IDEFICS_STANDARD_STD = [0.26862954, 0.26130258, 0.27577711]\n \n \n+class IdeficsImageProcessorKwargs(ImagesKwargs):\n+    \"\"\"\n+    transform (`Callable`, *optional*):\n+        A custom transform function that accepts a single image can be passed for training. For example,\n+        `torchvision.Compose` can be used to compose multiple transforms. If `None` - an inference mode is\n+        assumed - and then a preset of inference-specific transforms will be applied to the images\n+    image_size (`dict[str, int]`, *optional*):\n+        Resize to image size\n+    \"\"\"\n+\n+    transform: Optional[Callable]\n+    image_size: Optional[dict[str, int]]\n+\n+\n def convert_to_rgb(image):\n     # `image.convert(\"RGB\")` would only work for .jpg images, as it creates a wrong background\n     # for transparent images. The call to `alpha_composite` handles this case\n@@ -74,6 +89,7 @@ class IdeficsImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = IdeficsImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "4b5ccaffe5c8e47991a2b77149ecaaa67161d283",
            "filename": "src/transformers/models/idefics/processing_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 13,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -16,13 +16,12 @@\n Processor class for IDEFICS.\n \"\"\"\n \n-from typing import Callable, Optional, Union\n+from typing import Optional, Union\n from urllib.parse import urlparse\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n from ...processing_utils import (\n-    ImagesKwargs,\n     ProcessingKwargs,\n     ProcessorMixin,\n     TextKwargs,\n@@ -40,28 +39,19 @@\n IMAGE_TOKEN = \"<image>\"\n \n \n-class IdeficsImagesKwargs(ImagesKwargs, total=False):\n-    transform: Optional[Callable]\n-    image_size: Optional[dict[str, int]]\n-    image_mean: Optional[Union[float, list[float]]]\n-    image_std: Optional[Union[float, list[float]]]\n-\n-\n class IdeficsTextKwargs(TextKwargs, total=False):\n     add_eos_token: Optional[bool]\n     add_end_of_utterance_token: Optional[bool]\n \n \n class IdeficsProcessorKwargs(ProcessingKwargs, total=False):\n     text_kwargs: IdeficsTextKwargs\n-    images_kwargs: IdeficsImagesKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"add_special_tokens\": False,\n             \"padding\": \"longest\",\n             \"add_eos_token\": False,\n         },\n-        \"images_kwargs\": {},\n         \"common_kwargs\": {\"return_tensors\": \"pt\"},\n     }\n \n@@ -198,8 +188,6 @@ def __call__(\n             list[list[TextInput]],\n             list[list[PreTokenizedInput]],\n         ] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[IdeficsProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"This method takes batched or non-batched prompts made of text and images and converts them into prompts that"
        },
        {
            "sha": "b9b741a9704b8b0ad8c5f2b4e282843d43b555e8",
            "filename": "src/transformers/models/idefics2/image_processing_idefics2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -35,6 +35,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, is_vision_available, logging\n \n \n@@ -46,6 +47,15 @@\n     from PIL import Image\n \n \n+class Idefics2ImageProcessorKwargs(ImagesKwargs):\n+    \"\"\"\n+    do_image_splitting (`bool`, *optional*, defaults to `False`):\n+        Whether to split the image into a sequence 4 equal sub-images concatenated with the original image.\n+    \"\"\"\n+\n+    do_image_splitting: Optional[bool]\n+\n+\n def get_resize_output_image_size(image, size, input_data_format) -> tuple[int, int]:\n     \"\"\"\n     Get the output size of the image after resizing given a dictionary specifying the max and min sizes.\n@@ -186,6 +196,7 @@ class Idefics2ImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"pixel_attention_mask\"]\n+    valid_kwargs = Idefics2ImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "36ae6ea5fbc7c16db356c941303a84d91a3c0627",
            "filename": "src/transformers/models/idefics2/image_processing_idefics2_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -21,7 +21,6 @@\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n     BatchFeature,\n-    DefaultFastImageProcessorKwargs,\n     SizeDict,\n     group_images_by_shape,\n     reorder_images,\n@@ -35,7 +34,7 @@\n )\n from ...processing_utils import Unpack\n from ...utils import TensorType, auto_docstring, is_torchvision_available, logging\n-from .image_processing_idefics2 import convert_to_rgb\n+from .image_processing_idefics2 import Idefics2ImageProcessorKwargs, convert_to_rgb\n \n \n if is_torchvision_available():\n@@ -105,15 +104,6 @@ def make_pixel_mask(image: \"torch.Tensor\", output_size: tuple[int, int]) -> \"tor\n     return mask\n \n \n-class Idefics2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    do_image_splitting (`bool`, *optional*, defaults to `False`):\n-        Whether to split the image into a sequence 4 equal sub-images concatenated with the original image.\n-    \"\"\"\n-\n-    do_image_splitting: Optional[bool]\n-\n-\n @auto_docstring\n class Idefics2ImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BILINEAR\n@@ -127,7 +117,7 @@ class Idefics2ImageProcessorFast(BaseImageProcessorFast):\n     do_image_splitting = False\n     size = {\"shortest_edge\": 378, \"longest_edge\": 980}\n     model_input_names = [\"pixel_values\", \"pixel_attention_mask\"]\n-    valid_kwargs = Idefics2FastImageProcessorKwargs\n+    valid_kwargs = Idefics2ImageProcessorKwargs\n \n     def convert_to_rgb(self, image: ImageInput) -> ImageInput:\n         \"\"\"\n@@ -214,7 +204,7 @@ def pad(\n         return image, pixel_mask\n \n     @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[Idefics2FastImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[Idefics2ImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     def _preprocess("
        },
        {
            "sha": "c419a364125417c00edaf9755c23e1e20efa53c8",
            "filename": "src/transformers/models/idefics2/processing_idefics2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 14,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -22,7 +22,6 @@\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, is_valid_image, load_image\n from ...processing_utils import (\n-    ImagesKwargs,\n     ProcessingKwargs,\n     ProcessorMixin,\n     Unpack,\n@@ -46,20 +45,13 @@ def is_image_or_image_url(elem):\n     return is_url(elem) or is_valid_image(elem)\n \n \n-class Idefics2ImagesKwargs(ImagesKwargs, total=False):\n-    image_seq_len: Optional[int]\n-\n-\n class Idefics2ProcessorKwargs(ProcessingKwargs, total=False):\n-    images_kwargs: Idefics2ImagesKwargs\n-\n     _defaults = {\n         \"text_kwargs\": {\n             \"add_special_tokens\": True,\n             \"padding\": False,\n             \"is_split_into_words\": False,\n         },\n-        \"images_kwargs\": {},\n     }\n \n \n@@ -123,8 +115,6 @@ def __call__(\n         self,\n         images: Union[ImageInput, list[ImageInput], list[list[ImageInput]]] = None,\n         text: Union[TextInput, \"PreTokenizedInput\", list[TextInput], list[\"PreTokenizedInput\"]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[Idefics2ProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -181,8 +171,6 @@ def __call__(\n             tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n             **kwargs,\n         )\n-        image_seq_len = output_kwargs[\"images_kwargs\"].pop(\"image_seq_len\", None)\n-        image_seq_len = image_seq_len if image_seq_len is not None else self.image_seq_len\n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n \n         n_images_in_text = []\n@@ -197,12 +185,11 @@ def __call__(\n             # Replace the image token with fake tokens around the expanded image token sequence of length `image_seq_len`\n             fake_image_token = self.fake_image_token\n             image_token = self.image_token\n-            image_str = f\"{fake_image_token}{image_token * image_seq_len}{fake_image_token}\"\n+            image_str = f\"{fake_image_token}{image_token * self.image_seq_len}{fake_image_token}\"\n \n             if self.image_processor.do_image_splitting:\n                 # A single image token is split into 4 patches + 1 original image\n                 image_str = image_str * 5\n-                image_seq_len *= 5\n \n             prompt_strings = []\n             for sample in text:"
        },
        {
            "sha": "f098a9f54dc13130bfbbd5629e9cc521706c7f3b",
            "filename": "src/transformers/models/idefics3/image_processing_idefics3.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -35,6 +35,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, is_vision_available, logging\n \n \n@@ -47,6 +48,22 @@\n     from PIL import Image\n \n \n+class Idefics3ImageProcessorKwargs(ImagesKwargs):\n+    \"\"\"\n+    do_image_splitting (`bool`, *optional*, defaults to `True`):\n+        Whether to split the image into sub-images concatenated with the original image. They are split into patches\n+        such that each patch has a size of `max_image_size[\"height\"]` x `max_image_size[\"width\"]`.\n+    max_image_size (`Dict`, *optional*, defaults to `{\"longest_edge\": 364}`):\n+        Maximum resolution of the patches of images accepted by the model. This is a dictionary containing the key \"longest_edge\".\n+    return_row_col_info (`bool`, *optional*, defaults to `False`):\n+        Whether to return the row and column information of the images.\n+    \"\"\"\n+\n+    do_image_splitting: Optional[bool]\n+    max_image_size: Optional[dict[str, int]]\n+    return_row_col_info: Optional[bool]\n+\n+\n def _resize_output_size_rescale_to_max_len(\n     height: int, width: int, min_len: Optional[int] = 1, max_len: Optional[int] = None\n ) -> tuple[int, int]:\n@@ -291,6 +308,7 @@ class Idefics3ImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"pixel_attention_mask\"]\n+    valid_kwargs = Idefics3ImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "2f325f77931aa8f03c12de93a4b03553b805fabc",
            "filename": "src/transformers/models/idefics3/image_processing_idefics3_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 19,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -22,7 +22,6 @@\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n     BatchFeature,\n-    DefaultFastImageProcessorKwargs,\n     SizeDict,\n     group_images_by_shape,\n     reorder_images,\n@@ -36,6 +35,7 @@\n )\n from ...processing_utils import Unpack\n from ...utils import TensorType, auto_docstring, is_torchvision_available, logging\n+from .image_processing_idefics3 import Idefics3ImageProcessorKwargs\n \n \n if is_torchvision_available():\n@@ -169,22 +169,6 @@ def make_pixel_mask(image: \"torch.Tensor\", output_size: tuple[int, int]) -> \"tor\n     return mask\n \n \n-class Idefics3FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    do_image_splitting (`bool`, *optional*, defaults to `True`):\n-        Whether to split the image into sub-images concatenated with the original image. They are split into patches\n-        such that each patch has a size of `max_image_size[\"height\"]` x `max_image_size[\"width\"]`.\n-    max_image_size (`Dict`, *optional*, defaults to `{\"longest_edge\": 364}`):\n-        Maximum resolution of the patches of images accepted by the model. This is a dictionary containing the key \"longest_edge\".\n-    return_row_col_info (`bool`, *optional*, defaults to `False`):\n-        Whether to return the row and column information of the images.\n-    \"\"\"\n-\n-    do_image_splitting: Optional[bool]\n-    max_image_size: Optional[dict[str, int]]\n-    return_row_col_info: Optional[bool]\n-\n-\n @auto_docstring\n class Idefics3ImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.LANCZOS\n@@ -199,7 +183,7 @@ class Idefics3ImageProcessorFast(BaseImageProcessorFast):\n     do_image_splitting = True\n     do_pad = True\n     return_row_col_info = False\n-    valid_kwargs = Idefics3FastImageProcessorKwargs\n+    valid_kwargs = Idefics3ImageProcessorKwargs\n \n     def _prepare_images_structure(self, images: ImageInput, expected_ndims: int = 3) -> ImageInput:\n         \"\"\"\n@@ -367,7 +351,7 @@ def pad(\n         return image, pixel_mask\n \n     @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[Idefics3FastImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[Idefics3ImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     def _preprocess("
        },
        {
            "sha": "451af1d8a38f3e3c85fec303cf2c92e8f4a8bab1",
            "filename": "src/transformers/models/idefics3/processing_idefics3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 10,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -24,7 +24,7 @@\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, is_valid_image, load_image\n-from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import AddedToken, BatchEncoding, TextInput\n from ...utils import logging\n \n@@ -87,14 +87,7 @@ def get_image_prompt_string(\n     )\n \n \n-class Idefics3ImagesKwargs(ImagesKwargs, total=False):\n-    return_row_col_info: Optional[bool]\n-    max_image_size: Optional[dict[str, int]]\n-\n-\n class Idefics3ProcessorKwargs(ProcessingKwargs, total=False):\n-    images_kwargs: Idefics3ImagesKwargs\n-\n     _defaults = {\n         \"text_kwargs\": {\n             \"add_special_tokens\": True,\n@@ -179,8 +172,6 @@ def __call__(\n         self,\n         images: Union[ImageInput, list[ImageInput], list[list[ImageInput]]] = None,\n         text: Union[TextInput, \"PreTokenizedInput\", list[TextInput], list[\"PreTokenizedInput\"]] = None,\n-        audio=None,\n-        videos=None,\n         image_seq_len: Optional[int] = None,\n         **kwargs: Unpack[Idefics3ProcessorKwargs],\n     ) -> BatchEncoding:"
        },
        {
            "sha": "8f79cd58ec5f3f23bd090a8cdcf90d651cd61392",
            "filename": "src/transformers/models/imagegpt/image_processing_imagegpt.py",
            "status": "modified",
            "additions": 19,
            "deletions": 1,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -31,17 +31,34 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n-from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n+from ...processing_utils import ImagesKwargs\n+from ...utils import TensorType, filter_out_non_signature_kwargs, is_torch_available, is_vision_available, logging\n from ...utils.import_utils import requires\n \n \n if is_vision_available():\n     import PIL\n \n+if is_torch_available():\n+    import torch\n \n logger = logging.get_logger(__name__)\n \n \n+class ImageGPTImageProcessorKwargs(ImagesKwargs):\n+    \"\"\"\n+    clusters (`np.ndarray` or `list[list[int]]` or `torch.Tensor`, *optional*):\n+        The color clusters to use, of shape `(n_clusters, 3)` when color quantizing. Can be overridden by `clusters`\n+        in `preprocess`.\n+    do_color_quantize (`bool`, *optional*, defaults to `True`):\n+        Controls whether to apply color quantization to convert continuous pixel values to discrete cluster indices.\n+        When True, each pixel is assigned to its nearest color cluster, enabling ImageGPT's discrete token modeling.\n+    \"\"\"\n+\n+    clusters: Optional[Union[np.ndarray, list[list[int]], \"torch.Tensor\"]]\n+    do_color_quantize: Optional[bool]\n+\n+\n def squared_euclidean_distance(a, b):\n     b = b.T\n     a2 = np.sum(np.square(a), axis=1)\n@@ -83,6 +100,7 @@ class ImageGPTImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = ImageGPTImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "1be050b5ecf9fab0873d5edb714adba3e38a0983",
            "filename": "src/transformers/models/imagegpt/image_processing_imagegpt_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 17,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -23,7 +23,6 @@\n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n )\n from ...image_transforms import group_images_by_shape, reorder_images\n from ...image_utils import PILImageResampling\n@@ -32,6 +31,7 @@\n     TensorType,\n     auto_docstring,\n )\n+from .image_processing_imagegpt import ImageGPTImageProcessorKwargs\n \n \n def squared_euclidean_distance_torch(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n@@ -68,20 +68,6 @@ def color_quantize_torch(x: torch.Tensor, clusters: torch.Tensor) -> torch.Tenso\n     return torch.argmin(d, dim=1)\n \n \n-class ImageGPTFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    clusters (`np.ndarray` or `list[list[int]]` or `torch.Tensor`, *optional*):\n-        The color clusters to use, of shape `(n_clusters, 3)` when color quantizing. Can be overridden by `clusters`\n-        in `preprocess`.\n-    do_color_quantize (`bool`, *optional*, defaults to `True`):\n-        Controls whether to apply color quantization to convert continuous pixel values to discrete cluster indices.\n-        When True, each pixel is assigned to its nearest color cluster, enabling ImageGPT's discrete token modeling.\n-    \"\"\"\n-\n-    clusters: Optional[Union[np.ndarray, list[list[int]], torch.Tensor]]\n-    do_color_quantize: Optional[bool]\n-\n-\n @auto_docstring\n class ImageGPTImageProcessorFast(BaseImageProcessorFast):\n     model_input_names = [\"input_ids\"]\n@@ -92,12 +78,12 @@ class ImageGPTImageProcessorFast(BaseImageProcessorFast):\n     image_std = [0.5, 0.5, 0.5]\n     do_rescale = True\n     do_normalize = True\n-    valid_kwargs = ImageGPTFastImageProcessorKwargs\n+    valid_kwargs = ImageGPTImageProcessorKwargs\n \n     def __init__(\n         self,\n         clusters: Optional[Union[list, np.ndarray, torch.Tensor]] = None,  # keep as arg for backwards compatibility\n-        **kwargs: Unpack[ImageGPTFastImageProcessorKwargs],\n+        **kwargs: Unpack[ImageGPTImageProcessorKwargs],\n     ):\n         r\"\"\"\n         clusters (`np.ndarray` or `list[list[int]]` or `torch.Tensor`, *optional*):"
        },
        {
            "sha": "afe43c1fc7a73f8975bfe44aad8ab831b78adcf8",
            "filename": "src/transformers/models/instructblip/processing_instructblip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -43,7 +43,6 @@ class InstructBlipProcessorKwargs(ProcessingKwargs, total=False):\n             \"return_length\": False,\n             \"verbose\": True,\n         },\n-        \"images_kwargs\": {},\n     }\n \n \n@@ -85,8 +84,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[InstructBlipProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "12e0d395b05cb7103ee70636c616f8db3a2d28f9",
            "filename": "src/transformers/models/internvl/processing_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -19,19 +19,12 @@\n \n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput, concatenate_list, make_flat_list_of_images\n-from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...video_utils import VideoInput\n \n \n-class InternVLImagesKwargs(ImagesKwargs, total=False):\n-    crop_to_patches: Optional[bool]\n-    min_patches: Optional[int]\n-    max_patches: Optional[int]\n-\n-\n class InternVLProcessorKwargs(ProcessingKwargs, total=False):\n-    images_kwargs: InternVLImagesKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding_side\": \"left\",\n@@ -159,7 +152,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n-        audio=None,\n         videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[InternVLProcessorKwargs],\n     ) -> BatchFeature:"
        },
        {
            "sha": "d16c57522d1c0a1add691844e8f2d91b9e4ecf37",
            "filename": "src/transformers/models/internvl/video_processing_internvl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -43,7 +43,6 @@ class InternVLVideoProcessor(BaseVideoProcessor):\n     initial_shift = True\n     do_sample_frames = False  # Set to False for BC, recommended to set `True` in new models\n     valid_kwargs = InternVLVideoProcessorInitKwargs\n-    model_input_names = [\"pixel_values_videos\"]\n \n     def __init__(self, **kwargs: Unpack[InternVLVideoProcessorInitKwargs]):\n         super().__init__(**kwargs)"
        },
        {
            "sha": "06ea0fe0e4d1c984a15f3b8812cf36ff6c5ab12b",
            "filename": "src/transformers/models/janus/image_processing_janus.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -40,6 +40,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n \n \n@@ -50,6 +51,16 @@\n logger = logging.get_logger(__name__)\n \n \n+class JanusImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    min_size (`int`, *optional*, defaults to 14):\n+        The minimum allowed size for the resized image. Ensures that neither the height nor width\n+        falls below this value after resizing.\n+    \"\"\"\n+\n+    min_size: int\n+\n+\n class JanusImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a JANUS image processor.\n@@ -92,6 +103,8 @@ class JanusImageProcessor(BaseImageProcessor):\n \n     model_input_names = [\"pixel_values\"]\n \n+    valid_kwargs = JanusImageProcessorKwargs\n+\n     def __init__(\n         self,\n         do_resize: bool = True,"
        },
        {
            "sha": "4de23e80e63ae922cf3c04990ce315f52720a585",
            "filename": "src/transformers/models/janus/image_processing_janus_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -22,7 +22,6 @@\n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -38,16 +37,7 @@\n     TensorType,\n     auto_docstring,\n )\n-\n-\n-class JanusFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    r\"\"\"\n-    min_size (`int`, *optional*, defaults to 14):\n-        The minimum allowed size for the resized image. Ensures that neither the height nor width\n-        falls below this value after resizing.\n-    \"\"\"\n-\n-    min_size: int\n+from .image_processing_janus import JanusImageProcessorKwargs\n \n \n @auto_docstring\n@@ -61,9 +51,9 @@ class JanusImageProcessorFast(BaseImageProcessorFast):\n     do_rescale = True\n     do_normalize = True\n     do_pad = True\n-    valid_kwargs = JanusFastImageProcessorKwargs\n+    valid_kwargs = JanusImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[JanusFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[JanusImageProcessorKwargs]):\n         if kwargs.get(\"image_mean\") is None:\n             background_color = (127, 127, 127)\n         else:"
        },
        {
            "sha": "332dc689dc625a8a88309c166b85c6ff8ff6db1a",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 13,
            "deletions": 1,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -47,7 +47,7 @@\n )\n from ...modeling_outputs import ModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...processing_utils import Unpack\n+from ...processing_utils import ImagesKwargs, Unpack\n from ...utils import (\n     TensorType,\n     TransformersKwargs,\n@@ -1289,6 +1289,16 @@ def generate(\n             return generated_tokens\n \n \n+class JanusImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    min_size (`int`, *optional*, defaults to 14):\n+        The minimum allowed size for the resized image. Ensures that neither the height nor width\n+        falls below this value after resizing.\n+    \"\"\"\n+\n+    min_size: int\n+\n+\n class JanusImageProcessor(BlipImageProcessor):\n     r\"\"\"\n     Constructs a JANUS image processor.\n@@ -1329,6 +1339,8 @@ class JanusImageProcessor(BlipImageProcessor):\n             Whether to pad the image to square or not.\n     \"\"\"\n \n+    valid_kwargs = JanusImageProcessorKwargs\n+\n     def __init__(\n         self,\n         do_resize: bool = True,"
        },
        {
            "sha": "15c237c4ced403f5ca730b84237e1c19f5edca3d",
            "filename": "src/transformers/models/janus/processing_janus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -81,8 +81,6 @@ def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         images: Optional[ImageInput] = None,\n-        videos=None,\n-        audio=None,\n         **kwargs: Unpack[JanusProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "98f8925e8a6984b88eb42c5b7ca01e26b8e3afce",
            "filename": "src/transformers/models/kosmos2/processing_kosmos2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -136,8 +136,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, list[TextInput]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[Kosmos2ProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "5f337e4b04c9f5109f554c8f6eab829db3d6b70f",
            "filename": "src/transformers/models/kosmos2_5/image_processing_kosmos2_5.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -34,6 +34,7 @@\n     to_numpy_array,\n     valid_images,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, is_torch_available, logging\n from ...utils.import_utils import requires_backends\n \n@@ -45,6 +46,19 @@\n DEFAULT_FONT_PATH = \"ybelkada/fonts\"\n \n \n+class Kosmos2_5ImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    patch_size (`Dict[str, int]`, *optional*, defaults to `{\"height\": 16, \"width\": 16}`):\n+        The patch size to use for the image. According to Kosmos2_5 paper and code, the patch size is 16x16.\n+    max_patches (`int`, *optional*, defaults to 4096):\n+        The maximum number of patches to extract from the image as per the\n+        [KOSMOS 2.5 paper](https://huggingface.co/papers/2309.11419).\n+    \"\"\"\n+\n+    patch_size: Optional[dict[str, int]]\n+    max_patches: Optional[int]\n+\n+\n # Copied from transformers.models.pix2struct.image_processing_pix2struct.torch_extract_patches\n def torch_extract_patches(image_tensor, patch_height, patch_width):\n     \"\"\"\n@@ -92,6 +106,7 @@ class Kosmos2_5ImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"flattened_patches\"]\n+    valid_kwargs = Kosmos2_5ImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "d892436ea6527fd0548fcd7a47b58e13a2a8ff7c",
            "filename": "src/transformers/models/kosmos2_5/image_processing_kosmos2_5_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 17,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -22,13 +22,13 @@\n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n from ...image_utils import ChannelDimension, ImageInput, get_image_size\n from ...processing_utils import Unpack\n from ...utils import TensorType, auto_docstring\n+from .image_processing_kosmos2_5 import Kosmos2_5ImageProcessorKwargs\n \n \n # Similar to transformers.models.pix2struct.image_processing_pix2struct.torch_extract_patches but dealing with a batch of images directly.\n@@ -56,19 +56,6 @@ def torch_extract_patches(image_tensor, patch_height, patch_width):\n     return patches\n \n \n-class Kosmos2_5FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    r\"\"\"\n-    patch_size (`Dict[str, int]`, *optional*, defaults to `{\"height\": 16, \"width\": 16}`):\n-        The patch size to use for the image. According to Kosmos2_5 paper and code, the patch size is 16x16.\n-    max_patches (`int`, *optional*, defaults to 4096):\n-        The maximum number of patches to extract from the image as per the\n-        [KOSMOS 2.5 paper](https://huggingface.co/papers/2309.11419).\n-    \"\"\"\n-\n-    patch_size: Optional[dict[str, int]]\n-    max_patches: Optional[int]\n-\n-\n @auto_docstring\n class Kosmos2_5ImageProcessorFast(BaseImageProcessorFast):\n     # To be checked against the slow image processor\n@@ -78,13 +65,13 @@ class Kosmos2_5ImageProcessorFast(BaseImageProcessorFast):\n     patch_size = {\"height\": 16, \"width\": 16}\n     max_patches = 4096\n     rescale_factor = None\n-    valid_kwargs = Kosmos2_5FastImageProcessorKwargs\n+    valid_kwargs = Kosmos2_5ImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[Kosmos2_5FastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[Kosmos2_5ImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[Kosmos2_5FastImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[Kosmos2_5ImageProcessorKwargs]) -> BatchFeature:\n         r\"\"\"\n         patch_size (`Dict[str, int]`, *optional*, defaults to `{\"height\": 16, \"width\": 16}`):\n             The patch size to use for the image. According to Kosmos2_5 paper and code, the patch size is 16x16."
        },
        {
            "sha": "cb6f27777a0f3647b01a995d9e1aa6cf33fe6d7c",
            "filename": "src/transformers/models/kosmos2_5/processing_kosmos2_5.py",
            "status": "modified",
            "additions": 7,
            "deletions": 16,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fprocessing_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fprocessing_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fprocessing_kosmos2_5.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -20,7 +20,7 @@\n \n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput\n-from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import TextInput\n from ...utils import is_torch_available\n \n@@ -29,14 +29,7 @@\n     import torch\n \n \n-class Kosmos2_5ImagesKwargs(ImagesKwargs, total=False):\n-    max_patches: Optional[int]\n-    num_image_tokens: Optional[int]\n-\n-\n class Kosmos2_5ProcessorKwargs(ProcessingKwargs, total=False):\n-    text_kwargs: TextKwargs\n-    images_kwargs: Kosmos2_5ImagesKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": True,\n@@ -46,7 +39,6 @@ class Kosmos2_5ProcessorKwargs(ProcessingKwargs, total=False):\n         },\n         \"images_kwargs\": {\n             \"max_patches\": 4096,\n-            \"num_image_tokens\": 2048,\n         },\n         \"common_kwargs\": {\"return_tensors\": \"pt\"},\n     }\n@@ -65,24 +57,25 @@ class Kosmos2_5Processor(ProcessorMixin):\n             An instance of [`Kosmos2_5ImageProcessor`]. The image processor is a required input.\n         tokenizer (Union[`T5TokenizerFast`, `T5Tokenizer`]):\n             An instance of ['T5TokenizerFast`] or ['T5Tokenizer`]. The tokenizer is a required input.\n+        num_image_tokens (`int`, *optional*, defaults to 2048):\n+            Number of image tokens used as a placeholder.\n     \"\"\"\n \n     attributes = [\"image_processor\", \"tokenizer\"]\n     image_processor_class = \"AutoImageProcessor\"\n     tokenizer_class = \"PreTrainedTokenizerFast\"\n \n-    def __init__(self, image_processor, tokenizer):\n+    def __init__(self, image_processor, tokenizer, num_image_tokens: int = 2048):\n         self.image_start_token = tokenizer.boi_token  # \"<image>\" : fixed token for the start of image\n         self.image_end_token = tokenizer.eoi_token  # \"</image>\" : fixed token for the end of image\n         self.image_token = tokenizer.image_token  # \"<s>\" : within a <image> ... </image> pair, these <s> tokens indicate they are positions reserved for an image\n+        self.num_image_tokens = num_image_tokens\n         super().__init__(image_processor, tokenizer)\n \n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, list[TextInput]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[Kosmos2_5ProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -104,8 +97,6 @@ def __call__(\n             tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n             **kwargs,\n         )\n-        num_image_tokens = output_kwargs[\"images_kwargs\"].setdefault(\"num_image_tokens\", None)\n-\n         encoding = BatchFeature()\n \n         if images is not None:\n@@ -114,7 +105,7 @@ def __call__(\n             image_encoding.pop(\"cols\")\n             encoding.update(image_encoding)\n \n-        prompt = f\"{self.tokenizer.bos_token}{self.image_start_token}{self.image_token * num_image_tokens}{self.image_end_token}\"\n+        prompt = f\"{self.tokenizer.bos_token}{self.image_start_token}{self.image_token * self.num_image_tokens}{self.image_end_token}\"\n \n         if text is not None:\n             if isinstance(text, str):\n@@ -124,7 +115,7 @@ def __call__(\n             input = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n \n             batch_size, seq_len = input.input_ids.shape\n-            image_embeds_position_mask = [0, -1] + [1] * num_image_tokens + [-1]\n+            image_embeds_position_mask = [0, -1] + [1] * self.num_image_tokens + [-1]\n             image_embeds_position_mask += [0] * (seq_len - len(image_embeds_position_mask))\n             image_embeds_position_mask = (\n                 torch.LongTensor(image_embeds_position_mask).unsqueeze(0).repeat(batch_size, 1)"
        },
        {
            "sha": "d5a7e95537c530b787f62eaf93149bc03e57caaf",
            "filename": "src/transformers/models/layoutlmv2/image_processing_layoutlmv2.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -30,6 +30,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import (\n     TensorType,\n     filter_out_non_signature_kwargs,\n@@ -51,6 +52,25 @@\n logger = logging.get_logger(__name__)\n \n \n+class LayoutLMv2ImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    apply_ocr (`bool`, *optional*, defaults to `True`):\n+        Whether to apply the Tesseract OCR engine to get words + normalized bounding boxes. Can be overridden by\n+        the `apply_ocr` parameter in the `preprocess` method.\n+    ocr_lang (`str`, *optional*):\n+        The language, specified by its ISO code, to be used by the Tesseract OCR engine. By default, English is\n+        used. Can be overridden by the `ocr_lang` parameter in the `preprocess` method.\n+    tesseract_config (`str`, *optional*):\n+        Any additional custom configuration flags that are forwarded to the `config` parameter when calling\n+        Tesseract. For example: '--psm 6'. Can be overridden by the `tesseract_config` parameter in the\n+        `preprocess` method.\n+    \"\"\"\n+\n+    apply_ocr: Optional[bool]\n+    ocr_lang: Optional[str]\n+    tesseract_config: Optional[str]\n+\n+\n def normalize_box(box, width, height):\n     return [\n         int(1000 * (box[0] / width)),\n@@ -125,6 +145,7 @@ class LayoutLMv2ImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = LayoutLMv2ImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "2d6e6bc21cb3c60f06805bb8916f643999ed1dd1",
            "filename": "src/transformers/models/layoutlmv2/image_processing_layoutlmv2_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 25,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -19,7 +19,7 @@\n import torch\n from torchvision.transforms.v2 import functional as F\n \n-from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature, DefaultFastImageProcessorKwargs\n+from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature\n from ...image_transforms import ChannelDimension, group_images_by_shape, reorder_images\n from ...image_utils import ImageInput, PILImageResampling, SizeDict\n from ...processing_utils import Unpack\n@@ -29,32 +29,12 @@\n     logging,\n     requires_backends,\n )\n-from .image_processing_layoutlmv2 import apply_tesseract\n+from .image_processing_layoutlmv2 import LayoutLMv2ImageProcessorKwargs, apply_tesseract\n \n \n logger = logging.get_logger(__name__)\n \n \n-class LayoutLMv2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    Args:\n-        apply_ocr (`bool`, *optional*, defaults to `True`):\n-            Whether to apply the Tesseract OCR engine to get words + normalized bounding boxes. Can be overridden by\n-            the `apply_ocr` parameter in the `preprocess` method.\n-        ocr_lang (`str`, *optional*):\n-            The language, specified by its ISO code, to be used by the Tesseract OCR engine. By default, English is\n-            used. Can be overridden by the `ocr_lang` parameter in the `preprocess` method.\n-        tesseract_config (`str`, *optional*):\n-            Any additional custom configuration flags that are forwarded to the `config` parameter when calling\n-            Tesseract. For example: '--psm 6'. Can be overridden by the `tesseract_config` parameter in the\n-            `preprocess` method.\n-    \"\"\"\n-\n-    apply_ocr: Optional[bool]\n-    ocr_lang: Optional[str]\n-    tesseract_config: Optional[str]\n-\n-\n @auto_docstring\n class LayoutLMv2ImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BILINEAR\n@@ -64,13 +44,13 @@ class LayoutLMv2ImageProcessorFast(BaseImageProcessorFast):\n     apply_ocr = True\n     ocr_lang = None\n     tesseract_config = \"\"\n-    valid_kwargs = LayoutLMv2FastImageProcessorKwargs\n+    valid_kwargs = LayoutLMv2ImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[LayoutLMv2FastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[LayoutLMv2ImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[LayoutLMv2FastImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[LayoutLMv2ImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     def _preprocess("
        },
        {
            "sha": "b9273dc75caddb2b821afbd63a8fae3cbf0b1075",
            "filename": "src/transformers/models/layoutlmv3/image_processing_layoutlmv3.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -34,6 +34,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import (\n     TensorType,\n     filter_out_non_signature_kwargs,\n@@ -55,6 +56,25 @@\n logger = logging.get_logger(__name__)\n \n \n+class LayoutLMv3ImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    apply_ocr (`bool`, *optional*, defaults to `True`):\n+        Whether to apply the Tesseract OCR engine to get words + normalized bounding boxes. Can be overridden by\n+        the `apply_ocr` parameter in the `preprocess` method.\n+    ocr_lang (`str`, *optional*):\n+        The language, specified by its ISO code, to be used by the Tesseract OCR engine. By default, English is\n+        used. Can be overridden by the `ocr_lang` parameter in the `preprocess` method.\n+    tesseract_config (`str`, *optional*):\n+        Any additional custom configuration flags that are forwarded to the `config` parameter when calling\n+        Tesseract. For example: '--psm 6'. Can be overridden by the `tesseract_config` parameter in the\n+        `preprocess` method.\n+    \"\"\"\n+\n+    apply_ocr: Optional[bool]\n+    ocr_lang: Optional[str]\n+    tesseract_config: Optional[str]\n+\n+\n def normalize_box(box, width, height):\n     return [\n         int(1000 * (box[0] / width)),\n@@ -143,6 +163,7 @@ class LayoutLMv3ImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = LayoutLMv3ImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "b614c5ec9449c2db661c2429ab798516c92f44dc",
            "filename": "src/transformers/models/layoutlmv3/image_processing_layoutlmv3_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 25,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -19,7 +19,7 @@\n import torch\n from torchvision.transforms.v2 import functional as F\n \n-from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature, DefaultFastImageProcessorKwargs\n+from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature\n from ...image_transforms import ChannelDimension, group_images_by_shape, reorder_images\n from ...image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD, ImageInput, PILImageResampling, SizeDict\n from ...processing_utils import Unpack\n@@ -29,32 +29,12 @@\n     logging,\n     requires_backends,\n )\n-from .image_processing_layoutlmv3 import apply_tesseract\n+from .image_processing_layoutlmv3 import LayoutLMv3ImageProcessorKwargs, apply_tesseract\n \n \n logger = logging.get_logger(__name__)\n \n \n-class LayoutLMv3FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    Args:\n-        apply_ocr (`bool`, *optional*, defaults to `True`):\n-            Whether to apply the Tesseract OCR engine to get words + normalized bounding boxes. Can be overridden by\n-            the `apply_ocr` parameter in the `preprocess` method.\n-        ocr_lang (`str`, *optional*):\n-            The language, specified by its ISO code, to be used by the Tesseract OCR engine. By default, English is\n-            used. Can be overridden by the `ocr_lang` parameter in the `preprocess` method.\n-        tesseract_config (`str`, *optional*):\n-            Any additional custom configuration flags that are forwarded to the `config` parameter when calling\n-            Tesseract. For example: '--psm 6'. Can be overridden by the `tesseract_config` parameter in the\n-            `preprocess` method.\n-    \"\"\"\n-\n-    apply_ocr: Optional[bool]\n-    ocr_lang: Optional[str]\n-    tesseract_config: Optional[str]\n-\n-\n @auto_docstring\n class LayoutLMv3ImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BILINEAR\n@@ -67,13 +47,13 @@ class LayoutLMv3ImageProcessorFast(BaseImageProcessorFast):\n     apply_ocr = True\n     ocr_lang = None\n     tesseract_config = \"\"\n-    valid_kwargs = LayoutLMv3FastImageProcessorKwargs\n+    valid_kwargs = LayoutLMv3ImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[LayoutLMv3FastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[LayoutLMv3ImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[LayoutLMv3FastImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[LayoutLMv3ImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     def _preprocess("
        },
        {
            "sha": "ad99504fcad6a4bfde7c16634caf650624777dca",
            "filename": "src/transformers/models/lfm2_vl/image_processing_lfm2_vl_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fimage_processing_lfm2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fimage_processing_lfm2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fimage_processing_lfm2_vl_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -22,7 +22,6 @@\n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -33,9 +32,7 @@\n     PILImageResampling,\n     SizeDict,\n )\n-from ...processing_utils import (\n-    Unpack,\n-)\n+from ...processing_utils import ImagesKwargs, Unpack\n from ...utils import (\n     TensorType,\n     auto_docstring,\n@@ -172,7 +169,7 @@ def pad_along_first_dim(\n     return images, pixel_mask\n \n \n-class Lfm2VlFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+class Lfm2VlImageProcessorKwargs(ImagesKwargs):\n     \"\"\"\n     downsample_factor (`int`, *optional*, defaults to `2`):\n         The downsampling factor for images used when resizing the image.\n@@ -214,10 +211,10 @@ class Lfm2VlImageProcessorFast(BaseImageProcessorFast):\n     return_row_col_info = False\n     image_mean = IMAGENET_STANDARD_STD\n     image_std = IMAGENET_STANDARD_MEAN\n-    valid_kwargs = Lfm2VlFastImageProcessorKwargs\n+    valid_kwargs = Lfm2VlImageProcessorKwargs\n     model_input_names = [\"pixel_values\", \"pixel_attention_mask\", \"spatial_shapes\"]\n \n-    def __init__(self, **kwargs: Unpack[Lfm2VlFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[Lfm2VlImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n         max_thumbnail_image_patches = self.max_image_tokens * self.downsample_factor**2"
        },
        {
            "sha": "e2678f556d02e45ba2f16b1e4bf64a6628489691",
            "filename": "src/transformers/models/llama4/image_processing_llama4_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -25,12 +25,11 @@\n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n from ...image_utils import ImageInput, PILImageResampling, SizeDict\n-from ...processing_utils import Unpack\n+from ...processing_utils import ImagesKwargs, Unpack\n from ...utils import (\n     TensorType,\n     auto_docstring,\n@@ -309,8 +308,8 @@ def get_best_fit(\n     return optimal_canvas\n \n \n-class Llama4ImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n+class Llama4ImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n     max_patches (`int`, *optional*, defaults to 16):\n         The maximum number of patches to be extracted from the image.\n         Can be overridden by the `max_patches` parameter in the `preprocess` method."
        },
        {
            "sha": "df371bdfd71071278ec03a11d829f55de06e941a",
            "filename": "src/transformers/models/llama4/processing_llama4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllama4%2Fprocessing_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllama4%2Fprocessing_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fprocessing_llama4.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -16,20 +16,14 @@\n \n from typing import Optional, Union\n \n-from transformers.processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n+from transformers.processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from transformers.tokenization_utils_base import PreTokenizedInput, TextInput\n \n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput, make_flat_list_of_images\n \n \n-class Llama4ImagesKwargs(ImagesKwargs, total=False):\n-    max_patches: Optional[int]\n-    resize_to_max_canvas: Optional[bool]\n-\n-\n class Llama4ProcessorKwargs(ProcessingKwargs, total=False):\n-    images_kwargs: Llama4ImagesKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding_side\": \"left\",\n@@ -139,8 +133,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[Llama4ProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "66ccb49c367122ce8c88cd15a5d61cd7d0a9105c",
            "filename": "src/transformers/models/llava/image_processing_llava_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -22,29 +22,23 @@\n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n from ...image_utils import (\n     OPENAI_CLIP_MEAN,\n     OPENAI_CLIP_STD,\n     ChannelDimension,\n-    ImageInput,\n     PILImageResampling,\n     SizeDict,\n     get_image_size,\n )\n-from ...processing_utils import Unpack\n from ...utils import (\n     TensorType,\n     auto_docstring,\n )\n \n \n-class LlavaFastImageProcessorKwargs(DefaultFastImageProcessorKwargs): ...\n-\n-\n @auto_docstring\n class LlavaImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BICUBIC\n@@ -59,14 +53,6 @@ class LlavaImageProcessorFast(BaseImageProcessorFast):\n     do_rescale = True\n     do_normalize = True\n     do_convert_rgb = True\n-    valid_kwargs = LlavaFastImageProcessorKwargs\n-\n-    def __init__(self, **kwargs: Unpack[LlavaFastImageProcessorKwargs]) -> None:\n-        super().__init__(**kwargs)\n-\n-    @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[LlavaFastImageProcessorKwargs]) -> BatchFeature:\n-        return super().preprocess(images, **kwargs)\n \n     def pad_to_square(\n         self,"
        },
        {
            "sha": "6f8d9e3a14ccf77fd1625d2c3fd121e76dfed648",
            "filename": "src/transformers/models/llava/processing_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -38,7 +38,6 @@\n class LlavaProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {\n         \"text_kwargs\": {\"padding\": False, \"return_mm_token_type_ids\": False},\n-        \"images_kwargs\": {},\n     }\n \n \n@@ -94,8 +93,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[LlavaProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "56ebc10f391da12e712ae96b9eb0b0a7abfc6ec3",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -48,6 +48,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, is_vision_available, logging\n \n \n@@ -58,6 +59,17 @@\n     from PIL import Image\n \n \n+class LlavaNextImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    image_grid_pinpoints (`list[list[int]]`, *optional*):\n+        A list of possible resolutions to use for processing high resolution images. The best resolution is selected\n+        based on the original size of the image. Can be overridden by `image_grid_pinpoints` in the `preprocess`\n+        method.\n+    \"\"\"\n+\n+    image_grid_pinpoints: Optional[list[list[int]]]\n+\n+\n def divide_to_patches(image: np.ndarray, patch_size: int, input_data_format) -> list[np.ndarray]:\n     \"\"\"\n     Divides an image into patches of a specified size.\n@@ -152,6 +164,7 @@ class LlavaNextImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"image_sizes\"]\n+    valid_kwargs = LlavaNextImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "19d6fb941e7bfdbcb5fb1c8180589dfa47bfd7bb",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 15,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -22,7 +22,6 @@\n from ...image_processing_utils import BatchFeature, get_patch_output_size, select_best_resolution\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     divide_to_patches,\n     group_images_by_shape,\n     reorder_images,\n@@ -41,17 +40,7 @@\n     TensorType,\n     auto_docstring,\n )\n-\n-\n-class LlavaNextFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    image_grid_pinpoints (`list[list[int]]`, *optional*):\n-        A list of possible resolutions to use for processing high resolution images. The best resolution is selected\n-        based on the original size of the image. Can be overridden by `image_grid_pinpoints` in the `preprocess`\n-        method.\n-    \"\"\"\n-\n-    image_grid_pinpoints: Optional[list[list[int]]]\n+from .image_processing_llava_next import LlavaNextImageProcessorKwargs\n \n \n @auto_docstring\n@@ -71,13 +60,13 @@ class LlavaNextImageProcessorFast(BaseImageProcessorFast):\n     do_convert_rgb = True\n     do_pad = True\n     image_grid_pinpoints = [[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008]]\n-    valid_kwargs = LlavaNextFastImageProcessorKwargs\n+    valid_kwargs = LlavaNextImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[LlavaNextFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[LlavaNextImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[LlavaNextFastImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[LlavaNextImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     def _resize_for_patching("
        },
        {
            "sha": "04493518a0206ba96284f9d7e678eda3e392831b",
            "filename": "src/transformers/models/llava_next/processing_llava_next.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -104,8 +104,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[LlavaNextProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "015e4cdea6df3f001a824c0d3f52c173e8ab9ea7",
            "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -116,7 +116,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        audio=None,\n         videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[LlavaNextVideoProcessorKwargs],\n     ) -> BatchFeature:"
        },
        {
            "sha": "c353100cee79cbeca3c6a791fb62d44a96d51d90",
            "filename": "src/transformers/models/llava_next_video/video_processing_llava_next_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fvideo_processing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fvideo_processing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fvideo_processing_llava_next_video.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -15,13 +15,9 @@\n \"\"\"Video processor class for LLaVa-NeXT-Video.\"\"\"\n \n from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling\n-from ...processing_utils import Unpack, VideosKwargs\n from ...video_processing_utils import BaseVideoProcessor\n \n \n-class LlavaNextVideoFastVideoProcessorInitKwargs(VideosKwargs): ...\n-\n-\n class LlavaNextVideoVideoProcessor(BaseVideoProcessor):\n     resample = PILImageResampling.BICUBIC\n     image_mean = OPENAI_CLIP_MEAN\n@@ -35,11 +31,6 @@ class LlavaNextVideoVideoProcessor(BaseVideoProcessor):\n     do_normalize = True\n     do_convert_rgb = True\n     do_sample_frames = False  # Set to False for BC, recommended to set `True` in new models\n-    valid_kwargs = LlavaNextVideoFastVideoProcessorInitKwargs\n-    model_input_names = [\"pixel_values_videos\"]\n-\n-    def __init__(self, **kwargs: Unpack[LlavaNextVideoFastVideoProcessorInitKwargs]):\n-        super().__init__(**kwargs)\n \n \n __all__ = [\"LlavaNextVideoVideoProcessor\"]"
        },
        {
            "sha": "119df9550a2ad2b3f2bbb39462e1e075d8b66ad4",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -47,6 +47,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, is_vision_available, logging\n \n \n@@ -57,6 +58,17 @@\n     from PIL import Image\n \n \n+class LlavaOnevisionImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    image_grid_pinpoints (`list[list[int]]`, *optional*):\n+        A list of possible resolutions to use for processing high resolution images. The best resolution is selected\n+        based on the original size of the image. Can be overridden by `image_grid_pinpoints` in the `preprocess`\n+        method.\n+    \"\"\"\n+\n+    image_grid_pinpoints: Optional[list[list[int]]]\n+\n+\n # Copied from transformers.models.llava_next.image_processing_llava_next.divide_to_patches\n def divide_to_patches(image: np.ndarray, patch_size: int, input_data_format) -> list[np.ndarray]:\n     \"\"\"\n@@ -146,6 +158,7 @@ class LlavaOnevisionImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"image_sizes\", \"batch_num_images\"]\n+    valid_kwargs = LlavaOnevisionImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "b309583461ce4f0169fd2db7a0e24f6d9ab377a6",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 15,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -27,7 +27,6 @@\n from ...image_processing_utils import BatchFeature, get_patch_output_size, select_best_resolution\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     divide_to_patches,\n     group_images_by_shape,\n     reorder_images,\n@@ -43,17 +42,7 @@\n )\n from ...processing_utils import Unpack\n from ...utils import TensorType, auto_docstring\n-\n-\n-class LlavaOnevisionFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    image_grid_pinpoints (`list[list[int]]`, *optional*):\n-        A list of possible resolutions to use for processing high resolution images. The best resolution is selected\n-        based on the original size of the image. Can be overridden by `image_grid_pinpoints` in the `preprocess`\n-        method.\n-    \"\"\"\n-\n-    image_grid_pinpoints: Optional[list[list[int]]]\n+from .image_processing_llava_onevision import LlavaOnevisionImageProcessorKwargs\n \n \n @auto_docstring\n@@ -71,14 +60,14 @@ class LlavaOnevisionImageProcessorFast(BaseImageProcessorFast):\n     do_convert_rgb = True\n     do_pad = True\n     image_grid_pinpoints = [[384, 384], [384, 768], [384, 1152], [384, 1536], [384, 1920], [384, 2304], [768, 384], [768, 768], [768, 1152], [768, 1536], [768, 1920], [768, 2304], [1152, 384], [1152, 768], [1152, 1152], [1152, 1536], [1152, 1920], [1152, 2304], [1536, 384], [1536, 768], [1536, 1152], [1536, 1536], [1536, 1920], [1536, 2304], [1920, 384], [1920, 768], [1920, 1152], [1920, 1536], [1920, 1920], [1920, 2304], [2304, 384], [2304, 768], [2304, 1152], [2304, 1536], [2304, 1920], [2304, 2304]]  # fmt: skip\n-    valid_kwargs = LlavaOnevisionFastImageProcessorKwargs\n+    valid_kwargs = LlavaOnevisionImageProcessorKwargs\n     model_input_names = [\"pixel_values\", \"image_sizes\", \"batch_num_images\"]\n \n-    def __init__(self, **kwargs: Unpack[LlavaOnevisionFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[LlavaOnevisionImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[LlavaOnevisionFastImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[LlavaOnevisionImageProcessorKwargs]) -> BatchFeature:\n         if isinstance(images, (tuple, list)) and isinstance(images[0], (tuple, list)):\n             # if the first element is a list, we assume that all elements are lists\n             batch_num_images = [len(x) for x in images]"
        },
        {
            "sha": "890fcdd7ecaa42b958f7e629185a5f8d954b87b9",
            "filename": "src/transformers/models/llava_onevision/modular_llava_onevision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -35,7 +35,7 @@\n \n from ...cache_utils import Cache\n from ...image_processing_utils import BatchFeature\n-from ...image_processing_utils_fast import DefaultFastImageProcessorKwargs, group_images_by_shape, reorder_images\n+from ...image_processing_utils_fast import group_images_by_shape, reorder_images\n from ...image_utils import (\n     OPENAI_CLIP_MEAN,\n     OPENAI_CLIP_STD,\n@@ -53,22 +53,12 @@\n     can_return_tuple,\n     logging,\n )\n+from .image_processing_llava_onevision import LlavaOnevisionImageProcessorKwargs\n \n \n logger = logging.get_logger(__name__)\n \n \n-class LlavaOnevisionFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    image_grid_pinpoints (`list[list[int]]`, *optional*):\n-        A list of possible resolutions to use for processing high resolution images. The best resolution is selected\n-        based on the original size of the image. Can be overridden by `image_grid_pinpoints` in the `preprocess`\n-        method.\n-    \"\"\"\n-\n-    image_grid_pinpoints: Optional[list[list[int]]]\n-\n-\n class LlavaOnevisionImageProcessorFast(LlavaNextImageProcessorFast):\n     resample = PILImageResampling.BICUBIC\n     image_mean = OPENAI_CLIP_MEAN\n@@ -129,7 +119,7 @@ def pad_to_square(\n         return padded_images\n \n     @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[LlavaOnevisionFastImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[LlavaOnevisionImageProcessorKwargs]) -> BatchFeature:\n         if isinstance(images, (tuple, list)) and isinstance(images[0], (tuple, list)):\n             # if the first element is a list, we assume that all elements are lists\n             batch_num_images = [len(x) for x in images]"
        },
        {
            "sha": "ff8eae5dd87a53afd27c9e6d42cfc56ebd306be7",
            "filename": "src/transformers/models/llava_onevision/processing_llava_onevision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -42,7 +42,6 @@ class LlavaOnevisionProcessorKwargs(ProcessingKwargs, total=False):\n             \"return_mm_token_type_ids\": False,\n         },\n         \"image_kwargs\": {},\n-        \"videos_kwargs\": {},\n     }\n \n \n@@ -114,7 +113,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        audio=None,\n         videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[LlavaOnevisionProcessorKwargs],\n     ) -> BatchFeature:"
        },
        {
            "sha": "b201085f5ee6df0f31dc345749a7dc4134ad8272",
            "filename": "src/transformers/models/llava_onevision/video_processing_llava_onevision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fvideo_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fvideo_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fvideo_processing_llava_onevision.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -15,13 +15,9 @@\n \"\"\"Video processor class for LLaVa-Onevision.\"\"\"\n \n from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling\n-from ...processing_utils import Unpack, VideosKwargs\n from ...video_processing_utils import BaseVideoProcessor\n \n \n-class LlavaOnevisionFastVideoProcessorInitKwargs(VideosKwargs): ...\n-\n-\n class LlavaOnevisionVideoProcessor(BaseVideoProcessor):\n     resample = PILImageResampling.BICUBIC\n     image_mean = OPENAI_CLIP_MEAN\n@@ -36,11 +32,6 @@ class LlavaOnevisionVideoProcessor(BaseVideoProcessor):\n     do_normalize = True\n     do_convert_rgb = True\n     do_sample_frames = False  # Set to False for BC, recommended to set `True` in new models\n-    valid_kwargs = LlavaOnevisionFastVideoProcessorInitKwargs\n-    model_input_names = [\"pixel_values_videos\"]\n-\n-    def __init__(self, **kwargs: Unpack[LlavaOnevisionFastVideoProcessorInitKwargs]):\n-        super().__init__(**kwargs)\n \n \n __all__ = [\"LlavaOnevisionVideoProcessor\"]"
        },
        {
            "sha": "d9d580955fbd46539a3a9322846e268b65aa7999",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -41,6 +41,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import (\n     IMAGENET_DEFAULT_MEAN,\n     IMAGENET_DEFAULT_STD,\n@@ -60,6 +61,25 @@\n     from torch import nn\n \n \n+class Mask2FormerImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    ignore_index (`int`, *optional*):\n+        Label to be assigned to background pixels in segmentation maps. If provided, segmentation map pixels\n+        denoted with 0 (background) will be replaced with `ignore_index`.\n+    do_reduce_labels (`bool`, *optional*, defaults to `False`):\n+        Whether or not to decrement all label values of segmentation maps by 1. Usually used for datasets where 0\n+        is used for background, and background itself is not included in all classes of a dataset (e.g. ADE20k).\n+        The background label will be replaced by `ignore_index`.\n+    num_labels (`int`, *optional*):\n+        The number of labels in the segmentation map.\n+    \"\"\"\n+\n+    size_divisor: Optional[int]\n+    ignore_index: Optional[int]\n+    do_reduce_labels: Optional[bool]\n+    num_labels: Optional[int]\n+\n+\n # Copied from transformers.models.detr.image_processing_detr.get_size_with_aspect_ratio\n def get_size_with_aspect_ratio(image_size, size, max_size=None) -> tuple[int, int]:\n     \"\"\"\n@@ -440,6 +460,7 @@ class Mask2FormerImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n+    valid_kwargs = Mask2FormerImageProcessorKwargs\n \n     @filter_out_non_signature_kwargs(extra=[\"max_size\", *INIT_SERVICE_KWARGS])\n     def __init__("
        },
        {
            "sha": "6b8cd184581b7814d735240f7649d6aa79f7bd4a",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 27,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -28,7 +28,6 @@\n from ...image_processing_utils import BatchFeature, get_size_dict\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     SizeDict,\n     get_image_size_for_max_height_width,\n     get_max_height_width,\n@@ -45,6 +44,7 @@\n from ...processing_utils import Unpack\n from ...utils import TensorType, auto_docstring, logging\n from .image_processing_mask2former import (\n+    Mask2FormerImageProcessorKwargs,\n     compute_segments,\n     convert_segmentation_to_rle,\n     get_size_with_aspect_ratio,\n@@ -55,28 +55,6 @@\n logger = logging.get_logger(__name__)\n \n \n-class Mask2FormerFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    r\"\"\"\n-    size_divisor (`int`, *optional*, defaults to 32):\n-        Some backbones need images divisible by a certain number. If not passed, it defaults to the value used in\n-        Swin Transformer.\n-    ignore_index (`int`, *optional*):\n-        Label to be assigned to background pixels in segmentation maps. If provided, segmentation map pixels\n-        denoted with 0 (background) will be replaced with `ignore_index`.\n-    do_reduce_labels (`bool`, *optional*, defaults to `False`):\n-        Whether or not to decrement all label values of segmentation maps by 1. Usually used for datasets where 0\n-        is used for background, and background itself is not included in all classes of a dataset (e.g. ADE20k).\n-        The background label will be replaced by `ignore_index`.\n-    num_labels (`int`, *optional*):\n-        The number of labels in the segmentation map.\n-    \"\"\"\n-\n-    size_divisor: Optional[int]\n-    ignore_index: Optional[int]\n-    do_reduce_labels: Optional[bool]\n-    num_labels: Optional[int]\n-\n-\n def convert_segmentation_map_to_binary_masks_fast(\n     segmentation_map: \"torch.Tensor\",\n     instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n@@ -127,9 +105,9 @@ class Mask2FormerImageProcessorFast(BaseImageProcessorFast):\n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n     size_divisor = 32\n     do_reduce_labels = False\n-    valid_kwargs = Mask2FormerFastImageProcessorKwargs\n+    valid_kwargs = Mask2FormerImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[Mask2FormerFastImageProcessorKwargs]) -> None:\n+    def __init__(self, **kwargs: Unpack[Mask2FormerImageProcessorKwargs]) -> None:\n         if \"pad_and_return_pixel_mask\" in kwargs:\n             kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n \n@@ -259,7 +237,7 @@ def preprocess(\n         images: ImageInput,\n         segmentation_maps: Optional[ImageInput] = None,\n         instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]] = None,\n-        **kwargs: Unpack[Mask2FormerFastImageProcessorKwargs],\n+        **kwargs: Unpack[Mask2FormerImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n         segmentation_maps (`ImageInput`, *optional*):\n@@ -282,7 +260,7 @@ def _preprocess_image_like_inputs(\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n-        **kwargs: Unpack[Mask2FormerFastImageProcessorKwargs],\n+        **kwargs: Unpack[Mask2FormerImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess image-like inputs."
        },
        {
            "sha": "8353856fb868b607c124c2bea0e5f5910b18e3ad",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -42,6 +42,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import (\n     IMAGENET_DEFAULT_MEAN,\n     IMAGENET_DEFAULT_STD,\n@@ -66,6 +67,25 @@\n     from torch import nn\n \n \n+class MaskFormerImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    ignore_index (`int`, *optional*):\n+        Label to be assigned to background pixels in segmentation maps. If provided, segmentation map pixels\n+        denoted with 0 (background) will be replaced with `ignore_index`.\n+    do_reduce_labels (`bool`, *optional*, defaults to `False`):\n+        Whether or not to decrement all label values of segmentation maps by 1. Usually used for datasets where 0\n+        is used for background, and background itself is not included in all classes of a dataset (e.g. ADE20k).\n+        The background label will be replaced by `ignore_index`.\n+    num_labels (`int`, *optional*):\n+        The number of labels in the segmentation map.\n+    \"\"\"\n+\n+    size_divisor: Optional[int]\n+    ignore_index: Optional[int]\n+    do_reduce_labels: Optional[bool]\n+    num_labels: Optional[int]\n+\n+\n # Copied from transformers.models.detr.image_processing_detr.get_size_with_aspect_ratio\n def get_size_with_aspect_ratio(image_size, size, max_size=None) -> tuple[int, int]:\n     \"\"\"\n@@ -446,6 +466,7 @@ class MaskFormerImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n+    valid_kwargs = MaskFormerImageProcessorKwargs\n \n     @filter_out_non_signature_kwargs(extra=[\"max_size\", *INIT_SERVICE_KWARGS])\n     def __init__("
        },
        {
            "sha": "d174b4ada0a9d2ccd5bacddac0afb630a365895a",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 27,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -25,7 +25,6 @@\n from ...image_processing_utils import BatchFeature, get_size_dict\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     SizeDict,\n     get_image_size_for_max_height_width,\n     get_max_height_width,\n@@ -46,6 +45,7 @@\n     logging,\n )\n from .image_processing_maskformer import (\n+    MaskFormerImageProcessorKwargs,\n     compute_segments,\n     convert_segmentation_to_rle,\n     get_size_with_aspect_ratio,\n@@ -95,28 +95,6 @@ def convert_segmentation_map_to_binary_masks_fast(\n     return binary_masks.float(), labels.long()\n \n \n-class MaskFormerFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    r\"\"\"\n-    size_divisor (`int`, *optional*, defaults to 32):\n-        Some backbones need images divisible by a certain number. If not passed, it defaults to the value used in\n-        Swin Transformer.\n-    ignore_index (`int`, *optional*):\n-        Label to be assigned to background pixels in segmentation maps. If provided, segmentation map pixels\n-        denoted with 0 (background) will be replaced with `ignore_index`.\n-    do_reduce_labels (`bool`, *optional*, defaults to `False`):\n-        Whether or not to decrement all label values of segmentation maps by 1. Usually used for datasets where 0\n-        is used for background, and background itself is not included in all classes of a dataset (e.g. ADE20k).\n-        The background label will be replaced by `ignore_index`.\n-    num_labels (`int`, *optional*):\n-        The number of labels in the segmentation map.\n-    \"\"\"\n-\n-    size_divisor: Optional[int]\n-    ignore_index: Optional[int]\n-    do_reduce_labels: Optional[bool]\n-    num_labels: Optional[int]\n-\n-\n @auto_docstring\n class MaskFormerImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BILINEAR\n@@ -132,9 +110,9 @@ class MaskFormerImageProcessorFast(BaseImageProcessorFast):\n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n     size_divisor = 32\n     do_reduce_labels = False\n-    valid_kwargs = MaskFormerFastImageProcessorKwargs\n+    valid_kwargs = MaskFormerImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[MaskFormerFastImageProcessorKwargs]) -> None:\n+    def __init__(self, **kwargs: Unpack[MaskFormerImageProcessorKwargs]) -> None:\n         if \"pad_and_return_pixel_mask\" in kwargs:\n             kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n \n@@ -264,7 +242,7 @@ def preprocess(\n         images: ImageInput,\n         segmentation_maps: Optional[ImageInput] = None,\n         instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]] = None,\n-        **kwargs: Unpack[MaskFormerFastImageProcessorKwargs],\n+        **kwargs: Unpack[MaskFormerImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n         segmentation_maps (`ImageInput`, *optional*):\n@@ -287,7 +265,7 @@ def _preprocess_image_like_inputs(\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n-        **kwargs: Unpack[MaskFormerFastImageProcessorKwargs],\n+        **kwargs: Unpack[MaskFormerImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess image-like inputs."
        },
        {
            "sha": "cd79f7de31210da3cf4c6bd3d317cad6b9ccc02b",
            "filename": "src/transformers/models/mllama/image_processing_mllama.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -38,6 +38,7 @@\n     to_numpy_array,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, logging\n \n \n@@ -49,6 +50,15 @@\n logger = logging.get_logger(__name__)\n \n \n+class MllamaImageProcessorKwargs(ImagesKwargs):\n+    \"\"\"\n+    max_image_tiles (`int`, *optional*):\n+        The maximum number of tiles allowed.\n+    \"\"\"\n+\n+    max_image_tiles: Optional[int]\n+\n+\n @lru_cache(maxsize=10)\n def get_all_supported_aspect_ratios(max_image_tiles: int) -> list[tuple[int, int]]:\n     \"\"\"\n@@ -567,6 +577,7 @@ class MllamaImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"num_tiles\", \"aspect_ratio_ids\", \"aspect_ratio_mask\"]\n+    valid_kwargs = MllamaImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "3955006a4f9ec4af30e8d3bff1016c26a8fd643f",
            "filename": "src/transformers/models/mllama/processing_mllama.py",
            "status": "modified",
            "additions": 4,
            "deletions": 18,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -21,17 +21,11 @@\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, make_nested_list_of_images\n-from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n \n \n-class MllamaImagesKwargs(ImagesKwargs, total=False):\n-    max_image_tiles: Optional[int]\n-\n-\n class MllamaProcessorKwargs(ProcessingKwargs, total=False):\n-    images_kwargs: MllamaImagesKwargs\n-\n     _defaults = {\n         \"image_kwargs\": {\n             \"max_image_tiles\": 4,\n@@ -225,8 +219,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[MllamaProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -267,10 +259,8 @@ def __call__(\n             **kwargs,\n         )\n \n-        text_kwargs = output_kwargs[\"text_kwargs\"]\n-        text_kwargs[\"return_tensors\"] = None\n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n         images_kwargs = output_kwargs[\"images_kwargs\"]\n-        common_kwargs = output_kwargs[\"common_kwargs\"]\n \n         data = {}\n         if text is not None:\n@@ -280,8 +270,7 @@ def __call__(\n                 raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n             n_images_in_text = [t.count(self.image_token) for t in text]\n             text = [build_string_from_input(text_item, self.bos_token, self.image_token) for text_item in text]\n-            _ = text_kwargs.pop(\"padding_side\", None)  # hack until padding-side is an accepted kwarg by tokenizers\n-            encoding = self.tokenizer(text, **text_kwargs)\n+            encoding = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n             self._check_special_mm_tokens(text, encoding, modalities=[\"image\"])\n             n_images_in_ids = [token_ids.count(self.image_token_id) for token_ids in encoding[\"input_ids\"]]\n             data.update(encoding)\n@@ -334,10 +323,7 @@ def __call__(\n             )\n             data[\"cross_attention_mask\"] = cross_attention_mask\n \n-        return_tensors = common_kwargs.pop(\"return_tensors\", None)\n-        batch_feature = BatchFeature(data=data, tensor_type=return_tensors)\n-\n-        return batch_feature\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n \n     def post_process_image_text_to_text(\n         self, generated_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False, **kwargs"
        },
        {
            "sha": "dc10170734ec1a10750668e1e5eb08a10dbd97aa",
            "filename": "src/transformers/models/mobilenet_v1/image_processing_mobilenet_v1_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -16,8 +16,6 @@\n \n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n-    Unpack,\n )\n from ...image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD, PILImageResampling\n from ...utils import auto_docstring\n@@ -36,8 +34,5 @@ class MobileNetV1ImageProcessorFast(BaseImageProcessorFast):\n     do_rescale = True\n     do_normalize = True\n \n-    def __init__(self, **kwargs: Unpack[DefaultFastImageProcessorKwargs]) -> None:\n-        super().__init__(**kwargs)\n-\n \n __all__ = [\"MobileNetV1ImageProcessorFast\"]"
        },
        {
            "sha": "e8dfe992544a4e757e99fe5f2b119d1c8d1bfa2d",
            "filename": "src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -37,6 +37,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_torch_available, is_torch_tensor, logging\n \n \n@@ -50,6 +51,17 @@\n logger = logging.get_logger(__name__)\n \n \n+class MobileNetV2ImageProcessorKwargs(ImagesKwargs):\n+    \"\"\"\n+    do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n+        Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n+        is used for background, and background itself is not included in all classes of a dataset (e.g.\n+        ADE20k). The background label will be replaced by 255.\n+    \"\"\"\n+\n+    do_reduce_labels: Optional[bool]\n+\n+\n @requires(backends=(\"vision\",))\n class MobileNetV2ImageProcessor(BaseImageProcessor):\n     r\"\"\"\n@@ -96,6 +108,7 @@ class MobileNetV2ImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = MobileNetV2ImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "2c8329a034c1465e8a345f539362daaca23a8a00",
            "filename": "src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 16,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -22,7 +22,6 @@\n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -40,17 +39,7 @@\n     TensorType,\n     auto_docstring,\n )\n-\n-\n-class MobileNetV2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n-        Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n-        is used for background, and background itself is not included in all classes of a dataset (e.g.\n-        ADE20k). The background label will be replaced by 255.\n-    \"\"\"\n-\n-    do_reduce_labels: Optional[bool]\n+from .image_processing_mobilenet_v2 import MobileNetV2ImageProcessorKwargs\n \n \n @auto_docstring\n@@ -66,9 +55,9 @@ class MobileNetV2ImageProcessorFast(BaseImageProcessorFast):\n     do_rescale = True\n     do_normalize = True\n     do_reduce_labels = False\n-    valid_kwargs = MobileNetV2FastImageProcessorKwargs\n+    valid_kwargs = MobileNetV2ImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[MobileNetV2FastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[MobileNetV2ImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     # Copied from transformers.models.beit.image_processing_beit_fast.BeitImageProcessorFast.reduce_label\n@@ -87,7 +76,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         segmentation_maps: Optional[ImageInput] = None,\n-        **kwargs: Unpack[MobileNetV2FastImageProcessorKwargs],\n+        **kwargs: Unpack[MobileNetV2ImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n         segmentation_maps (`ImageInput`, *optional*):\n@@ -102,7 +91,7 @@ def _preprocess_image_like_inputs(\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n-        **kwargs: Unpack[MobileNetV2FastImageProcessorKwargs],\n+        **kwargs: Unpack[MobileNetV2ImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess image-like inputs."
        },
        {
            "sha": "576ef9f449dc96a6d921c666cd530e3fa86b2942",
            "filename": "src/transformers/models/mobilevit/image_processing_mobilevit.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -31,6 +31,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import (\n     TensorType,\n     filter_out_non_signature_kwargs,\n@@ -52,6 +53,20 @@\n logger = logging.get_logger(__name__)\n \n \n+class MobileVitImageProcessorKwargs(ImagesKwargs):\n+    \"\"\"\n+    do_flip_channel_order (`bool`, *optional*, defaults to `self.do_flip_channel_order`):\n+        Whether to flip the color channels from RGB to BGR or vice versa.\n+    do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n+        Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n+        is used for background, and background itself is not included in all classes of a dataset (e.g.\n+        ADE20k). The background label will be replaced by 255.\n+    \"\"\"\n+\n+    do_flip_channel_order: Optional[bool]\n+    do_reduce_labels: Optional[bool]\n+\n+\n @requires(backends=(\"vision\",))\n class MobileViTImageProcessor(BaseImageProcessor):\n     r\"\"\"\n@@ -91,6 +106,7 @@ class MobileViTImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = MobileVitImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "81d745c2b54dc743320f2961614b9915e1225e09",
            "filename": "src/transformers/models/mobilevit/image_processing_mobilevit_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 19,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -22,7 +22,6 @@\n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -38,20 +37,7 @@\n     TensorType,\n     auto_docstring,\n )\n-\n-\n-class MobileVitFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    do_flip_channel_order (`bool`, *optional*, defaults to `self.do_flip_channel_order`):\n-        Whether to flip the color channels from RGB to BGR or vice versa.\n-    do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n-        Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n-        is used for background, and background itself is not included in all classes of a dataset (e.g.\n-        ADE20k). The background label will be replaced by 255.\n-    \"\"\"\n-\n-    do_flip_channel_order: Optional[bool]\n-    do_reduce_labels: Optional[bool]\n+from .image_processing_mobilevit import MobileVitImageProcessorKwargs\n \n \n @auto_docstring\n@@ -67,9 +53,9 @@ class MobileViTImageProcessorFast(BaseImageProcessorFast):\n     do_convert_rgb = None\n     do_flip_channel_order = True\n     do_reduce_labels = False\n-    valid_kwargs = MobileVitFastImageProcessorKwargs\n+    valid_kwargs = MobileVitImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[MobileVitFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[MobileVitImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     # Copied from transformers.models.beit.image_processing_beit_fast.BeitImageProcessorFast.reduce_label\n@@ -88,7 +74,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         segmentation_maps: Optional[ImageInput] = None,\n-        **kwargs: Unpack[MobileVitFastImageProcessorKwargs],\n+        **kwargs: Unpack[MobileVitImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n         segmentation_maps (`ImageInput`, *optional*):\n@@ -103,7 +89,7 @@ def _preprocess_image_like_inputs(\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n-        **kwargs: Unpack[MobileVitFastImageProcessorKwargs],\n+        **kwargs: Unpack[MobileVitImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess image-like inputs."
        },
        {
            "sha": "0a5c445645e00708cd7cd06c701003b94a24a3e3",
            "filename": "src/transformers/models/nougat/image_processing_nougat.py",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -40,6 +40,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, logging\n from ...utils.import_utils import is_vision_available\n \n@@ -51,6 +52,21 @@\n     import PIL\n \n \n+class NougatImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    do_crop_margin (`bool`, *optional*, defaults to `True`):\n+        Whether to crop the image margins.\n+    do_thumbnail (`bool`, *optional*, defaults to `True`):\n+        Whether to resize the image using thumbnail method.\n+    do_align_long_axis (`bool`, *optional*, defaults to `False`):\n+        Whether to align the long axis of the image with the long axis of `size` by rotating by 90 degrees.\n+    \"\"\"\n+\n+    do_crop_margin: Optional[bool]\n+    do_thumbnail: Optional[bool]\n+    do_align_long_axis: Optional[bool]\n+\n+\n class NougatImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a Nougat image processor.\n@@ -87,6 +103,7 @@ class NougatImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = NougatImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "b059688d00468e35a066b26fd697b931d0c02198",
            "filename": "src/transformers/models/nougat/image_processing_nougat_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 20,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -22,7 +22,6 @@\n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -42,22 +41,7 @@\n     TensorType,\n     auto_docstring,\n )\n-\n-\n-class NougatFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    Args:\n-    do_crop_margin (`bool`, *optional*, defaults to `True`):\n-            Whether to crop the image margins.\n-    do_thumbnail (`bool`, *optional*, defaults to `True`):\n-            Whether to resize the image using thumbnail method.\n-    do_align_long_axis (`bool`, *optional*, defaults to `False`):\n-            Whether to align the long axis of the image with the long axis of `size` by rotating by 90 degrees.\n-    \"\"\"\n-\n-    do_crop_margin: Optional[bool]\n-    do_thumbnail: Optional[bool]\n-    do_align_long_axis: Optional[bool]\n+from .image_processing_nougat import NougatImageProcessorKwargs\n \n \n @auto_docstring\n@@ -73,13 +57,13 @@ class NougatImageProcessorFast(BaseImageProcessorFast):\n     do_pad: bool = True\n     do_rescale = True\n     do_crop_margin: bool = True\n-    valid_kwargs = NougatFastImageProcessorKwargs\n+    valid_kwargs = NougatImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[NougatFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[NougatImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[NougatFastImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[NougatImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     def python_find_non_zero("
        },
        {
            "sha": "842fe5d9bddf3476b8bd7dfb39b4e256f64b9adc",
            "filename": "src/transformers/models/omdet_turbo/processing_omdet_turbo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -65,7 +65,6 @@ class OmDetTurboProcessorKwargs(ProcessingKwargs, total=False):\n             \"verbose\": True,\n             \"task\": None,\n         },\n-        \"images_kwargs\": {},\n     }\n \n \n@@ -227,8 +226,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[list[str], list[list[str]]]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[OmDetTurboProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "86ce8abf084e4d78bccc9c220a4f79403c15e1f3",
            "filename": "src/transformers/models/oneformer/image_processing_oneformer.py",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -44,6 +44,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import (\n     IMAGENET_DEFAULT_MEAN,\n     IMAGENET_DEFAULT_STD,\n@@ -63,6 +64,30 @@\n     from torch import nn\n \n \n+class OneFormerImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    repo_path (`str`, *optional*, defaults to `shi-labs/oneformer_demo`):\n+        Path to a local directory or Hugging Face Hub repository containing model metadata.\n+    class_info_file (`str`, *optional*):\n+        Path to the JSON file within the repository that contains class metadata.\n+    num_text (`int`, *optional*):\n+        Number of text queries for the text encoder, used as task-guiding prompts.\n+    num_labels (`int`, *optional*):\n+        Number of semantic classes for segmentation, determining the output layer's size.\n+    ignore_index (`int`, *optional*):\n+        Label to ignore in segmentation maps, often used for padding.\n+    do_reduce_labels (`bool`, *optional*, defaults to `False`):\n+        Whether to decrement all label values by 1, mapping the background class to `ignore_index`.\n+    \"\"\"\n+\n+    repo_path: Optional[str]\n+    class_info_file: Optional[str]\n+    num_text: Optional[int]\n+    num_labels: Optional[int]\n+    ignore_index: Optional[int]\n+    do_reduce_labels: Optional[bool]\n+\n+\n # Copied from transformers.models.detr.image_processing_detr.max_across_indices\n def max_across_indices(values: Iterable[Any]) -> list[Any]:\n     \"\"\"\n@@ -423,6 +448,7 @@ class OneFormerImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"pixel_mask\", \"task_inputs\"]\n+    valid_kwargs = OneFormerImageProcessorKwargs\n \n     @filter_out_non_signature_kwargs(extra=[\"max_size\", \"metadata\", *INIT_SERVICE_KWARGS])\n     def __init__("
        },
        {
            "sha": "a14b0015b4984c77ad7120d1f730e503f339a942",
            "filename": "src/transformers/models/oneformer/image_processing_oneformer_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 30,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -23,7 +23,6 @@\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n     BatchFeature,\n-    DefaultFastImageProcessorKwargs,\n     get_max_height_width,\n     group_images_by_shape,\n     reorder_images,\n@@ -42,7 +41,7 @@\n     auto_docstring,\n     logging,\n )\n-from .image_processing_oneformer import load_metadata, prepare_metadata\n+from .image_processing_oneformer import OneFormerImageProcessorKwargs, load_metadata, prepare_metadata\n \n \n logger = logging.get_logger(__name__)\n@@ -300,30 +299,6 @@ def get_oneformer_resize_output_image_size(\n     return (new_long, new_short) if width <= height else (new_short, new_long)\n \n \n-class OneFormerFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    r\"\"\"\n-    repo_path (`str`, *optional*, defaults to `shi-labs/oneformer_demo`):\n-        Path to a local directory or Hugging Face Hub repository containing model metadata.\n-    class_info_file (`str`, *optional*):\n-        Path to the JSON file within the repository that contains class metadata.\n-    num_text (`int`, *optional*):\n-        Number of text queries for the text encoder, used as task-guiding prompts.\n-    num_labels (`int`, *optional*):\n-        Number of semantic classes for segmentation, determining the output layer's size.\n-    ignore_index (`int`, *optional*):\n-        Label to ignore in segmentation maps, often used for padding.\n-    do_reduce_labels (`bool`, *optional*, defaults to `False`):\n-        Whether to decrement all label values by 1, mapping the background class to `ignore_index`.\n-    \"\"\"\n-\n-    repo_path: Optional[str]\n-    class_info_file: Optional[str]\n-    num_text: Optional[int]\n-    num_labels: Optional[int]\n-    ignore_index: Optional[int]\n-    do_reduce_labels: Optional[bool]\n-\n-\n @auto_docstring\n class OneFormerImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BILINEAR\n@@ -344,10 +319,10 @@ class OneFormerImageProcessorFast(BaseImageProcessorFast):\n     class_info_file = None\n     num_text = None\n     num_labels = None\n-    valid_kwargs = OneFormerFastImageProcessorKwargs\n+    valid_kwargs = OneFormerImageProcessorKwargs\n     model_input_names = [\"pixel_values\", \"pixel_mask\", \"task_inputs\"]\n \n-    def __init__(self, **kwargs: Unpack[OneFormerFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[OneFormerImageProcessorKwargs]):\n         super().__init__(**kwargs)\n         if self.class_info_file:\n             self.metadata = prepare_metadata(load_metadata(self.repo_path, self.class_info_file))\n@@ -359,7 +334,7 @@ def preprocess(\n         task_inputs: Optional[list[str]] = None,\n         segmentation_maps: Optional[ImageInput] = None,\n         instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]] = None,\n-        **kwargs: Unpack[OneFormerFastImageProcessorKwargs],\n+        **kwargs: Unpack[OneFormerImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n         task_inputs (`list[str]`, *optional*):\n@@ -386,7 +361,7 @@ def _preprocess_image_like_inputs(\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n-        **kwargs: Unpack[OneFormerFastImageProcessorKwargs],\n+        **kwargs: Unpack[OneFormerImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess image-like inputs."
        },
        {
            "sha": "5d0c2e2097d3299712b4b3aefaf74563caa52133",
            "filename": "src/transformers/models/ovis2/image_processing_ovis2.py",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -33,6 +33,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n \n \n@@ -43,6 +44,29 @@\n logger = logging.get_logger(__name__)\n \n \n+class Ovis2ImageProcessorKwargs(ImagesKwargs):\n+    \"\"\"\n+    crop_to_patches (`bool`, *optional*, defaults to `False`):\n+        Whether to crop the image to patches. Can be overridden by the `crop_to_patches` parameter in the\n+        `preprocess` method.\n+    min_patches (`int`, *optional*, defaults to 1):\n+        The minimum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n+        set to `True`. Can be overridden by the `min_patches` parameter in the `preprocess` method.\n+    max_patches (`int`, *optional*, defaults to 12):\n+        The maximum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n+        set to `True`. Can be overridden by the `max_patches` parameter in the `preprocess` method.\n+    use_covering_area_grid (`bool`, *optional*, defaults to `True`):\n+        Whether to use the covering area grid to determine the number of patches. Only has an effect if\n+        `crop_to_patches` is set to `True`. Can be overridden by the `use_covering_area_grid` parameter in the\n+        `preprocess` method.\n+    \"\"\"\n+\n+    crop_to_patches: Optional[bool]\n+    min_patches: Optional[int]\n+    max_patches: Optional[int]\n+    use_covering_area_grid: Optional[bool]\n+\n+\n # Similar to image_processing_mllama.get_all_supported_aspect_ratios\n @lru_cache(maxsize=10)\n def get_all_supported_aspect_ratios(min_image_tiles: int, max_image_tiles: int) -> list[tuple[int, int]]:\n@@ -224,6 +248,7 @@ class Ovis2ImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = Ovis2ImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "ea618e073526deb293245e25c474909efbdf420f",
            "filename": "src/transformers/models/ovis2/image_processing_ovis2_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 26,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -21,7 +21,6 @@\n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -37,31 +36,7 @@\n     TensorType,\n     auto_docstring,\n )\n-from .image_processing_ovis2 import get_min_tile_covering_grid, get_optimal_tiled_canvas\n-\n-\n-class Ovis2ImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    Args:\n-        crop_to_patches (`bool`, *optional*, defaults to `False`):\n-            Whether to crop the image to patches. Can be overridden by the `crop_to_patches` parameter in the\n-            `preprocess` method.\n-        min_patches (`int`, *optional*, defaults to 1):\n-            The minimum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n-            set to `True`. Can be overridden by the `min_patches` parameter in the `preprocess` method.\n-        max_patches (`int`, *optional*, defaults to 12):\n-            The maximum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n-            set to `True`. Can be overridden by the `max_patches` parameter in the `preprocess` method.\n-        use_covering_area_grid (`bool`, *optional*, defaults to `True`):\n-            Whether to use the covering area grid to determine the number of patches. Only has an effect if\n-            `crop_to_patches` is set to `True`. Can be overridden by the `use_covering_area_grid` parameter in the\n-            `preprocess` method.\n-    \"\"\"\n-\n-    crop_to_patches: Optional[bool]\n-    min_patches: Optional[int]\n-    max_patches: Optional[int]\n-    use_covering_area_grid: Optional[bool]\n+from .image_processing_ovis2 import Ovis2ImageProcessorKwargs, get_min_tile_covering_grid, get_optimal_tiled_canvas\n \n \n @auto_docstring"
        },
        {
            "sha": "25022f4f6c8c4563742b99082f6fd91dc36b56f1",
            "filename": "src/transformers/models/owlv2/image_processing_owlv2_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 21,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -25,17 +25,9 @@\n import torch\n from torchvision.transforms.v2 import functional as F\n \n-from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature, DefaultFastImageProcessorKwargs\n+from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature\n from ...image_transforms import center_to_corners_format, group_images_by_shape, reorder_images\n-from ...image_utils import (\n-    OPENAI_CLIP_MEAN,\n-    OPENAI_CLIP_STD,\n-    ChannelDimension,\n-    ImageInput,\n-    PILImageResampling,\n-    SizeDict,\n-)\n-from ...processing_utils import Unpack\n+from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, ChannelDimension, PILImageResampling, SizeDict\n from ...utils import TensorType, auto_docstring\n from .image_processing_owlv2 import _scale_boxes, box_iou\n \n@@ -44,9 +36,6 @@\n     from .modeling_owlv2 import Owlv2ObjectDetectionOutput\n \n \n-class Owlv2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs): ...\n-\n-\n @auto_docstring\n class Owlv2ImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BILINEAR\n@@ -63,7 +52,6 @@ class Owlv2ImageProcessorFast(BaseImageProcessorFast):\n     model_input_names = [\"pixel_values\"]\n     rescale_factor = 1 / 255\n     do_pad = True\n-    valid_kwargs = Owlv2FastImageProcessorKwargs\n \n     def post_process(self, outputs, target_sizes):\n         \"\"\"\n@@ -240,13 +228,6 @@ def post_process_image_guided_detection(self, outputs, threshold=0.0, nms_thresh\n \n         return results\n \n-    def __init__(self, **kwargs: Unpack[Owlv2FastImageProcessorKwargs]):\n-        super().__init__(**kwargs)\n-\n-    @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[Owlv2FastImageProcessorKwargs]):\n-        return super().preprocess(images, **kwargs)\n-\n     def _pad_images(self, images: \"torch.Tensor\", constant_value: float = 0.5) -> \"torch.Tensor\":\n         \"\"\"\n         Pad an image with zeros to the given size."
        },
        {
            "sha": "c58db1efd46e2ee89a2b95971d6e4a00b9b6fc46",
            "filename": "src/transformers/models/owlv2/modular_owlv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodular_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodular_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodular_owlv2.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -21,30 +21,23 @@\n from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils_fast import (\n-    BaseImageProcessorFast,\n     BatchFeature,\n-    DefaultFastImageProcessorKwargs,\n )\n from ...image_transforms import group_images_by_shape, reorder_images\n from ...image_utils import (\n     OPENAI_CLIP_MEAN,\n     OPENAI_CLIP_STD,\n     ChannelDimension,\n-    ImageInput,\n     PILImageResampling,\n     SizeDict,\n )\n-from ...processing_utils import Unpack\n from ...utils import (\n     TensorType,\n     auto_docstring,\n )\n from ..owlvit.image_processing_owlvit_fast import OwlViTImageProcessorFast\n \n \n-class Owlv2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs): ...\n-\n-\n @auto_docstring\n class Owlv2ImageProcessorFast(OwlViTImageProcessorFast):\n     resample = PILImageResampling.BILINEAR\n@@ -56,17 +49,9 @@ class Owlv2ImageProcessorFast(OwlViTImageProcessorFast):\n     do_rescale = True\n     do_normalize = True\n     do_pad = True\n-    valid_kwargs = Owlv2FastImageProcessorKwargs\n     crop_size = None\n     do_center_crop = None\n \n-    def __init__(self, **kwargs: Unpack[Owlv2FastImageProcessorKwargs]):\n-        BaseImageProcessorFast.__init__(self, **kwargs)\n-\n-    @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[Owlv2FastImageProcessorKwargs]):\n-        return BaseImageProcessorFast.preprocess(self, images, **kwargs)\n-\n     def _pad_images(self, images: \"torch.Tensor\", constant_value: float = 0.5) -> \"torch.Tensor\":\n         \"\"\"\n         Pad an image with zeros to the given size."
        },
        {
            "sha": "65f111e2ca793ff9849ccc32928e87e98ae4f961",
            "filename": "src/transformers/models/owlv2/processing_owlv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -47,7 +47,6 @@ class Owlv2ProcessorKwargs(ProcessingKwargs, total=False):\n         \"text_kwargs\": {\n             \"padding\": \"max_length\",\n         },\n-        \"images_kwargs\": {},\n         \"common_kwargs\": {\n             \"return_tensors\": \"np\",\n         },\n@@ -79,8 +78,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[Owlv2ProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -123,7 +120,7 @@ def __call__(\n             **kwargs,\n         )\n         query_images = output_kwargs[\"images_kwargs\"].pop(\"query_images\", None)\n-        return_tensors = output_kwargs[\"common_kwargs\"][\"return_tensors\"]\n+        return_tensors = output_kwargs[\"text_kwargs\"][\"return_tensors\"]\n \n         if text is None and query_images is None and images is None:\n             raise ValueError("
        },
        {
            "sha": "e7fb401d9a76fb896d9f4d311c93abb4c76ba0fd",
            "filename": "src/transformers/models/owlvit/processing_owlvit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -47,7 +47,6 @@ class OwlViTProcessorKwargs(ProcessingKwargs, total=False):\n         \"text_kwargs\": {\n             \"padding\": \"max_length\",\n         },\n-        \"images_kwargs\": {},\n         \"common_kwargs\": {\n             \"return_tensors\": \"np\",\n         },\n@@ -89,8 +88,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[OwlViTProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -133,7 +130,7 @@ def __call__(\n             **kwargs,\n         )\n         query_images = output_kwargs[\"images_kwargs\"].pop(\"query_images\", None)\n-        return_tensors = output_kwargs[\"common_kwargs\"][\"return_tensors\"]\n+        return_tensors = output_kwargs[\"text_kwargs\"][\"return_tensors\"]\n \n         if text is None and query_images is None and images is None:\n             raise ValueError("
        },
        {
            "sha": "7fa636ab796be2fcef227df50b15359fe46ec885",
            "filename": "src/transformers/models/paligemma/processing_paligemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -23,7 +23,6 @@\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, is_valid_image\n from ...processing_utils import (\n-    ImagesKwargs,\n     MultiModalData,\n     ProcessingKwargs,\n     ProcessorMixin,\n@@ -44,13 +43,8 @@ class PaliGemmaTextKwargs(TextKwargs):\n     suffix: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]]\n \n \n-class PaliGemmaImagesKwargs(ImagesKwargs):\n-    do_convert_rgb: Optional[bool]\n-\n-\n class PaliGemmaProcessorKwargs(ProcessingKwargs, total=False):\n     text_kwargs: PaliGemmaTextKwargs\n-    images_kwargs: PaliGemmaImagesKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,\n@@ -150,8 +144,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[PaliGemmaProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "c6491b4bc703d98de4b12ef63ae83bbd2aec0676",
            "filename": "src/transformers/models/perception_lm/image_processing_perception_lm_fast.py",
            "status": "modified",
            "additions": 16,
            "deletions": 11,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fimage_processing_perception_lm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fimage_processing_perception_lm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fimage_processing_perception_lm_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -25,7 +25,6 @@\n )\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     get_image_size,\n     group_images_by_shape,\n     reorder_images,\n@@ -36,11 +35,14 @@\n     ChannelDimension,\n     PILImageResampling,\n )\n-from ...processing_utils import Unpack\n-from ...utils import TensorType, auto_docstring\n+from ...processing_utils import ImagesKwargs, Unpack\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+)\n \n \n-class PerceptionLMFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+class PerceptionLMImageProcessorKwargs(ImagesKwargs):\n     r\"\"\"\n     vision_input_type (`str`, *optional*, defaults to `\"thumb+tile\"`):\n         Vision processing strategy. `\"thumb+tile\"` uses both thumbnails and multiple tiles for\n@@ -51,9 +53,9 @@ class PerceptionLMFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n         Maximum number of tiles an image can be split into based on its aspect ratio.\n     \"\"\"\n \n-    vision_input_type: str = \"thumb+tile\"\n-    tile_size: int = 448\n-    max_num_tiles: int = 36\n+    vision_input_type: Optional[str]\n+    tile_size: Optional[int]\n+    max_num_tiles: Optional[int]\n \n \n @auto_docstring\n@@ -66,14 +68,17 @@ class PerceptionLMImageProcessorFast(BaseImageProcessorFast):\n     do_rescale = True\n     do_normalize = True\n     do_convert_rgb = True\n+    vision_input_type = \"thumb+tail\"\n+    tile_size = 448\n+    max_num_tiles = 36\n     size = {\"width\": 448, \"height\": 448}  # for backward compatibility in tests\n-    valid_kwargs = PerceptionLMFastImageProcessorKwargs\n+    valid_kwargs = PerceptionLMImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[PerceptionLMFastImageProcessorKwargs]) -> None:\n+    def __init__(self, **kwargs: Unpack[PerceptionLMImageProcessorKwargs]) -> None:\n         super().__init__(**kwargs)\n \n     @auto_docstring\n-    def preprocess(self, images, **kwargs: Unpack[PerceptionLMFastImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images, **kwargs: Unpack[PerceptionLMImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     @staticmethod\n@@ -267,7 +272,7 @@ def _preprocess(\n         max_num_tiles: int,\n         return_tensors: Optional[Union[str, TensorType]],\n         disable_grouping: bool,\n-        **kwargs: Unpack[PerceptionLMFastImageProcessorKwargs],\n+        **kwargs: Unpack[PerceptionLMImageProcessorKwargs],\n     ) -> BatchFeature:\n         # Group images by size for batched transformation\n         grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)"
        },
        {
            "sha": "e57418ef92f7c4d0a54d95a1f2087bdee0e1bfbf",
            "filename": "src/transformers/models/perception_lm/processing_perception_lm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -89,7 +89,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        audio=None,\n         videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[PerceptionLMProcessorKwargs],\n     ) -> BatchFeature:"
        },
        {
            "sha": "9b5c5d0b67a097b88993f46311c56dec224c7b23",
            "filename": "src/transformers/models/perception_lm/video_processing_perception_lm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fvideo_processing_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fvideo_processing_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fvideo_processing_perception_lm.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -14,13 +14,9 @@\n \"\"\"Video processor class for PerceptionLM.\"\"\"\n \n from ...image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD, PILImageResampling\n-from ...processing_utils import Unpack, VideosKwargs\n from ...video_processing_utils import BaseVideoProcessor\n \n \n-class PerceptionLMFastVideoProcessorInitKwargs(VideosKwargs): ...\n-\n-\n class PerceptionLMVideoProcessor(BaseVideoProcessor):\n     resample = PILImageResampling.BICUBIC\n     image_mean = IMAGENET_STANDARD_MEAN\n@@ -31,11 +27,6 @@ class PerceptionLMVideoProcessor(BaseVideoProcessor):\n     do_rescale = True\n     do_normalize = True\n     do_convert_rgb = True\n-    valid_kwargs = PerceptionLMFastVideoProcessorInitKwargs\n-    model_input_names = [\"pixel_values_videos\"]\n-\n-    def __init__(self, **kwargs: Unpack[PerceptionLMFastVideoProcessorInitKwargs]):\n-        super().__init__(**kwargs)\n \n \n __all__ = [\"PerceptionLMVideoProcessor\"]"
        },
        {
            "sha": "5c750fae953ead0951ba1bd1c0c5e7776b503984",
            "filename": "src/transformers/models/phi4_multimodal/image_processing_phi4_multimodal_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fimage_processing_phi4_multimodal_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fimage_processing_phi4_multimodal_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fimage_processing_phi4_multimodal_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -21,10 +21,10 @@\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n     BatchFeature,\n-    DefaultFastImageProcessorKwargs,\n     Unpack,\n )\n from ...image_utils import ImageInput, PILImageResampling, SizeDict\n+from ...processing_utils import ImagesKwargs\n from ...utils import (\n     TensorType,\n     auto_docstring,\n@@ -35,7 +35,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class Phi4MultimodalFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+class Phi4MultimodalImageProcessorKwargs(ImagesKwargs):\n     r\"\"\"\n     patch_size (`int`, *optional*):\n         The size of the patch.\n@@ -59,10 +59,10 @@ class Phi4MultimodalImageProcessorFast(BaseImageProcessorFast):\n     do_rescale = True\n     do_normalize = True\n     do_convert_rgb = True\n-    valid_kwargs = Phi4MultimodalFastImageProcessorKwargs\n+    valid_kwargs = Phi4MultimodalImageProcessorKwargs\n     model_input_names = [\"image_pixel_values\", \"image_sizes\", \"image_attention_mask\"]\n \n-    def __init__(self, **kwargs: Unpack[Phi4MultimodalFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[Phi4MultimodalImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     def find_closest_aspect_ratio(self, aspect_ratio, target_ratios, width, height, image_size):\n@@ -158,7 +158,7 @@ def pad_mask_to_max_num_crops(self, masks, max_crops=5):\n     def preprocess(\n         self,\n         images: ImageInput,\n-        **kwargs: Unpack[Phi4MultimodalFastImageProcessorKwargs],\n+        **kwargs: Unpack[Phi4MultimodalImageProcessorKwargs],\n     ) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n "
        },
        {
            "sha": "e0c6303690295b662abdf5bb28d8a51b76ec9c14",
            "filename": "src/transformers/models/pix2struct/image_processing_pix2struct.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -32,6 +32,7 @@\n     to_numpy_array,\n     valid_images,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, is_torch_available, is_vision_available, logging\n from ...utils.import_utils import requires_backends\n \n@@ -48,6 +49,18 @@\n DEFAULT_FONT_PATH = \"ybelkada/fonts\"\n \n \n+class Pix2StructImageProcessorKwargs(ImagesKwargs):\n+    \"\"\"\n+    max_patches (`int`, *optional*):\n+        Maximum number of patches to extract.\n+    header_text (`Union[list[str], str]`, *optional*):\n+        Text to render as a header. Only has an effect if `image_processor.is_vqa` is `True`.\n+    \"\"\"\n+\n+    max_patches: Optional[int]\n+    header_text: Optional[Union[list[str], str]]\n+\n+\n # adapted from: https://discuss.pytorch.org/t/tf-image-extract-patches-in-pytorch/171409/2\n def torch_extract_patches(image_tensor, patch_height, patch_width):\n     \"\"\"\n@@ -208,6 +221,7 @@ class Pix2StructImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"flattened_patches\", \"attention_mask\"]\n+    valid_kwargs = Pix2StructImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "fba2fe93ef19d141ec775bc8ab4f998daa6e93a8",
            "filename": "src/transformers/models/pix2struct/processing_pix2struct.py",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -16,21 +16,15 @@\n Processor class for Pix2Struct.\n \"\"\"\n \n-from typing import Optional, Union\n+from typing import Union\n \n from ...feature_extraction_utils import BatchFeature\n-from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n from ...utils import logging\n \n \n-class Pix2StructImagesKwargs(ImagesKwargs, total=False):\n-    max_patches: Optional[int]\n-    header_text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]]\n-\n-\n class Pix2StructProcessorKwargs(ProcessingKwargs, total=False):\n-    images_kwargs: Pix2StructImagesKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"add_special_tokens\": True,\n@@ -79,8 +73,6 @@ def __call__(\n         self,\n         images=None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[Pix2StructProcessorKwargs],\n     ) -> Union[BatchEncoding, BatchFeature]:\n         \"\"\""
        },
        {
            "sha": "f5df895e66a4a2164c3d9a8c8aff21ff74c753d9",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -38,6 +38,7 @@\n     validate_kwargs,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, is_vision_available, logging\n from ...utils.import_utils import requires_backends\n \n@@ -49,6 +50,15 @@\n     import PIL\n \n \n+class PixtralImageProcessorKwargs(ImagesKwargs):\n+    \"\"\"\n+    patch_size (`dict[str, int]` *optional*, defaults to `{\"height\": 16, \"width\": 16}`):\n+        Size of the patches in the model, used to calculate the output image size. Can be overridden by `patch_size` in the `preprocess` method.\n+    \"\"\"\n+\n+    patch_size: Optional[dict[str, int]]\n+\n+\n # Adapted from function in image_transforms.py to ensure any transparent pixels are converted to white.\n def convert_to_rgb(image: ImageInput) -> ImageInput:\n     \"\"\"\n@@ -171,6 +181,7 @@ class PixtralImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"image_sizes\"]\n+    valid_kwargs = PixtralImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "4a877d633048ffb0b4e5675558643a8d55c34952",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 14,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -22,7 +22,6 @@\n from ...image_processing_utils import BatchFeature, get_size_dict\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -33,21 +32,12 @@\n     auto_docstring,\n     logging,\n )\n-from .image_processing_pixtral import get_resize_output_image_size\n+from .image_processing_pixtral import PixtralImageProcessorKwargs, get_resize_output_image_size\n \n \n logger = logging.get_logger(__name__)\n \n \n-class PixtralFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    patch_size (`dict[str, int]` *optional*, defaults to `{\"height\": 16, \"width\": 16}`):\n-        Size of the patches in the model, used to calculate the output image size. Can be overridden by `patch_size` in the `preprocess` method.\n-    \"\"\"\n-\n-    patch_size: Optional[dict[str, int]]\n-\n-\n @auto_docstring\n class PixtralImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BICUBIC\n@@ -60,15 +50,15 @@ class PixtralImageProcessorFast(BaseImageProcessorFast):\n     do_rescale = True\n     do_normalize = True\n     do_convert_rgb = True\n-    valid_kwargs = PixtralFastImageProcessorKwargs\n+    valid_kwargs = PixtralImageProcessorKwargs\n \n     model_input_names = [\"pixel_values\", \"image_sizes\"]\n \n-    def __init__(self, **kwargs: Unpack[PixtralFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[PixtralImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[PixtralFastImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[PixtralImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     def resize("
        },
        {
            "sha": "5bb9fd78032835915590dc18624afad90c37cb82",
            "filename": "src/transformers/models/pixtral/processing_pixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -45,7 +45,6 @@ class PixtralProcessorKwargs(ProcessingKwargs, total=False):\n             \"padding\": False,\n             \"return_mm_token_type_ids\": False,\n         },\n-        \"images_kwargs\": {},\n         \"common_kwargs\": {\n             \"return_tensors\": \"pt\",\n         },\n@@ -120,8 +119,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[PixtralProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -164,7 +161,8 @@ def __call__(\n         patch_size = self.patch_size * self.spatial_merge_size\n \n         if images is not None:\n-            image_inputs = self.image_processor(images, patch_size=patch_size, **output_kwargs[\"images_kwargs\"])\n+            output_kwargs[\"images_kwargs\"][\"patch_size\"] = patch_size\n+            image_inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n         else:\n             image_inputs = {}\n "
        },
        {
            "sha": "7d03f828128594258ebb24c5e8a1ab5df334983f",
            "filename": "src/transformers/models/poolformer/image_processing_poolformer.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -37,6 +37,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n \n \n@@ -47,6 +48,15 @@\n logger = logging.get_logger(__name__)\n \n \n+class PoolFormerImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    crop_pct (`float`, *optional*, defaults to `self.crop_pct`):\n+        Percentage of the image to crop. Only has an effect if `do_resize` is set to `True`.\n+    \"\"\"\n+\n+    crop_pct: Optional[float]\n+\n+\n class PoolFormerImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a PoolFormer image processor.\n@@ -99,6 +109,7 @@ class PoolFormerImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = PoolFormerImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "594d076a924c5eeccc4db26660151ce6b9bcf56c",
            "filename": "src/transformers/models/poolformer/image_processing_poolformer_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 14,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -19,7 +19,7 @@\n import torch\n from torchvision.transforms.v2 import functional as F\n \n-from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature, DefaultFastImageProcessorKwargs\n+from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature\n from ...image_transforms import (\n     ChannelDimension,\n     get_resize_output_image_size,\n@@ -40,16 +40,7 @@\n     TensorType,\n     auto_docstring,\n )\n-\n-\n-class PoolFormerFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    Args:\n-        crop_pct (`float`, *optional*, defaults to `self.crop_pct`):\n-            Percentage of the image to crop. Only has an effect if `do_resize` is set to `True`.\n-    \"\"\"\n-\n-    crop_pct: Optional[float]\n+from .image_processing_poolformer import PoolFormerImageProcessorKwargs\n \n \n @auto_docstring\n@@ -65,13 +56,13 @@ class PoolFormerImageProcessorFast(BaseImageProcessorFast):\n     do_center_crop = True\n     do_rescale = True\n     do_normalize = True\n-    valid_kwargs = PoolFormerFastImageProcessorKwargs\n+    valid_kwargs = PoolFormerImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[PoolFormerFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[PoolFormerImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[PoolFormerFastImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[PoolFormerImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     def resize("
        },
        {
            "sha": "b8220a30fa42a211357e79daff30494bfe3acbf4",
            "filename": "src/transformers/models/prompt_depth_anything/image_processing_prompt_depth_anything.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -38,6 +38,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import (\n     TensorType,\n     filter_out_non_signature_kwargs,\n@@ -53,6 +54,22 @@\n logger = logging.get_logger(__name__)\n \n \n+class PromptDepthAnythingImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    keep_aspect_ratio (`bool`, *optional*):\n+        If `True`, the image is resized to the largest possible size such that the aspect ratio is preserved.\n+    ensure_multiple_of (`int`, *optional*):\n+        If `do_resize` is `True`, the image is resized to a size that is a multiple of this value.\n+    prompt_scale_to_meter (`float`, *optional*):\n+        Scale factor to convert the prompt depth to meters.\n+    \"\"\"\n+\n+    keep_aspect_ratio: Optional[bool]\n+    ensure_multiple_of: Optional[int]\n+    size_divisor: Optional[int]\n+    prompt_scale_to_meter: Optional[float]\n+\n+\n def _constrain_to_multiple_of(val, multiple, min_val=0, max_val=None):\n     x = round(val / multiple) * multiple\n \n@@ -136,6 +153,7 @@ class PromptDepthAnythingImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"prompt_depth\"]\n+    valid_kwargs = PromptDepthAnythingImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "552d921700bcd1193443c5d0e85b14b30ba5cd85",
            "filename": "src/transformers/models/prompt_depth_anything/image_processing_prompt_depth_anything_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 26,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -28,7 +28,6 @@\n \n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -45,6 +44,7 @@\n     auto_docstring,\n     requires_backends,\n )\n+from .image_processing_prompt_depth_anything import PromptDepthAnythingImageProcessorKwargs\n \n \n def _constrain_to_multiple_of(val, multiple, min_val=0, max_val=None):\n@@ -89,27 +89,6 @@ def _get_resize_output_image_size(\n     return (new_height, new_width)\n \n \n-class PromptDepthAnythingFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    keep_aspect_ratio (`bool`, *optional*):\n-        If `True`, the image is resized to the largest possible size such that the aspect ratio is preserved.\n-    ensure_multiple_of (`int`, *optional*):\n-        If `do_resize` is `True`, the image is resized to a size that is a multiple of this value.\n-    do_pad (`bool`, *optional*):\n-        Whether to apply center padding.\n-    size_divisor (`int`, *optional*):\n-        If `do_pad` is `True`, pads the image dimensions to be divisible by this value.\n-    prompt_scale_to_meter (`float`, *optional*):\n-        Scale factor to convert the prompt depth to meters.\n-    \"\"\"\n-\n-    keep_aspect_ratio: Optional[bool]\n-    ensure_multiple_of: Optional[int]\n-    do_pad: Optional[bool]\n-    size_divisor: Optional[int]\n-    prompt_scale_to_meter: Optional[float]\n-\n-\n @auto_docstring\n class PromptDepthAnythingImageProcessorFast(BaseImageProcessorFast):\n     model_input_names = [\"pixel_values\", \"prompt_depth\"]\n@@ -126,17 +105,17 @@ class PromptDepthAnythingImageProcessorFast(BaseImageProcessorFast):\n     do_pad = False\n     size_divisor = None\n     prompt_scale_to_meter = 0.001\n-    valid_kwargs = PromptDepthAnythingFastImageProcessorKwargs\n+    valid_kwargs = PromptDepthAnythingImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[PromptDepthAnythingFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[PromptDepthAnythingImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     @auto_docstring\n     def preprocess(\n         self,\n         images: ImageInput,\n         prompt_depth: Optional[ImageInput] = None,\n-        **kwargs: Unpack[PromptDepthAnythingFastImageProcessorKwargs],\n+        **kwargs: Unpack[PromptDepthAnythingImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n         prompt_depth (`ImageInput`, *optional*):\n@@ -213,7 +192,7 @@ def _preprocess_image_like_inputs(\n         device: Optional[Union[str, \"torch.device\"]] = None,\n         prompt_scale_to_meter: Optional[float] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n-        **kwargs: Unpack[PromptDepthAnythingFastImageProcessorKwargs],\n+        **kwargs: Unpack[PromptDepthAnythingImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess image-like inputs, including the main images and optional prompt depth."
        },
        {
            "sha": "95f687e1414ab9af248075d7378eb87ac7132924",
            "filename": "src/transformers/models/qwen2_5_omni/processing_qwen2_5_omni.py",
            "status": "modified",
            "additions": 9,
            "deletions": 14,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -25,34 +25,29 @@\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n-from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack, VideosKwargs\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack, VideosKwargs\n from ...tokenization_utils_base import AudioInput, PreTokenizedInput, TextInput\n from ...video_utils import VideoInput\n \n \n+# Redefine kwargs for videos because Qwen-Omni uses some kwargs for processing omni\n+# and does not use them in video processor class\n class Qwen2_5_OmniVideosKwargs(VideosKwargs):\n-    fps: Optional[list[Union[int, float]]]\n-    use_audio_in_video: Optional[bool]\n-    seconds_per_chunk: Optional[float]\n-    position_id_per_seconds: Optional[int]\n-    min_pixels: Optional[int]\n-    max_pixels: Optional[int]\n-    patch_size: Optional[int]\n-    temporal_patch_size: Optional[int]\n-    merge_size: Optional[int]\n-\n-\n-class Qwen2_5_OmniImagesKwargs(ImagesKwargs):\n     min_pixels: Optional[int]\n     max_pixels: Optional[int]\n     patch_size: Optional[int]\n     temporal_patch_size: Optional[int]\n     merge_size: Optional[int]\n+    min_frames: Optional[int]\n+    max_frames: Optional[int]\n+    use_audio_in_video: Optional[bool]\n+    seconds_per_chunk: Optional[float]\n+    position_id_per_seconds: Optional[int]\n \n \n class Qwen2_5OmniProcessorKwargs(ProcessingKwargs, total=False):\n     videos_kwargs: Qwen2_5_OmniVideosKwargs\n-    images_kwargs: Qwen2_5_OmniImagesKwargs\n+\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,"
        },
        {
            "sha": "3d3f325e86f699bc4b8f97e23f08960994de5212",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -40,7 +40,7 @@\n     VisionAttention,\n     VisionRotaryEmbedding,\n )\n-from transformers.models.qwen2_vl.processing_qwen2_vl import Qwen2VLImagesKwargs, Qwen2VLProcessor\n+from transformers.models.qwen2_vl.processing_qwen2_vl import Qwen2VLProcessor\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache\n@@ -49,7 +49,7 @@\n from ...image_utils import ImageInput\n from ...modeling_flash_attention_utils import is_flash_attn_available\n from ...modeling_layers import GradientCheckpointingLayer\n-from ...processing_utils import MultiModalData, ProcessingKwargs, Unpack, VideosKwargs\n+from ...processing_utils import MultiModalData, ProcessingKwargs, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import is_torchdynamo_compiling, logging\n from ...video_utils import VideoInput\n@@ -839,17 +839,7 @@ def prepare_inputs_for_generation(\n         return model_inputs\n \n \n-class Qwen2_5_VLVideosProcessorKwargs(VideosKwargs, total=False):\n-    fps: Union[list[float], float]\n-\n-\n-class Qwen2_5_VLImagesKwargs(Qwen2VLImagesKwargs):\n-    pass\n-\n-\n class Qwen2_5_VLProcessorKwargs(ProcessingKwargs, total=False):\n-    images_kwargs: Qwen2_5_VLImagesKwargs\n-    videos_kwargs: Qwen2_5_VLVideosProcessorKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,"
        },
        {
            "sha": "8d249fb2d51c0aef504f2dbf73d063527821ea39",
            "filename": "src/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 15,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -30,26 +30,12 @@\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n-from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack, VideosKwargs\n+from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...video_utils import VideoInput\n \n \n-class Qwen2_5_VLVideosProcessorKwargs(VideosKwargs, total=False):\n-    fps: Union[list[float], float]\n-\n-\n-class Qwen2_5_VLImagesKwargs(ImagesKwargs):\n-    min_pixels: Optional[int]\n-    max_pixels: Optional[int]\n-    patch_size: Optional[int]\n-    temporal_patch_size: Optional[int]\n-    merge_size: Optional[int]\n-\n-\n class Qwen2_5_VLProcessorKwargs(ProcessingKwargs, total=False):\n-    images_kwargs: Qwen2_5_VLImagesKwargs\n-    videos_kwargs: Qwen2_5_VLVideosProcessorKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,"
        },
        {
            "sha": "fe218bd05b9d3e9c86601c2323556ca1ced29cde",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 23,
            "deletions": 0,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -44,13 +44,35 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, logging\n from ...video_utils import VideoInput, make_batched_videos\n \n \n logger = logging.get_logger(__name__)\n \n \n+class Qwen2VLImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    min_pixels (`int`, *optional*, defaults to `56 * 56`):\n+        The min pixels of the image to resize the image.\n+    max_pixels (`int`, *optional*, defaults to `28 * 28 * 1280`):\n+        The max pixels of the image to resize the image.\n+    patch_size (`int`, *optional*, defaults to 14):\n+        The spatial patch size of the vision encoder.\n+    temporal_patch_size (`int`, *optional*, defaults to 2):\n+        The temporal patch size of the vision encoder.\n+    merge_size (`int`, *optional*, defaults to 2):\n+        The merge size of the vision encoder to llm encoder.\n+    \"\"\"\n+\n+    min_pixels: Optional[int]\n+    max_pixels: Optional[int]\n+    patch_size: Optional[int]\n+    temporal_patch_size: Optional[int]\n+    merge_size: Optional[int]\n+\n+\n def smart_resize(\n     height: int, width: int, factor: int = 28, min_pixels: int = 56 * 56, max_pixels: int = 14 * 14 * 4 * 1280\n ):\n@@ -116,6 +138,7 @@ class Qwen2VLImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"image_grid_thw\", \"pixel_values_videos\", \"video_grid_thw\"]\n+    valid_kwargs = Qwen2VLImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "bcbcaa6a8ad41b2e33aea6143048e65590881072",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 27,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -27,7 +27,6 @@\n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -46,33 +45,12 @@\n     logging,\n )\n from ...video_utils import VideoInput, make_batched_videos\n-from .image_processing_qwen2_vl import smart_resize\n+from .image_processing_qwen2_vl import Qwen2VLImageProcessorKwargs, smart_resize\n \n \n logger = logging.get_logger(__name__)\n \n \n-class Qwen2VLFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    min_pixels (`int`, *optional*, defaults to `56 * 56`):\n-        The min pixels of the image to resize the image.\n-    max_pixels (`int`, *optional*, defaults to `28 * 28 * 1280`):\n-        The max pixels of the image to resize the image.\n-    patch_size (`int`, *optional*, defaults to 14):\n-        The spatial patch size of the vision encoder.\n-    temporal_patch_size (`int`, *optional*, defaults to 2):\n-        The temporal patch size of the vision encoder.\n-    merge_size (`int`, *optional*, defaults to 2):\n-        The merge size of the vision encoder to llm encoder.\n-    \"\"\"\n-\n-    min_pixels: Optional[int]\n-    max_pixels: Optional[int]\n-    patch_size: Optional[int]\n-    temporal_patch_size: Optional[int]\n-    merge_size: Optional[int]\n-\n-\n @auto_docstring\n class Qwen2VLImageProcessorFast(BaseImageProcessorFast):\n     do_resize = True\n@@ -88,10 +66,10 @@ class Qwen2VLImageProcessorFast(BaseImageProcessorFast):\n     merge_size = 2\n     min_pixels = None\n     max_pixels = None\n-    valid_kwargs = Qwen2VLFastImageProcessorKwargs\n+    valid_kwargs = Qwen2VLImageProcessorKwargs\n     model_input_names = [\"pixel_values\", \"image_grid_thw\", \"pixel_values_videos\", \"video_grid_thw\"]\n \n-    def __init__(self, **kwargs: Unpack[Qwen2VLFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[Qwen2VLImageProcessorKwargs]):\n         size = kwargs.pop(\"size\", None)\n         min_pixels = kwargs.pop(\"min_pixels\", None)\n         max_pixels = kwargs.pop(\"max_pixels\", None)\n@@ -136,7 +114,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         videos: Optional[VideoInput] = None,\n-        **kwargs: Unpack[Qwen2VLFastImageProcessorKwargs],\n+        **kwargs: Unpack[Qwen2VLImageProcessorKwargs],\n     ) -> BatchFeature:\n         return super().preprocess(images, videos, **kwargs)\n \n@@ -147,7 +125,7 @@ def _preprocess_image_like_inputs(\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n-        **kwargs: Unpack[DefaultFastImageProcessorKwargs],\n+        **kwargs: Unpack[Qwen2VLImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess image-like inputs."
        },
        {
            "sha": "f630d039edbd2ee9cc3f3b579fb204b35f06ad6e",
            "filename": "src/transformers/models/qwen2_vl/processing_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 10,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -27,7 +27,7 @@\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n-from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import logging\n from ...video_utils import VideoInput\n@@ -36,16 +36,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class Qwen2VLImagesKwargs(ImagesKwargs):\n-    min_pixels: Optional[int]\n-    max_pixels: Optional[int]\n-    patch_size: Optional[int]\n-    temporal_patch_size: Optional[int]\n-    merge_size: Optional[int]\n-\n-\n class Qwen2VLProcessorKwargs(ProcessingKwargs, total=False):\n-    images_kwargs: Qwen2VLImagesKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,"
        },
        {
            "sha": "a118f7d2260bcb9853f5218da025caf9949110ca",
            "filename": "src/transformers/models/qwen3_omni_moe/processing_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 15,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -20,41 +20,35 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import re\n-from typing import Optional, Union\n+from typing import Optional\n \n import numpy as np\n \n from ...audio_utils import AudioInput\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n-from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, VideosKwargs\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, VideosKwargs\n from ...tokenization_utils_base import TextInput\n from ...video_utils import VideoInput, make_batched_videos\n \n \n+# Redefine kwargs for videos because Qwen-Omni uses some kwargs for processing omni\n+# and does not use them in video processor class\n class Qwen3OmniMoeVideosKwargs(VideosKwargs):\n-    fps: Optional[list[Union[int, float]]]\n-    use_audio_in_video: Optional[bool]\n-    seconds_per_chunk: Optional[float]\n-    position_id_per_seconds: Optional[int]\n-    min_pixels: Optional[int]\n-    max_pixels: Optional[int]\n-    patch_size: Optional[int]\n-    temporal_patch_size: Optional[int]\n-    merge_size: Optional[int]\n-\n-\n-class Qwen3OmniMoeImagesKwargs(ImagesKwargs):\n     min_pixels: Optional[int]\n     max_pixels: Optional[int]\n     patch_size: Optional[int]\n     temporal_patch_size: Optional[int]\n     merge_size: Optional[int]\n+    min_frames: Optional[int]\n+    max_frames: Optional[int]\n+    use_audio_in_video: Optional[bool]\n+    seconds_per_chunk: Optional[float]\n+    position_id_per_seconds: Optional[int]\n \n \n class Qwen3OmniMoeProcessorKwargs(ProcessingKwargs, total=False):\n     videos_kwargs: Qwen3OmniMoeVideosKwargs\n-    images_kwargs: Qwen3OmniMoeImagesKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,"
        },
        {
            "sha": "ff5346fb94b60c810888cb4472803cf92fc1dcc2",
            "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -31,7 +31,7 @@\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update, rope_config_validation\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n-from ...processing_utils import ProcessingKwargs, Unpack, VideosKwargs\n+from ...processing_utils import ProcessingKwargs, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import auto_docstring, is_torchdynamo_compiling, logging\n from ...utils.generic import check_model_inputs\n@@ -50,7 +50,7 @@\n     VisionAttention,\n     VisionRotaryEmbedding,\n )\n-from ..qwen2_vl.processing_qwen2_vl import Qwen2VLImagesKwargs, Qwen2VLProcessor\n+from ..qwen2_vl.processing_qwen2_vl import Qwen2VLProcessor\n from ..qwen3.modeling_qwen3 import (\n     Qwen3Attention,\n     Qwen3DecoderLayer,\n@@ -1252,17 +1252,7 @@ def prepare_inputs_for_generation(\n         return model_inputs\n \n \n-class Qwen3VLVideosProcessorKwargs(VideosKwargs, total=False):\n-    pass\n-\n-\n-class Qwen3VLImagesKwargs(Qwen2VLImagesKwargs):\n-    pass\n-\n-\n class Qwen3VLProcessorKwargs(ProcessingKwargs, total=False):\n-    images_kwargs: Qwen3VLImagesKwargs\n-    videos_kwargs: Qwen3VLVideosProcessorKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,"
        },
        {
            "sha": "49199f0c3ecc5edd5f57cd1dcfa384bfa55dd04c",
            "filename": "src/transformers/models/qwen3_vl/processing_qwen3_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 16,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fprocessing_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fprocessing_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fprocessing_qwen3_vl.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -19,13 +19,13 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Optional, Union\n+from typing import Union\n \n import numpy as np\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n-from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack, VideosKwargs\n+from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import logging\n from ...video_utils import VideoInput\n@@ -34,21 +34,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class Qwen3VLVideosProcessorKwargs(VideosKwargs, total=False):\n-    pass\n-\n-\n-class Qwen3VLImagesKwargs(ImagesKwargs):\n-    min_pixels: Optional[int]\n-    max_pixels: Optional[int]\n-    patch_size: Optional[int]\n-    temporal_patch_size: Optional[int]\n-    merge_size: Optional[int]\n-\n-\n class Qwen3VLProcessorKwargs(ProcessingKwargs, total=False):\n-    images_kwargs: Qwen3VLImagesKwargs\n-    videos_kwargs: Qwen3VLVideosProcessorKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,"
        },
        {
            "sha": "ec65d0e7731d6509f4232812cdd6dcfc6e4ab926",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr.py",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -48,6 +48,7 @@\n     validate_annotations,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import (\n     filter_out_non_signature_kwargs,\n     is_torch_available,\n@@ -66,6 +67,29 @@\n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION,)\n \n \n+class RTDetrImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n+        Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n+    do_convert_annotations (`bool`, *optional*, defaults to `True`):\n+        Controls whether to convert the annotations to the format expected by the RT_DETR model. Converts the\n+        bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n+        Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n+    return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n+        Whether to return segmentation masks.\n+    annotations (`AnnotationType` or `list[AnnotationType]`, *optional*):\n+        Annotations to transform according to the padding that is applied to the images.\n+    masks_path (`str` or `pathlib.Path`, *optional*):\n+        Path to the directory containing the segmentation masks.\n+    \"\"\"\n+\n+    format: Optional[Union[str, AnnotationFormat]]\n+    do_convert_annotations: Optional[bool]\n+    return_segmentation_masks: Optional[bool]\n+    annotations: Optional[Union[AnnotationType, list[AnnotationType]]]\n+    masks_path: Optional[Union[str, pathlib.Path]]\n+\n+\n # Copied from transformers.models.detr.image_processing_detr.get_size_with_aspect_ratio\n def get_size_with_aspect_ratio(image_size, size, max_size=None) -> tuple[int, int]:\n     \"\"\"\n@@ -406,6 +430,7 @@ class RTDetrImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n+    valid_kwargs = RTDetrImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "44946eeed9e3c541f291e7bf443d35865e70c05b",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 40,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -13,7 +13,6 @@\n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     SizeDict,\n     get_image_size_for_max_height_width,\n     get_max_height_width,\n@@ -34,24 +33,7 @@\n from ...processing_utils import Unpack\n from ...utils import TensorType, auto_docstring, requires_backends\n from ...utils.import_utils import requires\n-from .image_processing_rt_detr import get_size_with_aspect_ratio\n-\n-\n-class RTDetrFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    r\"\"\"\n-    format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n-        Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n-    do_convert_annotations (`bool`, *optional*, defaults to `True`):\n-        Controls whether to convert the annotations to the format expected by the RT_DETR model. Converts the\n-        bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n-        Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n-    return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n-        Whether to return segmentation masks.\n-    \"\"\"\n-\n-    format: Optional[Union[str, AnnotationFormat]]\n-    do_convert_annotations: Optional[bool]\n-    return_segmentation_masks: Optional[bool]\n+from .image_processing_rt_detr import RTDetrImageProcessorKwargs, get_size_with_aspect_ratio\n \n \n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n@@ -130,10 +112,10 @@ class RTDetrImageProcessorFast(BaseImageProcessorFast):\n     size = {\"height\": 640, \"width\": 640}\n     default_to_square = False\n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n-    valid_kwargs = RTDetrFastImageProcessorKwargs\n+    valid_kwargs = RTDetrImageProcessorKwargs\n     do_convert_annotations = True\n \n-    def __init__(self, **kwargs: Unpack[RTDetrFastImageProcessorKwargs]) -> None:\n+    def __init__(self, **kwargs: Unpack[RTDetrImageProcessorKwargs]) -> None:\n         # Backwards compatibility\n         do_convert_annotations = kwargs.get(\"do_convert_annotations\")\n         do_normalize = kwargs.get(\"do_normalize\")\n@@ -356,26 +338,9 @@ def pad(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        annotations: Optional[Union[AnnotationType, list[AnnotationType]]] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        **kwargs: Unpack[RTDetrFastImageProcessorKwargs],\n+        **kwargs: Unpack[RTDetrImageProcessorKwargs],\n     ) -> BatchFeature:\n-        r\"\"\"\n-        annotations (`AnnotationType` or `list[AnnotationType]`, *optional*):\n-            List of annotations associated with the image or batch of images. If annotation is for object\n-            detection, the annotations should be a dictionary with the following keys:\n-            - \"image_id\" (`int`): The image id.\n-            - \"annotations\" (`list[Dict]`): List of annotations for an image. Each annotation should be a\n-                dictionary. An image can have no annotations, in which case the list should be empty.\n-            If annotation is for segmentation, the annotations should be a dictionary with the following keys:\n-            - \"image_id\" (`int`): The image id.\n-            - \"segments_info\" (`list[Dict]`): List of segments for an image. Each segment should be a dictionary.\n-                An image can have no segments, in which case the list should be empty.\n-            - \"file_name\" (`str`): The file name of the image.\n-        masks_path (`str` or `pathlib.Path`, *optional*):\n-            Path to the directory containing the segmentation masks.\n-        \"\"\"\n-        return super().preprocess(images, annotations, masks_path, **kwargs)\n+        return super().preprocess(images, **kwargs)\n \n     def _preprocess(\n         self,"
        },
        {
            "sha": "0f72fdd528453a21d44ee23e1a8a8d2d5064b448",
            "filename": "src/transformers/models/rt_detr/modular_rt_detr.py",
            "status": "modified",
            "additions": 6,
            "deletions": 11,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -4,7 +4,7 @@\n import torch\n from torchvision.transforms.v2 import functional as F\n \n-from transformers.models.detr.image_processing_detr_fast import DetrFastImageProcessorKwargs, DetrImageProcessorFast\n+from transformers.models.detr.image_processing_detr_fast import DetrImageProcessorFast\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import BaseImageProcessorFast, SizeDict, get_max_height_width\n@@ -26,6 +26,7 @@\n     logging,\n     requires_backends,\n )\n+from .image_processing_rt_detr import RTDetrImageProcessorKwargs\n \n \n logger = logging.get_logger(__name__)\n@@ -92,10 +93,6 @@ def prepare_coco_detection_annotation(\n     return new_target\n \n \n-class RTDetrFastImageProcessorKwargs(DetrFastImageProcessorKwargs):\n-    pass\n-\n-\n class RTDetrImageProcessorFast(DetrImageProcessorFast):\n     resample = PILImageResampling.BILINEAR\n     image_mean = IMAGENET_DEFAULT_MEAN\n@@ -109,9 +106,9 @@ class RTDetrImageProcessorFast(DetrImageProcessorFast):\n     size = {\"height\": 640, \"width\": 640}\n     default_to_square = False\n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n-    valid_kwargs = RTDetrFastImageProcessorKwargs\n+    valid_kwargs = RTDetrImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[RTDetrFastImageProcessorKwargs]) -> None:\n+    def __init__(self, **kwargs: Unpack[RTDetrImageProcessorKwargs]) -> None:\n         # Backwards compatibility\n         do_convert_annotations = kwargs.get(\"do_convert_annotations\")\n         do_normalize = kwargs.get(\"do_normalize\")\n@@ -123,11 +120,9 @@ def __init__(self, **kwargs: Unpack[RTDetrFastImageProcessorKwargs]) -> None:\n     def preprocess(\n         self,\n         images: ImageInput,\n-        annotations: Optional[Union[AnnotationType, list[AnnotationType]]] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        **kwargs: Unpack[RTDetrFastImageProcessorKwargs],\n+        **kwargs: Unpack[RTDetrImageProcessorKwargs],\n     ) -> BatchFeature:\n-        return BaseImageProcessorFast.preprocess(self, images, annotations, masks_path, **kwargs)\n+        return BaseImageProcessorFast.preprocess(self, images, **kwargs)\n \n     def prepare_annotation(\n         self,"
        },
        {
            "sha": "e9da260a6e9c994deaa0bed3232671f8f08c4524",
            "filename": "src/transformers/models/sam/image_processing_sam.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -37,6 +37,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import (\n     TensorType,\n     filter_out_non_signature_kwargs,\n@@ -57,6 +58,19 @@\n logger = logging.get_logger(__name__)\n \n \n+class SamImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    mask_size (`dict[str, int]`, *optional*):\n+        The size `{\"longest_edge\": int}` to resize the segmentation maps to.\n+    mask_pad_size (`dict[str, int]`, *optional*):\n+        The size `{\"height\": int, \"width\": int}` to pad the segmentation maps to. Must be larger than any segmentation\n+        map size provided for preprocessing.\n+    \"\"\"\n+\n+    mask_size: Optional[dict[str, int]]\n+    mask_pad_size: Optional[dict[str, int]]\n+\n+\n class SamImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a SAM image processor.\n@@ -107,6 +121,7 @@ class SamImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = SamImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "e77b69ee1e2b75a7b514c658885479c19be604ab",
            "filename": "src/transformers/models/sam/image_processing_sam_fast.py",
            "status": "modified",
            "additions": 6,
            "deletions": 21,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -26,10 +26,7 @@\n from torchvision.transforms.v2 import functional as F_t\n \n from ...image_processing_utils import BatchFeature, get_size_dict\n-from ...image_processing_utils_fast import (\n-    BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n-)\n+from ...image_processing_utils_fast import BaseImageProcessorFast\n from ...image_utils import (\n     IMAGENET_DEFAULT_MEAN,\n     IMAGENET_DEFAULT_STD,\n@@ -41,19 +38,7 @@\n )\n from ...processing_utils import Unpack\n from ...utils import auto_docstring\n-\n-\n-class SamFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    r\"\"\"\n-    mask_size (`dict[str, int]`, *optional*):\n-        The size `{\"longest_edge\": int}` to resize the segmentation maps to.\n-    mask_pad_size (`dict[str, int]`, *optional*):\n-        The size `{\"height\": int, \"width\": int}` to pad the segmentation maps to. Must be larger than any segmentation\n-        map size provided for preprocessing.\n-    \"\"\"\n-\n-    mask_size: Optional[dict[str, int]]\n-    mask_pad_size: Optional[dict[str, int]]\n+from .image_processing_sam import SamImageProcessorKwargs\n \n \n @auto_docstring\n@@ -68,13 +53,13 @@ class SamImageProcessorFast(BaseImageProcessorFast):\n     do_normalize = True\n     do_convert_rgb = True\n \n-    valid_kwargs = SamFastImageProcessorKwargs\n+    valid_kwargs = SamImageProcessorKwargs\n \n     do_pad = True\n     pad_size = {\"height\": 1024, \"width\": 1024}\n     mask_pad_size = {\"height\": 256, \"width\": 256}\n \n-    def __init__(self, **kwargs: Unpack[SamFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[SamImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     def _get_preprocess_shape(self, old_shape: tuple[int, int], longest_edge: int):\n@@ -172,7 +157,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         segmentation_maps: Optional[ImageInput] = None,\n-        **kwargs: Unpack[SamFastImageProcessorKwargs],\n+        **kwargs: Unpack[SamImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n         segmentation_maps (`ImageInput`, *optional*):\n@@ -187,7 +172,7 @@ def _preprocess_image_like_inputs(\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n-        **kwargs: Unpack[SamFastImageProcessorKwargs],\n+        **kwargs: Unpack[SamImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess image-like inputs."
        },
        {
            "sha": "bc82daf2034dad2bbdb9cd51f0093a19c0cd0e06",
            "filename": "src/transformers/models/sam/processing_sam.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -23,9 +23,8 @@\n \n from ...image_utils import ImageInput\n from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin\n-from ...tokenization_utils_base import AudioInput, BatchEncoding, PreTokenizedInput, TextInput\n+from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n from ...utils import is_torch_available\n-from ...video_utils import VideoInput\n \n \n if is_torch_available():\n@@ -38,6 +37,8 @@ class SamImagesKwargs(ImagesKwargs):\n     input_labels: Optional[list[list[int]]]\n     input_boxes: Optional[list[list[list[float]]]]\n     point_pad_value: Optional[int]\n+    mask_size: Optional[dict[str, int]]\n+    mask_pad_size: Optional[dict[str, int]]\n \n \n class SamProcessorKwargs(ProcessingKwargs, total=False):\n@@ -73,8 +74,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n-        audio: Optional[AudioInput] = None,\n-        video: Optional[VideoInput] = None,\n         **kwargs,\n     ) -> BatchEncoding:\n         \"\"\"\n@@ -114,7 +113,7 @@ def __call__(\n             input_points=input_points,\n             input_labels=input_labels,\n             input_boxes=input_boxes,\n-            return_tensors=output_kwargs[\"common_kwargs\"].get(\"return_tensors\"),\n+            return_tensors=output_kwargs[\"images_kwargs\"].get(\"return_tensors\"),\n             point_pad_value=point_pad_value,\n         )\n "
        },
        {
            "sha": "5ae472f536388fe629d08784d1fc091edf9f0041",
            "filename": "src/transformers/models/sam2/image_processing_sam2_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsam2%2Fimage_processing_sam2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsam2%2Fimage_processing_sam2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fimage_processing_sam2_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -29,7 +29,7 @@\n from torchvision.ops.boxes import batched_nms\n \n from ...image_processing_utils import BatchFeature, get_size_dict\n-from ...image_processing_utils_fast import BaseImageProcessorFast, DefaultFastImageProcessorKwargs\n+from ...image_processing_utils_fast import BaseImageProcessorFast\n from ...image_utils import (\n     IMAGENET_DEFAULT_MEAN,\n     IMAGENET_DEFAULT_STD,\n@@ -39,11 +39,11 @@\n     SizeDict,\n     pil_torch_interpolation_mapping,\n )\n-from ...processing_utils import Unpack\n+from ...processing_utils import ImagesKwargs, Unpack\n from ...utils import TensorType, auto_docstring\n \n \n-class Sam2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+class Sam2FastImageProcessorKwargs(ImagesKwargs):\n     r\"\"\"\n     mask_size (`dict[str, int]`, *optional*):\n         The size `{\"height\": int, \"width\": int}` to resize the segmentation maps to."
        },
        {
            "sha": "40414566267f447a31902a3f9ffb4fd99152b93c",
            "filename": "src/transformers/models/sam2/modular_sam2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 3,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -24,7 +24,7 @@\n \n from ...activations import ACT2FN\n from ...image_processing_utils import BatchFeature, get_size_dict\n-from ...image_processing_utils_fast import BaseImageProcessorFast, DefaultFastImageProcessorKwargs\n+from ...image_processing_utils_fast import BaseImageProcessorFast\n from ...image_utils import (\n     IMAGENET_DEFAULT_MEAN,\n     IMAGENET_DEFAULT_STD,\n@@ -36,7 +36,7 @@\n )\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...processing_utils import Unpack\n+from ...processing_utils import ImagesKwargs, Unpack\n from ...utils import (\n     ModelOutput,\n     TensorType,\n@@ -70,7 +70,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class Sam2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+class Sam2FastImageProcessorKwargs(ImagesKwargs):\n     r\"\"\"\n     mask_size (`dict[str, int]`, *optional*):\n         The size `{\"height\": int, \"width\": int}` to resize the segmentation maps to.\n@@ -118,6 +118,19 @@ def _preprocess(\n     ) -> \"torch.Tensor\":\n         return BaseImageProcessorFast._preprocess(self, images, return_tensors=return_tensors, **kwargs).pixel_values\n \n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput] = None,\n+        **kwargs: Unpack[Sam2FastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The segmentation maps to preprocess.\n+        \"\"\"\n+        return super().preprocess(images, segmentation_maps, **kwargs)\n+\n     def _preprocess_image_like_inputs(\n         self,\n         images: ImageInput,"
        },
        {
            "sha": "902e68832836f74c640ba9d561020273d2fbe160",
            "filename": "src/transformers/models/sam_hq/processing_samhq.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_samhq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_samhq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_samhq.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -23,9 +23,8 @@\n \n from ...image_utils import ImageInput\n from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n-from ...tokenization_utils_base import AudioInput, BatchEncoding, PreTokenizedInput, TextInput\n+from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n from ...utils import is_torch_available\n-from ...video_utils import VideoInput\n \n \n if is_torch_available():\n@@ -38,6 +37,8 @@ class SamHQImagesKwargs(ImagesKwargs):\n     input_labels: Optional[list[list[int]]]\n     input_boxes: Optional[list[list[list[float]]]]\n     point_pad_value: Optional[int]\n+    mask_size: Optional[dict[str, int]]\n+    mask_pad_size: Optional[dict[str, int]]\n \n \n class SamHQProcessorKwargs(ProcessingKwargs, total=False):\n@@ -78,8 +79,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n-        audio: Optional[AudioInput] = None,\n-        video: Optional[VideoInput] = None,\n         **kwargs: Unpack[SamHQProcessorKwargs],\n     ) -> BatchEncoding:\n         \"\"\"\n@@ -118,7 +117,7 @@ def __call__(\n             input_points=input_points,\n             input_labels=input_labels,\n             input_boxes=input_boxes,\n-            return_tensors=output_kwargs[\"common_kwargs\"].get(\"return_tensors\"),\n+            return_tensors=output_kwargs[\"images_kwargs\"].get(\"return_tensors\"),\n             point_pad_value=output_kwargs[\"images_kwargs\"].get(\"point_pad_value\"),\n         )\n "
        },
        {
            "sha": "ce9ace8115a4a172714ab19a204159cd80ec8a25",
            "filename": "src/transformers/models/segformer/image_processing_segformer.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -33,6 +33,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import (\n     TensorType,\n     filter_out_non_signature_kwargs,\n@@ -54,6 +55,17 @@\n logger = logging.get_logger(__name__)\n \n \n+class SegformerImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n+        Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n+        is used for background, and background itself is not included in all classes of a dataset (e.g.\n+        ADE20k). The background label will be replaced by 255.\n+    \"\"\"\n+\n+    do_reduce_labels: Optional[bool]\n+\n+\n @requires(backends=(\"vision\",))\n class SegformerImageProcessor(BaseImageProcessor):\n     r\"\"\"\n@@ -92,6 +104,7 @@ class SegformerImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = SegformerImageProcessorKwargs\n \n     @filter_out_non_signature_kwargs(extra=INIT_SERVICE_KWARGS)\n     def __init__("
        },
        {
            "sha": "d3dc35e609de12a4113b1b378176bf25223fa2e2",
            "filename": "src/transformers/models/segformer/image_processing_segformer_fast.py",
            "status": "modified",
            "additions": 6,
            "deletions": 21,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -25,12 +25,7 @@\n from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n-from ...image_processing_utils_fast import (\n-    BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n-    group_images_by_shape,\n-    reorder_images,\n-)\n+from ...image_processing_utils_fast import BaseImageProcessorFast, group_images_by_shape, reorder_images\n from ...image_utils import (\n     IMAGENET_DEFAULT_MEAN,\n     IMAGENET_DEFAULT_STD,\n@@ -42,17 +37,7 @@\n )\n from ...processing_utils import Unpack\n from ...utils import TensorType, auto_docstring\n-\n-\n-class SegformerFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    r\"\"\"\n-    do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n-        Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n-        is used for background, and background itself is not included in all classes of a dataset (e.g.\n-        ADE20k). The background label will be replaced by 255.\n-    \"\"\"\n-\n-    do_reduce_labels: Optional[bool]\n+from .image_processing_segformer import SegformerImageProcessorKwargs\n \n \n @auto_docstring\n@@ -68,10 +53,10 @@ class SegformerImageProcessorFast(BaseImageProcessorFast):\n     do_rescale = True\n     do_normalize = True\n     do_reduce_labels = False\n-    valid_kwargs = SegformerFastImageProcessorKwargs\n+    valid_kwargs = SegformerImageProcessorKwargs\n     rescale_factor = 1 / 255\n \n-    def __init__(self, **kwargs: Unpack[SegformerFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[SegformerImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     def reduce_label(self, labels: list[\"torch.Tensor\"]):\n@@ -89,7 +74,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         segmentation_maps: Optional[ImageInput] = None,\n-        **kwargs: Unpack[SegformerFastImageProcessorKwargs],\n+        **kwargs: Unpack[SegformerImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n         segmentation_maps (`ImageInput`, *optional*):\n@@ -104,7 +89,7 @@ def _preprocess_image_like_inputs(\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n-        **kwargs: Unpack[SegformerFastImageProcessorKwargs],\n+        **kwargs: Unpack[SegformerImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess image-like inputs."
        },
        {
            "sha": "6bbbe9ecd4fde54f4120ed67e90c227830e0c012",
            "filename": "src/transformers/models/segformer/modular_segformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodular_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodular_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodular_segformer.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -19,7 +19,7 @@\n import torch\n from torchvision.transforms.v2 import functional as F\n \n-from transformers.models.beit.image_processing_beit_fast import BeitFastImageProcessorKwargs, BeitImageProcessorFast\n+from transformers.models.beit.image_processing_beit_fast import BeitImageProcessorFast\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n@@ -38,10 +38,7 @@\n from ...utils import (\n     TensorType,\n )\n-\n-\n-class SegformerFastImageProcessorKwargs(BeitFastImageProcessorKwargs):\n-    pass\n+from .image_processing_segformer import SegformerImageProcessorKwargs\n \n \n class SegformerImageProcessorFast(BeitImageProcessorFast):\n@@ -64,7 +61,7 @@ def _preprocess_image_like_inputs(\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n-        **kwargs: Unpack[SegformerFastImageProcessorKwargs],\n+        **kwargs: Unpack[SegformerImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess image-like inputs."
        },
        {
            "sha": "8c221e826167651d83f64d9299a65b6c8889dfc3",
            "filename": "src/transformers/models/shieldgemma2/processing_shieldgemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fprocessing_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fprocessing_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fprocessing_shieldgemma2.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -87,8 +87,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text=None,\n-        videos=None,\n-        audio=None,\n         **kwargs: Unpack[ShieldGemma2ProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"Generates a batch of inputs from the provided images.\n@@ -120,8 +118,6 @@ def __call__(\n             `(len(images) * len(policies), )`, and the order within the batch will be\n             img1_policy1, ... img1_policyN, ... imgM_policyN.\n         \"\"\"\n-        del text, videos, audio\n-\n         if not images:\n             raise ValueError(\"ShieldGemma 2 needs images to classify\")\n         elif not isinstance(images, Sequence):"
        },
        {
            "sha": "caff1bce0bc95ce6ba621b1489cce0f8d9b79f4b",
            "filename": "src/transformers/models/siglip2/image_processing_siglip2.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -37,6 +37,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n \n \n@@ -47,6 +48,19 @@\n     from PIL import Image\n \n \n+class Siglip2ImageProcessorKwargs(ImagesKwargs):\n+    \"\"\"\n+    patch_size (`int`, *optional*, defaults to 16):\n+        The size (resolution) of each patch the image will be split to.\n+    max_num_patches (`int`, *optional*, defaults to 256):\n+        The image will be resized to have at most this number of patches,\n+        and then padded in \"patch\" dimension to match this number exactly.\n+    \"\"\"\n+\n+    patch_size: Optional[int]\n+    max_num_patches: Optional[int]\n+\n+\n @lru_cache(maxsize=256)\n def get_image_size_for_max_num_patches(\n     image_height: int, image_width: int, patch_size: int, max_num_patches: int, eps: float = 1e-5\n@@ -159,6 +173,7 @@ class Siglip2ImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"pixel_attention_mask\", \"spatial_shapes\"]\n+    valid_kwargs = Siglip2ImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "347ec372d410a4124a4bbc514fdb1c77ff8a3386",
            "filename": "src/transformers/models/siglip2/image_processing_siglip2_fast.py",
            "status": "modified",
            "additions": 6,
            "deletions": 26,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -20,22 +20,15 @@\n from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n-from ...image_processing_utils_fast import (\n-    BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n-    SizeDict,\n-)\n-from ...image_utils import (\n-    ImageInput,\n-    PILImageResampling,\n-)\n+from ...image_processing_utils_fast import BaseImageProcessorFast, SizeDict\n+from ...image_utils import ImageInput, PILImageResampling\n from ...processing_utils import Unpack\n from ...utils import (\n     TensorType,\n     auto_docstring,\n     logging,\n )\n-from .image_processing_siglip2 import get_image_size_for_max_num_patches\n+from .image_processing_siglip2 import Siglip2ImageProcessorKwargs, get_image_size_for_max_num_patches\n \n \n logger = logging.get_logger(__name__)\n@@ -71,19 +64,6 @@ def pad_along_first_dim(\n     return tensor, mask\n \n \n-class Siglip2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    patch_size (`int`, *optional*, defaults to 16):\n-        The size (resolution) of each patch the image will be split to.\n-    max_num_patches (`int`, *optional*, defaults to 256):\n-        The image will be resized to have at most this number of patches,\n-        and then padded in \"patch\" dimension to match this number exactly.\n-    \"\"\"\n-\n-    patch_size: Optional[int]\n-    max_num_patches: Optional[int]\n-\n-\n @auto_docstring\n class Siglip2ImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BILINEAR\n@@ -94,10 +74,10 @@ class Siglip2ImageProcessorFast(BaseImageProcessorFast):\n     do_normalize = True\n     patch_size = 16\n     max_num_patches = 256\n-    valid_kwargs = Siglip2FastImageProcessorKwargs\n+    valid_kwargs = Siglip2ImageProcessorKwargs\n     unused_kwargs = [\"size\", \"do_center_crop\", \"crop_size\"]\n \n-    def __init__(self, **kwargs: Unpack[Siglip2FastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[Siglip2ImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     def _validate_preprocess_kwargs(self, **kwargs) -> tuple:\n@@ -106,7 +86,7 @@ def _validate_preprocess_kwargs(self, **kwargs) -> tuple:\n         return super()._validate_preprocess_kwargs(**kwargs)\n \n     @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[Siglip2FastImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[Siglip2ImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     def _preprocess("
        },
        {
            "sha": "b16650303da4c43e3b8a8e789038e60330d111d8",
            "filename": "src/transformers/models/siglip2/processing_siglip2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 10,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fprocessing_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fprocessing_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fprocessing_siglip2.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -16,19 +16,10 @@\n Image/Text processor class for SigLIP2.\n \"\"\"\n \n-from typing import Optional\n-\n-from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin\n-\n-\n-class Siglip2ImagesKwargs(ImagesKwargs, total=False):\n-    max_num_patches: Optional[int]\n-    patch_size: Optional[int]\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin\n \n \n class Siglip2ProcessorKwargs(ProcessingKwargs, total=False):\n-    images_kwargs: Siglip2ImagesKwargs\n-\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": \"max_length\","
        },
        {
            "sha": "e231c1ec6b070ed20dc4fb8ee506ab95fe9d14f3",
            "filename": "src/transformers/models/smolvlm/image_processing_smolvlm.py",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -41,6 +41,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, is_vision_available, logging\n \n \n@@ -50,6 +51,24 @@\n \n \n logger = logging.get_logger(__name__)\n+\n+\n+class SmolVLMImageProcessorKwargs(ImagesKwargs):\n+    \"\"\"\n+    do_image_splitting (`bool`, *optional*, defaults to `True`):\n+        Whether to split the image into sub-images concatenated with the original image. They are split into patches\n+        such that each patch has a size of `max_image_size[\"height\"]` x `max_image_size[\"width\"]`.\n+    max_image_size (`Dict`, *optional*, defaults to `{\"longest_edge\": 364}`):\n+        Maximum resolution of the patches of images accepted by the model. This is a dictionary containing the key \"longest_edge\".\n+    return_row_col_info (`bool`, *optional*, defaults to `False`):\n+        Whether to return the row and column information of the images.\n+    \"\"\"\n+\n+    do_image_splitting: Optional[bool]\n+    max_image_size: Optional[dict[str, int]]\n+    return_row_col_info: Optional[bool]\n+\n+\n MAX_IMAGE_SIZE = 4096  # 4k resolution as absolute maximum\n \n \n@@ -288,6 +307,7 @@ class SmolVLMImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"pixel_attention_mask\"]\n+    valid_kwargs = SmolVLMImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "5bff89823f32d6b1e8ffbcefc5ffc8b084d082b2",
            "filename": "src/transformers/models/smolvlm/image_processing_smolvlm_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 26,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -25,13 +25,7 @@\n import torch\n \n from ...image_processing_utils import BatchFeature\n-from ...image_processing_utils_fast import (\n-    BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n-    SizeDict,\n-    group_images_by_shape,\n-    reorder_images,\n-)\n+from ...image_processing_utils_fast import BaseImageProcessorFast, SizeDict, group_images_by_shape, reorder_images\n from ...image_utils import (\n     IMAGENET_STANDARD_MEAN,\n     IMAGENET_STANDARD_STD,\n@@ -41,6 +35,7 @@\n )\n from ...processing_utils import Unpack\n from ...utils import TensorType, auto_docstring, is_torchvision_available, logging\n+from .image_processing_smolvlm import SmolVLMImageProcessorKwargs\n \n \n if is_torchvision_available():\n@@ -49,23 +44,6 @@\n \n logger = logging.get_logger(__name__)\n \n-\n-class SmolVLMFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    do_image_splitting (`bool`, *optional*, defaults to `True`):\n-        Whether to split the image into sub-images concatenated with the original image. They are split into patches\n-        such that each patch has a size of `max_image_size[\"height\"]` x `max_image_size[\"width\"]`.\n-    max_image_size (`Dict`, *optional*, defaults to `{\"longest_edge\": 364}`):\n-        Maximum resolution of the patches of images accepted by the model. This is a dictionary containing the key \"longest_edge\".\n-    return_row_col_info (`bool`, *optional*, defaults to `False`):\n-        Whether to return the row and column information of the images.\n-    \"\"\"\n-\n-    do_image_splitting: Optional[bool]\n-    max_image_size: Optional[dict[str, int]]\n-    return_row_col_info: Optional[bool]\n-\n-\n MAX_IMAGE_SIZE = 4096  # 4k resolution as absolute maximum\n \n \n@@ -189,7 +167,7 @@ class SmolVLMImageProcessorFast(BaseImageProcessorFast):\n     do_image_splitting = True\n     do_pad = True\n     return_row_col_info = False\n-    valid_kwargs = SmolVLMFastImageProcessorKwargs\n+    valid_kwargs = SmolVLMImageProcessorKwargs\n \n     def _prepare_images_structure(self, images: ImageInput, expected_ndims: int = 3) -> ImageInput:\n         \"\"\"\n@@ -357,7 +335,7 @@ def pad(\n         return image, pixel_mask\n \n     @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[SmolVLMFastImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[SmolVLMImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     def _preprocess("
        },
        {
            "sha": "86d07e238f1bbbaca9f7020053ca094b85faf1c8",
            "filename": "src/transformers/models/smolvlm/processing_smolvlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -21,7 +21,7 @@\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, make_nested_list_of_images\n-from ...processing_utils import AllKwargsForChatTemplate, ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...processing_utils import AllKwargsForChatTemplate, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import BatchEncoding, TextInput\n from ...utils import is_num2words_available, is_vision_available, logging\n from ...video_utils import VideoInput\n@@ -103,14 +103,7 @@ def get_image_prompt_string(\n     )\n \n \n-class SmolVLMImagesKwargs(ImagesKwargs, total=False):\n-    return_row_col_info: Optional[bool]\n-    max_image_size: Optional[dict[str, int]]\n-\n-\n class SmolVLMProcessorKwargs(ProcessingKwargs, total=False):\n-    images_kwargs: SmolVLMImagesKwargs\n-\n     _defaults = {\n         \"text_kwargs\": {\n             \"add_special_tokens\": True,\n@@ -248,7 +241,6 @@ def __call__(\n         self,\n         images: Union[ImageInput, list[ImageInput], list[list[ImageInput]]] = None,\n         text: Union[TextInput, \"PreTokenizedInput\", list[TextInput], list[\"PreTokenizedInput\"]] = None,\n-        audio=None,\n         videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[SmolVLMProcessorKwargs],\n     ) -> BatchEncoding:"
        },
        {
            "sha": "ce73dfb4a82e844f1340cd667dc067455ccb20f4",
            "filename": "src/transformers/models/smolvlm/video_processing_smolvlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -92,7 +92,6 @@ def get_resize_output_image_size(\n \n class SmolVLMVideoProcessorInitKwargs(VideosKwargs):\n     max_image_size: Optional[dict[str, int]]\n-    do_pad: Optional[bool]\n \n \n class SmolVLMVideoProcessor(BaseVideoProcessor):"
        },
        {
            "sha": "633d9b0b16b902d0aa8779b202029255d0238396",
            "filename": "src/transformers/models/superpoint/image_processing_superpoint.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -30,6 +30,7 @@\n     to_numpy_array,\n     valid_images,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, logging, requires_backends\n \n \n@@ -45,6 +46,15 @@\n logger = logging.get_logger(__name__)\n \n \n+class SuperPointImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    do_grayscale (`bool`, *optional*, defaults to `True`):\n+        Whether to convert the image to grayscale. Can be overridden by `do_grayscale` in the `preprocess` method.\n+    \"\"\"\n+\n+    do_grayscale: Optional[bool] = True\n+\n+\n def is_grayscale(\n     image: np.ndarray,\n     input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -120,6 +130,7 @@ class SuperPointImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = SuperPointImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "3750441fc9f048339950f175719d5bb476b92a91",
            "filename": "src/transformers/models/superpoint/image_processing_superpoint_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 12,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -21,7 +21,6 @@\n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -34,6 +33,7 @@\n     TensorType,\n     auto_docstring,\n )\n+from .image_processing_superpoint import SuperPointImageProcessorKwargs\n \n \n if TYPE_CHECKING:\n@@ -53,15 +53,6 @@ def is_grayscale(\n     )\n \n \n-class SuperPointFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    r\"\"\"\n-    do_grayscale (`bool`, *optional*, defaults to `True`):\n-        Whether to convert the image to grayscale. Can be overridden by `do_grayscale` in the `preprocess` method.\n-    \"\"\"\n-\n-    do_grayscale: Optional[bool] = True\n-\n-\n def convert_to_grayscale(\n     image: \"torch.Tensor\",\n ) -> \"torch.Tensor\":\n@@ -90,9 +81,9 @@ class SuperPointImageProcessorFast(BaseImageProcessorFast):\n     do_rescale = True\n     rescale_factor = 1 / 255\n     do_normalize = None\n-    valid_kwargs = SuperPointFastImageProcessorKwargs\n+    valid_kwargs = SuperPointImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[SuperPointFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[SuperPointImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     def _preprocess("
        },
        {
            "sha": "018a1bf0f4dfb37f1efc2e06ac3fb64e62d4aecd",
            "filename": "src/transformers/models/swin2sr/image_processing_swin2sr.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -30,13 +30,18 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, logging\n from ...utils.deprecation import deprecate_kwarg\n \n \n logger = logging.get_logger(__name__)\n \n \n+class Swin2SRImageProcessorKwargs(ImagesKwargs):\n+    size_divisor: Optional[int]\n+\n+\n class Swin2SRImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a Swin2SR image processor.\n@@ -51,6 +56,7 @@ class Swin2SRImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = Swin2SRImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "bee3da36c9b6888bbc9482f2d31d965a18d4338b",
            "filename": "src/transformers/models/swin2sr/image_processing_swin2sr_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 14,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -22,7 +22,6 @@\n from ...image_processing_utils import BatchFeature, ChannelDimension, get_image_size\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -34,30 +33,21 @@\n     logging,\n )\n from ...utils.deprecation import deprecate_kwarg\n+from .image_processing_swin2sr import Swin2SRImageProcessorKwargs\n \n \n logger = logging.get_logger(__name__)\n \n \n-class Swin2SRFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    size_divisor (`int`, *optional*, defaults to `8`):\n-        The size of the sliding window for the local attention. It will be used to pad the image\n-        to the size divisible by `size_divisor`\n-    \"\"\"\n-\n-    size_divisor: Optional[int]\n-\n-\n @auto_docstring\n class Swin2SRImageProcessorFast(BaseImageProcessorFast):\n     do_rescale = True\n     rescale_factor = 1 / 255\n     do_pad = True\n     size_divisor = 8\n-    valid_kwargs = Swin2SRFastImageProcessorKwargs\n+    valid_kwargs = Swin2SRImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[Swin2SRFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[Swin2SRImageProcessorKwargs]):\n         pad_size = kwargs.pop(\"pad_size\", None)\n         kwargs.setdefault(\"size_divisor\", pad_size)\n         super().__init__(**kwargs)\n@@ -76,7 +66,7 @@ def pad_size(self, value):\n         )\n         self.size_divisor = value\n \n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[Swin2SRFastImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[Swin2SRImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     @deprecate_kwarg(\"size\", version=\"v5\", new_name=\"size_divisor\")"
        },
        {
            "sha": "1a4d68522205ee863fbaa03111d9aeb4dc24c179",
            "filename": "src/transformers/models/textnet/image_processing_textnet.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -39,6 +39,7 @@\n     validate_kwargs,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, is_vision_available, logging\n \n \n@@ -48,6 +49,10 @@\n     import PIL\n \n \n+class TextNetImageProcessorKwargs(ImagesKwargs):\n+    size_divisor: Optional[int]\n+\n+\n class TextNetImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a TextNet image processor.\n@@ -90,6 +95,7 @@ class TextNetImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = TextNetImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "eba6e14e64bc9328d135a4587f0a2772ad9d1bdd",
            "filename": "src/transformers/models/textnet/image_processing_textnet_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 13,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -20,7 +20,7 @@\n from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n-from ...image_processing_utils_fast import BaseImageProcessorFast, DefaultFastImageProcessorKwargs\n+from ...image_processing_utils_fast import BaseImageProcessorFast\n from ...image_transforms import (\n     get_resize_output_image_size,\n     group_images_by_shape,\n@@ -39,15 +39,7 @@\n     TensorType,\n     auto_docstring,\n )\n-\n-\n-class TextNetFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    size_divisor (`int`, *optional*, defaults to 32):\n-        Ensures height and width are rounded to a multiple of this value after resizing.\n-    \"\"\"\n-\n-    size_divisor: Optional[int]\n+from .image_processing_textnet import TextNetImageProcessorKwargs\n \n \n @auto_docstring\n@@ -64,13 +56,13 @@ class TextNetImageProcessorFast(BaseImageProcessorFast):\n     do_normalize = True\n     do_convert_rgb = True\n     size_divisor = 32\n-    valid_kwargs = TextNetFastImageProcessorKwargs\n+    valid_kwargs = TextNetImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[TextNetFastImageProcessorKwargs]) -> None:\n+    def __init__(self, **kwargs: Unpack[TextNetImageProcessorKwargs]) -> None:\n         super().__init__(**kwargs)\n \n     @auto_docstring\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[TextNetFastImageProcessorKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[TextNetImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     def resize("
        },
        {
            "sha": "81c7db2850b395d97f224e05eb37729d1031a688",
            "filename": "src/transformers/models/trocr/processing_trocr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -69,8 +69,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[TrOCRProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "67c1ffe4fae8429fb699518f29a496bddac1448a",
            "filename": "src/transformers/models/tvp/image_processing_tvp.py",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -39,6 +39,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n \n \n@@ -49,6 +50,21 @@\n logger = logging.get_logger(__name__)\n \n \n+class TvpImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    do_flip_channel_order (`bool`, *optional*):\n+        Whether to flip the channel order of the image from RGB to BGR.\n+    constant_values (`float` or `List[float]`, *optional*):\n+        Value used to fill the padding area when `pad_mode` is `'constant'`.\n+    pad_mode (`str`, *optional*):\n+        Padding mode to use â€” `'constant'`, `'edge'`, `'reflect'`, or `'symmetric'`.\n+    \"\"\"\n+\n+    do_flip_channel_order: Optional[bool]\n+    constant_values: Optional[Union[float, list[float]]]\n+    pad_mode: Optional[str]\n+\n+\n # Copied from transformers.models.vivit.image_processing_vivit.make_batched\n def make_batched(videos) -> list[list[ImageInput]]:\n     if isinstance(videos, (list, tuple)) and isinstance(videos[0], (list, tuple)) and is_valid_image(videos[0][0]):\n@@ -133,6 +149,7 @@ class TvpImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = TvpImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "7cd550d75194ba0950bfc9905962db418279d4e3",
            "filename": "src/transformers/models/tvp/image_processing_tvp_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 19,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -22,7 +22,6 @@\n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -36,21 +35,7 @@\n )\n from ...processing_utils import Unpack\n from ...utils import TensorType, auto_docstring\n-\n-\n-class TvpFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    r\"\"\"\n-    do_flip_channel_order (`bool`, *optional*):\n-        Whether to flip the channel order of the image from RGB to BGR.\n-    constant_values (`float` or `List[float]`, *optional*):\n-        Value used to fill the padding area when `pad_mode` is `'constant'`.\n-    pad_mode (`str`, *optional*):\n-        Padding mode to use â€” `'constant'`, `'edge'`, `'reflect'`, or `'symmetric'`.\n-    \"\"\"\n-\n-    do_flip_channel_order: Optional[bool]\n-    constant_values: Optional[Union[float, list[float]]]\n-    pad_mode: Optional[str]\n+from .image_processing_tvp import TvpImageProcessorKwargs\n \n \n @auto_docstring\n@@ -71,16 +56,16 @@ class TvpImageProcessorFast(BaseImageProcessorFast):\n     pad_mode = \"constant\"\n     do_normalize = True\n     do_flip_channel_order = True\n-    valid_kwargs = TvpFastImageProcessorKwargs\n+    valid_kwargs = TvpImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[TvpFastImageProcessorKwargs]):\n+    def __init__(self, **kwargs: Unpack[TvpImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     @auto_docstring\n     def preprocess(\n         self,\n         videos: Union[ImageInput, list[ImageInput], list[list[ImageInput]]],\n-        **kwargs: Unpack[TvpFastImageProcessorKwargs],\n+        **kwargs: Unpack[TvpImageProcessorKwargs],\n     ) -> BatchFeature:\n         return super().preprocess(videos, **kwargs)\n "
        },
        {
            "sha": "1be71aea63e29e5d0bd2b879e4d0ef79cb5bb5f4",
            "filename": "src/transformers/models/udop/processing_udop.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -48,7 +48,6 @@ class UdopProcessorKwargs(ProcessingKwargs, total=False):\n             \"return_length\": False,\n             \"verbose\": True,\n         },\n-        \"images_kwargs\": {},\n     }\n \n \n@@ -85,8 +84,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[UdopProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "371a419d4a3627421dc7b4e1f8b8563182782815",
            "filename": "src/transformers/models/video_llava/video_processing_video_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fvideo_processing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fvideo_processing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fvideo_processing_video_llava.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -15,13 +15,9 @@\n \"\"\"Video processor class for Video-LLaVA.\"\"\"\n \n from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling\n-from ...processing_utils import Unpack, VideosKwargs\n from ...video_processing_utils import BaseVideoProcessor\n \n \n-class VideoLlavaFastVideoProcessorInitKwargs(VideosKwargs): ...\n-\n-\n class VideoLlavaVideoProcessor(BaseVideoProcessor):\n     resample = PILImageResampling.BICUBIC\n     image_mean = OPENAI_CLIP_MEAN\n@@ -35,11 +31,6 @@ class VideoLlavaVideoProcessor(BaseVideoProcessor):\n     do_normalize = True\n     do_convert_rgb = True\n     do_sample_frames = False  # Set to False for BC, recommended to set `True` in new models\n-    valid_kwargs = VideoLlavaFastVideoProcessorInitKwargs\n-    model_input_names = [\"pixel_values_videos\"]\n-\n-    def __init__(self, **kwargs: Unpack[VideoLlavaFastVideoProcessorInitKwargs]):\n-        super().__init__(**kwargs)\n \n \n __all__ = [\"VideoLlavaVideoProcessor\"]"
        },
        {
            "sha": "bb29e1d1ee30f96a592c21770995c3f32457917d",
            "filename": "src/transformers/models/vilt/image_processing_vilt.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -35,6 +35,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n from ...utils.import_utils import requires\n \n@@ -46,6 +47,10 @@\n logger = logging.get_logger(__name__)\n \n \n+class ViltImageProcessorKwargs(ImagesKwargs):\n+    size_divisor: Optional[int]\n+\n+\n def max_across_indices(values: Iterable[Any]) -> list[Any]:\n     \"\"\"\n     Return the maximum value across all indices of an iterable of values.\n@@ -162,6 +167,7 @@ class ViltImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = ViltImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "11537f74656df549e3b58f8e8d0a04f49b6d6efb",
            "filename": "src/transformers/models/vilt/image_processing_vilt_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 15,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -22,7 +22,6 @@\n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     get_max_height_width,\n     group_images_by_shape,\n     reorder_images,\n@@ -32,26 +31,14 @@\n     TensorType,\n     auto_docstring,\n )\n+from .image_processing_vilt import ViltImageProcessorKwargs\n \n \n # Set maximum size based on the typical aspect ratio of the COCO dataset\n MAX_LONGER_EDGE = 1333\n MAX_SHORTER_EDGE = 800\n \n \n-class ViltFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    Args:\n-        size_divisor (`int`, *optional*, defaults to 32):\n-            The size to make the height and width divisible by.\n-        rescale_factor (`float`, *optional*, defaults to 1/255):\n-            The factor to rescale the image by.\n-    \"\"\"\n-\n-    size_divisor: Optional[int]\n-    rescale_factor: Optional[float]\n-\n-\n @auto_docstring\n class ViltImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BICUBIC\n@@ -65,7 +52,7 @@ class ViltImageProcessorFast(BaseImageProcessorFast):\n     do_pad = True\n     default_to_square = False\n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n-    valid_kwargs = ViltFastImageProcessorKwargs\n+    valid_kwargs = ViltImageProcessorKwargs\n \n     def _preprocess(\n         self,"
        },
        {
            "sha": "5b5126ad4a85a0e170e6934c61d77ce7fbb9081f",
            "filename": "src/transformers/models/vilt/processing_vilt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fvilt%2Fprocessing_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fvilt%2Fprocessing_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fprocessing_vilt.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -17,17 +17,11 @@\n \"\"\"\n \n import warnings\n-from typing import Optional\n \n-from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin\n-\n-\n-class ViltImagesKwargs(ImagesKwargs):\n-    size_divisor: Optional[int]\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin\n \n \n class ViltProcessorKwargs(ProcessingKwargs, total=False):\n-    images_kwargs: ViltImagesKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"add_special_tokens\": True,"
        },
        {
            "sha": "95933c053ce59282eae41f8361eb60894dd54766",
            "filename": "src/transformers/models/vitmatte/image_processing_vitmatte.py",
            "status": "modified",
            "additions": 11,
            "deletions": 5,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -33,13 +33,18 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, logging\n from ...utils.deprecation import deprecate_kwarg\n \n \n logger = logging.get_logger(__name__)\n \n \n+class VitMatteImageProcessorKwargs(ImagesKwargs):\n+    size_divisor: Optional[int]\n+\n+\n class VitMatteImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a ViTMatte image processor.\n@@ -68,6 +73,7 @@ class VitMatteImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = VitMatteImageProcessorKwargs\n \n     def __init__(\n         self,\n@@ -107,15 +113,15 @@ def size_divisibility(self, value):\n     def pad_image(\n         self,\n         image: np.ndarray,\n-        size_divisibility: int = 32,\n+        size_divisor: int = 32,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Args:\n             image (`np.ndarray`):\n                 Image to pad.\n-            size_divisibility (`int`, *optional*, defaults to 32):\n+            size_divisor (`int`, *optional*, defaults to 32):\n                 The width and height of the image will be padded to be divisible by this number.\n             data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n                 The channel dimension format for the output image. Can be one of:\n@@ -134,8 +140,8 @@ def pad_image(\n \n         height, width = get_image_size(image, input_data_format)\n \n-        pad_height = 0 if height % size_divisibility == 0 else size_divisibility - height % size_divisibility\n-        pad_width = 0 if width % size_divisibility == 0 else size_divisibility - width % size_divisibility\n+        pad_height = 0 if height % size_divisor == 0 else size_divisor - height % size_divisor\n+        pad_width = 0 if width % size_divisor == 0 else size_divisor - width % size_divisor\n         if pad_width + pad_height > 0:\n             padding = ((0, pad_height), (0, pad_width))\n             image = pad(image, padding=padding, data_format=data_format, input_data_format=input_data_format)\n@@ -265,7 +271,7 @@ def preprocess(\n \n         if do_pad:\n             images = [\n-                self.pad_image(image, size_divisibility=size_divisor, input_data_format=input_data_format)\n+                self.pad_image(image, size_divisor=size_divisor, input_data_format=input_data_format)\n                 for image in images\n             ]\n "
        },
        {
            "sha": "dd09b987090d35d1c678f120a05cce6c8ebb1347",
            "filename": "src/transformers/models/vitmatte/image_processing_vitmatte_fast.py",
            "status": "modified",
            "additions": 10,
            "deletions": 19,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -22,7 +22,6 @@\n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -40,20 +39,12 @@\n     filter_out_non_signature_kwargs,\n     logging,\n )\n+from .image_processing_vitmatte import VitMatteImageProcessorKwargs\n \n \n logger = logging.get_logger(__name__)\n \n \n-class VitMatteFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    size_divisor (`int`, *optional*, defaults to 32):\n-        The width and height of the image will be padded to be divisible by this number.\n-    \"\"\"\n-\n-    size_divisor: Optional[int]\n-\n-\n @auto_docstring\n class VitMatteImageProcessorFast(BaseImageProcessorFast):\n     do_rescale: bool = True\n@@ -63,9 +54,9 @@ class VitMatteImageProcessorFast(BaseImageProcessorFast):\n     image_std: Optional[Union[float, list[float]]] = IMAGENET_STANDARD_STD\n     do_pad: bool = True\n     size_divisor: int = 32\n-    valid_kwargs = VitMatteFastImageProcessorKwargs\n+    valid_kwargs = VitMatteImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[VitMatteFastImageProcessorKwargs]) -> None:\n+    def __init__(self, **kwargs: Unpack[VitMatteImageProcessorKwargs]) -> None:\n         size_divisibility = kwargs.pop(\"size_divisibility\", None)\n         kwargs.setdefault(\"size_divisor\", size_divisibility)\n         super().__init__(**kwargs)\n@@ -87,21 +78,21 @@ def size_divisibility(self, value):\n     def _pad_image(\n         self,\n         images: torch.Tensor,\n-        size_divisibility: int = 32,\n+        size_divisor: int = 32,\n     ) -> torch.Tensor:\n         \"\"\"\n-        Pads an image or batched images constantly so that width and height are divisible by size_divisibility\n+        Pads an image or batched images constantly so that width and height are divisible by size_divisor\n \n         Args:\n             image (`torch.Tensor`):\n                 Image to pad.\n-            size_divisibility (`int`, *optional*, defaults to 32):\n+            size_divisor (`int`, *optional*, defaults to 32):\n                 The width and height of the image will be padded to be divisible by this number.\n         \"\"\"\n         height, width = get_image_size(images, channel_dim=ChannelDimension.FIRST)\n \n-        pad_height = 0 if height % size_divisibility == 0 else size_divisibility - height % size_divisibility\n-        pad_width = 0 if width % size_divisibility == 0 else size_divisibility - width % size_divisibility\n+        pad_height = 0 if height % size_divisor == 0 else size_divisor - height % size_divisor\n+        pad_width = 0 if width % size_divisor == 0 else size_divisor - width % size_divisor\n \n         if pad_width + pad_height > 0:\n             padding = (0, 0, pad_width, pad_height)\n@@ -114,7 +105,7 @@ def preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n         trimaps: list[\"torch.Tensor\"],\n-        **kwargs: Unpack[VitMatteFastImageProcessorKwargs],\n+        **kwargs: Unpack[VitMatteImageProcessorKwargs],\n     ) -> BatchFeature:\n         r\"\"\"\n         trimaps (`list[torch.Tensor]`):\n@@ -129,7 +120,7 @@ def _preprocess_image_like_inputs(\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n-        **kwargs: Unpack[VitMatteFastImageProcessorKwargs],\n+        **kwargs: Unpack[VitMatteImageProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess image-like inputs."
        },
        {
            "sha": "ad806ba4cc3d1c145116c06480d4c1ab89704884",
            "filename": "src/transformers/models/vjepa2/video_processing_vjepa2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fvideo_processing_vjepa2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fvideo_processing_vjepa2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fvideo_processing_vjepa2.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -19,9 +19,6 @@\n from ...video_processing_utils import BaseVideoProcessor\n \n \n-class VJEPA2VideoProcessorInitKwargs(VideosKwargs): ...\n-\n-\n class VJEPA2VideoProcessor(BaseVideoProcessor):\n     resample = PILImageResampling.BILINEAR\n     image_mean = IMAGENET_DEFAULT_MEAN\n@@ -32,10 +29,8 @@ class VJEPA2VideoProcessor(BaseVideoProcessor):\n     do_rescale = True\n     do_center_crop = True\n     do_normalize = True\n-    valid_kwargs = VJEPA2VideoProcessorInitKwargs\n-    model_input_names = [\"pixel_values_videos\"]\n \n-    def __init__(self, **kwargs: Unpack[VJEPA2VideoProcessorInitKwargs]):\n+    def __init__(self, **kwargs: Unpack[VideosKwargs]):\n         crop_size = kwargs.get(\"crop_size\", 256)\n         if not isinstance(crop_size, int):\n             if not isinstance(crop_size, dict) or \"height\" not in crop_size:"
        },
        {
            "sha": "124835e35338b61ca061fa92bb1466e9e0dd8b80",
            "filename": "src/transformers/models/voxtral/processing_voxtral.py",
            "status": "modified",
            "additions": 9,
            "deletions": 18,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -198,9 +198,8 @@ def apply_chat_template(\n         )\n         text_kwargs = output_kwargs[\"text_kwargs\"]\n         audio_kwargs = output_kwargs[\"audio_kwargs\"]\n-        common_kwargs = output_kwargs[\"common_kwargs\"]\n+        return_tensors = text_kwargs.get(\"return_tensors\", None)\n \n-        return_tensors = common_kwargs.pop(\"return_tensors\", None)\n         if return_tensors != \"pt\":\n             raise ValueError(f\"{self.__class__.__name__} only supports `return_tensors='pt'`.\")\n \n@@ -270,16 +269,10 @@ def __call__(\n                 f\"{self.audio_token} is present in the provided text which is not supported by VoxtralProcessor. Please use the `apply_chat_template` method instead.\"\n             )\n \n-        output_kwargs = self._merge_kwargs(\n-            VoxtralProcessorKwargs,\n-            **kwargs,\n-        )\n-        text_kwargs = output_kwargs[\"text_kwargs\"]\n-        common_kwargs = output_kwargs[\"common_kwargs\"]\n-\n-        out = self.tokenizer(text, **text_kwargs)\n+        output_kwargs = self._merge_kwargs(VoxtralProcessorKwargs, **kwargs)\n+        out = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n \n-        return BatchFeature(data=out, tensor_type=common_kwargs.pop(\"return_tensors\", None))\n+        return BatchFeature(data=out, tensor_type=output_kwargs[\"text_kwargs\"].get(\"return_tensors\", None))\n \n     # TODO: @eustlb, this should be moved to mistral_common + testing\n     def apply_transcription_request(\n@@ -327,7 +320,6 @@ def apply_transcription_request(\n         )\n         text_kwargs = output_kwargs[\"text_kwargs\"]\n         audio_kwargs = output_kwargs[\"audio_kwargs\"]\n-        common_kwargs = output_kwargs[\"common_kwargs\"]\n \n         is_str = isinstance(audio, str)\n         is_list_of_str = all(isinstance(el, str) for el in audio)\n@@ -344,15 +336,14 @@ def apply_transcription_request(\n                 )\n \n         sampling_rate = audio_kwargs[\"sampling_rate\"]\n-        return_dict = common_kwargs.pop(\"return_dict\", False)\n-        tokenize = common_kwargs.pop(\"tokenize\", False)\n \n         # make sure to remove from text_kwargs and audio_kwargs\n-        for k in (\"return_dict\", \"tokenize\"):\n-            text_kwargs.pop(k, None)\n-            audio_kwargs.pop(k, None)\n+        return_dict = text_kwargs.pop(\"return_dict\", False)\n+        tokenize = text_kwargs.pop(\"tokenize\", False)\n+        _ = audio_kwargs.pop(\"return_dict\", False)\n+        _ = audio_kwargs.pop(\"tokenize\", False)\n \n-        return_tensors = common_kwargs.pop(\"return_tensors\", None)\n+        return_tensors = text_kwargs.pop(\"return_tensors\", None)\n         if return_tensors != \"pt\":\n             raise ValueError(f\"{self.__class__.__name__} only supports `return_tensors='pt'`.\")\n "
        },
        {
            "sha": "ee8e3abd195d6635fac0847218ba31e7cbceda9f",
            "filename": "src/transformers/models/wav2vec2/processing_wav2vec2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -76,8 +76,6 @@ def __call__(\n         self,\n         audio: Optional[AudioInput] = None,\n         text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n-        images=None,\n-        videos=None,\n         **kwargs: Unpack[Wav2Vec2ProcessorKwargs],\n     ):\n         \"\"\"\n@@ -112,7 +110,6 @@ def __call__(\n                 audio,\n                 **output_kwargs[\"audio_kwargs\"],\n                 **output_kwargs[\"text_kwargs\"],\n-                **output_kwargs[\"common_kwargs\"],\n             )\n \n         if audio is not None:"
        },
        {
            "sha": "fc95fc04c7542bf30f9891d189d2ca397b766817",
            "filename": "src/transformers/models/wav2vec2_bert/processing_wav2vec2_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -73,8 +73,6 @@ def __call__(\n         self,\n         audio: Optional[AudioInput] = None,\n         text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n-        images=None,\n-        videos=None,\n         **kwargs: Unpack[Wav2Vec2BertProcessorKwargs],\n     ):\n         \"\"\""
        },
        {
            "sha": "21aac76adac8f84bd54cee8ca56f067e1d0731c7",
            "filename": "src/transformers/models/yolos/image_processing_yolos.py",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -51,6 +51,7 @@\n     validate_kwargs,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import (\n     TensorType,\n     is_scipy_available,\n@@ -80,6 +81,29 @@\n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n \n \n+class YolosImageProcessorKwargs(ImagesKwargs):\n+    r\"\"\"\n+    format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n+        Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n+    do_convert_annotations (`bool`, *optional*, defaults to `True`):\n+        Controls whether to convert the annotations to the format expected by the YOLOS model. Converts the\n+        bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n+        Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n+    return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n+        Whether to return segmentation masks.\n+    annotations (`AnnotationType` or `list[AnnotationType]`, *optional*):\n+        Annotations to transform according to the padding that is applied to the images.\n+    masks_path (`str` or `pathlib.Path`, *optional*):\n+        Path to the directory containing the segmentation masks.\n+    \"\"\"\n+\n+    format: Optional[Union[str, AnnotationFormat]]\n+    do_convert_annotations: Optional[bool]\n+    return_segmentation_masks: Optional[bool]\n+    annotations: Optional[Union[AnnotationType, list[AnnotationType]]]\n+    masks_path: Optional[Union[str, pathlib.Path]]\n+\n+\n # Copied from transformers.models.detr.image_processing_detr.get_max_height_width\n def get_max_height_width(\n     images: list[np.ndarray], input_data_format: Optional[Union[str, ChannelDimension]] = None\n@@ -744,6 +768,7 @@ class YolosImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n+    valid_kwargs = YolosImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "fc1f1852862f60fea429ae55cbd32842a84e7030",
            "filename": "src/transformers/models/yolos/image_processing_yolos_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 40,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -14,7 +14,6 @@\n from ...image_processing_utils import BatchFeature, get_size_dict\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     SizeDict,\n     get_image_size_for_max_height_width,\n     get_max_height_width,\n@@ -35,28 +34,11 @@\n from ...processing_utils import Unpack\n from ...utils import TensorType, auto_docstring, logging\n from ...utils.import_utils import requires\n+from .image_processing_yolos import YolosImageProcessorKwargs\n \n \n logger = logging.get_logger(__name__)\n \n-\n-class YolosFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    r\"\"\"\n-    format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n-        Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n-    do_convert_annotations (`bool`, *optional*, defaults to `True`):\n-        Controls whether to convert the annotations to the format expected by the YOLOS model. Converts the\n-        bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n-        Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n-    return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n-        Whether to return segmentation masks.\n-    \"\"\"\n-\n-    format: Optional[Union[str, AnnotationFormat]]\n-    do_convert_annotations: Optional[bool]\n-    return_segmentation_masks: Optional[bool]\n-\n-\n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n \n \n@@ -320,9 +302,9 @@ class YolosImageProcessorFast(BaseImageProcessorFast):\n     size = {\"shortest_edge\": 800, \"longest_edge\": 1333}\n     default_to_square = False\n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n-    valid_kwargs = YolosFastImageProcessorKwargs\n+    valid_kwargs = YolosImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[YolosFastImageProcessorKwargs]) -> None:\n+    def __init__(self, **kwargs: Unpack[YolosImageProcessorKwargs]) -> None:\n         if \"pad_and_return_pixel_mask\" in kwargs:\n             kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n \n@@ -584,25 +566,8 @@ def pad(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        annotations: Optional[Union[AnnotationType, list[AnnotationType]]] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        **kwargs: Unpack[YolosFastImageProcessorKwargs],\n+        **kwargs: Unpack[YolosImageProcessorKwargs],\n     ) -> BatchFeature:\n-        r\"\"\"\n-        annotations (`AnnotationType` or `list[AnnotationType]`, *optional*):\n-            List of annotations associated with the image or batch of images. If annotation is for object\n-            detection, the annotations should be a dictionary with the following keys:\n-            - \"image_id\" (`int`): The image id.\n-            - \"annotations\" (`list[Dict]`): List of annotations for an image. Each annotation should be a\n-                dictionary. An image can have no annotations, in which case the list should be empty.\n-            If annotation is for segmentation, the annotations should be a dictionary with the following keys:\n-            - \"image_id\" (`int`): The image id.\n-            - \"segments_info\" (`list[Dict]`): List of segments for an image. Each segment should be a dictionary.\n-                An image can have no segments, in which case the list should be empty.\n-            - \"file_name\" (`str`): The file name of the image.\n-        masks_path (`str` or `pathlib.Path`, *optional*):\n-            Path to the directory containing the segmentation masks.\n-        \"\"\"\n         if \"pad_and_return_pixel_mask\" in kwargs:\n             kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n             logger.warning_once(\n@@ -617,7 +582,7 @@ def preprocess(\n             )\n             kwargs[\"size\"] = kwargs.pop(\"max_size\")\n \n-        return super().preprocess(images, annotations, masks_path, **kwargs)\n+        return super().preprocess(images, **kwargs)\n \n     def _preprocess(\n         self,"
        },
        {
            "sha": "e8ad44dd76c3eab2019d2653abfa7344abaca665",
            "filename": "src/transformers/models/zoedepth/image_processing_zoedepth.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -40,6 +40,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import (\n     TensorType,\n     filter_out_non_signature_kwargs,\n@@ -61,6 +62,25 @@\n logger = logging.get_logger(__name__)\n \n \n+class ZoeDepthImageProcessorKwargs(ImagesKwargs):\n+    \"\"\"\n+    keep_aspect_ratio (`bool`, *optional*, defaults to `True`):\n+        If `True`, the image is resized by choosing the smaller of the height and width scaling factors and using it\n+        for both dimensions. This ensures that the image is scaled down as little as possible while still fitting\n+        within the desired output size. In case `ensure_multiple_of` is also set, the image is further resized to a\n+        size that is a multiple of this value by flooring the height and width to the nearest multiple of this value.\n+        Can be overridden by `keep_aspect_ratio` in `preprocess`.\n+    ensure_multiple_of (`int`, *optional*, defaults to 32):\n+        If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Works by flooring\n+        the height and width to the nearest multiple of this value.\n+        Works both with and without `keep_aspect_ratio` being set to `True`.\n+        Can be overridden by `ensure_multiple_of` in `preprocess`.\n+    \"\"\"\n+\n+    keep_aspect_ratio: Optional[bool]\n+    ensure_multiple_of: Optional[int]\n+\n+\n def get_resize_output_image_size(\n     input_image: np.ndarray,\n     output_size: Union[int, Iterable[int]],\n@@ -145,6 +165,7 @@ class ZoeDepthImageProcessor(BaseImageProcessor):\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = ZoeDepthImageProcessorKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "852ee161aff17c06494602220f84dea9cddbb8ad",
            "filename": "src/transformers/models/zoedepth/image_processing_zoedepth_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 24,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth_fast.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -28,7 +28,6 @@\n )\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -48,32 +47,13 @@\n     logging,\n     requires_backends,\n )\n-from .image_processing_zoedepth import get_resize_output_image_size\n+from .image_processing_zoedepth import ZoeDepthImageProcessorKwargs, get_resize_output_image_size\n from .modeling_zoedepth import ZoeDepthDepthEstimatorOutput\n \n \n logger = logging.get_logger(__name__)\n \n \n-class ZoeDepthFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    keep_aspect_ratio (`bool`, *optional*, defaults to `True`):\n-        If `True`, the image is resized by choosing the smaller of the height and width scaling factors and using it\n-        for both dimensions. This ensures that the image is scaled down as little as possible while still fitting\n-        within the desired output size. In case `ensure_multiple_of` is also set, the image is further resized to a\n-        size that is a multiple of this value by flooring the height and width to the nearest multiple of this value.\n-        Can be overridden by `keep_aspect_ratio` in `preprocess`.\n-    ensure_multiple_of (`int`, *optional*, defaults to 32):\n-        If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Works by flooring\n-        the height and width to the nearest multiple of this value.\n-        Works both with and without `keep_aspect_ratio` being set to `True`.\n-        Can be overridden by `ensure_multiple_of` in `preprocess`.\n-    \"\"\"\n-\n-    keep_aspect_ratio: Optional[bool]\n-    ensure_multiple_of: Optional[int]\n-\n-\n @auto_docstring\n class ZoeDepthImageProcessorFast(BaseImageProcessorFast):\n     do_pad = True\n@@ -86,16 +66,16 @@ class ZoeDepthImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BILINEAR\n     keep_aspect_ratio = True\n     ensure_multiple_of = 1 / 32\n-    valid_kwargs = ZoeDepthFastImageProcessorKwargs\n+    valid_kwargs = ZoeDepthImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[ZoeDepthFastImageProcessorKwargs]) -> None:\n+    def __init__(self, **kwargs: Unpack[ZoeDepthImageProcessorKwargs]) -> None:\n         super().__init__(**kwargs)\n \n     @auto_docstring\n     def preprocess(\n         self,\n         images: ImageInput,\n-        **kwargs: Unpack[ZoeDepthFastImageProcessorKwargs],\n+        **kwargs: Unpack[ZoeDepthImageProcessorKwargs],\n     ) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n "
        },
        {
            "sha": "e7786d1ba61d198f47d848930c6bb4f2b6dca750",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 63,
            "deletions": 24,
            "changes": 87,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -72,6 +72,8 @@\n \n \n if is_torch_available():\n+    import torch\n+\n     from .modeling_utils import PreTrainedAudioTokenizerBase\n \n \n@@ -156,6 +158,7 @@ class TextKwargs(TypedDict, total=False):\n     verbose: Optional[bool]\n     padding_side: Optional[str]\n     return_mm_token_type_ids: Optional[bool]\n+    return_tensors: Optional[Union[str, TensorType]]\n \n \n class ImagesKwargs(TypedDict, total=False):\n@@ -164,6 +167,8 @@ class ImagesKwargs(TypedDict, total=False):\n     class methods and docstrings.\n \n     Attributes:\n+        do_convert_rgb (`bool`):\n+            Whether to convert the video to RGB format.\n         do_resize (`bool`, *optional*):\n             Whether to resize the image.\n         size (`dict[str, int]`, *optional*):\n@@ -183,7 +188,7 @@ class methods and docstrings.\n         image_std (`float` or `list[float]`, *optional*):\n             Standard deviation to use if normalizing the image.\n         do_pad (`bool`, *optional*):\n-            Whether to pad the image to the `(max_height, max_width)` of the images in the batch.\n+            Whether to pad the images in the batch.\n         pad_size (`dict[str, int]`, *optional*):\n             The size `{\"height\": int, \"width\" int}` to pad the images to.\n         do_center_crop (`bool`, *optional*):\n@@ -192,10 +197,13 @@ class methods and docstrings.\n             The channel dimension format for the output image.\n         input_data_format (`ChannelDimension` or `str`, *optional*):\n             The channel dimension format for the input image.\n-        device (`str`, *optional*):\n+        device (`Union[str, torch.Tensor]`, *optional*):\n             The device to use for processing (e.g. \"cpu\", \"cuda\"), only relevant for fast image processing.\n+        disable_grouping (`bool`, *optional*):\n+            Whether to group images by shapes when processing or not, only relevant for fast image processing.\n     \"\"\"\n \n+    do_convert_rgb: Optional[bool]\n     do_resize: Optional[bool]\n     size: Optional[dict[str, int]]\n     crop_size: Optional[dict[str, int]]\n@@ -210,7 +218,9 @@ class methods and docstrings.\n     do_center_crop: Optional[bool]\n     data_format: Optional[ChannelDimension]\n     input_data_format: Optional[Union[str, ChannelDimension]]\n-    device: Optional[str]\n+    device: Optional[Union[str, \"torch.device\"]]\n+    disable_grouping: Optional[bool]\n+    return_tensors: Optional[Union[str, TensorType]]\n \n \n class VideosKwargs(TypedDict, total=False):\n@@ -240,6 +250,8 @@ class VideosKwargs(TypedDict, total=False):\n             Standard deviation to use if normalizing the video.\n         do_center_crop (`bool`, *optional*):\n             Whether to center crop the video.\n+        do_pad (`bool`, *optional*):\n+            Whether to pad the images in the batch.\n         do_sample_frames (`bool`, *optional*):\n             Whether to sample frames from the video before processing or to process the whole video.\n         video_metadata (`Union[VideoMetadata, dict]`, *optional*):\n@@ -254,6 +266,8 @@ class VideosKwargs(TypedDict, total=False):\n             The channel dimension format for the output video.\n         input_data_format (`ChannelDimension` or `str`, *optional*):\n             The channel dimension format for the input video.\n+        device (`Union[str, torch.Tensor]`, *optional*):\n+            The device to use for processing (e.g. \"cpu\", \"cuda\"), only relevant for fast image processing.\n         return_metadata (`ChannelDimension` or `str`, *optional*):\n             Whether to return video metadata or not.\n     \"\"\"\n@@ -269,15 +283,17 @@ class VideosKwargs(TypedDict, total=False):\n     image_mean: Optional[Union[float, list[float]]]\n     image_std: Optional[Union[float, list[float]]]\n     do_center_crop: Optional[bool]\n+    do_pad: Optional[bool]\n     crop_size: Optional[dict[str, int]]\n     data_format: Optional[ChannelDimension]\n     input_data_format: Optional[Union[str, ChannelDimension]]\n-    device: Optional[str]\n+    device: Optional[Union[str, \"torch.device\"]]\n     do_sample_frames: Optional[bool]\n     video_metadata: Optional[Union[VideoMetadata, dict]]\n     fps: Optional[Union[int, float]]\n     num_frames: Optional[int]\n     return_metadata: Optional[bool]\n+    return_tensors: Optional[Union[str, TensorType]]\n \n \n class AudioKwargs(TypedDict, total=False):\n@@ -317,9 +333,6 @@ class AudioKwargs(TypedDict, total=False):\n     truncation: Optional[bool]\n     pad_to_multiple_of: Optional[int]\n     return_attention_mask: Optional[bool]\n-\n-\n-class CommonKwargs(TypedDict, total=False):\n     return_tensors: Optional[Union[str, TensorType]]\n \n \n@@ -364,9 +377,6 @@ class CustomProcessorKwargs(ProcessingKwargs, total=False):\n \n     _defaults = {}\n \n-    common_kwargs: CommonKwargs = {\n-        **CommonKwargs.__annotations__,\n-    }\n     text_kwargs: TextKwargs = {\n         **TextKwargs.__annotations__,\n     }\n@@ -1245,15 +1255,20 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n             \"images_kwargs\": {},\n             \"audio_kwargs\": {},\n             \"videos_kwargs\": {},\n-            \"common_kwargs\": {},\n         }\n \n         default_kwargs = {\n             \"text_kwargs\": {},\n             \"images_kwargs\": {},\n             \"audio_kwargs\": {},\n             \"videos_kwargs\": {},\n-            \"common_kwargs\": {},\n+        }\n+\n+        map_preprocessor_kwargs = {\n+            \"text_kwargs\": \"tokenizer\",\n+            \"images_kwargs\": \"image_processor\",\n+            \"audio_kwargs\": \"feature_extractor\",\n+            \"videos_kwargs\": \"video_processor\",\n         }\n \n         possible_modality_keywords = {\"text\", \"audio\", \"videos\", \"images\"}\n@@ -1262,8 +1277,22 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n         # get defaults from set model processor kwargs if they exist\n         for modality in default_kwargs:\n             default_kwargs[modality] = ModelProcessorKwargs._defaults.get(modality, {}).copy()\n+            # Some preprocessors define a set of accepted \"valid_kwargs\" (currently only vision).\n+            # In those cases, we donâ€™t declare a `ModalityKwargs` attribute in the TypedDict.\n+            # Instead, we dynamically obtain the kwargs from the preprocessor and merge them\n+            # with the general kwargs set. This ensures consistency between preprocessor and\n+            # processor classes, and helps prevent accidental mismatches.\n+            modality_valid_kwargs = set(ModelProcessorKwargs.__annotations__[modality].__annotations__)\n+            if modality in map_preprocessor_kwargs:\n+                preprocessor = getattr(self, map_preprocessor_kwargs[modality], None)\n+                preprocessor_valid_kwargs = (\n+                    getattr(preprocessor, \"valid_kwargs\", None) if preprocessor is not None else None\n+                )\n+                modality_valid_kwargs.update(\n+                    set(preprocessor_valid_kwargs.__annotations__ if preprocessor_valid_kwargs is not None else [])\n+                )\n             # update defaults with arguments from tokenizer init\n-            for modality_key in ModelProcessorKwargs.__annotations__[modality].__annotations__:\n+            for modality_key in modality_valid_kwargs:\n                 # init with tokenizer init kwargs if necessary\n                 if tokenizer_init_kwargs is not None and modality_key in tokenizer_init_kwargs:\n                     value = (\n@@ -1279,7 +1308,16 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n         # update modality kwargs with passed kwargs\n         non_modality_kwargs = set(kwargs) - set(output_kwargs)\n         for modality, output_kwarg in output_kwargs.items():\n-            for modality_key in ModelProcessorKwargs.__annotations__[modality].__annotations__:\n+            modality_valid_kwargs = set(ModelProcessorKwargs.__annotations__[modality].__annotations__)\n+            if modality in map_preprocessor_kwargs:\n+                preprocessor = getattr(self, map_preprocessor_kwargs[modality], None)\n+                preprocessor_valid_kwargs = (\n+                    getattr(preprocessor, \"valid_kwargs\", None) if preprocessor is not None else None\n+                )\n+                modality_valid_kwargs.update(\n+                    set(preprocessor_valid_kwargs.__annotations__ if preprocessor_valid_kwargs is not None else [])\n+                )\n+            for modality_key in modality_valid_kwargs:\n                 # check if we received a structured kwarg dict or not to handle it correctly\n                 if modality in kwargs:\n                     kwarg_value = kwargs[modality].pop(modality_key, \"__empty__\")\n@@ -1311,17 +1349,18 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n         else:\n             # kwargs is a flat dictionary\n             for key, kwarg in kwargs.items():\n-                if key not in used_keys:\n-                    if key in ModelProcessorKwargs.__annotations__[\"common_kwargs\"].__annotations__:\n-                        output_kwargs[\"common_kwargs\"][key] = kwarg\n-                    elif key not in possible_modality_keywords:\n-                        logger.warning_once(\n-                            f\"Keyword argument `{key}` is not a valid argument for this processor and will be ignored.\"\n-                        )\n+                if key not in used_keys and key not in possible_modality_keywords:\n+                    logger.warning_once(\n+                        f\"Keyword argument `{key}` is not a valid argument for this processor and will be ignored.\"\n+                    )\n+\n+        # For `common_kwargs` just update all modality-specific kwargs with same key/values\n+        common_kwargs = kwargs.get(\"common_kwargs\", {})\n+        common_kwargs.update(ModelProcessorKwargs._defaults.get(\"common_kwargs\", {}))\n+        if common_kwargs:\n+            for kwarg in output_kwargs.values():\n+                kwarg.update(common_kwargs)\n \n-        # all modality-specific kwargs are updated with common kwargs\n-        for kwarg in output_kwargs.values():\n-            kwarg.update(output_kwargs[\"common_kwargs\"])\n         return output_kwargs\n \n     @classmethod"
        },
        {
            "sha": "cd3b9a18b1c5d51f87ae64cffd0b15c7b4edee59",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -913,7 +913,7 @@ def add_special_tokens(\n \n                 Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer\n                 assign the index of the `unk_token` to them).\n-            replace_additional_special_tokens (`bool`, *optional*,, defaults to `True`):\n+            replace_additional_special_tokens (`bool`, *optional*, defaults to `True`):\n                 If `True`, the existing list of additional special tokens will be replaced by the list provided in\n                 `special_tokens_dict`. Otherwise, `self._special_tokens_map[\"additional_special_tokens\"]` is just extended. In the former\n                 case, the tokens will NOT be removed from the tokenizer's full vocabulary - they are only being flagged"
        },
        {
            "sha": "135f20bf4cf96bc77d57c1606f78df3e7fbcacb5",
            "filename": "src/transformers/utils/auto_docstring.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fauto_docstring.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -102,6 +102,13 @@ class ImageProcessorArgs:\n         \"shape\": None,\n     }\n \n+    size_divisor = {\n+        \"description\": \"\"\"\n+    The size by which to make sure both the height and width can be divided.\n+    \"\"\",\n+        \"shape\": None,\n+    }\n+\n     default_to_square = {\n         \"description\": \"\"\"\n     Whether to default to a square image when resizing, if size is an int."
        },
        {
            "sha": "a9420a6710941be3b1e6bd6a2bf64a61aba1251c",
            "filename": "tests/models/got_ocr2/test_image_processing_got_ocr2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/tests%2Fmodels%2Fgot_ocr2%2Ftest_image_processing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/tests%2Fmodels%2Fgot_ocr2%2Ftest_image_processing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgot_ocr2%2Ftest_image_processing_got_ocr2.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -44,7 +44,6 @@ def __init__(\n         do_resize=True,\n         size=None,\n         do_normalize=True,\n-        do_pad=False,\n         image_mean=[0.48145466, 0.4578275, 0.40821073],\n         image_std=[0.26862954, 0.26130258, 0.27577711],\n         do_convert_rgb=True,\n@@ -62,7 +61,6 @@ def __init__(\n         self.do_normalize = do_normalize\n         self.image_mean = image_mean\n         self.image_std = image_std\n-        self.do_pad = do_pad\n         self.do_convert_rgb = do_convert_rgb\n \n     def prepare_image_processor_dict(self):\n@@ -73,7 +71,6 @@ def prepare_image_processor_dict(self):\n             \"image_mean\": self.image_mean,\n             \"image_std\": self.image_std,\n             \"do_convert_rgb\": self.do_convert_rgb,\n-            \"do_pad\": self.do_pad,\n         }\n \n     def expected_output_image_shape(self, images):"
        },
        {
            "sha": "9796d67cc5f64455765f48c51596a5edf2640088",
            "filename": "tests/utils/test_add_new_model_like.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/tests%2Futils%2Ftest_add_new_model_like.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/tests%2Futils%2Ftest_add_new_model_like.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_add_new_model_like.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -473,8 +473,8 @@ def test_phi4_with_all_processors(self):\n             )\n             from ..phi4_multimodal.feature_extraction_phi4_multimodal import Phi4MultimodalFeatureExtractor\n             from ..phi4_multimodal.image_processing_phi4_multimodal_fast import (\n-                Phi4MultimodalFastImageProcessorKwargs,\n                 Phi4MultimodalImageProcessorFast,\n+                Phi4MultimodalImageProcessorKwargs,\n             )\n             from ..phi4_multimodal.modeling_phi4_multimodal import (\n                 Phi4MultimodalAttention,\n@@ -643,7 +643,7 @@ class MyTest2ForCausalLM(Phi4MultimodalForCausalLM):\n                 pass\n \n \n-            class MyTest2FastImageProcessorKwargs(Phi4MultimodalFastImageProcessorKwargs):\n+            class MyTest2ImageProcessorKwargs(Phi4MultimodalImageProcessorKwargs):\n                 pass\n \n "
        },
        {
            "sha": "a566025d2e1cb541b33b2f2a5616abfd7fd76160",
            "filename": "utils/check_modular_conversion.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/utils%2Fcheck_modular_conversion.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/utils%2Fcheck_modular_conversion.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_modular_conversion.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -30,8 +30,8 @@ def process_file(\n     file_type=\"modeling_\",\n     show_diff=True,\n ):\n-    file_name_prefix = file_type.split(\"*\")[0]\n-    file_name_suffix = file_type.split(\"*\")[-1] if \"*\" in file_type else \"\"\n+    file_name_prefix = file_type.split(\".*\")[0]\n+    file_name_suffix = file_type.split(\".*\")[-1] if \".*\" in file_type else \"\"\n     file_path = modular_file_path.replace(\"modular_\", f\"{file_name_prefix}_\").replace(\".py\", f\"{file_name_suffix}.py\")\n     # Read the actual modeling file\n     with open(file_path, \"r\", encoding=\"utf-8\") as modeling_file:"
        },
        {
            "sha": "18c3a729368b7cd157ca68ea1ac43ac62dc4fa7b",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/5339f72b9bf2433de4692699a98440e842b4f11e/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5339f72b9bf2433de4692699a98440e842b4f11e/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=5339f72b9bf2433de4692699a98440e842b4f11e",
            "patch": "@@ -499,6 +499,7 @@ def augmented_dependencies_for_class_node(\n     \"configuration\",\n     \"tokenization\",\n     \"processing\",\n+    \"image_processing.*_fast\",\n     \"image_processing\",\n     \"video_processing\",\n     \"feature_extraction\",\n@@ -538,7 +539,7 @@ def visit_ImportFrom(self, node):\n         to be added (because it will be part of the imports)\"\"\"\n         import_module = self.python_module.code_for_node(node.module)\n         import_statement = \".\" * len(node.relative) + import_module\n-        if re.search(rf\"^\\.({self.match_patterns})_.*\", import_statement):\n+        if re.search(rf\"^\\.({self.match_patterns}).*\", import_statement):\n             for imported_object in node.names:\n                 # If an alias is present, we record it and not the original name\n                 if imported_object.evaluated_alias is not None:\n@@ -1056,10 +1057,11 @@ def replace_class_node(\n     \"Tokenizer\": \"tokenization\",\n     \"Processor\": \"processing\",\n     \"ImageProcessor\": \"image_processing\",\n-    \"ImageProcessorFast\": \"image_processing*_fast\",  # \"*\" indicates where to insert the model name before the \"_fast\" suffix\n+    \"ImageProcessorFast\": \"image_processing.*_fast\",  # \"*\" indicates where to insert the model name before the \"_fast\" suffix\n     \"VideoProcessor\": \"video_processing\",\n     \"VideoProcessorInitKwargs\": \"video_processing\",\n-    \"FastImageProcessorKwargs\": \"image_processing*_fast\",\n+    \"FastImageProcessorKwargs\": \"image_processing.*_fast\",\n+    \"ImageProcessorKwargs\": \"image_processing\",\n     \"FeatureExtractor\": \"feature_extraction\",\n     \"ProcessorKwargs\": \"processing\",\n     \"VideosKwargs\": \"processing\",\n@@ -1208,7 +1210,7 @@ def visit_ImportFrom(self, node: cst.ImportFrom) -> None:\n         if m.matches(node.module, m.Attribute()):\n             for imported_ in node.names:\n                 _import = re.search(\n-                    rf\"(?:transformers\\.models\\.)|(?:\\.\\.\\.models\\.)|(?:\\.\\.)\\w+\\.({self.match_patterns})_.*\",\n+                    rf\"(?:transformers\\.models\\.)|(?:\\.\\.\\.models\\.)|(?:\\.\\.)\\w+\\.({self.match_patterns}).*\",\n                     import_statement,\n                 )\n                 if _import:\n@@ -1257,7 +1259,7 @@ def visit_SimpleStatementLine(self, node):\n                 import_module = self.python_module.code_for_node(node.body[0].module)\n                 import_statement = \".\" * len(node.body[0].relative) + import_module\n                 if not (\n-                    re.search(rf\"(?:transformers\\.models\\.)|(?:\\.\\.)\\w+\\.({self.match_patterns})_.*\", import_statement)\n+                    re.search(rf\"(?:transformers\\.models\\.)|(?:\\.\\.)\\w+\\.({self.match_patterns}).*\", import_statement)\n                     and not any(import_to_skip in import_statement for import_to_skip in IMPORTS_TO_SKIP_IN_MODULAR)\n                 ):\n                     self.imports.append(node)\n@@ -1320,7 +1322,7 @@ def leave_Module(self, node):\n         # Note that we may visit several of the same file types, thus we save them per file type, not file\n         self.imported_objects_per_file = defaultdict(set)\n         for file, mapper in self.visited_modules.items():\n-            file_type = re.search(rf\"^transformers\\.models\\.\\w+\\.({self.match_patterns})_.*\", file).group(1)\n+            file_type = re.search(rf\"^transformers\\.models\\.\\w+\\.({self.match_patterns})\", file).group(1)\n             self.imported_objects_per_file[file_type].update(mapper.objects_imported_from_modeling)\n \n     def merge_model_specific_imports(self, visited_modules):\n@@ -1716,8 +1718,8 @@ def convert_modular_file(modular_file: str) -> dict[str, str]:\n def save_modeling_files(modular_file: str, converted_files: dict[str, str]):\n     \"\"\"Save all the `converted_files` from the `modular_file`.\"\"\"\n     for file_type in converted_files:\n-        file_name_prefix = file_type.split(\"*\")[0]\n-        file_name_suffix = file_type.split(\"*\")[-1] if \"*\" in file_type else \"\"\n+        file_name_prefix = file_type.split(\".*\")[0]\n+        file_name_suffix = file_type.split(\".*\")[-1] if \".*\" in file_type else \"\"\n         new_file_name = modular_file.replace(\"modular_\", f\"{file_name_prefix}_\").replace(\n             \".py\", f\"{file_name_suffix}.py\"\n         )"
        }
    ],
    "stats": {
        "total": 3558,
        "additions": 1574,
        "deletions": 1984
    }
}