{
    "author": "matthewdouglas",
    "message": "Fix new BNB test failures (#35345)",
    "sha": "6b1e86fd4d47a020082b7b4e4327c3b3c890e378",
    "files": [
        {
            "sha": "76094d0fe86272546a6b9aada78f73115157a010",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6b1e86fd4d47a020082b7b4e4327c3b3c890e378/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6b1e86fd4d47a020082b7b4e4327c3b3c890e378/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=6b1e86fd4d47a020082b7b4e4327c3b3c890e378",
            "patch": "@@ -172,7 +172,7 @@ def test_memory_footprint(self):\n         mem_fp16 = self.model_fp16.get_memory_footprint()\n         mem_4bit = self.model_4bit.get_memory_footprint()\n \n-        self.assertAlmostEqual(mem_fp16 / mem_4bit, self.EXPECTED_RELATIVE_DIFFERENCE)\n+        self.assertAlmostEqual(mem_fp16 / mem_4bit, self.EXPECTED_RELATIVE_DIFFERENCE, delta=1e-5)\n         linear = get_some_linear_layer(self.model_4bit)\n         self.assertTrue(linear.weight.__class__ == Params4bit)\n "
        },
        {
            "sha": "e73dd82f34a8cabfcffdf28b02329828466c3300",
            "filename": "tests/quantization/bnb/test_mixed_int8.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/6b1e86fd4d47a020082b7b4e4327c3b3c890e378/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6b1e86fd4d47a020082b7b4e4327c3b3c890e378/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py?ref=6b1e86fd4d47a020082b7b4e4327c3b3c890e378",
            "patch": "@@ -229,7 +229,7 @@ def test_memory_footprint(self):\n         mem_fp16 = self.model_fp16.get_memory_footprint()\n         mem_8bit = self.model_8bit.get_memory_footprint()\n \n-        self.assertAlmostEqual(mem_fp16 / mem_8bit, self.EXPECTED_RELATIVE_DIFFERENCE)\n+        self.assertAlmostEqual(mem_fp16 / mem_8bit, self.EXPECTED_RELATIVE_DIFFERENCE, delta=1e-5)\n         self.assertTrue(get_some_linear_layer(self.model_8bit).weight.__class__ == Int8Params)\n \n     def test_linear_are_8bit(self):\n@@ -938,8 +938,13 @@ class MixedInt8LlamaTest(MixedInt8Test):\n     model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n     EXPECTED_RELATIVE_DIFFERENCE = 1.7869331026479096\n     EXPECTED_OUTPUTS = set()\n+\n+    # Expected on Intel XPU\n     EXPECTED_OUTPUTS.add(\"Hello my name is John Smith and I am a software engineer. I\")\n \n+    # Expected on NVIDIA T4\n+    EXPECTED_OUTPUTS.add(\"Hello my name is John and I am a software engineer. I have\")\n+\n     def test_int8_from_pretrained(self):\n         r\"\"\"\n         Test whether loading a 8bit model from the Hub works as expected"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 7,
        "deletions": 2
    }
}