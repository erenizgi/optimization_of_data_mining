{
    "author": "guangy10",
    "message": "Bert is ExecuTorch compatible (#34424)\n\nCo-authored-by: Guang Yang <guangyang@fb.com>",
    "sha": "5392f12e1614383270ae8df524415a1f6b555773",
    "files": [
        {
            "sha": "aa9835d8cd67c1f68fbf75afcb6354c252d03164",
            "filename": "tests/models/bert/test_modeling_bert.py",
            "status": "modified",
            "additions": 42,
            "deletions": 0,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/5392f12e1614383270ae8df524415a1f6b555773/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5392f12e1614383270ae8df524415a1f6b555773/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py?ref=5392f12e1614383270ae8df524415a1f6b555773",
            "patch": "@@ -16,6 +16,8 @@\n import tempfile\n import unittest\n \n+from packaging import version\n+\n from transformers import AutoTokenizer, BertConfig, is_torch_available\n from transformers.models.auto import get_values\n from transformers.testing_utils import (\n@@ -749,3 +751,43 @@ def test_sdpa_ignored_mask(self):\n             self.assertTrue(\n                 torch.allclose(res_eager.last_hidden_state, res_sdpa.last_hidden_state, atol=1e-5, rtol=1e-4)\n             )\n+\n+    @slow\n+    def test_export(self):\n+        if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n+\n+        bert_model = \"google-bert/bert-base-uncased\"\n+        device = \"cpu\"\n+        attn_implementation = \"sdpa\"\n+        max_length = 512\n+\n+        tokenizer = AutoTokenizer.from_pretrained(bert_model)\n+        inputs = tokenizer(\n+            \"the man worked as a [MASK].\",\n+            return_tensors=\"pt\",\n+            padding=\"max_length\",\n+            max_length=max_length,\n+        )\n+\n+        model = BertForMaskedLM.from_pretrained(\n+            bert_model,\n+            device_map=device,\n+            attn_implementation=attn_implementation,\n+            use_cache=True,\n+        )\n+\n+        logits = model(**inputs).logits\n+        eg_predicted_mask = tokenizer.decode(logits[0, 6].topk(5).indices)\n+        self.assertEqual(eg_predicted_mask.split(), [\"carpenter\", \"waiter\", \"barber\", \"mechanic\", \"salesman\"])\n+\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n+            strict=True,\n+        )\n+\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n+        ep_predicted_mask = tokenizer.decode(result.logits[0, 6].topk(5).indices)\n+        self.assertEqual(eg_predicted_mask, ep_predicted_mask)"
        }
    ],
    "stats": {
        "total": 42,
        "additions": 42,
        "deletions": 0
    }
}