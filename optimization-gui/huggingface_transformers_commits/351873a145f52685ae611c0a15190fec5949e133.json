{
    "author": "fdschmidt93",
    "message": "fix: skip dropout in eval for flash_attn in various models (#33844)\n\n* fix(m2m_100): skip dropout in eval for flash_attn\r\n\r\n* fix(misc): skip dropout in eval for flash attn various models\r\n\r\n* chore(m2m_100): copy flash attn from bart\r\n\r\n* chore: run make fix-copies\r\n\r\n* [run-slow] bart, m2m_100",
    "sha": "351873a145f52685ae611c0a15190fec5949e133",
    "files": [
        {
            "sha": "d38d08a0dd5a5bb765ea5f896441384a873bce02",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=351873a145f52685ae611c0a15190fec5949e133",
            "patch": "@@ -263,7 +263,7 @@ def forward(\n             value,\n             attention_mask,\n             query_len,\n-            dropout=self.dropout,\n+            dropout=self.dropout if self.training else 0.0,\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n             is_causal=self.is_causal,\n         )"
        },
        {
            "sha": "ac10189ecf5b5879842b6385b2b8d0fb222ccfbb",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=351873a145f52685ae611c0a15190fec5949e133",
            "patch": "@@ -400,7 +400,7 @@ def forward(\n             value_states,\n             attention_mask,\n             q_len,\n-            dropout=self.dropout,\n+            dropout=self.dropout if self.training else 0.0,\n             is_causal=self.is_causal,\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n         )"
        },
        {
            "sha": "b6ad74e8c80f1b631f60bf7de806426e4e521d94",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=351873a145f52685ae611c0a15190fec5949e133",
            "patch": "@@ -594,7 +594,7 @@ def forward(\n             value_states,\n             attention_mask,\n             q_len,\n-            dropout=self.dropout,\n+            dropout=self.dropout if self.training else 0.0,\n             is_causal=self.is_causal,\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n         )"
        },
        {
            "sha": "08760a3f4ab238002285fd257ac0ff9eb3ae6f71",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=351873a145f52685ae611c0a15190fec5949e133",
            "patch": "@@ -664,7 +664,7 @@ def forward(\n             value_states,\n             attention_mask,\n             q_len,\n-            dropout=self.dropout,\n+            dropout=self.dropout if self.training else 0.0,\n             is_causal=self.is_causal,\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n         )"
        },
        {
            "sha": "9856eec0c229fa2133d6d22da1aea52d9a65541f",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 43,
            "deletions": 9,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=351873a145f52685ae611c0a15190fec5949e133",
            "patch": "@@ -339,7 +339,14 @@ def forward(\n         return attn_output, attn_weights_reshaped, past_key_value\n \n \n+# Copied from transformers.models.bart.modeling_bart.BartFlashAttention2 with Bart->M2M100\n class M2M100FlashAttention2(M2M100Attention):\n+    \"\"\"\n+    M2M100 flash attention module. This module inherits from `M2M100Attention` as the weights of the module stays\n+    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n+    flash attention and deal with padding tokens in case the input contains any of them.\n+    \"\"\"\n+\n     # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n@@ -361,7 +368,9 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+        # M2M100FlashAttention2 attention does not support output_attentions\n+        if output_attentions:\n+            raise ValueError(\"M2M100FlashAttention2 attention does not support output_attentions\")\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n@@ -412,25 +421,50 @@ def forward(\n         if past_key_value is not None:\n             kv_seq_len += past_key_value[0].shape[-2]\n \n+        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n+        # therefore the input hidden states gets silently casted in float32. Hence, we need\n+        # cast them back in the correct dtype just to be sure everything works as expected.\n+        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n+        # in fp32. (LlamaRMSNorm handles it correctly)\n+\n+        input_dtype = query_states.dtype\n+        if input_dtype == torch.float32:\n+            if torch.is_autocast_enabled():\n+                target_dtype = torch.get_autocast_gpu_dtype()\n+            # Handle the case where the model is quantized\n+            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n+                target_dtype = self.config._pre_quantization_dtype\n+            else:\n+                target_dtype = self.q_proj.weight.dtype\n+\n+            logger.warning_once(\n+                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n+                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n+                f\" {target_dtype}.\"\n+            )\n+\n+            query_states = query_states.to(target_dtype)\n+            key_states = key_states.to(target_dtype)\n+            value_states = value_states.to(target_dtype)\n+\n         attn_output = _flash_attention_forward(\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n             q_len,\n-            dropout=self.dropout,\n-            softmax_scale=None,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+            dropout=self.dropout if self.training else 0.0,\n             is_causal=self.is_causal,\n+            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n         )\n \n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, q_len, self.embed_dim)\n-\n+        attn_output = attn_output.reshape(bsz, q_len, -1)\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights, past_key_value\n \n \n # Copied from transformers.models.bart.modeling_bart.BartSdpaAttention with Bart->M2M100"
        },
        {
            "sha": "ebb325073f93784728457ffe63764bcb29d4e95d",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=351873a145f52685ae611c0a15190fec5949e133",
            "patch": "@@ -397,7 +397,7 @@ def forward(\n             value_states,\n             attention_mask,\n             q_len,\n-            dropout=self.dropout,\n+            dropout=self.dropout if self.training else 0.0,\n             is_causal=self.is_causal,\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n         )"
        },
        {
            "sha": "c9f3b88c68d87a928babf028c5f1f34c4cde83c5",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=351873a145f52685ae611c0a15190fec5949e133",
            "patch": "@@ -430,7 +430,7 @@ def forward(\n             value_states,\n             attention_mask,\n             q_len,\n-            dropout=self.dropout,\n+            dropout=self.dropout if self.training else 0.0,\n             is_causal=self.is_causal,\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n         )"
        },
        {
            "sha": "15cad4072dddab148c02152581e753891a241290",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=351873a145f52685ae611c0a15190fec5949e133",
            "patch": "@@ -446,7 +446,7 @@ def forward(\n             value_states,\n             attention_mask,\n             q_len,\n-            dropout=self.dropout,\n+            dropout=self.dropout if self.training else 0.0,\n             is_causal=self.is_causal,\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n         )"
        },
        {
            "sha": "a5ac1f836385459ccba7816fc31c32e2f362d297",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=351873a145f52685ae611c0a15190fec5949e133",
            "patch": "@@ -325,7 +325,7 @@ def forward(\n             value_states,\n             causal_mask,\n             tgt_len,\n-            dropout=self.dropout,\n+            dropout=self.dropout if self.training else 0.0,\n             is_causal=self.is_causal,\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n         )"
        },
        {
            "sha": "5dfe54e24ac20a94b29d2455ed163dc021d00fda",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=351873a145f52685ae611c0a15190fec5949e133",
            "patch": "@@ -668,7 +668,7 @@ def forward(\n             value_states,\n             attention_mask,\n             q_len,\n-            dropout=self.dropout,\n+            dropout=self.dropout if self.training else 0.0,\n             is_causal=self.is_causal,\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n         )"
        },
        {
            "sha": "d2779fc200f2df70f4d9f1614dd8e6f81d7be2c2",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=351873a145f52685ae611c0a15190fec5949e133",
            "patch": "@@ -700,7 +700,7 @@ def forward(\n             value_states,\n             attention_mask,\n             q_len,\n-            dropout=self.dropout,\n+            dropout=self.dropout if self.training else 0.0,\n             is_causal=self.is_causal,\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n         )"
        },
        {
            "sha": "7bb98434482d35cf8a1dec3cfb943045b8db94da",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=351873a145f52685ae611c0a15190fec5949e133",
            "patch": "@@ -717,7 +717,7 @@ def forward(\n             value_states,\n             attention_mask,\n             q_len,\n-            dropout=self.dropout,\n+            dropout=self.dropout if self.training else 0.0,\n             is_causal=self.is_causal,\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n         )"
        },
        {
            "sha": "d79936ab2b8420fd1d0d3337e0fb01434b61d882",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=351873a145f52685ae611c0a15190fec5949e133",
            "patch": "@@ -764,7 +764,7 @@ def forward(\n             value_states,\n             attention_mask,\n             q_len,\n-            dropout=self.dropout,\n+            dropout=self.dropout if self.training else 0.0,\n             is_causal=self.is_causal,\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n         )"
        },
        {
            "sha": "b10fc258c8ef45d601884d60806577e4ec242266",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/351873a145f52685ae611c0a15190fec5949e133/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=351873a145f52685ae611c0a15190fec5949e133",
            "patch": "@@ -456,7 +456,7 @@ def forward(\n             value_states,\n             causal_mask,\n             tgt_len,\n-            dropout=self.dropout,\n+            dropout=self.dropout if self.training else 0.0,\n             is_causal=self.is_causal,\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n         )"
        }
    ],
    "stats": {
        "total": 78,
        "additions": 56,
        "deletions": 22
    }
}