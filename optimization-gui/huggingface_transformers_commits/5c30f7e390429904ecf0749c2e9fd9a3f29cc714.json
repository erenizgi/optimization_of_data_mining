{
    "author": "ParagEkbote",
    "message": "Update Model Card for Encoder Decoder Model (#39272)\n\n* update model card.\n\n* add back the model contributors for mamba and mamba2.\n\n* update the model card.\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* update batches with correct alignment.\n\n* update examples and remove quantization example.\n\n* update the examples.\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* update example.\n\n* correct the example.\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "5c30f7e390429904ecf0749c2e9fd9a3f29cc714",
    "files": [
        {
            "sha": "f01d4c1a6733bd041c63e2be6a41b7729d599d2f",
            "filename": "docs/source/en/model_doc/encoder-decoder.md",
            "status": "modified",
            "additions": 88,
            "deletions": 84,
            "changes": 172,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c30f7e390429904ecf0749c2e9fd9a3f29cc714/docs%2Fsource%2Fen%2Fmodel_doc%2Fencoder-decoder.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c30f7e390429904ecf0749c2e9fd9a3f29cc714/docs%2Fsource%2Fen%2Fmodel_doc%2Fencoder-decoder.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fencoder-decoder.md?ref=5c30f7e390429904ecf0749c2e9fd9a3f29cc714",
            "patch": "@@ -14,115 +14,88 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Encoder Decoder Models\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n-<img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n-\">\n-<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+        <img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n+        \">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n </div>\n \n-## Overview\n-\n-The [`EncoderDecoderModel`] can be used to initialize a sequence-to-sequence model with any\n-pretrained autoencoding model as the encoder and any pretrained autoregressive model as the decoder.\n-\n-The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation tasks\n-was shown in [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://huggingface.co/papers/1907.12461) by\n-Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n+# Encoder Decoder Models\n \n-After such an [`EncoderDecoderModel`] has been trained/fine-tuned, it can be saved/loaded just like\n-any other models (see the examples for more information).\n+[`EncoderDecoderModel`](https://huggingface.co/papers/1706.03762) initializes a sequence-to-sequence model with any pretrained autoencoder and pretrained autoregressive model. It is effective for sequence generation tasks as demonstrated in [Text Summarization with Pretrained Encoders](https://huggingface.co/papers/1908.08345) which uses [`BertModel`] as the encoder and decoder.\n \n-An application of this architecture could be to leverage two pretrained [`BertModel`] as the encoder\n-and decoder for a summarization model as was shown in: [Text Summarization with Pretrained Encoders](https://huggingface.co/papers/1908.08345) by Yang Liu and Mirella Lapata.\n+> [!TIP]\n+> This model was contributed by [thomwolf](https://huggingface.co/thomwolf) and the TensorFlow/Flax version by [ydshieh](https://huggingface.co/ydshieh).\n+>\n+> Click on the Encoder Decoder models in the right sidebar for more examples of how to apply Encoder Decoder to different language tasks.\n \n-## Randomly initializing `EncoderDecoderModel` from model configurations.\n+The example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`], and from the command line.\n \n-[`EncoderDecoderModel`] can be randomly initialized from an encoder and a decoder config. In the following example, we show how to do this using the default [`BertModel`] configuration for the encoder and the default [`BertForCausalLM`] configuration for the decoder.\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n \n ```python\n->>> from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel\n+from transformers import pipeline\n \n->>> config_encoder = BertConfig()\n->>> config_decoder = BertConfig()\n+summarizer = pipeline(\n+    \"summarization\",\n+    model=\"patrickvonplaten/bert2bert-cnn_dailymail-fp16\",\n+    device=0\n+)\n \n->>> config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n->>> model = EncoderDecoderModel(config=config)\n+text = \"Plants create energy through a process known as photosynthesis. This involves capturing sunlight and converting carbon dioxide and water into glucose and oxygen.\"\n+print(summarizer(text))\n ```\n \n-## Initialising `EncoderDecoderModel` from a pretrained encoder and a pretrained decoder.\n-\n-[`EncoderDecoderModel`] can be initialized from a pretrained encoder checkpoint and a pretrained decoder checkpoint. Note that any pretrained auto-encoding model, *e.g.* BERT, can serve as the encoder and both pretrained auto-encoding models, *e.g.* BERT, pretrained causal language models, *e.g.* GPT2, as well as the pretrained decoder part of sequence-to-sequence models, *e.g.* decoder of BART, can be used as the decoder.\n-Depending on which architecture you choose as the decoder, the cross-attention layers might be randomly initialized.\n-Initializing [`EncoderDecoderModel`] from a pretrained encoder and decoder checkpoint requires the model to be fine-tuned on a downstream task, as has been shown in [the *Warm-starting-encoder-decoder blog post*](https://huggingface.co/blog/warm-starting-encoder-decoder).\n-To do so, the `EncoderDecoderModel` class provides a [`EncoderDecoderModel.from_encoder_decoder_pretrained`] method.\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n \n ```python\n->>> from transformers import EncoderDecoderModel, BertTokenizer\n+import torch  \n+from transformers import AutoModelForCausalLM, AutoTokenizer  \n \n->>> tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n->>> model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"google-bert/bert-base-uncased\", \"google-bert/bert-base-uncased\")\n-```\n+tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/bert2bert-cnn_dailymail-fp16\")\n+model = AutoModelForCausalLM.from_pretrained(\"patrickvonplaten/bert2bert-cnn_dailymail-fp16\", torch_dtype=torch.bfloat16, device_map=\"auto\",attn_implementation=\"sdpa\")  \n \n-## Loading an existing `EncoderDecoderModel` checkpoint and perform inference.\n+text = \"Plants create energy through a process known as photosynthesis. This involves capturing sunlight and converting carbon dioxide and water into glucose and oxygen.\"\n \n-To load fine-tuned checkpoints of the `EncoderDecoderModel` class, [`EncoderDecoderModel`] provides the `from_pretrained(...)` method just like any other model architecture in Transformers.\n+inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n \n-To perform inference, one uses the [`generate`] method, which allows to autoregressively generate text. This method supports various forms of decoding, such as greedy, beam search and multinomial sampling.\n+summary = model.generate(**inputs, max_length=60, num_beams=4, early_stopping=True)\n+print(tokenizer.decode(summary[0], skip_special_tokens=True))\n+```\n \n-```python\n->>> from transformers import AutoTokenizer, EncoderDecoderModel\n-\n->>> # load a fine-tuned seq2seq model and corresponding tokenizer\n->>> model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n->>> tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n-\n->>> # let's perform inference on a long piece of text\n->>> ARTICLE_TO_SUMMARIZE = (\n-...     \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n-...     \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n-...     \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n-... )\n->>> input_ids = tokenizer(ARTICLE_TO_SUMMARIZE, return_tensors=\"pt\").input_ids\n-\n->>> # autoregressively generate summary (uses greedy decoding by default)\n->>> generated_ids = model.generate(input_ids)\n->>> generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n->>> print(generated_text)\n-nearly 800 thousand customers were affected by the shutoffs. the aim is to reduce the risk of wildfires. nearly 800, 000 customers were expected to be affected by high winds amid dry conditions. pg & e said it scheduled the blackouts to last through at least midday tomorrow.\n+</hfoption>\n+<hfoption id=\"transformers CLI\">\n+\n+```bash\n+echo -e \"Plants create energy through a process known as photosynthesis. This involves capturing sunlight and converting carbon dioxide and water into glucose and oxygen.\" | transformers-cli run --task summarization --model \"patrickvonplaten/bert2bert-cnn_dailymail-fp16\" --device 0\n ```\n \n-## Loading a PyTorch checkpoint into `TFEncoderDecoderModel`.\n+</hfoption>\n+</hfoptions>\n \n-[`TFEncoderDecoderModel.from_pretrained`] currently doesn't support initializing the model from a\n-pytorch checkpoint. Passing `from_pt=True` to this method will throw an exception. If there are only pytorch\n-checkpoints for a particular encoder-decoder model, a workaround is:\n+## Notes\n \n-```python\n->>> # a workaround to load from pytorch checkpoint\n->>> from transformers import EncoderDecoderModel, TFEncoderDecoderModel\n+- [`EncoderDecoderModel`] can be initialized using any pretrained encoder and decoder. But depending on the decoder architecture, the cross-attention layers may be randomly initialized.\n \n->>> _model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2bert-cnn_dailymail-fp16\")\n+These models require downstream fine-tuning, as discussed in this [blog post](https://huggingface.co/blog/warm-starting-encoder-decoder). Use [`~EncoderDecoderModel.from_encoder_decoder_pretrained`] to combine encoder and decoder checkpoints.\n \n->>> _model.encoder.save_pretrained(\"./encoder\")\n->>> _model.decoder.save_pretrained(\"./decoder\")\n+```python\n+from transformers import EncoderDecoderModel, BertTokenizer\n \n->>> model = TFEncoderDecoderModel.from_encoder_decoder_pretrained(\n-...     \"./encoder\", \"./decoder\", encoder_from_pt=True, decoder_from_pt=True\n-... )\n->>> # This is only for copying some specific attributes of this particular model.\n->>> model.config = _model.config\n+tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n+model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n+    \"google-bert/bert-base-uncased\", \n+    \"google-bert/bert-base-uncased\"\n+)\n ```\n \n-## Training\n-\n-Once the model is created, it can be fine-tuned similar to BART, T5 or any other encoder-decoder model.\n-As you can see, only 2 inputs are required for the model in order to compute a loss: `input_ids` (which are the\n-`input_ids` of the encoded input sequence) and `labels` (which are the `input_ids` of the encoded\n-target sequence).\n+- Encoder Decoder models can be fine-tuned like BART, T5 or any other encoder-decoder model. Only 2 inputs are required to compute a loss, `input_ids` and `labels`. Refer to this [notebook](https://colab.research.google.com/drive/1WIk2bxglElfZewOHboPFNj8H44_VAyKE?usp=sharing#scrollTo=ZwQIEhKOrJpl) for a more detailed training example.\n \n ```python\n >>> from transformers import BertTokenizer, EncoderDecoderModel\n@@ -147,11 +120,42 @@ target sequence).\n >>> loss = model(input_ids=input_ids, labels=labels).loss\n ```\n \n-Detailed [colab](https://colab.research.google.com/drive/1WIk2bxglElfZewOHboPFNj8H44_VAyKE?usp=sharing#scrollTo=ZwQIEhKOrJpl) for training.\n+- [`EncoderDecoderModel`] can be randomly initialized from an encoder and a decoder config as shown below.\n+\n+```python\n+>>> from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel\n+\n+>>> config_encoder = BertConfig()\n+>>> config_decoder = BertConfig()\n \n-This model was contributed by [thomwolf](https://github.com/thomwolf). This model's TensorFlow and Flax versions\n-were contributed by [ydshieh](https://github.com/ydshieh).\n+>>> config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n+>>> model = EncoderDecoderModel(config=config)\n+```\n \n+- The Encoder Decoder Model can also be used for translation as shown below.\n+\n+```python\n+from transformers import AutoTokenizer, EncoderDecoderModel  \n+\n+# Load a pre-trained translation model  \n+model_name = \"google/bert2bert_L-24_wmt_en_de\" \n+tokenizer = AutoTokenizer.from_pretrained(model_name, pad_token=\"<pad>\", eos_token=\"</s>\", bos_token=\"<s>\")  \n+model = EncoderDecoderModel.from_pretrained(model_name)  \n+\n+# Input sentence to translate  \n+input_text = \"Plants create energy through a process known as\"  \n+\n+# Encode the input text  \n+inputs = tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False).input_ids  \n+\n+# Generate the translated output  \n+outputs = model.generate(inputs)[0]  \n+\n+# Decode the output tokens to get the translated sentence  \n+translated_text = tokenizer.decode(outputs, skip_special_tokens=True)  \n+\n+print(\"Translated text:\", translated_text)  \n+```\n \n ## EncoderDecoderConfig\n "
        },
        {
            "sha": "1e30e9af8b5bc4ac56696ed85bb8c1e8f8eafcc2",
            "filename": "docs/source/en/model_doc/mamba.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c30f7e390429904ecf0749c2e9fd9a3f29cc714/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c30f7e390429904ecf0749c2e9fd9a3f29cc714/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba.md?ref=5c30f7e390429904ecf0749c2e9fd9a3f29cc714",
            "patch": "@@ -28,6 +28,7 @@ You can find all the original Mamba checkpoints under the [State Space Models](h\n \n \n > [!TIP]\n+> This model was contributed by [Molbap](https://huggingface.co/Molbap) and [AntonV](https://huggingface.co/AntonV).\n > Click on the Mamba models in the right sidebar for more examples of how to apply Mamba to different language tasks.\n \n The example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`], and from the command line."
        },
        {
            "sha": "4d7de552d45515c45fa712ba09cd91d25c83a323",
            "filename": "docs/source/en/model_doc/mamba2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c30f7e390429904ecf0749c2e9fd9a3f29cc714/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c30f7e390429904ecf0749c2e9fd9a3f29cc714/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba2.md?ref=5c30f7e390429904ecf0749c2e9fd9a3f29cc714",
            "patch": "@@ -26,6 +26,7 @@ rendered properly in your Markdown viewer.\n You can find all the original Mamba 2 checkpoints under the [State Space Models](https://huggingface.co/state-spaces) organization, but the examples shown below use [mistralai/Mamba-Codestral-7B-v0.1](https://huggingface.co/mistralai/Mamba-Codestral-7B-v0.1) because a Hugging Face implementation isn't supported yet for the original checkpoints.\n \n > [!TIP]\n+> This model was contributed by [ArthurZ](https://huggingface.co/ArthurZ).\n > Click on the Mamba models in the right sidebar for more examples of how to apply Mamba to different language tasks.\n \n The example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`], and from the command line."
        }
    ],
    "stats": {
        "total": 174,
        "additions": 90,
        "deletions": 84
    }
}