{
    "author": "BakerBunker",
    "message": "Fix Qwen3-Omni inference when mixing video and image inputs in one batch (#41741)\n\n* Fix qwen3omni inference when mixing video and image inputs in one batch\n\n* Fix `router_aux_loss_coef`\n\n---------\n\nCo-authored-by: lvyuanjun.lyj <lvyuanjun.lyj@alibaba-inc.com>",
    "sha": "cb6f03fce4d1f096982f222add598c9625bf9d79",
    "files": [
        {
            "sha": "cd257bec0bf27ba775b5f352a13fd3df3fc8b356",
            "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 26,
            "deletions": 19,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb6f03fce4d1f096982f222add598c9625bf9d79/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb6f03fce4d1f096982f222add598c9625bf9d79/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py?ref=cb6f03fce4d1f096982f222add598c9625bf9d79",
            "patch": "@@ -1723,8 +1723,9 @@ def forward(\n             past_key_values=past_key_values,\n         )\n \n-    def _deepstack_process(self, hidden_states, visual_pos_masks, visual_embeds):\n-        visual_pos_masks = visual_pos_masks[..., 0]\n+    def _deepstack_process(\n+        self, hidden_states: torch.Tensor, visual_pos_masks: torch.Tensor, visual_embeds: torch.Tensor\n+    ):\n         visual_pos_masks = visual_pos_masks.to(hidden_states.device)\n         visual_embeds = visual_embeds.to(hidden_states.device, hidden_states.dtype)\n         hidden_states = hidden_states.clone()\n@@ -1859,6 +1860,7 @@ def __init__(self, config):\n         self.rope_deltas = None\n         self.num_experts = config.text_config.num_experts\n         self.num_experts_per_tok = config.text_config.num_experts_per_tok\n+        self.router_aux_loss_coef = config.text_config.router_aux_loss_coef\n         self.post_init()\n \n     def get_input_embeddings(self):\n@@ -2067,6 +2069,7 @@ def forward(\n \n         visual_embeds_multiscale = None\n         visual_pos_masks = None\n+        image_mask, video_mask = None, None\n         # 2. Merge text , audios , image and video\n         if input_features is not None:\n             audio_features = self.get_audio_features(\n@@ -2086,9 +2089,6 @@ def forward(\n             )\n             inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n \n-            visual_pos_masks = image_mask\n-            visual_embeds_multiscale = image_embeds_multiscale\n-\n         if pixel_values_videos is not None:\n             video_embeds, video_embeds_multiscale = self.get_video_features(pixel_values_videos, video_grid_thw)\n \n@@ -2098,20 +2098,27 @@ def forward(\n             )\n             inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n-            if visual_embeds_multiscale is None:\n-                visual_embeds_multiscale = video_embeds_multiscale\n-                visual_pos_masks = video_mask\n-            else:\n-                visual_pos_masks = video_mask | image_mask\n-                visual_embeds_multiscale_joint = ()\n-                image_mask_joint = image_mask[visual_pos_masks]\n-                video_mask_joint = video_mask[visual_pos_masks]\n-                for img_embed, vid_embed in zip(visual_embeds_multiscale, video_embeds_multiscale):\n-                    embed_joint = img_embed.new_zeros(visual_pos_masks.sum(), img_embed.shape[-1])\n-                    embed_joint[image_mask_joint, :] = img_embed\n-                    embed_joint[video_mask_joint, :] = vid_embed\n-                    visual_embeds_multiscale_joint = visual_embeds_multiscale_joint + (embed_joint,)\n-                visual_embeds_multiscale = visual_embeds_multiscale_joint\n+        if image_mask is not None and video_mask is not None:\n+            image_mask = image_mask[..., 0]\n+            video_mask = video_mask[..., 0]\n+            visual_pos_masks = video_mask | image_mask\n+            visual_embeds_multiscale_joint = ()\n+            image_mask_joint = image_mask[visual_pos_masks]\n+            video_mask_joint = video_mask[visual_pos_masks]\n+            for img_embed, vid_embed in zip(image_embeds_multiscale, video_embeds_multiscale):\n+                embed_joint = img_embed.new_zeros(visual_pos_masks.sum(), img_embed.shape[-1])\n+                embed_joint[image_mask_joint, :] = img_embed\n+                embed_joint[video_mask_joint, :] = vid_embed\n+                visual_embeds_multiscale_joint = visual_embeds_multiscale_joint + (embed_joint,)\n+            visual_embeds_multiscale = visual_embeds_multiscale_joint\n+        elif image_mask is not None:\n+            image_mask = image_mask[..., 0]\n+            visual_embeds_multiscale = image_embeds_multiscale\n+            visual_pos_masks = image_mask\n+        elif video_mask is not None:\n+            video_mask = video_mask[..., 0]\n+            visual_embeds_multiscale = video_embeds_multiscale\n+            visual_pos_masks = video_mask\n \n         if feature_attention_mask is not None:\n             audio_feature_lengths = torch.sum(feature_attention_mask, dim=1)"
        },
        {
            "sha": "526169066dc183e89d696cbc0f94388e9f0753c1",
            "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 23,
            "deletions": 21,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb6f03fce4d1f096982f222add598c9625bf9d79/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb6f03fce4d1f096982f222add598c9625bf9d79/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py?ref=cb6f03fce4d1f096982f222add598c9625bf9d79",
            "patch": "@@ -1243,10 +1243,6 @@ def __init__(self, config: Qwen3OmniMoeTextConfig):\n         )\n         self.rotary_emb = Qwen3OmniMoeThinkerTextRotaryEmbedding(config)\n \n-    def _deepstack_process(self, hidden_states, visual_pos_masks, visual_embeds):\n-        visual_pos_masks = visual_pos_masks[..., 0]\n-        return super()._deepstack_process(hidden_states, visual_pos_masks, visual_embeds)\n-\n \n @dataclass\n class Qwen3OmniMoeThinkerCausalLMOutputWithPast(MoeCausalLMOutputWithPast):\n@@ -1274,6 +1270,7 @@ def __init__(self, config):\n         super().__init__(config)\n         self.num_experts = config.text_config.num_experts\n         self.num_experts_per_tok = config.text_config.num_experts_per_tok\n+        self.router_aux_loss_coef = config.text_config.router_aux_loss_coef\n \n     def get_audio_features(\n         self,\n@@ -1342,6 +1339,7 @@ def forward(\n \n         visual_embeds_multiscale = None\n         visual_pos_masks = None\n+        image_mask, video_mask = None, None\n         # 2. Merge text , audios , image and video\n         if input_features is not None:\n             audio_features = self.get_audio_features(\n@@ -1361,9 +1359,6 @@ def forward(\n             )\n             inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n \n-            visual_pos_masks = image_mask\n-            visual_embeds_multiscale = image_embeds_multiscale\n-\n         if pixel_values_videos is not None:\n             video_embeds, video_embeds_multiscale = self.get_video_features(pixel_values_videos, video_grid_thw)\n \n@@ -1373,20 +1368,27 @@ def forward(\n             )\n             inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n-            if visual_embeds_multiscale is None:\n-                visual_embeds_multiscale = video_embeds_multiscale\n-                visual_pos_masks = video_mask\n-            else:\n-                visual_pos_masks = video_mask | image_mask\n-                visual_embeds_multiscale_joint = ()\n-                image_mask_joint = image_mask[visual_pos_masks]\n-                video_mask_joint = video_mask[visual_pos_masks]\n-                for img_embed, vid_embed in zip(visual_embeds_multiscale, video_embeds_multiscale):\n-                    embed_joint = img_embed.new_zeros(visual_pos_masks.sum(), img_embed.shape[-1])\n-                    embed_joint[image_mask_joint, :] = img_embed\n-                    embed_joint[video_mask_joint, :] = vid_embed\n-                    visual_embeds_multiscale_joint = visual_embeds_multiscale_joint + (embed_joint,)\n-                visual_embeds_multiscale = visual_embeds_multiscale_joint\n+        if image_mask is not None and video_mask is not None:\n+            image_mask = image_mask[..., 0]\n+            video_mask = video_mask[..., 0]\n+            visual_pos_masks = video_mask | image_mask\n+            visual_embeds_multiscale_joint = ()\n+            image_mask_joint = image_mask[visual_pos_masks]\n+            video_mask_joint = video_mask[visual_pos_masks]\n+            for img_embed, vid_embed in zip(image_embeds_multiscale, video_embeds_multiscale):\n+                embed_joint = img_embed.new_zeros(visual_pos_masks.sum(), img_embed.shape[-1])\n+                embed_joint[image_mask_joint, :] = img_embed\n+                embed_joint[video_mask_joint, :] = vid_embed\n+                visual_embeds_multiscale_joint = visual_embeds_multiscale_joint + (embed_joint,)\n+            visual_embeds_multiscale = visual_embeds_multiscale_joint\n+        elif image_mask is not None:\n+            image_mask = image_mask[..., 0]\n+            visual_embeds_multiscale = image_embeds_multiscale\n+            visual_pos_masks = image_mask\n+        elif video_mask is not None:\n+            video_mask = video_mask[..., 0]\n+            visual_embeds_multiscale = video_embeds_multiscale\n+            visual_pos_masks = video_mask\n \n         if feature_attention_mask is not None:\n             audio_feature_lengths = torch.sum(feature_attention_mask, dim=1)"
        }
    ],
    "stats": {
        "total": 89,
        "additions": 49,
        "deletions": 40
    }
}