{
    "author": "efsotr",
    "message": "Fix graph break in torch.compile when using FA2 with attention_mask=None and batch size > 1 (#37332)\n\n* Fix graph break in torch.compile when using FA2 with attention_mask=None and batch size > 1\n\n* fix code format\n\n* add test; replace position_ids with query_states becasue position_ids.shape[0] is always 1\n\n* add assert loss is not nan",
    "sha": "3ee72af6b6133be5280a1abcf2cb7b497555f537",
    "files": [
        {
            "sha": "7f3df32943206cdfad407204f5aea1f7a47ec9e6",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ee72af6b6133be5280a1abcf2cb7b497555f537/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ee72af6b6133be5280a1abcf2cb7b497555f537/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=3ee72af6b6133be5280a1abcf2cb7b497555f537",
            "patch": "@@ -385,8 +385,10 @@ def _flash_attention_forward(\n     # If position_ids is provided and check all examples do not contain only 1 sequence, If tensor in increasing\n     # then we probably have one sequence, otherwise it is packed. Additionally check we are in pre-fill/training stage.\n     # Use `flash_attn_varlen_func` to prevent cross-example attention and also allow padding free approach\n-    elif position_ids is not None and (\n-        max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())\n+    elif (\n+        position_ids is not None\n+        and query_states.shape[0] == 1\n+        and (max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all()))\n     ):\n         batch_size = query_states.size(0)\n "
        },
        {
            "sha": "f7183089044ee91f3f260b11bc9736477791b7bb",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 39,
            "deletions": 0,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ee72af6b6133be5280a1abcf2cb7b497555f537/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ee72af6b6133be5280a1abcf2cb7b497555f537/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=3ee72af6b6133be5280a1abcf2cb7b497555f537",
            "patch": "@@ -4082,6 +4082,45 @@ def test_flash_attn_2_fp32_ln(self):\n                     # with attention mask\n                     _ = model(dummy_input, attention_mask=dummy_attention_mask)\n \n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    @slow\n+    def test_flash_attn_2_can_compile_with_attention_mask_None_without_graph_break(self):\n+        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n+\n+        if not hasattr(self, \"_torch_compile_train_cls\"):\n+            self.skipTest(f\"{self.__class__.__name__} doesn't have the attribute `_torch_compile_train_cls`.\")\n+\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        if not is_torch_fp16_available_on_device(torch_device):\n+            self.skipTest(f\"float16 not supported on {torch_device} (on the specific device currently used)\")\n+\n+        torch.compiler.reset()\n+        torch_dtype = torch.float16\n+\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        config._attn_implementation = \"flash_attention_2\"\n+        cls = self._torch_compile_train_cls\n+        model = cls(config).to(device=torch_device, dtype=torch_dtype)\n+\n+        inputs = {\n+            \"input_ids\": torch.randint(low=1, high=model.config.vocab_size, size=(2, 10), device=torch_device),\n+            \"labels\": torch.randint(low=1, high=model.config.vocab_size, size=(2, 10), device=torch_device),\n+        }\n+\n+        model = torch.compile(model, fullgraph=True)\n+        # forward compilation\n+        set_seed(42)\n+        loss = model(**inputs).loss\n+        # backward compilation\n+        loss.backward()\n+\n+        assert not loss.isnan().any()\n+\n     @require_flash_attn\n     @require_torch_gpu\n     @mark.flash_attn_test"
        }
    ],
    "stats": {
        "total": 45,
        "additions": 43,
        "deletions": 2
    }
}