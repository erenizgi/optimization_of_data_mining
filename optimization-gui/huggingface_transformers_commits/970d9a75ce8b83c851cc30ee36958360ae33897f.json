{
    "author": "Sai-Suraj-27",
    "message": "Raise `TypeError` instead of ValueError for invalid types (#38660)\n\n* Raise TypeError instead of ValueError for invalid types.\n\n* Removed un-necessary changes.\n\n* Resolved conflicts\n\n* Code quality\n\n* Fix failing tests.\n\n* Fix failing tests.",
    "sha": "970d9a75ce8b83c851cc30ee36958360ae33897f",
    "files": [
        {
            "sha": "220400c3006b0d0739b6067000a767d8ee02bfd6",
            "filename": "examples/flax/question-answering/run_qa.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/examples%2Fflax%2Fquestion-answering%2Frun_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/examples%2Fflax%2Fquestion-answering%2Frun_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fquestion-answering%2Frun_qa.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -545,7 +545,7 @@ def main():\n \n     # region Tokenizer check: this script requires a fast tokenizer.\n     if not isinstance(tokenizer, PreTrainedTokenizerFast):\n-        raise ValueError(\n+        raise TypeError(\n             \"This example script only works for models that have a fast tokenizer. Check out the big table of models at\"\n             \" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet\"\n             \" this requirement\""
        },
        {
            "sha": "0a1985ce8a8e013429019542769c2282e52a7066",
            "filename": "examples/pytorch/question-answering/run_qa.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/examples%2Fpytorch%2Fquestion-answering%2Frun_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/examples%2Fpytorch%2Fquestion-answering%2Frun_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_qa.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -356,7 +356,7 @@ def main():\n \n     # Tokenizer check: this script requires a fast tokenizer.\n     if not isinstance(tokenizer, PreTrainedTokenizerFast):\n-        raise ValueError(\n+        raise TypeError(\n             \"This example script only works for models that have a fast tokenizer. Check out the big table of models at\"\n             \" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet\"\n             \" this requirement\""
        },
        {
            "sha": "e6b9ee8bbe80b6e4638f111081e906cdfeaf8cbf",
            "filename": "examples/pytorch/token-classification/run_ner.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -398,7 +398,7 @@ def get_label_list(labels):\n \n     # Tokenizer check: this script requires a fast tokenizer.\n     if not isinstance(tokenizer, PreTrainedTokenizerFast):\n-        raise ValueError(\n+        raise TypeError(\n             \"This example script only works for models that have a fast tokenizer. Check out the big table of models at\"\n             \" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet\"\n             \" this requirement\""
        },
        {
            "sha": "2acf7cb0623ad0aa88ba7f5403531b13df77aec1",
            "filename": "examples/tensorflow/question-answering/run_qa.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/examples%2Ftensorflow%2Fquestion-answering%2Frun_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/examples%2Ftensorflow%2Fquestion-answering%2Frun_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fquestion-answering%2Frun_qa.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -377,7 +377,7 @@ def main():\n \n     # region Tokenizer check: this script requires a fast tokenizer.\n     if not isinstance(tokenizer, PreTrainedTokenizerFast):\n-        raise ValueError(\n+        raise TypeError(\n             \"This example script only works for models that have a fast tokenizer. Check out the big table of models at\"\n             \" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet\"\n             \" this requirement\""
        },
        {
            "sha": "deb988171a75eb7260d9a3bc12ea13dc436faeb3",
            "filename": "src/transformers/image_transforms.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fimage_transforms.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fimage_transforms.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_transforms.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -416,7 +416,7 @@ def normalize(\n             The channel dimension format of the input image. If unset, will use the inferred format from the input.\n     \"\"\"\n     if not isinstance(image, np.ndarray):\n-        raise ValueError(\"image must be a numpy array\")\n+        raise TypeError(\"image must be a numpy array\")\n \n     if input_data_format is None:\n         input_data_format = infer_channel_dimension_format(image)"
        },
        {
            "sha": "2b7fd9e756c06365324dc957581cc9b82948e973",
            "filename": "src/transformers/integrations/integration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -768,7 +768,7 @@ def is_enabled(self) -> bool:\n     @classmethod\n     def _missing_(cls, value: Any) -> \"WandbLogModel\":\n         if not isinstance(value, str):\n-            raise ValueError(f\"Expecting to have a string `WANDB_LOG_MODEL` setting, but got {type(value)}\")\n+            raise TypeError(f\"Expecting to have a string `WANDB_LOG_MODEL` setting, but got {type(value)}\")\n         if value.upper() in ENV_VARS_TRUE_VALUES:\n             raise DeprecationWarning(\n                 f\"Setting `WANDB_LOG_MODEL` as {os.getenv('WANDB_LOG_MODEL')} is deprecated and will be removed in \""
        },
        {
            "sha": "46699710983a644b15b4107a80fd92a192a2da85",
            "filename": "src/transformers/modeling_tf_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_tf_utils.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -185,7 +185,7 @@ def wrapped_init(self, *args, **kwargs):\n             else:\n                 initializer(self, config, *args, **kwargs)\n         else:\n-            raise ValueError(\"Must pass either `config` (PretrainedConfig) or `config` (dict)\")\n+            raise TypeError(\"Must pass either `config` (PretrainedConfig) or `config` (dict)\")\n \n         self._config = config\n         self._kwargs = kwargs"
        },
        {
            "sha": "61110e926d3f0c4a7c5e36642858290526b5e91c",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -2088,7 +2088,7 @@ def __init_subclass__(cls, **kwargs):\n     def __init__(self, config: PretrainedConfig, *inputs, **kwargs):\n         super().__init__()\n         if not isinstance(config, PretrainedConfig):\n-            raise ValueError(\n+            raise TypeError(\n                 f\"Parameter config in `{self.__class__.__name__}(config)` should be an instance of class \"\n                 \"`PretrainedConfig`. To create a model from a pretrained model use \"\n                 f\"`model = {self.__class__.__name__}.from_pretrained(PRETRAINED_MODEL_NAME)`\""
        },
        {
            "sha": "f6303b4d382e60716ddebaa86843c5c53afb40df",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -999,7 +999,7 @@ def __call__(\n         if isinstance(text, str):\n             text = [text]\n         elif not isinstance(text, list) and not isinstance(text[0], str):\n-            raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+            raise TypeError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n         if images is not None:\n             image_inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])"
        },
        {
            "sha": "d694e59242383ad677faded64be5174cbb0aa07e",
            "filename": "src/transformers/models/aria/processing_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -120,7 +120,7 @@ def __call__(\n         if isinstance(text, str):\n             text = [text]\n         elif not isinstance(text, list) and not isinstance(text[0], str):\n-            raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+            raise TypeError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n         if images is not None:\n             image_inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])"
        },
        {
            "sha": "c859d4572df76638ab881113ff187e75dd211de0",
            "filename": "src/transformers/models/clip/tokenization_clip_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip_fast.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -81,7 +81,7 @@ def __init__(\n         )\n \n         if not isinstance(self.backend_tokenizer.pre_tokenizer, pre_tokenizers.Sequence):\n-            raise ValueError(\n+            raise TypeError(\n                 \"The `backend_tokenizer` provided does not match the expected format. The CLIP tokenizer has been\"\n                 \" heavily modified from transformers version 4.17.0. You need to convert the tokenizer you are using\"\n                 \" to be compatible with this version.The easiest way to do so is\""
        },
        {
            "sha": "0cfd7a466e016d195bbf1162fbab6dae5bdc2ff6",
            "filename": "src/transformers/models/gemma3/processing_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -97,7 +97,7 @@ def __call__(\n         if isinstance(text, str):\n             text = [text]\n         elif not isinstance(text, list) and not isinstance(text[0], str):\n-            raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+            raise TypeError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n         image_inputs = {}\n         if images is not None:"
        },
        {
            "sha": "d56a24cb57a0865c98a8c9448c5f386ee7f3b1b1",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -831,7 +831,7 @@ def _scale_boxes(boxes, target_sizes):\n     elif isinstance(target_sizes, torch.Tensor):\n         image_height, image_width = target_sizes.unbind(1)\n     else:\n-        raise ValueError(\"`target_sizes` must be a list, tuple or torch.Tensor\")\n+        raise TypeError(\"`target_sizes` must be a list, tuple or torch.Tensor\")\n \n     scale_factor = torch.stack([image_width, image_height, image_width, image_height], dim=1)\n     scale_factor = scale_factor.unsqueeze(1).to(boxes.device)"
        },
        {
            "sha": "695b65629a1b9a46c8d61abf1a6de2beb3451c57",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -307,7 +307,7 @@ def _scale_boxes(boxes, target_sizes):\n     elif isinstance(target_sizes, torch.Tensor):\n         image_height, image_width = target_sizes.unbind(1)\n     else:\n-        raise ValueError(\"`target_sizes` must be a list, tuple or torch.Tensor\")\n+        raise TypeError(\"`target_sizes` must be a list, tuple or torch.Tensor\")\n \n     scale_factor = torch.stack([image_width, image_height, image_width, image_height], dim=1)\n     scale_factor = scale_factor.unsqueeze(1).to(boxes.device)"
        },
        {
            "sha": "e49a79fddee88fa042dc3b219ef6d367bd2cbb1c",
            "filename": "src/transformers/models/grounding_dino/modular_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodular_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodular_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodular_grounding_dino.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -40,7 +40,7 @@ def _scale_boxes(boxes, target_sizes):\n     elif isinstance(target_sizes, torch.Tensor):\n         image_height, image_width = target_sizes.unbind(1)\n     else:\n-        raise ValueError(\"`target_sizes` must be a list, tuple or torch.Tensor\")\n+        raise TypeError(\"`target_sizes` must be a list, tuple or torch.Tensor\")\n \n     scale_factor = torch.stack([image_width, image_height, image_width, image_height], dim=1)\n     scale_factor = scale_factor.unsqueeze(1).to(boxes.device)"
        },
        {
            "sha": "5bac76cb44fc9486349cb7448e35a59449fdf49f",
            "filename": "src/transformers/models/llava/processing_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -145,7 +145,7 @@ def __call__(\n         if isinstance(text, str):\n             text = [text]\n         elif not isinstance(text, list) and not isinstance(text[0], str):\n-            raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+            raise TypeError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n         # try to expand inputs in processing if we have the necessary parts\n         prompt_strings = text"
        },
        {
            "sha": "c5760aa1694cf84eb64cfa67e77defd00c91ead5",
            "filename": "src/transformers/models/llava_next/processing_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -149,7 +149,7 @@ def __call__(\n         if isinstance(text, str):\n             text = [text]\n         elif not isinstance(text, list) and not isinstance(text[0], str):\n-            raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+            raise TypeError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n         prompt_strings = text\n         if image_inputs:"
        },
        {
            "sha": "b50aee5af229202204ee4e2a05fc282c460738a7",
            "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -176,7 +176,7 @@ def __call__(\n         if isinstance(text, str):\n             text = [text]\n         elif not isinstance(text, list) and not isinstance(text[0], str):\n-            raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+            raise TypeError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n         if image_inputs:\n             image_sizes = iter(image_inputs[\"image_sizes\"])"
        },
        {
            "sha": "5a17455a5247336fa5f5c5cfed462edbcc724be1",
            "filename": "src/transformers/models/llava_onevision/processing_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -157,7 +157,7 @@ def __call__(\n         if isinstance(text, str):\n             text = [text]\n         elif not isinstance(text, list) and not isinstance(text[0], str):\n-            raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+            raise TypeError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n         image_inputs = video_inputs = {}\n "
        },
        {
            "sha": "01ae14c9f5cb21c984a0ae54ed678b4f2734390b",
            "filename": "src/transformers/models/mllama/image_processing_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -473,7 +473,7 @@ def to_channel_dimension_format(\n             The image with the channel dimension set to `channel_dim`.\n     \"\"\"\n     if not isinstance(image, np.ndarray):\n-        raise ValueError(f\"Input image must be of type np.ndarray, got {type(image)}\")\n+        raise TypeError(f\"Input image must be of type np.ndarray, got {type(image)}\")\n \n     if input_channel_dim is None:\n         input_channel_dim = infer_channel_dimension_format(image)"
        },
        {
            "sha": "251e3d602b993d8335927cdbeed5518800f2bfb1",
            "filename": "src/transformers/models/myt5/tokenization_myt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fmyt5%2Ftokenization_myt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fmyt5%2Ftokenization_myt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmyt5%2Ftokenization_myt5.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -48,7 +48,7 @@ def __init__(self, rewriting_rules: Union[str, dict[str, str]]):\n             with open(rewriting_rules, \"r\") as f:\n                 rewriting_rules = json.load(f)\n         elif not isinstance(rewriting_rules, dict):\n-            raise ValueError(\n+            raise TypeError(\n                 f\"rewriting_rules should be either a path to json file or a dict, got {type(rewriting_rules)}\"\n             )\n "
        },
        {
            "sha": "0db195ba160ee3a82c820486839a74792802a0fb",
            "filename": "src/transformers/models/owlv2/image_processing_owlv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -86,7 +86,7 @@ def _scale_boxes(boxes, target_sizes):\n     elif isinstance(target_sizes, torch.Tensor):\n         image_height, image_width = target_sizes.unbind(1)\n     else:\n-        raise ValueError(\"`target_sizes` must be a list, tuple or torch.Tensor\")\n+        raise TypeError(\"`target_sizes` must be a list, tuple or torch.Tensor\")\n \n     # for owlv2 image is padded to max size unlike owlvit, that's why we have to scale boxes to max size\n     max_size = torch.max(image_height, image_width)"
        },
        {
            "sha": "4bf9306c564064828f25cef9e0d938c050446d8e",
            "filename": "src/transformers/models/owlvit/image_processing_owlvit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -82,7 +82,7 @@ def _scale_boxes(boxes, target_sizes):\n     elif isinstance(target_sizes, torch.Tensor):\n         image_height, image_width = target_sizes.unbind(1)\n     else:\n-        raise ValueError(\"`target_sizes` must be a list, tuple or torch.Tensor\")\n+        raise TypeError(\"`target_sizes` must be a list, tuple or torch.Tensor\")\n \n     scale_factor = torch.stack([image_width, image_height, image_width, image_height], dim=1)\n     scale_factor = scale_factor.unsqueeze(1).to(boxes.device)"
        },
        {
            "sha": "58c6619ba11c5817f7176150c5c74b1eb05a54a2",
            "filename": "src/transformers/models/phi4_multimodal/processing_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -128,7 +128,7 @@ def __call__(\n         if isinstance(text, str):\n             text = [text]\n         elif not isinstance(text, list) and not isinstance(text[0], str):\n-            raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+            raise TypeError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n         image_token = self.tokenizer.image_token\n         audio_token = self.tokenizer.audio_token"
        },
        {
            "sha": "80fefd3dc91023533875d2c533e662d1747cb88b",
            "filename": "src/transformers/models/phimoe/configuration_phimoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fphimoe%2Fconfiguration_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fphimoe%2Fconfiguration_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fconfiguration_phimoe.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -181,13 +181,11 @@ def __init__(\n             rope_scaling_short_mscale = self.rope_scaling.get(\"short_mscale\", None)\n             rope_scaling_long_mscale = self.rope_scaling.get(\"long_mscale\", None)\n             if not isinstance(rope_scaling_short_mscale, (int, float)):\n-                raise ValueError(\n+                raise TypeError(\n                     f\"`rope_scaling`'s short_mscale field must be a number, got {rope_scaling_short_mscale}\"\n                 )\n             if not isinstance(rope_scaling_long_mscale, (int, float)):\n-                raise ValueError(\n-                    f\"`rope_scaling`'s long_mscale field must be a number, got {rope_scaling_long_mscale}\"\n-                )\n+                raise TypeError(f\"`rope_scaling`'s long_mscale field must be a number, got {rope_scaling_long_mscale}\")\n \n         rope_config_validation(self)\n "
        },
        {
            "sha": "b38c5917d7b45ef2cc2a82e4a29c3e399c0ef7b6",
            "filename": "src/transformers/models/pixtral/processing_pixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -188,7 +188,7 @@ def __call__(\n         if isinstance(text, str):\n             text = [text]\n         elif not isinstance(text, list) and not isinstance(text[0], str):\n-            raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+            raise TypeError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n         # try to expand inputs in processing if we have the necessary parts\n         prompt_strings = text"
        },
        {
            "sha": "8174f22af2da5cacd880ff69045be42471dd5456",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -3247,7 +3247,7 @@ def __init__(\n     ):\n         super().__init__()\n         if not callable(activation):\n-            raise ValueError(\"Activation function must be callable\")\n+            raise TypeError(\"Activation function must be callable\")\n         self.act = activation\n         self.upsample = UpSample1d(up_ratio, up_kernel_size)\n         self.downsample = DownSample1d(down_ratio, down_kernel_size)"
        },
        {
            "sha": "5b501cf7f97cd74ee1ef5450ab51fe366f035bd7",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -3533,7 +3533,7 @@ def __init__(\n     ):\n         super().__init__()\n         if not callable(activation):\n-            raise ValueError(\"Activation function must be callable\")\n+            raise TypeError(\"Activation function must be callable\")\n         self.act = activation\n         self.upsample = UpSample1d(up_ratio, up_kernel_size)\n         self.downsample = DownSample1d(down_ratio, down_kernel_size)"
        },
        {
            "sha": "7591da5af94d9bcbb993439c63abf5d99d5af3ce",
            "filename": "src/transformers/models/sam/image_processing_sam.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -668,7 +668,7 @@ def _post_process_masks_pt(\n             if isinstance(masks[i], np.ndarray):\n                 masks[i] = torch.from_numpy(masks[i])\n             elif not isinstance(masks[i], torch.Tensor):\n-                raise ValueError(\"Input masks should be a list of `torch.tensors` or a list of `np.ndarray`\")\n+                raise TypeError(\"Input masks should be a list of `torch.tensors` or a list of `np.ndarray`\")\n             interpolated_mask = F.interpolate(masks[i], target_image_size, mode=\"bilinear\", align_corners=False)\n             interpolated_mask = interpolated_mask[..., : reshaped_input_sizes[i][0], : reshaped_input_sizes[i][1]]\n             interpolated_mask = F.interpolate(interpolated_mask, original_size, mode=\"bilinear\", align_corners=False)\n@@ -1119,7 +1119,7 @@ def _generate_crop_boxes(\n     \"\"\"\n \n     if isinstance(image, list):\n-        raise ValueError(\"Only one image is allowed for crop generation.\")\n+        raise TypeError(\"Only one image is allowed for crop generation.\")\n     image = to_numpy_array(image)\n     original_size = get_image_size(image, input_data_format)\n "
        },
        {
            "sha": "d7aababe6fd5fa7b1aa026060735e80df12a1da2",
            "filename": "src/transformers/models/video_llava/processing_video_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -152,7 +152,7 @@ def __call__(\n         if isinstance(text, str):\n             text = [text]\n         elif not isinstance(text, list) and not isinstance(text[0], str):\n-            raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+            raise TypeError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n         data = {}\n         if images is not None:"
        },
        {
            "sha": "32c048a8a9d09e553495f7500100d7fdd5534e2a",
            "filename": "src/transformers/models/vitpose/image_processing_vitpose.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -133,7 +133,7 @@ def get_keypoint_predictions(heatmaps: np.ndarray) -> tuple[np.ndarray, np.ndarr\n             Scores (confidence) of the keypoints.\n     \"\"\"\n     if not isinstance(heatmaps, np.ndarray):\n-        raise ValueError(\"Heatmaps should be np.ndarray\")\n+        raise TypeError(\"Heatmaps should be np.ndarray\")\n     if heatmaps.ndim != 4:\n         raise ValueError(\"Heatmaps should be 4-dimensional\")\n "
        },
        {
            "sha": "8952b58208677ec1984b163ff2a34318ff6f8267",
            "filename": "src/transformers/pipelines/text2text_generation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fpipelines%2Ftext2text_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fpipelines%2Ftext2text_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext2text_generation.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -150,7 +150,7 @@ def _parse_and_tokenize(self, *args, truncation):\n             args = (prefix + args[0],)\n             padding = False\n         else:\n-            raise ValueError(\n+            raise TypeError(\n                 f\" `args[0]`: {args[0]} have the wrong format. The should be either of type `str` or type `list`\"\n             )\n         inputs = self.tokenizer(*args, padding=padding, truncation=truncation, return_tensors=self.framework)"
        },
        {
            "sha": "7cda893e3c15cab96aaa51380ab9e9b05e243684",
            "filename": "src/transformers/quantizers/auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fauto.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -252,7 +252,7 @@ def register_config_fn(cls):\n             raise ValueError(f\"Config '{method}' already registered\")\n \n         if not issubclass(cls, QuantizationConfigMixin):\n-            raise ValueError(\"Config must extend QuantizationConfigMixin\")\n+            raise TypeError(\"Config must extend QuantizationConfigMixin\")\n \n         AUTO_QUANTIZATION_CONFIG_MAPPING[method] = cls\n         return cls"
        },
        {
            "sha": "e7d914170e3570427d0b56148ebb4b4b3d926b48",
            "filename": "src/transformers/quantizers/quantizer_bnb_8bit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -214,7 +214,7 @@ def create_quantized_param(\n         old_value = getattr(module, tensor_name)\n \n         if not isinstance(module._parameters[tensor_name], bnb.nn.Int8Params):\n-            raise ValueError(f\"Parameter `{tensor_name}` should only be a `bnb.nn.Int8Params` instance.\")\n+            raise TypeError(f\"Parameter `{tensor_name}` should only be a `bnb.nn.Int8Params` instance.\")\n         if (\n             old_value.device == torch.device(\"meta\")\n             and target_device not in [\"meta\", torch.device(\"meta\")]"
        },
        {
            "sha": "51bcc9a321e79b6a61e0b220bd5d64a69e8380d0",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -2184,7 +2184,7 @@ def _from_pretrained(\n                     added_tokens_decoder[int(idx)] = token\n                     added_tokens_map[str(token)] = token\n                 else:\n-                    raise ValueError(\n+                    raise TypeError(\n                         f\"Found a {token.__class__} in the saved `added_tokens_decoder`, should be a dictionary or an AddedToken instance\"\n                     )\n         else:"
        },
        {
            "sha": "6dc2a114360555e9f940f536b8e853cedf52af23",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -701,7 +701,7 @@ def __init__(\n             os.makedirs(self.args.output_dir, exist_ok=True)\n \n         if not callable(self.data_collator) and callable(getattr(self.data_collator, \"collate_batch\", None)):\n-            raise ValueError(\"The `data_collator` should be a simple callable (function, class with `__call__`).\")\n+            raise TypeError(\"The `data_collator` should be a simple callable (function, class with `__call__`).\")\n \n         if args.max_steps > 0 and args.num_train_epochs > 0:\n             logger.info(\"max_steps is given, it will override any value given in num_train_epochs\")\n@@ -1346,7 +1346,7 @@ def setup_low_rank_optimizer(\n                 raise ValueError(f\"You need to define `optim_target_modules` to use {optimizer_name} optimizers\")\n \n             if not isinstance(args.optim_target_modules, (list, str)):\n-                raise ValueError(\n+                raise TypeError(\n                     f\"`optim_target_modules` must be a list of strings, a regex string, or 'all-linear'. Got: {args.optim_target_modules}\"\n                 )\n "
        },
        {
            "sha": "f56182d2a54388b7b358d8d7e4bf30209a8aa6ef",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -2383,7 +2383,7 @@ def requires(*, backends=()):\n     \"\"\"\n \n     if not isinstance(backends, tuple):\n-        raise ValueError(\"Backends should be a tuple.\")\n+        raise TypeError(\"Backends should be a tuple.\")\n \n     applied_backends = []\n     for backend in backends:"
        },
        {
            "sha": "d6f4dd4663141365a1a4a0e0f2089df35caaed0f",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -1649,7 +1649,7 @@ def post_init(self):\n             from torchao.quantization.quant_api import AOBaseConfig\n \n             if not isinstance(self.quant_type, AOBaseConfig):\n-                raise ValueError(\n+                raise TypeError(\n                     f\"quant_type must be either a string or an AOBaseConfig instance, got {type(self.quant_type)}\"\n                 )\n         else:"
        },
        {
            "sha": "6ac415eb35560affd3bbcd7dcdfe000c34ebd0e0",
            "filename": "src/transformers/video_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fvideo_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/src%2Ftransformers%2Fvideo_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_utils.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -629,7 +629,7 @@ def convert_to_rgb(\n             The channel dimension format of the input video. If unset, will use the inferred format from the input.\n     \"\"\"\n     if not isinstance(video, np.ndarray):\n-        raise ValueError(f\"Video has to be a numpy array to convert to RGB format, but found {type(video)}\")\n+        raise TypeError(f\"Video has to be a numpy array to convert to RGB format, but found {type(video)}\")\n \n     # np.array usually comes with ChannelDimension.LAST so leet's convert it\n     if input_data_format is None:"
        },
        {
            "sha": "f1bd8dd35372d1773df3bbaf98aea23cbf6d51dd",
            "filename": "tests/models/clip/test_tokenization_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/tests%2Fmodels%2Fclip%2Ftest_tokenization_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/tests%2Fmodels%2Fclip%2Ftest_tokenization_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_tokenization_clip.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -174,7 +174,7 @@ def test_offsets_mapping_with_different_add_prefix_space_argument(self):\n     def test_log_warning(self):\n         # Test related to the breaking change introduced in transformers v4.17.0\n         # We need to check that an error in raised when the user try to load a previous version of the tokenizer.\n-        with self.assertRaises(ValueError) as context:\n+        with self.assertRaises(TypeError) as context:\n             self.get_rust_tokenizer(\"robot-test/old-clip-tokenizer\")\n \n         self.assertTrue("
        },
        {
            "sha": "8d94a42100d20ea9ad02c2410db394747d993042",
            "filename": "tests/models/grounding_dino/test_modeling_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -61,7 +61,7 @@ def generate_fake_bounding_boxes(n_boxes):\n     \"\"\"Generate bounding boxes in the format (center_x, center_y, width, height)\"\"\"\n     # Validate the input\n     if not isinstance(n_boxes, int):\n-        raise ValueError(\"n_boxes must be an integer\")\n+        raise TypeError(\"n_boxes must be an integer\")\n     if n_boxes <= 0:\n         raise ValueError(\"n_boxes must be a positive integer\")\n "
        },
        {
            "sha": "852df56f99745fa13ac7a0199f9ebea2dd878b44",
            "filename": "tests/models/sam/test_processor_sam.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/tests%2Fmodels%2Fsam%2Ftest_processor_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/tests%2Fmodels%2Fsam%2Ftest_processor_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_processor_sam.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -154,7 +154,7 @@ def test_post_process_masks(self):\n         self.assertEqual(masks[0].shape, (1, 3, 1764, 2646))\n \n         dummy_masks = [[1, 0], [0, 1]]\n-        with self.assertRaises(ValueError):\n+        with self.assertRaises(TypeError):\n             masks = processor.post_process_masks(dummy_masks, np.array(original_sizes), np.array(reshaped_input_size))\n \n     def test_rle_encoding(self):"
        },
        {
            "sha": "89a642ba606e8d6a22ecf8e06805282cfbe10ba5",
            "filename": "tests/models/sam_hq/test_processor_samhq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/tests%2Fmodels%2Fsam_hq%2Ftest_processor_samhq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/tests%2Fmodels%2Fsam_hq%2Ftest_processor_samhq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam_hq%2Ftest_processor_samhq.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -163,5 +163,5 @@ def test_post_process_masks(self):\n         self.assertEqual(masks[0].shape, (1, 3, 1764, 2646))\n \n         dummy_masks = [[1, 0], [0, 1]]\n-        with self.assertRaises(ValueError):\n+        with self.assertRaises(TypeError):\n             masks = processor.post_process_masks(dummy_masks, np.array(original_sizes), np.array(reshaped_input_size))"
        },
        {
            "sha": "31a91ea8485a7679e48ac546d8c23ccb05b3ac5c",
            "filename": "tests/pipelines/test_pipelines_text2text_generation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/tests%2Fpipelines%2Ftest_pipelines_text2text_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/tests%2Fpipelines%2Ftest_pipelines_text2text_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_text2text_generation.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -81,7 +81,7 @@ def run_pipeline_test(self, generator, _):\n             ],\n         )\n \n-        with self.assertRaises(ValueError):\n+        with self.assertRaises(TypeError):\n             generator(4)\n \n     @require_torch"
        },
        {
            "sha": "313dc099f1438004332f84f36f587e684a1cffba",
            "filename": "tests/test_image_transforms.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970d9a75ce8b83c851cc30ee36958360ae33897f/tests%2Ftest_image_transforms.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970d9a75ce8b83c851cc30ee36958360ae33897f/tests%2Ftest_image_transforms.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_image_transforms.py?ref=970d9a75ce8b83c851cc30ee36958360ae33897f",
            "patch": "@@ -246,7 +246,7 @@ def test_normalize(self):\n \n         # Test that exception is raised if inputs are incorrect\n         # Not a numpy array image\n-        with self.assertRaises(ValueError):\n+        with self.assertRaises(TypeError):\n             normalize(5, 5, 5)\n \n         # Number of mean values != number of channels"
        }
    ],
    "stats": {
        "total": 98,
        "additions": 48,
        "deletions": 50
    }
}