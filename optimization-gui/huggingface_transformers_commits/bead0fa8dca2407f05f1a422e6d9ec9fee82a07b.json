{
    "author": "zucchini-nlp",
    "message": "Cache: slight change in naming (#32421)\n\n* squash\r\n\r\n* codestyle\r\n\r\n* Update src/transformers/cache_utils.py\r\n\r\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\r\n\r\n* propagate changes to all cache classes\r\n\r\n* + whisper\r\n\r\n* fix tests\r\n\r\n* more fixes\r\n\r\n* add deprecation warning\r\n\r\n* fix copies\r\n\r\n* address comments\r\n\r\n* fix mistral also\r\n\r\n* these didn't have \"copied from\"\r\n\r\n---------\r\n\r\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
    "files": [
        {
            "sha": "b7e4ec914baebe728612a7746b36e88fc6d1e1d4",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 27,
            "deletions": 16,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -64,16 +64,27 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         # TODO: deprecate this function in favor of `cache_position`\n         raise NotImplementedError(\"Make sure to implement `get_seq_length` in a subclass.\")\n \n+    # Deprecate in favor of max-cache-shape because we want to be specifc by what we mean with \"max_length\"\n+    # Prev some cache objects didn't have \"max_length\" (SlidingWindowCache or SinkCache) because the cache object technically handles\n+    # infinite amount of tokens. In the codebase what we really need to check is the max capacity of certain cache instances, so\n+    # we change naming to be more explicit\n     def get_max_length(self) -> Optional[int]:\n-        \"\"\"Returns the maximum sequence length of the cached states, if there is any.\"\"\"\n-        raise NotImplementedError(\"Make sure to implement `get_max_length` in a subclass.\")\n+        logger.warning_once(\n+            \"`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. \"\n+            \"Calling `get_max_cache()` will raise error from v4.48\"\n+        )\n+        return self.get_max_cache_shape()\n+\n+    def get_max_cache_shape(self) -> Optional[int]:\n+        \"\"\"Returns the maximum sequence length (i.e. max capacity) of the cache object\"\"\"\n+        raise NotImplementedError(\"Make sure to implement `get_max_cache_shape` in a subclass.\")\n \n     def get_usable_length(self, new_seq_length: int, layer_idx: Optional[int] = 0) -> int:\n         \"\"\"Given the sequence length of the new inputs, returns the usable length of the cache.\"\"\"\n         # Cache without size limit -> all cache is usable\n         # Cache with size limit -> if the length cache plus the length of the new inputs is larger the maximum cache\n         #   length, we will need to evict part of the cache (and thus not all cache is usable)\n-        max_length = self.get_max_length()\n+        max_length = self.get_max_cache_shape()\n         previous_seq_length = self.get_seq_length(layer_idx)\n         if max_length is not None and previous_seq_length + new_seq_length > max_length:\n             return max_length - new_seq_length\n@@ -449,8 +460,8 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         layer_seq_length = self.key_cache[layer_idx].shape[-2] if not is_empty_layer else 0\n         return layer_seq_length\n \n-    def get_max_length(self) -> Optional[int]:\n-        \"\"\"Returns the maximum sequence length of the cached states. DynamicCache does not have a maximum length.\"\"\"\n+    def get_max_cache_shape(self) -> Optional[int]:\n+        \"\"\"Returns the maximum sequence length of the cache object. DynamicCache does not have a maximum length.\"\"\"\n         return None\n \n     def to_legacy_cache(self) -> Tuple[Tuple[torch.Tensor], Tuple[torch.Tensor]]:\n@@ -915,6 +926,8 @@ class SinkCache(Cache):\n         ```\n     \"\"\"\n \n+    is_sliding = True\n+\n     def __init__(self, window_length: int, num_sink_tokens: int) -> None:\n         super().__init__()\n         self.key_cache: List[torch.Tensor] = []\n@@ -968,8 +981,8 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n             return 0\n         return self.key_cache[layer_idx].shape[-2]\n \n-    def get_max_length(self) -> Optional[int]:\n-        \"\"\"Returns the maximum sequence length of the cached states.\"\"\"\n+    def get_max_cache_shape(self) -> Optional[int]:\n+        \"\"\"Returns the maximum sequence length of the cache object, in case of SinkCache it is the window length.\"\"\"\n         return self.window_length\n \n     def update(\n@@ -1221,8 +1234,7 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         # TODO: deprecate this function in favor of `cache_position`\n         return (self.key_cache[layer_idx][0, 0].any(dim=-1)).sum()\n \n-    def get_max_length(self) -> Optional[int]:\n-        \"\"\"Returns the maximum sequence length of the cached states.\"\"\"\n+    def get_max_cache_shape(self) -> Optional[int]:\n         return self.max_cache_len\n \n     def reset(self):\n@@ -1286,6 +1298,8 @@ class SlidingWindowCache(StaticCache):\n         ```\n     \"\"\"\n \n+    is_sliding = True\n+\n     # TODO (joao): remove `=None` in non-optional arguments in v4.46. Remove from `OBJECTS_TO_IGNORE` as well.\n     def __init__(\n         self,\n@@ -1361,9 +1375,8 @@ def update(\n \n         return k_out, v_out\n \n-    def get_max_length(self) -> Optional[int]:\n-        # in theory there is no limit because the sliding window size is fixed no matter how long the sentence is\n-        return None\n+    def get_max_cache_shape(self) -> Optional[int]:\n+        return self.max_cache_len\n \n     def reset(self):\n         for layer_idx in range(len(self.key_cache)):\n@@ -1712,9 +1725,7 @@ def update(\n             k_out.shape[2],\n         )\n \n-    def get_max_length(self) -> Optional[int]:\n-        # in theory there is no limit because the sliding window size is fixed\n-        # no matter how long the sentence is\n+    def get_max_cache_shape(self) -> Optional[int]:\n         return self.max_cache_len\n \n     def get_seq_length(self, layer_idx: Optional[int] = 0):\n@@ -2045,7 +2056,7 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         # TODO(gante): Remove this.\n         return self._seen_tokens\n \n-    def get_max_length(self) -> Optional[int]:\n+    def get_max_cache_shape(self) -> Optional[int]:\n         \"\"\"Returns the maximum sequence length of the cached states.\"\"\"\n \n         return self.max_cache_len"
        },
        {
            "sha": "3abb5bae1a94227e0c3ca5af56002a0476a629df",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -417,7 +417,7 @@ def prepare_inputs_for_generation(\n                 attention_mask = causal_mask_creation_function(\n                     attention_mask,\n                     sequence_length=sequence_length,\n-                    target_length=past_key_values.get_max_length(),\n+                    target_length=past_key_values.get_max_cache_shape(),\n                     dtype=self.get_output_embeddings().weight.dtype,\n                     device=device,\n                     cache_position=cache_position,"
        },
        {
            "sha": "a8e01b4ed7c04c349a15de7ff3c9066c340afc4e",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -763,7 +763,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]"
        },
        {
            "sha": "fd76c0b11522674a3683bc9281356c3a74248bd5",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -1402,7 +1402,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]"
        },
        {
            "sha": "a6b39347a6ec1a76c698f04ea300db21bbb4aaf1",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -606,7 +606,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -772,7 +772,7 @@ def prepare_inputs_for_generation(\n             attention_mask = self.transformer._prepare_4d_causal_attention_mask_with_cache_position(\n                 attention_mask,\n                 sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_length(),\n+                target_length=past_key_values.get_max_cache_shape(),\n                 dtype=self.lm_head.weight.dtype,\n                 device=device,\n                 cache_position=cache_position,"
        },
        {
            "sha": "a5d3721f5bdb03e0b75cf8a89444a5a03ad9745e",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -975,7 +975,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]"
        },
        {
            "sha": "ef81e43d0294f0a4f2699f977711702b8181485b",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -1136,7 +1136,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]"
        },
        {
            "sha": "f48accab44bfc2be3c8c8398778182715bdf9062",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -1137,7 +1137,7 @@ def _update_causal_mask(\n         min_dtype = torch.finfo(dtype).min\n         batch_size, sequence_length, _ = input_tensor.shape\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]"
        },
        {
            "sha": "8fb34e01b8ea1a7840537b44c4ed73a73c884d21",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -891,7 +891,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]"
        },
        {
            "sha": "8498d5525ef2154750f5f7b0011510acd7cf99a9",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -880,7 +880,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if isinstance(past_key_values, HybridCache):\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = attention_mask.shape[-1] if attention_mask is not None else input_tensor.shape[1]\n \n@@ -1145,7 +1145,7 @@ def prepare_inputs_for_generation(\n             attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n                 attention_mask,\n                 sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_length(),\n+                target_length=past_key_values.get_max_cache_shape(),\n                 dtype=self.lm_head.weight.dtype,\n                 device=device,\n                 cache_position=cache_position,"
        },
        {
            "sha": "f75bcdff0d129df4dbb3928e3d62e8708e9f84a4",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -718,7 +718,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if isinstance(past_key_values, HybridCache):\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = attention_mask.shape[-1] if attention_mask is not None else input_tensor.shape[1]\n \n@@ -887,7 +887,7 @@ def prepare_inputs_for_generation(\n             attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n                 attention_mask,\n                 sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_length(),\n+                target_length=past_key_values.get_max_cache_shape(),\n                 dtype=self.lm_head.weight.dtype,\n                 device=device,\n                 cache_position=cache_position,"
        },
        {
            "sha": "efff87fae6daa4ed7b7c33a98fd2c080100d887a",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -820,7 +820,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -986,7 +986,7 @@ def prepare_inputs_for_generation(\n             attention_mask = self.transformer._prepare_4d_causal_attention_mask_with_cache_position(\n                 attention_mask,\n                 sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_length(),\n+                target_length=past_key_values.get_max_cache_shape(),\n                 dtype=self.lm_head.weight.dtype,\n                 device=device,\n                 cache_position=cache_position,"
        },
        {
            "sha": "f4636db0a97b44fb3e7a44574320fed54a1bd3be",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -1017,7 +1017,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]"
        },
        {
            "sha": "b618f531e52f66a83c7a8fdf830072ff669acd5f",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -721,7 +721,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]"
        },
        {
            "sha": "ce03e10e96c6dc6ceea3e09ee7f7982229768ecc",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -915,7 +915,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -1116,7 +1116,7 @@ def prepare_inputs_for_generation(\n             attention_mask = self.transformer._prepare_4d_causal_attention_mask_with_cache_position(\n                 attention_mask,\n                 sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_length(),\n+                target_length=past_key_values.get_max_cache_shape(),\n                 dtype=self.lm_head.weight.dtype,\n                 device=device,\n                 cache_position=cache_position,"
        },
        {
            "sha": "f9a3538f488a543c3528f38badbaff12b13e04b7",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -1447,7 +1447,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]"
        },
        {
            "sha": "635eb9b9e15721f9676af80348c2010dd64ad789",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -1663,7 +1663,7 @@ def prepare_inputs_for_generation(\n         if past_key_values is not None:\n             # Past key values are always initialized with a `Cache` object -> no need for if-else anymore\n             past_length = past_key_values.get_seq_length()\n-            max_cache_length = past_key_values.get_max_length()\n+            max_cache_length = past_key_values.get_max_cache_shape()\n \n             # Keep only the unprocessed tokens:\n             # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where"
        },
        {
            "sha": "fe3067a145314aa526d5acf3cc0ec561f8828ac8",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -1251,7 +1251,7 @@ def prepare_inputs_for_generation(\n         if past_key_values is not None:\n             # Past key values are always initialized with a `Cache` object -> no need for if-else anymore\n             past_length = past_key_values.get_seq_length()\n-            max_cache_length = past_key_values.get_max_length()\n+            max_cache_length = past_key_values.get_max_cache_shape()\n \n             # Keep only the unprocessed tokens:\n             # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where"
        },
        {
            "sha": "ba219f40176fce97969af1bccedf5562adc4bf54",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -1117,7 +1117,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]"
        },
        {
            "sha": "dde017bbb92797c5391888aa056f00c564f4a4a4",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -1011,7 +1011,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]"
        },
        {
            "sha": "b82c90cf2b9e0660a74c7bb5e1129f2060e075cd",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -1060,7 +1060,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -1078,6 +1078,7 @@ def _update_causal_mask(\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n+\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None"
        },
        {
            "sha": "35703fcf35f68a1e5180ec5a5b11aa7ffe580fa2",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -895,12 +895,9 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n-        # SlidingWindowCache\n-        if using_sliding_window_cache:\n-            target_length = max(sequence_length, self.config.sliding_window)\n-        # StaticCache\n-        elif using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+        # SlidingWindowCache or StaticCache\n+        if using_sliding_window_cache or using_static_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n         # DynamicCache or no cache\n         else:\n             target_length = ("
        },
        {
            "sha": "2a2410e3accef253e7dd750f9b8ebcac0d98294e",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -1101,7 +1101,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]"
        },
        {
            "sha": "46d9ddaeb970b2f11c9ef47f14f865d7d3b266dd",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 12,
            "deletions": 10,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -25,7 +25,7 @@\n \n from ... import PreTrainedModel\n from ...activations import ACT2FN\n-from ...cache_utils import Cache\n+from ...cache_utils import Cache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -1709,6 +1709,7 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,\n@@ -1726,11 +1727,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_static_cache = isinstance(past_key_values, StaticCache)\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        # TODO: we have only SDPA currently and there's a bug when attn-bias is passed. Need to add eager attn and return the line\n-        # self.config._attn_implementation == \"sdpa\" and\n-        if self.config._attn_implementation == \"sdpa\" and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -1740,13 +1740,15 @@ def _update_causal_mask(\n                 return None\n \n         dtype, device = input_tensor.dtype, input_tensor.device\n-\n         sequence_length = input_tensor.shape[1]\n-        target_length = (\n-            attention_mask.shape[-1]\n-            if isinstance(attention_mask, torch.Tensor)\n-            else past_seen_tokens + sequence_length + 1\n-        )\n+        if using_static_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n \n         # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n         causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position("
        },
        {
            "sha": "7d0390adc3c06f2da4efbe9130b8379ca450a81b",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -888,7 +888,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]"
        },
        {
            "sha": "7ab54146c9740bb19be0d7938f520d0292a44e98",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -930,7 +930,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]"
        },
        {
            "sha": "8c29f89ff3e7ea5732b40755e5d7d3b501a7f904",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -1089,7 +1089,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]"
        },
        {
            "sha": "7ae3469a4c93997566c9276b3a3eb8fc3e8eacc0",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -757,7 +757,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]"
        },
        {
            "sha": "3f770c9ec00b9b5f419688e4044869eb1f3266db",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -1048,7 +1048,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]"
        },
        {
            "sha": "ba5e8fec60d6123b88eb3bf9291835297fcca887",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -1069,7 +1069,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -1364,7 +1364,7 @@ def prepare_inputs_for_generation(\n             attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n                 attention_mask,\n                 sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_length(),\n+                target_length=past_key_values.get_max_cache_shape(),\n                 dtype=self.lm_head.weight.dtype,\n                 device=device,\n                 cache_position=cache_position,"
        },
        {
            "sha": "208772dac34402a5591ac2809245bd94709624ed",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -1236,7 +1236,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -1553,7 +1553,7 @@ def prepare_inputs_for_generation(\n             attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n                 attention_mask,\n                 sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_length(),\n+                target_length=past_key_values.get_max_cache_shape(),\n                 dtype=self.lm_head.weight.dtype,\n                 device=device,\n                 cache_position=cache_position,"
        },
        {
            "sha": "8f1226cdd8788ab84e664c8404d170618e299d1e",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -985,10 +985,9 @@ def _update_causal_mask(\n                 return None\n \n         dtype, device = input_tensor.dtype, input_tensor.device\n-\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]"
        },
        {
            "sha": "4e2605a6a5b7d5218d77d22a366f4510487d6634",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -1168,7 +1168,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]"
        },
        {
            "sha": "d26e32332e541204a5c9429c1a598f7fd6c4546c",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -1250,7 +1250,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -1815,7 +1815,7 @@ def prepare_inputs_for_generation(\n             attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n                 attention_mask,\n                 sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_length(),\n+                target_length=past_key_values.get_max_cache_shape(),\n                 dtype=self.lm_head.weight.dtype,\n                 device=device,\n                 cache_position=cache_position,"
        },
        {
            "sha": "fe3ad6498172a939c9c8f4422df30be8955c9d6e",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -1032,7 +1032,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]"
        },
        {
            "sha": "f7f5d7d18862c686438f09d4e2e2027d454d817d",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -961,7 +961,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]"
        },
        {
            "sha": "079965fc174a6334e8fad778a105dba81d394344",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -1399,7 +1399,7 @@ def _update_causal_mask(\n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -1867,7 +1867,7 @@ def prepare_inputs_for_generation(\n             decoder_attention_mask = self.get_decoder()._prepare_4d_causal_attention_mask_with_cache_position(\n                 decoder_attention_mask,\n                 sequence_length=sequence_length,\n-                target_length=past_key_values.self_attention_cache.get_max_length(),\n+                target_length=past_key_values.self_attention_cache.get_max_cache_shape(),\n                 dtype=self.proj_out.weight.dtype,\n                 device=decoder_input_ids.device,\n                 cache_position=cache_position,"
        }
    ],
    "stats": {
        "total": 162,
        "additions": 86,
        "deletions": 76
    }
}