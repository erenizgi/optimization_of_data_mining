{
    "author": "yonigozlan",
    "message": "Change Qwen2_VL image processors to have init and call accept the same kwargs (#36207)\n\nChange qwen2VL image processors to have init and call accept the same kwargs",
    "sha": "1c287aecfcf94f569eb8740be5be43bda6750682",
    "files": [
        {
            "sha": "d115b1d062f811fc18226d9150b76e2c12b2020e",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 85,
            "deletions": 21,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c287aecfcf94f569eb8740be5be43bda6750682/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c287aecfcf94f569eb8740be5be43bda6750682/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py?ref=1c287aecfcf94f569eb8740be5be43bda6750682",
            "patch": "@@ -91,6 +91,8 @@ class Qwen2VLImageProcessor(BaseImageProcessor):\n     Args:\n         do_resize (`bool`, *optional*, defaults to `True`):\n             Whether to resize the image's (height, width) dimensions.\n+        size (`Dict[str, int]`, *optional*, defaults to `{\"shortest_edge\": 56 * 56, \"longest_edge\": 28 * 28 * 1280}`):\n+            Size of the image after resizing. `shortest_edge` and `longest_edge` keys must be present.\n         resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n             Resampling filter to use when resizing the image.\n         do_rescale (`bool`, *optional*, defaults to `True`):\n@@ -122,46 +124,62 @@ class Qwen2VLImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n+        size: Dict[str, int] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         do_convert_rgb: bool = True,\n-        min_pixels: int = 56 * 56,\n-        max_pixels: int = 28 * 28 * 1280,\n+        min_pixels: int = None,\n+        max_pixels: int = None,\n         patch_size: int = 14,\n         temporal_patch_size: int = 2,\n         merge_size: int = 2,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n+        if size is not None and (\"shortest_edge\" not in size or \"longest_edge\" not in size):\n+            raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n+        else:\n+            size = {\"shortest_edge\": 56 * 56, \"longest_edge\": 28 * 28 * 1280}\n+        # backward compatibility: override size with min_pixels and max_pixels if they are provided\n+        if min_pixels is not None:\n+            size[\"shortest_edge\"] = min_pixels\n+        if max_pixels is not None:\n+            size[\"longest_edge\"] = max_pixels\n+        self.min_pixels = size[\"shortest_edge\"]\n+        self.max_pixels = size[\"longest_edge\"]\n+        self.size = size\n+\n         self.do_resize = do_resize\n         self.resample = resample\n         self.do_rescale = do_rescale\n         self.rescale_factor = rescale_factor\n         self.do_normalize = do_normalize\n         self.image_mean = image_mean if image_mean is not None else OPENAI_CLIP_MEAN\n         self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n-        self.min_pixels = min_pixels\n-        self.max_pixels = max_pixels\n+\n         self.patch_size = patch_size\n         self.temporal_patch_size = temporal_patch_size\n         self.merge_size = merge_size\n-        self.size = {\"shortest_edge\": min_pixels, \"longest_edge\": max_pixels}\n         self.do_convert_rgb = do_convert_rgb\n \n     def _preprocess(\n         self,\n         images: Union[ImageInput, VideoInput],\n         do_resize: bool = None,\n+        size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n         do_rescale: bool = None,\n         rescale_factor: float = None,\n         do_normalize: bool = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n+        patch_size: int = None,\n+        temporal_patch_size: int = None,\n+        merge_size: int = None,\n         do_convert_rgb: bool = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -176,6 +194,8 @@ def _preprocess(\n                 Optional list of dictionaries containing additional information about vision inputs.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+                Size of the image after resizing. `shortest_edge` and `longest_edge` keys must be present.\n             resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n                 Resampling filter to use if resizing the image. This can be one of the `PILImageResampling` enums.\n             do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n@@ -188,6 +208,12 @@ def _preprocess(\n                 Mean to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n             image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n                 Standard deviation to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n+            patch_size (`int`, *optional*, defaults to `self.patch_size`):\n+                The spacial patch size of the vision encoder.\n+            temporal_patch_size (`int`, *optional*, defaults to `self.temporal_patch_size`):\n+                The temporal patch size of the vision encoder.\n+            merge_size (`int`, *optional*, defaults to `self.merge_size`):\n+                The merge size of the vision encoder to llm encoder.\n             do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n                 Whether to convert the image to RGB.\n             data_format (`ChannelDimension`, *optional*, defaults to `ChannelDimension.FIRST`):\n@@ -226,9 +252,9 @@ def _preprocess(\n                 resized_height, resized_width = smart_resize(\n                     height,\n                     width,\n-                    factor=self.patch_size * self.merge_size,\n-                    min_pixels=self.min_pixels,\n-                    max_pixels=self.max_pixels,\n+                    factor=patch_size * merge_size,\n+                    min_pixels=size[\"shortest_edge\"],\n+                    max_pixels=size[\"longest_edge\"],\n                 )\n                 image = resize(\n                     image, size=(resized_height, resized_width), resample=resample, input_data_format=input_data_format\n@@ -248,26 +274,26 @@ def _preprocess(\n         patches = np.array(processed_images)\n         if data_format == ChannelDimension.LAST:\n             patches = patches.transpose(0, 3, 1, 2)\n-        if patches.shape[0] % self.temporal_patch_size != 0:\n-            repeats = np.repeat(patches[-1][np.newaxis], self.temporal_patch_size - 1, axis=0)\n+        if patches.shape[0] % temporal_patch_size != 0:\n+            repeats = np.repeat(patches[-1][np.newaxis], temporal_patch_size - 1, axis=0)\n             patches = np.concatenate([patches, repeats], axis=0)\n         channel = patches.shape[1]\n-        grid_t = patches.shape[0] // self.temporal_patch_size\n-        grid_h, grid_w = resized_height // self.patch_size, resized_width // self.patch_size\n+        grid_t = patches.shape[0] // temporal_patch_size\n+        grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n         patches = patches.reshape(\n             grid_t,\n-            self.temporal_patch_size,\n+            temporal_patch_size,\n             channel,\n-            grid_h // self.merge_size,\n-            self.merge_size,\n-            self.patch_size,\n-            grid_w // self.merge_size,\n-            self.merge_size,\n-            self.patch_size,\n+            grid_h // merge_size,\n+            merge_size,\n+            patch_size,\n+            grid_w // merge_size,\n+            merge_size,\n+            patch_size,\n         )\n         patches = patches.transpose(0, 3, 6, 4, 7, 2, 1, 5, 8)\n         flatten_patches = patches.reshape(\n-            grid_t * grid_h * grid_w, channel * self.temporal_patch_size * self.patch_size * self.patch_size\n+            grid_t * grid_h * grid_w, channel * temporal_patch_size * patch_size * patch_size\n         )\n \n         return flatten_patches, (grid_t, grid_h, grid_w)\n@@ -278,12 +304,17 @@ def preprocess(\n         videos: VideoInput = None,\n         do_resize: bool = None,\n         size: Dict[str, int] = None,\n+        min_pixels: int = None,\n+        max_pixels: int = None,\n         resample: PILImageResampling = None,\n         do_rescale: bool = None,\n         rescale_factor: float = None,\n         do_normalize: bool = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n+        patch_size: int = None,\n+        temporal_patch_size: int = None,\n+        merge_size: int = None,\n         do_convert_rgb: bool = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n@@ -316,6 +347,16 @@ def preprocess(\n             image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n                 Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n                 `True`.\n+            min_pixels (`int`, *optional*, defaults to `self.min_pixels`):\n+                The min pixels of the image to resize the image.\n+            max_pixels (`int`, *optional*, defaults to `self.max_pixels`):\n+                The max pixels of the image to resize the image.\n+            patch_size (`int`, *optional*, defaults to `self.patch_size`):\n+                The spacial patch size of the vision encoder.\n+            temporal_patch_size (`int`, *optional*, defaults to `self.temporal_patch_size`):\n+                The temporal patch size of the vision encoder.\n+            merge_size (`int`, *optional*, defaults to `self.merge_size`):\n+                The merge size of the vision encoder to llm encoder.\n             do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n                 Whether to convert the image to RGB.\n             return_tensors (`str` or `TensorType`, *optional*):\n@@ -338,14 +379,29 @@ def preprocess(\n                 - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n \n         \"\"\"\n+        if size is not None:\n+            if \"shortest_edge\" not in size or \"longest_edge\" not in size:\n+                raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n+            min_pixels = size[\"shortest_edge\"]\n+        else:\n+            size = self.size\n+        # backward compatibility: override size with min_pixels and max_pixels if they are provided\n+        if min_pixels is not None:\n+            size[\"shortest_edge\"] = min_pixels\n+        if max_pixels is not None:\n+            size[\"longest_edge\"] = max_pixels\n+\n         do_resize = do_resize if do_resize is not None else self.do_resize\n-        size = size if size is not None else self.size\n+\n         resample = resample if resample is not None else self.resample\n         do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n         rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n         do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n         image_mean = image_mean if image_mean is not None else self.image_mean\n         image_std = image_std if image_std is not None else self.image_std\n+        patch_size = patch_size if patch_size is not None else self.patch_size\n+        temporal_patch_size = temporal_patch_size if temporal_patch_size is not None else self.temporal_patch_size\n+        merge_size = merge_size if merge_size is not None else self.merge_size\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n \n         if images is not None:\n@@ -375,12 +431,16 @@ def preprocess(\n                 patches, image_grid_thw = self._preprocess(\n                     image,\n                     do_resize=do_resize,\n+                    size=size,\n                     resample=resample,\n                     do_rescale=do_rescale,\n                     rescale_factor=rescale_factor,\n                     do_normalize=do_normalize,\n                     image_mean=image_mean,\n                     image_std=image_std,\n+                    patch_size=patch_size,\n+                    temporal_patch_size=temporal_patch_size,\n+                    merge_size=merge_size,\n                     data_format=data_format,\n                     do_convert_rgb=do_convert_rgb,\n                     input_data_format=input_data_format,\n@@ -397,12 +457,16 @@ def preprocess(\n                 patches, video_grid_thw = self._preprocess(\n                     images,\n                     do_resize=do_resize,\n+                    size=size,\n                     resample=resample,\n                     do_rescale=do_rescale,\n                     rescale_factor=rescale_factor,\n                     do_normalize=do_normalize,\n                     image_mean=image_mean,\n                     image_std=image_std,\n+                    patch_size=patch_size,\n+                    temporal_patch_size=temporal_patch_size,\n+                    merge_size=merge_size,\n                     data_format=data_format,\n                     do_convert_rgb=do_convert_rgb,\n                     input_data_format=input_data_format,"
        },
        {
            "sha": "1e99125806ccfb7e25498df6d4dc7536dad9f746",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
            "status": "modified",
            "additions": 80,
            "deletions": 21,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c287aecfcf94f569eb8740be5be43bda6750682/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c287aecfcf94f569eb8740be5be43bda6750682/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py?ref=1c287aecfcf94f569eb8740be5be43bda6750682",
            "patch": "@@ -105,13 +105,26 @@ class Qwen2VLImageProcessorFast(BaseImageProcessorFast):\n     patch_size = 14\n     temporal_patch_size = 2\n     merge_size = 2\n-    min_pixels = 56 * 56\n-    max_pixels = 28 * 28 * 1280\n-    valid_kwargs = DefaultFastImageProcessorKwargs\n+    min_pixels = None\n+    max_pixels = None\n+    valid_kwargs = Qwen2VLFastImageProcessorKwargs\n     model_input_names = [\"pixel_values\", \"image_grid_thw\", \"pixel_values_videos\", \"video_grid_thw\"]\n \n     def __init__(self, **kwargs: Unpack[Qwen2VLFastImageProcessorKwargs]):\n-        super().__init__(**kwargs)\n+        size = kwargs.pop(\"size\", None)\n+        min_pixels = kwargs.pop(\"min_pixels\", None)\n+        max_pixels = kwargs.pop(\"max_pixels\", None)\n+        if size is not None and (\"shortest_edge\" not in size or \"longest_edge\" not in size):\n+            raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n+        else:\n+            size = self.size\n+        # backward compatibility: override size with min_pixels and max_pixels if they are provided\n+        if min_pixels is not None:\n+            size[\"shortest_edge\"] = min_pixels\n+        if max_pixels is not None:\n+            size[\"longest_edge\"] = max_pixels\n+\n+        super().__init__(size=size, min_pixels=min_pixels, max_pixels=max_pixels, **kwargs)\n \n     def _preprocess(\n         self,\n@@ -124,6 +137,9 @@ def _preprocess(\n         do_normalize: bool,\n         image_mean: Optional[Union[float, List[float]]],\n         image_std: Optional[Union[float, List[float]]],\n+        patch_size: int,\n+        temporal_patch_size: int,\n+        merge_size: int,\n         do_convert_rgb: bool,\n         input_data_format: Optional[Union[str, ChannelDimension]],\n         device: Optional[Union[str, torch.device]],\n@@ -138,6 +154,8 @@ def _preprocess(\n                 Optional list of dictionaries containing additional information about vision inputs.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+                Size of the image after resizing. `shortest_edge` and `longest_edge` keys must be present.\n             interpolation (`InterpolationMode`):\n                 Resampling filter to use if resizing the image.\n             do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n@@ -150,6 +168,12 @@ def _preprocess(\n                 Mean to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n             image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n                 Standard deviation to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n+            patch_size (`int`, *optional*, defaults to `self.patch_size`):\n+                The spacial patch size of the vision encoder.\n+            temporal_patch_size (`int`, *optional*, defaults to `self.temporal_patch_size`):\n+                The temporal patch size of the vision encoder.\n+            merge_size (`int`, *optional*, defaults to `self.merge_size`):\n+                The merge size of the vision encoder to llm encoder.\n             do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n                 Whether to convert the image to RGB.\n             input_data_format (`ChannelDimension` or `str`, *optional*):\n@@ -178,9 +202,9 @@ def _preprocess(\n                 resized_height, resized_width = smart_resize(\n                     height,\n                     width,\n-                    factor=self.patch_size * self.merge_size,\n-                    min_pixels=self.min_pixels,\n-                    max_pixels=self.max_pixels,\n+                    factor=patch_size * merge_size,\n+                    min_pixels=size[\"shortest_edge\"],\n+                    max_pixels=size[\"longest_edge\"],\n                 )\n                 stacked_images = F.resize(\n                     stacked_images, size=(resized_height, resized_width), interpolation=interpolation\n@@ -201,28 +225,28 @@ def _preprocess(\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n         patches = torch.stack(processed_images, dim=0)\n-        if patches.shape[0] % self.temporal_patch_size != 0:\n-            repeats = patches[-1].unsqueeze(0).repeat(self.temporal_patch_size - 1, 1, 1, 1)\n+        if patches.shape[0] % temporal_patch_size != 0:\n+            repeats = patches[-1].unsqueeze(0).repeat(temporal_patch_size - 1, 1, 1, 1)\n             patches = torch.cat([patches, repeats], dim=0)\n \n         channel = patches.shape[1]\n-        grid_t = patches.shape[0] // self.temporal_patch_size\n-        grid_h, grid_w = resized_height // self.patch_size, resized_width // self.patch_size\n+        grid_t = patches.shape[0] // temporal_patch_size\n+        grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n \n         patches = patches.view(\n             grid_t,\n-            self.temporal_patch_size,\n+            temporal_patch_size,\n             channel,\n-            grid_h // self.merge_size,\n-            self.merge_size,\n-            self.patch_size,\n-            grid_w // self.merge_size,\n-            self.merge_size,\n-            self.patch_size,\n+            grid_h // merge_size,\n+            merge_size,\n+            patch_size,\n+            grid_w // merge_size,\n+            merge_size,\n+            patch_size,\n         )\n         patches = patches.permute(0, 3, 6, 4, 7, 2, 1, 5, 8)\n         flatten_patches = patches.reshape(\n-            grid_t * grid_h * grid_w, channel * self.temporal_patch_size * self.patch_size * self.patch_size\n+            grid_t * grid_h * grid_w, channel * temporal_patch_size * patch_size * patch_size\n         )\n \n         return flatten_patches, (grid_t, grid_h, grid_w)\n@@ -239,6 +263,11 @@ def preprocess(\n         do_normalize: bool = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n+        min_pixels: int = None,\n+        max_pixels: int = None,\n+        patch_size: int = None,\n+        temporal_patch_size: int = None,\n+        merge_size: int = None,\n         do_convert_rgb: bool = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n@@ -257,8 +286,7 @@ def preprocess(\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n             size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n-                Size of the image after resizing. Shortest edge of the image is resized to size[\"shortest_edge\"], with\n-                the longest edge resized to keep the input aspect ratio.\n+                Size of the image after resizing. `shortest_edge` and `longest_edge` keys must be present.\n             resample (`int`, *optional*, defaults to `self.resample`):\n                 Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n                 has an effect if `do_resize` is set to `True`.\n@@ -273,6 +301,16 @@ def preprocess(\n             image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n                 Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n                 `True`.\n+            min_pixels (`int`, *optional*, defaults to `self.min_pixels`):\n+                The min pixels of the image to resize the image.\n+            max_pixels (`int`, *optional*, defaults to `self.max_pixels`):\n+                The max pixels of the image to resize the image.\n+            patch_size (`int`, *optional*, defaults to `self.patch_size`):\n+                The spacial patch size of the vision encoder.\n+            temporal_patch_size (`int`, *optional*, defaults to `self.temporal_patch_size`):\n+                The temporal patch size of the vision encoder.\n+            merge_size (`int`, *optional*, defaults to `self.merge_size`):\n+                The merge size of the vision encoder to llm encoder.\n             do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n                 Whether to convert the image to RGB.\n             return_tensors (`str` or `TensorType`, *optional*):\n@@ -296,6 +334,18 @@ def preprocess(\n             device (`torch.device`, *optional*):\n                 The device to process the images on. If unset, the device is inferred from the input images.\n         \"\"\"\n+        if size is not None:\n+            if \"shortest_edge\" not in size or \"longest_edge\" not in size:\n+                raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n+            min_pixels = size[\"shortest_edge\"]\n+        else:\n+            size = self.size\n+        # backward compatibility: override size with min_pixels and max_pixels if they are provided\n+        if min_pixels is not None:\n+            size[\"shortest_edge\"] = min_pixels\n+        if max_pixels is not None:\n+            size[\"longest_edge\"] = max_pixels\n+\n         do_resize = do_resize if do_resize is not None else self.do_resize\n         size = size if size is not None else self.size\n         resample = resample if resample is not None else self.resample\n@@ -304,6 +354,9 @@ def preprocess(\n         do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n         image_mean = image_mean if image_mean is not None else self.image_mean\n         image_std = image_std if image_std is not None else self.image_std\n+        patch_size = patch_size if patch_size is not None else self.patch_size\n+        temporal_patch_size = temporal_patch_size if temporal_patch_size is not None else self.temporal_patch_size\n+        merge_size = merge_size if merge_size is not None else self.merge_size\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n \n         # Make hashable for cache\n@@ -351,6 +404,9 @@ def preprocess(\n                     do_normalize=do_normalize,\n                     image_mean=image_mean,\n                     image_std=image_std,\n+                    patch_size=patch_size,\n+                    temporal_patch_size=temporal_patch_size,\n+                    merge_size=merge_size,\n                     do_convert_rgb=do_convert_rgb,\n                     input_data_format=input_data_format,\n                     device=device,\n@@ -374,6 +430,9 @@ def preprocess(\n                     do_normalize=do_normalize,\n                     image_mean=image_mean,\n                     image_std=image_std,\n+                    patch_size=patch_size,\n+                    temporal_patch_size=temporal_patch_size,\n+                    merge_size=merge_size,\n                     do_convert_rgb=do_convert_rgb,\n                     input_data_format=input_data_format,\n                     device=device,"
        }
    ],
    "stats": {
        "total": 207,
        "additions": 165,
        "deletions": 42
    }
}