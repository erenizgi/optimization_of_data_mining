{
    "author": "Coco58323",
    "message": "Skip non-selected experts for mixtral and qwen2_moe (#32429)\n\n* Skip non-selected experts for mixtral and qwen2_moe\n\n* Fix: tensor tolist()\n\n* WIP: tokenization test\n\n* fix modular source of truth\n\n* nits\n\n---------\n\nCo-authored-by: Arthur Zucker <arthur.zucker@gmail.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "aab087832724849463cf5b28a40a82ea97777b14",
    "files": [
        {
            "sha": "9f839536b7f3d86cc0ae8467c580934d75afda70",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/aab087832724849463cf5b28a40a82ea97777b14/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aab087832724849463cf5b28a40a82ea97777b14/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=aab087832724849463cf5b28a40a82ea97777b14",
            "patch": "@@ -135,11 +135,10 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         # this will be used to easily index which expert is going to be sollicitated\n         expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n \n-        # Loop over all available experts in the model and perform the computation on each expert\n-        for expert_idx in range(self.num_experts):\n+        expert_hitted = (expert_mask.sum(dim=(-1, -2)) > 0).nonzero(as_tuple=True)[0].tolist()\n+        for expert_idx in expert_hitted:\n             expert_layer = self.experts[expert_idx]\n             idx, top_x = torch.where(expert_mask[expert_idx])\n-\n             # Index the correct hidden states and compute the expert hidden state for\n             # the current expert. We need to make sure to multiply the output hidden\n             # states by `routing_weights` on the corresponding tokens (top-1 and top-2)"
        },
        {
            "sha": "b0f0ac1a88b2458b17c4fb46b9a32a050c136bbd",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/aab087832724849463cf5b28a40a82ea97777b14/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aab087832724849463cf5b28a40a82ea97777b14/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=aab087832724849463cf5b28a40a82ea97777b14",
            "patch": "@@ -209,11 +209,10 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         # this will be used to easily index which expert is going to be sollicitated\n         expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n \n-        # Loop over all available experts in the model and perform the computation on each expert\n-        for expert_idx in range(self.num_experts):\n+        expert_hitted = (expert_mask.sum(dim=(-1, -2)) > 0).nonzero(as_tuple=True)[0].tolist()\n+        for expert_idx in expert_hitted:\n             expert_layer = self.experts[expert_idx]\n             idx, top_x = torch.where(expert_mask[expert_idx])\n-\n             # Index the correct hidden states and compute the expert hidden state for\n             # the current expert. We need to make sure to multiply the output hidden\n             # states by `routing_weights` on the corresponding tokens (top-1 and top-2)"
        },
        {
            "sha": "930899a0c9dac97ab48bdae82cbce4285a1d8f96",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/aab087832724849463cf5b28a40a82ea97777b14/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aab087832724849463cf5b28a40a82ea97777b14/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=aab087832724849463cf5b28a40a82ea97777b14",
            "patch": "@@ -616,7 +616,8 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n \n         # Loop over all available experts in the model and perform the computation on each expert\n-        for expert_idx in range(self.num_experts):\n+        expert_hitted = (expert_mask.sum(dim=(-1, -2)) > 0).nonzero(as_tuple=True)[0].tolist()\n+        for expert_idx in expert_hitted:\n             expert_layer = self.experts[expert_idx]\n             idx, top_x = torch.where(expert_mask[expert_idx])\n "
        }
    ],
    "stats": {
        "total": 13,
        "additions": 6,
        "deletions": 7
    }
}