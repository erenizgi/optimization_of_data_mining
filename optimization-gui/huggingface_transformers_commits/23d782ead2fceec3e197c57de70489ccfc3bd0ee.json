{
    "author": "jonasrohw",
    "message": "Output dicts support in text generation pipeline (#35092)\n\n* Support for generate_argument: return_dict_in_generate=True, instead of returning a error\r\n\r\n* fix: call test with return_dict_in_generate=True\r\n\r\n* fix: Only import torch if it is present\r\n\r\n* update: Encapsulate output_dict changes\r\n\r\n* fix: added back original comments\r\n\r\n---------\r\n\r\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "23d782ead2fceec3e197c57de70489ccfc3bd0ee",
    "files": [
        {
            "sha": "0a8e6e845c7808f7add2fae812fb49e8ca298034",
            "filename": "src/transformers/pipelines/text_generation.py",
            "status": "modified",
            "additions": 51,
            "deletions": 4,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/23d782ead2fceec3e197c57de70489ccfc3bd0ee/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/23d782ead2fceec3e197c57de70489ccfc3bd0ee/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext_generation.py?ref=23d782ead2fceec3e197c57de70489ccfc3bd0ee",
            "patch": "@@ -3,11 +3,13 @@\n import types\n from typing import Dict\n \n-from ..utils import add_end_docstrings, is_tf_available, is_torch_available\n+from ..utils import ModelOutput, add_end_docstrings, is_tf_available, is_torch_available\n from .base import Pipeline, build_pipeline_init_args\n \n \n if is_torch_available():\n+    import torch\n+\n     from ..models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n     from .pt_utils import KeyDataset\n \n@@ -380,13 +382,44 @@ def _forward(self, model_inputs, **generate_kwargs):\n         if \"generation_config\" not in generate_kwargs:\n             generate_kwargs[\"generation_config\"] = self.generation_config\n \n-        generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n+        output = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n+\n+        if isinstance(output, ModelOutput):\n+            generated_sequence = output.sequences\n+            other_outputs = {k: v for k, v in output.items() if k != \"sequences\"}\n+            out_b = generated_sequence.shape[0]\n+\n+            if self.framework == \"pt\":\n+                for key, value in other_outputs.items():\n+                    if isinstance(value, torch.Tensor) and value.shape[0] == out_b:\n+                        other_outputs[key] = value.reshape(in_b, out_b // in_b, *value.shape[1:])\n+                    if isinstance(value, tuple) and len(value[0]) == out_b:\n+                        value = torch.stack(value).swapaxes(0, 1)\n+                        other_outputs[key] = value\n+            elif self.framework == \"tf\":\n+                for key, value in other_outputs.items():\n+                    if isinstance(value, tf.Tensor) and value.shape[0] == out_b:\n+                        other_outputs[key] = tf.reshape(value, (in_b, out_b // in_b, *value.shape[1:]))\n+                    if isinstance(value, tuple) and len(value[0]) == out_b:\n+                        value = tf.stack(value).swapaxes(0, 1)\n+                        other_outputs[key] = value\n+        else:\n+            generated_sequence = output\n+            other_outputs = {}\n+\n         out_b = generated_sequence.shape[0]\n         if self.framework == \"pt\":\n             generated_sequence = generated_sequence.reshape(in_b, out_b // in_b, *generated_sequence.shape[1:])\n         elif self.framework == \"tf\":\n             generated_sequence = tf.reshape(generated_sequence, (in_b, out_b // in_b, *generated_sequence.shape[1:]))\n-        return {\"generated_sequence\": generated_sequence, \"input_ids\": input_ids, \"prompt_text\": prompt_text}\n+\n+        model_outputs = {\n+            \"generated_sequence\": generated_sequence,\n+            \"input_ids\": input_ids,\n+            \"prompt_text\": prompt_text,\n+        }\n+        model_outputs.update(other_outputs)\n+        return model_outputs\n \n     def postprocess(\n         self,\n@@ -400,7 +433,19 @@ def postprocess(\n         prompt_text = model_outputs[\"prompt_text\"]\n         generated_sequence = generated_sequence.numpy().tolist()\n         records = []\n-        for sequence in generated_sequence:\n+        other_outputs = model_outputs.get(\"additional_outputs\", {})\n+        splitted_keys = {}\n+        if other_outputs:\n+            if self.framework == \"pt\":\n+                for k, v in other_outputs.items():\n+                    if isinstance(v, torch.Tensor) and v.shape[0] == len(generated_sequence):\n+                        splitted_keys[k] = v.numpy().tolist()\n+            elif self.framework == \"tf\":\n+                for k, v in other_outputs.items():\n+                    if isinstance(v, tf.Tensor) and v.shape[0] == len(generated_sequence):\n+                        splitted_keys[k] = v.numpy().tolist()\n+\n+        for idx, sequence in enumerate(generated_sequence):\n             if return_type == ReturnType.TENSORS:\n                 record = {\"generated_token_ids\": sequence}\n             elif return_type in {ReturnType.NEW_TEXT, ReturnType.FULL_TEXT}:\n@@ -444,6 +489,8 @@ def postprocess(\n                             # When we're not starting from a prefill, the output is a new assistant message\n                             all_text = list(prompt_text.messages) + [{\"role\": \"assistant\", \"content\": all_text}]\n                 record = {\"generated_text\": all_text}\n+                for key, values in splitted_keys.items():\n+                    record[key] = values[idx]\n             records.append(record)\n \n         return records"
        },
        {
            "sha": "5c5d3de17a1dcdba1b6d7b2a974d0d6664ef9b5c",
            "filename": "tests/pipelines/test_pipelines_text_generation.py",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/23d782ead2fceec3e197c57de70489ccfc3bd0ee/tests%2Fpipelines%2Ftest_pipelines_text_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/23d782ead2fceec3e197c57de70489ccfc3bd0ee/tests%2Fpipelines%2Ftest_pipelines_text_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_text_generation.py?ref=23d782ead2fceec3e197c57de70489ccfc3bd0ee",
            "patch": "@@ -653,6 +653,31 @@ def test_pipeline_length_setting_warning(self):\n             _ = text_generator(prompt, max_length=10)\n         self.assertNotIn(logger_msg, cl.out)\n \n+    def test_return_dict_in_generate(self):\n+        text_generator = pipeline(\"text-generation\", model=\"hf-internal-testing/tiny-random-gpt2\", max_new_tokens=16)\n+        out = text_generator(\n+            [\"This is great !\", \"Something else\"], return_dict_in_generate=True, output_logits=True, output_scores=True\n+        )\n+        self.assertEqual(\n+            out,\n+            [\n+                [\n+                    {\n+                        \"generated_text\": ANY(str),\n+                        \"logits\": ANY(list),\n+                        \"scores\": ANY(list),\n+                    },\n+                ],\n+                [\n+                    {\n+                        \"generated_text\": ANY(str),\n+                        \"logits\": ANY(list),\n+                        \"scores\": ANY(list),\n+                    },\n+                ],\n+            ],\n+        )\n+\n     @require_torch\n     def test_pipeline_assisted_generation(self):\n         \"\"\"Tests that we can run assisted generation in the pipeline\"\"\""
        }
    ],
    "stats": {
        "total": 80,
        "additions": 76,
        "deletions": 4
    }
}