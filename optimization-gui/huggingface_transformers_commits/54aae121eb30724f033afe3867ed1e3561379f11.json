{
    "author": "eustlb",
    "message": "[Whisper] Fix whisper tokenizer (#34537)\n\n* handle single timestamp ending\r\n\r\n* include last timestamp token\r\n\r\n* handle single timestamp ending\r\n\r\n* avoid floating points arithm limitations\r\n\r\n* ensure float64 operations\r\n\r\n* new test\r\n\r\n* make fixup\r\n\r\n* make copies\r\n\r\n* handle edge case double tokens ending with different tokens\r\n\r\n* handle single timestamp ending\r\n\r\n* make fixup\r\n\r\n* handle conditioning on prev segments\r\n\r\n* fix\r\n\r\n* Update src/transformers/models/whisper/generation_whisper.py\r\n\r\nCo-authored-by: Yoach Lacombe <52246514+ylacombe@users.noreply.github.com>\r\n\r\n* [run-slow] whisper\r\n\r\n* don't call item() to avoid unnecessary sync\r\n\r\n* fix\r\n\r\n---------\r\n\r\nCo-authored-by: Yoach Lacombe <52246514+ylacombe@users.noreply.github.com>\r\nCo-authored-by: Eustache Le Bihan <eustlb@users.noreply.huggingface.co>",
    "sha": "54aae121eb30724f033afe3867ed1e3561379f11",
    "files": [
        {
            "sha": "2f58375f3de7516be71817150e27c42133d7ad04",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 30,
            "deletions": 10,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/54aae121eb30724f033afe3867ed1e3561379f11/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54aae121eb30724f033afe3867ed1e3561379f11/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=54aae121eb30724f033afe3867ed1e3561379f11",
            "patch": "@@ -308,6 +308,7 @@ def generate(\n         num_segment_frames: Optional[int] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         time_precision: float = 0.02,\n+        time_precision_features: float = 0.01,\n         return_token_timestamps: Optional[bool] = None,\n         return_segments: bool = False,\n         return_dict_in_generate: Optional[bool] = None,\n@@ -417,6 +418,8 @@ def generate(\n             time_precision (`int`, *optional*, defaults to 0.02):\n                 The duration of output token in seconds. *E.g.* 0.02 means that a generated token on average accounts\n                 for 20 ms.\n+            time_precision_features (`int`, *optional*, defaults to 0.01):\n+                The duration represented by a feature frame in seconds.\n             return_token_timestamps (`bool`, *optional*):\n                 Whether to return token-level timestamps with the text. This can be used with or without the\n                 `return_timestamps` option. To get word-level timestamps, use the tokenizer to group the tokens into\n@@ -629,7 +632,7 @@ def generate(\n                 cur_bsz=cur_bsz,\n                 batch_idx_map=batch_idx_map,\n             )\n-            time_offset = seek * time_precision / input_stride\n+            time_offset = seek.to(torch.float64) * time_precision / input_stride\n             seek_num_frames = (max_frames - seek).clamp(max=num_segment_frames)\n \n             # 6.2 cut out next 30s segment from input features\n@@ -658,6 +661,7 @@ def generate(\n                 config=self.config,\n                 device=init_tokens.device,\n                 suppress_tokens=suppress_tokens,\n+                timestamp_begin=timestamp_begin,\n                 kwargs=kwargs,\n             )\n \n@@ -718,6 +722,7 @@ def generate(\n                     timestamp_begin=timestamp_begin,\n                     seek_num_frames=seek_num_frames,\n                     time_precision=time_precision,\n+                    time_precision_features=time_precision_features,\n                     input_stride=input_stride,\n                     prev_idx=prev_i,\n                     idx=i,\n@@ -1665,6 +1670,7 @@ def _prepare_decoder_input_ids(\n         config,\n         device,\n         suppress_tokens,\n+        timestamp_begin,\n         kwargs,\n     ):\n         if \"decoder_input_ids\" in kwargs:\n@@ -1684,6 +1690,14 @@ def _prepare_decoder_input_ids(\n             # according to https://github.com/openai/whisper/blob/e58f28804528831904c3b6f2c0e473f346223433/whisper/decoding.py#L609\n             active_segments = [current_segments[i] if do_condition_on_prev_tokens[i] else None for i in batch_idx_map]\n \n+            for segments in active_segments:\n+                for seg in segments:\n+                    if len(seg[\"tokens\"]) > 2 and seg[\"tokens\"][-2] >= timestamp_begin:\n+                        # the segment finishes with two timestamp tokens\n+                        # we need to ignore the last timestamp token\n+                        # see https://github.com/huggingface/transformers/pull/34537\n+                        seg[\"tokens\"] = seg[\"tokens\"][:-1]\n+\n             if prompt_ids is not None and generation_config.prompt_condition_type == \"all-segments\":\n                 prev_ids = prompt_ids\n             else:\n@@ -1778,6 +1792,7 @@ def _retrieve_segment(\n         timestamp_begin,\n         seek_num_frames,\n         time_precision,\n+        time_precision_features,\n         input_stride,\n         prev_idx,\n         idx,\n@@ -1799,17 +1814,22 @@ def _retrieve_segment(\n             segments = []\n             if single_timestamp_ending:\n                 slices.append(len(seek_sequence))\n+            else:\n+                # we want to include the last timestamp token in the last segment to know it was no single ending\n+                slices[-1] += 1\n \n             last_slice = 0\n             # Add each segment to list of all segments\n-            for current_slice in slices:\n+            for i, current_slice in enumerate(slices):\n+                is_last_slice = i == len(slices) - 1\n                 sliced_tokens = seek_sequence[last_slice:current_slice]\n-                start_timestamp_pos = sliced_tokens[0].item() - timestamp_begin\n-                end_timestamp_pos = sliced_tokens[-1].item() - timestamp_begin\n+                start_timestamp_pos = sliced_tokens[0] - timestamp_begin\n+                idx_sliced_tokens = -1 if not is_last_slice or single_timestamp_ending else -2\n+                end_timestamp_pos = sliced_tokens[idx_sliced_tokens] - timestamp_begin\n                 segments.append(\n                     {\n-                        \"start\": time_offset[prev_idx] + start_timestamp_pos * time_precision,\n-                        \"end\": time_offset[prev_idx] + end_timestamp_pos * time_precision,\n+                        \"start\": time_offset[prev_idx] + start_timestamp_pos.to(torch.float64) * time_precision,\n+                        \"end\": time_offset[prev_idx] + end_timestamp_pos.to(torch.float64) * time_precision,\n                         \"tokens\": sliced_tokens,\n                         \"result\": seek_outputs[idx],\n                     }\n@@ -1827,16 +1847,16 @@ def _retrieve_segment(\n                 # otherwise, ignore the unfinished segment and seek to the last timestamp\n                 # here we throw away all predictions after the last predicted \"end of segment\"\n                 # since we are cutting right in the middle of an audio\n-                last_timestamp_pos = seek_sequence[last_slice - 1].item() - timestamp_begin\n+                last_timestamp_pos = seek_sequence[last_slice - 2].item() - timestamp_begin\n                 segment_offset = last_timestamp_pos * input_stride\n         else:\n             # If whisper does not predict any \"end of segment\" token, then\n             # the whole decoding is considered a segment and we add it to the list of segments\n             timestamps = seek_sequence[timestamp_tokens.nonzero().flatten()]\n-            last_timestamp_pos = seek_num_frames[prev_idx]\n-            if timestamps.numel() > 0 and timestamps[-1].item() != timestamp_begin:\n+            last_timestamp_pos = int(seek_num_frames[prev_idx] * time_precision_features / time_precision)\n+            if timestamps.numel() > 0 and timestamps[-1] != timestamp_begin:\n                 # no consecutive timestamps but it has a timestamp; use the last one.\n-                last_timestamp_pos = timestamps[-1].item() - timestamp_begin\n+                last_timestamp_pos = (timestamps[-1] - timestamp_begin).to(torch.float64)\n             segments = [\n                 {\n                     \"start\": time_offset[prev_idx],"
        },
        {
            "sha": "e537ef95da675182ae1cb988a57fb0e5a97d776f",
            "filename": "src/transformers/models/whisper/tokenization_whisper.py",
            "status": "modified",
            "additions": 27,
            "deletions": 7,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/54aae121eb30724f033afe3867ed1e3561379f11/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54aae121eb30724f033afe3867ed1e3561379f11/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py?ref=54aae121eb30724f033afe3867ed1e3561379f11",
            "patch": "@@ -528,7 +528,9 @@ def basic_normalize(text, remove_diacritics=False):\n         normalizer = BasicTextNormalizer(remove_diacritics=remove_diacritics)\n         return normalizer(text)\n \n-    def _decode_with_timestamps(self, token_ids, skip_special_tokens=False, time_precision=0.02) -> str:\n+    def _decode_with_timestamps(\n+        self, token_ids, skip_special_tokens=False, time_precision=0.02, segment_size=1500\n+    ) -> str:\n         \"\"\"\n         Timestamp tokens are above the special tokens' id range and are ignored by `decode()`. This method decodes\n         given tokens with timestamps tokens annotated, e.g. \"<|1.08|>\".\n@@ -538,15 +540,25 @@ def _decode_with_timestamps(self, token_ids, skip_special_tokens=False, time_pre\n \n         cur_max_timestamp = 0.0\n         prev_segments_len = 0.0\n+        penultimate_timestamp = 0.0\n \n-        for token in token_ids:\n+        for i, token in enumerate(token_ids):\n             if token >= timestamp_begin:\n                 timestamp = float((token - timestamp_begin) * time_precision)\n \n                 if timestamp < cur_max_timestamp:\n                     # next segment has started\n-                    prev_segments_len += cur_max_timestamp\n+                    last_was_single_ending = i >= 2 and not (\n+                        token_ids[i - 1] >= timestamp_begin and token_ids[i - 2] >= timestamp_begin\n+                    )\n+                    if last_was_single_ending:\n+                        prev_segments_len += time_precision * segment_size\n+                    else:\n+                        cur_max_timestamp = penultimate_timestamp\n+                        prev_segments_len += penultimate_timestamp\n+                        outputs = outputs[:-2]\n \n+                penultimate_timestamp = cur_max_timestamp\n                 cur_max_timestamp = timestamp\n \n                 outputs.append(f\"<|{(timestamp + prev_segments_len):.2f}|>\")\n@@ -558,7 +570,7 @@ def _decode_with_timestamps(self, token_ids, skip_special_tokens=False, time_pre\n         ]\n         return \"\".join(outputs)\n \n-    def _compute_offsets(self, token_ids, time_precision=0.02):\n+    def _compute_offsets(self, token_ids, time_precision=0.02, segment_size=1500):\n         \"\"\"\n         Compute offsets for a given tokenized input\n \n@@ -567,6 +579,8 @@ def _compute_offsets(self, token_ids, time_precision=0.02):\n                 List of tokenized input ids. Can be obtained using the `__call__` method.\n             time_precision (`float`, *optional*, defaults to 0.02):\n                 The time ratio to convert from token to time.\n+            segment_size (`int`, *optional*, defaults to 1500):\n+                The number of features in the input mel spectrogram.\n         \"\"\"\n         offsets = []\n         # ensure torch tensor of token ids is placed on cpu\n@@ -597,7 +611,13 @@ def _compute_offsets(self, token_ids, time_precision=0.02):\n \n                 if start_timestamp_position < cur_max_timestamp:\n                     # next segment has started\n-                    prev_segments_len += cur_max_timestamp\n+                    is_single_ending = last_slice >= 2 and not (\n+                        token_ids[last_slice - 2] >= timestamp_begin and token_ids[last_slice - 1] >= timestamp_begin\n+                    )\n+                    if is_single_ending:\n+                        prev_segments_len += segment_size\n+                    else:\n+                        prev_segments_len += cur_max_timestamp\n \n                 cur_max_timestamp = end_timestamp_position\n \n@@ -609,8 +629,8 @@ def _compute_offsets(self, token_ids, time_precision=0.02):\n                     {\n                         \"text\": text,\n                         \"timestamp\": (\n-                            (start_timestamp_position + prev_segments_len) * time_precision,\n-                            (end_timestamp_position + prev_segments_len) * time_precision,\n+                            start_timestamp_position * time_precision + prev_segments_len * time_precision,\n+                            end_timestamp_position * time_precision + prev_segments_len * time_precision,\n                         ),\n                     }\n                 )"
        },
        {
            "sha": "f0383cb0def76fc4bc4341f1e67c77aa9b51952f",
            "filename": "src/transformers/models/whisper/tokenization_whisper_fast.py",
            "status": "modified",
            "additions": 28,
            "deletions": 8,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/54aae121eb30724f033afe3867ed1e3561379f11/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54aae121eb30724f033afe3867ed1e3561379f11/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper_fast.py?ref=54aae121eb30724f033afe3867ed1e3561379f11",
            "patch": "@@ -169,7 +169,9 @@ def _encode_plus(self, *args, **kwargs) -> BatchEncoding:\n         return super()._encode_plus(*args, **kwargs)\n \n     # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer._decode_with_timestamps\n-    def _decode_with_timestamps(self, token_ids, skip_special_tokens=False, time_precision=0.02) -> str:\n+    def _decode_with_timestamps(\n+        self, token_ids, skip_special_tokens=False, time_precision=0.02, segment_size=1500\n+    ) -> str:\n         \"\"\"\n         Timestamp tokens are above the special tokens' id range and are ignored by `decode()`. This method decodes\n         given tokens with timestamps tokens annotated, e.g. \"<|1.08|>\".\n@@ -179,15 +181,25 @@ def _decode_with_timestamps(self, token_ids, skip_special_tokens=False, time_pre\n \n         cur_max_timestamp = 0.0\n         prev_segments_len = 0.0\n+        penultimate_timestamp = 0.0\n \n-        for token in token_ids:\n+        for i, token in enumerate(token_ids):\n             if token >= timestamp_begin:\n                 timestamp = float((token - timestamp_begin) * time_precision)\n \n                 if timestamp < cur_max_timestamp:\n                     # next segment has started\n-                    prev_segments_len += cur_max_timestamp\n-\n+                    last_was_single_ending = i >= 2 and not (\n+                        token_ids[i - 1] >= timestamp_begin and token_ids[i - 2] >= timestamp_begin\n+                    )\n+                    if last_was_single_ending:\n+                        prev_segments_len += time_precision * segment_size\n+                    else:\n+                        cur_max_timestamp = penultimate_timestamp\n+                        prev_segments_len += penultimate_timestamp\n+                        outputs = outputs[:-2]\n+\n+                penultimate_timestamp = cur_max_timestamp\n                 cur_max_timestamp = timestamp\n \n                 outputs.append(f\"<|{(timestamp + prev_segments_len):.2f}|>\")\n@@ -200,7 +212,7 @@ def _decode_with_timestamps(self, token_ids, skip_special_tokens=False, time_pre\n         return \"\".join(outputs)\n \n     # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer._compute_offsets\n-    def _compute_offsets(self, token_ids, time_precision=0.02):\n+    def _compute_offsets(self, token_ids, time_precision=0.02, segment_size=1500):\n         \"\"\"\n         Compute offsets for a given tokenized input\n \n@@ -209,6 +221,8 @@ def _compute_offsets(self, token_ids, time_precision=0.02):\n                 List of tokenized input ids. Can be obtained using the `__call__` method.\n             time_precision (`float`, *optional*, defaults to 0.02):\n                 The time ratio to convert from token to time.\n+            segment_size (`int`, *optional*, defaults to 1500):\n+                The number of features in the input mel spectrogram.\n         \"\"\"\n         offsets = []\n         # ensure torch tensor of token ids is placed on cpu\n@@ -239,7 +253,13 @@ def _compute_offsets(self, token_ids, time_precision=0.02):\n \n                 if start_timestamp_position < cur_max_timestamp:\n                     # next segment has started\n-                    prev_segments_len += cur_max_timestamp\n+                    is_single_ending = last_slice >= 2 and not (\n+                        token_ids[last_slice - 2] >= timestamp_begin and token_ids[last_slice - 1] >= timestamp_begin\n+                    )\n+                    if is_single_ending:\n+                        prev_segments_len += segment_size\n+                    else:\n+                        prev_segments_len += cur_max_timestamp\n \n                 cur_max_timestamp = end_timestamp_position\n \n@@ -251,8 +271,8 @@ def _compute_offsets(self, token_ids, time_precision=0.02):\n                     {\n                         \"text\": text,\n                         \"timestamp\": (\n-                            (start_timestamp_position + prev_segments_len) * time_precision,\n-                            (end_timestamp_position + prev_segments_len) * time_precision,\n+                            start_timestamp_position * time_precision + prev_segments_len * time_precision,\n+                            end_timestamp_position * time_precision + prev_segments_len * time_precision,\n                         ),\n                     }\n                 )"
        },
        {
            "sha": "faab43854cce11f950f25dd5eeff4fd9eed138f5",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 88,
            "deletions": 0,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/54aae121eb30724f033afe3867ed1e3561379f11/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/54aae121eb30724f033afe3867ed1e3561379f11/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=54aae121eb30724f033afe3867ed1e3561379f11",
            "patch": "@@ -2096,6 +2096,94 @@ def test_tiny_longform_timestamps_generation(self):\n         transcript = processor.batch_decode(generated_ids[\"sequences\"], skip_special_tokens=True, output_offsets=True)\n         self.assertEqual(transcript[0][\"offsets\"], EXPECTED_TRANSCRIPT)\n \n+    @slow\n+    def test_small_longform_timestamps_generation(self):\n+        processor = WhisperProcessor.from_pretrained(\"openai/whisper-small.en\")\n+        model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small.en\")\n+        model.to(torch_device)\n+\n+        dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n+        sample = dataset[0][\"audio\"][\"array\"]\n+        sampling_rate = dataset[0][\"audio\"][\"sampling_rate\"]\n+\n+        sample = [*sample[: 15 * sampling_rate], *np.zeros(16 * sampling_rate).tolist(), *sample[15 * sampling_rate :]]\n+        sample = np.array(sample)\n+\n+        input_features = processor(\n+            sample,\n+            sampling_rate=16_000,\n+            padding=\"longest\",\n+            truncation=False,\n+            return_attention_mask=True,\n+            return_tensors=\"pt\",\n+        ).input_features\n+\n+        input_features = input_features.to(torch_device)\n+        generated_ids = model.generate(input_features, return_timestamps=True, return_segments=True)\n+\n+        EXPECTED_TRANSCRIPT = [\n+            {\n+                \"text\": \" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.\",\n+                \"timestamp\": (0.0, 6.38),\n+            },\n+            {\n+                \"text\": \" Nor is Mr. Quilter's manner less interesting than his matter.\",\n+                \"timestamp\": (6.38, 11.32),\n+            },\n+            {\n+                \"text\": \" He tells us that at this festive season of the year,\",\n+                \"timestamp\": (11.32, 15.0),\n+            },\n+            {\n+                \"text\": \" With Christmas and roast beef looming before us, similes drawn from eating and its results\",\n+                \"timestamp\": (30.0, 36.76),\n+            },\n+            {\n+                \"text\": \" occur most readily to the mind.\",\n+                \"timestamp\": (36.76, 39.80),\n+            },\n+            {\n+                \"text\": \" He has grave doubts whether Sir Frederick Layton's work is really Greek after all and\",\n+                \"timestamp\": (39.80, 45.36),\n+            },\n+            {\n+                \"text\": \" can discover in it but little of rocky Ithaca.\",\n+                \"timestamp\": (45.36, 49.0),\n+            },\n+            {\n+                \"text\": \" Lenell's pictures are a sort of up-guards-and-atom paintings, and Mason's exquisite ittles\",\n+                \"timestamp\": (49.0, 56.28),\n+            },\n+            {\n+                \"text\": \" are as national as a jingo poem. Mr. Burkett fosters landscape's smile at one much in\",\n+                \"timestamp\": (56.28, 64.12),\n+            },\n+            {\n+                \"text\": \" the same way that Mr. Karker used to flash his teeth. And Mr. John Collier gives his\",\n+                \"timestamp\": (64.12, 70.76),\n+            },\n+            {\n+                \"text\": \" sitter a cheerful slap on the back before he says, like a shampoo or in a Turkish bath,\",\n+                \"timestamp\": (70.76, 77.16),\n+            },\n+            {\n+                \"text\": \" Next Man\",\n+                \"timestamp\": (77.16, 78.16),\n+            },\n+        ]\n+\n+        transcript = processor.batch_decode(generated_ids[\"sequences\"], skip_special_tokens=True, output_offsets=True)\n+        self.assertEqual(transcript[0][\"offsets\"], EXPECTED_TRANSCRIPT)\n+\n+        transcript_segments = [\n+            {\n+                \"text\": processor.decode(seg[\"tokens\"], skip_special_tokens=True),\n+                \"timestamp\": (seg[\"start\"].item(), seg[\"end\"].item()),\n+            }\n+            for seg in generated_ids[\"segments\"][0]\n+        ]\n+        self.assertEqual(transcript_segments, EXPECTED_TRANSCRIPT)\n+\n     @slow\n     def test_large_timestamp_generation(self):\n         set_seed(0)"
        }
    ],
    "stats": {
        "total": 198,
        "additions": 173,
        "deletions": 25
    }
}