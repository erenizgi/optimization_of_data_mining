{
    "author": "hiyouga",
    "message": "fix llama4 training (#37319)",
    "sha": "debfe904c94d1104e94c71d2276a0d522f0f5081",
    "files": [
        {
            "sha": "707731114dec465df001485b42e2888ed4bdc3d7",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/debfe904c94d1104e94c71d2276a0d522f0f5081/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/debfe904c94d1104e94c71d2276a0d522f0f5081/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=debfe904c94d1104e94c71d2276a0d522f0f5081",
            "patch": "@@ -356,7 +356,7 @@ def forward(\n             attn_scales = (\n                 torch.log(torch.floor((cache_position.float() + 1.0) / self.floor_scale) + 1.0) * self.attn_scale + 1.0\n             )\n-            attn_scales = attn_scales.view((*input_shape, 1, 1))\n+            attn_scales = attn_scales.view((1, input_shape[-1], 1, 1)).expand((*input_shape, 1, 1))  # batch size > 1\n             query_states = (query_states * attn_scales).to(query_states.dtype)\n \n         query_states = query_states.transpose(1, 2)\n@@ -692,6 +692,7 @@ def forward(\n                     position_ids,\n                     past_key_values,\n                     output_attentions,\n+                    False,  # output_router_logits is False\n                     use_cache,\n                     cache_position,\n                     freq_cis,\n@@ -1375,6 +1376,7 @@ def forward(\n                 layer_outputs = self._gradient_checkpointing_func(\n                     encoder_layer.__call__,\n                     hidden_states,\n+                    freqs_ci,\n                     attention_mask,\n                     output_attentions,\n                 )\n@@ -1445,7 +1447,7 @@ def forward(self, hidden_states):\n \n class Llama4VisionModel(Llama4PreTrainedModel):\n     base_model_prefix = \"vision_model\"\n-    _no_split_modules = [\"Llama4VisionAttention\"]\n+    _no_split_modules = [\"Llama4VisionEncoderLayer\"]\n     config_class = Llama4VisionConfig\n \n     def __init__(self, config: Llama4VisionConfig):\n@@ -1754,8 +1756,7 @@ def forward(\n                 )\n \n             expanded_mask = final_mask_1d.unsqueeze(-1).expand(-1, inputs_embeds.size(-1))\n-            inputs_embeds.masked_scatter_(expanded_mask, projected_vision_flat)\n-\n+            inputs_embeds = inputs_embeds.masked_scatter(expanded_mask, projected_vision_flat)\n             inputs_embeds = inputs_embeds.view(original_inputs_embeds_shape)\n \n         outputs = self.language_model("
        }
    ],
    "stats": {
        "total": 9,
        "additions": 5,
        "deletions": 4
    }
}