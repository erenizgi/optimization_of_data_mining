{
    "author": "ArthurZucker",
    "message": "Add support for __all__ and potentilly deleting functions (#33859)\n\n* Add support for __all__ and potentailly deleting functions\r\n\r\n* updates\r\n\r\n* update\r\n\r\n* nits\r\n\r\n* remove dummies\r\n\r\n* fix warning\r\n\r\n* fixup\r\n\r\n* style\r\n\r\n* update\r\n\r\n* fixup\r\n\r\n* skip copied from when # skip\r\n\r\n* remove log\r\n\r\n* bring dummies back\r\n\r\n* fixup\r\n\r\n* remove copied from\r\n\r\n* fixup\r\n\r\n* remove warnings from `make fix-copies`\r\n\r\n* fix doc issues\r\n\r\n* nits\r\n\r\n* Better error message !\r\n\r\n* add support for more flexible naming!\r\n\r\n* style\r\n\r\n* breaking style?\r\n\r\n* fix super() renaming issues\r\n\r\n* del not needed when you don't call super().__init__()\r\n\r\n* style\r\n\r\n* no more fmt on :)\r\n\r\n* properly remove `self`\r\n\r\n* fixup\r\n\r\n* fix\r\n\r\n* doc nits\r\n\r\n* add some doc ğŸ«¡",
    "sha": "a3add29097b5d0aadb57020507240b29d47dd7b3",
    "files": [
        {
            "sha": "dbc8d9116ed36eb37cd4372da7539354f0075c6a",
            "filename": "docs/source/en/modular_transformers.md",
            "status": "modified",
            "additions": 57,
            "deletions": 1,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3add29097b5d0aadb57020507240b29d47dd7b3/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3add29097b5d0aadb57020507240b29d47dd7b3/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodular_transformers.md?ref=a3add29097b5d0aadb57020507240b29d47dd7b3",
            "patch": "@@ -118,4 +118,60 @@ Additionally, you may find a list of examples here:\n \n ## What it is not\n \n-It is not a replacement for the modeling code (yet?), and if your model is not based on anything else that ever existed, then you can add a `modeling` file as usual.\n\\ No newline at end of file\n+It is not a replacement for the modeling code (yet?), and if your model is not based on anything else that ever existed, then you can add a `modeling` file as usual.\n+\n+\n+## Advanced usage\n+\n+### Removing attributes and functions\n+To remove attributes that are not used in your modular model, and that you don't want to see in the unravelled modeling: \n+\n+```python\n+class GemmaModel(LlamaModel):                 |           class GemmaModel(PreTrainedModel):\n+    def __init__(self, config):               |              def __init__(self, config):\n+        super().__init__(self, eos_token)     |                 super().__init__(config)\n+        del self.embed_tokens                 |                 self.padding_idx = config.pad_token_id\n+                                              |                 self.vocab_size = config.vocab_size\n+                                              |\n+                                              |                 self.layers = nn.ModuleList(\n+                                              |                     [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+                                              |                 )\n+                                              |                 self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+                                              |                 self.rotary_emb = LlamaRotaryEmbedding(config=config)\n+                                              |                 self.gradient_checkpointing = False\n+                                              |                 \n+                                              |                 # Initialize weights and apply final processing\n+                                              |                 self.post_init()\n+```\n+If you check the original `LlamaModel`, it has a `embed_tokens` which was removed here (as you would expect!)\n+\n+Removing a function is pretty similar, you just need to write it with a `raise ValueError(\"\")` to mimick the behaviour you actually want when you remove a parent function in python.\n+\n+```python\n+class GemmaTokenizer(LlamaTokenizer):\n+    ...\n+\n+    def get_spm_processor(self):\n+        raise AttributeError(\"Not needed for Gemma\")\n+\n+    def unk_token_length(self):\n+        raise AttributeError(\"Not needed for Gemma\")\n+```\n+\n+### Calling `super()`\n+We recently shipped a few features that allow you to go from:\n+```python\n+class GemmaTokenizer(LlamaTokenizer, PretrainedTokenizerFast):         |           class GemmaModel(nn.Module):\n+    def __init__(self, eos_token=\"</s>\"):                              |             def __init__(self):\n+        eos_token = AddedToken(eos_token)                              |                eos_token = AddedToken(eos_token)\n+        PretrainedTokenizerFast.__init__(self, eos_token)              |                super().__init__(eos_token)\n+```\n+This is useful want you **don't** want to unravel the call to `super()`, and you want to differentiate which super init call you are doing!\n+\n+### Special naming\n+We now also support special cases like\n+```python\n+class GemmaVisionModel(CLIPModel):                                 \n+    pass\n+```\n+where the name of your class `GemmaVision` is not the same as the modular `Gemma`. This is super useful for composite models\n\\ No newline at end of file"
        },
        {
            "sha": "b5b1fc6aec85e629efc97530979212c87b8b2e21",
            "filename": "examples/modular-transformers/modeling_dummy.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3add29097b5d0aadb57020507240b29d47dd7b3/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3add29097b5d0aadb57020507240b29d47dd7b3/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy.py?ref=a3add29097b5d0aadb57020507240b29d47dd7b3",
            "patch": "@@ -4,7 +4,6 @@\n #         the file from the modular. If any change should be done, please apply the change to the\n #                           modular_xxx.py file directly. One of our CI enforces this\n #           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-\n import math\n from typing import List, Optional, Tuple, Union\n "
        },
        {
            "sha": "f8908f7d53a0dfac1d97e605d2ac2f75e75a0a47",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3add29097b5d0aadb57020507240b29d47dd7b3/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3add29097b5d0aadb57020507240b29d47dd7b3/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=a3add29097b5d0aadb57020507240b29d47dd7b3",
            "patch": "@@ -115,7 +115,6 @@\n     \"data.metrics\": [],\n     \"data.processors\": [],\n     \"debug_utils\": [],\n-    \"deepspeed\": [],\n     \"dependency_versions_check\": [],\n     \"dependency_versions_table\": [],\n     \"dynamic_module_utils\": [],"
        },
        {
            "sha": "6fd22d8c5cbaf3b0642620405fb6ec3384d56ba9",
            "filename": "src/transformers/deepspeed.py",
            "status": "removed",
            "additions": 0,
            "deletions": 41,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fdeepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bead0fa8dca2407f05f1a422e6d9ec9fee82a07b/src%2Ftransformers%2Fdeepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdeepspeed.py?ref=bead0fa8dca2407f05f1a422e6d9ec9fee82a07b",
            "patch": "@@ -1,41 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Integration with Deepspeed - kept for backward compatiblity, if you plan to make any edit, make sure to modify the file\n-in `integrations/deepspeed` instead.\n-\n-Check: https://github.com/huggingface/transformers/pull/25599\n-\"\"\"\n-\n-import warnings\n-\n-\n-warnings.warn(\n-    \"transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\",\n-    FutureWarning,\n-)\n-\n-# Backward compatibility imports, to make sure all those objects can be found in integrations/deepspeed\n-from .integrations.deepspeed import (  # noqa\n-    HfDeepSpeedConfig,\n-    HfTrainerDeepSpeedConfig,\n-    deepspeed_config,\n-    deepspeed_init,\n-    deepspeed_load_checkpoint,\n-    deepspeed_optim_sched,\n-    is_deepspeed_available,\n-    is_deepspeed_zero3_enabled,\n-    set_hf_deepspeed_config,\n-    unset_hf_deepspeed_config,\n-)"
        },
        {
            "sha": "90255086eef8176ffc0baa5e1af8c5b9e4ef0a9b",
            "filename": "src/transformers/models/gemma/configuration_gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3add29097b5d0aadb57020507240b29d47dd7b3/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3add29097b5d0aadb57020507240b29d47dd7b3/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py?ref=a3add29097b5d0aadb57020507240b29d47dd7b3",
            "patch": "@@ -143,3 +143,6 @@ def __init__(\n             tie_word_embeddings=tie_word_embeddings,\n             **kwargs,\n         )\n+\n+\n+__all__ = [\"GemmaConfig\"]"
        },
        {
            "sha": "df7f38014a14f01646e0d8d2a91a6ffd47dc862f",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3add29097b5d0aadb57020507240b29d47dd7b3/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3add29097b5d0aadb57020507240b29d47dd7b3/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=a3add29097b5d0aadb57020507240b29d47dd7b3",
            "patch": "@@ -1314,3 +1314,6 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\"GemmaModel\", \"GemmaForCausalLM\", \"GemmaForSequenceClassification\", \"GemmaForTokenClassification\"]"
        },
        {
            "sha": "7130a30dc9be58ea248a9a43a65b08675bdab967",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 178,
            "deletions": 1,
            "changes": 179,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3add29097b5d0aadb57020507240b29d47dd7b3/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3add29097b5d0aadb57020507240b29d47dd7b3/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=a3add29097b5d0aadb57020507240b29d47dd7b3",
            "patch": "@@ -14,8 +14,9 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import math\n-from typing import List, Optional, Tuple, Union\n+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union\n \n+import sentencepiece as spm\n import torch\n import torch.utils.checkpoint\n from torch import nn\n@@ -27,6 +28,7 @@\n from ...modeling_flash_attention_utils import _flash_attention_forward\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n+from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n from ..llama.modeling_llama import (\n     LlamaDecoderLayer,\n@@ -38,6 +40,15 @@\n     apply_rotary_pos_emb,\n     repeat_kv,\n )\n+from ..llama.tokenization_llama import LlamaTokenizer\n+\n+\n+if TYPE_CHECKING:\n+    from ...tokenization_utils_base import TextInput\n+\n+VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer.model\"}\n+\n+SPIECE_UNDERLINE = \"â–\"\n \n \n logger = logging.get_logger(__name__)\n@@ -164,6 +175,162 @@ def __init__(\n         )\n \n \n+class GemmaTokenizer(LlamaTokenizer, PreTrainedTokenizer):\n+    \"\"\"\n+    Construct a Gemma tokenizer. Based on byte-level Byte-Pair-Encoding. The default padding token is unset as there is\n+    no padding token in the original model.\n+\n+    Args:\n+        vocab_file (`str`):\n+            Path to the vocabulary file.\n+        unk_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<unk>\"`):\n+            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n+            token instead.\n+        bos_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<bos>\"`):\n+            The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n+        eos_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<eos>\"`):\n+            The end of sequence token.\n+        pad_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<pad>\"`):\n+            A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by\n+            attention mechanisms or loss computation.\n+        sp_model_kwargs (`Dict[str, Any]`, `Optional`, *optional*):\n+            Will be passed to the `SentencePieceProcessor.__init__()` method. The [Python wrapper for\n+            SentencePiece](https://github.com/google/sentencepiece/tree/master/python) can be used, among other things,\n+            to set:\n+\n+            - `enable_sampling`: Enable subword regularization.\n+            - `nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.\n+\n+              - `nbest_size = {0,1}`: No sampling is performed.\n+              - `nbest_size > 1`: samples from the nbest_size results.\n+              - `nbest_size < 0`: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)\n+                using forward-filtering-and-backward-sampling algorithm.\n+\n+            - `alpha`: Smoothing parameter for unigram sampling, and dropout probability of merge operations for\n+              BPE-dropout.\n+\n+        add_bos_token (`bool`, *optional*, defaults to `True`):\n+            Whether or not to add an `bos_token` at the start of sequences.\n+        add_eos_token (`bool`, *optional*, defaults to `False`):\n+            Whether or not to add an `eos_token` at the end of sequences.\n+        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n+            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n+            extra spaces.\n+        use_default_system_prompt (`bool`, *optional*, defaults to `False`):\n+            Whether or not the default system prompt for Gemma should be used.\n+        spaces_between_special_tokens (`bool`, *optional*, defaults to `False`):\n+            Whether or not to add spaces between special tokens.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        vocab_file,\n+        unk_token=\"<unk>\",\n+        bos_token=\"<bos>\",\n+        eos_token=\"<eos>\",\n+        pad_token=\"<pad>\",\n+        sp_model_kwargs: Optional[Dict[str, Any]] = None,\n+        add_bos_token=True,\n+        add_eos_token=False,\n+        clean_up_tokenization_spaces=False,\n+        use_default_system_prompt=False,\n+        spaces_between_special_tokens=False,\n+        **kwargs,\n+    ):\n+        self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\n+        bos_token = AddedToken(bos_token, normalized=False, special=True) if isinstance(bos_token, str) else bos_token\n+        eos_token = AddedToken(eos_token, normalized=False, special=True) if isinstance(eos_token, str) else eos_token\n+        unk_token = AddedToken(unk_token, normalized=False, special=True) if isinstance(unk_token, str) else unk_token\n+        pad_token = AddedToken(pad_token, normalized=False, special=True) if isinstance(pad_token, str) else pad_token\n+\n+        self.vocab_file = vocab_file\n+        self.add_bos_token = add_bos_token\n+        self.add_eos_token = add_eos_token\n+        self.use_default_system_prompt = use_default_system_prompt\n+        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n+        self.sp_model.Load(vocab_file)\n+\n+        PreTrainedTokenizer.__init__(\n+            self,\n+            bos_token=bos_token,\n+            eos_token=eos_token,\n+            unk_token=unk_token,\n+            pad_token=pad_token,\n+            add_bos_token=add_bos_token,\n+            add_eos_token=add_eos_token,\n+            sp_model_kwargs=sp_model_kwargs,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n+            use_default_system_prompt=use_default_system_prompt,\n+            spaces_between_special_tokens=spaces_between_special_tokens,\n+            **kwargs,\n+        )\n+\n+    def get_spm_processor(self):\n+        raise AttributeError(\"Not needed for Gemma\")\n+\n+    def unk_token_length(self):\n+        raise AttributeError(\"Not needed for Gemma\")\n+\n+    def tokenize(self, text: \"TextInput\", **kwargs) -> List[str]:\n+        \"\"\"\n+        Args:\n+            text: TextInput\n+        Simply calls PreTrainedTokenizer's method\n+        \"\"\"\n+        return PreTrainedTokenizer.tokenize(self, text, **kwargs)\n+\n+    def _tokenize(self, text, **kwargs):\n+        \"\"\"\n+        Args:\n+            text: TextInput\n+        Returns a tokenized string. The Gemma tokenizer never adds a prefix space.\n+        \"\"\"\n+        return self.sp_model.encode(text, out_type=str)\n+\n+    def _decode(\n+        self,\n+        token_ids: List[int],\n+        skip_special_tokens: bool = False,\n+        spaces_between_special_tokens: bool = False,\n+        **kwargs,\n+    ) -> str:\n+        sub_texts = []\n+        current_sub_text = []\n+        for ids in token_ids:\n+            if skip_special_tokens and ids in self.all_special_ids:\n+                continue\n+            if ids in self._added_tokens_decoder:\n+                if current_sub_text:\n+                    sub_texts.append(self.sp_model.decode(current_sub_text))\n+                sub_texts.append(self._added_tokens_decoder[ids].content)\n+                current_sub_text = []\n+            else:\n+                current_sub_text.append(ids)\n+        if current_sub_text:\n+            sub_texts.append(self.sp_model.decode(current_sub_text))\n+\n+        if spaces_between_special_tokens:\n+            sub_texts = \" \".join(sub_texts)\n+        else:\n+            sub_texts = \"\".join(sub_texts)\n+\n+        return sub_texts.replace(SPIECE_UNDERLINE, \" \")\n+\n+    def convert_tokens_to_string(self, tokens):\n+        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n+        current_sub_tokens = []\n+        out_string = \"\"\n+        for token in tokens:\n+            # make sure that special tokens are not decoded using sentencepiece model\n+            if token in self._added_tokens_encoder:\n+                out_string += self.sp_model.decode(current_sub_tokens) + token\n+                current_sub_tokens = []\n+            else:\n+                current_sub_tokens.append(token)\n+        out_string += self.sp_model.decode(current_sub_tokens)\n+        return out_string\n+\n+\n class GemmaRMSNorm(nn.Module):\n     def __init__(self, dim: int, eps: float = 1e-6):\n         super().__init__()\n@@ -874,3 +1041,13 @@ def __init__(self, config):\n         super().__init__(config)\n         self.model = GemmaModel(config)\n         self.post_init()\n+\n+\n+__all__ = [\n+    \"GemmaConfig\",\n+    \"GemmaTokenizer\",\n+    \"GemmaModel\",\n+    \"GemmaForCausalLM\",\n+    \"GemmaForSequenceClassification\",\n+    \"GemmaForTokenClassification\",\n+]"
        },
        {
            "sha": "5233037262fe766add0e045e92852bb7ebe54910",
            "filename": "src/transformers/models/gemma/tokenization_gemma.py",
            "status": "modified",
            "additions": 52,
            "deletions": 46,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3add29097b5d0aadb57020507240b29d47dd7b3/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3add29097b5d0aadb57020507240b29d47dd7b3/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma.py?ref=a3add29097b5d0aadb57020507240b29d47dd7b3",
            "patch": "@@ -1,5 +1,12 @@\n+#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#               This file was automatically generated from <path_to_modular_file.py>.\n+#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#         the file from the modular. If any change should be done, please apply the change to the\n+#                           modular_xxx.py file directly. One of our CI enforces this\n+#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n # coding=utf-8\n-# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+# Copyright 2024 Google Inc. HuggingFace Inc. team. All rights reserved.\n+#\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -12,9 +19,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n-\"\"\"Tokenization classes for Gemma.\"\"\"\n-\n import os\n from shutil import copyfile\n from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple\n@@ -26,7 +30,7 @@\n \n \n if TYPE_CHECKING:\n-    pass\n+    from ...tokenization_utils_base import TextInput\n \n logger = logging.get_logger(__name__)\n \n@@ -110,7 +114,6 @@ def __init__(\n         self.add_bos_token = add_bos_token\n         self.add_eos_token = add_eos_token\n         self.use_default_system_prompt = use_default_system_prompt\n-\n         self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n         self.sp_model.Load(vocab_file)\n \n@@ -121,85 +124,60 @@ def __init__(\n             pad_token=pad_token,\n             add_bos_token=add_bos_token,\n             add_eos_token=add_eos_token,\n-            sp_model_kwargs=self.sp_model_kwargs,\n+            sp_model_kwargs=sp_model_kwargs,\n             clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n             use_default_system_prompt=use_default_system_prompt,\n             spaces_between_special_tokens=spaces_between_special_tokens,\n             **kwargs,\n         )\n \n-    # Copied from transformers.models.llama.tokenization_llama.LlamaTokenizer.__getstate__\n     def __getstate__(self):\n         state = self.__dict__.copy()\n         state[\"sp_model\"] = None\n         state[\"sp_model_proto\"] = self.sp_model.serialized_model_proto()\n         return state\n \n-    # Copied from transformers.models.llama.tokenization_llama.LlamaTokenizer.__setstate__\n     def __setstate__(self, d):\n         self.__dict__ = d\n         self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n         self.sp_model.LoadFromSerializedProto(self.sp_model_proto)\n \n     @property\n-    # Copied from transformers.models.llama.tokenization_llama.LlamaTokenizer.vocab_size\n     def vocab_size(self):\n         \"\"\"Returns vocab size\"\"\"\n         return self.sp_model.get_piece_size()\n \n-    # Copied from transformers.models.llama.tokenization_llama.LlamaTokenizer.get_vocab\n     def get_vocab(self):\n         \"\"\"Returns vocab as a dict\"\"\"\n         vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size)}\n         vocab.update(self.added_tokens_encoder)\n         return vocab\n \n+    def tokenize(self, text: \"TextInput\", **kwargs) -> List[str]:\n+        \"\"\"\n+        Args:\n+            text: TextInput\n+        Simply calls PreTrainedTokenizer's method\n+        \"\"\"\n+        return super().tokenize(text, **kwargs)\n+\n     def _tokenize(self, text, **kwargs):\n         \"\"\"\n+        Args:\n+            text: TextInput\n         Returns a tokenized string. The Gemma tokenizer never adds a prefix space.\n         \"\"\"\n         return self.sp_model.encode(text, out_type=str)\n \n-    # Copied from transformers.models.llama.tokenization_llama.LlamaTokenizer._convert_token_to_id\n     def _convert_token_to_id(self, token):\n         \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n         return self.sp_model.piece_to_id(token)\n \n-    # Copied from transformers.models.llama.tokenization_llama.LlamaTokenizer._convert_id_to_token\n     def _convert_id_to_token(self, index):\n         \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n         token = self.sp_model.IdToPiece(index)\n         return token\n \n-    def _decode(\n-        self,\n-        token_ids: List[int],\n-        skip_special_tokens: bool = False,\n-        spaces_between_special_tokens: bool = False,\n-        **kwargs,\n-    ) -> str:\n-        sub_texts = []\n-        current_sub_text = []\n-        for ids in token_ids:\n-            if skip_special_tokens and ids in self.all_special_ids:\n-                continue\n-            if ids in self._added_tokens_decoder:\n-                if current_sub_text:\n-                    sub_texts.append(self.sp_model.decode(current_sub_text))\n-                sub_texts.append(self._added_tokens_decoder[ids].content)\n-                current_sub_text = []\n-            else:\n-                current_sub_text.append(ids)\n-        if current_sub_text:\n-            sub_texts.append(self.sp_model.decode(current_sub_text))\n-\n-        if spaces_between_special_tokens:\n-            sub_texts = \" \".join(sub_texts)\n-        else:\n-            sub_texts = \"\".join(sub_texts)\n-\n-        return sub_texts.replace(SPIECE_UNDERLINE, \" \")\n-\n     def convert_tokens_to_string(self, tokens):\n         \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n         current_sub_tokens = []\n@@ -214,7 +192,6 @@ def convert_tokens_to_string(self, tokens):\n         out_string += self.sp_model.decode(current_sub_tokens)\n         return out_string\n \n-    # Copied from transformers.models.llama.tokenization_llama.LlamaTokenizer.save_vocabulary\n     def save_vocabulary(self, save_directory, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         \"\"\"\n         Save the vocabulary and special tokens file to a directory.\n@@ -242,7 +219,6 @@ def save_vocabulary(self, save_directory, filename_prefix: Optional[str] = None)\n \n         return (out_vocab_file,)\n \n-    # Copied from transformers.models.llama.tokenization_llama.LlamaTokenizer.build_inputs_with_special_tokens\n     def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n         bos_token_id = [self.bos_token_id] if self.add_bos_token else []\n         eos_token_id = [self.eos_token_id] if self.add_eos_token else []\n@@ -254,7 +230,6 @@ def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n \n         return output\n \n-    # Copied from transformers.models.llama.tokenization_llama.LlamaTokenizer.get_special_tokens_mask\n     def get_special_tokens_mask(\n         self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n     ) -> List[int]:\n@@ -292,7 +267,6 @@ def get_special_tokens_mask(\n             + eos_token_id\n         )\n \n-    # Copied from transformers.models.llama.tokenization_llama.LlamaTokenizer.create_token_type_ids_from_sequences\n     def create_token_type_ids_from_sequences(\n         self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n     ) -> List[int]:\n@@ -325,3 +299,35 @@ def create_token_type_ids_from_sequences(\n             output += [1] * len(bos_token_id + token_ids_1 + eos_token_id)\n \n         return output\n+\n+    def _decode(\n+        self,\n+        token_ids: List[int],\n+        skip_special_tokens: bool = False,\n+        spaces_between_special_tokens: bool = False,\n+        **kwargs,\n+    ) -> str:\n+        sub_texts = []\n+        current_sub_text = []\n+        for ids in token_ids:\n+            if skip_special_tokens and ids in self.all_special_ids:\n+                continue\n+            if ids in self._added_tokens_decoder:\n+                if current_sub_text:\n+                    sub_texts.append(self.sp_model.decode(current_sub_text))\n+                sub_texts.append(self._added_tokens_decoder[ids].content)\n+                current_sub_text = []\n+            else:\n+                current_sub_text.append(ids)\n+        if current_sub_text:\n+            sub_texts.append(self.sp_model.decode(current_sub_text))\n+\n+        if spaces_between_special_tokens:\n+            sub_texts = \" \".join(sub_texts)\n+        else:\n+            sub_texts = \"\".join(sub_texts)\n+\n+        return sub_texts.replace(SPIECE_UNDERLINE, \" \")\n+\n+\n+__all__ = [\"GemmaTokenizer\"]"
        },
        {
            "sha": "c3a2c7add309e1f04b248086dc16043a2359b99a",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3add29097b5d0aadb57020507240b29d47dd7b3/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3add29097b5d0aadb57020507240b29d47dd7b3/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=a3add29097b5d0aadb57020507240b29d47dd7b3",
            "patch": "@@ -58,7 +58,6 @@\n \n \n @dataclass\n-# Copied from transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput with Blip2->InstructBlipVideo\n class InstructBlipVideoForConditionalGenerationModelOutput(ModelOutput):\n     \"\"\"\n     Class defining the outputs of [`InstructBlipVideoForConditionalGeneration`].\n@@ -91,7 +90,6 @@ def to_tuple(self) -> Tuple[Any]:\n         )\n \n \n-# Copied from transformers.models.blip.modeling_blip.BlipVisionEmbeddings with Blip->InstructBlipVideo\n class InstructBlipVideoVisionEmbeddings(nn.Module):\n     def __init__(self, config: InstructBlipVideoVisionConfig):\n         super().__init__()\n@@ -166,7 +164,6 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding: boo\n         return embeddings\n \n \n-# Copied from transformers.models.blip_2.modeling_blip_2.Blip2Attention with Blip2->InstructBlipVideo\n class InstructBlipVideoAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -248,7 +245,6 @@ def forward(\n         return outputs\n \n \n-# Copied from transformers.models.blip.modeling_blip.BlipMLP\n class InstructBlipVideoMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -264,7 +260,6 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-# Copied from transformers.models.blip.modeling_blip.BlipEncoderLayer with Blip->InstructBlipVideo\n class InstructBlipVideoEncoderLayer(nn.Module):\n     def __init__(self, config: InstructBlipVideoConfig):\n         super().__init__()\n@@ -330,7 +325,6 @@ class InstructBlipVideoPreTrainedModel(PreTrainedModel):\n     ]\n     _keep_in_fp32_modules = []\n \n-    # Copied from transformers.models.blip_2.modeling_blip_2.Blip2PreTrainedModel._init_weights with Blip2->InstructBlipVideo\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_range\n@@ -450,7 +444,6 @@ def _init_weights(self, module):\n \"\"\"\n \n \n-# Copied from transformers.models.blip.modeling_blip.BlipEncoder with Blip->InstructBlipVideo\n class InstructBlipVideoEncoder(nn.Module):\n     \"\"\"\n     Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n@@ -537,7 +530,6 @@ def forward(\n         )\n \n \n-# Copied from transformers.models.blip.modeling_blip.BlipVisionModel with Blip->InstructBlipVideo, BLIP->INSTRUCTBLIPVIDEO\n class InstructBlipVideoVisionModel(InstructBlipVideoPreTrainedModel):\n     main_input_name = \"pixel_values\"\n     config_class = InstructBlipVideoVisionConfig\n@@ -738,7 +730,6 @@ def forward(\n         return outputs\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->InstructBlipVideoQFormer\n class InstructBlipVideoQFormerSelfOutput(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -753,7 +744,6 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-# Copied from transformers.models.blip_2.modeling_blip_2.Blip2QFormerAttention with Blip2->InstructBlipVideo\n class InstructBlipVideoQFormerAttention(nn.Module):\n     def __init__(self, config, is_cross_attention=False):\n         super().__init__()\n@@ -803,7 +793,6 @@ def forward(\n         return outputs\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->InstructBlipVideoQFormer\n class InstructBlipVideoQFormerIntermediate(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -819,7 +808,6 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->InstructBlipVideoQFormer\n class InstructBlipVideoQFormerOutput(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -937,7 +925,6 @@ def feed_forward_chunk_query(self, attention_output):\n         return layer_output\n \n \n-# Copied from transformers.models.blip_2.modeling_blip_2.Blip2QFormerEncoder with Blip2->InstructBlipVideo\n class InstructBlipVideoQFormerEncoder(nn.Module):\n     def __init__(self, config):\n         super().__init__()"
        },
        {
            "sha": "8e99e4eef59d68906e6ab958e00bacbce75a9014",
            "filename": "src/transformers/models/llama/tokenization_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3add29097b5d0aadb57020507240b29d47dd7b3/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3add29097b5d0aadb57020507240b29d47dd7b3/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama.py?ref=a3add29097b5d0aadb57020507240b29d47dd7b3",
            "patch": "@@ -43,14 +43,12 @@\n B_INST, E_INST = \"[INST]\", \"[/INST]\"\n B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n \n-# fmt: off\n DEFAULT_SYSTEM_PROMPT = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your \\\n answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure\\\n  that your responses are socially unbiased and positive in nature.\n \n If a question does not make any sense, or is not factually coherent, explain why instead of answering something not \\\n-correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n-# fmt: on\n+correct. If you don't know the answer to a question, please don't share false information.\"\"\"  # fmt: skip\n \n \n class LlamaTokenizer(PreTrainedTokenizer):"
        },
        {
            "sha": "ea1114df7c2ce7bb43ae4ef31544211a6b2a3391",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3add29097b5d0aadb57020507240b29d47dd7b3/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3add29097b5d0aadb57020507240b29d47dd7b3/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=a3add29097b5d0aadb57020507240b29d47dd7b3",
            "patch": "@@ -235,7 +235,6 @@ def forward(self, image_features):\n         return image_features_spatial_pool.flatten(2).transpose(1, 2).contiguous()\n \n \n-# Copied from transformers.models.llava.modeling_llava.LlavaMultiModalProjector with Llava->LlavaNextVideo\n class LlavaNextVideoMultiModalProjector(nn.Module):\n     def __init__(self, config: LlavaNextVideoConfig):\n         super().__init__()\n@@ -272,7 +271,6 @@ def forward(self, image_features):\n     \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n     LLAVA_NEXT_VIDEO_START_DOCSTRING,\n )\n-# Copied from transformers.models.llava.modeling_llava.LlavaPreTrainedModel with Llava->LlavaNextVideo,llava->llava_next_video\n class LlavaNextVideoPreTrainedModel(PreTrainedModel):\n     config_class = LlavaNextVideoConfig\n     base_model_prefix = \"model\"\n@@ -426,35 +424,27 @@ def padding_side(self, padding_side: str):\n             raise ValueError(f\"{padding_side} is not `left` or `right`.\")\n         self._padding_side = padding_side\n \n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.get_input_embeddings\n     def get_input_embeddings(self):\n         return self.language_model.get_input_embeddings()\n \n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.set_input_embeddings\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.get_output_embeddings\n     def get_output_embeddings(self):\n         return self.language_model.get_output_embeddings()\n \n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.set_output_embeddings\n     def set_output_embeddings(self, new_embeddings):\n         self.language_model.set_output_embeddings(new_embeddings)\n \n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.set_decoder\n     def set_decoder(self, decoder):\n         self.language_model.set_decoder(decoder)\n \n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.get_decoder\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n \n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.tie_weights\n     def tie_weights(self):\n         return self.language_model.tie_weights()\n \n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.resize_token_embeddings\n     def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of=None) -> nn.Embedding:\n         model_embeds = self.language_model.resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n         # update vocab size"
        },
        {
            "sha": "eb606208bf7bf6ed35de264c1491b9d66913f142",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3add29097b5d0aadb57020507240b29d47dd7b3/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3add29097b5d0aadb57020507240b29d47dd7b3/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=a3add29097b5d0aadb57020507240b29d47dd7b3",
            "patch": "@@ -25,8 +25,8 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...deepspeed import is_deepspeed_zero3_enabled\n from ...generation import GenerationMixin\n+from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutput,"
        },
        {
            "sha": "da44913e747c86ad2dd70d7e7f66b193a62409fa",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3add29097b5d0aadb57020507240b29d47dd7b3/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3add29097b5d0aadb57020507240b29d47dd7b3/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=a3add29097b5d0aadb57020507240b29d47dd7b3",
            "patch": "@@ -25,8 +25,8 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...deepspeed import is_deepspeed_zero3_enabled\n from ...generation import GenerationMixin\n+from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutput,"
        },
        {
            "sha": "cc3089da3f38faca3b62b822467949d3956715b0",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 181,
            "deletions": 29,
            "changes": 210,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3add29097b5d0aadb57020507240b29d47dd7b3/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3add29097b5d0aadb57020507240b29d47dd7b3/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=a3add29097b5d0aadb57020507240b29d47dd7b3",
            "patch": "@@ -16,7 +16,8 @@\n import glob\n import importlib\n import re\n-from typing import Dict\n+from collections import defaultdict\n+from typing import Dict, List, Set\n \n import libcst as cst\n from check_copies import run_ruff\n@@ -113,7 +114,11 @@ def visit_SimpleStatementLine(self, node):\n         if m.matches(node, m.SimpleStatementLine(body=[m.Assign()])) and m.matches(\n             self.get_metadata(cst.metadata.ParentNodeProvider, node), m.Module()\n         ):\n-            self.assignments[node.body[0].targets[0].target.value] = node\n+            if hasattr(node.body[0].targets[0].target, \"value\"):\n+                self.assignments[node.body[0].targets[0].target.value] = node\n+            else:\n+                for idx, target in enumerate(list(node.body[0].targets[0].target.elements)):\n+                    self.assignments[target.value.value] = node.body[0].value.elements[idx].value\n         if m.matches(node, m.SimpleStatementLine(body=[m.Import() | m.ImportFrom()])):\n             self.imports[node.body[0].names] = node\n \n@@ -217,11 +222,21 @@ def replace(match):\n \n         return compiled_regex.sub(replace, text)\n \n+    def convert_to_camelcase(self, text):\n+        # Regex pattern to match consecutive uppercase letters and lowercase the first set\n+        result = re.sub(r\"^[A-Z]+(?=[A-Z][a-z])\", lambda m: m.group(0).capitalize(), text, count=1)\n+        return result\n+\n     @m.leave(m.Name() | m.SimpleString() | m.Comment())\n     def replace_name(self, original_node, updated_node):\n+        if re.findall(r\"# Copied from\", updated_node.value):\n+            return cst.RemoveFromParent()\n         update = self.preserve_case_replace(updated_node.value)\n         return updated_node.with_changes(value=update)\n \n+    def leave_ClassDef(self, original_node, updated_node):\n+        return updated_node.with_changes(name=cst.Name(self.convert_to_camelcase(updated_node.name.value)))\n+\n \n def find_classes_in_file(module: cst.Module, old_id=\"llama\", new_id=\"gemma\", given_old_name=None, given_new_name=None):\n     \"\"\"Helper function to rename and then parse a source file using the ClassFinder\"\"\"\n@@ -251,6 +266,63 @@ def SUPER_CALL_NODE(func_name):\n     return m.Call(func=m.Attribute(value=m.Call(func=m.Name(\"super\")), attr=m.Name(func_name)))\n \n \n+def is_call_to_super(node, func_name):\n+    return m.matches(\n+        node, m.SimpleStatementLine(body=[m.Return(SUPER_CALL_NODE(func_name)) | m.Expr(SUPER_CALL_NODE(func_name))])\n+    )\n+\n+\n+# Transformer class to replace ClassB.call_to_method and ClassB().call_to_method with super().call_to_method\n+class ReplaceMethodCallTransformer(cst.CSTTransformer):\n+    def __init__(self, all_bases: Set[str]):\n+        self.all_bases = all_bases\n+\n+    def leave_Attribute(self, original_node: cst.Attribute, updated_node: cst.Attribute) -> cst.CSTNode:\n+        # Handle ClassB.call_to_method\n+        if (\n+            isinstance(original_node.value, cst.Name)\n+            and original_node.value.value in self.all_bases\n+            and isinstance(original_node.attr, cst.Name)\n+        ):\n+            # Replace with super().call_to_method\n+            return updated_node.with_changes(\n+                value=cst.Call(cst.Name(\"super\")),\n+            )\n+        # Handle ClassB().call_to_method\n+        elif (\n+            isinstance(original_node.value, cst.Call)\n+            and isinstance(original_node.value.func, cst.Name)\n+            and original_node.value.func.value in self.all_bases\n+            and isinstance(original_node.attr, cst.Name)\n+        ):\n+            # Replace with super().call_to_method\n+            return updated_node.with_changes(func=cst.Attribute(value=cst.Call(func=cst.Name(\"super\"))))\n+        return updated_node\n+\n+    def leave_Call(self, original_node: cst.Call, updated_node: cst.Call) -> cst.CSTNode:\n+        # Check if the function being called is of the form ClassB().func_a or ClassB.func_a\n+        if isinstance(original_node.func, cst.Attribute) and (\n+            # Match ClassB().func_a(...)\n+            (\n+                isinstance(original_node.func.value, cst.Call)\n+                and isinstance(original_node.func.value.func, cst.Name)\n+                and original_node.func.value.func.value in self.all_bases\n+            )\n+            or\n+            # Match ClassB.func_a(...)\n+            (isinstance(original_node.func.value, cst.Name) and original_node.func.value.value in self.all_bases)\n+        ):\n+            # Check if the first argument is 'self', and remove it\n+            if len(original_node.args) > 0 and m.matches(original_node.args[0].value, m.Name(\"self\")):\n+                # Create the new argument list without 'self'\n+                new_args = updated_node.args[1:]\n+            else:\n+                new_args = updated_node.args\n+\n+            return updated_node.with_changes(args=new_args)\n+        return updated_node\n+\n+\n def get_docstring_indent(docstring):\n     # Match the first line after the opening triple quotes\n     match = re.search(r'(?:\"\"\"|\\'\\'\\'|```)\\n(\\s+)', docstring)\n@@ -263,7 +335,7 @@ def get_docstring_indent(docstring):\n def merge_docstrings(original_docstring, updated_docstring):\n     # indent_level = get_docstring_indent(updated_docstring)\n     original_level = get_docstring_indent(original_docstring)\n-    if \"        Args:\\n        \" not in updated_docstring:\n+    if not re.findall(r\"\\n\\s*Args:\\n\", updated_docstring):\n         # Split the docstring at the example section, assuming `\"\"\"` is used to define the docstring\n         parts = original_docstring.split(\"```\")\n         if \"```\" in updated_docstring and len(parts) > 1:\n@@ -292,13 +364,15 @@ def merge_docstrings(original_docstring, updated_docstring):\n class SuperTransformer(cst.CSTTransformer):\n     METADATA_DEPENDENCIES = (ParentNodeProvider,)\n \n-    def __init__(self, python_module: cst.Module, original_methods, updated_methods, class_name=\"\"):\n+    def __init__(self, python_module: cst.Module, original_methods, updated_methods, class_name=\"\", all_bases=None):\n         self.python_module = python_module\n         self.original_methods = original_methods\n         self.updated_methods = updated_methods\n         self.all_assign_target = {}\n         self.deleted_targets = {}  # child node can delete some arguments\n         self.class_name = class_name\n+        self.all_bases = all_bases or []\n+        self.transformer = ReplaceMethodCallTransformer(set(self.all_bases))\n \n     def update_body(self, existing_body, new_statements):\n         \"\"\"\n@@ -356,18 +430,14 @@ def replace_super_calls(self, node: cst.IndentedBlock, func_name: str) -> cst.CS\n             parent_has_docstring = m.matches(self.original_methods[func_name].body.body[0], DOCSTRING_NODE)\n         new_body = []\n         has_super_call = False\n-        for idx, expr in enumerate(node.body):\n-            if m.matches(\n-                expr,\n-                m.SimpleStatementLine(\n-                    body=[m.Return(SUPER_CALL_NODE(func_name)) | m.Expr(SUPER_CALL_NODE(func_name))]\n-                ),\n-            ):\n-                if idx != 0 and func_name == \"__init__\":\n-                    raise ValueError(f\"The call to super() in {self.class_name} should be at the top of the init\")\n-                new_body.extend(self.update_body(self.original_methods[func_name].body.body, node.body))\n+\n+        for expr in node.body:\n+            if is_call_to_super(expr, func_name):\n                 has_super_call = True\n-            elif m.matches(expr, DOCSTRING_NODE):\n+                new_body.extend(self.update_body(self.original_methods[func_name].body.body, node.body))\n+            else:\n+                expr = expr.visit(self.transformer)\n+            if m.matches(expr, DOCSTRING_NODE):\n                 self.has_docstring = True\n                 if parent_has_docstring:  # actually here we ought to de-duplicate?\n                     original_docstring = self.original_methods[func_name].body.body[0].body[0].value.value\n@@ -406,15 +476,17 @@ def leave_Return(self, original_node: cst.Return, updated_node: cst.Return) -> c\n         return updated_node\n \n \n-def replace_call_to_super(class_finder: ClassFinder, updated_node: cst.ClassDef, class_name: str):\n+def replace_call_to_super(\n+    class_finder: ClassFinder, updated_node: cst.ClassDef, class_name: str, all_bases: List[str]\n+):\n     \"\"\"\n     Given the `class_name`, the `updated_node`'s call to super are unpacked.\n \n                     |    ```python                          |               |    ```python\n                     |    class GemmaModel(LlamaModel):      |               |       class GemmaModel(nn.Module):\n                     |        def __init__(self):            |               |           def __init__(self):\n-    Going from:     |            self.dropout = 0.2         |       to:     |               self.dropout = 0.2\n-                    |            super().__init__()         |               |               super().__init__(config)\n+    Going from:     |            super().__init__()         |       to:     |               super().__init__(config)\n+                    |            self.dropout = 0.2         |               |               self.dropout = 0.2\n                     |     ```                               |               |               self.padding_idx = config.pad_token_id\n                                                                             |               self.vocab_size = config.vocab_size\n                                                                             |               self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n@@ -453,7 +525,14 @@ def replace_call_to_super(class_finder: ClassFinder, updated_node: cst.ClassDef,\n                 new_params = new_params.with_changes(\n                     params=list(parent_params.values()), star_kwarg=func.params.star_kwarg\n                 )\n-            func = func.with_changes(body=updated_methods[name].body, params=new_params)\n+            if not re.match(\n+                r\"\\ndef .*\\(.*\\):\\n    raise.*Error\\(.*\",\n+                class_finder.python_module.code_for_node(updated_methods[name]),\n+            ):\n+                func = func.with_changes(body=updated_methods[name].body, params=new_params)\n+            else:\n+                continue\n+\n         if m.matches(func, m.SimpleStatementLine(body=[m.Assign()])):\n             target = class_finder.python_module.code_for_node(func.body[0].targets[0])\n             assign_targets[target] = func\n@@ -492,7 +571,7 @@ def replace_call_to_super(class_finder: ClassFinder, updated_node: cst.ClassDef,\n     temp_module = cst.Module(body=[result_node])\n     new_module = MetadataWrapper(temp_module)\n     new_replacement_class = new_module.visit(\n-        SuperTransformer(temp_module, original_methods, updated_methods, class_name)\n+        SuperTransformer(temp_module, original_methods, updated_methods, class_name, all_bases)\n     )\n     new_replacement_body = new_replacement_class.body[0].body  # get the indented block\n \n@@ -508,6 +587,31 @@ def replace_call_to_super(class_finder: ClassFinder, updated_node: cst.ClassDef,\n }\n \n \n+def get_new_part(class_name, base_class):\n+    \"\"\"\n+    When `MyClassNameAttention` inherits from `MistralAttention`, we need\n+    to process the name to properly find dependencies.\n+\n+    Here we take what is the same (Attention) and what is different\n+    when finding the dependencies.\n+    \"\"\"\n+    common_suffix_len = 0\n+    for i in range(1, min(len(class_name), len(base_class)) + 1):\n+        if class_name[-i] == base_class[-i]:\n+            common_suffix_len += 1\n+        else:\n+            break\n+\n+    if common_suffix_len > 0:\n+        new_part = class_name[:-common_suffix_len]\n+    else:\n+        new_part = class_name\n+\n+    # Convert the remaining new part to snake_case\n+    snake_case = re.sub(r\"(?<!^)(?=[A-Z])\", \"_\", new_part).lower()\n+    return snake_case\n+\n+\n class ModularConverterTransformer(CSTTransformer):\n     METADATA_DEPENDENCIES = (ParentNodeProvider, ScopeProvider, PositionProvider)\n \n@@ -538,6 +642,7 @@ def __init__(self, python_module, new_name, given_old_name=None, given_new_name=\n         }\n         self.match_patterns = \"|\".join(self.files.keys())\n         self.all_definitions = {}\n+        self.class_to_file_type = {}\n \n     def visit_ImportFrom(self, node: cst.ImportFrom) -> None:\n         \"\"\"When visiting imports from `transformers.models.xxx` we need to:\n@@ -630,13 +735,33 @@ def leave_ClassDef(self, original_node, updated_node):\n                     self.given_new_name,\n                 )\n                 visited_module[super_file_name] = class_finder\n+                list_dependencies = {\n+                    dep: class_finder.class_start_line.get(dep, 1000)\n+                    for dep in class_finder.class_dependency_mapping.get(class_name, [])\n+                }\n             else:  # we are re-using the previously parsed data\n                 class_finder = visited_module[super_file_name]\n \n-            list_dependencies = {\n-                dep: class_finder.class_start_line.get(dep, 1000)\n-                for dep in class_finder.class_dependency_mapping.get(class_name, [])\n-            }\n+                list_dependencies = {\n+                    dep: class_finder.class_start_line.get(dep, 1000)\n+                    for dep in class_finder.class_dependency_mapping.get(class_name, [])\n+                }\n+            if list_dependencies == []:\n+                # so, maybe standard renaming did not work (the class name is different)\n+                # we try with another renaming pattern\n+                potential_given_name = get_new_part(class_name, super_class)\n+                del visited_module[super_file_name]\n+                class_finder = find_classes_in_file(\n+                    self.transformers_imports[super_file_name],\n+                    model_name,\n+                    potential_given_name,\n+                    self.model_name,\n+                    potential_given_name,\n+                )\n+                list_dependencies = {\n+                    dep: class_finder.class_start_line.get(dep, 1000)\n+                    for dep in class_finder.class_dependency_mapping.get(class_name, [])\n+                }\n \n             list_dependencies = sorted(list_dependencies.items(), key=lambda x: x[1], reverse=True)\n             start_insert_idx = self.global_scope_index\n@@ -668,19 +793,23 @@ def leave_ClassDef(self, original_node, updated_node):\n                     self.inserted_deps.append(dependency)\n \n             if len(list_dependencies) > 0:\n-                updated_node = replace_call_to_super(class_finder, updated_node, class_name)\n+                updated_node = replace_call_to_super(class_finder, updated_node, class_name, all_bases)\n             else:\n                 raise ValueError(\n-                    f\"Unable to find dependencies for {super_class} in {super_file_name}. Here are the dependencies found: {class_finder.class_dependency_mapping}. (The automatic renaming might have gone wrong!)\"\n+                    f\"We were unable to find dependencies for {class_name} (based on inheriting from {super_class})\"\n+                    f\"   Here are all the global dependencies that we found in you modular file: {list(class_finder.class_dependency_mapping.keys())}.\"\n+                    f\"   This usually means that the name of `{class_name}` does not match the pattern of `{super_class}`\"\n                 )\n \n         # Now, if a class was defined without parents, we look for the name\n         match_pattern = \"|\".join(TYPE_TO_FILE_TYPE.keys())\n         match = re.search(rf\"({match_pattern})$\", class_name)\n         if match:\n             key = TYPE_TO_FILE_TYPE[match.group(1)]\n+            self.class_to_file_type[class_name] = key\n             self.files[key][class_name] = {\"insert_idx\": self.global_scope_index, \"node\": updated_node}\n         else:\n+            self.class_to_file_type[class_name] = \"modeling\"\n             self.files[\"modeling\"][class_name] = {\"insert_idx\": self.global_scope_index, \"node\": updated_node}\n         return updated_node\n \n@@ -690,14 +819,37 @@ def leave_FunctionDef(self, original_node, node):\n             self.all_definitions[node.name.value] = node\n         return node\n \n+    def visit_Assign(self, node: cst.Assign) -> None:\n+        # Check if the assignment target is '__all__'\n+        if isinstance(node.targets[0].target, cst.Name) and node.targets[0].target.value == \"__all__\":\n+            if isinstance(node.value, cst.List):\n+                # Extract the elements from the list\n+                all_all_to_add = defaultdict(list)\n+                for elt in node.value.elements:\n+                    if isinstance(elt.value, cst.SimpleString):\n+                        # Remove quotes and add the string to the elements list\n+                        class_name = elt.value.value\n+                        file = self.class_to_file_type[\n+                            elt.value.evaluated_value\n+                        ]  # evaluated value give the content of the string\n+                        all_all_to_add[file] += [class_name]\n+                for f_type, new_alls in all_all_to_add.items():\n+                    updated_node = node.with_changes(\n+                        value=cst.List(elements=[cst.Element(value=cst.SimpleString(value=k)) for k in new_alls])\n+                    )\n+                    self.files[f_type][class_name] = {\n+                        \"insert_idx\": self.global_scope_index + 100,\n+                        \"node\": updated_node,\n+                    }\n+\n     def leave_If(self, original_node, node):\n         parent_node = self.get_metadata(cst.metadata.ParentNodeProvider, original_node)\n         if m.matches(parent_node, m.Module()):\n             full_statement = self.python_module.code_for_node(original_node.test)\n             if re.search(r\"[\\s\\S]*is_.*available\", full_statement):\n                 self.all_safe_imports.append(node)\n-            elif full_statement not in self.new_body:\n-                self.new_body[node] = {\"insert_idx\": self.global_scope_index, \"node\": node}\n+            elif full_statement not in self.all_imports:\n+                logger.warning(f\"one import is protected with `if`. Hard guess where it's used {full_statement}\")\n         return node\n \n     def leave_Module(self, original_node: cst.Assign, node):\n@@ -764,7 +916,7 @@ def save_modeling_file(modular_file, converted_file):\n     parser = argparse.ArgumentParser()\n     parser.add_argument(\n         \"--files_to_parse\",\n-        default=[\"examples/modular-transformers/modular_dummy.py\"],\n+        default=[\"src/transformers/models/gemma/modular_gemma.py\"],\n         nargs=\"+\",\n         help=\"A list of `modular_xxxx` files that should be converted to single model file\",\n     )"
        },
        {
            "sha": "232eed95b9dd04f4c46f0f9e769cdab33cb9f3af",
            "filename": "utils/not_doctested.txt",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3add29097b5d0aadb57020507240b29d47dd7b3/utils%2Fnot_doctested.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3add29097b5d0aadb57020507240b29d47dd7b3/utils%2Fnot_doctested.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnot_doctested.txt?ref=a3add29097b5d0aadb57020507240b29d47dd7b3",
            "patch": "@@ -373,7 +373,6 @@ src/transformers/data/processors/squad.py\n src/transformers/data/processors/utils.py\n src/transformers/data/processors/xnli.py\n src/transformers/debug_utils.py\n-src/transformers/deepspeed.py\n src/transformers/dependency_versions_check.py\n src/transformers/dependency_versions_table.py\n src/transformers/dynamic_module_utils.py"
        }
    ],
    "stats": {
        "total": 626,
        "additions": 477,
        "deletions": 149
    }
}