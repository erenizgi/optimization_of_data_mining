{
    "author": "zRzRzRzRzRzRzR",
    "message": "4.1V Model and GLM-4.5V Model Conversion Code Updates (#41784)\n\n* update for new model convert\n\n* Update convert_glm4v_moe_mgt_weights_to_hf.py\n\n* restore\n\n* Update convert_glm4v_mgt_weights_to_hf.py\n\n* update\n\n* 1\n\n* Update convert_glm4v_moe_mgt_weights_to_hf.py\n\n* Update convert_glm4v_mgt_weights_to_hf.py\n\n* finish\n\n* update\n\n* 2\n\n* 2\n\n* 1\n\n* Update convert_glm4v_moe_mgt_weights_to_hf.py\n\n* update\n\n* update with tie_word_embeddings place",
    "sha": "a127710b3a91b3d323b27be8e06ddf507b009b87",
    "files": [
        {
            "sha": "722dab5759fcc0f984412e534191bcc457b303c5",
            "filename": "src/transformers/models/glm4v/convert_glm4v_mgt_weights_to_hf.py",
            "status": "modified",
            "additions": 424,
            "deletions": 326,
            "changes": 750,
            "blob_url": "https://github.com/huggingface/transformers/blob/a127710b3a91b3d323b27be8e06ddf507b009b87/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconvert_glm4v_mgt_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a127710b3a91b3d323b27be8e06ddf507b009b87/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconvert_glm4v_mgt_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconvert_glm4v_mgt_weights_to_hf.py?ref=a127710b3a91b3d323b27be8e06ddf507b009b87",
            "patch": "@@ -18,15 +18,11 @@\n import os\n import pickle\n import re\n-from collections.abc import Callable\n from pathlib import Path\n-from typing import Optional\n \n import torch\n from safetensors.torch import save_file\n \n-from ...utils import strtobool\n-\n \n # Avoid Using Megatron Lib\n class UnpicklerWrapper(pickle.Unpickler):\n@@ -49,20 +45,68 @@ def dict_access_multi(a_dict, keys):\n     return dict_access_multi(a_dict[keys[0]], keys[1:])\n \n \n+def _build_neox_to_llama_perm(rotary_dim: int) -> torch.Tensor:\n+    half = rotary_dim // 2\n+    perm = torch.empty(rotary_dim, dtype=torch.long)\n+    perm[0::2] = torch.arange(0, half)\n+    perm[1::2] = torch.arange(half, rotary_dim)\n+    return perm\n+\n+\n+def _apply_rope_permute(q_or_k: torch.Tensor, blocks: int, head_dim: int, rotary_dim: int, neox_to_llama: bool = True):\n+    if rotary_dim == 0:\n+        return q_or_k\n+\n+    if neox_to_llama:\n+        perm = _build_neox_to_llama_perm(rotary_dim).to(q_or_k.device)\n+    else:\n+        perm = torch.empty(rotary_dim, dtype=torch.long, device=q_or_k.device)\n+        half = rotary_dim // 2\n+        perm[0::2] = torch.arange(0, half, device=q_or_k.device)\n+        perm[1::2] = torch.arange(half, rotary_dim, device=q_or_k.device)\n+        inv = torch.empty_like(perm)\n+        inv[perm] = torch.arange(rotary_dim, device=q_or_k.device)\n+        perm = inv\n+\n+    if q_or_k.dim() == 2:\n+        h = q_or_k.view(blocks, head_dim, -1)\n+        h[:, :rotary_dim, ...] = h[:, perm, ...]\n+        return h.reshape(q_or_k.shape)\n+    else:\n+        h = q_or_k.view(blocks, head_dim)\n+        h[:, :rotary_dim] = h[:, perm]\n+        return h.reshape(q_or_k.shape)\n+\n+\n def merge_qkv(\n     sd_list,\n     original_tp,\n     num_attention_heads,\n     multi_query_group_num,\n     attention_dim,\n-    multi_query_attention,\n     interleaved_qkv,\n+    convert_neox_to_llama: bool = True,\n ):\n-    if not multi_query_attention and interleaved_qkv:\n-        return torch.cat(sd_list, dim=0)\n-    q, k, v = [], [], []\n+    rotary_dim = attention_dim // 2\n+    group_size = (num_attention_heads // multi_query_group_num + 2) * attention_dim\n+    q_chunks, k_chunks, v_chunks = [], [], []\n+\n     for sd in sd_list:\n-        if multi_query_attention:\n+        if interleaved_qkv:\n+            shape = sd.shape\n+            x = sd.view((multi_query_group_num // original_tp, group_size) + shape[1:])\n+            q_, k_, v_ = x.split(\n+                [\n+                    (num_attention_heads // multi_query_group_num) * attention_dim,\n+                    attention_dim,\n+                    attention_dim,\n+                ],\n+                dim=1,\n+            )\n+            q_chunks.append(q_.reshape((-1,) + shape[1:]).clone())\n+            k_chunks.append(k_.reshape((-1,) + shape[1:]).clone())\n+            v_chunks.append(v_.reshape((-1,) + shape[1:]).clone())\n+        else:\n             q_, k_, v_ = sd.split(\n                 [\n                     num_attention_heads * attention_dim // original_tp,\n@@ -71,34 +115,45 @@ def merge_qkv(\n                 ],\n                 dim=0,\n             )\n-        else:\n-            q_, k_, v_ = sd.chunk(dim=0, chunks=3)\n+            q_chunks.append(q_.clone())\n+            k_chunks.append(k_.clone())\n+            v_chunks.append(v_.clone())\n+\n+    q = torch.cat(q_chunks, dim=0)\n+    k = torch.cat(k_chunks, dim=0)\n+    v = torch.cat(v_chunks, dim=0)\n+\n+    if convert_neox_to_llama and rotary_dim > 0:\n+        q = _apply_rope_permute(q, num_attention_heads, attention_dim, rotary_dim, neox_to_llama=True)\n+        k = _apply_rope_permute(k, multi_query_group_num, attention_dim, rotary_dim, neox_to_llama=True)\n+\n+    return q, k, v\n+\n+\n+def merge_qkv_vit(sd_list, original_tp, num_attention_heads, multi_query_group_num, attention_dim):\n+    group_size = (num_attention_heads // multi_query_group_num + 2) * attention_dim\n+    q, k, v = [], [], []\n+    for sd in sd_list:\n+        shape = sd.shape\n+        q_, k_, v_ = sd.view((multi_query_group_num // original_tp, group_size) + (shape[1:])).split(\n+            [\n+                (num_attention_heads // multi_query_group_num * attention_dim),\n+                attention_dim,\n+                attention_dim,\n+            ],\n+            dim=1,\n+        )\n+        q_ = q_.reshape((-1,) + (shape[1:]))\n+        k_ = k_.reshape((-1,) + (shape[1:]))\n+        v_ = v_.reshape((-1,) + (shape[1:]))\n         q.append(q_.clone())\n         k.append(k_.clone())\n         v.append(v_.clone())\n+\n     q = torch.cat(q, dim=0)\n     k = torch.cat(k, dim=0)\n     v = torch.cat(v, dim=0)\n-    if not interleaved_qkv:\n-        rotary_dim = attention_dim // 2\n-        half_rot = rotary_dim // 2\n-        perm_rot = torch.empty(rotary_dim, dtype=torch.long)\n-        perm_rot[0::2] = torch.arange(0, half_rot)\n-        perm_rot[1::2] = torch.arange(half_rot, rotary_dim)\n-        if q.dim() == 2:\n-            qh = q.view(num_attention_heads, attention_dim, -1)\n-            kh = k.view(multi_query_group_num, attention_dim, -1)\n-            qh[:, :rotary_dim, :] = qh[:, perm_rot, :]\n-            kh[:, :rotary_dim, :] = kh[:, perm_rot, :]\n-            q = qh.reshape(-1, q.size(-1))\n-            k = kh.reshape(-1, k.size(-1))\n-        else:\n-            qh = q.view(num_attention_heads, attention_dim)\n-            kh = k.view(multi_query_group_num, attention_dim)\n-            qh[:, :rotary_dim] = qh[:, perm_rot]\n-            kh[:, :rotary_dim] = kh[:, perm_rot]\n-            q = qh.reshape(-1)\n-            k = kh.reshape(-1)\n+\n     return q, k, v\n \n \n@@ -111,6 +166,8 @@ def merge_glu(sd_list):\n \n \n def merge_glu_vit(sd_list, original_tp=None):\n+    if not isinstance(sd_list, list):\n+        sd_list = [sd_list]\n     gate_proj = torch.cat([sd.chunk(dim=0, chunks=2)[0].clone() for sd in sd_list], dim=0)\n     up_proj = torch.cat([sd.chunk(dim=0, chunks=2)[1].clone() for sd in sd_list], dim=0)\n     return gate_proj, up_proj\n@@ -126,36 +183,6 @@ def split_glu(sd, cnt, idx):\n     )\n \n \n-def merge_qkv_vit(sd_list, original_tp=None):\n-    q, k, v = [], [], []\n-    for sd in sd_list:\n-        q_, k_, v_ = sd.chunk(dim=0, chunks=3)\n-        q.append(q_.clone().contiguous())\n-        k.append(k_.clone().contiguous())\n-        v.append(v_.clone().contiguous())\n-    q = torch.cat(q, dim=0)\n-    k = torch.cat(k, dim=0)\n-    v = torch.cat(v, dim=0)\n-    combined = torch.cat([q, k, v], dim=0)\n-    return combined\n-\n-\n-def merge_tensors_vit(\n-    tp_sd: list[dict],\n-    keys: list[str],\n-    original_tp: int,\n-    target_tp: int,\n-    slice_dim: Optional[int] = None,\n-    merge_fn: Optional[Callable] = None,\n-):\n-    cnt = original_tp // target_tp\n-    sd_list = [dict_access_multi(tp_sd[i], keys) for i in range(cnt)]\n-    if slice_dim is not None:\n-        return torch.cat(sd_list, dim=slice_dim)\n-    assert merge_fn is not None\n-    return merge_fn(sd_list, original_tp)\n-\n-\n def merge_tensors(\n     tp_sd,\n     keys,\n@@ -184,6 +211,9 @@ def save_sharded_model(state_dict, output_path, max_shard_size_gb=5, num_layers=\n \n         for key, value in state_dict.items():\n             if f\"model.language_model.layers.{layer_idx}.\" in key:\n+                if isinstance(value, list):\n+                    assert len(value) == 1, f\"{key} {value}\"\n+                    value = value[0]\n                 layered_dict[layer_key][key] = value\n \n     for layer_idx in range(vision_num_layers):\n@@ -203,9 +233,9 @@ def save_sharded_model(state_dict, output_path, max_shard_size_gb=5, num_layers=\n \n     # Determine layer ordering\n     layer_order = []\n-    for i in range(40):\n+    for i in range(num_layers):\n         layer_order.append(f\"layer_{i}\")\n-    for i in range(24):\n+    for i in range(vision_num_layers):\n         layer_order.append(f\"visual_layer_{i}\")\n     layer_order.append(\"others\")\n \n@@ -251,323 +281,390 @@ def save_sharded_model(state_dict, output_path, max_shard_size_gb=5, num_layers=\n \n \n def merge_tp_weights(model_path, output_path, vllm_config_path=None):\n-    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n-        raise ValueError(\n-            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n-            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n-            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n-            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n-        )\n+    origin_tp, origin_ep, origin_pp = -1, -1, -1\n \n-    tp_size = 0\n+    check_ep_or_pp_later = False\n     for item in Path(model_path).iterdir():\n         if item.is_dir():\n-            match = re.match(r\"mp_rank_(\\d{2})\", item.name)\n+            match = re.match(r\"mp_rank_(\\d{2})(?:_(\\d{3}))?(?:_(\\d{3}))?\", item.name)\n             if match:\n-                tp = int(match.group(1))\n-                tp_size = max(tp_size, tp + 1)\n-\n-    print(f\"Detected tensor parallel degree TP={tp_size}\")\n+                groups = match.groups()\n+                tp = int(groups[0])\n+                origin_tp = max(origin_tp, tp + 1)\n+                # maybe TP-EP or TP-PP, need check later\n+                if groups[1] is not None and groups[2] is None:\n+                    pp = int(groups[1])\n+                    origin_pp = max(origin_pp, pp + 1)\n+                    origin_ep = 1\n+                    check_ep_or_pp_later = True\n+                elif groups[1] is not None and groups[2] is not None:\n+                    pp = int(groups[1])\n+                    ep = int(groups[2])\n+                    origin_pp = max(origin_pp, pp + 1)\n+                    origin_ep = max(origin_ep, ep + 1)\n+                else:\n+                    origin_ep = 1\n+                    origin_pp = 1\n+\n+    tensor_names_by_file = {}\n+    mgt_sd = {}\n+    for item in Path(model_path).iterdir():\n+        if item.is_dir():\n+            match = re.match(r\"mp_rank_(\\d{2})(?:_(\\d{3}))?(?:_(\\d{3}))?$\", item.name)\n+            if match:\n+                groups = match.groups()\n+                tp = int(groups[0])\n+                pp = int(groups[1]) if groups[1] is not None else 0\n+                ep = int(groups[2]) if groups[2] is not None else 0\n+\n+                file_path = item / \"model_optim_rng.pt\"\n+                assert file_path.exists(), f\"model_optim_rng.pt not found in {item}\"\n+\n+                file_sd = torch.load(file_path, map_location=\"cpu\", weights_only=False)\n+\n+                for k in list(file_sd.keys()):\n+                    if \"_extra_state\" in k or \"dummy_parameter\" in k:\n+                        file_sd.pop(k)\n+\n+                mgt_sd[(tp, pp, ep)] = file_sd\n+\n+                tensor_names = set()\n+                if \"model\" in file_sd:\n+                    for key in file_sd[\"model\"].keys():\n+                        tensor_names.add(key)\n+                tensor_names_by_file[(tp, pp, ep)] = tensor_names\n+\n+    change_pp_to_ep = False\n+    if check_ep_or_pp_later:\n+        prefix_distribution = {}\n+\n+        for (tp, pp, ep), prefixes in tensor_names_by_file.items():\n+            for prefix in prefixes:\n+                if prefix not in prefix_distribution:\n+                    prefix_distribution[prefix] = set()\n+                prefix_distribution[prefix].add((tp, pp, ep))\n+\n+        for prefix, locations in prefix_distribution.items():\n+            if len(locations) > 1:\n+                pp_values = {loc[1] for loc in locations}\n+                if len(pp_values) > 1:\n+                    print(f\"find '{prefix}' in multi ranks {pp_values} the parallelism should be TP-EP\")\n+                    origin_ep = origin_pp\n+                    origin_pp = 1\n+                    change_pp_to_ep = True\n+                    break\n+                else:\n+                    print(f\"find '{prefix}' only in one ep, parallelism should be TP-PP\")\n+                    break\n+\n+    print(f\"Detected tensor parallel degree TP={origin_tp} EP={origin_ep} PP={origin_pp}\")\n+    assert max(origin_tp, origin_ep) * origin_pp == len(tensor_names_by_file), \"maybe some problem in origin weight\"\n+\n+    organized_sd = {}\n+    for (tp, pp, ep), file_sd in mgt_sd.items():\n+        if change_pp_to_ep:\n+            pp, ep = ep, pp\n+        organized_sd.setdefault(pp, {})\n+        organized_sd[pp][(ep, tp)] = file_sd\n+        find_vpp = \"model0\" in file_sd\n+\n+    # support VPP, if each pp rank has n vpp blocks, we will treat the original model\n+    # was parallel as pp n * origin_pp\n+    if find_vpp:\n+        organized_sd_vpp = {}\n+        for i in range(origin_pp):\n+            for (ep, tp), file_sd in organized_sd[i].items():\n+                model_keys = sorted(\n+                    [key for key in file_sd.keys() if key.startswith(\"model\") and key[5:].isdigit()],\n+                    key=lambda x: int(x[5:]),\n+                )\n+                vp_blocks = len(model_keys)\n+                for idx, key in enumerate(model_keys):\n+                    assert key in file_sd, f\"model {key} not found\"\n+                    organized_sd_vpp.setdefault(idx * origin_pp + i, {})\n+                    organized_sd_vpp[idx * origin_pp + i][(ep, tp)] = {\"model\": file_sd[key]}\n+        origin_pp = origin_pp * vp_blocks\n+        organized_sd = organized_sd_vpp\n+\n+    ignore_list = [\"_extra_state\", \"dummy_parameter\"]\n+    layer_share_list = [\n+        \"norm\",\n+        \"conv3d\",\n+        \"downsample\",\n+        \"router\",\n+        \"mlp.linear_fc2.bias\",\n+        \"self_attention.linear_proj.bias\",\n+        \"position_embeddings\",\n+    ]\n \n-    if tp_size <= 1:\n-        print(\"Model is already at TP=1, no need to merge\")\n-        return\n+    full_weights = {}\n+\n+    vit_layer_offset = 0\n+    llm_layer_offset = 0\n+    llm_layer_pattern = re.compile(r\"^(decoder\\.layers\\.)(\\d+)(\\..*)$\")\n+    vit_layer_pattern = re.compile(r\"^(vision_model\\.transformer\\.layers\\.)(\\d+)(\\..*)$\")\n+    for pp in sorted(organized_sd.keys()):\n+        pp_dict = organized_sd[pp]\n+        next_llm_layer_offset = llm_layer_offset\n+        next_vit_layer_offset = vit_layer_offset\n+        ep_map = {}\n+        tp_map = {}\n+        tp_seen = set()\n+        for (ep, tp), item in pp_dict.items():\n+            if tp not in tp_seen:\n+                tp_seen.add(tp)\n+                tp_map[tp] = item\n+            ep_map[ep] = item\n+\n+        for tp in sorted(tp_map.keys()):\n+            sd = tp_map[tp]\n+            for full_name, tensor in sd[\"model\"].items():\n+                if any(x in full_name for x in ignore_list):\n+                    continue\n+                llm_name_match = llm_layer_pattern.match(full_name)\n+                if llm_name_match:\n+                    # Use a closure to avoid global variable issues\n+                    def offset_layer(x, offset=llm_layer_offset):\n+                        nonlocal next_llm_layer_offset\n+                        _real_layer = int(x.group(2)) + offset\n+                        next_llm_layer_offset = max(next_llm_layer_offset, _real_layer + 1)\n+                        return f\"{x.group(1)}{_real_layer}{x.group(3)}\"\n+\n+                    full_name = llm_layer_pattern.sub(offset_layer, full_name)\n+                vit_name_match = vit_layer_pattern.match(full_name)\n+                if vit_name_match:\n+                    # Use a closure to avoid global variable issues\n+                    def offset_layer(x, offset=vit_layer_offset):\n+                        nonlocal next_vit_layer_offset\n+                        _real_layer = int(x.group(2)) + offset\n+                        next_vit_layer_offset = max(next_vit_layer_offset, _real_layer + 1)\n+                        return f\"{x.group(1)}{_real_layer}{x.group(3)}\"\n+\n+                    full_name = vit_layer_pattern.sub(offset_layer, full_name)\n+                if layer_share_list and any(x in full_name for x in layer_share_list):\n+                    if full_name not in full_weights:\n+                        full_weights[full_name] = tensor\n+                    else:\n+                        assert torch.equal(tensor, full_weights[full_name]), (\n+                            f\"detect diff param in tp named: {full_name}\"\n+                        )\n+                elif not re.search(r\"\\.experts\\.\", full_name):\n+                    full_weights.setdefault(full_name, [None for _ in range(origin_tp)])\n+                    full_weights[full_name][tp] = tensor\n+\n+        for ep in sorted(ep_map.keys()):\n+            sd = ep_map[ep]\n+            for full_name, tensor in sd[\"model\"].items():\n+                if any(x in full_name for x in ignore_list):\n+                    continue\n+                name_match = llm_layer_pattern.match(full_name)\n+                if name_match:\n+                    # Use a closure to avoid global variable issues\n+                    def offset_layer(x, offset=llm_layer_offset):\n+                        nonlocal next_llm_layer_offset\n+                        _real_layer = int(x.group(2)) + offset\n+                        next_llm_layer_offset = max(next_llm_layer_offset, _real_layer + 1)\n+                        return f\"{x.group(1)}{_real_layer}{x.group(3)}\"\n+\n+                    full_name = llm_layer_pattern.sub(offset_layer, full_name)\n+                if re.search(r\"\\.experts\\.\", full_name):\n+                    full_weights.setdefault(full_name, [None for _ in range(origin_ep)])\n+                    full_weights[full_name][ep] = tensor\n+        llm_layer_offset = next_llm_layer_offset\n+        vit_layer_offset = next_vit_layer_offset\n+\n+    for k in sorted(full_weights.keys()):\n+        item = full_weights[k]\n+        if isinstance(item, list):\n+            print(f\"{k} {len(item)} {item[0].shape} {item[0].dtype}\", flush=True)\n+        else:\n+            print(f\"{k} {item.shape} {item.dtype}\", flush=True)\n \n     print(f\"Loading vLLM configuration file: {vllm_config_path}\")\n     with open(vllm_config_path, \"r\") as f:\n         model_config = json.load(f)\n-        num_layers = model_config.get(\"num_layers\", 40)\n-        vision_num_layers = model_config.get(\"vision_config\", {}).get(\"num_hidden_layers\", 24)\n-        num_heads = model_config.get(\"num_attention_heads\", 32)\n-        num_kv_heads = model_config.get(\"num_query_groups\", 2)\n+        text_config = model_config.get(\"text_config\", {})\n+        vision_config = model_config.get(\"vision_config\", {})\n+\n+        num_layers = text_config.get(\"num_hidden_layers\", 40)\n+        num_heads = text_config.get(\"num_attention_heads\", 32)\n+        num_kv_heads = text_config.get(\"num_key_value_heads\", 2)\n         hidden_size = model_config.get(\"hidden_size\", 4096)\n         head_dim = model_config.get(\"attention_dim\", hidden_size // num_heads)\n+        vision_num_layers = vision_config.get(\"depth\", 24)\n+        vit_n_head = vision_config.get(\"num_heads\", 12)\n \n     print(\n         f\"Model parameters: num_layers={num_layers}, vision_num_layers={vision_num_layers}, \"\n-        f\"num_heads={num_heads}, multi_query_group_num={num_kv_heads}, hidden_size={hidden_size}\"\n+        f\"num_heads={num_heads}, multi_query_group_num={num_kv_heads}\"\n     )\n \n-    weights = []\n-    for tp_rank in range(tp_size):\n-        print(f\"Loading TP shard {tp_rank}...\")\n-        weight_path = Path(model_path) / f\"mp_rank_{tp_rank:02d}\" / \"model_optim_rng.pt\"\n-        sd = torch.load(weight_path, map_location=\"cpu\", pickle_module=pickle)\n-\n-        for k in list(sd.keys()):\n-            if \"_extra_state\" in k or \"dummy_parameter\" in k:\n-                sd.pop(k)\n-\n-        if \"model\" in sd:\n-            weights.append(sd[\"model\"])\n-        else:\n-            raise ValueError(f\"'model' key not found in {weight_path}\")\n-\n-    if not weights:\n-        raise ValueError(\"No valid weight files found\")\n-\n     print(\"Merging tensor parallel weights...\")\n-    original_pp_enabled = os.path.exists(Path(model_path) / \"mp_rank_00_000\")\n-    original_tp, original_pp = tp_size, 1\n-    target_tp = 1\n-    print(f\"TP and PP INFO: original_tp: {original_tp}, original_pp:{original_pp}, target_tp: {target_tp}\")\n-    mgt_sd = [\n-        [\n-            torch.load(\n-                Path(model_path)\n-                / (f\"mp_rank_{j:02d}_{i:03d}\" if original_pp_enabled else f\"mp_rank_{j:02d}\")\n-                / \"model_optim_rng.pt\",\n-                map_location=\"cpu\",\n-                pickle_module=pickle,\n-            )\n-            for j in range(original_tp)\n-        ]\n-        for i in range(original_pp)\n-    ]\n \n-    interleaved_qkv = False\n-    multi_query_attention = True\n+    interleaved_qkv = True\n     num_attention_heads = num_heads\n     multi_query_group_num = num_kv_heads\n     attention_dim = head_dim\n     complete_state_dict = {}\n-    keys = [\"model\"]\n-    rank = 0\n \n     # LLM\n-    for pp in range(original_pp):\n-        layer_i = 0\n-        mgt_encoder_tp_0 = dict_access_multi(mgt_sd[pp][rank], keys)\n-\n-        while f\"decoder.layers.{layer_i}.self_attention.linear_qkv.layer_norm_weight\" in mgt_encoder_tp_0:\n-            complete_state_dict.update(\n-                {\n-                    f\"model.language_model.layers.{layer_i}.input_layernorm.weight\": mgt_encoder_tp_0[\n-                        f\"decoder.layers.{layer_i}.self_attention.linear_qkv.layer_norm_weight\"\n-                    ],\n-                    f\"model.language_model.layers.{layer_i}.post_attention_layernorm.weight\": mgt_encoder_tp_0[\n-                        f\"decoder.layers.{layer_i}.mlp.linear_fc1.layer_norm_weight\"\n-                    ],\n-                    f\"model.language_model.layers.{layer_i}.post_self_attn_layernorm.weight\": mgt_encoder_tp_0[\n-                        f\"decoder.layers.{layer_i}.post_self_attn_layernorm.weight\"\n-                    ],\n-                    f\"model.language_model.layers.{layer_i}.post_mlp_layernorm.weight\": mgt_encoder_tp_0[\n-                        f\"decoder.layers.{layer_i}.post_mlp_layernorm.weight\"\n-                    ],\n-                }\n+    layer_i = 0\n+    while f\"decoder.layers.{layer_i}.self_attention.linear_qkv.layer_norm_weight\" in full_weights:\n+        if f\"decoder.layers.{layer_i}.self_attention.linear_qkv.layer_norm_weight\" in full_weights:\n+            complete_state_dict[f\"model.language_model.layers.{layer_i}.input_layernorm.weight\"] = full_weights[\n+                f\"decoder.layers.{layer_i}.self_attention.linear_qkv.layer_norm_weight\"\n+            ]\n+\n+        if f\"decoder.layers.{layer_i}.pre_mlp_layernorm.weight\" in full_weights:\n+            complete_state_dict[f\"model.language_model.layers.{layer_i}.post_attention_layernorm.weight\"] = (\n+                full_weights[f\"decoder.layers.{layer_i}.pre_mlp_layernorm.weight\"]\n             )\n-\n-            q, k, v = merge_tensors(\n-                tp_sd=mgt_sd[pp],\n-                keys=keys + [f\"decoder.layers.{layer_i}.self_attention.linear_qkv.weight\"],\n-                original_tp=original_tp,\n-                target_tp=target_tp,\n-                current_tp=0,\n-                merge_fn=lambda sd_list: merge_qkv(\n-                    sd_list,\n-                    original_tp,\n-                    num_attention_heads,\n-                    multi_query_group_num,\n-                    attention_dim,\n-                    multi_query_attention,\n-                    interleaved_qkv,\n-                ),\n+        elif f\"decoder.layers.{layer_i}.mlp.linear_fc1.layer_norm_weight\" in full_weights:\n+            complete_state_dict[f\"model.language_model.layers.{layer_i}.post_attention_layernorm.weight\"] = (\n+                full_weights[f\"decoder.layers.{layer_i}.mlp.linear_fc1.layer_norm_weight\"]\n             )\n \n-            complete_state_dict[f\"model.language_model.layers.{layer_i}.self_attn.q_proj.weight\"] = q.clone()\n-            complete_state_dict[f\"model.language_model.layers.{layer_i}.self_attn.k_proj.weight\"] = k.clone()\n-            complete_state_dict[f\"model.language_model.layers.{layer_i}.self_attn.v_proj.weight\"] = v.clone()\n-\n-            if f\"decoder.layers.{layer_i}.self_attention.linear_qkv.bias\" in mgt_encoder_tp_0:\n-                q_bias, k_bias, v_bias = merge_tensors(\n-                    tp_sd=mgt_sd[pp],\n-                    keys=keys + [f\"decoder.layers.{layer_i}.self_attention.linear_qkv.bias\"],\n-                    original_tp=original_tp,\n-                    target_tp=target_tp,\n-                    current_tp=0,\n-                    merge_fn=lambda sd_list: merge_qkv(\n-                        sd_list,\n-                        original_tp,\n-                        num_attention_heads,\n-                        multi_query_group_num,\n-                        attention_dim,\n-                        multi_query_attention,\n-                        interleaved_qkv,\n-                    ),\n-                )\n-                complete_state_dict[f\"model.language_model.layers.{layer_i}.self_attn.q_proj.bias\"] = q_bias.clone()\n-                complete_state_dict[f\"model.language_model.layers.{layer_i}.self_attn.k_proj.bias\"] = k_bias.clone()\n-                complete_state_dict[f\"model.language_model.layers.{layer_i}.self_attn.v_proj.bias\"] = v_bias.clone()\n-\n-            o_proj = merge_tensors(\n-                tp_sd=mgt_sd[pp],\n-                keys=keys + [f\"decoder.layers.{layer_i}.self_attention.linear_proj.weight\"],\n-                original_tp=original_tp,\n-                target_tp=target_tp,\n-                current_tp=0,\n-                slice_dim=1,\n+        # GLM-4.1V Only\n+        if f\"decoder.layers.{layer_i}.post_mlp_layernorm.weight\" in full_weights:\n+            complete_state_dict[f\"model.language_model.layers.{layer_i}.post_mlp_layernorm.weight\"] = full_weights[\n+                f\"decoder.layers.{layer_i}.post_mlp_layernorm.weight\"\n+            ]\n+\n+        if f\"decoder.layers.{layer_i}.post_self_attn_layernorm.weight\" in full_weights:\n+            complete_state_dict[f\"model.language_model.layers.{layer_i}.post_self_attn_layernorm.weight\"] = (\n+                full_weights[f\"decoder.layers.{layer_i}.post_self_attn_layernorm.weight\"]\n             )\n-            complete_state_dict[f\"model.language_model.layers.{layer_i}.self_attn.o_proj.weight\"] = o_proj.clone()\n-\n-            # MLP - Use gate_up_proj\n-            complete_state_dict[f\"model.language_model.layers.{layer_i}.mlp.gate_up_proj.weight\"] = merge_tensors(\n-                tp_sd=mgt_sd[pp],\n-                keys=keys + [f\"decoder.layers.{layer_i}.mlp.linear_fc1.weight\"],\n-                original_tp=original_tp,\n-                target_tp=target_tp,\n-                current_tp=0,\n-                merge_fn=merge_glu,\n-            ).clone()\n-            complete_state_dict[f\"model.language_model.layers.{layer_i}.mlp.down_proj.weight\"] = merge_tensors(\n-                tp_sd=mgt_sd[pp],\n-                keys=keys + [f\"decoder.layers.{layer_i}.mlp.linear_fc2.weight\"],\n-                original_tp=original_tp,\n-                target_tp=target_tp,\n-                current_tp=0,\n-                slice_dim=1,\n+\n+        q, k, v = merge_qkv(\n+            sd_list=full_weights[f\"decoder.layers.{layer_i}.self_attention.linear_qkv.weight\"],\n+            original_tp=origin_tp,\n+            num_attention_heads=num_attention_heads,\n+            multi_query_group_num=multi_query_group_num,\n+            attention_dim=attention_dim,\n+            interleaved_qkv=interleaved_qkv,\n+        )\n+\n+        complete_state_dict[f\"model.language_model.layers.{layer_i}.self_attn.q_proj.weight\"] = q.clone()\n+        complete_state_dict[f\"model.language_model.layers.{layer_i}.self_attn.k_proj.weight\"] = k.clone()\n+        complete_state_dict[f\"model.language_model.layers.{layer_i}.self_attn.v_proj.weight\"] = v.clone()\n+\n+        if f\"decoder.layers.{layer_i}.self_attention.linear_qkv.bias\" in full_weights:\n+            q_bias, k_bias, v_bias = merge_qkv(\n+                sd_list=full_weights[f\"decoder.layers.{layer_i}.self_attention.linear_qkv.bias\"],\n+                original_tp=origin_tp,\n+                num_attention_heads=num_attention_heads,\n+                multi_query_group_num=multi_query_group_num,\n+                attention_dim=attention_dim,\n+                interleaved_qkv=interleaved_qkv,\n             )\n-            layer_i += 1\n-\n-    # Embedded Model, LM Head, and Norm\n-    embed_tokens = merge_tensors(\n-        tp_sd=mgt_sd[0],\n-        keys=[\"model\", \"embedding.word_embeddings.weight\"],\n-        original_tp=original_tp,\n-        target_tp=target_tp,\n-        current_tp=0,\n-        slice_dim=0,\n-    )\n+            complete_state_dict[f\"model.language_model.layers.{layer_i}.self_attn.q_proj.bias\"] = q_bias.clone()\n+            complete_state_dict[f\"model.language_model.layers.{layer_i}.self_attn.k_proj.bias\"] = k_bias.clone()\n+            complete_state_dict[f\"model.language_model.layers.{layer_i}.self_attn.v_proj.bias\"] = v_bias.clone()\n+\n+        o_proj = torch.cat(full_weights[f\"decoder.layers.{layer_i}.self_attention.linear_proj.weight\"], dim=1)\n+        complete_state_dict[f\"model.language_model.layers.{layer_i}.self_attn.o_proj.weight\"] = o_proj.clone()\n+\n+        # MLP - Use gate_up_proj\n+        gate_up_proj = torch.cat(full_weights[f\"decoder.layers.{layer_i}.mlp.linear_fc1.weight\"], dim=0)\n+        complete_state_dict[f\"model.language_model.layers.{layer_i}.mlp.gate_up_proj.weight\"] = gate_up_proj.clone()\n+        complete_state_dict[f\"model.language_model.layers.{layer_i}.mlp.down_proj.weight\"] = torch.cat(\n+            full_weights[f\"decoder.layers.{layer_i}.mlp.linear_fc2.weight\"], dim=1\n+        )\n+        layer_i += 1\n+\n+    # Embedd Model, LM Head, and Norm\n+    embed_tokens = torch.cat(full_weights[\"embedding.word_embeddings.weight\"], dim=0)\n     complete_state_dict[\"model.language_model.embed_tokens.weight\"] = embed_tokens.clone()\n-    lm_head = merge_tensors(\n-        tp_sd=mgt_sd[-1],\n-        keys=[\"model\", \"output_layer.weight\"],\n-        original_tp=original_tp,\n-        target_tp=target_tp,\n-        current_tp=0,\n-        slice_dim=0,\n-    )\n+\n+    lm_head = torch.cat(full_weights[\"output_layer.weight\"], dim=0)\n     complete_state_dict[\"lm_head.weight\"] = lm_head.clone()\n-    complete_state_dict[\"model.language_model.norm.weight\"] = mgt_sd[-1][rank][\"model\"][\n-        \"decoder.final_layernorm.weight\"\n-    ].clone()\n-    mgt_encoder_tp_0 = dict_access_multi(mgt_sd[0][0], keys)\n+    complete_state_dict[\"model.language_model.norm.weight\"] = full_weights[\"decoder.final_layernorm.weight\"].clone()\n \n     # VLM\n     for layer_i in range(vision_num_layers):\n-        complete_state_dict[f\"model.visual.blocks.{layer_i}.norm1.weight\"] = mgt_encoder_tp_0[\n-            f\"vision_model.transformer.layers.{layer_i}.input_layernorm.weight\"\n+        complete_state_dict[f\"model.visual.blocks.{layer_i}.norm1.weight\"] = full_weights[\n+            f\"vision_model.transformer.layers.{layer_i}.self_attention.linear_qkv.layer_norm_weight\"\n         ]\n-        complete_state_dict[f\"model.visual.blocks.{layer_i}.norm2.weight\"] = mgt_encoder_tp_0[\n-            f\"vision_model.transformer.layers.{layer_i}.pre_mlp_layernorm.weight\"\n+        complete_state_dict[f\"model.visual.blocks.{layer_i}.norm2.weight\"] = full_weights[\n+            f\"vision_model.transformer.layers.{layer_i}.mlp.linear_fc1.layer_norm_weight\"\n         ]\n \n-        qkv_weight = merge_tensors_vit(\n-            tp_sd=mgt_sd[0],\n-            keys=keys + [f\"vision_model.transformer.layers.{layer_i}.self_attention.linear_qkv.weight\"],\n-            original_tp=original_tp,\n-            target_tp=target_tp,\n-            merge_fn=merge_qkv_vit,\n+        q, k, v = merge_qkv_vit(\n+            sd_list=full_weights[f\"vision_model.transformer.layers.{layer_i}.self_attention.linear_qkv.weight\"],\n+            original_tp=origin_tp,\n+            num_attention_heads=vit_n_head,\n+            multi_query_group_num=vit_n_head,\n+            attention_dim=attention_dim,\n         )\n-        complete_state_dict[f\"model.visual.blocks.{layer_i}.attn.qkv.weight\"] = qkv_weight.clone()\n-\n-        proj_weight = merge_tensors_vit(\n-            tp_sd=mgt_sd[0],\n-            keys=keys + [f\"vision_model.transformer.layers.{layer_i}.self_attention.linear_proj.weight\"],\n-            original_tp=original_tp,\n-            target_tp=target_tp,\n-            slice_dim=1,\n+        complete_state_dict[f\"model.visual.blocks.{layer_i}.attn.qkv.weight\"] = torch.cat((q, k, v), dim=0)\n+\n+        proj_weight = torch.cat(\n+            full_weights[f\"vision_model.transformer.layers.{layer_i}.self_attention.linear_proj.weight\"], dim=1\n         )\n         complete_state_dict[f\"model.visual.blocks.{layer_i}.attn.proj.weight\"] = proj_weight.clone()\n \n-        gate_proj_weight, up_proj_weight = merge_tensors_vit(\n-            tp_sd=mgt_sd[0],\n-            keys=keys + [f\"vision_model.transformer.layers.{layer_i}.mlp.linear_fc1.weight\"],\n-            original_tp=original_tp,\n-            target_tp=target_tp,\n-            merge_fn=lambda sd_list, original_tp: merge_glu_vit(sd_list, original_tp),\n+        gate_proj_weight, up_proj_weight = merge_glu_vit(\n+            full_weights[f\"vision_model.transformer.layers.{layer_i}.mlp.linear_fc1.weight\"]\n         )\n+\n         complete_state_dict[f\"model.visual.blocks.{layer_i}.mlp.gate_proj.weight\"] = gate_proj_weight.clone()\n         complete_state_dict[f\"model.visual.blocks.{layer_i}.mlp.up_proj.weight\"] = up_proj_weight.clone()\n \n-        down_proj_weight = merge_tensors_vit(\n-            tp_sd=mgt_sd[0],\n-            keys=keys + [f\"vision_model.transformer.layers.{layer_i}.mlp.linear_fc2.weight\"],\n-            original_tp=original_tp,\n-            target_tp=target_tp,\n-            slice_dim=1,\n+        down_proj_weight = torch.cat(\n+            full_weights[f\"vision_model.transformer.layers.{layer_i}.mlp.linear_fc2.weight\"], dim=1\n         )\n         complete_state_dict[f\"model.visual.blocks.{layer_i}.mlp.down_proj.weight\"] = down_proj_weight.clone()\n \n     complete_state_dict[\"model.visual.downsample.weight\"] = (\n-        mgt_sd[0][0][\"model\"][\"vision_model.downsample.weight\"].clone().contiguous()\n+        full_weights[\"vision_model.downsample.weight\"].clone().contiguous()\n     )\n     complete_state_dict[\"model.visual.downsample.bias\"] = (\n-        mgt_sd[0][0][\"model\"][\"vision_model.downsample.bias\"].clone().contiguous()\n+        full_weights[\"vision_model.downsample.bias\"].clone().contiguous()\n     )\n \n     # Merger\n-    gate_proj, up_proj = merge_tensors_vit(\n-        tp_sd=mgt_sd[0],\n-        keys=keys + [\"vision_projection.encoder.linear_fc1.weight\"],\n-        original_tp=original_tp,\n-        target_tp=target_tp,\n-        merge_fn=merge_glu_vit,\n-    )\n+    gate_proj, up_proj = merge_glu_vit(full_weights[\"vision_projection.encoder.linear_fc1.weight\"])\n \n-    down_proj = merge_tensors_vit(\n-        tp_sd=mgt_sd[0],\n-        keys=keys + [\"vision_projection.encoder.linear_fc2.weight\"],\n-        original_tp=original_tp,\n-        target_tp=target_tp,\n-        slice_dim=1,\n-    )\n-    proj = merge_tensors_vit(\n-        tp_sd=mgt_sd[0],\n-        keys=keys + [\"vision_projection.encoder.linear_fc_extra.weight\"],\n-        original_tp=original_tp,\n-        target_tp=target_tp,\n-        slice_dim=0,\n-    )\n+    down_proj = torch.cat(full_weights[\"vision_projection.encoder.linear_fc2.weight\"], dim=1)\n+    proj = torch.cat(full_weights[\"vision_projection.linear_fc_extra.weight\"], dim=0)\n \n     complete_state_dict[\"model.visual.merger.gate_proj.weight\"] = gate_proj.clone().contiguous()\n     complete_state_dict[\"model.visual.merger.up_proj.weight\"] = up_proj.clone().contiguous()\n     complete_state_dict[\"model.visual.merger.down_proj.weight\"] = down_proj.clone().contiguous()\n     complete_state_dict[\"model.visual.merger.proj.weight\"] = proj.clone().contiguous()\n \n-    complete_state_dict[\"model.visual.merger.post_projection_norm.weight\"] = (\n-        mgt_sd[0][0][\"model\"][\"vision_projection.encoder.layer_norm.weight\"].clone().contiguous()\n-    )\n-    complete_state_dict[\"model.visual.merger.post_projection_norm.bias\"] = (\n-        mgt_sd[0][0][\"model\"][\"vision_projection.encoder.layer_norm.bias\"].clone().contiguous()\n-    )\n+    if \"vision_projection.layer_norm.weight\" in full_weights:\n+        complete_state_dict[\"model.visual.merger.post_projection_norm.weight\"] = full_weights[\n+            \"vision_projection.layer_norm.weight\"\n+        ]\n+    if \"vision_projection.layer_norm.bias\" in full_weights:\n+        complete_state_dict[\"model.visual.merger.post_projection_norm.bias\"] = full_weights[\n+            \"vision_projection.layer_norm.bias\"\n+        ]\n+\n     complete_state_dict[\"model.visual.embeddings.position_embedding.weight\"] = (\n-        mgt_sd[0][0][\"model\"][\"vision_model.position_embeddings.weight\"].clone().contiguous()\n+        full_weights[\"vision_model.position_embeddings.weight\"].clone().contiguous()\n     )\n     complete_state_dict[\"model.visual.patch_embed.proj.weight\"] = (\n-        mgt_sd[0][0][\"model\"][\"vision_model.conv3d.weight\"].clone().contiguous()\n+        full_weights[\"vision_model.conv3d.weight\"].clone().contiguous()\n     )\n     complete_state_dict[\"model.visual.patch_embed.proj.bias\"] = (\n-        mgt_sd[0][0][\"model\"][\"vision_model.conv3d.bias\"].clone().contiguous()\n+        full_weights[\"vision_model.conv3d.bias\"].clone().contiguous()\n     )\n \n     # Check for additional vision model norm layers mentioned in the expected output\n-    if \"vision_model.post_conv_layernorm.weight\" in mgt_encoder_tp_0:\n+    if \"vision_model.post_conv_layernorm.weight\" in full_weights:\n         complete_state_dict[\"model.visual.post_conv_layernorm.weight\"] = (\n-            mgt_sd[0][0][\"model\"][\"vision_model.post_conv_layernorm.weight\"].clone().contiguous()\n+            full_weights[\"vision_model.post_conv_layernorm.weight\"].clone().contiguous()\n         )\n \n-    if \"vision_model.post_layernorm.weight\" in mgt_encoder_tp_0:\n+    if \"vision_model.post_layernorm.weight\" in full_weights:\n         complete_state_dict[\"model.visual.post_layernorm.weight\"] = (\n-            mgt_sd[0][0][\"model\"][\"vision_model.post_layernorm.weight\"].clone().contiguous()\n+            full_weights[\"vision_model.post_layernorm.weight\"].clone().contiguous()\n         )\n \n     print(f\"Total keys in state dict: {len(complete_state_dict)}\")\n \n-    for key, value in complete_state_dict.items():\n-        if isinstance(value, torch.Tensor):\n-            complete_state_dict[key] = value.to(torch.bfloat16)\n-    print(\"Converted all tensors to bfloat16\")\n-    # Save Model weight\n     save_sharded_model(\n         complete_state_dict,\n         output_path=output_path,\n@@ -579,33 +676,37 @@ def merge_tp_weights(model_path, output_path, vllm_config_path=None):\n     hf_config = {\n         \"architectures\": [\"Glm4vForConditionalGeneration\"],\n         \"model_type\": \"glm4v\",\n-        \"attention_bias\": model_config.get(\"add_qkv_bias\", True),\n-        \"attention_dropout\": 0.0,\n-        \"pad_token_id\": model_config.get(\"pad_token_id\", 151329),\n-        \"eos_token_id\": model_config.get(\"eos_token_id\", [151329, 151336, 151338]),\n         \"image_start_token_id\": model_config.get(\"image_start_token_id\", 151339),\n         \"image_end_token_id\": model_config.get(\"image_end_token_id\", 151340),\n         \"video_start_token_id\": model_config.get(\"video_start_token_id\", 151341),\n         \"video_end_token_id\": model_config.get(\"video_end_token_id\", 151342),\n-        \"image_token_id\": model_config.get(\"image_token_id\", 151343),\n-        \"video_token_id\": model_config.get(\"video_token_id\", 151344),\n-        \"hidden_act\": model_config.get(\"hidden_act\", \"silu\"),\n-        \"hidden_size\": model_config.get(\"hidden_size\", 4096),\n+        \"transformers_version\": \"4.57.1\",\n+    }\n+    txt_config = {\n+        \"model_type\": \"glm4v_text\",\n+        \"attention_bias\": model_config.get(\"add_qkv_bias\", True),\n+        \"attention_dropout\": 0.0,\n+        \"pad_token_id\": model_config.get(\"pad_token_id\", 151329),\n+        \"eos_token_id\": model_config.get(\"eos_token_id\", [151329, 151336, 151338]),\n+        \"image_token_id\": model_config.get(\"image_token_id\", 151363),\n+        \"video_token_id\": model_config.get(\"video_token_id\", 151364),\n+        \"hidden_act\": text_config.get(\"hidden_act\", \"silu\"),\n+        \"hidden_size\": text_config.get(\"hidden_size\", 4096),\n         \"initializer_range\": 0.02,\n-        \"intermediate_size\": model_config.get(\"ffn_hidden_size\", 13696),\n-        \"max_position_embeddings\": model_config.get(\"seq_length\", 32768),\n-        \"num_attention_heads\": model_config.get(\"num_attention_heads\", 32),\n-        \"num_hidden_layers\": model_config.get(\"num_layers\", 40),\n-        \"num_key_value_heads\": model_config.get(\"multi_query_group_num\", 2),\n-        \"rms_norm_eps\": model_config.get(\"layernorm_epsilon\", 1e-05),\n-        \"rope_theta\": model_config.get(\"rotary_base\", 10000.0),\n-        \"tie_word_embeddings\": False,\n-        \"dtype\": model_config.get(\"dtype\", \"bfloat16\"),\n-        \"transformers_version\": \"4.53.0dev\",\n-        \"use_cache\": model_config.get(\"use_cache\", True),\n-        \"vocab_size\": model_config.get(\"vocab_size\", 151552),\n+        \"intermediate_size\": text_config.get(\"intermediate_size\", 13696),\n+        \"max_position_embeddings\": text_config.get(\"seq_length\", 131072),\n+        \"num_attention_heads\": text_config.get(\"num_attention_heads\", 32),\n+        \"num_hidden_layers\": text_config.get(\"num_layers\", 40),\n+        \"num_key_value_heads\": text_config.get(\"num_key_value_heads\", 2),\n+        \"rms_norm_eps\": text_config.get(\"layernorm_epsilon\", 1e-05),\n+        \"dtype\": text_config.get(\"torch_dtype\", \"bfloat16\"),\n+        \"use_cache\": text_config.get(\"use_cache\", True),\n+        \"vocab_size\": text_config.get(\"vocab_size\", 151552),\n         \"partial_rotary_factor\": 0.5,\n+        \"tie_word_embeddings\": False,\n+        \"rope_parameters\": {\"rope_type\": \"default\", \"rope_theta\": 10000.0, \"mrope_section\": [8, 12, 12]},\n     }\n+    hf_config[\"text_config\"] = txt_config\n \n     if \"vision_config\" in model_config:\n         vision_config = {\n@@ -626,9 +727,6 @@ def merge_tp_weights(model_path, output_path, vllm_config_path=None):\n         }\n         hf_config[\"vision_config\"] = vision_config\n \n-    if \"rope_parameters\" in model_config:\n-        hf_config[\"rope_parameters\"] = model_config[\"rope_parameters\"]\n-\n     config_path = os.path.join(output_path, \"config.json\")\n     with open(config_path, \"w\") as f:\n         json.dump(hf_config, f, indent=2)"
        },
        {
            "sha": "1949966d27380d6f99f77e05fbb12938f97d4f20",
            "filename": "src/transformers/models/glm4v_moe/convert_glm4v_moe_mgt_weights_to_hf.py",
            "status": "modified",
            "additions": 14,
            "deletions": 36,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/a127710b3a91b3d323b27be8e06ddf507b009b87/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconvert_glm4v_moe_mgt_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a127710b3a91b3d323b27be8e06ddf507b009b87/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconvert_glm4v_moe_mgt_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconvert_glm4v_moe_mgt_weights_to_hf.py?ref=a127710b3a91b3d323b27be8e06ddf507b009b87",
            "patch": "@@ -1,3 +1,4 @@\n+# coding=utf-8\n # Copyright 2025 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -57,10 +58,6 @@ def merge_qkv(\n     for sd in sd_list:\n         if interleaved_qkv:\n             shape = sd.shape\n-            print(f\"zjldbg input {shape} {(multi_query_group_num // original_tp, group_size) + (shape[1:])}\")\n-            print(\n-                f\"zjldbg xxx {shape} {(num_attention_heads // multi_query_group_num * attention_dim, attention_dim) + (shape[1:])}\"\n-            )\n             q_, k_, v_ = sd.view((multi_query_group_num // original_tp, group_size) + (shape[1:])).split(\n                 [\n                     (num_attention_heads // multi_query_group_num * attention_dim),\n@@ -118,20 +115,6 @@ def split_glu(sd, cnt, idx):\n     )\n \n \n-def merge_qkv_vit(sd_list, source=None):\n-    q, k, v = [], [], []\n-    for sd in sd_list:\n-        q_, k_, v_ = sd.chunk(dim=0, chunks=3)\n-        q.append(q_.clone().contiguous())\n-        k.append(k_.clone().contiguous())\n-        v.append(v_.clone().contiguous())\n-    q = torch.cat(q, dim=0)\n-    k = torch.cat(k, dim=0)\n-    v = torch.cat(v, dim=0)\n-    combined = torch.cat([q, k, v], dim=0)\n-    return combined\n-\n-\n def find_expert_weight(input_dict, layer_num, fc1=True):\n     if fc1:\n         pattern = re.compile(rf\"^decoder\\.layers\\.{layer_num}\\.mlp\\.experts\\.linear_fc1\\.weight(\\d+)$\")\n@@ -476,7 +459,6 @@ def offset_layer(x, offset=llm_layer_offset):\n     print(\"Merging tensor parallel weights...\")\n \n     interleaved_qkv = True\n-    multi_query_attention = True\n     num_attention_heads = llm_num_heads\n     multi_query_group_num = num_kv_heads\n     attention_dim = head_dim\n@@ -600,16 +582,12 @@ def offset_layer(x, offset=llm_layer_offset):\n             f\"vision_model.transformer.layers.{layer_i}.mlp.linear_fc1.layer_norm_weight\"\n         ]\n \n-        # qkv_weight = merge_qkv_vit(\n-        #     full_weights[f\"vision_model.transformer.layers.{layer_i}.self_attention.linear_qkv.weight\"])\n-        # complete_state_dict[f\"model.visual.blocks.{layer_i}.attn.qkv.weight\"] = qkv_weight.clone()\n         q, k, v = merge_qkv(\n             sd_list=full_weights[f\"vision_model.transformer.layers.{layer_i}.self_attention.linear_qkv.weight\"],\n             original_tp=origin_tp,\n             num_attention_heads=vit_n_head,\n             multi_query_group_num=vit_n_head,\n             attention_dim=attention_dim,\n-            multi_query_attention=multi_query_attention,\n             interleaved_qkv=interleaved_qkv,\n         )\n         complete_state_dict[f\"model.visual.blocks.{layer_i}.attn.qkv.weight\"] = torch.cat((q, k, v), dim=0)\n@@ -694,18 +672,21 @@ def offset_layer(x, offset=llm_layer_offset):\n     hf_config = {\n         \"architectures\": [\"Glm4vMoeForConditionalGeneration\"],\n         \"model_type\": \"glm4v_moe\",\n-        \"attention_bias\": model_config.get(\"add_qkv_bias\", True),\n-        \"attention_dropout\": 0.0,\n-        \"pad_token_id\": model_config.get(\"pad_token_id\", 151329),\n-        \"eos_token_id\": model_config.get(\"eos_token_id\", [151329, 151336, 151338]),\n         \"image_start_token_id\": model_config.get(\"image_start_token_id\", 151339),\n         \"image_end_token_id\": model_config.get(\"image_end_token_id\", 151340),\n         \"video_start_token_id\": model_config.get(\"video_start_token_id\", 151341),\n         \"video_end_token_id\": model_config.get(\"video_end_token_id\", 151342),\n-        \"image_token_id\": model_config.get(\"image_token_id\", 151343),\n-        \"video_token_id\": model_config.get(\"video_token_id\", 151344),\n+        \"transformers_version\": \"4.57.0.dev0\",\n     }\n     txt_config = {\n+        \"model_type\": \"glm4v_moe_text\",\n+        \"attention_bias\": model_config.get(\"add_qkv_bias\", True),\n+        \"use_qk_norm\": model_config.get(\"use_qk_norm\", False),\n+        \"attention_dropout\": 0.0,\n+        \"pad_token_id\": model_config.get(\"pad_token_id\", 151329),\n+        \"eos_token_id\": model_config.get(\"eos_token_id\", [151329, 151336, 151338]),\n+        \"image_token_id\": model_config.get(\"image_token_id\", 151363),\n+        \"video_token_id\": model_config.get(\"video_token_id\", 151364),\n         \"hidden_act\": text_config.get(\"hidden_act\", \"silu\"),\n         \"hidden_size\": text_config.get(\"hidden_size\", 4096),\n         \"initializer_range\": 0.02,\n@@ -715,24 +696,24 @@ def offset_layer(x, offset=llm_layer_offset):\n         \"num_hidden_layers\": text_config.get(\"num_layers\", 46),\n         \"num_key_value_heads\": text_config.get(\"multi_query_group_num\", 2),\n         \"rms_norm_eps\": text_config.get(\"layernorm_epsilon\", 1e-05),\n-        \"rope_theta\": text_config.get(\"rotary_base\", 10000.0),\n-        \"tie_word_embeddings\": False,\n-        \"torch_dtype\": text_config.get(\"torch_dtype\", \"bfloat16\"),\n-        \"transformers_version\": \"4.53.0dev\",\n+        \"dtype\": text_config.get(\"torch_dtype\", \"bfloat16\"),\n         \"use_cache\": text_config.get(\"use_cache\", True),\n         \"vocab_size\": text_config.get(\"vocab_size\", 151424),\n         \"partial_rotary_factor\": 0.5,\n+        \"tie_word_embeddings\": False,\n         \"moe_intermediate_size\": text_config.get(\"moe_intermediate_size\", 1408),\n         \"n_group\": text_config.get(\"n_group\", 1),\n         \"n_routed_experts\": text_config.get(\"n_routed_experts\", 128),\n         \"n_shared_experts\": text_config.get(\"n_shared_experts\", 1),\n         \"norm_topk_prob\": text_config.get(\"norm_topk_prob\", True),\n         \"num_experts_per_tok\": text_config.get(\"num_experts_per_tok\", 8),\n+        \"rope_parameters\": {\"rope_type\": \"default\", \"rope_theta\": 10000.0, \"mrope_section\": [8, 12, 12]},\n     }\n     hf_config[\"text_config\"] = txt_config\n \n     if \"vision_config\" in model_config:\n         vision_config = {\n+            \"model_type\": \"glm4v_moe\",\n             \"hidden_size\": model_config[\"vision_config\"].get(\"hidden_size\", 1536),\n             \"depth\": model_config[\"vision_config\"].get(\"num_layers\", 24),\n             \"num_heads\": model_config[\"vision_config\"].get(\"num_attention_heads\", 12),\n@@ -750,9 +731,6 @@ def offset_layer(x, offset=llm_layer_offset):\n         }\n         hf_config[\"vision_config\"] = vision_config\n \n-    if \"rope_scaling\" in model_config:\n-        hf_config[\"rope_scaling\"] = model_config[\"rope_scaling\"]\n-\n     config_path = os.path.join(output_path, \"config.json\")\n     with open(config_path, \"w\") as f:\n         json.dump(hf_config, f, indent=2)"
        }
    ],
    "stats": {
        "total": 800,
        "additions": 438,
        "deletions": 362
    }
}