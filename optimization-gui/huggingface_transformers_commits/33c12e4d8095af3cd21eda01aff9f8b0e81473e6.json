{
    "author": "Cyrilvallez",
    "message": "Fix CI (#35208)\n\nfix aria",
    "sha": "33c12e4d8095af3cd21eda01aff9f8b0e81473e6",
    "files": [
        {
            "sha": "c3e3e424a4baa49698d8515b013b285240ed4f77",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 94,
            "deletions": 124,
            "changes": 218,
            "blob_url": "https://github.com/huggingface/transformers/blob/33c12e4d8095af3cd21eda01aff9f8b0e81473e6/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/33c12e4d8095af3cd21eda01aff9f8b0e81473e6/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=33c12e4d8095af3cd21eda01aff9f8b0e81473e6",
            "patch": "@@ -432,93 +432,6 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return output + shared_expert_output\n \n \n-class AriaTextRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        dim=None,\n-        max_position_embeddings=2048,\n-        base=10000,\n-        device=None,\n-        scaling_factor=1.0,\n-        rope_type=\"default\",\n-        config: Optional[AriaTextConfig] = None,\n-    ):\n-        super().__init__()\n-        # TODO (joao): remove the `if` below, only used for BC\n-        self.rope_kwargs = {}\n-        if config is None:\n-            logger.warning_once(\n-                \"`AriaTextRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n-                \"`config` argument. All other arguments will be removed in v4.46\"\n-            )\n-            self.rope_kwargs = {\n-                \"rope_type\": rope_type,\n-                \"factor\": scaling_factor,\n-                \"dim\": dim,\n-                \"base\": base,\n-                \"max_position_embeddings\": max_position_embeddings,\n-            }\n-            self.rope_type = rope_type\n-            self.max_seq_len_cached = max_position_embeddings\n-            self.original_max_seq_len = max_position_embeddings\n-        else:\n-            # BC: \"rope_type\" was originally \"type\"\n-            if config.rope_scaling is not None:\n-                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-            else:\n-                self.rope_type = \"default\"\n-            self.max_seq_len_cached = config.max_position_embeddings\n-            self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n-    @torch.no_grad()\n-    def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-\n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n@@ -594,9 +507,6 @@ def __init__(self, config: AriaTextConfig, layer_idx: Optional[int] = None):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n \n-        # TODO (joao): remove in v4.46 (RoPE is computed in the model, not in the decoder layers)\n-        self.rotary_emb = AriaTextRotaryEmbedding(config=self.config)\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -606,7 +516,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n@@ -620,16 +530,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -692,7 +593,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if isinstance(past_key_value, StaticCache):\n@@ -716,16 +617,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -807,7 +699,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n@@ -838,16 +730,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -928,7 +811,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n@@ -1067,6 +950,93 @@ def _init_weights(self, module):\n             nn.init.trunc_normal_(module.query, std=std)\n \n \n+class AriaTextRotaryEmbedding(nn.Module):\n+    def __init__(\n+        self,\n+        dim=None,\n+        max_position_embeddings=2048,\n+        base=10000,\n+        device=None,\n+        scaling_factor=1.0,\n+        rope_type=\"default\",\n+        config: Optional[AriaTextConfig] = None,\n+    ):\n+        super().__init__()\n+        # TODO (joao): remove the `if` below, only used for BC\n+        self.rope_kwargs = {}\n+        if config is None:\n+            logger.warning_once(\n+                \"`AriaTextRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n+                \"`config` argument. All other arguments will be removed in v4.46\"\n+            )\n+            self.rope_kwargs = {\n+                \"rope_type\": rope_type,\n+                \"factor\": scaling_factor,\n+                \"dim\": dim,\n+                \"base\": base,\n+                \"max_position_embeddings\": max_position_embeddings,\n+            }\n+            self.rope_type = rope_type\n+            self.max_seq_len_cached = max_position_embeddings\n+            self.original_max_seq_len = max_position_embeddings\n+        else:\n+            # BC: \"rope_type\" was originally \"type\"\n+            if config.rope_scaling is not None:\n+                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+            else:\n+                self.rope_type = \"default\"\n+            self.max_seq_len_cached = config.max_position_embeddings\n+            self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n ARIA_TEXT_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):"
        }
    ],
    "stats": {
        "total": 218,
        "additions": 94,
        "deletions": 124
    }
}