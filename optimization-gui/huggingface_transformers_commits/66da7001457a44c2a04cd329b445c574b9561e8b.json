{
    "author": "ydshieh",
    "message": "Fix GLM4 checkpoints (#38412)\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* test style bot\n\n* Apply style fixes\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",
    "sha": "66da7001457a44c2a04cd329b445c574b9561e8b",
    "files": [
        {
            "sha": "05d129a23e554d30abe44b9bbb20fc8560282bc5",
            "filename": "src/transformers/models/glm4/configuration_glm4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/66da7001457a44c2a04cd329b445c574b9561e8b/src%2Ftransformers%2Fmodels%2Fglm4%2Fconfiguration_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66da7001457a44c2a04cd329b445c574b9561e8b/src%2Ftransformers%2Fmodels%2Fglm4%2Fconfiguration_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fconfiguration_glm4.py?ref=66da7001457a44c2a04cd329b445c574b9561e8b",
            "patch": "@@ -22,7 +22,7 @@ class Glm4Config(PretrainedConfig):\n     This is the configuration class to store the configuration of a [`Glm4Model`]. It is used to instantiate an Glm4\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the Glm4-4-9b-chat.\n-    e.g. [THUDM/glm-4-0414-9b-chat-chat](https://huggingface.co/THUDM/glm-4-0414-9b-chat-chat)\n+    e.g. [THUDM/GLM-4-9B-0414](https://huggingface.co/THUDM/GLM-4-9B-0414)\n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n     documentation from [`PretrainedConfig`] for more information.\n     Args:"
        },
        {
            "sha": "9a22b2561751bf78285b5b60d1b6963ec9d1e6d1",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/66da7001457a44c2a04cd329b445c574b9561e8b/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66da7001457a44c2a04cd329b445c574b9561e8b/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=66da7001457a44c2a04cd329b445c574b9561e8b",
            "patch": "@@ -551,8 +551,8 @@ def forward(\n         ```python\n         >>> from transformers import AutoTokenizer, Glm4ForCausalLM\n \n-        >>> model = Glm4ForCausalLM.from_pretrained(\"THUDM/GLM-4-9B-Chat-0414\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"THUDM/GLM-4-9B-Chat-0414\")\n+        >>> model = Glm4ForCausalLM.from_pretrained(\"THUDM/GLM-4-9B-0414\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"THUDM/GLM-4-9B-0414\")\n \n         >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n         >>> inputs = tokenizer(prompt, return_tensors=\"pt\")"
        },
        {
            "sha": "1cf98ce159c2ff84f242b508ed15d26650d8c1bf",
            "filename": "src/transformers/models/glm4/modular_glm4.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/66da7001457a44c2a04cd329b445c574b9561e8b/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodular_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66da7001457a44c2a04cd329b445c574b9561e8b/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodular_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodular_glm4.py?ref=66da7001457a44c2a04cd329b445c574b9561e8b",
            "patch": "@@ -31,7 +31,7 @@\n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"THUDM/GLM-4-9B-Chat-0414\"\n+_CHECKPOINT_FOR_DOC = \"THUDM/GLM-4-9B-0414\"\n \n \n class Glm4MLP(Phi3MLP):\n@@ -119,8 +119,8 @@ def forward(\n         ```python\n         >>> from transformers import AutoTokenizer, Glm4ForCausalLM\n \n-        >>> model = Glm4ForCausalLM.from_pretrained(\"THUDM/GLM-4-9B-Chat-0414\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"THUDM/GLM-4-9B-Chat-0414\")\n+        >>> model = Glm4ForCausalLM.from_pretrained(\"THUDM/GLM-4-9B-0414\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"THUDM/GLM-4-9B-0414\")\n \n         >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n         >>> inputs = tokenizer(prompt, return_tensors=\"pt\")"
        },
        {
            "sha": "bd5e92a281fb6e39c29e8e0e9b2975a81ca9b63a",
            "filename": "tests/models/glm4/test_modeling_glm4.py",
            "status": "modified",
            "additions": 68,
            "deletions": 37,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/66da7001457a44c2a04cd329b445c574b9561e8b/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66da7001457a44c2a04cd329b445c574b9561e8b/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py?ref=66da7001457a44c2a04cd329b445c574b9561e8b",
            "patch": "@@ -20,6 +20,8 @@\n \n from transformers import AutoModelForCausalLM, AutoTokenizer, Glm4Config, is_torch_available\n from transformers.testing_utils import (\n+    Expectations,\n+    cleanup,\n     require_flash_attn,\n     require_torch,\n     require_torch_large_gpu,\n@@ -80,113 +82,142 @@ class Glm4ModelTest(CausalLMModelTest, unittest.TestCase):\n @require_torch_large_gpu\n class Glm4IntegrationTest(unittest.TestCase):\n     input_text = [\"Hello I am doing\", \"Hi today\"]\n-    model_id = \"THUDM/glm-4-0414-9b-chat\"\n-    revision = \"refs/pr/15\"\n+    model_id = \"THUDM/GLM-4-9B-0414\"\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n \n     def test_model_9b_fp16(self):\n-        EXPECTED_TEXTS = [\n-            \"Hello I am doing a project on the history of the internetSolution:\\n\\nStep 1: Introduction\\nThe history of the\",\n-            \"Hi today I am going to show you how to make a simple and easy to make a DIY paper flower.\",\n-        ]\n+        EXPECTED_TEXTS = Expectations(\n+            {\n+                (\"cuda\", 7): [],\n+                (\"cuda\", 8): [\n+                    \"Hello I am doing a project on the history of the internet and I need to know what the first website was and what\",\n+                    \"Hi today I am going to tell you about the most common disease in the world. This disease is called diabetes\",\n+                ],\n+            }\n+        )\n+        EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n \n         model = AutoModelForCausalLM.from_pretrained(\n-            self.model_id, low_cpu_mem_usage=True, torch_dtype=torch.float16, revision=self.revision\n+            self.model_id, low_cpu_mem_usage=True, torch_dtype=torch.float16\n         ).to(torch_device)\n \n-        tokenizer = AutoTokenizer.from_pretrained(self.model_id, revision=self.revision)\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n         inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n \n         output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n         output_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n \n-        self.assertEqual(output_text, EXPECTED_TEXTS)\n+        self.assertEqual(output_text, EXPECTED_TEXT)\n \n     def test_model_9b_bf16(self):\n-        EXPECTED_TEXTS = [\n-            \"Hello I am doing a project on the history of the internetSolution:\\n\\nStep 1: Introduction\\nThe history of the\",\n-            \"Hi today I am going to show you how to make a simple and easy to make a DIY paper flower.\",\n-        ]\n+        EXPECTED_TEXTS = Expectations(\n+            {\n+                (\"cuda\", 7): [],\n+                (\"cuda\", 8): [\n+                    \"Hello I am doing a project on the history of the internet and I need to know what the first website was and what\",\n+                    \"Hi today I am going to tell you about the most common disease in the world. This disease is called diabetes\",\n+                ],\n+            }\n+        )\n+        EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n \n         model = AutoModelForCausalLM.from_pretrained(\n-            self.model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16, revision=self.revision\n+            self.model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\n         ).to(torch_device)\n \n-        tokenizer = AutoTokenizer.from_pretrained(self.model_id, revision=self.revision)\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n         inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n \n         output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n         output_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n \n-        self.assertEqual(output_text, EXPECTED_TEXTS)\n+        self.assertEqual(output_text, EXPECTED_TEXT)\n \n     def test_model_9b_eager(self):\n-        EXPECTED_TEXTS = [\n-            \"Hello I am doing a project on the history of the internetSolution:\\n\\nStep 1: Introduction\\nThe history of the\",\n-            \"Hi today I am going to show you how to make a simple and easy to make a DIY paper flower.\",\n-        ]\n+        EXPECTED_TEXTS = Expectations(\n+            {\n+                (\"cuda\", 7): [],\n+                (\"cuda\", 8): [\n+                    \"Hello I am doing a project on the history of the internet and I need to know what the first website was and what\",\n+                    \"Hi today I am going to tell you about the most common disease in the world. This disease is called diabetes\",\n+                ],\n+            }\n+        )\n+        EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n \n         model = AutoModelForCausalLM.from_pretrained(\n             self.model_id,\n             low_cpu_mem_usage=True,\n             torch_dtype=torch.bfloat16,\n             attn_implementation=\"eager\",\n-            revision=self.revision,\n         )\n         model.to(torch_device)\n \n-        tokenizer = AutoTokenizer.from_pretrained(self.model_id, revision=self.revision)\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n         inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n \n         output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n         output_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n \n-        self.assertEqual(output_text, EXPECTED_TEXTS)\n+        self.assertEqual(output_text, EXPECTED_TEXT)\n \n     @require_torch_sdpa\n     def test_model_9b_sdpa(self):\n-        EXPECTED_TEXTS = [\n-            \"Hello I am doing a project on the history of the internetSolution:\\n\\nStep 1: Introduction\\nThe history of the\",\n-            \"Hi today I am going to show you how to make a simple and easy to make a DIY paper flower.\",\n-        ]\n+        EXPECTED_TEXTS = Expectations(\n+            {\n+                (\"cuda\", 7): [],\n+                (\"cuda\", 8): [\n+                    \"Hello I am doing a project on the history of the internet and I need to know what the first website was and what\",\n+                    \"Hi today I am going to tell you about the most common disease in the world. This disease is called diabetes\",\n+                ],\n+            }\n+        )\n+        EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n \n         model = AutoModelForCausalLM.from_pretrained(\n             self.model_id,\n             low_cpu_mem_usage=True,\n             torch_dtype=torch.bfloat16,\n             attn_implementation=\"sdpa\",\n-            revision=self.revision,\n         )\n         model.to(torch_device)\n \n-        tokenizer = AutoTokenizer.from_pretrained(self.model_id, revision=self.revision)\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n         inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n \n         output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n         output_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n \n-        self.assertEqual(output_text, EXPECTED_TEXTS)\n+        self.assertEqual(output_text, EXPECTED_TEXT)\n \n     @require_flash_attn\n     @pytest.mark.flash_attn_test\n     def test_model_9b_flash_attn(self):\n-        EXPECTED_TEXTS = [\n-            \"Hello I am doing a project on the history of the internetSolution:\\n\\nStep 1: Introduction\\nThe history of the\",\n-            \"Hi today I am going to show you how to make a simple and easy to make a DIY paper flower.\",\n-        ]\n+        EXPECTED_TEXTS = Expectations(\n+            {\n+                (\"cuda\", 7): [],\n+                (\"cuda\", 8): [\n+                    \"Hello I am doing a project on the history of the internet and I need to know what the first website was and what\",\n+                    \"Hi today I am going to tell you about the most common disease in the world. This disease is called diabetes\",\n+                ],\n+            }\n+        )\n+        EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n \n         model = AutoModelForCausalLM.from_pretrained(\n             self.model_id,\n             low_cpu_mem_usage=True,\n             torch_dtype=torch.bfloat16,\n             attn_implementation=\"flash_attention_2\",\n-            revision=self.revision,\n         )\n         model.to(torch_device)\n \n-        tokenizer = AutoTokenizer.from_pretrained(self.model_id, revision=self.revision)\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n         inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n \n         output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n         output_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n \n-        self.assertEqual(output_text, EXPECTED_TEXTS)\n+        self.assertEqual(output_text, EXPECTED_TEXT)"
        }
    ],
    "stats": {
        "total": 117,
        "additions": 74,
        "deletions": 43
    }
}