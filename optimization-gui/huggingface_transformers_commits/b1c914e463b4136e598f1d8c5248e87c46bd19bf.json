{
    "author": "zucchini-nlp",
    "message": "Fix device mismatch errors (#33851)\n\nfix device mismatch errors",
    "sha": "b1c914e463b4136e598f1d8c5248e87c46bd19bf",
    "files": [
        {
            "sha": "9cb4d1f5a9aadb1ecb0da61dcd36ffda04239dbe",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1c914e463b4136e598f1d8c5248e87c46bd19bf/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1c914e463b4136e598f1d8c5248e87c46bd19bf/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=b1c914e463b4136e598f1d8c5248e87c46bd19bf",
            "patch": "@@ -521,7 +521,10 @@ def forward(\n             # TODO: @raushan retain only the new behavior after v4.47\n             else:\n                 special_image_mask = (\n-                    (input_ids == self.config.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n+                    (input_ids == self.config.image_token_index)\n+                    .unsqueeze(-1)\n+                    .expand_as(inputs_embeds)\n+                    .to(inputs_embeds.device)\n                 )\n                 image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n                 inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)"
        },
        {
            "sha": "6ece93b6f7a860bcdd0ea5195b249d8dd6b9c45e",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1c914e463b4136e598f1d8c5248e87c46bd19bf/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1c914e463b4136e598f1d8c5248e87c46bd19bf/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=b1c914e463b4136e598f1d8c5248e87c46bd19bf",
            "patch": "@@ -898,7 +898,10 @@ def forward(\n             # TODO: @raushan retain only the new behavior after v4.47\n             else:\n                 special_image_mask = (\n-                    (input_ids == self.config.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n+                    (input_ids == self.config.image_token_index)\n+                    .unsqueeze(-1)\n+                    .expand_as(inputs_embeds)\n+                    .to(inputs_embeds.device)\n                 )\n                 image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n                 inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)"
        },
        {
            "sha": "95a69826f6a02e6f8aa12e287c0f13067891b5e5",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1c914e463b4136e598f1d8c5248e87c46bd19bf/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1c914e463b4136e598f1d8c5248e87c46bd19bf/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=b1c914e463b4136e598f1d8c5248e87c46bd19bf",
            "patch": "@@ -980,13 +980,19 @@ def forward(\n         else:\n             if image_features is not None:\n                 special_image_mask = (\n-                    (input_ids == self.config.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n+                    (input_ids == self.config.image_token_index)\n+                    .unsqueeze(-1)\n+                    .expand_as(inputs_embeds)\n+                    .to(inputs_embeds.device)\n                 )\n                 image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n                 inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n             if video_features is not None:\n                 special_image_mask = (\n-                    (input_ids == self.config.video_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n+                    (input_ids == self.config.video_token_index)\n+                    .unsqueeze(-1)\n+                    .expand_as(inputs_embeds)\n+                    .to(inputs_embeds.device)\n                 )\n                 video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n                 inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)"
        },
        {
            "sha": "ab7d4fd602b01af23ebf61bfb29ee9dc9d8aa398",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1c914e463b4136e598f1d8c5248e87c46bd19bf/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1c914e463b4136e598f1d8c5248e87c46bd19bf/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=b1c914e463b4136e598f1d8c5248e87c46bd19bf",
            "patch": "@@ -485,13 +485,19 @@ def forward(\n         else:\n             if image_features is not None:\n                 special_image_mask = (\n-                    (input_ids == self.config.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n+                    (input_ids == self.config.image_token_index)\n+                    .unsqueeze(-1)\n+                    .expand_as(inputs_embeds)\n+                    .to(inputs_embeds.device)\n                 )\n                 image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n                 inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n             if video_features is not None:\n                 special_image_mask = (\n-                    (input_ids == self.config.video_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n+                    (input_ids == self.config.video_token_index)\n+                    .unsqueeze(-1)\n+                    .expand_as(inputs_embeds)\n+                    .to(inputs_embeds.device)\n                 )\n                 video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n                 inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)"
        },
        {
            "sha": "85418a134aa17eeebe897724a74e0a21aad9acbd",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 12,
            "deletions": 2,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1c914e463b4136e598f1d8c5248e87c46bd19bf/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1c914e463b4136e598f1d8c5248e87c46bd19bf/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=b1c914e463b4136e598f1d8c5248e87c46bd19bf",
            "patch": "@@ -1690,14 +1690,24 @@ def forward(\n             if pixel_values is not None:\n                 pixel_values = pixel_values.type(self.visual.get_dtype())\n                 image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n-                image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n+                image_mask = (\n+                    (input_ids == self.config.image_token_id)\n+                    .unsqueeze(-1)\n+                    .expand_as(inputs_embeds)\n+                    .to(inputs_embeds.device)\n+                )\n                 image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n                 inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n \n             if pixel_values_videos is not None:\n                 pixel_values_videos = pixel_values_videos.type(self.visual.get_dtype())\n                 video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)\n-                video_mask = (input_ids == self.config.video_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n+                video_mask = (\n+                    (input_ids == self.config.video_token_id)\n+                    .unsqueeze(-1)\n+                    .expand_as(inputs_embeds)\n+                    .to(inputs_embeds.device)\n+                )\n                 video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n                 inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n "
        },
        {
            "sha": "97bc9f5802029a9aa622b289fa9df979bc10ed3d",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1c914e463b4136e598f1d8c5248e87c46bd19bf/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1c914e463b4136e598f1d8c5248e87c46bd19bf/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=b1c914e463b4136e598f1d8c5248e87c46bd19bf",
            "patch": "@@ -621,14 +621,20 @@ def forward(\n             else:\n                 if image_outputs is not None:\n                     special_image_mask = (\n-                        (input_ids == self.config.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n+                        (input_ids == self.config.image_token_index)\n+                        .unsqueeze(-1)\n+                        .expand_as(inputs_embeds)\n+                        .to(inputs_embeds.device)\n                     )\n                     image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n                     inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n                 if video_outputs is not None:\n                     special_image_mask = (\n-                        (input_ids == self.config.video_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n+                        (input_ids == self.config.video_token_index)\n+                        .unsqueeze(-1)\n+                        .expand_as(inputs_embeds)\n+                        .to(inputs_embeds.device)\n                     )\n                     video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n                     inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)"
        },
        {
            "sha": "55ccd12367401f51ba7fd0c53ad0ca121b070c9e",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1c914e463b4136e598f1d8c5248e87c46bd19bf/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1c914e463b4136e598f1d8c5248e87c46bd19bf/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=b1c914e463b4136e598f1d8c5248e87c46bd19bf",
            "patch": "@@ -514,7 +514,10 @@ def forward(\n             # TODO: @raushan retain only the new behavior after v4.47\n             else:\n                 special_image_mask = (\n-                    (input_ids == self.config.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n+                    (input_ids == self.config.image_token_index)\n+                    .unsqueeze(-1)\n+                    .expand_as(inputs_embeds)\n+                    .to(inputs_embeds.device)\n                 )\n                 image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n                 inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)"
        }
    ],
    "stats": {
        "total": 59,
        "additions": 48,
        "deletions": 11
    }
}