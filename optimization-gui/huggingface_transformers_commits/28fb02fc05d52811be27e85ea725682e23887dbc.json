{
    "author": "zucchini-nlp",
    "message": "VLMs: enable generation tests - last batch (#34484)\n\n* add tests for 3 more vlms\r\n\r\n* fix fuyu back\r\n\r\n* skip test",
    "sha": "28fb02fc05d52811be27e85ea725682e23887dbc",
    "files": [
        {
            "sha": "2df5dbc8b29177a58022f3f0fe975d48a335731f",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 11,
            "deletions": 2,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/28fb02fc05d52811be27e85ea725682e23887dbc/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28fb02fc05d52811be27e85ea725682e23887dbc/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=28fb02fc05d52811be27e85ea725682e23887dbc",
            "patch": "@@ -346,7 +346,7 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n \n-        if past_key_values:\n+        if past_key_values is not None:\n             input_ids = input_ids[:, -1:]\n \n         position_ids = kwargs.get(\"position_ids\", None)\n@@ -355,7 +355,7 @@ def prepare_inputs_for_generation(\n             position_ids = attention_mask.long().cumsum(-1) - 1\n             position_ids.masked_fill_(attention_mask == 0, 1)\n             if past_key_values:\n-                position_ids = position_ids[:, -1].unsqueeze(-1)\n+                position_ids = position_ids[:, -1:]\n \n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n         if inputs_embeds is not None and past_key_values is None:\n@@ -377,3 +377,12 @@ def prepare_inputs_for_generation(\n             }\n         )\n         return model_inputs\n+\n+    @staticmethod\n+    def _reorder_cache(past_key_values, beam_idx):\n+        reordered_past = ()\n+        for layer_past in past_key_values:\n+            reordered_past += (\n+                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n+            )\n+        return reordered_past"
        },
        {
            "sha": "3b6ec9b2d844e0bbdde2fda6c0f7efdfa2ec4a59",
            "filename": "src/transformers/models/pix2struct/configuration_pix2struct.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/28fb02fc05d52811be27e85ea725682e23887dbc/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fconfiguration_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28fb02fc05d52811be27e85ea725682e23887dbc/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fconfiguration_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fconfiguration_pix2struct.py?ref=28fb02fc05d52811be27e85ea725682e23887dbc",
            "patch": "@@ -91,6 +91,10 @@ class Pix2StructTextConfig(PretrainedConfig):\n         \"hidden_size\": \"hidden_size\",\n         \"num_attention_heads\": \"num_heads\",\n         \"num_hidden_layers\": \"num_layers\",\n+        \"decoder_attention_heads\": \"num_heads\",\n+        \"encoder_attention_heads\": \"num_heads\",\n+        \"encoder_layers\": \"num_layers\",\n+        \"decoder_layers\": \"num_layers\",\n     }\n \n     def __init__(\n@@ -354,6 +358,8 @@ def __init__(\n             vision_config = {}\n             logger.info(\"vision_config is None. Initializing the Pix2StructVisionConfig with default values.\")\n \n+        text_config[\"is_encoder_decoder\"] = is_encoder_decoder\n+        text_config[\"tie_word_embeddings\"] = tie_word_embeddings\n         self.text_config = Pix2StructTextConfig(**text_config)\n         self.vision_config = Pix2StructVisionConfig(**vision_config)\n "
        },
        {
            "sha": "34adc132f8829a838e6f01ad999bcbf965702dd0",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/28fb02fc05d52811be27e85ea725682e23887dbc/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28fb02fc05d52811be27e85ea725682e23887dbc/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=28fb02fc05d52811be27e85ea725682e23887dbc",
            "patch": "@@ -1382,19 +1382,22 @@ def test_generate_with_head_masking(self):\n         attention_names = [\"encoder_attentions\", \"decoder_attentions\", \"cross_attentions\"]\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            text_config = config.get_text_config()\n \n             # We want to test only encoder-decoder models\n-            if not config.is_encoder_decoder:\n+            if not text_config.is_encoder_decoder:\n                 continue\n             model = model_class(config).to(torch_device)\n \n             head_masking = {\n-                \"head_mask\": torch.zeros(config.encoder_layers, config.encoder_attention_heads, device=torch_device),\n+                \"head_mask\": torch.zeros(\n+                    text_config.encoder_layers, text_config.encoder_attention_heads, device=torch_device\n+                ),\n                 \"decoder_head_mask\": torch.zeros(\n-                    config.decoder_layers, config.decoder_attention_heads, device=torch_device\n+                    text_config.decoder_layers, text_config.decoder_attention_heads, device=torch_device\n                 ),\n                 \"cross_attn_head_mask\": torch.zeros(\n-                    config.decoder_layers, config.decoder_attention_heads, device=torch_device\n+                    text_config.decoder_layers, text_config.decoder_attention_heads, device=torch_device\n                 ),\n             }\n "
        },
        {
            "sha": "bcac135be7210b7c64db555a8193a23fc0701877",
            "filename": "tests/models/fuyu/test_modeling_fuyu.py",
            "status": "modified",
            "additions": 15,
            "deletions": 1,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/28fb02fc05d52811be27e85ea725682e23887dbc/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28fb02fc05d52811be27e85ea725682e23887dbc/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py?ref=28fb02fc05d52811be27e85ea725682e23887dbc",
            "patch": "@@ -17,12 +17,15 @@\n import io\n import unittest\n \n+import pytest\n import requests\n+from parameterized import parameterized\n \n from transformers import FuyuConfig, is_torch_available, is_vision_available\n from transformers.testing_utils import require_torch, require_torch_gpu, slow, torch_device\n from transformers.utils import cached_property\n \n+from ...generation.test_utils import GenerationTesterMixin\n from ...test_modeling_common import ModelTesterMixin, ids_tensor, random_attention_mask\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n@@ -263,8 +266,9 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class FuyuModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class FuyuModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (FuyuForCausalLM,) if is_torch_available() else ()\n+    all_generative_model_classes = (FuyuForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\"text-generation\": FuyuForCausalLM, \"image-text-to-text\": FuyuForCausalLM} if is_torch_available() else {}\n     )\n@@ -296,6 +300,16 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n+    @pytest.mark.generate\n+    @parameterized.expand([(\"random\",), (\"same\",)])\n+    @unittest.skip(\"Fuyu doesn't support assisted generation due to the need to crop/extend image patches indices\")\n+    def test_assisted_decoding_matches_greedy_search(self):\n+        pass\n+\n+    @unittest.skip(\"Fuyu doesn't support assisted generation due to the need to crop/extend image patches indices\")\n+    def test_assisted_decoding_sample(self):\n+        pass\n+\n     # TODO: Fix me (once this model gets more usage)\n     @unittest.skip(reason=\"Does not work on the tiny model.\")\n     def test_disk_offload_bin(self):"
        },
        {
            "sha": "9b5089a635da976bc46f8802898d7db734986422",
            "filename": "tests/models/kosmos2/test_modeling_kosmos2.py",
            "status": "modified",
            "additions": 67,
            "deletions": 1,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/28fb02fc05d52811be27e85ea725682e23887dbc/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28fb02fc05d52811be27e85ea725682e23887dbc/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py?ref=28fb02fc05d52811be27e85ea725682e23887dbc",
            "patch": "@@ -21,7 +21,9 @@\n import unittest\n \n import numpy as np\n+import pytest\n import requests\n+from parameterized import parameterized\n \n from transformers import AutoModelForImageTextToText, AutoProcessor, Kosmos2Config\n from transformers.models.kosmos2.configuration_kosmos2 import Kosmos2TextConfig, Kosmos2VisionConfig\n@@ -37,6 +39,7 @@\n     is_vision_available,\n )\n \n+from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n     ModelTesterMixin,\n@@ -205,6 +208,7 @@ def __init__(self, parent, text_kwargs=None, vision_kwargs=None, latent_query_nu\n         self.text_model_tester = Kosmos2TextModelTester(parent, **text_kwargs)\n         self.vision_model_tester = Kosmos2VisionModelTester(parent, **vision_kwargs)\n         self.batch_size = self.text_model_tester.batch_size  # need bs for batching_equivalence test\n+        self.seq_length = self.text_model_tester.seq_length\n         self.latent_query_num = latent_query_num\n         self.is_training = is_training\n \n@@ -253,7 +257,7 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class Kosmos2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class Kosmos2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (Kosmos2Model, Kosmos2ForConditionalGeneration) if is_torch_available() else ()\n     all_generative_model_classes = (Kosmos2ForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n@@ -451,6 +455,68 @@ def check_same_values(layer_1, layer_2):\n             # self.assertTrue(model.transformer.wte.weight.shape, model.lm_head.weight.shape)\n             # self.assertTrue(check_same_values(model.transformer.wte, model.lm_head))\n \n+    @pytest.mark.generate\n+    @parameterized.expand([(\"greedy\", 1), (\"beam search\", 2)])\n+    @unittest.skip(\n+        \"KOSMOS-2 doesn't support inputs embeds. The test isn't skipped by checking input args because KOSMOS-2 has `generate()` overwritten\"\n+    )\n+    def test_generate_from_inputs_embeds(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    def test_left_padding_compatibility(self):\n+        # Overwrite because Kosmos-2 need to padd pixel values and pad image-attn-mask\n+\n+        def _prepare_model_kwargs(input_ids, attention_mask, pad_size, signature):\n+            model_kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n+            if \"position_ids\" in signature:\n+                position_ids = torch.cumsum(attention_mask, dim=-1) - 1\n+                position_ids.masked_fill_(attention_mask == 0, 1)\n+                model_kwargs[\"position_ids\"] = position_ids\n+            if \"cache_position\" in signature:\n+                cache_position = torch.arange(input_ids.shape[-1], device=torch_device)\n+                model_kwargs[\"cache_position\"] = cache_position\n+            if \"image_embeds_position_mask\" in signature:\n+                image_embeds_position_mask = torch.zeros_like(input_ids)\n+                image_embeds_position_mask[:, (pad_size + 1) : pad_size + 1 + self.model_tester.latent_query_num] = 1\n+                model_kwargs[\"image_embeds_position_mask\"] = image_embeds_position_mask\n+            return model_kwargs\n+\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            input_ids = inputs_dict[\"input_ids\"]\n+            pixel_values = inputs_dict[\"pixel_values\"]\n+            attention_mask = inputs_dict.get(\"attention_mask\")\n+            if attention_mask is None:\n+                attention_mask = torch.ones_like(input_ids)\n+\n+            model = model_class(config).to(torch_device).eval()\n+            signature = inspect.signature(model.forward).parameters.keys()\n+\n+            # no cache as some models require special cache classes to be init outside forward\n+            model.generation_config.use_cache = False\n+\n+            # Without padding\n+            model_kwargs = _prepare_model_kwargs(input_ids, attention_mask, pad_size=0, signature=signature)\n+            next_logits_wo_padding = model(**model_kwargs, pixel_values=pixel_values).logits[:, -1, :]\n+\n+            # With left-padding (length 32)\n+            # can hardcode pad_token to be 0 as we'll do attn masking anyway\n+            pad_token_id = (\n+                config.get_text_config().pad_token_id if config.get_text_config().pad_token_id is not None else 0\n+            )\n+            pad_size = (input_ids.shape[0], 32)\n+            padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * pad_token_id\n+            padded_input_ids = torch.cat((padding, input_ids), dim=1)\n+            padded_attention_mask = torch.cat((torch.zeros_like(padding), attention_mask), dim=1)\n+            model_kwargs = _prepare_model_kwargs(\n+                padded_input_ids, padded_attention_mask, pad_size=32, signature=signature\n+            )\n+            next_logits_with_padding = model(**model_kwargs, pixel_values=pixel_values).logits[:, -1, :]\n+\n+            # They should result in very similar logits\n+            self.assertTrue(torch.allclose(next_logits_wo_padding, next_logits_with_padding, atol=1e-3))\n+\n     @slow\n     def test_model_from_pretrained(self):\n         model_name = \"microsoft/kosmos-2-patch14-224\""
        },
        {
            "sha": "adec2c893a05fad23646ada4e0a4911818da0c4f",
            "filename": "tests/models/pix2struct/test_modeling_pix2struct.py",
            "status": "modified",
            "additions": 23,
            "deletions": 1,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/28fb02fc05d52811be27e85ea725682e23887dbc/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28fb02fc05d52811be27e85ea725682e23887dbc/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py?ref=28fb02fc05d52811be27e85ea725682e23887dbc",
            "patch": "@@ -27,6 +27,7 @@\n from transformers.testing_utils import require_torch, require_vision, slow, torch_device\n from transformers.utils import is_torch_available, is_vision_available\n \n+from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n     ModelTesterMixin,\n@@ -388,6 +389,7 @@ def __init__(self, parent, text_kwargs=None, vision_kwargs=None, is_training=Tru\n         self.batch_size = self.text_model_tester.batch_size  # need bs for batching_equivalence test\n         self.seq_length = self.text_model_tester.seq_length  # need seq_length for common tests\n         self.is_training = is_training\n+        self.max_patches = self.vision_model_tester.max_patches\n \n     def prepare_config_and_inputs(self):\n         text_config, input_ids, attention_mask = self.text_model_tester.prepare_config_and_inputs()\n@@ -417,7 +419,7 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class Pix2StructModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class Pix2StructModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (Pix2StructForConditionalGeneration,) if is_torch_available() else ()\n     all_generative_model_classes = (Pix2StructForConditionalGeneration,) if is_torch_available() else {}\n     pipeline_model_mapping = (\n@@ -751,6 +753,26 @@ def test_load_vision_text_config(self):\n             text_config = Pix2StructTextConfig.from_pretrained(tmp_dir_name)\n             self.assertDictEqual(config.text_config.to_dict(), text_config.to_dict())\n \n+    def _check_encoder_attention_for_generate(self, attentions, batch_size, config, seq_length):\n+        # overwrite because # pix2struct seq length depends on image inputs\n+        seq_length = self.model_tester.max_patches\n+        encoder_expected_shape = (batch_size, config.num_attention_heads, seq_length, seq_length)\n+        self.assertIsInstance(attentions, tuple)\n+        self.assertListEqual(\n+            [layer_attentions.shape for layer_attentions in attentions],\n+            [encoder_expected_shape] * len(attentions),\n+        )\n+\n+    def _check_encoder_hidden_states_for_generate(self, hidden_states, batch_size, config, seq_length):\n+        # overwrite because # pix2struct seq length depends on image inputs\n+        seq_length = self.model_tester.max_patches\n+        encoder_expected_shape = (batch_size, seq_length, config.hidden_size)\n+        self.assertIsInstance(hidden_states, tuple)\n+        self.assertListEqual(\n+            [layer_hidden_states.shape for layer_hidden_states in hidden_states],\n+            [encoder_expected_shape] * len(hidden_states),\n+        )\n+\n \n # We will verify our results on an image of a stop sign\n def prepare_img():"
        }
    ],
    "stats": {
        "total": 138,
        "additions": 129,
        "deletions": 9
    }
}