{
    "author": "Aravind-11",
    "message": "Add GLPNImageProcessorFast  (#41725)\n\n* Add GLPNImageProcessorFast for torch backend\n\n* Address review feedback\n\n- Simplified to_dict() method\n- Keep tensors as torch instead of converting to numpy for heterogeneous shapes\n- Removed unnecessary shape guards in post_process_depth_estimation\n- Improved variable names (tgt -> target_size, d -> resized)\n- Removed unnecessary GLPNImageProcessorKwargs class\n\n* Address review feedback\n\n- Simplified to_dict() method\n- Keep tensors as torch instead of converting to numpy for heterogeneous shapes\n- Removed unnecessary shape guards in post_process_depth_estimation\n- Improved variable names (tgt -> target_size, d -> resized)\n- Removed unnecessary GLPNImageProcessorKwargs class\n\n* commits after 2nd review\n\n* Address all review feedback and add explicit batched test\n\n- Simplified to_dict() with descriptive variable names (d->output_dict)\n- Fixed resize operation: changed from crop to proper resize with interpolation\n- Added padding for heterogeneous batch shapes in both slow and fast processors\n- Fused rescale and normalize operations for efficiency\n- Improved all variable names (tgt->target_size, d->depth_4d->resized)\n- Added GLPNImageProcessorKwargs class in slow processor and imported in fast\n- Renamed test_equivalence_slow_fast to test_slow_fast_equivalence\n- Added explicit test_slow_fast_equivalence_batched test\n- All 20 tests passing\n\n* using padding from utils\n\n* simplify glpn image processor fast\n\n* fix docstring\n\n---------\n\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "9a19171fad3025f57fae72d8f3598f44b68102e5",
    "files": [
        {
            "sha": "8081a6e0c66f498deb46aee2644eaab0c03fefea",
            "filename": "docs/source/en/model_doc/glpn.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a19171fad3025f57fae72d8f3598f44b68102e5/docs%2Fsource%2Fen%2Fmodel_doc%2Fglpn.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a19171fad3025f57fae72d8f3598f44b68102e5/docs%2Fsource%2Fen%2Fmodel_doc%2Fglpn.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglpn.md?ref=9a19171fad3025f57fae72d8f3598f44b68102e5",
            "patch": "@@ -61,6 +61,11 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n [[autodoc]] GLPNImageProcessor\n     - preprocess\n \n+## GLPNImageProcessorFast\n+\n+[[autodoc]] GLPNImageProcessorFast\n+    - preprocess\n+\n ## GLPNModel\n \n [[autodoc]] GLPNModel"
        },
        {
            "sha": "6ffc8882c9bb6e182fd278c93e6ba9161bb0624b",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a19171fad3025f57fae72d8f3598f44b68102e5/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a19171fad3025f57fae72d8f3598f44b68102e5/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=9a19171fad3025f57fae72d8f3598f44b68102e5",
            "patch": "@@ -306,6 +306,8 @@ def resize(\n                 Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n             interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n                 `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BICUBIC`.\n+            antialias (`bool`, *optional*, defaults to `True`):\n+                Whether to use antialiasing.\n \n         Returns:\n             `torch.Tensor`: The resized image."
        },
        {
            "sha": "653f3b43a2c20d1a0466be05be6a7f49d08fd8ad",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a19171fad3025f57fae72d8f3598f44b68102e5/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a19171fad3025f57fae72d8f3598f44b68102e5/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=9a19171fad3025f57fae72d8f3598f44b68102e5",
            "patch": "@@ -103,7 +103,7 @@\n             (\"gemma3n\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n             (\"git\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"glm4v\", (\"Glm4vImageProcessor\", \"Glm4vImageProcessorFast\")),\n-            (\"glpn\", (\"GLPNImageProcessor\", None)),\n+            (\"glpn\", (\"GLPNImageProcessor\", \"GLPNImageProcessorFast\")),\n             (\"got_ocr2\", (\"GotOcr2ImageProcessor\", \"GotOcr2ImageProcessorFast\")),\n             (\"grounding-dino\", (\"GroundingDinoImageProcessor\", \"GroundingDinoImageProcessorFast\")),\n             (\"groupvit\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),"
        },
        {
            "sha": "8d81194031c755915a9a3ed86a8ac6b2e37a6e4e",
            "filename": "src/transformers/models/glpn/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a19171fad3025f57fae72d8f3598f44b68102e5/src%2Ftransformers%2Fmodels%2Fglpn%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a19171fad3025f57fae72d8f3598f44b68102e5/src%2Ftransformers%2Fmodels%2Fglpn%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2F__init__.py?ref=9a19171fad3025f57fae72d8f3598f44b68102e5",
            "patch": "@@ -21,6 +21,7 @@\n     from .configuration_glpn import *\n     from .feature_extraction_glpn import *\n     from .image_processing_glpn import *\n+    from .image_processing_glpn_fast import *\n     from .modeling_glpn import *\n else:\n     import sys"
        },
        {
            "sha": "a509408400344689f7e7420e84cf60e38428c387",
            "filename": "src/transformers/models/glpn/image_processing_glpn.py",
            "status": "modified",
            "additions": 22,
            "deletions": 1,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a19171fad3025f57fae72d8f3598f44b68102e5/src%2Ftransformers%2Fmodels%2Fglpn%2Fimage_processing_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a19171fad3025f57fae72d8f3598f44b68102e5/src%2Ftransformers%2Fmodels%2Fglpn%2Fimage_processing_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Fimage_processing_glpn.py?ref=9a19171fad3025f57fae72d8f3598f44b68102e5",
            "patch": "@@ -39,6 +39,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, logging, requires_backends\n \n \n@@ -49,6 +50,17 @@\n logger = logging.get_logger(__name__)\n \n \n+class GLPNImageProcessorKwargs(ImagesKwargs, total=False):\n+    \"\"\"\n+    size_divisor (`int`, *optional*, defaults to 32):\n+        When `do_resize` is `True`, images are resized so their height and width are rounded down to the closest\n+        multiple of `size_divisor`.\n+    \"\"\"\n+\n+    size_divisor: int\n+    resample: PILImageResampling\n+\n+\n @requires(backends=(\"vision\",))\n class GLPNImageProcessor(BaseImageProcessor):\n     r\"\"\"\n@@ -66,22 +78,27 @@ class GLPNImageProcessor(BaseImageProcessor):\n         do_rescale (`bool`, *optional*, defaults to `True`):\n             Whether or not to apply the scaling factor (to make pixel values floats between 0. and 1.). Can be\n             overridden by `do_rescale` in `preprocess`.\n+        rescale_factor (`float`, *optional*, defaults to `1 / 255`):\n+            The scaling factor to apply to the pixel values. Can be overridden by `rescale_factor` in `preprocess`.\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = GLPNImageProcessorKwargs\n \n     def __init__(\n         self,\n         do_resize: bool = True,\n         size_divisor: int = 32,\n         resample=PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n+        rescale_factor: Optional[float] = 1 / 255,\n         **kwargs,\n     ) -> None:\n         self.do_resize = do_resize\n         self.do_rescale = do_rescale\n         self.size_divisor = size_divisor\n         self.resample = resample\n+        self.rescale_factor = rescale_factor\n         super().__init__(**kwargs)\n \n     def resize(\n@@ -142,6 +159,7 @@ def preprocess(\n         size_divisor: Optional[int] = None,\n         resample=None,\n         do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n         return_tensors: Optional[Union[TensorType, str]] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -181,6 +199,7 @@ def preprocess(\n         \"\"\"\n         do_resize = do_resize if do_resize is not None else self.do_resize\n         do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n         size_divisor = size_divisor if size_divisor is not None else self.size_divisor\n         resample = resample if resample is not None else self.resample\n \n@@ -217,7 +236,9 @@ def preprocess(\n             ]\n \n         if do_rescale:\n-            images = [self.rescale(image, scale=1 / 255, input_data_format=input_data_format) for image in images]\n+            images = [\n+                self.rescale(image, scale=rescale_factor, input_data_format=input_data_format) for image in images\n+            ]\n \n         images = [\n             to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images"
        },
        {
            "sha": "a906dc29c271582bbb098a8713b445f607293395",
            "filename": "src/transformers/models/glpn/image_processing_glpn_fast.py",
            "status": "added",
            "additions": 136,
            "deletions": 0,
            "changes": 136,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a19171fad3025f57fae72d8f3598f44b68102e5/src%2Ftransformers%2Fmodels%2Fglpn%2Fimage_processing_glpn_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a19171fad3025f57fae72d8f3598f44b68102e5/src%2Ftransformers%2Fmodels%2Fglpn%2Fimage_processing_glpn_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Fimage_processing_glpn_fast.py?ref=9a19171fad3025f57fae72d8f3598f44b68102e5",
            "patch": "@@ -0,0 +1,136 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for GLPN.\"\"\"\n+\n+from typing import Optional, Union\n+\n+import torch\n+from torchvision.transforms.v2 import functional as F\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import BaseImageProcessorFast, group_images_by_shape, reorder_images\n+from ...image_utils import (\n+    PILImageResampling,\n+    SizeDict,\n+)\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    requires_backends,\n+)\n+from .image_processing_glpn import GLPNImageProcessorKwargs\n+\n+\n+@auto_docstring\n+class GLPNImageProcessorFast(BaseImageProcessorFast):\n+    do_resize = True\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    resample = PILImageResampling.BILINEAR\n+    size_divisor = 32\n+    valid_kwargs = GLPNImageProcessorKwargs\n+\n+    def _validate_preprocess_kwargs(self, **kwargs):\n+        # pop `do_resize` to not raise an error as `size` is not None\n+        kwargs.pop(\"do_resize\", None)\n+        return super()._validate_preprocess_kwargs(**kwargs)\n+\n+    def resize(\n+        self,\n+        image: \"torch.Tensor\",\n+        size_divisor: int,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n+        antialias: bool = True,\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Resize an image to `(size[\"height\"], size[\"width\"])`.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`SizeDict`):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BICUBIC`.\n+            antialias (`bool`, *optional*, defaults to `True`):\n+                Whether to use antialiasing.\n+\n+        Returns:\n+            `torch.Tensor`: The resized image.\n+        \"\"\"\n+        height, width = image.shape[-2:]\n+        # Rounds the height and width down to the closest multiple of size_divisor\n+        new_h = height // size_divisor * size_divisor\n+        new_w = width // size_divisor * size_divisor\n+        return super().resize(\n+            image, SizeDict(height=new_h, width=new_w), interpolation=interpolation, antialias=antialias\n+        )\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size_divisor: Optional[int] = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n+        do_rescale: bool = True,\n+        rescale_factor: Optional[float] = 1 / 255,\n+        do_normalize: bool = False,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        disable_grouping: Optional[bool] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        resample: Optional[PILImageResampling] = None,\n+        **kwargs,\n+    ) -> BatchFeature:\n+        grouped_images, grouped_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        processed_groups = {}\n+\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(stacked_images, size_divisor=size_divisor, interpolation=interpolation)\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_groups[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_groups, grouped_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n+\n+    def post_process_depth_estimation(self, outputs, target_sizes=None):\n+        \"\"\"\n+        Convert raw model outputs to final depth predictions.\n+        Mirrors slow GLPN: PyTorch interpolate w/ bicubic, align_corners=False.\n+        \"\"\"\n+        requires_backends(self, \"torch\")\n+        predicted_depth = outputs.predicted_depth\n+\n+        results = []\n+        target_sizes = target_sizes or [None] * predicted_depth.shape[0]\n+        for depth, target_size in zip(predicted_depth, target_sizes):\n+            if target_size is not None:\n+                # Add batch and channel dimensions for interpolation\n+                depth_4d = depth[None, None, ...]\n+                resized = torch.nn.functional.interpolate(\n+                    depth_4d, size=target_size, mode=\"bicubic\", align_corners=False\n+                )\n+                depth = resized.squeeze(0).squeeze(0)\n+            results.append({\"predicted_depth\": depth})\n+\n+        return results\n+\n+\n+__all__ = [\"GLPNImageProcessorFast\"]"
        },
        {
            "sha": "396f7e9543e7b384efae4fce529a267462d40adf",
            "filename": "tests/models/glpn/test_image_processing_glpn.py",
            "status": "modified",
            "additions": 61,
            "deletions": 6,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a19171fad3025f57fae72d8f3598f44b68102e5/tests%2Fmodels%2Fglpn%2Ftest_image_processing_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a19171fad3025f57fae72d8f3598f44b68102e5/tests%2Fmodels%2Fglpn%2Ftest_image_processing_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglpn%2Ftest_image_processing_glpn.py?ref=9a19171fad3025f57fae72d8f3598f44b68102e5",
            "patch": "@@ -18,7 +18,7 @@\n import numpy as np\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -31,6 +31,9 @@\n \n     from transformers import GLPNImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import GLPNImageProcessorFast\n+\n \n class GLPNImageProcessingTester:\n     def __init__(\n@@ -87,19 +90,32 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n             torchify=torchify,\n         )\n \n+    def prepare_depth_outputs(self):\n+        if not is_torch_available():\n+            return None\n+        depth_tensors = prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=1,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=True,\n+            torchify=True,\n+        )\n+        depth_tensors = [depth_tensor.squeeze(0) for depth_tensor in depth_tensors]\n+        stacked_depth_tensors = torch.stack(depth_tensors, dim=0)\n+        return type(\"DepthOutput\", (), {\"predicted_depth\": stacked_depth_tensors})\n+\n \n @require_torch\n @require_vision\n class GLPNImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = GLPNImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = GLPNImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n         self.image_processor_tester = GLPNImageProcessingTester(self)\n-\n-    @property\n-    def image_processor_dict(self):\n-        return self.image_processor_tester.prepare_image_processor_dict()\n+        self.image_processor_dict = self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n         image_processing = self.image_processing_class(**self.image_processor_dict)\n@@ -115,7 +131,6 @@ def test_call_pil(self):\n         image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n         for image in image_inputs:\n             self.assertIsInstance(image, Image.Image)\n-\n         # Test not batched input (GLPNImageProcessor doesn't support batching)\n         encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n         expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n@@ -161,3 +176,43 @@ def test_call_numpy_4_channels(self):\n         expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n         self.assertTrue(tuple(encoded_images.shape) == (1, *expected_output_image_shape))\n         self.image_processing_class.num_channels = 3\n+\n+    # override as glpn image processors don't support heterogeneous batching\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+\n+    def test_post_process_depth_equivalence(self):\n+        # Check that both processors produce equivalent post-processed depth maps\n+        if self.fast_image_processing_class is None:\n+            self.skipTest(\"TorchVision not available\")\n+\n+        outputs = self.image_processor_tester.prepare_depth_outputs()\n+        slow = self.image_processing_class(**self.image_processor_dict)\n+        fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        # target_sizes simulate resized inference outputs\n+        target_sizes = [(240, 320)] * self.image_processor_tester.batch_size\n+        processed_slow = slow.post_process_depth_estimation(outputs, target_sizes=target_sizes)\n+        processed_fast = fast.post_process_depth_estimation(outputs, target_sizes=target_sizes)\n+\n+        # Compare per-sample predicted depth tensors\n+        for pred_slow, pred_fast in zip(processed_slow, processed_fast):\n+            depth_slow = pred_slow[\"predicted_depth\"]\n+            depth_fast = pred_fast[\"predicted_depth\"]\n+            torch.testing.assert_close(depth_fast, depth_slow, atol=1e-1, rtol=1e-3)\n+            self.assertLessEqual(torch.mean(torch.abs(depth_fast.float() - depth_slow.float())).item(), 5e-3)"
        }
    ],
    "stats": {
        "total": 236,
        "additions": 228,
        "deletions": 8
    }
}