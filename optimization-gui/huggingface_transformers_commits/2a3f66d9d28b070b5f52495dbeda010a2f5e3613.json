{
    "author": "SunMarc",
    "message": "Deprecate warmup_ratio (#41326)\n\n* dep\n\n* style\n\n* deprecate warmup_ratio\n\n* better\n\n* fix\n\n* Revert \"style\"\n\nThis reverts commit cf4f9e7c4f7837a88eea6eeabf8b4dfe9455f6dc.\n\n* Revert \"dep\"\n\nThis reverts commit 1800beb13f407ddb881d0af936860643e84ba085.\n\n* update version",
    "sha": "2a3f66d9d28b070b5f52495dbeda010a2f5e3613",
    "files": [
        {
            "sha": "e95d6e9c8d66535ed692a2052a1c54139a2c772e",
            "filename": "docs/source/en/optimizers.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fen%2Foptimizers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fen%2Foptimizers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Foptimizers.md?ref=2a3f66d9d28b070b5f52495dbeda010a2f5e3613",
            "patch": "@@ -154,7 +154,7 @@ pip install schedulefree\n \n [Schedule Free optimizer (SFO)](https://hf.co/papers/2405.15682) replaces the base optimizers momentum with a combination of averaging and interpolation. Unlike a traditional scheduler, SFO completely removes the need to anneal the learning rate.\n \n-SFO supports the RAdam (`schedule_free_radam`), AdamW (`schedule_free_adamw`) and SGD (`schedule_free_sgd`) optimizers. The RAdam scheduler doesn't require `warmup_steps` or `warmup_ratio`.\n+SFO supports the RAdam (`schedule_free_radam`), AdamW (`schedule_free_adamw`) and SGD (`schedule_free_sgd`) optimizers. The RAdam scheduler doesn't require `warmup_steps`.\n \n By default, it is recommended to set `lr_scheduler_type=\"constant\"`. Other `lr_scheduler_type` values may also work, but combining SFO optimizers with other learning rate schedules could affect SFOs intended behavior and performance.\n "
        },
        {
            "sha": "cc8a8238022a6d9b80146e8c8c8a4f879e015736",
            "filename": "docs/source/en/tasks/audio_classification.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fen%2Ftasks%2Faudio_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fen%2Ftasks%2Faudio_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Faudio_classification.md?ref=2a3f66d9d28b070b5f52495dbeda010a2f5e3613",
            "patch": "@@ -220,7 +220,7 @@ At this point, only three steps remain:\n ...     gradient_accumulation_steps=4,\n ...     per_device_eval_batch_size=32,\n ...     num_train_epochs=10,\n-...     warmup_ratio=0.1,\n+...     warmup_steps=0.1,\n ...     logging_steps=10,\n ...     load_best_model_at_end=True,\n ...     metric_for_best_model=\"accuracy\","
        },
        {
            "sha": "0af4be8ed6b9a8cbf016875c2c7a6ec3d502f4cf",
            "filename": "docs/source/en/tasks/image_classification.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fen%2Ftasks%2Fimage_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fen%2Ftasks%2Fimage_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fimage_classification.md?ref=2a3f66d9d28b070b5f52495dbeda010a2f5e3613",
            "patch": "@@ -211,7 +211,7 @@ At this point, only three steps remain:\n ...     gradient_accumulation_steps=4,\n ...     per_device_eval_batch_size=16,\n ...     num_train_epochs=3,\n-...     warmup_ratio=0.1,\n+...     warmup_steps=0.1,\n ...     logging_steps=10,\n ...     load_best_model_at_end=True,\n ...     metric_for_best_model=\"accuracy\","
        },
        {
            "sha": "c6b5a0e17d9f409e9cfc0b9e727aeee03d1a970c",
            "filename": "docs/source/en/tasks/video_classification.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fen%2Ftasks%2Fvideo_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fen%2Ftasks%2Fvideo_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fvideo_classification.md?ref=2a3f66d9d28b070b5f52495dbeda010a2f5e3613",
            "patch": "@@ -378,7 +378,7 @@ Most of the training arguments are self-explanatory, but one that is quite impor\n ...     learning_rate=5e-5,\n ...     per_device_train_batch_size=batch_size,\n ...     per_device_eval_batch_size=batch_size,\n-...     warmup_ratio=0.1,\n+...     warmup_steps=0.1,\n ...     logging_steps=10,\n ...     load_best_model_at_end=True,\n ...     metric_for_best_model=\"accuracy\","
        },
        {
            "sha": "bc63a93c88d275c74459cf18bdb100633b9fb5ee",
            "filename": "docs/source/es/tasks/audio_classification.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fes%2Ftasks%2Faudio_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fes%2Ftasks%2Faudio_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Ftasks%2Faudio_classification.md?ref=2a3f66d9d28b070b5f52495dbeda010a2f5e3613",
            "patch": "@@ -220,7 +220,7 @@ Al llegar a este punto, solo quedan tres pasos:\n ...     gradient_accumulation_steps=4,\n ...     per_device_eval_batch_size=32,\n ...     num_train_epochs=10,\n-...     warmup_ratio=0.1,\n+...     warmup_steps=0.1,\n ...     logging_steps=10,\n ...     load_best_model_at_end=True,\n ...     metric_for_best_model=\"accuracy\","
        },
        {
            "sha": "84d2bfd492a1f8b6acabbc834edd7ee5e0802d2c",
            "filename": "docs/source/ja/main_classes/deepspeed.md",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fja%2Fmain_classes%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fja%2Fmain_classes%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Fdeepspeed.md?ref=2a3f66d9d28b070b5f52495dbeda010a2f5e3613",
            "patch": "@@ -1292,7 +1292,7 @@ DeepSpeed ã¯ã€`LRRangeTest`ã€`OneCycle`ã€`WarmupLR`ã€ãŠã‚ˆã³`WarmupDecayL\n   ã—ãŸãŒã£ã¦ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’è¨­å®šã—ãªã„å ´åˆã€ã“ã‚ŒãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§è¨­å®šã•ã‚Œã‚‹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã«ãªã‚Šã¾ã™ã€‚\n \n è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§ `scheduler` ã‚¨ãƒ³ãƒˆãƒªã‚’è¨­å®šã—ãªã„å ´åˆã€[`Trainer`] ã¯\n-`--lr_scheduler_type`ã€`--learning_rate`ã€ãŠã‚ˆã³ `--warmup_steps` ã¾ãŸã¯ `--warmup_ratio` ã®å€¤ã‚’è¨­å®šã—ã¾ã™ã€‚\n+`--lr_scheduler_type`ã€`--learning_rate`ã€ãŠã‚ˆã³ `--warmup_steps` ã®å€¤ã‚’è¨­å®šã—ã¾ã™ã€‚\n ğŸ¤— ãã‚Œã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€‚\n \n ä»¥ä¸‹ã¯ã€`WarmupLR`ã®è‡ªå‹•æ§‹æˆã•ã‚ŒãŸ`scheduler`ã‚¨ãƒ³ãƒˆãƒªã®ä¾‹ã§ã™ã€‚\n@@ -1316,8 +1316,7 @@ DeepSpeed ã¯ã€`LRRangeTest`ã€`OneCycle`ã€`WarmupLR`ã€ãŠã‚ˆã³`WarmupDecayL\n \n - `warmup_min_lr` ã®å€¤ã¯ `0` ã§ã™ã€‚\n - `warmup_max_lr` ã¨ `--learning_rate` ã®å€¤ã€‚\n-- `warmup_num_steps` ã¨ `--warmup_steps` ã®å€¤ (æŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ)ã€‚ãã‚Œä»¥å¤–ã®å ´åˆã¯ `--warmup_ratio` ã‚’ä½¿ç”¨ã—ã¾ã™\n-  ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ã‚¹ãƒ†ãƒƒãƒ—ã®æ•°ã‚’ä¹—ç®—ã—ã€åˆ‡ã‚Šä¸Šã’ã¾ã™ã€‚\n+- `warmup_num_steps` ã¨ `--warmup_steps` ã®å€¤ (æŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ)\n - `total_num_steps` ã«ã¯ `--max_steps` ã®å€¤ã‚’æŒ‡å®šã™ã‚‹ã‹ã€æŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯å®Ÿè¡Œæ™‚ã«è‡ªå‹•çš„ã«å°å‡ºã•ã‚Œã¾ã™ã€‚\n   ç’°å¢ƒã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚ºã€ãŠã‚ˆã³ãã®ä»–ã®ã‚³ãƒãƒ³ãƒ‰ ãƒ©ã‚¤ãƒ³å¼•æ•° (\n   `WarmupDecayLR`)ã€‚"
        },
        {
            "sha": "e1831aa50c38740bb495f222fe1b6999d9d1754e",
            "filename": "docs/source/ja/tasks/audio_classification.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fja%2Ftasks%2Faudio_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fja%2Ftasks%2Faudio_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Faudio_classification.md?ref=2a3f66d9d28b070b5f52495dbeda010a2f5e3613",
            "patch": "@@ -219,7 +219,7 @@ MInDS-14 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ãƒ¬ãƒ¼ãƒˆã¯ 8khz ã§ã™ (ã“\n ...     gradient_accumulation_steps=4,\n ...     per_device_eval_batch_size=32,\n ...     num_train_epochs=10,\n-...     warmup_ratio=0.1,\n+...     warmup_steps=0.1,\n ...     logging_steps=10,\n ...     load_best_model_at_end=True,\n ...     metric_for_best_model=\"accuracy\","
        },
        {
            "sha": "164176a911d5c90e889e2ce92587c226e8016f3a",
            "filename": "docs/source/ja/tasks/image_classification.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fja%2Ftasks%2Fimage_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fja%2Ftasks%2Fimage_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fimage_classification.md?ref=2a3f66d9d28b070b5f52495dbeda010a2f5e3613",
            "patch": "@@ -216,7 +216,7 @@ Datasetsã€ğŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰ Food-101 ãƒ‡ãƒ¼ã‚¿ã‚»\n ...     gradient_accumulation_steps=4,\n ...     per_device_eval_batch_size=16,\n ...     num_train_epochs=3,\n-...     warmup_ratio=0.1,\n+...     warmup_steps=0.1,\n ...     logging_steps=10,\n ...     load_best_model_at_end=True,\n ...     metric_for_best_model=\"accuracy\","
        },
        {
            "sha": "32e871f0ab4927552e1f05b37ba8ab0a1ed4b2e7",
            "filename": "docs/source/ja/tasks/video_classification.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fja%2Ftasks%2Fvideo_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fja%2Ftasks%2Fvideo_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fvideo_classification.md?ref=2a3f66d9d28b070b5f52495dbeda010a2f5e3613",
            "patch": "@@ -360,7 +360,7 @@ You should probably TRAIN this model on a down-stream task to be able to use it\n ...     learning_rate=5e-5,\n ...     per_device_train_batch_size=batch_size,\n ...     per_device_eval_batch_size=batch_size,\n-...     warmup_ratio=0.1,\n+...     warmup_steps=0.1,\n ...     logging_steps=10,\n ...     load_best_model_at_end=True,\n ...     metric_for_best_model=\"accuracy\","
        },
        {
            "sha": "a5bf877ed6e570fb3c2494767240033934bfb992",
            "filename": "docs/source/ko/optimizers.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fko%2Foptimizers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fko%2Foptimizers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Foptimizers.md?ref=2a3f66d9d28b070b5f52495dbeda010a2f5e3613",
            "patch": "@@ -154,7 +154,7 @@ pip install schedulefree\n \n [Schedule Free optimizer (SFO)](https://hf.co/papers/2405.15682)ëŠ” ê¸°ë³¸ ì˜µí‹°ë§ˆì´ì €ì˜ ëª¨ë©˜í…€ ëŒ€ì‹  í‰ê· í™”(averaging)ì™€ ë³´ê°„(interpolation)ì„ ì¡°í•©í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤. ë•ë¶„ì— ê¸°ì¡´ì˜ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ì™€ ë‹¬ë¦¬, SFOëŠ” í•™ìŠµë¥ ì„ ì ì§„ì ìœ¼ë¡œ ë‚®ì¶”ëŠ” ì ˆì°¨ê°€ ì•„ì˜ˆ í•„ìš” ì—†ìŠµë‹ˆë‹¤.\n \n-SFOëŠ” RAdam(`schedule_free_radam`), AdamW(`schedule_free_adamw`), SGD(`schedule_free_sgd`) ì˜µí‹°ë§ˆì´ì €ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. RAdam ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” `warmup_steps`ë‚˜ `warmup_ratio` ì„¤ì •ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. \n+SFOëŠ” RAdam(`schedule_free_radam`), AdamW(`schedule_free_adamw`), SGD(`schedule_free_sgd`) ì˜µí‹°ë§ˆì´ì €ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. RAdam ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” `warmup_steps`.\n \n ê¸°ë³¸ì ìœ¼ë¡œ `lr_scheduler_type=\"constant\"`ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. ë‹¤ë¥¸ `lr_scheduler_type` ê°’ë„ ë™ì‘í•  ìˆœ ìˆìœ¼ë‚˜, SFO ì˜µí‹°ë§ˆì´ì €ì™€ ë‹¤ë¥¸ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ì„ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ SFOì˜ ì˜ë„ëœ ë™ì‘ê³¼ ì„±ëŠ¥ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n "
        },
        {
            "sha": "983692bc100c3b9fc7d286dc1dd90841650bc7ed",
            "filename": "docs/source/ko/tasks/audio_classification.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fko%2Ftasks%2Faudio_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fko%2Ftasks%2Faudio_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Faudio_classification.md?ref=2a3f66d9d28b070b5f52495dbeda010a2f5e3613",
            "patch": "@@ -221,7 +221,7 @@ MinDS-14 ë°ì´í„° ì„¸íŠ¸ì˜ ìƒ˜í”Œë§ ì†ë„ëŠ” 8khzì´ë¯€ë¡œ(ì´ ì •ë³´ëŠ” [\n ...     gradient_accumulation_steps=4,\n ...     per_device_eval_batch_size=32,\n ...     num_train_epochs=10,\n-...     warmup_ratio=0.1,\n+...     warmup_steps=0.1,\n ...     logging_steps=10,\n ...     load_best_model_at_end=True,\n ...     metric_for_best_model=\"accuracy\","
        },
        {
            "sha": "3e1e829ae8d5b2036b45aa4a5179d521341c619d",
            "filename": "docs/source/ko/tasks/image_classification.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fko%2Ftasks%2Fimage_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fko%2Ftasks%2Fimage_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fimage_classification.md?ref=2a3f66d9d28b070b5f52495dbeda010a2f5e3613",
            "patch": "@@ -212,7 +212,7 @@ Hugging Face ê³„ì •ì— ë¡œê·¸ì¸í•˜ì—¬ ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ê³  ì»¤ë®¤ë‹ˆí‹°ì—\n ...     gradient_accumulation_steps=4,\n ...     per_device_eval_batch_size=16,\n ...     num_train_epochs=3,\n-...     warmup_ratio=0.1,\n+...     warmup_steps=0.1,\n ...     logging_steps=10,\n ...     load_best_model_at_end=True,\n ...     metric_for_best_model=\"accuracy\","
        },
        {
            "sha": "b220323aa2e336cc253a4a5c2444aa03aba96ca3",
            "filename": "docs/source/ko/tasks/video_classification.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fko%2Ftasks%2Fvideo_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fko%2Ftasks%2Fvideo_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fvideo_classification.md?ref=2a3f66d9d28b070b5f52495dbeda010a2f5e3613",
            "patch": "@@ -357,7 +357,7 @@ You should probably TRAIN this model on a down-stream task to be able to use it\n ...     learning_rate=5e-5,\n ...     per_device_train_batch_size=batch_size,\n ...     per_device_eval_batch_size=batch_size,\n-...     warmup_ratio=0.1,\n+...     warmup_steps=0.1,\n ...     logging_steps=10,\n ...     load_best_model_at_end=True,\n ...     metric_for_best_model=\"accuracy\","
        },
        {
            "sha": "8cd047ed5a0a019fd30c0e0ac1afabbfb0a8aeb4",
            "filename": "docs/source/zh/main_classes/deepspeed.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fzh%2Fmain_classes%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/docs%2Fsource%2Fzh%2Fmain_classes%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmain_classes%2Fdeepspeed.md?ref=2a3f66d9d28b070b5f52495dbeda010a2f5e3613",
            "patch": "@@ -1206,7 +1206,7 @@ DeepSpeedæ”¯æŒ`LRRangeTest`ã€`OneCycle`ã€`WarmupLR`å’Œ`WarmupDecayLR`å­¦ä¹ \n - é€šè¿‡ `--lr_scheduler_type constant_with_warmup` å®ç° `WarmupLR`\n - é€šè¿‡ `--lr_scheduler_type linear` å®ç° `WarmupDecayLR`ã€‚è¿™ä¹Ÿæ˜¯ `--lr_scheduler_type` çš„é»˜è®¤å€¼ï¼Œå› æ­¤ï¼Œå¦‚æœä¸é…ç½®è°ƒåº¦å™¨ï¼Œè¿™å°†æ˜¯é»˜è®¤é…ç½®çš„è°ƒåº¦å™¨ã€‚\n \n-å¦‚æœåœ¨é…ç½®æ–‡ä»¶ä¸­ä¸é…ç½® `scheduler` æ¡ç›®ï¼Œ[`Trainer`] å°†ä½¿ç”¨ `--lr_scheduler_type`ã€`--learning_rate` å’Œ `--warmup_steps` æˆ– `--warmup_ratio` çš„å€¼æ¥é…ç½®å…¶ğŸ¤— Transformers ç‰ˆæœ¬ã€‚\n+å¦‚æœåœ¨é…ç½®æ–‡ä»¶ä¸­ä¸é…ç½® `scheduler` æ¡ç›®ï¼Œ[`Trainer`] å°†ä½¿ç”¨ `--lr_scheduler_type`ã€`--learning_rate` å’Œ `--warmup_steps` çš„å€¼æ¥é…ç½®å…¶ğŸ¤— Transformers ç‰ˆæœ¬ã€‚\n \n ä»¥ä¸‹æ˜¯ `WarmupLR` çš„è‡ªåŠ¨é…ç½®ç¤ºä¾‹ï¼š\n \n@@ -1227,7 +1227,7 @@ DeepSpeedæ”¯æŒ`LRRangeTest`ã€`OneCycle`ã€`WarmupLR`å’Œ`WarmupDecayLR`å­¦ä¹ \n \n - `warmup_min_lr` çš„å€¼ä¸º `0`ã€‚\n - `warmup_max_lr` çš„å€¼ä¸º `--learning_rate`ã€‚\n-- `warmup_num_steps` çš„å€¼ä¸º `--warmup_steps`ï¼ˆå¦‚æœæä¾›ï¼‰ã€‚å¦åˆ™ï¼Œå°†ä½¿ç”¨ `--warmup_ratio` ä¹˜ä»¥è®­ç»ƒæ­¥éª¤çš„æ•°é‡ï¼Œå¹¶å››èˆäº”å…¥ã€‚\n+- `warmup_num_steps` çš„å€¼ä¸º `--warmup_steps`ï¼ˆå¦‚æœæä¾›ï¼‰ã€‚\n - `total_num_steps` çš„å€¼ä¸º `--max_steps` æˆ–è€…å¦‚æœæ²¡æœ‰æä¾›ï¼Œå°†åœ¨è¿è¡Œæ—¶æ ¹æ®ç¯å¢ƒã€æ•°æ®é›†çš„å¤§å°å’Œå…¶ä»–å‘½ä»¤è¡Œå‚æ•°ï¼ˆå¯¹äº `WarmupDecayLR` æ¥è¯´éœ€è¦ï¼‰è‡ªåŠ¨æ¨å¯¼ã€‚\n \n å½“ç„¶ï¼Œæ‚¨å¯ä»¥æ¥ç®¡ä»»ä½•æˆ–æ‰€æœ‰çš„é…ç½®å€¼ï¼Œå¹¶è‡ªè¡Œè®¾ç½®è¿™äº›å€¼ï¼š"
        },
        {
            "sha": "8872563716c0202eba2313df4096c3ec1e8f5146",
            "filename": "examples/pytorch/audio-classification/README.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/examples%2Fpytorch%2Faudio-classification%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/examples%2Fpytorch%2Faudio-classification%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Faudio-classification%2FREADME.md?ref=2a3f66d9d28b070b5f52495dbeda010a2f5e3613",
            "patch": "@@ -41,7 +41,7 @@ python run_audio_classification.py \\\n     --learning_rate 3e-5 \\\n     --max_length_seconds 1 \\\n     --attention_mask False \\\n-    --warmup_ratio 0.1 \\\n+    --warmup_steps 0.1 \\\n     --num_train_epochs 5 \\\n     --per_device_train_batch_size 32 \\\n     --gradient_accumulation_steps 4 \\\n@@ -82,7 +82,7 @@ python run_audio_classification.py \\\n     --learning_rate 3e-4 \\\n     --max_length_seconds 16 \\\n     --attention_mask False \\\n-    --warmup_ratio 0.1 \\\n+    --warmup_steps 0.1 \\\n     --num_train_epochs 10 \\\n     --per_device_train_batch_size 8 \\\n     --gradient_accumulation_steps 4 \\"
        },
        {
            "sha": "f5a1289efb4315098e8134d0fdba971ca88ba511",
            "filename": "examples/pytorch/image-pretraining/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/examples%2Fpytorch%2Fimage-pretraining%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/examples%2Fpytorch%2Fimage-pretraining%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-pretraining%2FREADME.md?ref=2a3f66d9d28b070b5f52495dbeda010a2f5e3613",
            "patch": "@@ -165,7 +165,7 @@ python run_mae.py \\\n     --lr_scheduler_type cosine \\\n     --weight_decay 0.05 \\\n     --num_train_epochs 800 \\\n-    --warmup_ratio 0.05 \\\n+    --warmup_steps 0.05 \\\n     --per_device_train_batch_size 8 \\\n     --per_device_eval_batch_size 8 \\\n     --logging_strategy steps \\"
        },
        {
            "sha": "2a53bb9ba4ff1c8772ebce1cdcba96ea11647503",
            "filename": "src/transformers/modelcard.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/src%2Ftransformers%2Fmodelcard.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/src%2Ftransformers%2Fmodelcard.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodelcard.py?ref=2a3f66d9d28b070b5f52495dbeda010a2f5e3613",
            "patch": "@@ -752,8 +752,6 @@ def extract_hyperparameters_from_trainer(trainer):\n             hyperparameters[\"optimizer\"] = f\"Use {optimizer_name} and the args are:\\n{optimizer_args}\"\n \n     hyperparameters[\"lr_scheduler_type\"] = trainer.args.lr_scheduler_type.value\n-    if trainer.args.warmup_ratio != 0.0:\n-        hyperparameters[\"lr_scheduler_warmup_ratio\"] = trainer.args.warmup_ratio\n     if trainer.args.warmup_steps != 0.0:\n         hyperparameters[\"lr_scheduler_warmup_steps\"] = trainer.args.warmup_steps\n     if trainer.args.max_steps != -1:"
        },
        {
            "sha": "c57fcafdfcc63cfed0e23f7e9cdec131d72b8fa6",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 27,
            "deletions": 27,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a3f66d9d28b070b5f52495dbeda010a2f5e3613/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=2a3f66d9d28b070b5f52495dbeda010a2f5e3613",
            "patch": "@@ -285,10 +285,9 @@ class TrainingArguments:\n             The scheduler type to use. See the documentation of [`SchedulerType`] for all possible values.\n         lr_scheduler_kwargs (`dict` or `str`, *optional*, defaults to `None`):\n             The extra arguments for the lr_scheduler. See the documentation of each scheduler for possible values.\n-        warmup_ratio (`float`, *optional*, defaults to 0.0):\n-            Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.\n-        warmup_steps (`int`, *optional*, defaults to 0):\n-            Number of steps used for a linear warmup from 0 to `learning_rate`. Overrides any effect of `warmup_ratio`.\n+        warmup_steps (`int` or `float`, *optional*, defaults to 0):\n+            Number of steps used for a linear warmup from 0 to `learning_rate`.  Should be an integer or a float in range `[0,1)`.\n+            If smaller than 1, will be interpreted as ratio of steps used for a linear warmup from 0 to `learning_rate`.\n         log_level (`str`, *optional*, defaults to `passive`):\n             Logger log level to use on the main process. Possible choices are the log levels as strings: 'debug',\n             'info', 'warning', 'error' and 'critical', plus a 'passive' level which doesn't set anything and keeps the\n@@ -850,10 +849,14 @@ class TrainingArguments:\n             )\n         },\n     )\n-    warmup_ratio: float = field(\n-        default=0.0, metadata={\"help\": \"Linear warmup over warmup_ratio fraction of total steps.\"}\n+    warmup_ratio: Optional[float] = field(\n+        default=None,\n+        metadata={\n+            \"help\": \"This argument is deprecated and will be removed in v5. Use `warmup_steps` instead as it also works with float values.\"\n+        },\n     )\n-    warmup_steps: int = field(default=0, metadata={\"help\": \"Linear warmup over warmup_steps.\"})\n+\n+    warmup_steps: float = field(default=0, metadata={\"help\": \"Linear warmup over warmup_steps.\"})\n \n     log_level: str = field(\n         default=\"passive\",\n@@ -1637,16 +1640,12 @@ def __post_init__(self):\n         elif not isinstance(self.report_to, list):\n             self.report_to = [self.report_to]\n \n-        if self.warmup_ratio < 0 or self.warmup_ratio > 1:\n-            raise ValueError(\"warmup_ratio must lie in range [0,1]\")\n-        elif self.warmup_ratio > 0 and self.warmup_steps > 0:\n-            logger.info(\n-                \"Both warmup_ratio and warmup_steps given, warmup_steps will override any effect of warmup_ratio\"\n-                \" during training\"\n-            )\n+        if self.warmup_ratio is not None:\n+            logger.warning(\"warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\")\n+            self.warmup_steps = self.warmup_ratio\n \n-        if not isinstance(self.warmup_steps, int) or self.warmup_steps < 0:\n-            raise ValueError(\"warmup_steps must be of type int and must be 0 or a positive integer.\")\n+        if self.warmup_steps < 0:\n+            raise ValueError(\"warmup_steps must be an integer or a float\")\n \n         if isinstance(self.debug, str):\n             self.debug = [DebugOption(s) for s in self.debug.split()]\n@@ -2047,7 +2046,7 @@ def get_warmup_steps(self, num_training_steps: int):\n         Get number of steps used for a linear warmup.\n         \"\"\"\n         warmup_steps = (\n-            self.warmup_steps if self.warmup_steps > 0 else math.ceil(num_training_steps * self.warmup_ratio)\n+            int(self.warmup_steps) if self.warmup_steps >= 1 else math.ceil(num_training_steps * self.warmup_steps)\n         )\n         return warmup_steps\n \n@@ -2535,8 +2534,8 @@ def set_lr_scheduler(\n         name: Union[str, SchedulerType] = \"linear\",\n         num_epochs: float = 3.0,\n         max_steps: int = -1,\n-        warmup_ratio: float = 0,\n-        warmup_steps: int = 0,\n+        warmup_steps: float = 0,\n+        warmup_ratio: Optional[float] = None,\n     ):\n         \"\"\"\n         A method that regroups all arguments linked to the learning rate scheduler and its hyperparameters.\n@@ -2551,27 +2550,28 @@ def set_lr_scheduler(\n                 If set to a positive number, the total number of training steps to perform. Overrides `num_train_epochs`.\n                 For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until\n                 `max_steps` is reached.\n-            warmup_ratio (`float`, *optional*, defaults to 0.0):\n-                Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.\n-            warmup_steps (`int`, *optional*, defaults to 0):\n-                Number of steps used for a linear warmup from 0 to `learning_rate`. Overrides any effect of\n-                `warmup_ratio`.\n+            warmup_steps (`float`, *optional*, defaults to 0):\n+                Number of steps used for a linear warmup from 0 to `learning_rate`.  Should be an integer or a float in range `[0,1)`.\n+                If smaller than 1, will be interpreted as ratio of steps used for a linear warmup from 0 to `learning_rate`.\n \n         Example:\n \n         ```py\n         >>> from transformers import TrainingArguments\n \n         >>> args = TrainingArguments(\"working_dir\")\n-        >>> args = args.set_lr_scheduler(name=\"cosine\", warmup_ratio=0.05)\n-        >>> args.warmup_ratio\n+        >>> args = args.set_lr_scheduler(name=\"cosine\", warmup_steps=0.05)\n+        >>> args.warmup_steps\n         0.05\n         ```\n         \"\"\"\n+        if warmup_ratio is not None:\n+            logger.warning(\"warmup_ratio is deprecated and will be removed in v5. Use `warmup_steps` instead.\")\n+            warmup_steps = warmup_ratio\n+\n         self.lr_scheduler_type = SchedulerType(name)\n         self.num_train_epochs = num_epochs\n         self.max_steps = max_steps\n-        self.warmup_ratio = warmup_ratio\n         self.warmup_steps = warmup_steps\n         return self\n "
        }
    ],
    "stats": {
        "total": 95,
        "additions": 46,
        "deletions": 49
    }
}