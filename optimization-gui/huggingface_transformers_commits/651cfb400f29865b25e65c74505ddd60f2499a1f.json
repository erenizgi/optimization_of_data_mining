{
    "author": "yonigozlan",
    "message": "Add support for modular with fast image processors (#35379)\n\n* Add support for modular with fast image processors\r\n\r\n* fix order and remove copied from\r\n\r\n* add comment for \"image_processing*_fast\"",
    "sha": "651cfb400f29865b25e65c74505ddd60f2499a1f",
    "files": [
        {
            "sha": "565da27ded7c63f34494a7a59eb61515d35cc77e",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr_fast.py",
            "status": "modified",
            "additions": 28,
            "deletions": 56,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/651cfb400f29865b25e65c74505ddd60f2499a1f/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/651cfb400f29865b25e65c74505ddd60f2499a1f/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py?ref=651cfb400f29865b25e65c74505ddd60f2499a1f",
            "patch": "@@ -1,19 +1,9 @@\n-# coding=utf-8\n-# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Fast Image processor class for Deformable DETR.\"\"\"\n-\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/deformable_detr/modular_deformable_detr.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_deformable_detr.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n import functools\n import pathlib\n from typing import Any, Dict, List, Optional, Tuple, Union\n@@ -26,10 +16,7 @@\n     get_max_height_width,\n     safe_squeeze,\n )\n-from ...image_transforms import (\n-    center_to_corners_format,\n-    corners_to_center_format,\n-)\n+from ...image_transforms import center_to_corners_format, corners_to_center_format\n from ...image_utils import (\n     IMAGENET_DEFAULT_MEAN,\n     IMAGENET_DEFAULT_STD,\n@@ -43,7 +30,6 @@\n     get_image_type,\n     infer_channel_dimension_format,\n     make_list_of_images,\n-    pil_torch_interpolation_mapping,\n     validate_annotations,\n     validate_kwargs,\n )\n@@ -55,32 +41,30 @@\n     is_vision_available,\n     logging,\n )\n-from .image_processing_deformable_detr import (\n-    get_size_with_aspect_ratio,\n-)\n+from .image_processing_deformable_detr import get_size_with_aspect_ratio\n \n \n if is_torch_available():\n     import torch\n \n-if is_torchvision_available():\n-    from torchvision.io import read_image\n+if is_vision_available():\n+    from ...image_utils import pil_torch_interpolation_mapping\n \n-    if is_vision_available():\n-        from ...image_utils import pil_torch_interpolation_mapping\n \n-    if is_torchvision_v2_available():\n-        from torchvision.transforms.v2 import functional as F\n-    else:\n-        from torchvision.transforms import functional as F\n+if is_torchvision_v2_available():\n+    from torchvision.io import read_image\n+    from torchvision.transforms.v2 import functional as F\n+elif is_torchvision_available():\n+    from torchvision.io import read_image\n+    from torchvision.transforms import functional as F\n \n \n logger = logging.get_logger(__name__)\n \n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n \n \n-# Copied from transformers.models.detr.image_processing_detr_fast.convert_coco_poly_to_mask\n+# inspired by https://github.com/facebookresearch/deformable_detr/blob/master/datasets/coco.py#L33\n def convert_coco_poly_to_mask(segmentations, height: int, width: int, device: torch.device) -> torch.Tensor:\n     \"\"\"\n     Convert a COCO polygon annotation to a mask.\n@@ -115,15 +99,15 @@ def convert_coco_poly_to_mask(segmentations, height: int, width: int, device: to\n     return masks\n \n \n-# Copied from transformers.models.detr.image_processing_detr_fast.prepare_coco_detection_annotation with DETR->DeformableDetr\n+# inspired by https://github.com/facebookresearch/deformable_detr/blob/master/datasets/coco.py#L50\n def prepare_coco_detection_annotation(\n     image,\n     target,\n     return_segmentation_masks: bool = False,\n     input_data_format: Optional[Union[ChannelDimension, str]] = None,\n ):\n     \"\"\"\n-    Convert the target in COCO format into the format expected by DeformableDetr.\n+    Convert the target in COCO format into the format expected by DEFORMABLE_DETR.\n     \"\"\"\n     image_height, image_width = image.size()[-2:]\n \n@@ -180,7 +164,6 @@ def prepare_coco_detection_annotation(\n     return new_target\n \n \n-# Copied from transformers.models.detr.image_processing_detr_fast.masks_to_boxes\n def masks_to_boxes(masks: torch.Tensor) -> torch.Tensor:\n     \"\"\"\n     Compute the bounding boxes around the provided panoptic segmentation masks.\n@@ -215,7 +198,9 @@ def masks_to_boxes(masks: torch.Tensor) -> torch.Tensor:\n     return torch.stack([x_min, y_min, x_max, y_max], 1)\n \n \n-# Copied from transformers.models.detr.image_processing_detr_fast.rgb_to_id\n+# 2 functions below adapted from https://github.com/cocodataset/panopticapi/blob/master/panopticapi/utils.py\n+# Copyright (c) 2018, Alexander Kirillov\n+# All rights reserved.\n def rgb_to_id(color):\n     \"\"\"\n     Converts RGB color to unique ID.\n@@ -227,7 +212,6 @@ def rgb_to_id(color):\n     return int(color[0] + 256 * color[1] + 256 * 256 * color[2])\n \n \n-# Copied from transformers.models.detr.image_processing_detr_fast.prepare_coco_panoptic_annotation with DETR->DeformableDetr\n def prepare_coco_panoptic_annotation(\n     image: torch.Tensor,\n     target: Dict,\n@@ -236,7 +220,7 @@ def prepare_coco_panoptic_annotation(\n     input_data_format: Union[ChannelDimension, str] = None,\n ) -> Dict:\n     \"\"\"\n-    Prepare a coco panoptic annotation for DeformableDetr.\n+    Prepare a coco panoptic annotation for DEFORMABLE_DETR.\n     \"\"\"\n     image_height, image_width = get_image_size(image, channel_dim=input_data_format)\n     annotation_path = pathlib.Path(masks_path) / target[\"file_name\"]\n@@ -279,13 +263,13 @@ def prepare_coco_panoptic_annotation(\n \n class DeformableDetrImageProcessorFast(BaseImageProcessorFast):\n     r\"\"\"\n-    Constructs a fast Deformable DETR image processor.\n+    Constructs a fast DeformableDetr image processor.\n \n     Args:\n         format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n             Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n         do_resize (`bool`, *optional*, defaults to `True`):\n-            Controls whether to resize the image's (height, width) dimensions to the specified `size`. Can be\n+            Controls whether to resize the image's `(height, width)` dimensions to the specified `size`. Can be\n             overridden by the `do_resize` parameter in the `preprocess` method.\n         size (`Dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 800, \"longest_edge\": 1333}`):\n             Size of the image's `(height, width)` dimensions after resizing. Can be overridden by the `size` parameter\n@@ -316,7 +300,7 @@ class DeformableDetrImageProcessorFast(BaseImageProcessorFast):\n             Standard deviation values to use when normalizing the image. Can be a single value or a list of values, one\n             for each channel. Can be overridden by the `image_std` parameter in the `preprocess` method.\n         do_convert_annotations (`bool`, *optional*, defaults to `True`):\n-            Controls whether to convert the annotations to the format expected by the DETR model. Converts the\n+            Controls whether to convert the annotations to the format expected by the DEFORMABLE_DETR model. Converts the\n             bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n             Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n         do_pad (`bool`, *optional*, defaults to `True`):\n@@ -332,7 +316,6 @@ class DeformableDetrImageProcessorFast(BaseImageProcessorFast):\n \n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n \n-    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast.__init__\n     def __init__(\n         self,\n         format: Union[str, AnnotationFormat] = AnnotationFormat.COCO_DETECTION,\n@@ -404,7 +387,6 @@ def __init__(\n         ]\n \n     @classmethod\n-    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast.from_dict with Detr->DeformableDetr\n     def from_dict(cls, image_processor_dict: Dict[str, Any], **kwargs):\n         \"\"\"\n         Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n@@ -418,7 +400,6 @@ def from_dict(cls, image_processor_dict: Dict[str, Any], **kwargs):\n             image_processor_dict[\"pad_and_return_pixel_mask\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n         return super().from_dict(image_processor_dict, **kwargs)\n \n-    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast.prepare_annotation with DETR->DeformableDetr\n     def prepare_annotation(\n         self,\n         image: torch.Tensor,\n@@ -429,7 +410,7 @@ def prepare_annotation(\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> Dict:\n         \"\"\"\n-        Prepare an annotation for feeding into DeformableDetr model.\n+        Prepare an annotation for feeding into DEFORMABLE_DETR model.\n         \"\"\"\n         format = format if format is not None else self.format\n \n@@ -451,7 +432,6 @@ def prepare_annotation(\n             raise ValueError(f\"Format {format} is not supported.\")\n         return target\n \n-    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast.resize\n     def resize(\n         self,\n         image: torch.Tensor,\n@@ -506,7 +486,6 @@ def resize(\n         )\n         return image\n \n-    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast.resize_annotation\n     def resize_annotation(\n         self,\n         annotation: Dict[str, Any],\n@@ -560,7 +539,6 @@ def resize_annotation(\n \n         return new_annotation\n \n-    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast.normalize_annotation\n     def normalize_annotation(self, annotation: Dict, image_size: Tuple[int, int]) -> Dict:\n         image_height, image_width = image_size\n         norm_annotation = {}\n@@ -576,7 +554,6 @@ def normalize_annotation(self, annotation: Dict, image_size: Tuple[int, int]) ->\n                 norm_annotation[key] = value\n         return norm_annotation\n \n-    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast._update_annotation_for_padded_image\n     def _update_annotation_for_padded_image(\n         self,\n         annotation: Dict,\n@@ -612,7 +589,6 @@ def _update_annotation_for_padded_image(\n                 new_annotation[key] = value\n         return new_annotation\n \n-    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast.pad\n     def pad(\n         self,\n         image: torch.Tensor,\n@@ -644,7 +620,6 @@ def pad(\n         return image, pixel_mask, annotation\n \n     @functools.lru_cache(maxsize=1)\n-    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast._validate_input_arguments\n     def _validate_input_arguments(\n         self,\n         do_rescale: bool,\n@@ -673,7 +648,6 @@ def _validate_input_arguments(\n         if do_normalize and None in (image_mean, image_std):\n             raise ValueError(\"Image mean and standard deviation must be specified if do_normalize is True.\")\n \n-    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast.preprocess\n     def preprocess(\n         self,\n         images: ImageInput,\n@@ -874,7 +848,7 @@ def preprocess(\n         processed_annotations = []\n         pixel_masks = []  # Initialize pixel_masks here\n         for image, annotation in zip(images, annotations if annotations is not None else [None] * len(images)):\n-            # prepare (COCO annotations as a list of Dict -> DETR target as a single Dict per image)\n+            # prepare (COCO annotations as a list of Dict -> DEFORMABLE_DETR target as a single Dict per image)\n             if annotations is not None:\n                 annotation = self.prepare_annotation(\n                     image,\n@@ -950,7 +924,6 @@ def preprocess(\n             ]\n         return encoded_inputs\n \n-    # Copied from transformers.models.deformable_detr.image_processing_deformable_detr.DeformableDetrImageProcessor.post_process\n     def post_process(self, outputs, target_sizes):\n         \"\"\"\n         Converts the raw output of [`DeformableDetrForObjectDetection`] into final bounding boxes in (top_left_x,\n@@ -996,7 +969,6 @@ def post_process(self, outputs, target_sizes):\n \n         return results\n \n-    # Copied from transformers.models.deformable_detr.image_processing_deformable_detr.DeformableDetrImageProcessor.post_process_object_detection\n     def post_process_object_detection(\n         self, outputs, threshold: float = 0.5, target_sizes: Union[TensorType, List[Tuple]] = None, top_k: int = 100\n     ):"
        },
        {
            "sha": "5dfb6048c3431b151ae1cf47157abf7e917c536c",
            "filename": "src/transformers/models/deformable_detr/modular_deformable_detr.py",
            "status": "added",
            "additions": 144,
            "deletions": 0,
            "changes": 144,
            "blob_url": "https://github.com/huggingface/transformers/blob/651cfb400f29865b25e65c74505ddd60f2499a1f/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodular_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/651cfb400f29865b25e65c74505ddd60f2499a1f/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodular_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodular_deformable_detr.py?ref=651cfb400f29865b25e65c74505ddd60f2499a1f",
            "patch": "@@ -0,0 +1,144 @@\n+from typing import List, Tuple, Union\n+\n+from transformers.models.detr.image_processing_detr_fast import DetrImageProcessorFast\n+\n+from ...image_transforms import center_to_corners_format\n+from ...utils import (\n+    TensorType,\n+    is_torch_available,\n+    logging,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class DeformableDetrImageProcessorFast(DetrImageProcessorFast):\n+    def post_process(self, outputs, target_sizes):\n+        \"\"\"\n+        Converts the raw output of [`DeformableDetrForObjectDetection`] into final bounding boxes in (top_left_x,\n+        top_left_y, bottom_right_x, bottom_right_y) format. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`DeformableDetrObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n+                Tensor containing the size (height, width) of each image of the batch. For evaluation, this must be the\n+                original image size (before any data augmentation). For visualization, this should be the image size\n+                after data augment, but before padding.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n+            in the batch as predicted by the model.\n+        \"\"\"\n+        logger.warning_once(\n+            \"`post_process` is deprecated and will be removed in v5 of Transformers, please use\"\n+            \" `post_process_object_detection` instead, with `threshold=0.` for equivalent results.\",\n+        )\n+\n+        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n+\n+        if len(out_logits) != len(target_sizes):\n+            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n+        if target_sizes.shape[1] != 2:\n+            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n+\n+        prob = out_logits.sigmoid()\n+        topk_values, topk_indexes = torch.topk(prob.view(out_logits.shape[0], -1), 100, dim=1)\n+        scores = topk_values\n+        topk_boxes = torch.div(topk_indexes, out_logits.shape[2], rounding_mode=\"floor\")\n+        labels = topk_indexes % out_logits.shape[2]\n+        boxes = center_to_corners_format(out_bbox)\n+        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n+\n+        # and from relative [0, 1] to absolute [0, height] coordinates\n+        img_h, img_w = target_sizes.unbind(1)\n+        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n+        boxes = boxes * scale_fct[:, None, :]\n+\n+        results = [{\"scores\": s, \"labels\": l, \"boxes\": b} for s, l, b in zip(scores, labels, boxes)]\n+\n+        return results\n+\n+    def post_process_object_detection(\n+        self, outputs, threshold: float = 0.5, target_sizes: Union[TensorType, List[Tuple]] = None, top_k: int = 100\n+    ):\n+        \"\"\"\n+        Converts the raw output of [`DeformableDetrForObjectDetection`] into final bounding boxes in (top_left_x,\n+        top_left_y, bottom_right_x, bottom_right_y) format. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`DetrObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            threshold (`float`, *optional*):\n+                Score threshold to keep object detection predictions.\n+            target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n+                (height, width) of each image in the batch. If left to None, predictions will not be resized.\n+            top_k (`int`, *optional*, defaults to 100):\n+                Keep only top k bounding boxes before filtering by thresholding.\n+\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n+            in the batch as predicted by the model.\n+        \"\"\"\n+        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n+\n+        if target_sizes is not None:\n+            if len(out_logits) != len(target_sizes):\n+                raise ValueError(\n+                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n+                )\n+\n+        prob = out_logits.sigmoid()\n+        prob = prob.view(out_logits.shape[0], -1)\n+        k_value = min(top_k, prob.size(1))\n+        topk_values, topk_indexes = torch.topk(prob, k_value, dim=1)\n+        scores = topk_values\n+        topk_boxes = torch.div(topk_indexes, out_logits.shape[2], rounding_mode=\"floor\")\n+        labels = topk_indexes % out_logits.shape[2]\n+        boxes = center_to_corners_format(out_bbox)\n+        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n+\n+        # and from relative [0, 1] to absolute [0, height] coordinates\n+        if target_sizes is not None:\n+            if isinstance(target_sizes, List):\n+                img_h = torch.Tensor([i[0] for i in target_sizes])\n+                img_w = torch.Tensor([i[1] for i in target_sizes])\n+            else:\n+                img_h, img_w = target_sizes.unbind(1)\n+            scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n+            boxes = boxes * scale_fct[:, None, :]\n+\n+        results = []\n+        for s, l, b in zip(scores, labels, boxes):\n+            score = s[s > threshold]\n+            label = l[s > threshold]\n+            box = b[s > threshold]\n+            results.append({\"scores\": score, \"labels\": label, \"boxes\": box})\n+\n+        return results\n+\n+    def post_process_segmentation():\n+        raise NotImplementedError(\"Segmentation post-processing is not implemented for Deformable DETR yet.\")\n+\n+    def post_process_instance():\n+        raise NotImplementedError(\"Instance post-processing is not implemented for Deformable DETR yet.\")\n+\n+    def post_process_panoptic():\n+        raise NotImplementedError(\"Panoptic post-processing is not implemented for Deformable DETR yet.\")\n+\n+    def post_process_instance_segmentation():\n+        raise NotImplementedError(\"Segmentation post-processing is not implemented for Deformable DETR yet.\")\n+\n+    def post_process_semantic_segmentation():\n+        raise NotImplementedError(\"Semantic segmentation post-processing is not implemented for Deformable DETR yet.\")\n+\n+    def post_process_panoptic_segmentation():\n+        raise NotImplementedError(\"Panoptic segmentation post-processing is not implemented for Deformable DETR yet.\")\n+\n+\n+__all__ = [\"DeformableDetrImageProcessorFast\"]"
        },
        {
            "sha": "7cf567c99a27c3649182b236938ad284319329a0",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/651cfb400f29865b25e65c74505ddd60f2499a1f/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/651cfb400f29865b25e65c74505ddd60f2499a1f/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=651cfb400f29865b25e65c74505ddd60f2499a1f",
            "patch": "@@ -72,17 +72,15 @@\n if is_vision_available():\n     import PIL\n \n+    from ...image_utils import pil_torch_interpolation_mapping\n \n-if is_torchvision_available():\n-    from torchvision.io import read_image\n-\n-    if is_vision_available():\n-        from ...image_utils import pil_torch_interpolation_mapping\n \n-    if is_torchvision_v2_available():\n-        from torchvision.transforms.v2 import functional as F\n-    else:\n-        from torchvision.transforms import functional as F\n+if is_torchvision_v2_available():\n+    from torchvision.io import read_image\n+    from torchvision.transforms.v2 import functional as F\n+elif is_torchvision_available():\n+    from torchvision.io import read_image\n+    from torchvision.transforms import functional as F\n \n \n logger = logging.get_logger(__name__)"
        },
        {
            "sha": "70ce29a211d224fc80288d1fbf5d7acc1707ef57",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr_fast.py",
            "status": "modified",
            "additions": 19,
            "deletions": 44,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/651cfb400f29865b25e65c74505ddd60f2499a1f/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/651cfb400f29865b25e65c74505ddd60f2499a1f/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py?ref=651cfb400f29865b25e65c74505ddd60f2499a1f",
            "patch": "@@ -1,19 +1,9 @@\n-# coding=utf-8\n-# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Fast Image processor class for RT-DETR.\"\"\"\n-\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/rt_detr/modular_rt_detr.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_rt_detr.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n import functools\n import pathlib\n from typing import Any, Dict, List, Optional, Tuple, Union\n@@ -26,10 +16,7 @@\n     get_max_height_width,\n     safe_squeeze,\n )\n-from ...image_transforms import (\n-    center_to_corners_format,\n-    corners_to_center_format,\n-)\n+from ...image_transforms import center_to_corners_format, corners_to_center_format\n from ...image_utils import (\n     IMAGENET_DEFAULT_MEAN,\n     IMAGENET_DEFAULT_STD,\n@@ -51,30 +38,25 @@\n     is_torch_available,\n     is_torchvision_available,\n     is_torchvision_v2_available,\n-    logging,\n+    is_vision_available,\n     requires_backends,\n )\n-from .image_processing_rt_detr import (\n-    get_size_with_aspect_ratio,\n-)\n+from .image_processing_rt_detr import get_size_with_aspect_ratio\n \n \n if is_torch_available():\n     import torch\n \n-\n-if is_torchvision_available():\n+if is_vision_available():\n     from ...image_utils import pil_torch_interpolation_mapping\n \n-    if is_torchvision_v2_available():\n-        from torchvision.transforms.v2 import functional as F\n-    else:\n-        from torchvision.transforms import functional as F\n-\n \n-logger = logging.get_logger(__name__)\n+if is_torchvision_v2_available():\n+    from torchvision.transforms.v2 import functional as F\n+elif is_torchvision_available():\n+    from torchvision.transforms import functional as F\n \n-SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION,)\n+SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n \n \n def prepare_coco_detection_annotation(\n@@ -138,13 +120,13 @@ def prepare_coco_detection_annotation(\n \n class RTDetrImageProcessorFast(BaseImageProcessorFast):\n     r\"\"\"\n-    Constructs a fast RT-DETR DETR image processor.\n+    Constructs a fast RTDetr image processor.\n \n     Args:\n         format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n             Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n         do_resize (`bool`, *optional*, defaults to `True`):\n-            Controls whether to resize the image's (height, width) dimensions to the specified `size`. Can be\n+            Controls whether to resize the image's `(height, width)` dimensions to the specified `size`. Can be\n             overridden by the `do_resize` parameter in the `preprocess` method.\n         size (`Dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 800, \"longest_edge\": 1333}`):\n             Size of the image's `(height, width)` dimensions after resizing. Can be overridden by the `size` parameter\n@@ -175,7 +157,7 @@ class RTDetrImageProcessorFast(BaseImageProcessorFast):\n             Standard deviation values to use when normalizing the image. Can be a single value or a list of values, one\n             for each channel. Can be overridden by the `image_std` parameter in the `preprocess` method.\n         do_convert_annotations (`bool`, *optional*, defaults to `True`):\n-            Controls whether to convert the annotations to the format expected by the DETR model. Converts the\n+            Controls whether to convert the annotations to the format expected by the RT_DETR model. Converts the\n             bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n             Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n         do_pad (`bool`, *optional*, defaults to `False`):\n@@ -237,7 +219,7 @@ def prepare_annotation(\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> Dict:\n         \"\"\"\n-        Prepare an annotation for feeding into RTDETR model.\n+        Prepare an annotation for feeding into RT_DETR model.\n         \"\"\"\n         format = format if format is not None else self.format\n \n@@ -250,7 +232,6 @@ def prepare_annotation(\n             raise ValueError(f\"Format {format} is not supported.\")\n         return target\n \n-    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast.resize\n     def resize(\n         self,\n         image: torch.Tensor,\n@@ -305,7 +286,6 @@ def resize(\n         )\n         return image\n \n-    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast.resize_annotation\n     def resize_annotation(\n         self,\n         annotation: Dict[str, Any],\n@@ -359,7 +339,6 @@ def resize_annotation(\n \n         return new_annotation\n \n-    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast.normalize_annotation\n     def normalize_annotation(self, annotation: Dict, image_size: Tuple[int, int]) -> Dict:\n         image_height, image_width = image_size\n         norm_annotation = {}\n@@ -375,7 +354,6 @@ def normalize_annotation(self, annotation: Dict, image_size: Tuple[int, int]) ->\n                 norm_annotation[key] = value\n         return norm_annotation\n \n-    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast._update_annotation_for_padded_image\n     def _update_annotation_for_padded_image(\n         self,\n         annotation: Dict,\n@@ -411,7 +389,6 @@ def _update_annotation_for_padded_image(\n                 new_annotation[key] = value\n         return new_annotation\n \n-    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast.pad\n     def pad(\n         self,\n         image: torch.Tensor,\n@@ -443,7 +420,6 @@ def pad(\n         return image, pixel_mask, annotation\n \n     @functools.lru_cache(maxsize=1)\n-    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast._validate_input_arguments\n     def _validate_input_arguments(\n         self,\n         do_rescale: bool,\n@@ -726,7 +702,6 @@ def preprocess(\n             ]\n         return encoded_inputs\n \n-    # Copied from transformers.models.rt_detr.image_processing_rt_detr.RTDetrImageProcessor.post_process_object_detection\n     def post_process_object_detection(\n         self,\n         outputs,"
        },
        {
            "sha": "59f2ea566e974761d9f694c667f0b5406cc4a6c5",
            "filename": "src/transformers/models/rt_detr/modular_rt_detr.py",
            "status": "added",
            "additions": 577,
            "deletions": 0,
            "changes": 577,
            "blob_url": "https://github.com/huggingface/transformers/blob/651cfb400f29865b25e65c74505ddd60f2499a1f/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/651cfb400f29865b25e65c74505ddd60f2499a1f/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py?ref=651cfb400f29865b25e65c74505ddd60f2499a1f",
            "patch": "@@ -0,0 +1,577 @@\n+import pathlib\n+from typing import Dict, List, Optional, Tuple, Union\n+\n+from transformers.models.detr.image_processing_detr_fast import DetrImageProcessorFast\n+\n+from ...image_processing_utils import BatchFeature, get_size_dict\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    SizeDict,\n+    get_max_height_width,\n+)\n+from ...image_transforms import center_to_corners_format\n+from ...image_utils import (\n+    IMAGENET_DEFAULT_MEAN,\n+    IMAGENET_DEFAULT_STD,\n+    AnnotationFormat,\n+    AnnotationType,\n+    ChannelDimension,\n+    ImageInput,\n+    ImageType,\n+    PILImageResampling,\n+    get_image_size,\n+    get_image_type,\n+    infer_channel_dimension_format,\n+    make_list_of_images,\n+    validate_annotations,\n+)\n+from ...utils import (\n+    TensorType,\n+    filter_out_non_signature_kwargs,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    is_vision_available,\n+    logging,\n+    requires_backends,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_vision_available():\n+    from ...image_utils import pil_torch_interpolation_mapping\n+\n+\n+if is_torchvision_v2_available():\n+    from torchvision.transforms.v2 import functional as F\n+elif is_torchvision_available():\n+    from torchvision.transforms import functional as F\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION,)\n+\n+\n+def prepare_coco_detection_annotation(\n+    image,\n+    target,\n+    return_segmentation_masks: bool = False,\n+    input_data_format: Optional[Union[ChannelDimension, str]] = None,\n+):\n+    \"\"\"\n+    Convert the target in COCO format into the format expected by RT-DETR.\n+    \"\"\"\n+    image_height, image_width = image.size()[-2:]\n+\n+    image_id = target[\"image_id\"]\n+    image_id = torch.as_tensor([image_id], dtype=torch.int64, device=image.device)\n+\n+    # Get all COCO annotations for the given image.\n+    annotations = target[\"annotations\"]\n+    classes = []\n+    area = []\n+    boxes = []\n+    keypoints = []\n+    for obj in annotations:\n+        if \"iscrowd\" not in obj or obj[\"iscrowd\"] == 0:\n+            classes.append(obj[\"category_id\"])\n+            area.append(obj[\"area\"])\n+            boxes.append(obj[\"bbox\"])\n+            if \"keypoints\" in obj:\n+                keypoints.append(obj[\"keypoints\"])\n+\n+    classes = torch.as_tensor(classes, dtype=torch.int64, device=image.device)\n+    area = torch.as_tensor(area, dtype=torch.float32, device=image.device)\n+    iscrowd = torch.zeros_like(classes, dtype=torch.int64, device=image.device)\n+    # guard against no boxes via resizing\n+    boxes = torch.as_tensor(boxes, dtype=torch.float32, device=image.device).reshape(-1, 4)\n+    boxes[:, 2:] += boxes[:, :2]\n+    boxes[:, 0::2] = boxes[:, 0::2].clip(min=0, max=image_width)\n+    boxes[:, 1::2] = boxes[:, 1::2].clip(min=0, max=image_height)\n+\n+    keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n+\n+    new_target = {\n+        \"image_id\": image_id,\n+        \"class_labels\": classes[keep],\n+        \"boxes\": boxes[keep],\n+        \"area\": area[keep],\n+        \"iscrowd\": iscrowd[keep],\n+        \"orig_size\": torch.as_tensor([int(image_height), int(image_width)], dtype=torch.int64, device=image.device),\n+    }\n+\n+    if keypoints:\n+        keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=image.device)\n+        # Apply the keep mask here to filter the relevant annotations\n+        keypoints = keypoints[keep]\n+        num_keypoints = keypoints.shape[0]\n+        keypoints = keypoints.reshape((-1, 3)) if num_keypoints else keypoints\n+        new_target[\"keypoints\"] = keypoints\n+\n+    return new_target\n+\n+\n+class RTDetrImageProcessorFast(DetrImageProcessorFast, BaseImageProcessorFast):\n+    r\"\"\"\n+    Constructs a fast RTDetr image processor.\n+\n+    Args:\n+        format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n+            Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Controls whether to resize the image's `(height, width)` dimensions to the specified `size`. Can be\n+            overridden by the `do_resize` parameter in the `preprocess` method.\n+        size (`Dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 800, \"longest_edge\": 1333}`):\n+            Size of the image's `(height, width)` dimensions after resizing. Can be overridden by the `size` parameter\n+            in the `preprocess` method. Available options are:\n+                - `{\"height\": int, \"width\": int}`: The image will be resized to the exact size `(height, width)`.\n+                    Do NOT keep the aspect ratio.\n+                - `{\"shortest_edge\": int, \"longest_edge\": int}`: The image will be resized to a maximum size respecting\n+                    the aspect ratio and keeping the shortest edge less or equal to `shortest_edge` and the longest edge\n+                    less or equal to `longest_edge`.\n+                - `{\"max_height\": int, \"max_width\": int}`: The image will be resized to the maximum size respecting the\n+                    aspect ratio and keeping the height less or equal to `max_height` and the width less or equal to\n+                    `max_width`.\n+        resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BILINEAR`):\n+            Resampling filter to use if resizing the image.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Controls whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by the\n+            `do_rescale` parameter in the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Can be overridden by the `rescale_factor` parameter in the\n+            `preprocess` method.\n+        do_normalize (`bool`, *optional*, defaults to `False`):\n+            Controls whether to normalize the image. Can be overridden by the `do_normalize` parameter in the\n+            `preprocess` method.\n+        image_mean (`float` or `List[float]`, *optional*, defaults to `IMAGENET_DEFAULT_MEAN`):\n+            Mean values to use when normalizing the image. Can be a single value or a list of values, one for each\n+            channel. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n+        image_std (`float` or `List[float]`, *optional*, defaults to `IMAGENET_DEFAULT_STD`):\n+            Standard deviation values to use when normalizing the image. Can be a single value or a list of values, one\n+            for each channel. Can be overridden by the `image_std` parameter in the `preprocess` method.\n+        do_convert_annotations (`bool`, *optional*, defaults to `True`):\n+            Controls whether to convert the annotations to the format expected by the RT_DETR model. Converts the\n+            bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n+            Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n+        do_pad (`bool`, *optional*, defaults to `False`):\n+            Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n+            method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n+            If `pad_size` is provided, the image will be padded to the specified dimensions.\n+            Otherwise, the image will be padded to the maximum height and width of the batch.\n+        pad_size (`Dict[str, int]`, *optional*):\n+            The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+            provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+            height and width in the batch.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        format: Union[str, AnnotationFormat] = AnnotationFormat.COCO_DETECTION,\n+        do_resize: bool = True,\n+        size: Dict[str, int] = None,\n+        resample: Union[PILImageResampling, \"F.InterpolationMode\"] = PILImageResampling.BILINEAR,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n+        do_normalize: bool = False,\n+        image_mean: Union[float, List[float]] = None,\n+        image_std: Union[float, List[float]] = None,\n+        do_convert_annotations: bool = True,\n+        do_pad: bool = False,\n+        pad_size: Optional[Dict[str, int]] = None,\n+        **kwargs,\n+    ) -> None:\n+        size = size if size is not None else {\"height\": 640, \"width\": 640}\n+        size = get_size_dict(size, default_to_square=False)\n+\n+        if do_convert_annotations is None:\n+            do_convert_annotations = do_normalize\n+\n+        BaseImageProcessorFast.__init__(**kwargs)\n+        self.format = format\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.resample = resample\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.do_convert_annotations = do_convert_annotations\n+        self.image_mean = image_mean if image_mean is not None else IMAGENET_DEFAULT_MEAN\n+        self.image_std = image_std if image_std is not None else IMAGENET_DEFAULT_STD\n+        self.do_pad = do_pad\n+        self.pad_size = pad_size\n+\n+    def prepare_annotation(\n+        self,\n+        image: torch.Tensor,\n+        target: Dict,\n+        format: Optional[AnnotationFormat] = None,\n+        return_segmentation_masks: bool = None,\n+        masks_path: Optional[Union[str, pathlib.Path]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> Dict:\n+        format = format if format is not None else self.format\n+\n+        if format == AnnotationFormat.COCO_DETECTION:\n+            return_segmentation_masks = False if return_segmentation_masks is None else return_segmentation_masks\n+            target = prepare_coco_detection_annotation(\n+                image, target, return_segmentation_masks, input_data_format=input_data_format\n+            )\n+        else:\n+            raise ValueError(f\"Format {format} is not supported.\")\n+        return target\n+\n+    @filter_out_non_signature_kwargs(extra=[\"device\"])\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        annotations: Optional[Union[AnnotationType, List[AnnotationType]]] = None,\n+        return_segmentation_masks: bool = None,\n+        masks_path: Optional[Union[str, pathlib.Path]] = None,\n+        do_resize: Optional[bool] = None,\n+        size: Optional[Dict[str, int]] = None,\n+        resample: Optional[Union[PILImageResampling, \"F.InterpolationMode\"]] = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[Union[int, float]] = None,\n+        do_normalize: Optional[bool] = None,\n+        do_convert_annotations: Optional[bool] = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_pad: Optional[bool] = None,\n+        format: Optional[Union[str, AnnotationFormat]] = None,\n+        return_tensors: Optional[Union[TensorType, str]] = None,\n+        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        pad_size: Optional[Dict[str, int]] = None,\n+        **kwargs,\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess an image or a batch of images so that it can be used by the model.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image or batch of images to preprocess. Expects a single or batch of images with pixel values ranging\n+                from 0 to 255. If passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            annotations (`AnnotationType` or `List[AnnotationType]`, *optional*):\n+                List of annotations associated with the image or batch of images. If annotation is for object\n+                detection, the annotations should be a dictionary with the following keys:\n+                - \"image_id\" (`int`): The image id.\n+                - \"annotations\" (`List[Dict]`): List of annotations for an image. Each annotation should be a\n+                  dictionary. An image can have no annotations, in which case the list should be empty.\n+                If annotation is for segmentation, the annotations should be a dictionary with the following keys:\n+                - \"image_id\" (`int`): The image id.\n+                - \"segments_info\" (`List[Dict]`): List of segments for an image. Each segment should be a dictionary.\n+                  An image can have no segments, in which case the list should be empty.\n+                - \"file_name\" (`str`): The file name of the image.\n+            return_segmentation_masks (`bool`, *optional*, defaults to self.return_segmentation_masks):\n+                Whether to return segmentation masks.\n+            masks_path (`str` or `pathlib.Path`, *optional*):\n+                Path to the directory containing the segmentation masks.\n+            do_resize (`bool`, *optional*, defaults to self.do_resize):\n+                Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to self.size):\n+                Size of the image's `(height, width)` dimensions after resizing. Available options are:\n+                    - `{\"height\": int, \"width\": int}`: The image will be resized to the exact size `(height, width)`.\n+                        Do NOT keep the aspect ratio.\n+                    - `{\"shortest_edge\": int, \"longest_edge\": int}`: The image will be resized to a maximum size respecting\n+                        the aspect ratio and keeping the shortest edge less or equal to `shortest_edge` and the longest edge\n+                        less or equal to `longest_edge`.\n+                    - `{\"max_height\": int, \"max_width\": int}`: The image will be resized to the maximum size respecting the\n+                        aspect ratio and keeping the height less or equal to `max_height` and the width less or equal to\n+                        `max_width`.\n+            resample (`PILImageResampling` or `InterpolationMode`, *optional*, defaults to self.resample):\n+                Resampling filter to use when resizing the image.\n+            do_rescale (`bool`, *optional*, defaults to self.do_rescale):\n+                Whether to rescale the image.\n+            rescale_factor (`float`, *optional*, defaults to self.rescale_factor):\n+                Rescale factor to use when rescaling the image.\n+            do_normalize (`bool`, *optional*, defaults to self.do_normalize):\n+                Whether to normalize the image.\n+            do_convert_annotations (`bool`, *optional*, defaults to self.do_convert_annotations):\n+                Whether to convert the annotations to the format expected by the model. Converts the bounding\n+                boxes from the format `(top_left_x, top_left_y, width, height)` to `(center_x, center_y, width, height)`\n+                and in relative coordinates.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to self.image_mean):\n+                Mean to use when normalizing the image.\n+            image_std (`float` or `List[float]`, *optional*, defaults to self.image_std):\n+                Standard deviation to use when normalizing the image.\n+            do_pad (`bool`, *optional*, defaults to self.do_pad):\n+                Whether to pad the image. If `True`, padding will be applied to the bottom and right of\n+                the image with zeros. If `pad_size` is provided, the image will be padded to the specified\n+                dimensions. Otherwise, the image will be padded to the maximum height and width of the batch.\n+            format (`str` or `AnnotationFormat`, *optional*, defaults to self.format):\n+                Format of the annotations.\n+            return_tensors (`str` or `TensorType`, *optional*, defaults to self.return_tensors):\n+                Type of tensors to return. If `None`, will return the list of images.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+            pad_size (`Dict[str, int]`, *optional*):\n+                The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+                provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+                height and width in the batch.\n+        \"\"\"\n+        do_resize = self.do_resize if do_resize is None else do_resize\n+        size = self.size if size is None else size\n+        size = get_size_dict(size=size, default_to_square=True)\n+        resample = self.resample if resample is None else resample\n+        do_rescale = self.do_rescale if do_rescale is None else do_rescale\n+        rescale_factor = self.rescale_factor if rescale_factor is None else rescale_factor\n+        do_normalize = self.do_normalize if do_normalize is None else do_normalize\n+        image_mean = self.image_mean if image_mean is None else image_mean\n+        image_std = self.image_std if image_std is None else image_std\n+        do_convert_annotations = (\n+            self.do_convert_annotations if do_convert_annotations is None else do_convert_annotations\n+        )\n+        do_pad = self.do_pad if do_pad is None else do_pad\n+        pad_size = self.pad_size if pad_size is None else pad_size\n+        format = self.format if format is None else format\n+        return_tensors = \"pt\" if return_tensors is None else return_tensors\n+        device = kwargs.pop(\"device\", None)\n+\n+        # Make hashable for cache\n+        size = SizeDict(**size)\n+        image_mean = tuple(image_mean) if isinstance(image_mean, list) else image_mean\n+        image_std = tuple(image_std) if isinstance(image_std, list) else image_std\n+\n+        images = make_list_of_images(images)\n+        image_type = get_image_type(images[0])\n+\n+        if image_type not in [ImageType.PIL, ImageType.TORCH, ImageType.NUMPY]:\n+            raise ValueError(f\"Unsupported input image type {image_type}\")\n+\n+        self._validate_input_arguments(\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+            return_tensors=return_tensors,\n+            data_format=data_format,\n+        )\n+\n+        if annotations is not None and isinstance(annotations, dict):\n+            annotations = [annotations]\n+\n+        if annotations is not None and len(images) != len(annotations):\n+            raise ValueError(\n+                f\"The number of images ({len(images)}) and annotations ({len(annotations)}) do not match.\"\n+            )\n+\n+        format = AnnotationFormat(format)\n+        if annotations is not None:\n+            validate_annotations(format, SUPPORTED_ANNOTATION_FORMATS, annotations)\n+\n+        data = {}\n+        if image_type == ImageType.PIL:\n+            images = [F.pil_to_tensor(image) for image in images]\n+        elif image_type == ImageType.NUMPY:\n+            # not using F.to_tensor as it doesn't handle (C, H, W) numpy arrays\n+            images = [torch.from_numpy(image).contiguous() for image in images]\n+\n+        if device is not None:\n+            images = [image.to(device) for image in images]\n+\n+        # We assume that all images have the same channel dimension format.\n+        if input_data_format is None:\n+            input_data_format = infer_channel_dimension_format(images[0])\n+        if input_data_format == ChannelDimension.LAST:\n+            images = [image.permute(2, 0, 1).contiguous() for image in images]\n+            input_data_format = ChannelDimension.FIRST\n+\n+        if do_rescale and do_normalize:\n+            # fused rescale and normalize\n+            new_mean = torch.tensor(image_mean, device=images[0].device) * (1.0 / rescale_factor)\n+            new_std = torch.tensor(image_std, device=images[0].device) * (1.0 / rescale_factor)\n+\n+        processed_images = []\n+        processed_annotations = []\n+        pixel_masks = []  # Initialize pixel_masks here\n+        for image, annotation in zip(images, annotations if annotations is not None else [None] * len(images)):\n+            # prepare (COCO annotations as a list of Dict -> DETR target as a single Dict per image)\n+            if annotations is not None:\n+                annotation = self.prepare_annotation(\n+                    image,\n+                    annotation,\n+                    format,\n+                    return_segmentation_masks=return_segmentation_masks,\n+                    masks_path=masks_path,\n+                    input_data_format=input_data_format,\n+                )\n+\n+            if do_resize:\n+                interpolation = (\n+                    pil_torch_interpolation_mapping[resample]\n+                    if isinstance(resample, (PILImageResampling, int))\n+                    else resample\n+                )\n+                resized_image = self.resize(image, size=size, interpolation=interpolation)\n+                if annotations is not None:\n+                    annotation = self.resize_annotation(\n+                        annotation,\n+                        orig_size=image.size()[-2:],\n+                        target_size=resized_image.size()[-2:],\n+                    )\n+                image = resized_image\n+\n+            if do_rescale and do_normalize:\n+                # fused rescale and normalize\n+                image = F.normalize(image.to(dtype=torch.float32), new_mean, new_std)\n+            elif do_rescale:\n+                image = image * rescale_factor\n+            elif do_normalize:\n+                image = F.normalize(image, image_mean, image_std)\n+\n+            if do_convert_annotations and annotations is not None:\n+                annotation = self.normalize_annotation(annotation, get_image_size(image, input_data_format))\n+\n+            processed_images.append(image)\n+            processed_annotations.append(annotation)\n+        images = processed_images\n+        annotations = processed_annotations if annotations is not None else None\n+\n+        if do_pad:\n+            # depends on all resized image shapes so we need another loop\n+            if pad_size is not None:\n+                padded_size = (pad_size[\"height\"], pad_size[\"width\"])\n+            else:\n+                padded_size = get_max_height_width(images)\n+\n+            padded_images = []\n+            padded_annotations = []\n+            for image, annotation in zip(images, annotations if annotations is not None else [None] * len(images)):\n+                # Pads images and returns their mask: {'pixel_values': ..., 'pixel_mask': ...}\n+                if padded_size == image.size()[-2:]:\n+                    padded_images.append(image)\n+                    pixel_masks.append(torch.ones(padded_size, dtype=torch.int64, device=image.device))\n+                    padded_annotations.append(annotation)\n+                    continue\n+                image, pixel_mask, annotation = self.pad(\n+                    image, padded_size, annotation=annotation, update_bboxes=do_convert_annotations\n+                )\n+                padded_images.append(image)\n+                padded_annotations.append(annotation)\n+                pixel_masks.append(pixel_mask)\n+            images = padded_images\n+            annotations = padded_annotations if annotations is not None else None\n+            data.update({\"pixel_mask\": torch.stack(pixel_masks, dim=0)})\n+\n+        data.update({\"pixel_values\": torch.stack(images, dim=0)})\n+        encoded_inputs = BatchFeature(data, tensor_type=return_tensors)\n+        if annotations is not None:\n+            encoded_inputs[\"labels\"] = [\n+                BatchFeature(annotation, tensor_type=return_tensors) for annotation in annotations\n+            ]\n+        return encoded_inputs\n+\n+    def post_process_object_detection(\n+        self,\n+        outputs,\n+        threshold: float = 0.5,\n+        target_sizes: Union[TensorType, List[Tuple]] = None,\n+        use_focal_loss: bool = True,\n+    ):\n+        \"\"\"\n+        Converts the raw output of [`DetrForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n+        bottom_right_x, bottom_right_y) format. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`DetrObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            threshold (`float`, *optional*, defaults to 0.5):\n+                Score threshold to keep object detection predictions.\n+            target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n+                `(height, width)` of each image in the batch. If unset, predictions will not be resized.\n+            use_focal_loss (`bool` defaults to `True`):\n+                Variable informing if the focal loss was used to predict the outputs. If `True`, a sigmoid is applied\n+                to compute the scores of each detection, otherwise, a softmax function is used.\n+\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n+            in the batch as predicted by the model.\n+        \"\"\"\n+        requires_backends(self, [\"torch\"])\n+        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n+        # convert from relative cxcywh to absolute xyxy\n+        boxes = center_to_corners_format(out_bbox)\n+        if target_sizes is not None:\n+            if len(out_logits) != len(target_sizes):\n+                raise ValueError(\n+                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n+                )\n+            if isinstance(target_sizes, List):\n+                img_h, img_w = torch.as_tensor(target_sizes).unbind(1)\n+            else:\n+                img_h, img_w = target_sizes.unbind(1)\n+            scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n+            boxes = boxes * scale_fct[:, None, :]\n+\n+        num_top_queries = out_logits.shape[1]\n+        num_classes = out_logits.shape[2]\n+\n+        if use_focal_loss:\n+            scores = torch.nn.functional.sigmoid(out_logits)\n+            scores, index = torch.topk(scores.flatten(1), num_top_queries, axis=-1)\n+            labels = index % num_classes\n+            index = index // num_classes\n+            boxes = boxes.gather(dim=1, index=index.unsqueeze(-1).repeat(1, 1, boxes.shape[-1]))\n+        else:\n+            scores = torch.nn.functional.softmax(out_logits)[:, :, :-1]\n+            scores, labels = scores.max(dim=-1)\n+            if scores.shape[1] > num_top_queries:\n+                scores, index = torch.topk(scores, num_top_queries, dim=-1)\n+                labels = torch.gather(labels, dim=1, index=index)\n+                boxes = torch.gather(boxes, dim=1, index=index.unsqueeze(-1).tile(1, 1, boxes.shape[-1]))\n+\n+        results = []\n+        for score, label, box in zip(scores, labels, boxes):\n+            results.append(\n+                {\n+                    \"scores\": score[score > threshold],\n+                    \"labels\": label[score > threshold],\n+                    \"boxes\": box[score > threshold],\n+                }\n+            )\n+\n+        return results\n+\n+    def from_dict():\n+        raise NotImplementedError(\"No need to override this method for RT-DETR yet.\")\n+\n+    def post_process():\n+        raise NotImplementedError(\"Post-processing is not implemented for RT-DETR yet.\")\n+\n+    def post_process_segmentation():\n+        raise NotImplementedError(\"Segmentation post-processing is not implemented for RT-DETR yet.\")\n+\n+    def post_process_instance():\n+        raise NotImplementedError(\"Instance post-processing is not implemented for RT-DETR yet.\")\n+\n+    def post_process_panoptic():\n+        raise NotImplementedError(\"Panoptic post-processing is not implemented for RT-DETR yet.\")\n+\n+    def post_process_instance_segmentation():\n+        raise NotImplementedError(\"Segmentation post-processing is not implemented for RT-DETR yet.\")\n+\n+    def post_process_semantic_segmentation():\n+        raise NotImplementedError(\"Semantic segmentation post-processing is not implemented for RT-DETR yet.\")\n+\n+    def post_process_panoptic_segmentation():\n+        raise NotImplementedError(\"Panoptic segmentation post-processing is not implemented for RT-DETR yet.\")\n+\n+\n+__all__ = [\"RTDetrImageProcessorFast\"]"
        },
        {
            "sha": "5946d6ef1687d7b23f670febc2b5fee6aab4c43c",
            "filename": "utils/check_modular_conversion.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/651cfb400f29865b25e65c74505ddd60f2499a1f/utils%2Fcheck_modular_conversion.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/651cfb400f29865b25e65c74505ddd60f2499a1f/utils%2Fcheck_modular_conversion.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_modular_conversion.py?ref=651cfb400f29865b25e65c74505ddd60f2499a1f",
            "patch": "@@ -18,7 +18,9 @@\n \n \n def process_file(modular_file_path, generated_modeling_content, file_type=\"modeling_\", fix_and_overwrite=False):\n-    file_path = modular_file_path.replace(\"modular_\", f\"{file_type}_\")\n+    file_name_prefix = file_type.split(\"*\")[0]\n+    file_name_suffix = file_type.split(\"*\")[-1] if \"*\" in file_type else \"\"\n+    file_path = modular_file_path.replace(\"modular_\", f\"{file_name_prefix}_\").replace(\".py\", f\"{file_name_suffix}.py\")\n     # Read the actual modeling file\n     with open(file_path, \"r\") as modeling_file:\n         content = modeling_file.read()"
        },
        {
            "sha": "4a9955c976fbfc755431ee4b23c0ca4def301e7a",
            "filename": "utils/create_dependency_mapping.py",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/651cfb400f29865b25e65c74505ddd60f2499a1f/utils%2Fcreate_dependency_mapping.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/651cfb400f29865b25e65c74505ddd60f2499a1f/utils%2Fcreate_dependency_mapping.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcreate_dependency_mapping.py?ref=651cfb400f29865b25e65c74505ddd60f2499a1f",
            "patch": "@@ -7,10 +7,15 @@ def topological_sort(dependencies):\n     new_dependencies = {}\n     graph = defaultdict(list)\n     for node, deps in dependencies.items():\n+        node_name = node.split(\"/\")[-2]\n         for dep in deps:\n-            if \"example\" not in node and \"auto\" not in dep:\n-                graph[dep.split(\".\")[-2]].append(node.split(\"/\")[-2])\n-        new_dependencies[node.split(\"/\")[-2]] = node\n+            dep_name = dep.split(\".\")[-2]\n+            if dep_name == node_name:\n+                # Skip self dependencies for topological sort as they create cycles\n+                continue\n+            if \"example\" not in node and \"auto\" not in dep and node_name not in graph[dep_name]:\n+                graph[dep_name].append(node_name)\n+        new_dependencies[node_name] = node\n \n     # Create a graph and in-degree count for each node\n     def filter_one_by_one(filtered_list, reverse):\n@@ -54,7 +59,7 @@ def extract_classes_and_imports(file_path):\n     for node in ast.walk(tree):\n         if isinstance(node, (ast.Import, ast.ImportFrom)):\n             module = node.module if isinstance(node, ast.ImportFrom) else None\n-            if module and (\".modeling_\" in module):\n+            if module and (\".modeling_\" in module or \"transformers.models\" in module):\n                 imports.add(module)\n     return imports\n "
        },
        {
            "sha": "2f7512639f9a225b753723698e69a0b967f08ae6",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/651cfb400f29865b25e65c74505ddd60f2499a1f/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/651cfb400f29865b25e65c74505ddd60f2499a1f/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=651cfb400f29865b25e65c74505ddd60f2499a1f",
            "patch": "@@ -1059,6 +1059,7 @@ def replace_class_node(\n     \"Tokenizer\": \"tokenization\",\n     \"Processor\": \"processing\",\n     \"ImageProcessor\": \"image_processing\",\n+    \"ImageProcessorFast\": \"image_processing*_fast\",  # \"*\" indicates where to insert the model name before the \"_fast\" suffix\n     \"FeatureExtractor\": \"feature_extractor\",\n     \"ProcessorKwargs\": \"processing\",\n     \"ImagesKwargs\": \"processing\",\n@@ -1658,19 +1659,24 @@ def convert_modular_file(modular_file):\n \n def save_modeling_file(modular_file, converted_file):\n     for file_type in converted_file.keys():\n+        file_name_prefix = file_type.split(\"*\")[0]\n+        file_name_suffix = file_type.split(\"*\")[-1] if \"*\" in file_type else \"\"\n+        new_file_name = modular_file.replace(\"modular_\", f\"{file_name_prefix}_\").replace(\n+            \".py\", f\"{file_name_suffix}.py\"\n+        )\n         non_comment_lines = len(\n             [line for line in converted_file[file_type][0].strip().split(\"\\n\") if not line.strip().startswith(\"#\")]\n         )\n         if len(converted_file[file_type][0].strip()) > 0 and non_comment_lines > 0:\n-            with open(modular_file.replace(\"modular_\", f\"{file_type}_\"), \"w\", encoding=\"utf-8\") as f:\n+            with open(new_file_name, \"w\", encoding=\"utf-8\") as f:\n                 f.write(converted_file[file_type][0])\n         else:\n             non_comment_lines = len(\n                 [line for line in converted_file[file_type][0].strip().split(\"\\n\") if not line.strip().startswith(\"#\")]\n             )\n             if len(converted_file[file_type][1].strip()) > 0 and non_comment_lines > 0:\n                 logger.warning(\"The modeling code contains errors, it's written without formatting\")\n-                with open(modular_file.replace(\"modular_\", f\"{file_type}_\"), \"w\", encoding=\"utf-8\") as f:\n+                with open(new_file_name, \"w\", encoding=\"utf-8\") as f:\n                     f.write(converted_file[file_type][1])\n \n "
        }
    ],
    "stats": {
        "total": 911,
        "additions": 795,
        "deletions": 116
    }
}