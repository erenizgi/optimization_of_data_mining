{
    "author": "rohitthewanderer",
    "message": "[DOCS] : Improved mimi model card (#39824)\n\n* [DOCS] : Improved mimi model card\n\n* Removed additional header\n\n* Review: addressed feedback\n\n* Update mimi.md\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "3bafa128dc3d159be554b9ae19cbeb5114ea3113",
    "files": [
        {
            "sha": "4f3f4782127a502671ce85a61f1f4c2d4bb96ab8",
            "filename": "docs/source/en/model_doc/mimi.md",
            "status": "modified",
            "additions": 19,
            "deletions": 21,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bafa128dc3d159be554b9ae19cbeb5114ea3113/docs%2Fsource%2Fen%2Fmodel_doc%2Fmimi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bafa128dc3d159be554b9ae19cbeb5114ea3113/docs%2Fsource%2Fen%2Fmodel_doc%2Fmimi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmimi.md?ref=3bafa128dc3d159be554b9ae19cbeb5114ea3113",
            "patch": "@@ -14,30 +14,29 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Mimi\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n-<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n </div>\n \n-## Overview\n-\n-The Mimi model was proposed in [Moshi: a speech-text foundation model for real-time dialogue](https://kyutai.org/Moshi.pdf) by Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave and Neil Zeghidour. Mimi is a high-fidelity audio codec model developed by the Kyutai team, that combines semantic and acoustic information into audio tokens running at 12Hz and a bitrate of 1.1kbps. In other words, it can be used to map audio waveforms into “audio tokens”, known as “codebooks”.\n+# Mimi\n \n-The abstract from the paper is the following:\n+[Mimi](huggingface.co/papers/2410.00037) is a neural audio codec model with pretrained and quantized variants, designed for efficient speech representation and compression. The model operates at 1.1 kbps with a 12 Hz frame rate and uses a convolutional encoder-decoder architecture combined with a residual vector quantizer of 16 codebooks. Mimi outputs dual token streams i.e. semantic and acoustic to balance linguistic richness with high fidelity reconstruction. Key features include a causal streaming encoder for low-latency use, dual-path tokenization for flexible downstream generation, and integration readiness with large speech models like Moshi.\n \n-*We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning— such as emotion or non-speech sounds— is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this “Inner Monologue” method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at github.com/kyutai-labs/moshi.* \n+You can find the original Mimi checkpoints under the [Kyutai](https://huggingface.co/kyutai/models?search=mimi) organization.\n \n-Its architecture is based on [Encodec](./encodec) with several major differences:\n-* it uses a much lower frame-rate.\n-* it uses additional transformers for encoding and decoding for better latent contextualization\n-* it uses a different quantization scheme: one codebook is dedicated to semantic projection.\n+>[!TIP]\n+> This model was contributed by [ylacombe](https://huggingface.co/ylacombe).\n+>\n+> Click on the Mimi models in the right sidebar for more examples of how to apply Mimi.\n \n-## Usage example \n+The example below demonstrates how to encode and decode audio with the [`AutoModel`] class.\n \n-Here is a quick example of how to encode and decode an audio using this model:\n+<hfoptions id=\"usage\">\n+<hfoption id=\"AutoModel\">\n \n ```python \n >>> from datasets import load_dataset, Audio\n@@ -59,9 +58,8 @@ Here is a quick example of how to encode and decode an audio using this model:\n >>> audio_values = model(inputs[\"input_values\"], inputs[\"padding_mask\"]).audio_values\n ```\n \n-This model was contributed by [Yoach Lacombe (ylacombe)](https://huggingface.co/ylacombe).\n-The original code can be found [here](https://github.com/kyutai-labs/moshi).\n-\n+</hfoption>\n+</hfoptions>\n \n ## MimiConfig\n \n@@ -72,4 +70,4 @@ The original code can be found [here](https://github.com/kyutai-labs/moshi).\n [[autodoc]] MimiModel\n     - decode\n     - encode\n-    - forward\n\\ No newline at end of file\n+    - forward"
        }
    ],
    "stats": {
        "total": 40,
        "additions": 19,
        "deletions": 21
    }
}