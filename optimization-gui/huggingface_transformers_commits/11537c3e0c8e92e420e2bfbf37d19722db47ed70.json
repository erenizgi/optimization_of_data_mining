{
    "author": "gante",
    "message": "[trainer] handle case where EOS token is None in `generation_config` (#40127)\n\n* handle case where EOS token is None in gen config\n\n* update eli5 dataset",
    "sha": "11537c3e0c8e92e420e2bfbf37d19722db47ed70",
    "files": [
        {
            "sha": "f6e71fe117677f87612f148470584b1014a228bd",
            "filename": "docs/source/en/tasks/language_modeling.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/11537c3e0c8e92e420e2bfbf37d19722db47ed70/docs%2Fsource%2Fen%2Ftasks%2Flanguage_modeling.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/11537c3e0c8e92e420e2bfbf37d19722db47ed70/docs%2Fsource%2Fen%2Ftasks%2Flanguage_modeling.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Flanguage_modeling.md?ref=11537c3e0c8e92e420e2bfbf37d19722db47ed70",
            "patch": "@@ -29,7 +29,7 @@ the left. This means the model cannot see future tokens. GPT-2 is an example of\n \n This guide will show you how to:\n \n-1. Finetune [DistilGPT2](https://huggingface.co/distilbert/distilgpt2) on the [r/askscience](https://www.reddit.com/r/askscience/) subset of the [ELI5](https://huggingface.co/datasets/eli5) dataset.\n+1. Finetune [DistilGPT2](https://huggingface.co/distilbert/distilgpt2) on the [r/askscience](https://www.reddit.com/r/askscience/) subset of the [ELI5](https://huggingface.co/datasets/dany0407/eli5_category) dataset.\n 2. Use your finetuned model for inference.\n \n <Tip>\n@@ -54,12 +54,12 @@ We encourage you to log in to your Hugging Face account so you can upload and sh\n \n ## Load ELI5 dataset\n \n-Start by loading the first 5000 examples from the [ELI5-Category](https://huggingface.co/datasets/eli5_category) dataset with the ðŸ¤— Datasets library. This'll give you a chance to experiment and make sure everything works before spending more time training on the full dataset.\n+Start by loading the first 5000 examples from the [ELI5-Category](https://huggingface.co/datasets/dany0407/eli5_category) dataset with the ðŸ¤— Datasets library. This'll give you a chance to experiment and make sure everything works before spending more time training on the full dataset.\n \n ```py\n >>> from datasets import load_dataset\n \n->>> eli5 = load_dataset(\"eli5_category\", split=\"train[:5000]\")\n+>>> eli5 = load_dataset(\"dany0407/eli5_category\", split=\"train[:5000]\")\n ```\n \n Split the dataset's `train` split into a train and test set with the [`~datasets.Dataset.train_test_split`] method:"
        },
        {
            "sha": "1fa4baf9e74700d3c388abcfe807deb738b34ba8",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/11537c3e0c8e92e420e2bfbf37d19722db47ed70/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11537c3e0c8e92e420e2bfbf37d19722db47ed70/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=11537c3e0c8e92e420e2bfbf37d19722db47ed70",
            "patch": "@@ -943,7 +943,9 @@ def _align_special_tokens(self):\n             # The generation config may hold more than one EOS token. We preserve the original EOS tokens: any of the\n             # EOS tokens defined here will halt generation.\n             if model_has_generation_config:\n-                all_eos_tokens = [tokenizer.eos_token_id] + list(self.model.generation_config.eos_token_id)\n+                all_eos_tokens = [tokenizer.eos_token_id]\n+                if self.model.generation_config.eos_token_id is not None:\n+                    all_eos_tokens += list(self.model.generation_config.eos_token_id)\n                 self.model.generation_config.eos_token_id = [token for token in all_eos_tokens if token is not None]\n \n         # 2 - Align BOS\n@@ -971,8 +973,9 @@ def _align_special_tokens(self):\n         # 4 - Warn users about the changes\n         if len(updated_tokens) > 0:\n             logger.warning(\n-                \"The tokenizer has new special tokens that are also defined in the model configs. The model \"\n-                f\"configs were aligned accordingly. Updated tokens: {updated_tokens}\"\n+                \"The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. \"\n+                \"The model config and generation config were aligned accordingly, being updated with the tokenizer's \"\n+                f\"values. Updated tokens: {updated_tokens}.\"\n             )\n \n     def _set_signature_columns_if_needed(self):"
        }
    ],
    "stats": {
        "total": 15,
        "additions": 9,
        "deletions": 6
    }
}