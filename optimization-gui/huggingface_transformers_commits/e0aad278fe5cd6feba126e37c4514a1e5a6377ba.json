{
    "author": "Cyrilvallez",
    "message": "Never fallback to eager implicitly (#38327)\n\n* remove arg everywhere\n\n* Update warnings\n\n* add more models\n\n* Update sdpa_attention.py\n\n* fix style\n\n* fix\n\n* readd warnings but not for flex\n\n* Update test_modeling_common.py\n\n* skip\n\n* fix\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
    "files": [
        {
            "sha": "84d365f9aad41f35a2c1178535a894c187e2f5aa",
            "filename": "docs/source/en/modular_transformers.md",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodular_transformers.md?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -243,13 +243,7 @@ class Olmo2Attention(OlmoAttention):\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "784b2d15b6a0ead7986189c6976ac0ab540a4fe8",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -655,7 +655,6 @@ def prepare_inputs_for_generation(\n \n             # If it's not defined, it means the model uses the new general mask API\n             if causal_mask_creation_function is None:  # can't be found\n-                output_attentions = kwargs.get(\"output_attentions\", False)\n                 token_type_ids = getattr(model_input, \"token_type_ids\", None)\n                 # Some models may overwrite the general one\n                 causal_mask_creation_function = getattr(self, \"create_masks_for_generate\", create_masks_for_generate)\n@@ -666,7 +665,6 @@ def prepare_inputs_for_generation(\n                     attention_mask=attention_mask,\n                     cache_position=cache_position,\n                     past_key_values=past_key_values,\n-                    output_attentions=output_attentions,\n                     token_type_ids=token_type_ids,\n                 )\n             else:"
        },
        {
            "sha": "1e1228873f171c9f027a02fe0b786b5e51e214f6",
            "filename": "src/transformers/integrations/flex_attention.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflex_attention.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -235,10 +235,9 @@ def flex_attention_forward(\n     head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n-    if kwargs.get(\"output_attentions\", False) or head_mask is not None:\n+    if head_mask is not None:\n         logger.warning_once(\n-            \"`flex_attention` does not support `output_attentions=True` or `head_mask`.\"\n-            \" Please set your attention to `eager` if you want any of these features.\"\n+            \"`flex_attention` does not support `head_mask`. Please set your attention to `eager` if you want this feature.\"\n         )\n \n     if kwargs.get(\"dropout\", 0.0) > 0:"
        },
        {
            "sha": "8829c0711acfbd68d9de0ebc538a71772e2dbecd",
            "filename": "src/transformers/masking_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 33,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmasking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmasking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmasking_utils.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -644,30 +644,12 @@ def _preprocess_mask_arguments(\n     return False, attention_mask, kv_length, kv_offset\n \n \n-def _get_mask_interface(config: PretrainedConfig, output_attentions: bool = False) -> Callable:\n-    \"\"\"\n-    Return the mask interface (a function) to be used, based on the type of attention found in the config.\n-\n-    Args:\n-        config (`PretrainedConfig`):\n-            The model config.\n-        output_attentions (`bool`, optional):\n-            Whether we return the attention scores or not. By default `False`.\n-    \"\"\"\n-    mask_interface = ALL_MASK_ATTENTION_FUNCTIONS[config._attn_implementation]\n-    # Sdpa fallbacks to eager in the Attention modules if `output_attentions=True`\n-    if config._attn_implementation == \"sdpa\" and output_attentions:\n-        mask_interface = ALL_MASK_ATTENTION_FUNCTIONS[\"eager\"]\n-    return mask_interface\n-\n-\n def create_causal_mask(\n     config: PretrainedConfig,\n     input_embeds: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n     cache_position: torch.Tensor,\n     past_key_values: Optional[Cache],\n-    output_attentions: bool = False,\n     or_mask_function: Optional[Callable] = None,\n     and_mask_function: Optional[Callable] = None,\n ) -> Optional[Union[torch.Tensor, \"BlockMask\"]]:\n@@ -689,8 +671,6 @@ def create_causal_mask(\n             A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n         past_key_values (`Cache`, optional):\n             The past key values, if we use a cache.\n-        output_attentions (`bool`, optional):\n-            Whether we return the attention scores or not. By default `False`.\n         or_mask_function (`Callable`, optional):\n             An optional mask function to combine with the causal mask function (by doing the union of both). This is\n             useful to easily overlay another mask on top of the causal one, for example for image tokens handling.\n@@ -712,7 +692,7 @@ def create_causal_mask(\n \n     batch_size, dtype = input_embeds.shape[0], input_embeds.dtype\n     mask_factory_function = causal_mask_function\n-    mask_interface = _get_mask_interface(config, output_attentions)\n+    mask_interface = ALL_MASK_ATTENTION_FUNCTIONS[config._attn_implementation]\n \n     # Do not allow skip if we are compiling (this is to match BC)\n     # TODO: cyril -> probably revisit and remove this, but a lot of tests rely on it\n@@ -751,7 +731,6 @@ def create_sliding_window_causal_mask(\n     attention_mask: Optional[torch.Tensor],\n     cache_position: torch.Tensor,\n     past_key_values: Optional[Cache],\n-    output_attentions: bool = False,\n     or_mask_function: Optional[Callable] = None,\n     and_mask_function: Optional[Callable] = None,\n ) -> Optional[Union[torch.Tensor, \"BlockMask\"]]:\n@@ -774,8 +753,6 @@ def create_sliding_window_causal_mask(\n             A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n         past_key_values (`Cache`, optional):\n             The past key values, if we use a cache.\n-        output_attentions (`bool`, optional):\n-            Whether we return the attention scores or not. By default `False`.\n         or_mask_function (`Callable`, optional):\n             An optional mask function to combine with the sliding causal mask function (by doing the union of both). This is\n             useful to easily overlay another mask on top of the sliding causal one, for example for image tokens handling.\n@@ -801,7 +778,7 @@ def create_sliding_window_causal_mask(\n \n     batch_size, dtype = input_embeds.shape[0], input_embeds.dtype\n     mask_factory_function = sliding_window_causal_mask_function(sliding_window)\n-    mask_interface = _get_mask_interface(config, output_attentions)\n+    mask_interface = ALL_MASK_ATTENTION_FUNCTIONS[config._attn_implementation]\n \n     # Do not allow skip if we are compiling (this is to match BC)\n     # TODO: cyril -> probably revisit and remove this, but a lot of tests rely on it\n@@ -841,7 +818,6 @@ def create_chunked_causal_mask(\n     attention_mask: Optional[torch.Tensor],\n     cache_position: torch.Tensor,\n     past_key_values: Optional[Cache],\n-    output_attentions: bool = False,\n     or_mask_function: Optional[Callable] = None,\n     and_mask_function: Optional[Callable] = None,\n ) -> Optional[Union[torch.Tensor, \"BlockMask\"]]:\n@@ -864,8 +840,6 @@ def create_chunked_causal_mask(\n             A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n         past_key_values (`Cache`, optional):\n             The past key values, if we use a cache.\n-        output_attentions (`bool`, optional):\n-            Whether we return the attention scores or not. By default `False`.\n         or_mask_function (`Callable`, optional):\n             An optional mask function to combine with the chunked causal mask function (by doing the union of both). This is\n             useful to easily overlay another mask on top of the chunked causal one, for example for image tokens handling.\n@@ -898,7 +872,7 @@ def create_chunked_causal_mask(\n \n     batch_size, dtype = input_embeds.shape[0], input_embeds.dtype\n     mask_factory_function = chunked_causal_mask_function(chunk_size)\n-    mask_interface = _get_mask_interface(config, output_attentions)\n+    mask_interface = ALL_MASK_ATTENTION_FUNCTIONS[config._attn_implementation]\n \n     # Do not allow skip if we are compiling (this is to match BC)\n     # TODO: cyril -> probably revisit and remove this, but a lot of tests rely on it\n@@ -945,7 +919,6 @@ def create_masks_for_generate(\n     attention_mask: Optional[torch.Tensor],\n     cache_position: torch.Tensor,\n     past_key_values: Optional[Cache],\n-    output_attentions: bool = False,\n     or_mask_function: Optional[Callable] = None,\n     and_mask_function: Optional[Callable] = None,\n     **kwargs,\n@@ -967,8 +940,6 @@ def create_masks_for_generate(\n             A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n         past_key_values (`Cache`, optional):\n             The past key values, if we use a cache.\n-        output_attentions (`bool`, optional):\n-            Whether we return the attention scores or not. By default `False`.\n         or_mask_function (`Callable`, optional):\n             An optional mask function to combine with the other mask function (by doing the union of both). This is\n             useful to easily overlay another mask on top of the causal one, for example for image tokens handling.\n@@ -985,7 +956,6 @@ def create_masks_for_generate(\n         \"attention_mask\": attention_mask,\n         \"cache_position\": cache_position,\n         \"past_key_values\": past_key_values,\n-        \"output_attentions\": output_attentions,\n         \"or_mask_function\": or_mask_function,\n         \"and_mask_function\": and_mask_function,\n     }"
        },
        {
            "sha": "abb751ab7df873b6eb732030da13a9649bfa51ed",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -805,7 +805,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "661a3c9bb60eff8062cefeda173596ca1637fd13",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -205,13 +205,7 @@ def forward(\n         attention_interface: Callable = eager_attention_forward\n \n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -425,7 +419,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "c57b7217f1d22337ef37d06ae134809d5e1f5d12",
            "filename": "src/transformers/models/bitnet/modular_bitnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodular_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodular_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodular_bitnet.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -85,13 +85,7 @@ def forward(\n         attention_interface: Callable = eager_attention_forward\n \n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "0700eb8e9f60c96ee5f9e8d1a494266803a48d3d",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -261,13 +261,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -462,7 +456,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "e37c875be38d79624d97b49f90a21814e48b43c6",
            "filename": "src/transformers/models/cohere/modular_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -184,13 +184,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "5690864cfc55c76c05a5328715ebfd08cfad19f2",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -222,13 +222,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -439,7 +433,6 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n-                \"output_attentions\": output_attentions,\n             }\n             # Create the masks\n             causal_mask_mapping = {"
        },
        {
            "sha": "7a5cab506e23401581e73a2276b8bb20923e0ce4",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -309,13 +309,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -461,7 +455,6 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n-                \"output_attentions\": output_attentions,\n             }\n             # Create the masks\n             causal_mask_mapping = {"
        },
        {
            "sha": "e1f1d477b38c6eb2f629c4e8081152b84f65eb77",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -509,7 +509,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds\n@@ -821,7 +820,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "aab2d131c4589c501200a209d6b80d357286fb2f",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -247,7 +247,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "5804eeee4b17ef4a50cb9a497a71455aba520fd1",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -412,13 +412,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -608,7 +602,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "e7d5eaded7e593eb36d3f6c21d6a626c391d41a0",
            "filename": "src/transformers/models/deepseek_v3/modular_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -293,13 +293,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "68aa54180caae572b4a2910fef9302f9172deac0",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -397,23 +397,6 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"DiffLlamaModel is using DiffLlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n-            )\n-\n         bsz, q_len, _ = hidden_states.size()\n \n         query_states = self.q_proj(hidden_states)\n@@ -708,7 +691,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "b772a9f04d5ffe3132ca30a8045b937391ec9854",
            "filename": "src/transformers/models/diffllama/modular_diffllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -330,23 +330,6 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"DiffLlamaModel is using DiffLlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n-            )\n-\n         bsz, q_len, _ = hidden_states.size()\n \n         query_states = self.q_proj(hidden_states)"
        },
        {
            "sha": "c13eb25d9a6b594c06f45dd843b0bd29d2834156",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -1272,7 +1272,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "e508db3865eddf131f0a83321fceb8a10f2fbc7a",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -383,13 +383,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -1552,7 +1546,6 @@ def forward(\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         logits_to_keep (`int` or `torch.Tensor`, *optional*):\n             If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n             `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that"
        },
        {
            "sha": "540b7e7fee66fe52e4b3d5c671ba168c587f0a96",
            "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -251,13 +251,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -1307,7 +1301,6 @@ def forward(\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         logits_to_keep (`int` or `torch.Tensor`, *optional*):\n             If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n             `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that"
        },
        {
            "sha": "2a2960891982b0d600bb85ac8f0162948a81bf6e",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -415,7 +415,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         # embed positions"
        },
        {
            "sha": "e934df7ef8034004ae92d06eddde31c08fb399a4",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -416,7 +416,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         # embed positions"
        },
        {
            "sha": "7bb865bc5dcde394a6f36c90467ff5b51f3c9f16",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -218,13 +218,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -445,7 +439,6 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n-                \"output_attentions\": output_attentions,\n             }\n             # Create the masks\n             causal_mask_mapping = {"
        },
        {
            "sha": "31b251f4ca7751ece77f344d0a72bb728c187108",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -283,13 +283,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -428,7 +422,6 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n-                \"output_attentions\": output_attentions,\n             }\n             # Create the masks\n             causal_mask_mapping = {"
        },
        {
            "sha": "4a9fbfcd319066ad9e2636a8bcc715310b7bcc58",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -345,14 +345,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. \"\n-                    \"Falling back to eager attention. This warning can be removed using the argument \"\n-                    '`attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -566,7 +559,6 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n-                \"output_attentions\": output_attentions,\n             }\n             # Create the masks\n             causal_mask_mapping = {\n@@ -949,7 +941,6 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n-                \"output_attentions\": output_attentions,\n             }\n             if token_type_ids is not None and inputs_embeds.shape[1] != 1:\n                 # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`\n@@ -1200,7 +1191,6 @@ def create_masks_for_generate(\n         attention_mask: Optional[torch.Tensor],\n         cache_position: torch.Tensor,\n         past_key_values: Optional[Cache],\n-        output_attentions: bool = False,\n         token_type_ids: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> dict:\n@@ -1211,7 +1201,6 @@ def create_masks_for_generate(\n             \"attention_mask\": attention_mask,\n             \"cache_position\": cache_position,\n             \"past_key_values\": past_key_values,\n-            \"output_attentions\": output_attentions,\n         }\n         # Add the token type ids mask for generate as well\n         if token_type_ids is not None and input_embeds.shape[1] != 1:"
        },
        {
            "sha": "7f9d1be8d9aba88184934948399d44aef81783a4",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -424,14 +424,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. \"\n-                    \"Falling back to eager attention. This warning can be removed using the argument \"\n-                    '`attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -617,7 +610,6 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n-                \"output_attentions\": output_attentions,\n             }\n             # Create the masks\n             causal_mask_mapping = {\n@@ -840,7 +832,6 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n-                \"output_attentions\": output_attentions,\n             }\n             if token_type_ids is not None and inputs_embeds.shape[1] != 1:\n                 # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`\n@@ -1050,7 +1041,6 @@ def create_masks_for_generate(\n         attention_mask: Optional[torch.Tensor],\n         cache_position: torch.Tensor,\n         past_key_values: Optional[Cache],\n-        output_attentions: bool = False,\n         token_type_ids: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> dict:\n@@ -1061,7 +1051,6 @@ def create_masks_for_generate(\n             \"attention_mask\": attention_mask,\n             \"cache_position\": cache_position,\n             \"past_key_values\": past_key_values,\n-            \"output_attentions\": output_attentions,\n         }\n         # Add the token type ids mask for generate as well\n         if token_type_ids is not None and input_embeds.shape[1] != 1:"
        },
        {
            "sha": "235f8258c10c64387fabe682db3165d3d1ff3a7d",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -436,7 +436,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "f32bfb3a3928b48f6cb89f9c962b3d7debec7222",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -444,7 +444,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "16de0f23db9161304305458d99b5254fcd9387ff",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 2,
            "deletions": 22,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -166,28 +166,9 @@ def forward(\n             }\n             key_states, value_states = layer_past.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # Checking for fallbacks in case an unsupported feature is requested\n-        attention_type = self.config._attn_implementation\n-        if (output_attentions or head_mask is not None) and self.config._attn_implementation in [\n-            \"sdpa\",\n-            \"flash_attention_2\",\n-        ]:\n-            logger.warning_once(\n-                f\"Setting `attention_type` to `eager` because `{attention_type}` does not support\"\n-                f\" `output_attentions=True` or `head_mask`.\"\n-            )\n-            attention_type = \"eager\"\n-\n-        elif self.training and self.attention_dropout > 0 and self.config._attn_implementation == \"flex_attention\":\n-            logger.warning_once(\n-                f\"Setting `attention_type` to `eager` because `dropout` is not supported in `{attention_type}`.\"\n-            )\n-            attention_type = \"eager\"\n-\n         attention_interface: Callable = eager_attention_forward\n-        attention_interface = (\n-            ALL_ATTENTION_FUNCTIONS[attention_type] if attention_type != \"eager\" else attention_interface\n-        )\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         # Compute attention\n         attn_output, attn_weights = attention_interface(\n@@ -409,7 +390,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         # Prepare head mask if needed"
        },
        {
            "sha": "e7d67a97644f5de36b56435cfd72d1a45ce3175e",
            "filename": "src/transformers/models/gpt_neox/modular_gpt_neox.py",
            "status": "modified",
            "additions": 2,
            "deletions": 22,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -153,28 +153,9 @@ def forward(\n             }\n             key_states, value_states = layer_past.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # Checking for fallbacks in case an unsupported feature is requested\n-        attention_type = self.config._attn_implementation\n-        if (output_attentions or head_mask is not None) and self.config._attn_implementation in [\n-            \"sdpa\",\n-            \"flash_attention_2\",\n-        ]:\n-            logger.warning_once(\n-                f\"Setting `attention_type` to `eager` because `{attention_type}` does not support\"\n-                f\" `output_attentions=True` or `head_mask`.\"\n-            )\n-            attention_type = \"eager\"\n-\n-        elif self.training and self.attention_dropout > 0 and self.config._attn_implementation == \"flex_attention\":\n-            logger.warning_once(\n-                f\"Setting `attention_type` to `eager` because `dropout` is not supported in `{attention_type}`.\"\n-            )\n-            attention_type = \"eager\"\n-\n         attention_interface: Callable = eager_attention_forward\n-        attention_interface = (\n-            ALL_ATTENTION_FUNCTIONS[attention_type] if attention_type != \"eager\" else attention_interface\n-        )\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         # Compute attention\n         attn_output, attn_weights = attention_interface(\n@@ -356,7 +337,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         # Prepare head mask if needed"
        },
        {
            "sha": "11f2873f3dfc4473a0f61668cf9971d175de6063",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -439,7 +439,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "33f3b3363e92747e359c46ff8f48b3d37c56a6fb",
            "filename": "src/transformers/models/granite/modular_granite.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -181,7 +181,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "a3a314a6abb56ced17f02460309d6d7071bac2f4",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -452,13 +452,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "d6ff36bf324412bca0a355443e4aeeee0e2d9ba0",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -203,13 +203,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "dc429aa55bce71e1aede5cd241495b7261c3d787",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -387,13 +387,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "b9cb3bafc13da3685859c269e53b0b839d6ddf8e",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -421,7 +421,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "6f06b32c1686da7b9a9bd2095825de08304449e9",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -40,16 +40,12 @@\n     auto_docstring,\n     can_return_tuple,\n     is_torchdynamo_compiling,\n-    logging,\n     torch_int,\n )\n from ..auto import AutoModel\n from .configuration_internvl import InternVLConfig, InternVLVisionConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n @use_kernel_forward_from_hub(\"RMSNorm\")\n class InternVLVisionRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n@@ -151,13 +147,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "91e53ec523398211708bd1b02073cf3e8cb0d6e9",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -108,13 +108,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "959cdc6856f449122115173515bf20f87eea65eb",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -358,13 +358,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "a6965687781ce1b0df36ee5b42e87656d1bfc5f7",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -506,13 +506,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "4502cee6e5736761ccc39a5dd879c5b4fec902e1",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -425,7 +425,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "2f6202ef8d19668dbe8c6690e15ea05087dd9354",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 15,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -356,13 +356,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n         attn_output, attn_weights = attention_interface(\n             self,\n             query_states,\n@@ -570,7 +564,6 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n-                \"output_attentions\": output_attentions,\n             }\n             # Create the masks\n             causal_mask_mapping = {\n@@ -916,13 +909,7 @@ def forward(\n         attention_interface: Callable = vision_eager_attention_forward\n         # flex disable because breaks on TP 8, embed is 88 not power of 2\n         if self.config._attn_implementation not in [\"eager\", \"flex_attention\"]:\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "90881cbcd2b9f89b55b6a35c3d17f609a0a405fa",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -164,13 +164,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -404,7 +398,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "e943150f01beea01c3388db0993efb2fc55706c6",
            "filename": "src/transformers/models/mistral/modular_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -75,13 +75,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -168,7 +162,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "9147538f73ccdd4075365306285c7d306064872a",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -276,13 +276,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -525,7 +519,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "7132a4165f94e8a479197f38a83e6974b25ffa1f",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -370,7 +370,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "ad943fd9c5765c364ae4a5f38a7ba26090d89cb7",
            "filename": "src/transformers/models/mlcd/modeling_mlcd.py",
            "status": "modified",
            "additions": 2,
            "deletions": 11,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -28,13 +28,10 @@\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, can_return_tuple, logging, torch_int\n+from ...utils import auto_docstring, can_return_tuple, torch_int\n from .configuration_mlcd import MLCDVisionConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n class MLCDMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -281,13 +278,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "6186cbbbb45f671ffee7cd1e5571702c7f91f20f",
            "filename": "src/transformers/models/mlcd/modular_mlcd.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -226,13 +226,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "a3aebaed9a50a2fb6aeb1bc508e52a00c1f4242a",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -265,13 +265,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         is_causal = True if self.is_causal and attention_mask is None and q_len > 1 else False\n \n@@ -749,7 +743,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "6abc22ae99725bb911015ce4bba6cbd34431cc9b",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -361,13 +361,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         is_causal = True if self.is_causal and attention_mask is None and q_len > 1 else False\n \n@@ -755,7 +749,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "36999733b3a91dd087bf55c512f0d3ba7bc4ea5c",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -191,13 +191,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -406,7 +400,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "18a17533a3cd506857bf88157d7be266d4838f8a",
            "filename": "src/transformers/models/olmo/modular_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -111,13 +111,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "661a9341d67af7a075234d7af4ff32f6b22ba11b",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -179,13 +179,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -412,7 +406,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "103d6616c5f85b4fe753e7f96c9fa64b50e20eb5",
            "filename": "src/transformers/models/olmo2/modular_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -225,13 +225,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "3f2deffd9e0d31dbe2327e362074818b20188d48",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -173,13 +173,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -400,7 +394,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         inputs_embeds = self.embed_dropout(inputs_embeds)  # diff with Llama"
        },
        {
            "sha": "c1d40774bcda5450d75a70b681c65e4250345f78",
            "filename": "src/transformers/models/phi/modular_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -96,13 +96,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -251,7 +245,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         inputs_embeds = self.embed_dropout(inputs_embeds)  # diff with Llama"
        },
        {
            "sha": "08f93a468b5f521af912b847e2de94b10bc094ea",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -193,13 +193,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -459,7 +453,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "9c3413888072af51596224e53a84e52f0af3bc88",
            "filename": "src/transformers/models/phi3/modular_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -145,13 +145,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "858666b9f5a052f63d548e86192f59d7e4c67c55",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -1427,13 +1427,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -1766,7 +1760,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "e6ab2c1cb0dd2321667d742a53a819ce5987f7aa",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -1577,7 +1577,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "03df9df94f6c798b038f999eeb5136771ab7edba",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -165,13 +165,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -411,7 +405,6 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n-                \"output_attentions\": output_attentions,\n             }\n             # Create the masks\n             causal_mask_mapping = {"
        },
        {
            "sha": "10f3c3acca35d9be16e278521746b4e5d272f213",
            "filename": "src/transformers/models/qwen2/modular_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -75,13 +75,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -173,7 +167,6 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n-                \"output_attentions\": output_attentions,\n             }\n             # Create the masks\n             causal_mask_mapping = {"
        },
        {
            "sha": "ef1cd22e0c04d1d2cdcbd1e4d7320ffdd9be7c55",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -212,13 +212,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -437,7 +431,6 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n-                \"output_attentions\": output_attentions,\n             }\n             # Create the masks\n             causal_mask_mapping = {"
        },
        {
            "sha": "466eb3d02988db2953777352caf82774d3cad729",
            "filename": "src/transformers/models/qwen3/modular_qwen3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodular_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodular_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodular_qwen3.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -89,13 +89,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "77b2362fe192138cad922c75ed3cb59f90ecb8bc",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -177,13 +177,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -530,7 +524,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "6e102d8014262791338e6d842cd2e9ab505fbd1d",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -184,13 +184,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -405,7 +399,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "d415870e4949839592cc2af765da6f3bc7deec2c",
            "filename": "src/transformers/models/starcoder2/modular_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -103,13 +103,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -220,7 +214,6 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "63ea960028cdc5d57d8455806a7f2eaed1ab0eb7",
            "filename": "src/transformers/models/timesfm/modeling_timesfm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodeling_timesfm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodeling_timesfm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodeling_timesfm.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -246,13 +246,7 @@ def forward(\n \n         attention_interface: Callable = simple_eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "8918705f3aefc04d23d9330f120c14601b3a4cf4",
            "filename": "src/transformers/models/timesfm/modular_timesfm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodular_timesfm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodular_timesfm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodular_timesfm.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -202,13 +202,7 @@ def forward(\n \n         attention_interface: Callable = simple_eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "7733decd0198757397117a84fa76732987ee547a",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -269,13 +269,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "b2aed168239fa07feecec2d8678602283bac54c4",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -53,7 +53,6 @@\n else:\n     causal_conv1d_update, causal_conv1d_fn = None, None\n \n-\n logger = logging.get_logger(__name__)\n \n \n@@ -435,13 +434,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "c4e14dd14824e1a90a13ce9b7155a4bbbbc81181",
            "filename": "src/transformers/models/zamba2/modular_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -270,13 +270,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "87fab3f8af701365c2361ebe7faf8181ce9326ef",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0aad278fe5cd6feba126e37c4514a1e5a6377ba/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=e0aad278fe5cd6feba126e37c4514a1e5a6377ba",
            "patch": "@@ -4353,7 +4353,8 @@ def test_sliding_window_mask(self):\n             if hasattr(config, \"layer_types\"):\n                 del config_dict[\"layer_types\"]\n             new_config = config.__class__(**config_dict)\n-            model = model_class(new_config).to(torch_device)\n+            # We need to set eager as otherwise `output_attentions` is not supported\n+            model = model_class._from_config(new_config, attn_implementation=\"eager\").to(torch_device)\n             model.eval()\n             layer_types = getattr(model.config, \"layer_types\", [\"sliding_attention\"] * config.num_hidden_layers)\n             attentions = model(**inputs, output_attentions=True).attentions\n@@ -4370,7 +4371,8 @@ def test_sliding_window_mask(self):\n             if hasattr(config, \"layer_types\"):\n                 del config_dict[\"layer_types\"]\n             new_config = config.__class__(**config_dict)\n-            model = model_class(new_config).to(torch_device)\n+            # We need to set eager as otherwise `output_attentions` is not supported\n+            model = model_class._from_config(new_config, attn_implementation=\"eager\").to(torch_device)\n             model.eval()\n             attentions_not_sliding = model(**inputs, output_attentions=True).attentions\n             for layer_attention in attentions_not_sliding:"
        }
    ],
    "stats": {
        "total": 610,
        "additions": 66,
        "deletions": 544
    }
}