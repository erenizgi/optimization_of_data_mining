{
    "author": "wavy-jung",
    "message": "manual `head_dim` for `mixtral` model (#34281)",
    "sha": "8755dd26b7e5ac25987a03627d317624dcdad2a1",
    "files": [
        {
            "sha": "686c214ef25ce541f80005b396a2df0f7fd673a4",
            "filename": "src/transformers/models/mixtral/configuration_mixtral.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8755dd26b7e5ac25987a03627d317624dcdad2a1/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8755dd26b7e5ac25987a03627d317624dcdad2a1/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py?ref=8755dd26b7e5ac25987a03627d317624dcdad2a1",
            "patch": "@@ -53,6 +53,8 @@ class MixtralConfig(PretrainedConfig):\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n             by meanpooling all the original heads within that group. For more details checkout [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.\n+        head_dim (`int`, *optional*, defaults to `hidden_size // num_attention_heads`):\n+            The attention head dimension.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder.\n         max_position_embeddings (`int`, *optional*, defaults to `4096*32`):\n@@ -116,6 +118,7 @@ def __init__(\n         num_hidden_layers=32,\n         num_attention_heads=32,\n         num_key_value_heads=8,\n+        head_dim=None,\n         hidden_act=\"silu\",\n         max_position_embeddings=4096 * 32,\n         initializer_range=0.02,\n@@ -154,6 +157,7 @@ def __init__(\n         self.use_cache = use_cache\n         self.rope_theta = rope_theta\n         self.attention_dropout = attention_dropout\n+        self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n \n         self.num_experts_per_tok = num_experts_per_tok\n         self.num_local_experts = num_local_experts"
        },
        {
            "sha": "de1cd1097a53ffefa986d33bc693cfbfe27382a0",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 4,
            "deletions": 9,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8755dd26b7e5ac25987a03627d317624dcdad2a1/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8755dd26b7e5ac25987a03627d317624dcdad2a1/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=8755dd26b7e5ac25987a03627d317624dcdad2a1",
            "patch": "@@ -283,19 +283,14 @@ def __init__(self, config: MixtralConfig, layer_idx: Optional[int] = None):\n \n         self.hidden_size = config.hidden_size\n         self.num_heads = config.num_attention_heads\n-        self.head_dim = self.hidden_size // self.num_heads\n+        self.head_dim = config.head_dim\n         self.num_key_value_heads = config.num_key_value_heads\n         self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n         self.max_position_embeddings = config.max_position_embeddings\n         self.rope_theta = config.rope_theta\n         self.is_causal = True\n         self.attention_dropout = config.attention_dropout\n \n-        if (self.head_dim * self.num_heads) != self.hidden_size:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n         self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n         self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n@@ -374,7 +369,7 @@ def forward(\n             )\n \n         attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n+        attn_output = attn_output.reshape(bsz, q_len, -1)\n \n         attn_output = self.o_proj(attn_output)\n \n@@ -481,7 +476,7 @@ def forward(\n             is_causal=self.is_causal,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n+        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n \n         if not output_attentions:\n@@ -575,7 +570,7 @@ def forward(\n         )\n \n         attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n+        attn_output = attn_output.view(bsz, q_len, -1)\n \n         attn_output = self.o_proj(attn_output)\n "
        }
    ],
    "stats": {
        "total": 17,
        "additions": 8,
        "deletions": 9
    }
}