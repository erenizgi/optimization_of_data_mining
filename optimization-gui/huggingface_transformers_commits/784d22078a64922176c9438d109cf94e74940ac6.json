{
    "author": "faaany",
    "message": "[doc] use full path for run_qa.py  (#34914)\n\nuse full path for run_qa.py",
    "sha": "784d22078a64922176c9438d109cf94e74940ac6",
    "files": [
        {
            "sha": "06b50f0b15e10dc67875079a36835e1ddbd8cd0c",
            "filename": "docs/source/en/perf_infer_cpu.md",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/784d22078a64922176c9438d109cf94e74940ac6/docs%2Fsource%2Fen%2Fperf_infer_cpu.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/784d22078a64922176c9438d109cf94e74940ac6/docs%2Fsource%2Fen%2Fperf_infer_cpu.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_cpu.md?ref=784d22078a64922176c9438d109cf94e74940ac6",
            "patch": "@@ -42,7 +42,6 @@ Enable BetterTransformer with the [`PreTrainedModel.to_bettertransformer`] metho\n from transformers import AutoModelForCausalLM\n \n model = AutoModelForCausalLM.from_pretrained(\"bigcode/starcoder\")\n-model.to_bettertransformer()\n ```\n \n ## TorchScript\n@@ -54,7 +53,7 @@ For a gentle introduction to TorchScript, see the [Introduction to PyTorch Torch\n With the [`Trainer`] class, you can enable JIT mode for CPU inference by setting the `--jit_mode_eval` flag:\n \n ```bash\n-python run_qa.py \\\n+python examples/pytorch/question-answering/run_qa.py \\\n --model_name_or_path csarron/bert-base-uncased-squad-v1 \\\n --dataset_name squad \\\n --do_eval \\\n@@ -86,7 +85,7 @@ pip install intel_extension_for_pytorch\n Set the `--use_ipex` and `--jit_mode_eval` flags in the [`Trainer`] class to enable JIT mode with the graph optimizations:\n \n ```bash\n-python run_qa.py \\\n+python examples/pytorch/question-answering/run_qa.py \\\n --model_name_or_path csarron/bert-base-uncased-squad-v1 \\\n --dataset_name squad \\\n --do_eval \\"
        },
        {
            "sha": "ab2f735ecbdd50e01c7ff61c18a4412af4f4a670",
            "filename": "docs/source/en/perf_train_cpu.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/784d22078a64922176c9438d109cf94e74940ac6/docs%2Fsource%2Fen%2Fperf_train_cpu.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/784d22078a64922176c9438d109cf94e74940ac6/docs%2Fsource%2Fen%2Fperf_train_cpu.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_train_cpu.md?ref=784d22078a64922176c9438d109cf94e74940ac6",
            "patch": "@@ -51,7 +51,7 @@ To enable auto mixed precision with IPEX in Trainer, users should add `use_ipex`\n Take an example of the use cases on [Transformers question-answering](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering)\n \n - Training with IPEX using BF16 auto mixed precision on CPU:\n-<pre> python run_qa.py \\\n+<pre> python examples/pytorch/question-answering/run_qa.py \\\n --model_name_or_path google-bert/bert-base-uncased \\\n --dataset_name squad \\\n --do_train \\"
        },
        {
            "sha": "d6a029c471de083b65c8d8e4c046bd18b28305bc",
            "filename": "docs/source/en/perf_train_cpu_many.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/784d22078a64922176c9438d109cf94e74940ac6/docs%2Fsource%2Fen%2Fperf_train_cpu_many.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/784d22078a64922176c9438d109cf94e74940ac6/docs%2Fsource%2Fen%2Fperf_train_cpu_many.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_train_cpu_many.md?ref=784d22078a64922176c9438d109cf94e74940ac6",
            "patch": "@@ -75,7 +75,7 @@ The following command enables training with 2 processes on one Xeon node, with o\n  export CCL_WORKER_COUNT=1\n  export MASTER_ADDR=127.0.0.1\n  mpirun -n 2 -genv OMP_NUM_THREADS=23 \\\n- python3 run_qa.py \\\n+ python3 examples/pytorch/question-answering/run_qa.py \\\n  --model_name_or_path google-bert/bert-large-uncased \\\n  --dataset_name squad \\\n  --do_train \\\n@@ -104,7 +104,7 @@ Now, run the following command in node0 and **4DDP** will be enabled in node0 an\n  export MASTER_ADDR=xxx.xxx.xxx.xxx #node0 ip\n  mpirun -f hostfile -n 4 -ppn 2 \\\n  -genv OMP_NUM_THREADS=23 \\\n- python3 run_qa.py \\\n+ python3 examples/pytorch/question-answering/run_qa.py \\\n  --model_name_or_path google-bert/bert-large-uncased \\\n  --dataset_name squad \\\n  --do_train \\"
        }
    ],
    "stats": {
        "total": 11,
        "additions": 5,
        "deletions": 6
    }
}