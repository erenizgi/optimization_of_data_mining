{
    "author": "yijun-lee",
    "message": "ğŸŒ [i18n-KO] Translated `openai-gpt.md` to Korean (#33801)\n\n* docs: ko: openai-gpt.md\r\n\r\n* feat: nmt draft\r\n\r\n* fix: manual edits\r\n\r\n* fix: resolve suggestions\r\n\r\nCo-authored-by: Jiwook Han <33192762+mreraser@users.noreply.github.com>\r\nCo-authored-by: Chulhwa (Evan) Han <cjfghk5697@ajou.ac.kr>\r\n\r\n* fix: resolve suggestions\r\n\r\n* \bfix: resolve suggestions\r\n\r\n---------\r\n\r\nCo-authored-by: Jiwook Han <33192762+mreraser@users.noreply.github.com>\r\nCo-authored-by: Chulhwa (Evan) Han <cjfghk5697@ajou.ac.kr>",
    "sha": "c674f2e31340092eac203b9fd45281b94d145462",
    "files": [
        {
            "sha": "e33a67d36f31da6e4925a1da8ace7bbffc1e13e8",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c674f2e31340092eac203b9fd45281b94d145462/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/c674f2e31340092eac203b9fd45281b94d145462/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=c674f2e31340092eac203b9fd45281b94d145462",
            "patch": "@@ -398,8 +398,8 @@\n         title: (ë²ˆì—­ì¤‘) Funnel Transformer\n       - local: model_doc/gemma\n         title: Gemma\n-      - local: in_translation\n-        title: (ë²ˆì—­ì¤‘) GPT\n+      - local: model_doc/openai-gpt\n+        title: GPT\n       - local: in_translation\n         title: (ë²ˆì—­ì¤‘) GPT Neo\n       - local: in_translation"
        },
        {
            "sha": "7e8f57c743bf7173e4fd31657abfb0b27b469f83",
            "filename": "docs/source/ko/model_doc/openai-gpt.md",
            "status": "added",
            "additions": 149,
            "deletions": 0,
            "changes": 149,
            "blob_url": "https://github.com/huggingface/transformers/blob/c674f2e31340092eac203b9fd45281b94d145462/docs%2Fsource%2Fko%2Fmodel_doc%2Fopenai-gpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c674f2e31340092eac203b9fd45281b94d145462/docs%2Fsource%2Fko%2Fmodel_doc%2Fopenai-gpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fopenai-gpt.md?ref=c674f2e31340092eac203b9fd45281b94d145462",
            "patch": "@@ -0,0 +1,149 @@\n+<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# OpenAI GPT [[openai-gpt]]\n+\n+<div class=\"flex flex-wrap space-x-1\">\n+<a href=\"https://huggingface.co/models?filter=openai-gpt\">\n+<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-openai--gpt-blueviolet\">\n+</a>\n+<a href=\"https://huggingface.co/spaces/docs-demos/openai-gpt\">\n+<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n+</a>\n+</div>\n+\n+## ê°œìš” [[overview]]\n+\n+OpenAI GPT ëª¨ë¸ì€ Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskeverê°€ ì‘ì„±í•œ [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) ë…¼ë¬¸ì—ì„œ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” Toronto Book Corpusì™€ ê°™ì€ ì¥ê¸° ì˜ì¡´ì„±ì„ ê°€ì§„ ëŒ€ê·œëª¨ ë§ë­‰ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì–¸ì–´ ëª¨ë¸ë§ìœ¼ë¡œ ì‚¬ì „ í•™ìŠµëœ ì¸ê³¼ì (ë‹¨ë°©í–¥) íŠ¸ëœìŠ¤í¬ë¨¸ì…ë‹ˆë‹¤.\n+\n+ë…¼ë¬¸ì˜ ì´ˆë¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n+\n+*ìì—°ì–´ ì´í•´ëŠ” í…ìŠ¤íŠ¸ í•¨ì˜, ì§ˆë¬¸ ì‘ë‹µ, ì˜ë¯¸ ìœ ì‚¬ì„± í‰ê°€, ë¬¸ì„œ ë¶„ë¥˜ì™€ ê°™ì€ ë‹¤ì–‘í•œ ì‘ì—…ì„ í¬í•¨í•©ë‹ˆë‹¤. ë¹„ë¡ ëŒ€ê·œëª¨ì˜ ë ˆì´ë¸”ì´ ì—†ëŠ” í…ìŠ¤íŠ¸ ë§ë­‰ì¹˜ê°€ í’ë¶€í•˜ê¸°ëŠ” í•˜ì§€ë§Œ, ì´ëŸ¬í•œ íŠ¹ì • ì‘ì—…ì— ëŒ€í•œ í•™ìŠµì„ ìœ„í•œ ë ˆì´ë¸”ëœ ë°ì´í„°ëŠ” ë¶€ì¡±í•˜ì—¬ íŒë³„ì ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ì´ ì ì ˆí•˜ê²Œ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë‹¤ì–‘í•œ ë ˆì´ë¸”ì´ ì—†ëŠ” í…ìŠ¤íŠ¸ ë§ë­‰ì¹˜ì— ëŒ€í•œ ì–¸ì–´ ëª¨ë¸ì˜ ìƒì„±ì  ì‚¬ì „ í•™ìŠµì„ ìˆ˜í–‰í•˜ê³ , ê° íŠ¹ì • ê³¼ì œì— ëŒ€í•œ íŒë³„ì  ë¯¸ì„¸ ì¡°ì •ì„ ìˆ˜í–‰í•¨ìœ¼ë¡œì¨ ì´ëŸ¬í•œ ê³¼ì œì—ì„œ í° ì„±ê³¼ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ì „ ì ‘ê·¼ ë°©ì‹ê³¼ ë‹¬ë¦¬, ìš°ë¦¬ëŠ” ëª¨ë¸ ì•„í‚¤í…ì²˜ì— ìµœì†Œí•œì˜ ë³€í™”ë¥¼ ìš”êµ¬í•˜ë©´ì„œ íš¨ê³¼ì ì¸ ì „ì´ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ë¯¸ì„¸ ì¡°ì • ì¤‘ì— ê³¼ì œ ì¸ì‹ ì…ë ¥ ë³€í™˜(task-aware input transformation)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ìì—°ì–´ ì´í•´ë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì˜ íš¨ê³¼ë¥¼ ì…ì¦í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ general task-agnostic ëª¨ë¸ì€ ê° ê³¼ì œì— íŠ¹ë³„íˆ ì„¤ê³„ëœ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ëŠ” íŒë³„ì ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ë³´ë‹¤ ë›°ì–´ë‚˜ë©°, ì—°êµ¬ëœ 12ê°œ ê³¼ì œ ì¤‘ 9ê°œ ë¶€ë¬¸ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥(state of the art)ì„ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.*\n+\n+[Write With Transformer](https://transformer.huggingface.co/doc/gpt)ëŠ” Hugging Faceê°€ ë§Œë“  ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ìœ¼ë¡œ, ì—¬ëŸ¬ ëª¨ë¸ì˜ ìƒì„± ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ë©° ê·¸ ì¤‘ì—ëŠ” GPTë„ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n+\n+ì´ ëª¨ë¸ì€ [thomwolf](https://huggingface.co/thomwolf)ì— ì˜í•´ ê¸°ì—¬ë˜ì—ˆìœ¼ë©°, ì›ë³¸ ì½”ë“œëŠ” [ì—¬ê¸°](https://github.com/openai/finetune-transformer-lm)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+## ì‚¬ìš© íŒ [[usage-tips]]\n+\n+- GPTëŠ” ì ˆëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì´ë¯€ë¡œ ì…ë ¥ì„ ì¼ë°˜ì ìœ¼ë¡œ ì™¼ìª½ë³´ë‹¤ëŠ” ì˜¤ë¥¸ìª½ì— íŒ¨ë”©í•˜ëŠ” ê²ƒì´ ê¶Œì¥ë©ë‹ˆë‹¤.\n+- GPTëŠ” ì¸ê³¼ ì–¸ì–´ ëª¨ë¸ë§(Causal Language Modeling, CLM) ëª©í‘œë¡œ í•™ìŠµë˜ì—ˆê¸° ë•Œë¬¸ì— ì‹œí€€ìŠ¤ì—ì„œ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ë¥¼ í™œìš©í•˜ë©´ *run_generation.py* ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ GPT-2ëŠ” êµ¬ë¬¸ì ìœ¼ë¡œ ì¼ê´€ëœ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ì°¸ê³ :\n+\n+*OpenAI GPT* ë…¼ë¬¸ì˜ ì›ë˜ í† í°í™” ê³¼ì •ì„ ì¬í˜„í•˜ë ¤ë©´ `ftfy`ì™€ `SpaCy`ë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤:\n+\n+```bash\n+pip install spacy ftfy==4.4.3\n+python -m spacy download en\n+```\n+\n+`ftfy`ì™€ `SpaCy`ë¥¼ ì„¤ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ [`OpenAIGPTTokenizer`]ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ BERTì˜ `BasicTokenizer`ë¥¼ ì‚¬ìš©í•œ í›„ Byte-Pair Encodingì„ í†µí•´ í† í°í™”í•©ë‹ˆë‹¤(ëŒ€ë¶€ë¶„ì˜ ì‚¬ìš©ì— ë¬¸ì œê°€ ì—†ìœ¼ë‹ˆ ê±±ì •í•˜ì§€ ë§ˆì„¸ìš”).\n+\n+## ë¦¬ì†ŒìŠ¤ [[resources]]\n+\n+OpenAI GPTë¥¼ ì‹œì‘í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ê³µì‹ Hugging Face ë° ì»¤ë®¤ë‹ˆí‹°(ğŸŒ í‘œì‹œ) ë¦¬ì†ŒìŠ¤ ëª©ë¡ì…ë‹ˆë‹¤. ì—¬ê¸°ì— ë¦¬ì†ŒìŠ¤ë¥¼ ì¶”ê°€í•˜ê³  ì‹¶ë‹¤ë©´, Pull Requestë¥¼ ì—´ì–´ì£¼ì‹œë©´ ê²€í† í•˜ê² ìŠµë‹ˆë‹¤! ë¦¬ì†ŒìŠ¤ëŠ” ê¸°ì¡´ ë¦¬ì†ŒìŠ¤ë¥¼ ë³µì œí•˜ì§€ ì•Šê³  ìƒˆë¡œìš´ ê²ƒì„ ë³´ì—¬ì£¼ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n+\n+<PipelineTag pipeline=\"text-classification\"/>\n+\n+- [SetFitì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë¶„ë¥˜ì—ì„œ OpenAI GPT-3ì„ ëŠ¥ê°€í•˜ëŠ” ë°©ë²•](https://www.philschmid.de/getting-started-setfit) ë¸”ë¡œê·¸ ê²Œì‹œë¬¼.\n+- ì¶”ê°€ ìë£Œ: [í…ìŠ¤íŠ¸ ë¶„ë¥˜ ê³¼ì œ ê°€ì´ë“œ](../tasks/sequence_classification)\n+\n+<PipelineTag pipeline=\"text-generation\"/>\n+\n+- [Hugging Faceì™€ í•¨ê»˜ ë¹„ì˜ì–´ GPT-2 ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•](https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface) ë¸”ë¡œê·¸.\n+- GPT-2ì™€ í•¨ê»˜ [Transformersë¥¼ ì‚¬ìš©í•œ ì–¸ì–´ ìƒì„±ì˜ ë‹¤ì–‘í•œ ë””ì½”ë”© ë°©ë²•](https://huggingface.co/blog/how-to-generate)ì— ëŒ€í•œ ë¸”ë¡œê·¸.\n+- [Scratchì—ì„œ CodeParrot ğŸ¦œì„ í›ˆë ¨í•˜ëŠ” ë°©ë²•](https://huggingface.co/blog/codeparrot), ëŒ€ê·œëª¨ GPT-2 ëª¨ë¸ì— ëŒ€í•œ ë¸”ë¡œê·¸.\n+- GPT-2ì™€ í•¨ê»˜ [TensorFlow ë° XLAë¥¼ ì‚¬ìš©í•œ ë” ë¹ ë¥¸ í…ìŠ¤íŠ¸ ìƒì„±](https://huggingface.co/blog/tf-xla-generate)ì— ëŒ€í•œ ë¸”ë¡œê·¸.\n+- [Megatron-LMìœ¼ë¡œ ì–¸ì–´ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ë°©ë²•](https://huggingface.co/blog/megatron-training)ì— ëŒ€í•œ ë¸”ë¡œê·¸.\n+- [ì¢‹ì•„í•˜ëŠ” ì•„í‹°ìŠ¤íŠ¸ì˜ ìŠ¤íƒ€ì¼ë¡œ ê°€ì‚¬ë¥¼ ìƒì„±í•˜ë„ë¡ GPT2ë¥¼ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb)ì— ëŒ€í•œ ë…¸íŠ¸ë¶. ğŸŒ\n+- [ì¢‹ì•„í•˜ëŠ” íŠ¸ìœ„í„° ì‚¬ìš©ìì˜ ìŠ¤íƒ€ì¼ë¡œ íŠ¸ìœ—ì„ ìƒì„±í•˜ë„ë¡ GPT2ë¥¼ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb)ì— ëŒ€í•œ ë…¸íŠ¸ë¶. ğŸŒ\n+- ğŸ¤— Hugging Face ì½”ìŠ¤ì˜ [ì¸ê³¼ ì–¸ì–´ ëª¨ë¸ë§](https://huggingface.co/course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch) ì¥.\n+- [`OpenAIGPTLMHeadModel`]ì€ [ì¸ê³¼ ì–¸ì–´ ëª¨ë¸ë§ ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#gpt-2gpt-and-causal-language-modeling), [í…ìŠ¤íŠ¸ ìƒì„± ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-generation/run_generation.py) ë° [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)ì— ì˜í•´ ì§€ì›ë©ë‹ˆë‹¤.\n+- [`TFOpenAIGPTLMHeadModel`]ì€ [ì¸ê³¼ ì–¸ì–´ ëª¨ë¸ë§ ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_clmpy) ë° [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)ì— ì˜í•´ ì§€ì›ë©ë‹ˆë‹¤.\n+- ì¶”ê°€ ìë£Œ: [ì¸ê³¼ ì–¸ì–´ ëª¨ë¸ë§ ê³¼ì œ ê°€ì´ë“œ](../tasks/language_modeling)\n+\n+<PipelineTag pipeline=\"token-classification\"/>\n+\n+- [Byte-Pair Encoding í† í°í™”](https://huggingface.co/course/en/chapter6/5)ì— ëŒ€í•œ ê°•ì˜ ìë£Œ.\n+\n+## OpenAIGPTConfig [[transformers.OpenAIGPTConfig]]\n+\n+[[autodoc]] OpenAIGPTConfig\n+\n+## OpenAIGPTTokenizer [[transformers.OpenAIGPTTokenizer]]\n+\n+[[autodoc]] OpenAIGPTTokenizer\n+    - save_vocabulary\n+\n+## OpenAIGPTTokenizerFast [[transformers.OpenAIGPTTokenizerFast]]\n+\n+[[autodoc]] OpenAIGPTTokenizerFast\n+\n+## OpenAI specific outputs [[transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput]]\n+\n+[[autodoc]] models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput\n+\n+[[autodoc]] models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModelOutput\n+\n+<frameworkcontent>\n+<pt>\n+\n+## OpenAIGPTModel [[transformers.OpenAIGPTModel]]\n+\n+[[autodoc]] OpenAIGPTModel\n+    - forward\n+\n+## OpenAIGPTLMHeadModel [[transformers.OpenAIGPTLMHeadModel]]\n+\n+[[autodoc]] OpenAIGPTLMHeadModel\n+    - forward\n+\n+## OpenAIGPTDoubleHeadsModel [[transformers.OpenAIGPTDoubleHeadsModel]]\n+\n+[[autodoc]] OpenAIGPTDoubleHeadsModel\n+    - forward\n+\n+## OpenAIGPTForSequenceClassification [[transformers.OpenAIGPTForSequenceClassification]]\n+\n+[[autodoc]] OpenAIGPTForSequenceClassification\n+    - forward\n+\n+</pt>\n+<tf>\n+\n+## TFOpenAIGPTModel [[transformers.TFOpenAIGPTModel]]\n+\n+[[autodoc]] TFOpenAIGPTModel\n+    - call\n+\n+## TFOpenAIGPTLMHeadModel [[transformers.TFOpenAIGPTLMHeadModel]]\n+\n+[[autodoc]] TFOpenAIGPTLMHeadModel\n+    - call\n+\n+## TFOpenAIGPTDoubleHeadsModel [[transformers.TFOpenAIGPTDoubleHeadsModel]]\n+\n+[[autodoc]] TFOpenAIGPTDoubleHeadsModel\n+    - call\n+\n+## TFOpenAIGPTForSequenceClassification [[transformers.TFOpenAIGPTForSequenceClassification]]\n+\n+[[autodoc]] TFOpenAIGPTForSequenceClassification\n+    - call\n+\n+</tf>\n+</frameworkcontent>"
        }
    ],
    "stats": {
        "total": 153,
        "additions": 151,
        "deletions": 2
    }
}