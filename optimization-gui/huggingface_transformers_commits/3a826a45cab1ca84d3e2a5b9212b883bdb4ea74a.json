{
    "author": "ash-01xor",
    "message": "Update Model card for GPT2 (#37101)\n\n* Update Model card for gpt2\n\n* Update link for gpt2 space\n\n* fixes docs based on suggestions\n\n* Add transformers-cli and quantization example for GPT-2\n\n* Remove resources and flash attention docs and fix typos",
    "sha": "3a826a45cab1ca84d3e2a5b9212b883bdb4ea74a",
    "files": [
        {
            "sha": "6ce006c9081dfc112bb5e1dea32e9836d75b891f",
            "filename": "docs/source/en/model_doc/gpt2.md",
            "status": "modified",
            "additions": 61,
            "deletions": 161,
            "changes": 222,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a826a45cab1ca84d3e2a5b9212b883bdb4ea74a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a826a45cab1ca84d3e2a5b9212b883bdb4ea74a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt2.md?ref=3a826a45cab1ca84d3e2a5b9212b883bdb4ea74a",
            "patch": "@@ -14,197 +14,97 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# OpenAI GPT2\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<a href=\"https://huggingface.co/models?filter=gpt2\">\n-<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-gpt2-blueviolet\">\n-</a>\n-<a href=\"https://huggingface.co/spaces/docs-demos/gpt2\">\n-<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n-</a>\n+<div style=\"float: right;\">\n+  <div class=\"flex flex-wrap space-x-1\">\n+    <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+    <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+    <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+  </div>\n </div>\n \n-## Overview\n \n-OpenAI GPT-2 model was proposed in [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) by Alec\n-Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei and Ilya Sutskever from [OpenAI](https://huggingface.co/openai). It's a causal (unidirectional)\n-transformer pretrained using language modeling on a very large corpus of ~40 GB of text data.\n+# GPT-2\n \n-The abstract from the paper is the following:\n+[GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) is a scaled up version of GPT, a causal transformer language model, with 10x more parameters and training data. The model was pretrained on a 40GB dataset to predict the next word in a sequence based on all the previous words. This approach enabled the model to perform many downstream tasks in a zero-shot setting.\n \n-*GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset[1] of 8 million\n-web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some\n-text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks\n-across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than\n-10X the amount of data.*\n+The model architecture uses a unidirectional (causal) attention mechanism where each token can only attend to previous tokens, making it particularly effective for text generation tasks.\n \n-[Write With Transformer](https://transformer.huggingface.co/doc/gpt2-large) is a webapp created and hosted by\n-Hugging Face showcasing the generative capabilities of several models. GPT-2 is one of them and is available in five\n-different sizes: small, medium, large, xl and a distilled version of the small checkpoint: *distilgpt-2*.\n+You can find all the original GPT-2 checkpoints under the [OpenAI community](https://huggingface.co/openai-community?search_models=gpt) organization.\n \n-This model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://openai.com/blog/better-language-models/).\n+> [!TIP]\n+> Click on the GPT-2 models in the right sidebar for more examples of how to apply GPT-2 to different language tasks.\n \n-## Usage tips\n+The example below demonstrates how to generate text with [`Pipeline`] or the [`AutoModel`], and from the command line.\n \n-- GPT-2 is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\n-  the left.\n-- GPT-2 was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next\n-  token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be\n-  observed in the *run_generation.py* example script.\n-- The model can take the *past_key_values* (for PyTorch) or *past* (for TF) as input, which is the previously computed\n-  key/value attention pairs. Using this (*past_key_values* or *past*) value prevents the model from re-computing\n-  pre-computed values in the context of text generation. For PyTorch, see *past_key_values* argument of the\n-  [`GPT2Model.forward`] method, or for TF the *past* argument of the\n-  [`TFGPT2Model.call`] method for more information on its usage.\n-- Enabling the *scale_attn_by_inverse_layer_idx* and *reorder_and_upcast_attn* flags will apply the training stability\n-  improvements from [Mistral](https://github.com/stanford-crfm/mistral/) (for PyTorch only).\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n \n-## Usage example\n+```py\n+import torch\n+from transformers import pipeline\n \n-The `generate()` method can be used to generate text using GPT2 model.\n-\n-```python\n->>> from transformers import AutoModelForCausalLM, AutoTokenizer\n-\n->>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n->>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n-\n->>> prompt = \"GPT2 is a model developed by OpenAI.\"\n-\n->>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n-\n->>> gen_tokens = model.generate(\n-...     input_ids,\n-...     do_sample=True,\n-...     temperature=0.9,\n-...     max_length=100,\n-... )\n->>> gen_text = tokenizer.batch_decode(gen_tokens)[0]\n+pipeline = pipeline(task=\"text-generation\", model=\"openai-community/gpt2\", torch_dtype=torch.float16, device=0)\n+pipeline(\"Hello, I'm a language model\")\n ```\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n \n-## Using Flash Attention 2\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n \n-Flash Attention 2 is a faster, optimized version of the attention scores computation which relies on `cuda` kernels.\n+model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\", torch_dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n+tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n \n-### Installation \n+input_ids = tokenzier(\"Hello, I'm a language model\". return_tensors=\"pt\").to(\"cuda\")\n \n-First, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the [official documentation](https://github.com/Dao-AILab/flash-attention#installation-and-features). If your hardware is not compatible with Flash Attention 2, you can still benefit from attention kernel optimisations through Better Transformer support covered [above](https://huggingface.co/docs/transformers/main/en/model_doc/bark#using-better-transformer).\n+output = model.generate(**input_ids, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n \n-Next, [install](https://github.com/Dao-AILab/flash-attention#installation-and-features) the latest version of Flash Attention 2:\n+</hfoption>\n+<hfoption id=\"transformers-cli\">\n \n ```bash\n-pip install -U flash-attn --no-build-isolation\n+echo -e \"Hello, I'm a language model\" | transformers-cli run --task text-generation --model openai-community/gpt2 --device 0\n ```\n \n-### Usage\n+</hfoption>\n+</hfoptions>\n \n-To load a model using Flash Attention 2, we can pass the argument `attn_implementation=\"flash_attention_2\"` to [`.from_pretrained`](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained). We'll also load the model in half-precision (e.g. `torch.float16`), since it results in almost no degradation to audio quality but significantly lower memory usage and faster inference:\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n \n-```python\n->>> import torch\n->>> from transformers import AutoModelForCausalLM, AutoTokenizer\n->>> device = \"cuda\" # the device to load the model onto\n+The example below uses [bitsandbytes](../quantization/bitsandbytes) to only quantize the weights to 4-bits.\n \n->>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\")\n->>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n \n->>> prompt = \"def hello_world():\"\n+quantization_config = BitsAndBytesConfig(\n+    load_in_4bit=True,  \n+    bnb_4bit_quant_type=\"nf4\",  \n+    bnb_4bit_compute_dtype=\"float16\",  \n+    bnb_4bit_use_double_quant=True \n+)\n \n->>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n->>> model.to(device)\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"openai-community/gpt2-xl\",\n+    quantization_config=quantization_config,\n+    device_map=\"auto\"  \n+)\n \n->>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n->>> tokenizer.batch_decode(generated_ids)[0]\n+tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-xl\")\n+inputs = tokenizer(\"Once upon a time, there was a magical forest\", return_tensors=\"pt\").to(\"cuda\")\n+outputs = model.generate(**inputs, max_new_tokens=100)\n+print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n ```\n \n+## Notes\n \n-### Expected speedups\n-\n-Below is an expected speedup diagram that compares pure inference time between the native implementation in transformers using `gpt2` checkpoint and the Flash Attention 2 version of the model using a sequence length of 512.\n-\n-<div style=\"text-align: center\">\n-<img src=\"https://huggingface.co/datasets/EduardoPacheco/documentation-images/resolve/main/gpt2_flash_attention_2_speedup.jpg\">\n-</div>\n-\n-\n-## Using Scaled Dot Product Attention (SDPA)\n-PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\n-encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n-[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n-or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n-page for more information.\n-\n-SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n-`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n-\n-```python\n-from transformers import AutoModelForCausalLM\n-model = AutoModelForCausalLM.from_pretrained(\"gpt2\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n-...\n-```\n-\n-For the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n-\n-On a local benchmark (rtx3080ti-16GB, PyTorch 2.2.1, OS Ubuntu 22.04) using `float16` with\n-[gpt2-large](https://huggingface.co/openai-community/gpt2-large), we saw the\n-following speedups during training and inference.\n-\n-### Training\n-| Batch size | Seq len |  Time per batch (Eager - s) | Time per batch (SDPA - s) | Speedup (%) | Eager peak mem (MB) | SDPA peak mem (MB) |    Mem saving (%) |\n-|-----------:|--------:|----------------------------:|--------------------------:|------------:|--------------------:|-------------------:|------------------:|\n-|          1 |     128 |                       0.039 |                     0.032 |      23.042 |             3482.32 |            3494.62 |            -0.352 |\n-|          1 |     256 |                       0.073 |                     0.059 |       25.15 |             3546.66 |             3552.6 |            -0.167 |\n-|          1 |     512 |                       0.155 |                     0.118 |       30.96 |              4230.1 |            3665.59 |              15.4 |\n-|          1 |    1024 |                       0.316 |                     0.209 |      50.839 |             8682.26 |            4881.09 |            77.875 |\n-|          2 |     128 |                        0.07 |                      0.06 |      15.324 |              3557.8 |            3545.91 |             0.335 |\n-|          2 |     256 |                       0.143 |                     0.122 |       16.53 |              3901.5 |            3657.68 |             6.666 |\n-|          2 |     512 |                       0.267 |                     0.213 |      25.626 |             7062.21 |            4876.47 |            44.822 |\n-|          2 |    1024 |                         OOM |                     0.404 |           / |                 OOM |            8096.35 | SDPA does not OOM |\n-|          4 |     128 |                       0.134 |                     0.128 |       4.412 |             3675.79 |            3648.72 |             0.742 |\n-|          4 |     256 |                       0.243 |                     0.217 |      12.292 |             6129.76 |            4871.12 |            25.839 |\n-|          4 |     512 |                       0.494 |                     0.406 |      21.687 |             12466.6 |            8102.64 |            53.858 |\n-|          4 |    1024 |                         OOM |                     0.795 |           / |                 OOM |            14568.2 | SDPA does not OOM |\n-\n-### Inference\n-| Batch size | Seq len | Per token latency Eager (ms) | Per token latency SDPA (ms) | Speedup (%) | Mem Eager (MB) | Mem SDPA (MB) | Mem saved (%) |\n-|-----------:|--------:|-----------------------------:|----------------------------:|------------:|---------------:|--------------:|--------------:|\n-|          1 |     128 |                        7.991 |                       6.968 |      14.681 |         1685.2 |       1701.32 |        -0.947 |\n-|          1 |     256 |                        8.462 |                       7.199 |      17.536 |        1745.49 |       1770.78 |        -1.428 |\n-|          1 |     512 |                         8.68 |                       7.853 |      10.529 |        1907.69 |       1921.29 |        -0.708 |\n-|          1 |     768 |                        9.101 |                       8.365 |       8.791 |        2032.93 |       2068.12 |        -1.701 |\n-|          2 |     128 |                        9.169 |                       9.001 |       1.861 |        1803.84 |        1811.4 |        -0.418 |\n-|          2 |     256 |                        9.907 |                        9.78 |       1.294 |        1907.72 |       1921.44 |        -0.714 |\n-|          2 |     512 |                       11.519 |                      11.644 |      -1.071 |        2176.86 |       2197.75 |        -0.951 |\n-|          2 |     768 |                       13.022 |                      13.407 |      -2.873 |         2464.3 |       2491.06 |        -1.074 |\n-|          4 |     128 |                       10.097 |                       9.831 |       2.709 |        1942.25 |       1985.13 |         -2.16 |\n-|          4 |     256 |                       11.599 |                      11.398 |       1.764 |        2177.28 |       2197.86 |        -0.937 |\n-|          4 |     512 |                       14.653 |                       14.45 |       1.411 |        2753.16 |       2772.57 |          -0.7 |\n-|          4 |     768 |                       17.846 |                      17.617 |       1.299 |        3327.04 |       3343.97 |        -0.506 |\n-\n-\n-\n-\n-## Resources\n-\n-A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with GPT2. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n-\n-<PipelineTag pipeline=\"text-generation\"/>\n-\n-- A blog on how to [Finetune a non-English GPT-2 Model with Hugging Face](https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface).\n-- A blog on [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate) with GPT-2.\n-- A blog on [Training CodeParrot ðŸ¦œ from Scratch](https://huggingface.co/blog/codeparrot), a large GPT-2 model.\n-- A blog on [Faster Text Generation with TensorFlow and XLA](https://huggingface.co/blog/tf-xla-generate) with GPT-2.\n-- A blog on [How to train a Language Model with Megatron-LM](https://huggingface.co/blog/megatron-training) with a GPT-2 model.\n-- A notebook on how to [finetune GPT2 to generate lyrics in the style of your favorite artist](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb). ðŸŒŽ\n-- A notebook on how to [finetune GPT2 to generate tweets in the style of your favorite Twitter user](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb). ðŸŒŽ\n-- [Causal language modeling](https://huggingface.co/course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch) chapter of the ðŸ¤— Hugging Face Course.\n-- [`GPT2LMHeadModel`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#gpt-2gpt-and-causal-language-modeling), [text generation example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-generation), and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).\n-- [`TFGPT2LMHeadModel`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_clmpy) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).\n-- [`FlaxGPT2LMHeadModel`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#causal-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/causal_language_modeling_flax.ipynb).\n-- [Text classification task guide](../tasks/sequence_classification)\n-- [Token classification task guide](../tasks/token_classification)\n-- [Causal language modeling task guide](../tasks/language_modeling)\n+- Pad inputs on the right because GPT-2 uses absolute position embeddings.\n+- GPT-2 can reuse previously computed key-value attention pairs. Access this feature with the [past_key_values](https://huggingface.co/docs/transformers//en/model_doc/gpt2#transformers.GPT2Model.forward.past_key_values) parameter in [`GPT2Model.forward`].\n+- Enable the [scale_attn_by_inverse_layer_idx](https://huggingface.co/docs/transformers/en/model_doc/gpt2#transformers.GPT2Config.scale_attn_by_inverse_layer_idx) and [reorder_and_upcast_attn](https://huggingface.co/docs/transformers/en/model_doc/gpt2#transformers.GPT2Config.reorder_and_upcast_attn) parameters to apply the training stability improvements from [Mistral](./mistral).\n \n ## GPT2Config\n "
        }
    ],
    "stats": {
        "total": 222,
        "additions": 61,
        "deletions": 161
    }
}