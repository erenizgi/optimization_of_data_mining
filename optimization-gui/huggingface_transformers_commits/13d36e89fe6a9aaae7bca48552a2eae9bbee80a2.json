{
    "author": "MekkCyber",
    "message": "Fix device_map check for ggml files (#37003)\n\nfix",
    "sha": "13d36e89fe6a9aaae7bca48552a2eae9bbee80a2",
    "files": [
        {
            "sha": "1f7aabc6bc86f9d8e8dbbef4155fd0a0e4f59346",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/13d36e89fe6a9aaae7bca48552a2eae9bbee80a2/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13d36e89fe6a9aaae7bca48552a2eae9bbee80a2/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=13d36e89fe6a9aaae7bca48552a2eae9bbee80a2",
            "patch": "@@ -4329,7 +4329,11 @@ def from_pretrained(\n                 \"You cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub.\"\n             )\n \n-        if gguf_file and device_map is not None and \"disk\" in device_map.values():\n+        if (\n+            gguf_file\n+            and device_map is not None\n+            and ((isinstance(device_map, dict) and \"disk\" in device_map.values()) or \"disk\" in device_map)\n+        ):\n             raise RuntimeError(\n                 \"One or more modules is configured to be mapped to disk. Disk offload is not supported for models \"\n                 \"loaded from GGUF files.\""
        }
    ],
    "stats": {
        "total": 6,
        "additions": 5,
        "deletions": 1
    }
}