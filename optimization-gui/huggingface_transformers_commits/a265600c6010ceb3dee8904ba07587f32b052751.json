{
    "author": "avishaiElmakies",
    "message": "add sdpa to OPT (#33298)\n\n* add sdpa to OPT\r\n\r\n* chore: remove redundant whitespace in OPTDecoder class\r\n\r\n* fixup\r\n\r\n* bug fix\r\n\r\n* add sdpa and attention generate test\r\n\r\n* fixup\r\n\r\n* Refactor OPTAttention forward method for improved readability and maintainability\r\n\r\n* undo refactor for _shape and key,val states\r\n\r\n* add OPT to doc, fixup didn't find it for some reason\r\n\r\n* change order\r\n\r\n* change default attn_implemntation in testing to eager\r\n\r\n* [run-slow] opt\r\n\r\n* change test_eager_matches_sdpa_generate to the one llama\r\n\r\n* Update default attention implementation in testing common\r\n\r\n* [run-slow] opt\r\n\r\n* remove uneeded print\r\n\r\n* [run-slow] opt\r\n\r\n* refactor model testers to have attn_implementation=\"eager\"\r\n\r\n* [run-slow] opt\r\n\r\n* convert test_eager_matches_sdpa_generate to opt-350M\r\n\r\n* bug fix when creating mask for opt\r\n\r\n* [run-slow] opt\r\n\r\n* if layer head mask default to eager\r\n\r\n* if head mask is not none fall to eager\r\n\r\n* [run-slow] opt\r\n\r\n* Update src/transformers/models/opt/modeling_opt.py\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* Clean up Unpack imports (#33631)\r\n\r\nclean up Unpack imports\r\n\r\n* Fix DPT /Dinov2 sdpa regression on main (#33660)\r\n\r\n* fallback to eager if output attentions.\r\n\r\n* fix copies\r\n\r\n* handle dependency errors in check_imports (#33622)\r\n\r\n* handle dependency errors in check_imports\r\n\r\n* change log level to warning\r\n\r\n* add back self.max_position_embeddings = config.max_position_embeddings (#33550)\r\n\r\n* add back self.max_position_embeddings = config.max_position_embeddings\r\n\r\n* fix-copies\r\n\r\n* Fix Llava conversion for LlavaQwen2ForCausalLM with Clip vision tower (#33613)\r\n\r\nfix llavaqwen2 model conversion\r\n\r\n* Uniformize kwargs for Udop processor and update docs (#33628)\r\n\r\n* Add optional kwargs and uniformize udop\r\n\r\n* cleanup Unpack\r\n\r\n* nit Udop\r\n\r\n* Generation: deprecate `PreTrainedModel` inheriting from `GenerationMixin`  (#33203)\r\n\r\n* Enable BNB multi-backend support (#31098)\r\n\r\n* enable cpu bnb path\r\n\r\n* fix style\r\n\r\n* fix code style\r\n\r\n* fix 4 bit path\r\n\r\n* Update src/transformers/utils/import_utils.py\r\n\r\nCo-authored-by: Aarni Koskela <akx@iki.fi>\r\n\r\n* add multi backend refactor tests\r\n\r\n* fix style\r\n\r\n* tweak 4bit quantizer + fix corresponding tests\r\n\r\n* tweak 8bit quantizer + *try* fixing corresponding tests\r\n\r\n* fix dequant bnb 8bit\r\n\r\n* account for Intel CPU in variability of expected outputs\r\n\r\n* enable cpu and xpu device map\r\n\r\n* further tweaks to account for Intel CPU\r\n\r\n* fix autocast to work with both cpu + cuda\r\n\r\n* fix comments\r\n\r\n* fix comments\r\n\r\n* switch to testing_utils.torch_device\r\n\r\n* allow for xpu in multi-gpu tests\r\n\r\n* fix tests 4bit for CPU NF4\r\n\r\n* fix bug with is_torch_xpu_available needing to be called as func\r\n\r\n* avoid issue where test reports attr err due to other failure\r\n\r\n* fix formatting\r\n\r\n* fix typo from resolving of merge conflict\r\n\r\n* polish based on last PR review\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\n\r\n* fix CI\r\n\r\n* Update src/transformers/integrations/integration_utils.py\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\r\n\r\n* Update src/transformers/integrations/integration_utils.py\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\r\n\r\n* fix error log\r\n\r\n* fix error msg\r\n\r\n* add \\n in error log\r\n\r\n* make quality\r\n\r\n* rm bnb cuda restriction in doc\r\n\r\n* cpu model don't need dispatch\r\n\r\n* fix doc\r\n\r\n* fix style\r\n\r\n* check cuda avaliable in testing\r\n\r\n* fix tests\r\n\r\n* Update docs/source/en/model_doc/chameleon.md\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\n\r\n* Update docs/source/en/model_doc/llava_next.md\r\n\r\nCo-authored-by: Aarni Koskela <akx@iki.fi>\r\n\r\n* Update tests/quantization/bnb/test_4bit.py\r\n\r\nCo-authored-by: Aarni Koskela <akx@iki.fi>\r\n\r\n* Update tests/quantization/bnb/test_4bit.py\r\n\r\nCo-authored-by: Aarni Koskela <akx@iki.fi>\r\n\r\n* fix doc\r\n\r\n* fix check multibackends\r\n\r\n* fix import sort\r\n\r\n* remove check torch in bnb\r\n\r\n* docs: update bitsandbytes references with multi-backend info\r\n\r\n* docs: fix small mistakes in bnb paragraph\r\n\r\n* run formatting\r\n\r\n* reveret bnb check\r\n\r\n* move bnb multi-backend check to import_utils\r\n\r\n* Update src/transformers/utils/import_utils.py\r\n\r\nCo-authored-by: Aarni Koskela <akx@iki.fi>\r\n\r\n* fix bnb check\r\n\r\n* minor fix for bnb\r\n\r\n* check lib first\r\n\r\n* fix code style\r\n\r\n* Revert \"run formatting\"\r\n\r\nThis reverts commit ac108c6d6b34f45a5745a736ba57282405cfaa61.\r\n\r\n* fix format\r\n\r\n* give warning when bnb version is low and no cuda found]\r\n\r\n* fix device assignment check to be multi-device capable\r\n\r\n* address akx feedback on get_avlbl_dev fn\r\n\r\n* revert partially, as we don't want the function that public, as docs would be too much (enforced)\r\n\r\n---------\r\n\r\nCo-authored-by: Aarni Koskela <akx@iki.fi>\r\nCo-authored-by: Titus von Koeller <9048635+Titus-von-Koeller@users.noreply.github.com>\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\r\n\r\n* Fix error string after refactoring into get_chat_template (#33652)\r\n\r\n* Fix error string after refactoring into get_chat_template\r\n\r\n* Take suggestion from CR\r\n\r\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\r\n\r\n---------\r\n\r\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\r\n\r\n* uniformize git processor (#33668)\r\n\r\n* uniformize git processor\r\n\r\n* update doctring\r\n\r\n* Modular `transformers`: modularity and inheritance for new model additions (#33248)\r\n\r\n* update exampel\r\n\r\n* update\r\n\r\n* push the converted diff files for testing and ci\r\n\r\n* correct one example\r\n\r\n* fix class attributes and docstring\r\n\r\n* nits\r\n\r\n* oups\r\n\r\n* fixed config!\r\n\r\n* update\r\n\r\n* nitd\r\n\r\n* class attributes are not matched against the other, this is missing\r\n\r\n* fixed overwriting self.xxx now onto the attributes I think\r\n\r\n* partial fix, now order with docstring\r\n\r\n* fix docstring order?\r\n\r\n* more fixes\r\n\r\n* update\r\n\r\n* fix missing docstrings!\r\n\r\n* examples don't all work yet\r\n\r\n* fixup\r\n\r\n* nit\r\n\r\n* updated\r\n\r\n* hick\r\n\r\n* update\r\n\r\n* delete\r\n\r\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n* fix\r\n\r\n* all default\r\n\r\n* no local import\r\n\r\n* fix more diff\r\n\r\n* some fix related to \"safe imports\"\r\n\r\n* push fixed\r\n\r\n* add helper!\r\n\r\n* style\r\n\r\n* add a check\r\n\r\n* all by default\r\n\r\n* add the\r\n\r\n* update\r\n\r\n* FINALLY!\r\n\r\n* nit\r\n\r\n* fix config dependencies\r\n\r\n* man that is it\r\n\r\n* fix fix\r\n\r\n* update diffs\r\n\r\n* fix the last issue\r\n\r\n* re-default to all\r\n\r\n* alll the fixes\r\n\r\n* nice\r\n\r\n* fix properties vs setter\r\n\r\n* fixup\r\n\r\n* updates\r\n\r\n* update dependencies\r\n\r\n* make sure to install what needs to be installed\r\n\r\n* fixup\r\n\r\n* quick fix for now\r\n\r\n* fix!\r\n\r\n* fixup\r\n\r\n* update\r\n\r\n* update\r\n\r\n* updates\r\n\r\n* whitespaces\r\n\r\n* nit\r\n\r\n* fix\r\n\r\n* simplify everything, and make it file agnostic (should work for image processors)\r\n\r\n* style\r\n\r\n* finish fixing all import issues\r\n\r\n* fixup\r\n\r\n* empty modeling should not be written!\r\n\r\n* Add logic to find who depends on what\r\n\r\n* update\r\n\r\n* cleanup\r\n\r\n* update\r\n\r\n* update gemma to support positions\r\n\r\n* some small nits\r\n\r\n* this is the correct docstring for gemma2\r\n\r\n* fix merging of docstrings\r\n\r\n* update\r\n\r\n* fixup\r\n\r\n* update\r\n\r\n* take doc into account\r\n\r\n* styling\r\n\r\n* update\r\n\r\n* fix hidden activation\r\n\r\n* more fixes\r\n\r\n* final fixes!\r\n\r\n* fixup\r\n\r\n* fixup instruct  blip video\r\n\r\n* update\r\n\r\n* fix bugs\r\n\r\n* align gemma2 with the rest as well\r\n\r\n* updats\r\n\r\n* revert\r\n\r\n* update\r\n\r\n* more reversiom\r\n\r\n* grind\r\n\r\n* more\r\n\r\n* arf\r\n\r\n* update\r\n\r\n* order will matter\r\n\r\n* finish del stuff\r\n\r\n* update\r\n\r\n* rename to modular\r\n\r\n* fixup\r\n\r\n* nits\r\n\r\n* update makefile\r\n\r\n* fixup\r\n\r\n* update order of the checks!\r\n\r\n* fix\r\n\r\n* fix docstring that has a call inside\r\n\r\n* fiix conversion check\r\n\r\n* style\r\n\r\n* add some initial documentation\r\n\r\n* update\r\n\r\n* update doc\r\n\r\n* some fixup\r\n\r\n* updates\r\n\r\n* yups\r\n\r\n* Mostly todo gimme a minut\r\n\r\n* update\r\n\r\n* fixup\r\n\r\n* revert some stuff\r\n\r\n* Review docs for the modular transformers (#33472)\r\n\r\nDocs\r\n\r\n* good update\r\n\r\n* fixup\r\n\r\n* mmm current updates lead to this code\r\n\r\n* okay, this fixes it\r\n\r\n* cool\r\n\r\n* fixes\r\n\r\n* update\r\n\r\n* nit\r\n\r\n* updates\r\n\r\n* nits\r\n\r\n* fix doc\r\n\r\n* update\r\n\r\n* revert bad changes\r\n\r\n* update\r\n\r\n* updates\r\n\r\n* proper update\r\n\r\n* update\r\n\r\n* update?\r\n\r\n* up\r\n\r\n* update\r\n\r\n* cool\r\n\r\n* nits\r\n\r\n* nits\r\n\r\n* bon bon\r\n\r\n* fix\r\n\r\n* ?\r\n\r\n* minimise changes\r\n\r\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n* updates?\r\n\r\n* fixed gemma2\r\n\r\n* kind of a hack\r\n\r\n* nits\r\n\r\n* update\r\n\r\n* remove `diffs` in favor of `modular`\r\n\r\n* fix make fix copies\r\n\r\n---------\r\n\r\nCo-authored-by: Lysandre Debut <hi@lysand.re>\r\n\r\n* Fix CIs post merging modular transformers (#33681)\r\n\r\nupdate\r\n\r\n* Fixed docstring for cohere model regarding unavailability of prune_heâ€¦ (#33253)\r\n\r\n* Fixed docstring for cohere model regarding unavailability of prune_head() methods\r\n\r\nThe docstring mentions that cohere model supports prune_heads() methods. I have fixed the docstring by explicitly mentioning that it doesn't support that functionality.\r\n\r\n* Update src/transformers/models/cohere/modeling_cohere.py\r\n\r\n---------\r\n\r\nCo-authored-by: Lysandre Debut <hi@lysand.re>\r\n\r\n* Generation tests: update imagegpt input name, remove unused functions (#33663)\r\n\r\n* Improve Error Messaging for Flash Attention 2 on CPU (#33655)\r\n\r\nUpdate flash-attn error message on CPU\r\n\r\nRebased to latest branch\r\n\r\n* Gemma2: fix config initialization (`cache_implementation`) (#33684)\r\n\r\n* Fix ByteLevel alphabet missing when Sequence pretokenizer is used (#33556)\r\n\r\n* Fix ByteLevel alphabet missing when Sequence pretokenizer is used\r\n\r\n* Fixed formatting with `ruff`.\r\n\r\n* Uniformize kwargs for image-text-to-text processors (#32544)\r\n\r\n* uniformize FUYU processor kwargs\r\n\r\n* Uniformize instructblip processor kwargs\r\n\r\n* Fix processor kwargs and tests Fuyu, InstructBlip, Kosmos2\r\n\r\n* Uniformize llava_next processor\r\n\r\n* Fix save_load test for processor with chat_template only as extra init args\r\n\r\n* Fix import Unpack\r\n\r\n* Fix Fuyu Processor import\r\n\r\n* Fix FuyuProcessor import\r\n\r\n* Fix FuyuProcessor\r\n\r\n* Add defaults for specific kwargs kosmos2\r\n\r\n* Fix Udop to return BatchFeature instead of BatchEncoding and uniformize kwargs\r\n\r\n* Add tests processor Udop\r\n\r\n* remove Copied from in processing Udop as change of input orders caused by BatchEncoding -> BatchFeature\r\n\r\n* Fix overwrite tests kwargs processors\r\n\r\n* Add warnings and BC for changes in processor inputs order, change docs, add BC for text_pair as arg for Udop\r\n\r\n* Fix processing test fuyu\r\n\r\n* remove unnecessary pad_token check in instructblip ProcessorTest\r\n\r\n* Fix BC tests and cleanup\r\n\r\n* FIx imports fuyu\r\n\r\n* Uniformize Pix2Struct\r\n\r\n* Fix wrong name for FuyuProcessorKwargs\r\n\r\n* Fix slow tests reversed inputs align fuyu llava-next, change udop warning\r\n\r\n* Fix wrong logging import udop\r\n\r\n* Add check images text input order\r\n\r\n* Fix copies\r\n\r\n* change text pair handling when positional arg\r\n\r\n* rebase on main, fix imports in test_processing_common\r\n\r\n* remove optional args and udop uniformization from this PR\r\n\r\n* fix failing tests\r\n\r\n* remove unnecessary test, fix processing utils and test processing common\r\n\r\n* cleanup Unpack\r\n\r\n* cleanup\r\n\r\n* fix conflict grounding dino\r\n\r\n* ðŸš¨ðŸš¨ Setting default behavior of assisted decoding (#33657)\r\n\r\n* tests: fix pytorch tensor placement errors (#33485)\r\n\r\nThis commit fixes the following errors:\r\n* Fix \"expected all tensors to be on the same device\" error\r\n* Fix \"can't convert device type tensor to numpy\"\r\n\r\nAccording to pytorch documentation torch.Tensor.numpy(force=False)\r\nperforms conversion only if tensor is on CPU (plus few other restrictions)\r\nwhich is not the case. For our case we need force=True since we just\r\nneed a data and don't care about tensors coherency.\r\n\r\nFixes: #33517\r\nSee: https://pytorch.org/docs/2.4/generated/torch.Tensor.numpy.html\r\n\r\nSigned-off-by: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>\r\n\r\n* bump tokenizers, fix added tokens fast (#32535)\r\n\r\n* update based on tokenizers release\r\n\r\n* update\r\n\r\n* nits\r\n\r\n* update\r\n\r\n* revert re addition\r\n\r\n* don't break that yet\r\n\r\n* fmt\r\n\r\n* revert unwanted\r\n\r\n* update tokenizers version\r\n\r\n* update dep table\r\n\r\n* update\r\n\r\n* update in conversion script as well\r\n\r\n* some fix\r\n\r\n* revert\r\n\r\n* fully revert\r\n\r\n* fix training\r\n\r\n* remove set trace\r\n\r\n* fixup\r\n\r\n* update\r\n\r\n* update\r\n\r\n* [Pixtral] Improve docs, rename model (#33491)\r\n\r\n* Improve docs, rename model\r\n\r\n* Fix style\r\n\r\n* Update repo id\r\n\r\n* fix code quality after merge\r\n\r\n* HFQuantizer implementation for compressed-tensors library (#31704)\r\n\r\n* Add compressed-tensors HFQuantizer implementation\r\n\r\n* flag serializable as False\r\n\r\n* run\r\n\r\n* revive lines deleted by ruff\r\n\r\n* fixes to load+save from sparseml, edit config to quantization_config, and load back\r\n\r\n* address satrat comment\r\n\r\n* compressed_tensors to compressed-tensors and revert back is_serializable\r\n\r\n* rename quant_method from sparseml to compressed-tensors\r\n\r\n* tests\r\n\r\n* edit tests\r\n\r\n* clean up tests\r\n\r\n* make style\r\n\r\n* cleanup\r\n\r\n* cleanup\r\n\r\n* add test skip for when compressed tensors is not installed\r\n\r\n* remove pydantic import + style\r\n\r\n* delay torch import in test\r\n\r\n* initial docs\r\n\r\n* update main init for compressed tensors config\r\n\r\n* make fix-copies\r\n\r\n* docstring\r\n\r\n* remove fill_docstring\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\n\r\n* review comments\r\n\r\n* review comments\r\n\r\n* comments - suppress warnings on state dict load, tests, fixes\r\n\r\n* bug-fix - remove unnecessary call to apply quant lifecycle\r\n\r\n* run_compressed compatability\r\n\r\n* revert changes not needed for compression\r\n\r\n* no longer need unexpected keys fn\r\n\r\n* unexpected keys not needed either\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\n\r\n* add to_diff_dict\r\n\r\n* update docs and expand testing\r\n\r\n* Update _toctree.yml with compressed-tensors\r\n\r\n* Update src/transformers/utils/quantization_config.py\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\r\n\r\n* update doc\r\n\r\n* add note about saving a loaded model\r\n\r\n---------\r\n\r\nCo-authored-by: George Ohashi <george@neuralmagic.com>\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\nCo-authored-by: Sara Adkins <sara@neuralmagic.com>\r\nCo-authored-by: Sara Adkins <sara.adkins65@gmail.com>\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\r\nCo-authored-by: Dipika Sikka <ds3822@columbia.edu>\r\nCo-authored-by: Dipika <dipikasikka1@gmail.com>\r\n\r\n* update model card for opt\r\n\r\n* add batch size to inference table\r\n\r\n* [slow-run] opt\r\n\r\n* [run-slow] opt\r\n\r\n---------\r\n\r\nSigned-off-by: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>\r\nCo-authored-by: Avishai Elmakies <avishai.elma@cs.huji.ac.il>\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\nCo-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>\r\nCo-authored-by: chengchengpei <5881383+chengchengpei@users.noreply.github.com>\r\nCo-authored-by: Isotr0py <2037008807@qq.com>\r\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\r\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\r\nCo-authored-by: jiqing-feng <jiqing.feng@intel.com>\r\nCo-authored-by: Aarni Koskela <akx@iki.fi>\r\nCo-authored-by: Titus von Koeller <9048635+Titus-von-Koeller@users.noreply.github.com>\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\r\nCo-authored-by: Tibor Reiss <75096465+tibor-reiss@users.noreply.github.com>\r\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\r\nCo-authored-by: Lysandre Debut <hi@lysand.re>\r\nCo-authored-by: Muhammad Naufil <m.naufil1@gmail.com>\r\nCo-authored-by: sizhky <yyeshr@gmail.com>\r\nCo-authored-by: Umar Butler <umar@umar.au>\r\nCo-authored-by: Jonathan Mamou <jonathan.mamou@intel.com>\r\nCo-authored-by: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>\r\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\r\nCo-authored-by: Arthur Zucker <arthur.zucker@gmail.com>\r\nCo-authored-by: Benjamin Fineran <bfineran@users.noreply.github.com>\r\nCo-authored-by: George Ohashi <george@neuralmagic.com>\r\nCo-authored-by: Sara Adkins <sara@neuralmagic.com>\r\nCo-authored-by: Sara Adkins <sara.adkins65@gmail.com>\r\nCo-authored-by: Dipika Sikka <ds3822@columbia.edu>\r\nCo-authored-by: Dipika <dipikasikka1@gmail.com>",
    "sha": "a265600c6010ceb3dee8904ba07587f32b052751",
    "files": [
        {
            "sha": "c82064bae894f7f56fe1a4304cac6ef0fb02326e",
            "filename": "docs/source/en/model_doc/opt.md",
            "status": "modified",
            "additions": 67,
            "deletions": 0,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/a265600c6010ceb3dee8904ba07587f32b052751/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a265600c6010ceb3dee8904ba07587f32b052751/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md?ref=a265600c6010ceb3dee8904ba07587f32b052751",
            "patch": "@@ -110,6 +110,73 @@ Below is an expected speedup diagram that compares pure inference time between t\n </div>\n \n \n+### Using Scaled Dot Product Attention (SDPA)\n+PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\n+encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n+[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n+or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n+page for more information.\n+\n+SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n+`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n+\n+```python\n+from transformers import OPTForCausalLM\n+model = OPTForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n+...\n+```\n+\n+For the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n+\n+On a local benchmark (L40S-45GB, PyTorch 2.4.0, OS Debian GNU/Linux 11) using `float16` with\n+[facebook/opt-350m](https://huggingface.co/facebook/opt-350m), we saw the\n+following speedups during training and inference.\n+\n+### Training\n+\n+|    batch_size |    seq_len |  Time per batch (eager - s)   |    Time per batch (sdpa - s) |  Speedup (%)   |  Eager peak mem (MB)   |    sdpa peak mem (MB) |  Mem saving (%)   |\n+|--------------:|-----------:|:------------------------------|-----------------------------:|:---------------|:-----------------------|----------------------:|:------------------|\n+|             1 |        128 | 0.047                         |                        0.037 | 26.360         | 1474.611               |               1474.32 | 0.019             |\n+|             1 |        256 | 0.046                         |                        0.037 | 24.335         | 1498.541               |               1499.49 | -0.063            |\n+|             1 |        512 | 0.046                         |                        0.037 | 24.959         | 1973.544               |               1551.35 | 27.215            |\n+|             1 |       1024 | 0.062                         |                        0.038 | 65.135         | 4867.113               |               1698.35 | 186.578           |\n+|             1 |       2048 | 0.230                         |                        0.039 | 483.933        | 15662.224              |               2715.75 | 476.718           |\n+|             2 |        128 | 0.045                         |                        0.037 | 20.455         | 1498.164               |               1499.49 | -0.089            |\n+|             2 |        256 | 0.046                         |                        0.037 | 24.027         | 1569.367               |               1551.35 | 1.161             |\n+|             2 |        512 | 0.045                         |                        0.037 | 20.965         | 3257.074               |               1698.35 | 91.778            |\n+|             2 |       1024 | 0.122                         |                        0.038 | 225.958        | 9054.405               |               2715.75 | 233.403           |\n+|             2 |       2048 | 0.464                         |                        0.067 | 593.646        | 30572.058              |               4750.55 | 543.548           |\n+|             4 |        128 | 0.045                         |                        0.037 | 21.918         | 1549.448               |               1551.35 | -0.123            |\n+|             4 |        256 | 0.044                         |                        0.038 | 18.084         | 2451.768               |               1698.35 | 44.361            |\n+|             4 |        512 | 0.069                         |                        0.037 | 84.421         | 5833.180               |               2715.75 | 114.791           |\n+|             4 |       1024 | 0.262                         |                        0.062 | 319.475        | 17427.842              |               4750.55 | 266.860           |\n+|             4 |       2048 | OOM                           |                        0.062 | Eager OOM      | OOM                    |               4750.55 | Eager OOM         |\n+|             8 |        128 | 0.044                         |                        0.037 | 18.436         | 2049.115               |               1697.78 | 20.694            |\n+|             8 |        256 | 0.048                         |                        0.036 | 32.887         | 4222.567               |               2715.75 | 55.484            |\n+|             8 |        512 | 0.153                         |                        0.06  | 154.862        | 10985.391              |               4750.55 | 131.245           |\n+|             8 |       1024 | 0.526                         |                        0.122 | 330.697        | 34175.763              |               8821.18 | 287.428           |\n+|             8 |       2048 | OOM                           |                        0.122 | Eager OOM      | OOM                    |               8821.18 | Eager OOM         |\n+\n+### Inference\n+\n+|    batch_size |    seq_len |    Per token latency eager (ms) |    Per token latency SDPA (ms) |    Speedup (%) |    Mem eager (MB) |    Mem BT (MB) |    Mem saved (%) |\n+|--------------:|-----------:|--------------------------------:|-------------------------------:|---------------:|------------------:|---------------:|-----------------:|\n+|             1 |        128 |                          11.634 |                          8.647 |         34.546 |           717.676 |        717.674 |            0     |\n+|             1 |        256 |                          11.593 |                          8.86  |         30.851 |           742.852 |        742.845 |            0.001 |\n+|             1 |        512 |                          11.515 |                          8.816 |         30.614 |           798.232 |        799.593 |           -0.17  |\n+|             1 |       1024 |                          11.556 |                          8.915 |         29.628 |           917.265 |        895.538 |            2.426 |\n+|             2 |        128 |                          12.724 |                         11.002 |         15.659 |           762.434 |        762.431 |            0     |\n+|             2 |        256 |                          12.704 |                         11.063 |         14.83  |           816.809 |        816.733 |            0.009 |\n+|             2 |        512 |                          12.757 |                         10.947 |         16.535 |           917.383 |        918.339 |           -0.104 |\n+|             2 |       1024 |                          13.018 |                         11.018 |         18.147 |          1162.65  |       1114.81  |            4.291 |\n+|             4 |        128 |                          12.739 |                         10.959 |         16.243 |           856.335 |        856.483 |           -0.017 |\n+|             4 |        256 |                          12.718 |                         10.837 |         17.355 |           957.298 |        957.674 |           -0.039 |\n+|             4 |        512 |                          12.813 |                         10.822 |         18.393 |          1158.44  |       1158.45  |           -0.001 |\n+|             4 |       1024 |                          13.416 |                         11.06  |         21.301 |          1653.42  |       1557.19  |            6.18  |\n+|             8 |        128 |                          12.763 |                         10.891 |         17.193 |          1036.13  |       1036.51  |           -0.036 |\n+|             8 |        256 |                          12.89  |                         11.104 |         16.085 |          1236.98  |       1236.87  |            0.01  |\n+|             8 |        512 |                          13.327 |                         10.939 |         21.836 |          1642.29  |       1641.78  |            0.031 |\n+|             8 |       1024 |                          15.181 |                         11.175 |         35.848 |          2634.98  |       2443.35  |            7.843 |\n \n ## OPTConfig\n "
        },
        {
            "sha": "cf2dac617ffa3d470f73d3a2ee8a1b1f820ab3bd",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a265600c6010ceb3dee8904ba07587f32b052751/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a265600c6010ceb3dee8904ba07587f32b052751/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=a265600c6010ceb3dee8904ba07587f32b052751",
            "patch": "@@ -246,6 +246,7 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [NLLB](https://huggingface.co/docs/transformers/model_doc/nllb)\n * [OLMo](https://huggingface.co/docs/transformers/model_doc/olmo#transformers.OlmoModel)\n * [OLMoE](https://huggingface.co/docs/transformers/model_doc/olmoe#transformers.OlmoeModel)\n+* [OPT](https://huggingface.co/docs/transformers/en/model_doc/opt)\n * [PaliGemma](https://huggingface.co/docs/transformers/model_doc/paligemma#transformers.PaliGemmaForConditionalGeneration)\n * [Phi](https://huggingface.co/docs/transformers/model_doc/phi#transformers.PhiModel)\n * [Phi3](https://huggingface.co/docs/transformers/model_doc/phi3#transformers.Phi3Model)"
        },
        {
            "sha": "e4ef510f099d665e2a69e1b45c2f7c0dfd8cf4b2",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 152,
            "deletions": 25,
            "changes": 177,
            "blob_url": "https://github.com/huggingface/transformers/blob/a265600c6010ceb3dee8904ba07587f32b052751/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a265600c6010ceb3dee8904ba07587f32b052751/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=a265600c6010ceb3dee8904ba07587f32b052751",
            "patch": "@@ -23,7 +23,10 @@\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\n+from ...modeling_attn_mask_utils import (\n+    _prepare_4d_causal_attention_mask,\n+    _prepare_4d_causal_attention_mask_for_sdpa,\n+)\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -121,7 +124,7 @@ def __init__(\n         self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=self.enable_bias)\n         self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=self.enable_bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n+    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int) -> torch.Tensor:\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n \n     def forward(\n@@ -368,9 +371,108 @@ def forward(\n         return attn_output, attn_weights_reshaped, past_key_value\n \n \n+class OPTSdpaAttention(OPTAttention):\n+    \"\"\"\n+    OPT sdpa attention module. This module inherits from `OPTAttention` as the weights of the module stays untouched.\n+    The only required change would be on the forward pass where it needs to correctly call the public API of sdpa\n+    attention and deal with padding tokens in case the input contains any of them.\n+    \"\"\"\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        key_value_states: Optional[torch.Tensor] = None,\n+        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        layer_head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+        position_ids: Optional[torch.Tensor] = None,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        if output_attentions or layer_head_mask is not None:\n+            logger.warning_once(\n+                \"OPTModel is using SDPA attention, which currently does not support output_attentions=True.\"\n+                'failing back to eager attention. remove warning using attn_implementation=\"eager\".'\n+            )\n+\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                attention_mask=attention_mask,\n+                layer_head_mask=layer_head_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                key_value_states=key_value_states,\n+            )  # TODO after merge add position_ids=position_ids\n+        is_cross_attention = key_value_states is not None\n+\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states) * self.scaling\n+        query_states = self._shape(query_states, -1, bsz)\n+\n+        # get key, value proj\n+        if is_cross_attention and past_key_value is not None:\n+            # reuse k,v, cross_attentions\n+            key_states = past_key_value[0]\n+            value_states = past_key_value[1]\n+        elif is_cross_attention:\n+            # cross_attentions\n+            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n+            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n+        elif past_key_value is not None:\n+            # reuse k, v, self_attention\n+            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n+            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n+            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+        else:\n+            # self_attention\n+            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n+            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+\n+        if self.is_decoder:\n+            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n+            # Further calls to cross_attention layer can then reuse all cross-attention\n+            # key/value_states (first \"if\" case)\n+            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n+            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n+            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n+            # if encoder bi-directional self-attention `past_key_value` is always `None`\n+            past_key_value = (key_states, value_states)\n+\n+        # shape now is (bsz, num_heads, seq_len, head_dim), all are continuous\n+\n+        causal_mask = attention_mask\n+        if attention_mask is not None:\n+            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n+\n+        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+        is_causal = True if causal_mask is None and q_len > 1 else False\n+\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attn_mask=causal_mask,\n+            dropout_p=self.dropout if self.training else 0.0,\n+            is_causal=is_causal,\n+            # this model uses the scaling factor in the query projection for some reason, but not in Q@K^T\n+            # so we need to scale to remove scaling in SDPA to have similar results with eager.\n+            # Maybe needs a change in the model to remove scaling in query projection\n+            scale=1.0,\n+        )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output = attn_output.view(bsz, q_len, -1)\n+        attn_output = self.out_proj(attn_output)\n+\n+        return attn_output, None, past_key_value\n+\n+\n OPT_ATTENTION_CLASSES = {\n     \"eager\": OPTAttention,\n     \"flash_attention_2\": OptFlashAttention2,\n+    \"sdpa\": OPTSdpaAttention,\n }\n \n \n@@ -499,6 +601,7 @@ class OPTPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"OPTDecoderLayer\"]\n     _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n \n     def _init_weights(self, module):\n         std = self.config.init_std\n@@ -620,6 +723,7 @@ def __init__(self, config: OPTConfig):\n \n         self.layers = nn.ModuleList([OPTDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n         self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n+        self._use_sdpa = config._attn_implementation == \"sdpa\"\n \n         self.gradient_checkpointing = False\n         # Initialize weights and apply final processing\n@@ -631,6 +735,49 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    def _update_causal_mask(\n+        self,\n+        inputs_embeds: torch.Tensor,\n+        input_shape: Tuple[int, int],\n+        past_key_values_length: int,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+    ):\n+        \"\"\"\n+        Updates the causal mask for the decoder.\n+        \"\"\"\n+        batch_size, seq_length = input_shape\n+        mask_seq_length = past_key_values_length + seq_length\n+        if self._use_flash_attention_2:\n+            # 2d mask is passed through the layers\n+            causal_attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n+            attention_mask = (\n+                torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n+                if attention_mask is None\n+                else attention_mask\n+            )\n+\n+            return causal_attention_mask, attention_mask\n+\n+        if attention_mask is None:\n+            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n+        elif attention_mask.shape[1] != mask_seq_length:\n+            raise ValueError(\n+                f\"The provided attention mask has length {attention_mask.shape[1]}, but its length should be \"\n+                f\"{mask_seq_length} (sum of the lengths of current and past inputs)\"\n+            )\n+        if self._use_sdpa and not output_attentions and head_mask is None:\n+            causal_attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n+                attention_mask, input_shape, inputs_embeds, past_key_values_length\n+            )\n+        else:\n+            causal_attention_mask = _prepare_4d_causal_attention_mask(\n+                attention_mask, input_shape, inputs_embeds, past_key_values_length\n+            )\n+\n+        return causal_attention_mask, attention_mask\n+\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -718,32 +865,12 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        batch_size, seq_length = input_shape\n         past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n-        # required mask seq length can be calculated via length of past\n-        mask_seq_length = past_key_values_length + seq_length\n \n+        causal_attention_mask, attention_mask = self._update_causal_mask(\n+            inputs_embeds, input_shape, past_key_values_length, attention_mask, head_mask, output_attentions\n+        )\n         # embed positions\n-        if self._use_flash_attention_2:\n-            # 2d mask is passed through the layers\n-            causal_attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n-            attention_mask = (\n-                torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n-                if attention_mask is None\n-                else attention_mask\n-            )\n-        else:\n-            # 4d mask is passed through the layers\n-            if attention_mask is None:\n-                attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n-            elif attention_mask.shape[1] != mask_seq_length:\n-                raise ValueError(\n-                    f\"The provided attention mask has length {attention_mask.shape[1]}, but its length should be \"\n-                    f\"{mask_seq_length} (sum of the lengths of current and past inputs)\"\n-                )\n-            causal_attention_mask = _prepare_4d_causal_attention_mask(\n-                attention_mask, input_shape, inputs_embeds, past_key_values_length\n-            )\n \n         if position_ids is None:\n             position_ids = torch.cumsum(attention_mask, dim=1)"
        },
        {
            "sha": "5ebf23d86a3238b59cea17e577ae974eb53adff2",
            "filename": "tests/models/opt/test_modeling_flax_opt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a265600c6010ceb3dee8904ba07587f32b052751/tests%2Fmodels%2Fopt%2Ftest_modeling_flax_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a265600c6010ceb3dee8904ba07587f32b052751/tests%2Fmodels%2Fopt%2Ftest_modeling_flax_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopt%2Ftest_modeling_flax_opt.py?ref=a265600c6010ceb3dee8904ba07587f32b052751",
            "patch": "@@ -70,6 +70,7 @@ def __init__(\n         embed_dim=16,\n         word_embed_proj_dim=16,\n         initializer_range=0.02,\n+        attn_implemetation=\"eager\",\n     ):\n         self.parent = parent\n         self.batch_size = batch_size\n@@ -92,6 +93,7 @@ def __init__(\n         self.word_embed_proj_dim = word_embed_proj_dim\n         self.initializer_range = initializer_range\n         self.is_encoder_decoder = False\n+        self.attn_implementation = attn_implemetation\n \n     def prepare_config_and_inputs(self):\n         input_ids = np.clip(ids_tensor([self.batch_size, self.seq_length - 1], self.vocab_size), 3, self.vocab_size)\n@@ -114,6 +116,7 @@ def prepare_config_and_inputs(self):\n             word_embed_proj_dim=self.word_embed_proj_dim,\n             initializer_range=self.initializer_range,\n             use_cache=False,\n+            attn_implementation=self.attn_implementation,\n         )\n         inputs_dict = prepare_opt_inputs_dict(config, input_ids)\n         return config, inputs_dict"
        },
        {
            "sha": "2093dfe685b3eec1331e8680e4594e85472a42a8",
            "filename": "tests/models/opt/test_modeling_opt.py",
            "status": "modified",
            "additions": 73,
            "deletions": 1,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/a265600c6010ceb3dee8904ba07587f32b052751/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a265600c6010ceb3dee8904ba07587f32b052751/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py?ref=a265600c6010ceb3dee8904ba07587f32b052751",
            "patch": "@@ -21,7 +21,14 @@\n import timeout_decorator  # noqa\n \n from transformers import OPTConfig, is_torch_available\n-from transformers.testing_utils import require_torch, require_torch_accelerator, require_torch_fp16, slow, torch_device\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_torch_accelerator,\n+    require_torch_fp16,\n+    require_torch_sdpa,\n+    slow,\n+    torch_device,\n+)\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -83,6 +90,7 @@ def __init__(\n         num_labels=3,\n         word_embed_proj_dim=16,\n         type_sequence_label_size=2,\n+        attn_implementation=\"eager\",\n     ):\n         self.parent = parent\n         self.batch_size = batch_size\n@@ -106,6 +114,7 @@ def __init__(\n         self.type_sequence_label_size = type_sequence_label_size\n         self.word_embed_proj_dim = word_embed_proj_dim\n         self.is_encoder_decoder = False\n+        self.attn_implementation = attn_implementation\n \n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size).clamp(\n@@ -135,6 +144,7 @@ def get_config(self):\n             embed_dim=self.embed_dim,\n             is_encoder_decoder=False,\n             word_embed_proj_dim=self.word_embed_proj_dim,\n+            attn_implementation=self.attn_implementation,\n         )\n \n     def get_pipeline_config(self):\n@@ -329,6 +339,68 @@ def test_opt_sequence_classification_model_for_multi_label(self):\n         result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n         self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n \n+    @require_torch_sdpa\n+    @slow\n+    def test_eager_matches_sdpa_generate(self):\n+        \"\"\"\n+        Overwritting the common test as the test is flaky on tiny models\n+        \"\"\"\n+        max_new_tokens = 30\n+\n+        tokenizer = GPT2Tokenizer.from_pretrained(\"facebook/opt-350M\")\n+\n+        texts = [\n+            \"hi here's a longer context, getting longer and\",\n+            \"Hello this is a very long sentence my friend, very long for real\",\n+            \"Today I am in Paris and\",\n+        ]\n+\n+        model_sdpa = OPTForCausalLM.from_pretrained(\n+            \"facebook/opt-350M\",\n+            torch_dtype=torch.float16,\n+            low_cpu_mem_usage=True,\n+            attn_implementation=\"sdpa\",\n+        ).to(torch_device)\n+\n+        self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+\n+        model_eager = OPTForCausalLM.from_pretrained(\n+            \"facebook/opt-350M\",\n+            torch_dtype=torch.float16,\n+            low_cpu_mem_usage=True,\n+            attn_implementation=\"eager\",\n+        ).to(torch_device)\n+\n+        self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+\n+        for _, submodule in model_eager.named_modules():\n+            if \"SdpaAttention\" in submodule.__class__.__name__:\n+                raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+        has_sdpa = False\n+        for _, submodule in model_sdpa.named_modules():\n+            if \"SdpaAttention\" in submodule.__class__.__name__:\n+                has_sdpa = True\n+                break\n+        if not has_sdpa:\n+            raise ValueError(\"The SDPA model should have SDPA attention layers\")\n+\n+        for padding_side in [\"left\", \"right\"]:\n+            tokenizer.padding_side = padding_side\n+            tokenizer.pad_token = tokenizer.eos_token\n+\n+            inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(torch_device)\n+\n+            res_eager = model_eager.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n+            res_sdpa = model_sdpa.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n+\n+            with self.subTest(f\"{padding_side}\"):\n+                torch.testing.assert_close(\n+                    res_eager,\n+                    res_sdpa,\n+                    msg=f\"\\n{tokenizer.batch_decode(res_eager)} \\nvs\\n{tokenizer.batch_decode(res_sdpa)}\",\n+                )\n+\n     @unittest.skip(reason=\"Does not work on the tiny model as we keep hitting edge cases.\")\n     def test_model_parallelism(self):\n         super().test_model_parallelism()"
        },
        {
            "sha": "39c38170e3f6330103ad89b3c6b5232aa5c5a5e7",
            "filename": "tests/models/opt/test_modeling_tf_opt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a265600c6010ceb3dee8904ba07587f32b052751/tests%2Fmodels%2Fopt%2Ftest_modeling_tf_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a265600c6010ceb3dee8904ba07587f32b052751/tests%2Fmodels%2Fopt%2Ftest_modeling_tf_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopt%2Ftest_modeling_tf_opt.py?ref=a265600c6010ceb3dee8904ba07587f32b052751",
            "patch": "@@ -66,6 +66,7 @@ def __init__(\n         bos_token_id=0,\n         embed_dim=16,\n         word_embed_proj_dim=16,\n+        attn_implementation=\"eager\",\n     ):\n         self.parent = parent\n         self.batch_size = batch_size\n@@ -87,6 +88,7 @@ def __init__(\n         self.embed_dim = embed_dim\n         self.word_embed_proj_dim = word_embed_proj_dim\n         self.is_encoder_decoder = False\n+        self.attn_implementation = attn_implementation\n \n     def prepare_config_and_inputs_for_common(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length - 1], self.vocab_size)\n@@ -108,6 +110,7 @@ def prepare_config_and_inputs_for_common(self):\n             embed_dim=self.embed_dim,\n             word_embed_proj_dim=self.word_embed_proj_dim,\n             is_encoder_decoder=False,\n+            attn_implementation=self.attn_implementation,\n             **self.config_updates,\n         )\n         inputs_dict = prepare_opt_inputs_dict(config, input_ids)"
        }
    ],
    "stats": {
        "total": 325,
        "additions": 299,
        "deletions": 26
    }
}