{
    "author": "Rocketknight1",
    "message": "Stop inheriting tests (again) (#42247)\n\n* Stop inheriting tests!\n\n* Just use a del instead\n\n* fixup\n\n* Stop using del!\n\n* make fixup",
    "sha": "16924cd33ad57c19a290121ca83f4ceefab8f1b7",
    "files": [
        {
            "sha": "1b126e0ccd8d58e742359c10f102e95e8e4ff4e0",
            "filename": "tests/models/cohere2/test_modeling_cohere2.py",
            "status": "modified",
            "additions": 18,
            "deletions": 6,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/16924cd33ad57c19a290121ca83f4ceefab8f1b7/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16924cd33ad57c19a290121ca83f4ceefab8f1b7/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py?ref=16924cd33ad57c19a290121ca83f4ceefab8f1b7",
            "patch": "@@ -34,8 +34,7 @@\n     torch_device,\n )\n \n-from ...models.cohere.test_modeling_cohere import CohereModelTest, CohereModelTester\n-from ...test_configuration_common import ConfigTester\n+from ...models.cohere.test_modeling_cohere import CohereModelTester\n \n \n if is_torch_available():\n@@ -46,6 +45,11 @@\n         Cohere2Model,\n     )\n \n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n \n class Cohere2ModelTester(CohereModelTester):\n     config_class = Cohere2Config\n@@ -55,7 +59,7 @@ class Cohere2ModelTester(CohereModelTester):\n \n \n @require_torch\n-class Cohere2ModelTest(CohereModelTest, unittest.TestCase):\n+class Cohere2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (Cohere2Model, Cohere2ForCausalLM) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n@@ -67,10 +71,21 @@ class Cohere2ModelTest(CohereModelTest, unittest.TestCase):\n     )\n     _is_stateful = True\n \n+    # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n+    # This is because we are hitting edge cases with the causal_mask buffer\n+    model_split_percents = [0.5, 0.7, 0.8]\n+\n     def setUp(self):\n         self.model_tester = Cohere2ModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=Cohere2Config, hidden_size=37)\n \n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n \n @slow\n @require_read_token\n@@ -269,6 +284,3 @@ def test_generation_beyond_sliding_window(self, attn_implementation: str):\n         output_text = tokenizer.batch_decode(out)\n \n         self.assertEqual(output_text, EXPECTED_COMPLETIONS)\n-\n-\n-del CohereModelTest, CohereModelTester  # So the parent tests don't run in this file too"
        },
        {
            "sha": "47ede51be516b40848a72e5a624c532953636351",
            "filename": "tests/models/granitemoehybrid/test_modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 229,
            "deletions": 5,
            "changes": 234,
            "blob_url": "https://github.com/huggingface/transformers/blob/16924cd33ad57c19a290121ca83f4ceefab8f1b7/tests%2Fmodels%2Fgranitemoehybrid%2Ftest_modeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16924cd33ad57c19a290121ca83f4ceefab8f1b7/tests%2Fmodels%2Fgranitemoehybrid%2Ftest_modeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoehybrid%2Ftest_modeling_granitemoehybrid.py?ref=16924cd33ad57c19a290121ca83f4ceefab8f1b7",
            "patch": "@@ -14,24 +14,32 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch GraniteMoeHybrid model.\"\"\"\n \n+import inspect\n+import tempfile\n import unittest\n \n import pytest\n+from pytest import mark\n \n from transformers import (\n     AutoTokenizer,\n+    DataCollatorWithFlattening,\n     GraniteMoeHybridConfig,\n     is_torch_available,\n )\n from transformers.testing_utils import (\n+    require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n     slow,\n     torch_device,\n )\n \n from ...generation.test_utils import GenerationTesterMixin\n-from ...models.bamba.test_modeling_bamba import BambaModelTest, BambaModelTester\n+from ...models.bamba.test_modeling_bamba import BambaModelTester\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin\n+from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n if is_torch_available():\n@@ -77,7 +85,7 @@ def get_config(self):\n \n \n @require_torch\n-class GraniteMoeHybridModelTest(BambaModelTest, GenerationTesterMixin, unittest.TestCase):\n+class GraniteMoeHybridModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     model_tester_class = GraniteMoeHybridModelTester\n     all_model_classes = (\n         (\n@@ -96,6 +104,225 @@ class GraniteMoeHybridModelTest(BambaModelTest, GenerationTesterMixin, unittest.\n         else {}\n     )\n \n+    # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n+    # This is because we are hitting edge cases with the causal_mask buffer\n+    model_split_percents = [0.5, 0.7, 0.8]\n+\n+    def _check_caches_are_equal(\n+        self, cache1: HybridMambaAttentionDynamicCache, cache2: HybridMambaAttentionDynamicCache\n+    ):\n+        if not isinstance(cache1, HybridMambaAttentionDynamicCache) or not isinstance(\n+            cache2, HybridMambaAttentionDynamicCache\n+        ):\n+            raise ValueError(\"The wrong cache is being used!\")\n+\n+        if not len(cache1) == len(cache2):\n+            raise ValueError(\"Both caches do not have the same number of layers.\")\n+\n+        num_layers = len(cache1)\n+        for idx in range(num_layers):\n+            torch.testing.assert_close(cache1.key_cache[idx], cache2.key_cache[idx])\n+            torch.testing.assert_close(cache1.value_cache[idx], cache2.value_cache[idx])\n+            torch.testing.assert_close(cache1.conv_states[idx], cache2.conv_states[idx])\n+            torch.testing.assert_close(cache1.ssm_states[idx], cache2.ssm_states[idx])\n+\n+    def setUp(self):\n+        self.model_tester = self.model_tester_class(self)\n+        self.config_tester = ConfigTester(self, config_class=self.model_tester.config_class, hidden_size=64)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_for_causal_lm(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_for_causal_lm(*config_and_inputs)\n+\n+    def test_decoder_model_past_with_large_inputs(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n+\n+    def test_attention_outputs(self):\n+        r\"\"\"\n+        Overriding the test_attention_outputs test as the Bamba model outputs attention only for its attention layers\n+        \"\"\"\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+\n+        seq_len = getattr(self.model_tester, \"seq_length\", None)\n+        encoder_seq_length = getattr(self.model_tester, \"encoder_seq_length\", seq_len)\n+        encoder_key_length = getattr(self.model_tester, \"key_length\", encoder_seq_length)\n+\n+        expected_num_attentions = self.model_tester.num_hidden_layers - len(self.model_tester.attn_layer_indices)\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), expected_num_attentions)\n+\n+            # check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), expected_num_attentions)\n+\n+            self.assertListEqual(\n+                list(attentions[0].shape[-3:]),\n+                [self.model_tester.num_attention_heads, encoder_seq_length, encoder_key_length],\n+            )\n+            out_len = len(outputs)\n+\n+            # Check attention is always last and order is fine\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            added_hidden_states = 1\n+            self.assertEqual(out_len + added_hidden_states, len(outputs))\n+\n+            self_attentions = outputs.attentions\n+\n+            self.assertEqual(len(self_attentions), expected_num_attentions)\n+            self.assertListEqual(\n+                list(self_attentions[0].shape[-3:]),\n+                [self.model_tester.num_attention_heads, encoder_seq_length, encoder_key_length],\n+            )\n+\n+    def test_batching_equivalence(self):\n+        # need to disable the tril input mask\n+        orig = self.model_tester.use_input_mask\n+        self.model_tester.use_input_mask = False\n+        super().test_batching_equivalence()\n+        self.model_tester.use_input_mask = orig\n+\n+    @pytest.mark.generate\n+    def test_left_padding_compatibility(self):\n+        # TODO: document why a random attention mask causes this test to fail, but a full mask doesn't\n+        unpadded_custom_inputs = {\"attention_mask\": None}\n+        super().test_left_padding_compatibility(unpadded_custom_inputs=unpadded_custom_inputs)\n+\n+    @unittest.skip(\n+        \"Bamba requires additionally specifying position_ids, seq_idx, and FlashAttentionKwargs for padding-free training.\"\n+    )\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"Bamba requires additionally specifying position_ids, seq_idx, and FlashAttentionKwargs for padding-free training.\"\n+    )\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids_and_fa_kwargs(self):\n+        pass\n+\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    @slow\n+    @unittest.skip(\n+        \"NotImplementedError: seq_idx support requires fast path support. Please install mamba_ssm and causal_conv1d\"\n+    )\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids_seq_idx_and_fa_kwargs(self):\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        max_new_tokens = 30\n+\n+        for model_class in self.all_generative_model_classes:\n+            if not model_class._supports_flash_attn:\n+                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            if 0 not in inputs_dict.get(\"attention_mask\", []) or \"attention_mask\" not in inputs_dict:\n+                self.skipTest(\"Model dummy inputs should contain padding in their attention mask\")\n+\n+            dummy_input = inputs_dict[model_class.main_input_name]\n+            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n+                dummy_input = dummy_input.to(torch.float16)\n+\n+            # make sure that all models have enough positions for generation\n+            if hasattr(config, \"max_position_embeddings\"):\n+                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n+\n+            model = model_class(config)\n+            if \"position_ids\" not in inspect.signature(model.forward).parameters:\n+                self.skipTest(\"Model does not support position_ids\")\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+\n+                # ensure left padding, to adapt for some models\n+                if 0 in inputs_dict[\"attention_mask\"][:, -1]:\n+                    inputs_dict[\"attention_mask\"] = inputs_dict[\"attention_mask\"].flip(1)\n+                dummy_attention_mask = inputs_dict[\"attention_mask\"]\n+                inputs_dict[\"input_ids\"][~dummy_attention_mask.bool()] = config.get_text_config().pad_token_id\n+                # Ensure inputs_dict also has labels in it, as their presence/absence can induce\n+                # dtype conversions. This also lets us compare losses.\n+                labels = inputs_dict[\"input_ids\"].clone()\n+                # Mask padding tokens\n+                labels[~dummy_attention_mask.bool()] = -100\n+                # Also need to mask the first non-trivial token to match the padding-free batch.\n+                first_nonneg_idx = (labels >= 0).int().argmax(dim=1)\n+                labels[torch.arange(labels.size(0), device=labels.device), first_nonneg_idx] = -100\n+                inputs_dict[\"labels\"] = labels\n+\n+                model = (\n+                    model_class.from_pretrained(\n+                        tmpdirname,\n+                        dtype=torch.float16,\n+                        attn_implementation=\"flash_attention_2\",\n+                    )\n+                    .to(torch_device)\n+                    .eval()\n+                )\n+\n+                # flatten\n+                features = [\n+                    {\"input_ids\": i[a.bool()].tolist()}\n+                    for i, a in zip(inputs_dict[\"input_ids\"], inputs_dict[\"attention_mask\"])\n+                ]\n+\n+                # add position_ids + fa_kwargs + seq_idx\n+                data_collator = DataCollatorWithFlattening(\n+                    return_tensors=\"pt\", return_seq_idx=True, return_flash_attn_kwargs=True\n+                )\n+                batch = data_collator(features)\n+                batch_accelerator = {k: t.to(torch_device) if torch.is_tensor(t) else t for k, t in batch.items()}\n+\n+                res_padded = model(**inputs_dict)\n+                res_padfree = model(**batch_accelerator)\n+\n+                logits_padded = res_padded.logits[inputs_dict[\"attention_mask\"].bool()]\n+                logits_padfree = res_padfree.logits[0]\n+\n+                torch.testing.assert_close(logits_padded.argmax(-1), logits_padfree.argmax(-1), rtol=0, atol=0)\n+                # acceptable numerical instability\n+                tol = torch.finfo(torch.float16).eps\n+                torch.testing.assert_close(logits_padded, logits_padfree, rtol=tol, atol=tol)\n+\n+                loss_padded = res_padded.loss\n+                loss_padfree = res_padfree.loss\n+                torch.testing.assert_close(loss_padded, loss_padfree)\n+\n     def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config):\n         self.assertIsInstance(past_key_values, HybridMambaAttentionDynamicCache)\n \n@@ -178,6 +405,3 @@ def test_model_generation(self):\n         text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n \n         self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n-\n-\n-del BambaModelTest, BambaModelTester  # So the parent tests don't run in this file too"
        }
    ],
    "stats": {
        "total": 258,
        "additions": 247,
        "deletions": 11
    }
}