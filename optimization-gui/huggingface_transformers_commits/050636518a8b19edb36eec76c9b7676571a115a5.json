{
    "author": "MekkCyber",
    "message": "Fix : HQQ config when hqq not available (#35655)\n\n* fix\r\n\r\n* make style\r\n\r\n* adding require_hqq\r\n\r\n* make style",
    "sha": "050636518a8b19edb36eec76c9b7676571a115a5",
    "files": [
        {
            "sha": "aa26c2c3df867d6f974f468ad3508218a8ecc4fd",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/050636518a8b19edb36eec76c9b7676571a115a5/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/050636518a8b19edb36eec76c9b7676571a115a5/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=050636518a8b19edb36eec76c9b7676571a115a5",
            "patch": "@@ -87,6 +87,7 @@\n     is_gguf_available,\n     is_grokadamw_available,\n     is_hadamard_available,\n+    is_hqq_available,\n     is_ipex_available,\n     is_jieba_available,\n     is_jinja_available,\n@@ -1213,6 +1214,13 @@ def require_auto_gptq(test_case):\n     return unittest.skipUnless(is_auto_gptq_available(), \"test requires auto-gptq\")(test_case)\n \n \n+def require_hqq(test_case):\n+    \"\"\"\n+    Decorator for hqq dependency\n+    \"\"\"\n+    return unittest.skipUnless(is_hqq_available(), \"test requires hqq\")(test_case)\n+\n+\n def require_auto_awq(test_case):\n     \"\"\"\n     Decorator for auto_awq dependency"
        },
        {
            "sha": "1883e93deb58de1b30156d264be4a23a95c25d77",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/050636518a8b19edb36eec76c9b7676571a115a5/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/050636518a8b19edb36eec76c9b7676571a115a5/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=050636518a8b19edb36eec76c9b7676571a115a5",
            "patch": "@@ -224,6 +224,10 @@ def __init__(\n     ):\n         if is_hqq_available():\n             from hqq.core.quantize import BaseQuantizeConfig as HQQBaseQuantizeConfig\n+        else:\n+            raise ImportError(\n+                \"A valid HQQ version (>=0.2.1) is not available. Please follow the instructions to install it: `https://github.com/mobiusml/hqq/`.\"\n+            )\n \n         for deprecated_key in [\"quant_zero\", \"quant_scale\", \"offload_meta\"]:\n             if deprecated_key in kwargs:"
        },
        {
            "sha": "c25aada6ed48ed2f2268095687807a1e20f15feb",
            "filename": "tests/quantization/hqq/test_hqq.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/050636518a8b19edb36eec76c9b7676571a115a5/tests%2Fquantization%2Fhqq%2Ftest_hqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/050636518a8b19edb36eec76c9b7676571a115a5/tests%2Fquantization%2Fhqq%2Ftest_hqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fhqq%2Ftest_hqq.py?ref=050636518a8b19edb36eec76c9b7676571a115a5",
            "patch": "@@ -19,6 +19,7 @@\n from transformers import AutoModelForCausalLM, AutoTokenizer, HqqConfig\n from transformers.testing_utils import (\n     require_accelerate,\n+    require_hqq,\n     require_torch_gpu,\n     require_torch_multi_gpu,\n     slow,\n@@ -86,6 +87,7 @@ def check_forward(test_module, model, batch_size=1, context_size=1024):\n \n \n @require_torch_gpu\n+@require_hqq\n class HqqConfigTest(unittest.TestCase):\n     def test_to_dict(self):\n         \"\"\"\n@@ -100,6 +102,7 @@ def test_to_dict(self):\n @slow\n @require_torch_gpu\n @require_accelerate\n+@require_hqq\n class HQQTest(unittest.TestCase):\n     def tearDown(self):\n         cleanup()\n@@ -122,6 +125,7 @@ def test_fp16_quantized_model(self):\n @require_torch_gpu\n @require_torch_multi_gpu\n @require_accelerate\n+@require_hqq\n class HQQTestMultiGPU(unittest.TestCase):\n     def tearDown(self):\n         cleanup()\n@@ -144,6 +148,7 @@ def test_fp16_quantized_model_multipgpu(self):\n @slow\n @require_torch_gpu\n @require_accelerate\n+@require_hqq\n class HQQSerializationTest(unittest.TestCase):\n     def tearDown(self):\n         cleanup()"
        }
    ],
    "stats": {
        "total": 17,
        "additions": 17,
        "deletions": 0
    }
}