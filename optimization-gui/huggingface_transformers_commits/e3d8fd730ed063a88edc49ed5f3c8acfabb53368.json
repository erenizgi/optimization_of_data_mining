{
    "author": "BryanBradfo",
    "message": "docs(pixtral): Update Pixtral model card to new format (#40442)\n\n* docs(pixtral): Update Pixtral model card to new format\n\n* docs(pixtral): Change cuda into auto for device_map\n\n* docs(pixtral): Apply suggestions from review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* docs(pixtral): Apply suggestions from review, changing mistral-community into Mistral AI\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* docs(pixtral): Apply suggestions from review [!TIP] part\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* docs(pixtral): Finalize model card with tested code examples\n\nThis commit finalizes the update for the Pixtral model card.\n\n* Fix the hfoption by the right one\n\n* @BryanBradfo docs(pixtral): Changing the redirection of bitsandbytes\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* docs(pixtral): Add of ` to highlight the tokens\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* docs(pixtral): Move image block per final review\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "e3d8fd730ed063a88edc49ed5f3c8acfabb53368",
    "files": [
        {
            "sha": "55ba09084292b8c291009b70ec431516b33c95f6",
            "filename": "docs/source/en/model_doc/pixtral.md",
            "status": "modified",
            "additions": 87,
            "deletions": 35,
            "changes": 122,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3d8fd730ed063a88edc49ed5f3c8acfabb53368/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3d8fd730ed063a88edc49ed5f3c8acfabb53368/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md?ref=e3d8fd730ed063a88edc49ed5f3c8acfabb53368",
            "patch": "@@ -15,74 +15,126 @@ rendered properly in your Markdown viewer.\n -->\n *This model was released on 2024-09-17 and added to Hugging Face Transformers on 2024-09-14.*\n \n-# Pixtral\n \n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n </div>\n \n-## Overview\n-\n-The [Pixtral](https://huggingface.co/papers/2410.07073) model was released by the Mistral AI team in a [blog post](https://mistral.ai/news/pixtral-12b/). Pixtral is a multimodal version of [Mistral](mistral), incorporating a 400 million parameter vision encoder trained from scratch.\n-\n-The intro from the blog says the following:\n+# Pixtral\n \n-*Pixtral is trained to understand both natural images and documents, achieving 52.5% on the MMMU reasoning benchmark, surpassing a number of larger models. The model shows strong abilities in tasks such as chart and figure understanding, document question answering, multimodal reasoning and instruction following. Pixtral is able to ingest images at their natural resolution and aspect ratio, giving the user flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Unlike previous open-source models, Pixtral does not compromise on text benchmark performance to excel in multimodal tasks.*\n+[Pixtral](https://huggingface.co/papers/2410.07073) is a multimodal model trained to understand natural images and documents. It accepts images in their natural resolution and aspect ratio without resizing or padding due to it's 2D RoPE embeddings. In addition, Pixtral has a long 128K token context window for processing a large number of images. Pixtral couples a 400M vision encoder with a 12B Mistral Nemo decoder.\n \n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/pixtral_architecture.webp\"\n alt=\"drawing\" width=\"600\"/>\n \n <small> Pixtral architecture. Taken from the <a href=\"https://mistral.ai/news/pixtral-12b/\">blog post.</a> </small>\n \n-Tips:\n-\n-- Pixtral is a multimodal model, taking images and text as input, and producing text as output.\n-- This model follows the [Llava](llava) architecture. The model uses [`PixtralVisionModel`] for its vision encoder, and [`MistralForCausalLM`] for its language decoder.\n-- The main contribution is the 2d ROPE (rotary position embeddings) on the images, and support for arbitrary image sizes (the images are not padded together nor are they resized).\n-- Similar to [Llava](llava), the model internally replaces the `[IMG]` token placeholders by image embeddings from the vision encoder. The format for one or multiple prompts is the following:\n-```\n-\"<s>[INST][IMG]\\nWhat are the things I should be cautious about when I visit this place?[/INST]\"\n-```\n-Then, the processor will replace each `[IMG]` token with a number of `[IMG]` tokens that depend on the height and the width of each image. Each *row* of the image is separated by an `[IMG_BREAK]` token, and each image is separated by an `[IMG_END]` token. It's advised to use the `apply_chat_template` method of the processor, which takes care of all of this and formats the text for you. If you're using `transformers>=4.49.0`, you can also get a vectorized output from `apply_chat_template`. See the [usage section](#usage) for more info.\n-\n+You can find all the original Pixtral checkpoints under the [Mistral AI](https://huggingface.co/mistralai/models?search=pixtral) organization.\n \n-This model was contributed by [amyeroberts](https://huggingface.co/amyeroberts) and [ArthurZ](https://huggingface.co/ArthurZ). The original code can be found [here](https://github.com/vllm-project/vllm/pull/8377).\n+> [!TIP]\n+> This model was contributed by [amyeroberts](https://huggingface.co/amyeroberts) and [ArthurZ](https://huggingface.co/ArthurZ).\n+> Click on the Pixtral models in the right sidebar for more examples of how to apply Pixtral to different vision and language tasks.\n \n+<hfoptions id=\"usage\">\n \n-## Usage\n-\n-At inference time, it's advised to use the processor's `apply_chat_template` method, which correctly formats the prompt for the model:\n+<hfoption id=\"AutoModel\">\n \n ```python\n+import torch\n from transformers import AutoProcessor, LlavaForConditionalGeneration\n \n model_id = \"mistral-community/pixtral-12b\"\n+model = LlavaForConditionalGeneration.from_pretrained(model_id, dtype=\"auto\", device_map=\"auto\")\n processor = AutoProcessor.from_pretrained(model_id)\n-model = LlavaForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\")\n+\n+url_dog = \"https://picsum.photos/id/237/200/300\"\n+url_mountain = \"https://picsum.photos/seed/picsum/200/300\"\n \n chat = [\n     {\n       \"role\": \"user\", \"content\": [\n         {\"type\": \"text\", \"content\": \"Can this animal\"}, \n-        {\"type\": \"image\", \"url\": \"https://picsum.photos/id/237/200/300\"}, \n+        {\"type\": \"image\", \"url\": url_dog}, \n         {\"type\": \"text\", \"content\": \"live here?\"}, \n-        {\"type\": \"image\", \"url\": \"https://picsum.photos/seed/picsum/200/300\"}\n+        {\"type\": \"image\", \"url\" : url_mountain}\n       ]\n     }\n ]\n \n-inputs = processor.apply_chat_template(\n-    chat,\n-    add_generation_prompt=True,\n-    tokenize=True,\n-    return_dict=True,\n-    return_tensors=\"pt\"\n-).to(model.device)\n-\n+inputs = processor.apply_chat_template(chat, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors\"pt\").to(model.device)\n generate_ids = model.generate(**inputs, max_new_tokens=500)\n output = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n ```\n \n+</hfoption>\n+\n+</hfoptions>\n+\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n+\n+The example below uses [bitsandbytes](../quantization/bitsandbytes) to quantize the model to 4-bits.\n+\n+```python\n+import torch\n+import requests\n+from PIL import Image\n+from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n+\n+model_id = \"mistral-community/pixtral-12b\"\n+\n+quantization_config = BitsAndBytesConfig(\n+    load_in_4bit=True,\n+    bnb_4bit_quant_type=\"nf4\",\n+    bnb_4bit_compute_dtype=torch.bfloat16\n+)\n+\n+model = LlavaForConditionalGeneration.from_pretrained(\n+    model_id,\n+    quantization_config=quantization_config,\n+    device_map=\"auto\"\n+)\n+processor = AutoProcessor.from_pretrained(model_id)\n+\n+dog_url = \"https://picsum.photos/id/237/200/300\"\n+mountain_url = \"https://picsum.photos/seed/picsum/200/300\"\n+dog_image = Image.open(requests.get(dog_url, stream=True).raw)\n+mountain_image = Image.open(requests.get(mountain_url, stream=True).raw)\n+\n+chat = [\n+    {\n+      \"role\": \"user\", \"content\": [\n+        {\"type\": \"text\", \"text\": \"Can this animal\"},\n+        {\"type\": \"image\"},\n+        {\"type\": \"text\", \"text\": \"live here?\"},\n+        {\"type\": \"image\"}\n+      ]\n+    }\n+]\n+\n+prompt = processor.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n+inputs = processor(text=prompt, images=[dog_image, mountain_image], return_tensors=\"pt\")\n+\n+inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(model.dtype)\n+inputs = {k: v.to(model.device) for k, v in inputs.items()}\n+\n+generate_ids = model.generate(**inputs, max_new_tokens=100)\n+output = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n+print(output)\n+```\n+\n+## Notes\n+\n+- Pixtral uses [`PixtralVisionModel`] as the vision encoder and [`MistralForCausalLM`]  for its language decoder.\n+- The model internally replaces `[IMG]` token placeholders with image embeddings.\n+\n+    ```py\n+    \"<s>[INST][IMG]\\nWhat are the things I should be cautious about when I visit this place?[/INST]\"\n+    ```\n+\n+    The `[IMG]` tokens are replaced with a number of `[IMG]` tokens that depend on the height and width of each image. Each row of the image is separated by a `[IMG_BREAK]` token and each image is separated by a `[IMG_END]` token. Use the [`~Processor.apply_chat_template`] method to handle these tokens for you.\n+\n ## PixtralVisionConfig\n \n [[autodoc]] PixtralVisionConfig"
        }
    ],
    "stats": {
        "total": 122,
        "additions": 87,
        "deletions": 35
    }
}