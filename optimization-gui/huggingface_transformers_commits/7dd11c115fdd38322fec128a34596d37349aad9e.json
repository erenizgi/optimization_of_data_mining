{
    "author": "Cyrilvallez",
    "message": "Properly protect the is_compiling checks (#42304)\n\nfix",
    "sha": "7dd11c115fdd38322fec128a34596d37349aad9e",
    "files": [
        {
            "sha": "6b1ac41ddde179879e02521ffae629b83cbfb949",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7dd11c115fdd38322fec128a34596d37349aad9e/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7dd11c115fdd38322fec128a34596d37349aad9e/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=7dd11c115fdd38322fec128a34596d37349aad9e",
            "patch": "@@ -50,7 +50,7 @@\n     is_vision_available,\n     logging,\n )\n-from .utils.import_utils import is_rocm_platform\n+from .utils.import_utils import is_rocm_platform, is_torchdynamo_compiling\n \n \n if is_vision_available():\n@@ -343,7 +343,7 @@ def resize(\n         # This is a workaround to avoid a bug in torch.compile when dealing with uint8 on AMD MI3XX GPUs\n         # Tracked in PyTorch issue: https://github.com/pytorch/pytorch/issues/155209\n         # TODO: remove this once the bug is fixed (detected with torch==2.7.0+git1fee196, torchvision==0.22.0+9eb57cd)\n-        if torch.compiler.is_compiling() and is_rocm_platform():\n+        if is_torchdynamo_compiling() and is_rocm_platform():\n             return self.compile_friendly_resize(image, new_size, interpolation, antialias)\n         return F.resize(image, new_size, interpolation=interpolation, antialias=antialias)\n "
        },
        {
            "sha": "87da76281717da86ab88c824d7c2d15cb951851d",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/7dd11c115fdd38322fec128a34596d37349aad9e/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7dd11c115fdd38322fec128a34596d37349aad9e/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=7dd11c115fdd38322fec128a34596d37349aad9e",
            "patch": "@@ -40,6 +40,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.import_utils import is_torchdynamo_compiling\n from ..auto import AutoModel\n from .configuration_csm import CsmConfig, CsmDepthDecoderConfig\n from .generation_csm import CsmGenerationMixin\n@@ -458,7 +459,7 @@ def forward(\n             The last hidden state of the backbone model. Such input is required when the first codebook token (the one generated by the backbone model)\n             is provided in the `input_ids` argument.\n         \"\"\"\n-        if position_ids is not None and not torch.compiler.is_compiling():\n+        if position_ids is not None and not is_torchdynamo_compiling():\n             logger.warning_once(\n                 \"Custom `position_ids` were provided but will be ignored. CSM depth decoder automatically determines position_ids \"\n                 \"from `cache_position` and as it requires them to be identical across the batch, the provided position_ids will be ignored.\"\n@@ -485,7 +486,7 @@ def forward(\n             if backbone_last_hidden_state is not None:\n                 inputs_embeds[:, 0] = backbone_last_hidden_state\n             else:\n-                if not torch.compiler.is_compiling() and input_ids_are_first_codebook:\n+                if not is_torchdynamo_compiling() and input_ids_are_first_codebook:\n                     logger.warning(\n                         \"When the first codebook token is provided, `backbone_last_hidden_state` should also be provided for correct inference.\"\n                     )"
        },
        {
            "sha": "0ecb41071cd2cc055d6c4e4aab654449803f5296",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/7dd11c115fdd38322fec128a34596d37349aad9e/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7dd11c115fdd38322fec128a34596d37349aad9e/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=7dd11c115fdd38322fec128a34596d37349aad9e",
            "patch": "@@ -29,6 +29,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import ModelOutput, auto_docstring, can_return_tuple, logging\n+from ...utils.import_utils import is_torchdynamo_compiling\n from ..auto import AutoModel\n from ..llama.modeling_llama import (\n     LlamaAttention,\n@@ -178,7 +179,7 @@ def forward(\n             The last hidden state of the backbone model. Such input is required when the first codebook token (the one generated by the backbone model)\n             is provided in the `input_ids` argument.\n         \"\"\"\n-        if position_ids is not None and not torch.compiler.is_compiling():\n+        if position_ids is not None and not is_torchdynamo_compiling():\n             logger.warning_once(\n                 \"Custom `position_ids` were provided but will be ignored. CSM depth decoder automatically determines position_ids \"\n                 \"from `cache_position` and as it requires them to be identical across the batch, the provided position_ids will be ignored.\"\n@@ -205,7 +206,7 @@ def forward(\n             if backbone_last_hidden_state is not None:\n                 inputs_embeds[:, 0] = backbone_last_hidden_state\n             else:\n-                if not torch.compiler.is_compiling() and input_ids_are_first_codebook:\n+                if not is_torchdynamo_compiling() and input_ids_are_first_codebook:\n                     logger.warning(\n                         \"When the first codebook token is provided, `backbone_last_hidden_state` should also be provided for correct inference.\"\n                     )"
        },
        {
            "sha": "ebe4adb4448feca64f46ebf83dc66573513053cd",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/7dd11c115fdd38322fec128a34596d37349aad9e/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7dd11c115fdd38322fec128a34596d37349aad9e/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=7dd11c115fdd38322fec128a34596d37349aad9e",
            "patch": "@@ -38,6 +38,7 @@\n from ...utils.import_utils import (\n     is_mamba_ssm_available,\n     is_mambapy_available,\n+    is_torchdynamo_compiling,\n )\n from .configuration_falcon_mamba import FalconMambaConfig\n \n@@ -510,7 +511,7 @@ def forward(\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )\n-        if is_fast_path_available and \"cuda\" in self.x_proj.weight.device.type and not torch._dynamo.is_compiling():\n+        if is_fast_path_available and \"cuda\" in self.x_proj.weight.device.type and not is_torchdynamo_compiling():\n             return self.cuda_kernels_forward(hidden_states, cache_params, cache_position, attention_mask)\n         return self.slow_forward(hidden_states, cache_params, cache_position, attention_mask)\n "
        },
        {
            "sha": "dde3487d447c7605ab6eea9a5527f2bcc58c607c",
            "filename": "src/transformers/models/falcon_mamba/modular_falcon_mamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/7dd11c115fdd38322fec128a34596d37349aad9e/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7dd11c115fdd38322fec128a34596d37349aad9e/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py?ref=7dd11c115fdd38322fec128a34596d37349aad9e",
            "patch": "@@ -21,10 +21,7 @@\n \n from ...integrations.hub_kernels import lazy_load_kernel\n from ...utils import auto_docstring, logging\n-from ...utils.import_utils import (\n-    is_mamba_ssm_available,\n-    is_mambapy_available,\n-)\n+from ...utils.import_utils import is_mamba_ssm_available, is_mambapy_available, is_torchdynamo_compiling\n from ..mamba.configuration_mamba import MambaConfig\n from ..mamba.modeling_mamba import (\n     MambaBlock,\n@@ -533,7 +530,7 @@ def forward(\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )\n-        if is_fast_path_available and \"cuda\" in self.x_proj.weight.device.type and not torch._dynamo.is_compiling():\n+        if is_fast_path_available and \"cuda\" in self.x_proj.weight.device.type and not is_torchdynamo_compiling():\n             return self.cuda_kernels_forward(hidden_states, cache_params, cache_position, attention_mask)\n         return self.slow_forward(hidden_states, cache_params, cache_position, attention_mask)\n "
        },
        {
            "sha": "f1d639d16bbd369211bf6e97eb4526e30acc22f1",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7dd11c115fdd38322fec128a34596d37349aad9e/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7dd11c115fdd38322fec128a34596d37349aad9e/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=7dd11c115fdd38322fec128a34596d37349aad9e",
            "patch": "@@ -35,7 +35,7 @@\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.generic import check_model_inputs\n-from ...utils.import_utils import is_causal_conv1d_available\n+from ...utils.import_utils import is_causal_conv1d_available, is_torchdynamo_compiling\n from .configuration_lfm2 import Lfm2Config\n \n \n@@ -536,7 +536,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n     ):\n-        if is_fast_path_available and \"cuda\" in hidden_states.device.type and not torch._dynamo.is_compiling():\n+        if is_fast_path_available and \"cuda\" in hidden_states.device.type and not is_torchdynamo_compiling():\n             return self.cuda_kernels_forward(hidden_states, past_key_values, cache_position, attention_mask)\n         return self.slow_forward(hidden_states, past_key_values, cache_position, attention_mask)\n "
        },
        {
            "sha": "0075280e6ddbb3776a92f96f0c70b9dc19850b45",
            "filename": "src/transformers/models/lfm2/modular_lfm2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7dd11c115fdd38322fec128a34596d37349aad9e/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7dd11c115fdd38322fec128a34596d37349aad9e/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py?ref=7dd11c115fdd38322fec128a34596d37349aad9e",
            "patch": "@@ -24,7 +24,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n-from ...utils.import_utils import is_causal_conv1d_available\n+from ...utils.import_utils import is_causal_conv1d_available, is_torchdynamo_compiling\n from ..bamba.modeling_bamba import apply_mask_to_padding_states\n from ..gemma2.modeling_gemma2 import Gemma2RotaryEmbedding\n from ..llama.modeling_llama import (\n@@ -371,7 +371,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n     ):\n-        if is_fast_path_available and \"cuda\" in hidden_states.device.type and not torch._dynamo.is_compiling():\n+        if is_fast_path_available and \"cuda\" in hidden_states.device.type and not is_torchdynamo_compiling():\n             return self.cuda_kernels_forward(hidden_states, past_key_values, cache_position, attention_mask)\n         return self.slow_forward(hidden_states, past_key_values, cache_position, attention_mask)\n "
        },
        {
            "sha": "3ec5c581eca40e1d6959b17df068289859998ec3",
            "filename": "src/transformers/models/lfm2_moe/modeling_lfm2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7dd11c115fdd38322fec128a34596d37349aad9e/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7dd11c115fdd38322fec128a34596d37349aad9e/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py?ref=7dd11c115fdd38322fec128a34596d37349aad9e",
            "patch": "@@ -36,7 +36,7 @@\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.generic import check_model_inputs\n-from ...utils.import_utils import is_causal_conv1d_available\n+from ...utils.import_utils import is_causal_conv1d_available, is_torchdynamo_compiling\n from .configuration_lfm2_moe import Lfm2MoeConfig\n \n \n@@ -606,7 +606,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n     ):\n-        if is_fast_path_available and \"cuda\" in hidden_states.device.type and not torch._dynamo.is_compiling():\n+        if is_fast_path_available and \"cuda\" in hidden_states.device.type and not is_torchdynamo_compiling():\n             return self.cuda_kernels_forward(hidden_states, past_key_values, cache_position, attention_mask)\n         return self.slow_forward(hidden_states, past_key_values, cache_position, attention_mask)\n "
        },
        {
            "sha": "5686f7d8d6ac71e1da781d64e3371cd06631c998",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/7dd11c115fdd38322fec128a34596d37349aad9e/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7dd11c115fdd38322fec128a34596d37349aad9e/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=7dd11c115fdd38322fec128a34596d37349aad9e",
            "patch": "@@ -34,10 +34,7 @@\n     auto_docstring,\n     logging,\n )\n-from ...utils.import_utils import (\n-    is_mamba_ssm_available,\n-    is_mambapy_available,\n-)\n+from ...utils.import_utils import is_mamba_ssm_available, is_mambapy_available, is_torchdynamo_compiling\n from .configuration_mamba import MambaConfig\n \n \n@@ -444,7 +441,7 @@ def forward(\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )\n-        if is_fast_path_available and \"cuda\" in self.x_proj.weight.device.type and not torch._dynamo.is_compiling():\n+        if is_fast_path_available and \"cuda\" in self.x_proj.weight.device.type and not is_torchdynamo_compiling():\n             return self.cuda_kernels_forward(hidden_states, cache_params, cache_position, attention_mask)\n         return self.slow_forward(hidden_states, cache_params, cache_position, attention_mask)\n "
        }
    ],
    "stats": {
        "total": 43,
        "additions": 20,
        "deletions": 23
    }
}