{
    "author": "poke1024",
    "message": "Add callback to monitor progress in whisper transcription (#37483)\n\n* Add callback to monitor progress in whisper transcription\n\n* Added `` around variables, rewording\n\n* Add example of `monitor_progress`.\n\n---------\n\nCo-authored-by: Eric B <ebezzam@gmail.com>",
    "sha": "9b3203f47b5cabb5c318ce1458438847df23fb1d",
    "files": [
        {
            "sha": "93dd29ae0fed13d88e7be93ca8f9cfc0f199d1ce",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 36,
            "deletions": 2,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b3203f47b5cabb5c318ce1458438847df23fb1d/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b3203f47b5cabb5c318ce1458438847df23fb1d/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=9b3203f47b5cabb5c318ce1458438847df23fb1d",
            "patch": "@@ -410,6 +410,7 @@ def generate(\n         return_segments: bool = False,\n         return_dict_in_generate: Optional[bool] = None,\n         force_unique_generate_call: Optional[bool] = None,\n+        monitor_progress: Optional[Callable[[torch.Tensor], None]] = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -461,6 +462,7 @@ def generate(\n                 `FullyShardedDataParallel` and DeepSpeed ZeRO Stage 3).\n             return_timestamps (`bool`, *optional*):\n                 Whether to return the timestamps with the text. This enables the `WhisperTimestampsLogitsProcessor`.\n+                For audios longer than 30 seconds, it is necessary to set `return_timestamps=True`.\n             task (`str`, *optional*):\n                 Task to use for generation, either \"translate\" or \"transcribe\".\n             language (`str` or list of `str`, *optional*):\n@@ -533,14 +535,19 @@ def generate(\n             force_unique_generate_call (`bool`, *optional*):\n                 Whether to force a unique call to the underlying GenerationMixin's [`~generation.GenerationMixin.generate`] method. This is useful for assisted decoding and testing purposes to ensure\n                 that only one call to [`~generation.GenerationMixin.generate`] is made and therefore decoder input token ids and eos token ids are returned.\n+            monitor_progress (`Callable[[torch.Tensor], None]`, *optional*):\n+                If provided, this function can be called to report the progress of the audio transcription. The function\n+                takes a tensor argument `p` of shape `(n, 2)`, where `n` is the batch size. `p[i, 0]`  contains the\n+                index of the audio frame that is currently being transcribed for batch item `i`. `p[i, 1]` contains\n+                the total number of frames for batch item `i`. No return value is expected.\n             kwargs (`dict[str, Any]`, *optional*):\n                 Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be\n                 forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n                 specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.\n         Return:\n             [`~utils.ModelOutput`] or `dict[str, Any]` or `torch.LongTensor`:\n \n-                A:\n+                One of the following:\n                 - [`~utils.ModelOutput`] when `return_dict_in_generate=True` and (`return_timestamps=False` or `force_unique_generate_call=True`), including the decoder input ids and end of sequence id.\n                 - `dict[str, Any]` when (`return_dict_in_generate=True` and `return_timestamps=True`) or `return_segments=True` or `return_token_timestamps=True`.\n                 - `torch.LongTensor` in all other cases, excluding the decoder input ids and end of sequence id.\n@@ -586,13 +593,37 @@ def generate(\n         >>> inputs = inputs.to(\"cuda\", torch.float32)\n \n         >>> # transcribe audio to ids\n-        >>> generated_ids = model.generate(**inputs)\n+        >>> generated_ids = model.generate(**inputs, return_timestamps=True)\n \n         >>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)\n         >>> transcription[0]\n         \" Folks, if you watch the show, you know, I spent a lot of time right over there. Patiently and astutely scrutinizing the boxwood and mahogany chest set of the day's biggest stories developing the central headline pawns, definitely maneuvering an oso topical night to F6, fainting a classic Sicilian, nade door variation on the news, all the while seeing eight moves deep and patiently marshalling the latest press releases into a fisher's shows in Lip Nitsky attack that culminates in the elegant lethal slow-played, all-passant checkmate that is my nightly monologue. But sometimes, sometimes, folks, I. CHEERING AND APPLAUSE Sometimes I startle away, cubside down in the monkey bars of a condemned playground on a super fun site. Get all hept up on goofballs. Rummage that were discarded tag bag of defective toys. Yank out a fist bowl of disembodied doll limbs, toss them on a stained kid's place mat from a defunct dennies. set up a table inside a rusty cargo container down by the Wharf and challenged toothless drifters to the godless bughouse blitz of tournament that is my segment. Meanwhile.\"\n         ```\n \n+        The `monitor_progress` callback can be used to monitor the progress of the transcription:\n+        ```python\n+        >>> from tqdm import tqdm\n+\n+        >>> # prepare inputs like above\n+\n+        >>> # define a callback to monitor the progress of the transcription.\n+        >>> with tqdm(desc=\"Progress\") as pbar:\n+        >>>     def monitor_progress(p_batch):\n+        >>>         i = torch.argmax(p_batch[:, 1])\n+        >>>         p = p_batch[i].detach().cpu()\n+        >>>         pbar.total = int(p[1])\n+        >>>         pbar.n = int(p[0])\n+        >>>         pbar.update()\n+\n+        >>>     # transcribe audio to ids\n+        >>>     generated_ids = model.generate(**inputs, return_timestamps=True, monitor_progress=monitor_progress)\n+\n+        >>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)\n+        >>> transcription[0]\n+        Progress:  95%|█████████████████████████████████████████████████████████████████████████████████████████████████▎    | 8497/8901 [00:04<00:00, 2052.79it/s]\n+        \" Folks, if you watch the show, you know, I spent a lot of time right over there. Patiently and astutely scrutinizing the boxwood and mahogany chest set of the day's biggest stories developing the central headline pawns, definitely maneuvering an oso topical night to F6, fainting a classic Sicilian, nade door variation on the news, all the while seeing eight moves deep and patiently marshalling the latest press releases into a fisher's shows in Lip Nitsky attack that culminates in the elegant lethal slow-played, all-passant checkmate that is my nightly monologue. But sometimes, sometimes, folks, I. CHEERING AND APPLAUSE Sometimes I startle away, cubside down in the monkey bars of a condemned playground on a super fun site. Get all hept up on goofballs. Rummage that were discarded tag bag of defective toys. Yank out a fist bowl of disembodied doll limbs, toss them on a stained kid's place mat from a defunct dennies. set up a table inside a rusty cargo container down by the Wharf and challenged toothless drifters to the godless bughouse blitz of tournament that is my segment. Meanwhile.\"\n+        ```\n+\n         - *Shortform transcription*: If passed mel input features are <= 30 seconds, there are two possibilities:\n             - `return_timestamps=False`: the whole audio will be transcribed with a single call to GenerationMixin's [`~generation.GenerationMixin.generate`].\n             - `return_timestamps=True`: the audio will be transcribed using the same logic as long-form transcription.\n@@ -763,6 +794,9 @@ def generate(\n \n         # 6 Transcribe audio until we reach the end of all input audios\n         while (seek < max_frames).any():\n+            if monitor_progress is not None:\n+                monitor_progress(torch.stack((seek, max_frames), dim=1))\n+\n             # 6.1 NOTE: When in longform transcription mode and batch size > 1 we need to dynamically reduce the batch size during the loop\n             # in case one audio finished earlier than another one. Thus, we need to keep a table of \"previous-index-2-current-index\" in order\n             # to know which original audio is being decoded"
        }
    ],
    "stats": {
        "total": 38,
        "additions": 36,
        "deletions": 2
    }
}