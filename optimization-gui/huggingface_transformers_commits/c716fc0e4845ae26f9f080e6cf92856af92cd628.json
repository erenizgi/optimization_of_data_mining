{
    "author": "niqodea",
    "message": "fix: use correct var names for check_tokenizers script (#33702)",
    "sha": "c716fc0e4845ae26f9f080e6cf92856af92cd628",
    "files": [
        {
            "sha": "ba73460ce4d60b0d1d99ae317723b9016348ea31",
            "filename": "scripts/check_tokenizers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c716fc0e4845ae26f9f080e6cf92856af92cd628/scripts%2Fcheck_tokenizers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c716fc0e4845ae26f9f080e6cf92856af92cd628/scripts%2Fcheck_tokenizers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/scripts%2Fcheck_tokenizers.py?ref=c716fc0e4845ae26f9f080e6cf92856af92cd628",
            "patch": "@@ -88,7 +88,7 @@ def check_details(line, spm_ids, tok_ids, slow, fast):\n                     if tok_ids[first + k : first + k + min_width] == spm_ids[first + i : first + i + min_width]\n                 ]\n                 for j in possible_matches:\n-                    if check_diff(spm_ids[first : first + i], tok_ids[first : first + j], sp, tok) and check_details(\n+                    if check_diff(spm_ids[first : first + i], tok_ids[first : first + j], slow, fast) and check_details(\n                         line,\n                         spm_ids[first + i : last],\n                         tok_ids[first + j : last],"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}