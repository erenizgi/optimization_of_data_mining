{
    "author": "yao-matrix",
    "message": "enable some falcon-mamba uts on xpu (#41428)\n\n* enable some falcon-mamba uts on xpu\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>",
    "sha": "b9be8a8775ff0899912f6db126fa8c770b6698cd",
    "files": [
        {
            "sha": "06ad59e20872f912ac193f58d1285a8d21e9087d",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/b9be8a8775ff0899912f6db126fa8c770b6698cd/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b9be8a8775ff0899912f6db126fa8c770b6698cd/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=b9be8a8775ff0899912f6db126fa8c770b6698cd",
            "patch": "@@ -170,8 +170,13 @@ def _lazy_load_causal_conv1d():\n     if is_kernels_available():\n         from kernels import get_kernel\n \n-        _causal_conv1d_kernel = get_kernel(\"kernels-community/causal-conv1d\")\n-        _causal_conv1d_cache = (_causal_conv1d_kernel.causal_conv1d_update, _causal_conv1d_kernel.causal_conv1d_fn)\n+        try:\n+            _causal_conv1d_kernel = get_kernel(\"kernels-community/causal-conv1d\")\n+        except FileNotFoundError:\n+            # no kernel binary match, fallback to slow path\n+            _causal_conv1d_cache = (None, None)\n+        else:\n+            _causal_conv1d_cache = (_causal_conv1d_kernel.causal_conv1d_update, _causal_conv1d_kernel.causal_conv1d_fn)\n     elif is_causal_conv1d_available():\n         from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n "
        },
        {
            "sha": "ceeefbec8851aa33b98b110f0bd23bcfea171afd",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/b9be8a8775ff0899912f6db126fa8c770b6698cd/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b9be8a8775ff0899912f6db126fa8c770b6698cd/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=b9be8a8775ff0899912f6db126fa8c770b6698cd",
            "patch": "@@ -65,8 +65,13 @@ def _lazy_load_causal_conv1d():\n     if is_kernels_available():\n         from kernels import get_kernel\n \n-        _causal_conv1d_kernel = get_kernel(\"kernels-community/causal-conv1d\")\n-        _causal_conv1d_cache = (_causal_conv1d_kernel.causal_conv1d_update, _causal_conv1d_kernel.causal_conv1d_fn)\n+        try:\n+            _causal_conv1d_kernel = get_kernel(\"kernels-community/causal-conv1d\")\n+        except FileNotFoundError:\n+            # no kernel binary match, fallback to slow path\n+            _causal_conv1d_cache = (None, None)\n+        else:\n+            _causal_conv1d_cache = (_causal_conv1d_kernel.causal_conv1d_update, _causal_conv1d_kernel.causal_conv1d_fn)\n     elif is_causal_conv1d_available():\n         from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n "
        },
        {
            "sha": "e148326ff79a11f056009008b07ccc55c9bb39b0",
            "filename": "tests/models/falcon_mamba/test_modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 11,
            "deletions": 1,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b9be8a8775ff0899912f6db126fa8c770b6698cd/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b9be8a8775ff0899912f6db126fa8c770b6698cd/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py?ref=b9be8a8775ff0899912f6db126fa8c770b6698cd",
            "patch": "@@ -23,6 +23,7 @@\n     Expectations,\n     cleanup,\n     require_bitsandbytes,\n+    require_deterministic_for_xpu,\n     require_torch,\n     require_torch_accelerator,\n     require_torch_large_accelerator,\n@@ -421,6 +422,7 @@ def test_generation_fp16(self):\n \n         EXPECTED_OUTPUTS = Expectations(\n             {\n+                (\"xpu\", 3): \"Hello today Iava,\\n\\nI am writing to you today to discuss the importance of maintaining a healthy lifestyle\",\n                 (\"cuda\", 7): \"Hello today I am going to show you how to make a simple and easy to make paper plane.\\nStep\",\n                 (\"cuda\", 8): 'Hello today Iava,\\n\\nI am writing to you today to discuss the importance of maintaining a healthy lifestyle',\n             }\n@@ -454,13 +456,13 @@ def test_generation_torch_compile(self):\n \n         inputs = self.tokenizer(self.text, return_tensors=\"pt\").to(torch_device)\n         out = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n-        print(self.tokenizer.batch_decode(out, skip_special_tokens=False)[0])\n \n         self.assertEqual(\n             self.tokenizer.batch_decode(out, skip_special_tokens=False)[0],\n             \"Hello today Iava,\\n\\nI am writing to you today to discuss the importance of maintaining a healthy lifestyle\",\n         )\n \n+    @require_deterministic_for_xpu\n     def test_batched_generation(self):\n         model_id = \"tiiuae/falcon-mamba-7b\"\n         tok = AutoTokenizer.from_pretrained(model_id)\n@@ -470,6 +472,10 @@ def test_batched_generation(self):\n \n         EXPECTED_OUTPUTS = Expectations(\n             {\n+                (\"xpu\", 3): [\n+                    'Hello today I will be talking about the “Theory of Relativity” by Albert Einstein.\\nThe',\n+                    'Hello my name is Younes and today I will be talking about the importance of the internet in our lives.\\nThe internet is a global',\n+                ],\n                 (\"cuda\", 7): [\n                     'Hello today I will be talking about the “Theory of Relativity” by Albert Einstein.\\nThe',\n                     'Hello my name is Younes and today I will be talking about the importance of the internet in our lives.\\nThe internet is a global',\n@@ -500,6 +506,10 @@ def test_batched_generation(self):\n \n         EXPECTED_OUTPUTS = Expectations(\n             {\n+                (\"xpu\", 3): [\n+                    ' I will be talking about the “Theory of Relativity” by Albert Einstein.\\nThe',\n+                    ' I will be talking about the importance of the internet in our lives.\\nThe internet is a global',\n+                ],\n                 (\"cuda\", 7): [\n                     ' I will be talking about the “Theory of Relativity” by Albert Einstein.\\nThe',\n                     ' I will be talking about the importance of the internet in our lives.\\nThe internet is a global',"
        }
    ],
    "stats": {
        "total": 30,
        "additions": 25,
        "deletions": 5
    }
}