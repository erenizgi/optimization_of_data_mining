{
    "author": "SunMarc",
    "message": "remove `tpu_num_cores` (#41383)\n\n* remove-tpu-num-cores\n\n* fix\n\n* let's remove it\n\n* style\n\n* Update examples/legacy/seq2seq/finetune_tpu.sh\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "feca4f3de717071b9e83e7cf8d0360289f750000",
    "files": [
        {
            "sha": "d165322b643471a7ccdfc189d4171deec9e8104d",
            "filename": "examples/legacy/seq2seq/finetune_tpu.sh",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/feca4f3de717071b9e83e7cf8d0360289f750000/examples%2Flegacy%2Fseq2seq%2Ffinetune_tpu.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/feca4f3de717071b9e83e7cf8d0360289f750000/examples%2Flegacy%2Fseq2seq%2Ffinetune_tpu.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ffinetune_tpu.sh?ref=feca4f3de717071b9e83e7cf8d0360289f750000",
            "patch": "@@ -16,8 +16,8 @@ export TPU_NUM_CORES=8\n \n # the proper usage is documented in the README, you need to specify data_dir, output_dir and model_name_or_path\n # run ./finetune_tpu.sh --help to see all the possible options\n-python xla_spawn.py --num_cores $TPU_NUM_CORES \\\n-    finetune_trainer.py \\\n+# To specify the number of cores to use, use the TPU_NUM_DEVICES environment variable\n+python xla_spawn.py finetune_trainer.py \\\n     --learning_rate=3e-5 \\\n     --do_train --do_eval \\\n     --eval_strategy steps \\"
        },
        {
            "sha": "c88e765d0bd7a9d9698e55b4b2b3694c1b30e7f2",
            "filename": "examples/legacy/seq2seq/finetune_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/feca4f3de717071b9e83e7cf8d0360289f750000/examples%2Flegacy%2Fseq2seq%2Ffinetune_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/feca4f3de717071b9e83e7cf8d0360289f750000/examples%2Flegacy%2Fseq2seq%2Ffinetune_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ffinetune_trainer.py?ref=feca4f3de717071b9e83e7cf8d0360289f750000",
            "patch": "@@ -295,9 +295,7 @@ def main():\n         data_args=data_args,\n         train_dataset=train_dataset,\n         eval_dataset=eval_dataset,\n-        data_collator=Seq2SeqDataCollator(\n-            tokenizer, data_args, model.config.decoder_start_token_id, training_args.tpu_num_cores\n-        ),\n+        data_collator=Seq2SeqDataCollator(tokenizer, data_args, model.config.decoder_start_token_id),\n         compute_metrics=compute_metrics_fn,\n         processing_class=tokenizer,\n     )"
        },
        {
            "sha": "af589f0c54f8c759a744f2091c451d44a029ce42",
            "filename": "examples/legacy/seq2seq/train_distil_marian_enro_tpu.sh",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/feca4f3de717071b9e83e7cf8d0360289f750000/examples%2Flegacy%2Fseq2seq%2Ftrain_distil_marian_enro_tpu.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/feca4f3de717071b9e83e7cf8d0360289f750000/examples%2Flegacy%2Fseq2seq%2Ftrain_distil_marian_enro_tpu.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ftrain_distil_marian_enro_tpu.sh?ref=feca4f3de717071b9e83e7cf8d0360289f750000",
            "patch": "@@ -16,10 +16,8 @@ export WANDB_PROJECT=distil-marian\n export BS=64\n export m=sshleifer/student_marian_en_ro_6_3\n export MAX_LEN=128\n-export TPU_NUM_CORES=8\n \n-python xla_spawn.py --num_cores $TPU_NUM_CORES \\\n-    finetune_trainer.py \\\n+python xla_spawn.py finetune_trainer.py \\\n     --tokenizer_name $m --model_name_or_path $m \\\n     --data_dir $ENRO_DIR \\\n     --output_dir marian_en_ro_6_3 \\"
        },
        {
            "sha": "9f96e6d6617d50d1254567a49016c424e86ee66f",
            "filename": "examples/legacy/seq2seq/utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/feca4f3de717071b9e83e7cf8d0360289f750000/examples%2Flegacy%2Fseq2seq%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/feca4f3de717071b9e83e7cf8d0360289f750000/examples%2Flegacy%2Fseq2seq%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Futils.py?ref=feca4f3de717071b9e83e7cf8d0360289f750000",
            "patch": "@@ -279,15 +279,14 @@ def collate_fn(self, batch) -> dict[str, torch.Tensor]:\n \n \n class Seq2SeqDataCollator:\n-    def __init__(self, tokenizer, data_args, decoder_start_token_id, tpu_num_cores=None):\n+    def __init__(self, tokenizer, data_args, decoder_start_token_id):\n         self.tokenizer = tokenizer\n         self.pad_token_id = tokenizer.pad_token_id\n         self.decoder_start_token_id = decoder_start_token_id\n         assert self.pad_token_id is not None, (\n             f\"pad_token_id is not defined for ({self.tokenizer.__class__.__name__}), it must be defined.\"\n         )\n         self.data_args = data_args\n-        self.tpu_num_cores = tpu_num_cores\n         self.dataset_kwargs = {\"add_prefix_space\": True} if isinstance(tokenizer, BartTokenizer) else {}\n         if data_args.src_lang is not None:\n             self.dataset_kwargs[\"src_lang\"] = data_args.src_lang\n@@ -336,7 +335,7 @@ def _encode(self, batch) -> dict[str, torch.Tensor]:\n             tgt_texts=[x[\"tgt_texts\"] for x in batch],\n             max_length=self.data_args.max_source_length,\n             max_target_length=self.data_args.max_target_length,\n-            padding=\"max_length\" if self.tpu_num_cores is not None else \"longest\",  # TPU hack\n+            padding=\"longest\",\n             return_tensors=\"pt\",\n             **self.dataset_kwargs,\n         )"
        },
        {
            "sha": "6005dda93c08bbcb1c9bd94456b84cc9cbdcffa7",
            "filename": "examples/legacy/seq2seq/xla_spawn.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/feca4f3de717071b9e83e7cf8d0360289f750000/examples%2Flegacy%2Fseq2seq%2Fxla_spawn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/feca4f3de717071b9e83e7cf8d0360289f750000/examples%2Flegacy%2Fseq2seq%2Fxla_spawn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fxla_spawn.py?ref=feca4f3de717071b9e83e7cf8d0360289f750000",
            "patch": "@@ -73,9 +73,9 @@ def main():\n     mod = importlib.import_module(mod_name)\n \n     # Patch sys.argv\n-    sys.argv = [args.training_script] + args.training_script_args + [\"--tpu_num_cores\", str(args.num_cores)]\n+    sys.argv = [args.training_script] + args.training_script_args\n \n-    xmp.spawn(mod._mp_fn, args=(), nprocs=args.num_cores)\n+    xmp.spawn(mod._mp_fn, args=())\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "70fad50df2dfc35f8b83b81133f50c3966498bee",
            "filename": "examples/pytorch/xla_spawn.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/feca4f3de717071b9e83e7cf8d0360289f750000/examples%2Fpytorch%2Fxla_spawn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/feca4f3de717071b9e83e7cf8d0360289f750000/examples%2Fpytorch%2Fxla_spawn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fxla_spawn.py?ref=feca4f3de717071b9e83e7cf8d0360289f750000",
            "patch": "@@ -73,7 +73,7 @@ def main():\n     mod = importlib.import_module(mod_name)\n \n     # Patch sys.argv\n-    sys.argv = [args.training_script] + args.training_script_args + [\"--tpu_num_cores\", str(args.num_cores)]\n+    sys.argv = [args.training_script] + args.training_script_args\n \n     xmp.spawn(mod._mp_fn, args=(), nprocs=args.num_cores)\n "
        },
        {
            "sha": "d78a07311a2986c97f2a8accc801c23db3c00d0e",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/feca4f3de717071b9e83e7cf8d0360289f750000/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/feca4f3de717071b9e83e7cf8d0360289f750000/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=feca4f3de717071b9e83e7cf8d0360289f750000",
            "patch": "@@ -385,8 +385,6 @@ class TrainingArguments:\n             experimental API and it may change.\n         ddp_backend (`str`, *optional*):\n             The backend to use for distributed training. Must be one of `\"nccl\"`, `\"mpi\"`, `\"ccl\"`, `\"gloo\"`, `\"hccl\"`.\n-        tpu_num_cores (`int`, *optional*):\n-            When training on TPU, the number of TPU cores (automatically passed by launcher script).\n         dataloader_drop_last (`bool`, *optional*, defaults to `False`):\n             Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)\n             or not.\n@@ -1009,9 +1007,6 @@ class TrainingArguments:\n             \"choices\": [\"nccl\", \"gloo\", \"mpi\", \"ccl\", \"hccl\", \"cncl\", \"mccl\"],\n         },\n     )\n-    tpu_num_cores: Optional[int] = field(\n-        default=None, metadata={\"help\": \"TPU: Number of TPU cores (automatically passed by launcher script)\"}\n-    )\n     debug: Union[str, list[DebugOption]] = field(\n         default=\"\",\n         metadata={"
        },
        {
            "sha": "af1906b11e59519ccf0acae5ba5d64a82b7e853f",
            "filename": "tests/trainer/test_trainer_tpu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/feca4f3de717071b9e83e7cf8d0360289f750000/tests%2Ftrainer%2Ftest_trainer_tpu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/feca4f3de717071b9e83e7cf8d0360289f750000/tests%2Ftrainer%2Ftest_trainer_tpu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_tpu.py?ref=feca4f3de717071b9e83e7cf8d0360289f750000",
            "patch": "@@ -67,10 +67,7 @@ def main():\n     sys.argv += [\"--output_dir\", \"./examples\"]\n     training_args = parser.parse_args_into_dataclasses()[0]\n \n-    logger.warning(\n-        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, \"\n-        f\"tpu_num_cores: {training_args.tpu_num_cores}\",\n-    )\n+    logger.warning(f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, \")\n \n     # Essentially, what we want to verify in the distributed case is\n     # that we get all samples back, in the right order."
        }
    ],
    "stats": {
        "total": 33,
        "additions": 10,
        "deletions": 23
    }
}