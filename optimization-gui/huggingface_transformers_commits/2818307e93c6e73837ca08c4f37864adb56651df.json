{
    "author": "gante",
    "message": "[generate] can instantiate `GenerationConfig(cache_implementation=\"static\")` (#35679)\n\nfix failing instantiation",
    "sha": "2818307e93c6e73837ca08c4f37864adb56651df",
    "files": [
        {
            "sha": "3f142ce772984b72b110b1443224a76f66c433ce",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 10,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/2818307e93c6e73837ca08c4f37864adb56651df/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2818307e93c6e73837ca08c4f37864adb56651df/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=2818307e93c6e73837ca08c4f37864adb56651df",
            "patch": "@@ -43,7 +43,7 @@\n \n logger = logging.get_logger(__name__)\n METADATA_FIELDS = (\"_from_model_config\", \"_commit_hash\", \"_original_object_hash\", \"transformers_version\")\n-NEEDS_CACHE_CONFIG = {}\n+CACHE_CONFIG_MAPPING = {}\n NEED_SETUP_CACHE_CLASSES_MAPPING = {}\n QUANT_BACKEND_CLASSES_MAPPING = {}\n ALL_CACHE_IMPLEMENTATIONS = []\n@@ -62,8 +62,8 @@\n     )\n     from .logits_process import SynthIDTextWatermarkLogitsProcessor, WatermarkLogitsProcessor\n \n-    NEEDS_CACHE_CONFIG[\"quantized\"] = QuantizedCacheConfig\n-    NEEDS_CACHE_CONFIG[\"static\"] = StaticCacheConfig\n+    CACHE_CONFIG_MAPPING[\"quantized\"] = QuantizedCacheConfig\n+    CACHE_CONFIG_MAPPING[\"static\"] = StaticCacheConfig\n     NEED_SETUP_CACHE_CLASSES_MAPPING = {\n         \"static\": StaticCache,\n         \"offloaded_static\": OffloadedStaticCache,\n@@ -73,7 +73,7 @@\n     }\n     QUANT_BACKEND_CLASSES_MAPPING = {\"quanto\": QuantoQuantizedCache, \"HQQ\": HQQQuantizedCache}\n     ALL_CACHE_IMPLEMENTATIONS = (\n-        list(NEED_SETUP_CACHE_CLASSES_MAPPING.keys()) + list(NEEDS_CACHE_CONFIG.keys()) + [\"offloaded\"]\n+        list(NEED_SETUP_CACHE_CLASSES_MAPPING.keys()) + list(CACHE_CONFIG_MAPPING.keys()) + [\"offloaded\"]\n     )\n \n \n@@ -409,11 +409,9 @@ def __init__(self, **kwargs):\n         self.use_cache = kwargs.pop(\"use_cache\", True)\n         self.cache_implementation = kwargs.pop(\"cache_implementation\", None)\n         self.cache_config = kwargs.pop(\"cache_config\", None)\n-        if self.cache_implementation is not None and self.cache_implementation in NEEDS_CACHE_CONFIG:\n-            cache_config_class = NEEDS_CACHE_CONFIG[self.cache_implementation]\n-            if self.cache_config is None:\n-                self.cache_config = cache_config_class()\n-            elif isinstance(self.cache_config, dict):\n+        if self.cache_implementation is not None and self.cache_implementation in CACHE_CONFIG_MAPPING:\n+            cache_config_class = CACHE_CONFIG_MAPPING[self.cache_implementation]\n+            if isinstance(self.cache_config, dict):\n                 self.cache_config = cache_config_class.from_dict(self.cache_config)\n         self.return_legacy_cache = kwargs.pop(\"return_legacy_cache\", None)\n \n@@ -766,7 +764,7 @@ def validate(self, is_init=False):\n                 f\"{ALL_CACHE_IMPLEMENTATIONS}\"\n             )\n         if self.cache_config is not None:\n-            cache_class = NEEDS_CACHE_CONFIG.get(self.cache_implementation)\n+            cache_class = CACHE_CONFIG_MAPPING.get(self.cache_implementation)\n             if cache_class is None:\n                 raise ValueError(\n                     \"You provided a `cache_config` but the cache implementation you are using \""
        },
        {
            "sha": "ef30599581ffae4501888bd895a73cf4f8edc43f",
            "filename": "tests/generation/test_configuration_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2818307e93c6e73837ca08c4f37864adb56651df/tests%2Fgeneration%2Ftest_configuration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2818307e93c6e73837ca08c4f37864adb56651df/tests%2Fgeneration%2Ftest_configuration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_configuration_utils.py?ref=2818307e93c6e73837ca08c4f37864adb56651df",
            "patch": "@@ -259,6 +259,12 @@ def test_generation_mode(self):\n         config = GenerationConfig()\n         self.assertEqual(config.get_generation_mode(assistant_model=\"foo\"), GenerationMode.ASSISTED_GENERATION)\n \n+    def test_static_cache_without_cache_config(self):\n+        \"\"\"Regression test for #35026 -- static cache should work without a cache config.\"\"\"\n+        config = GenerationConfig(cache_implementation=\"static\")\n+        self.assertEqual(config.cache_implementation, \"static\")\n+        self.assertEqual(config.cache_config, None)\n+\n \n class GenerationConfigSerializationTest(unittest.TestCase):\n     def test_serialize_generation_sequence_bias(self):"
        }
    ],
    "stats": {
        "total": 24,
        "additions": 14,
        "deletions": 10
    }
}