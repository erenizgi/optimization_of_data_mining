{
    "author": "Abdennacer-Badaoui",
    "message": "Fix T5 tests: use generation_config for generation parameters (#42419)\n\n* pass the generation parameters to generate()\n\n* fix use_task_specific_params to separate model.config and model.generation_config params\n\n* fix style\n\n* some fixes\n\n* remove redundant check\n\n* update expectation for llama_7b_bf16 on rocm\n\n* Update tests/models/llama/test_modeling_llama.py\n\nCo-authored-by: Rémi Ouazan <83456801+remi-or@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Rémi Ouazan <83456801+remi-or@users.noreply.github.com>",
    "sha": "603eb77a12b9ba5a3ed1482e1dd92868599bdf91",
    "files": [
        {
            "sha": "32186f5249785850953677c1bb866b0382a8ea3f",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/603eb77a12b9ba5a3ed1482e1dd92868599bdf91/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/603eb77a12b9ba5a3ed1482e1dd92868599bdf91/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=603eb77a12b9ba5a3ed1482e1dd92868599bdf91",
            "patch": "@@ -117,7 +117,7 @@ def test_model_7b_logits_bf16(self):\n             (\"xpu\", 3): torch.tensor([[-6.5208, -4.1218, -4.9377, -3.2536,  0.8127, -2.9811,  1.2918, -3.3848]]),\n             (\"cuda\", 7): torch.tensor([[-6.5061, -4.1147, -4.9669, -3.2038, 0.8069, -2.9694, 1.2864, -3.3786]]),\n             (\"cuda\", 8): torch.tensor([[-6.5208, -4.1218, -4.9377, -3.2536,  0.8127, -2.9811,  1.2918, -3.3848]]),\n-            (\"rocm\", (9, 4)): torch.tensor([[-6.5094, -4.1329, -4.9754, -3.5042,  0.8082, -2.9443,  1.2830, -3.3539]]),\n+            (\"rocm\", (9, 4)): torch.tensor([[-6.5067, -4.1154, -4.9819, -3.1408,  0.8117, -2.9435,  1.2883, -3.3221]]),\n         })\n \n         expected_mean = expected_means.get_expectation().to(torch_device)"
        },
        {
            "sha": "29f99c3675103853fc4d9d4b40e4e4349356ba3b",
            "filename": "tests/models/t5/test_modeling_t5.py",
            "status": "modified",
            "additions": 13,
            "deletions": 5,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/603eb77a12b9ba5a3ed1482e1dd92868599bdf91/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/603eb77a12b9ba5a3ed1482e1dd92868599bdf91/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py?ref=603eb77a12b9ba5a3ed1482e1dd92868599bdf91",
            "patch": "@@ -47,6 +47,7 @@\n     from transformers import (\n         AutoTokenizer,\n         ByT5Tokenizer,\n+        GenerationConfig,\n         T5EncoderModel,\n         T5ForConditionalGeneration,\n         T5ForQuestionAnswering,\n@@ -932,7 +933,17 @@ def is_pipeline_test_to_skip(\n \n \n def use_task_specific_params(model, task):\n-    model.config.update(model.config.task_specific_params[task])\n+    task_params = model.config.task_specific_params[task]\n+\n+    # Get all valid GenerationConfig attributes\n+    temp_config = GenerationConfig()\n+    generation_config_attrs = set(temp_config.to_dict().keys())\n+\n+    for key, value in task_params.items():\n+        if key in generation_config_attrs:\n+            setattr(model.generation_config, key, value)\n+        else:\n+            setattr(model.config, key, value)\n \n \n @require_torch\n@@ -1032,14 +1043,11 @@ def test_torch_quant(self):\n     @slow\n     def test_small_generation(self):\n         model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\").to(torch_device)\n-        model.config.max_length = 8\n-        model.config.num_beams = 1\n-        model.config.do_sample = False\n         tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n \n         input_ids = tokenizer(\"summarize: Hello there\", return_tensors=\"pt\").input_ids.to(torch_device)\n \n-        sequences = model.generate(input_ids)\n+        sequences = model.generate(input_ids, max_length=8, num_beams=1, do_sample=False)\n \n         output_str = tokenizer.batch_decode(sequences, skip_special_tokens=True)[0]\n         self.assertTrue(output_str == \"Hello there!\")"
        }
    ],
    "stats": {
        "total": 20,
        "additions": 14,
        "deletions": 6
    }
}