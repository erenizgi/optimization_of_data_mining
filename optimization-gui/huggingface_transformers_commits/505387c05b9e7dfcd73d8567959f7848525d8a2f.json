{
    "author": "ArthurZucker",
    "message": "Update from pretrained error when loading (#33380)\n\n* init commit\n\n* style\n\n* take comments into account\n\n* mrege with main and simplify\n\n* nits\n\n* final\n\n* small fixes\n\n* fix\n\n* super small update!\n\n* add another test\n\n* up up\n\n* update\n\n* fixes\n\n* sort them by default",
    "sha": "505387c05b9e7dfcd73d8567959f7848525d8a2f",
    "files": [
        {
            "sha": "a6427e297651341abec20ad4d8cde0e3880ef7d0",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 36,
            "deletions": 2,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/505387c05b9e7dfcd73d8567959f7848525d8a2f/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/505387c05b9e7dfcd73d8567959f7848525d8a2f/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=505387c05b9e7dfcd73d8567959f7848525d8a2f",
            "patch": "@@ -875,6 +875,40 @@ def _add_variant(weights_name: str, variant: Optional[str] = None) -> str:\n     return weights_name\n \n \n+def update_key_name(keys):\n+    \"\"\"\n+    Updates a dictionary of keys to pack layers together as layer.{0, 1, 4} instead of layers.0, layers.1, layers.4.\n+    \"\"\"\n+    key_dict = defaultdict(list)\n+    for key in keys:\n+        all_digits = re.findall(r\".(\\d+).\", key)\n+        for i, k in enumerate(all_digits):\n+            if len(key_dict[re.sub(r\".(\\d+).\", \".*.\", key)]) <= i:\n+                key_dict[re.sub(r\".(\\d+).\", \".*.\", key)].append(set())\n+            key_dict[re.sub(r\".(\\d+).\", \".*.\", key)][i].add(int(k))\n+\n+    final_keys = set()\n+    for key in keys:\n+        text = re.sub(r\".(\\d+).\", \".*.\", key)\n+        pattern = key_dict[text]\n+        final_text = \"\"\n+        for i, part in enumerate(text.split(\"*\")):\n+            if len(pattern) <= i:\n+                final_text += part\n+            else:\n+                data = [str(i) for i in sorted(pattern[i])]\n+                if len(data) > 10:\n+                    result = f\"{data[0]}...{data[-1]}\"\n+                else:\n+                    result = \", \".join(data)  # If there are only 1 or 2 elements, show them all\n+                if len(data) > 1:\n+                    final_text += part + \"{\" + result + \"}\"\n+                else:\n+                    final_text += part + data[0]\n+        final_keys.add(final_text)\n+    return sorted(final_keys)\n+\n+\n def _get_resolved_checkpoint_files(\n     pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n     subfolder: str,\n@@ -5313,7 +5347,7 @@ def _load_pretrained_model(\n             warner = logger.warning if model.__class__.__name__ in archs else logger.info\n             warner(\n                 f\"Some weights of the model checkpoint at {pretrained_model_name_or_path} were not used when\"\n-                f\" initializing {model.__class__.__name__}: {unexpected_keys}\\n- This IS expected if you are\"\n+                f\" initializing {model.__class__.__name__}: {update_key_name(unexpected_keys)}\\n- This IS expected if you are\"\n                 f\" initializing {model.__class__.__name__} from the checkpoint of a model trained on another task or\"\n                 \" with another architecture (e.g. initializing a BertForSequenceClassification model from a\"\n                 \" BertForPreTraining model).\\n- This IS NOT expected if you are initializing\"\n@@ -5323,7 +5357,7 @@ def _load_pretrained_model(\n         if len(missing_keys) > 0:\n             logger.warning(\n                 f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n-                f\" {pretrained_model_name_or_path} and are newly initialized: {missing_keys}\\nYou should probably\"\n+                f\" {pretrained_model_name_or_path} and are newly initialized: {update_key_name(missing_keys)}\\nYou should probably\"\n                 \" TRAIN this model on a down-stream task to be able to use it for predictions and inference.\"\n             )\n         if len(mismatched_keys) > 0:"
        },
        {
            "sha": "7c2ae71dca55b2bd02e489a56c88b09bf4018d62",
            "filename": "src/transformers/models/deepseek_v3/configuration_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/505387c05b9e7dfcd73d8567959f7848525d8a2f/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/505387c05b9e7dfcd73d8567959f7848525d8a2f/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py?ref=505387c05b9e7dfcd73d8567959f7848525d8a2f",
            "patch": "@@ -152,7 +152,7 @@ class DeepseekV3Config(PreTrainedConfig):\n         \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n     }\n     attribute_map = {\n-        \"num_experts\": \"n_routed_experts\",\n+        \"num_local_experts\": \"n_routed_experts\",\n     }\n \n     def __init__("
        },
        {
            "sha": "9302efc073fa93cc7391412ea64150b31e0971c6",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/505387c05b9e7dfcd73d8567959f7848525d8a2f/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/505387c05b9e7dfcd73d8567959f7848525d8a2f/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=505387c05b9e7dfcd73d8567959f7848525d8a2f",
            "patch": "@@ -58,6 +58,7 @@\n     logging,\n )\n from transformers.modeling_flash_attention_utils import is_flash_attn_available\n+from transformers.modeling_utils import update_key_name\n from transformers.models.mistral.modeling_mistral import MistralModel\n from transformers.testing_utils import (\n     TOKEN,\n@@ -1689,6 +1690,22 @@ def test_isin_mps_friendly(self):\n             torch.equal(torch.isin(random_ids, random_test_tensor), isin_mps_friendly(random_ids, random_test_tensor))\n         )\n \n+    def test_update_key_name(self):\n+        model = AutoModel.from_pretrained(\"google-t5/t5-base\", device_map=\"auto\")\n+\n+        new_keys = \"\\n\".join(sorted(update_key_name(model.state_dict().keys())))\n+\n+        EXPECTED_KEYS = \"\"\"decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\\ndecoder.block.{0...11}.layer.0.SelfAttention.k.weight\\ndecoder.block.{0...11}.layer.0.SelfAttention.o.weight\\ndecoder.block.{0...11}.layer.0.SelfAttention.q.weight\\ndecoder.block.{0...11}.layer.0.SelfAttention.v.weight\\ndecoder.block.{0...11}.layer.1.EncDecAttention.k.weight\\ndecoder.block.{0...11}.layer.1.EncDecAttention.o.weight\\ndecoder.block.{0...11}.layer.1.EncDecAttention.q.weight\\ndecoder.block.{0...11}.layer.1.EncDecAttention.v.weight\\ndecoder.block.{0...11}.layer.2.DenseReluDense.wi.weight\\ndecoder.block.{0...11}.layer.2.DenseReluDense.wo.weight\\ndecoder.block.{0...11}.layer.{0, 1, 2}.layer_norm.weight\\ndecoder.embed_tokens.weight\\ndecoder.final_layer_norm.weight\\nencoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\\nencoder.block.{0...11}.layer.0.SelfAttention.k.weight\\nencoder.block.{0...11}.layer.0.SelfAttention.o.weight\\nencoder.block.{0...11}.layer.0.SelfAttention.q.weight\\nencoder.block.{0...11}.layer.0.SelfAttention.v.weight\\nencoder.block.{0...11}.layer.1.DenseReluDense.wi.weight\\nencoder.block.{0...11}.layer.1.DenseReluDense.wo.weight\\nencoder.block.{0...11}.layer.{0, 1}.layer_norm.weight\\nencoder.embed_tokens.weight\\nencoder.final_layer_norm.weight\\nshared.weight\"\"\"\n+        self.assertEqual(new_keys, EXPECTED_KEYS)\n+\n+        EXPECTED_KEYS = \"\"\"embed_tokens.weight\\nlayers.{0, 1, 2}.mlp.down_proj.weight\\nlayers.{0, 1, 2}.mlp.gate_proj.weight\\nlayers.{0, 1, 2}.mlp.up_proj.weight\\nlayers.{0...60}.input_layernorm.weight\\nlayers.{0...60}.post_attention_layernorm.weight\\nlayers.{0...60}.self_attn.kv_a_layernorm.weight\\nlayers.{0...60}.self_attn.kv_a_proj_with_mqa.weight\\nlayers.{0...60}.self_attn.kv_b_proj.weight\\nlayers.{0...60}.self_attn.o_proj.weight\\nlayers.{0...60}.self_attn.q_a_layernorm.weight\\nlayers.{0...60}.self_attn.q_a_proj.weight\\nlayers.{0...60}.self_attn.q_b_proj.weight\\nlayers.{3...60}.mlp.experts.{0...255}.down_proj.weight\\nlayers.{3...60}.mlp.experts.{0...255}.gate_proj.weight\\nlayers.{3...60}.mlp.experts.{0...255}.up_proj.weight\\nlayers.{3...60}.mlp.gate.e_score_correction_bias\\nlayers.{3...60}.mlp.gate.weight\\nlayers.{3...60}.mlp.shared_experts.down_proj.weight\\nlayers.{3...60}.mlp.shared_experts.gate_proj.weight\\nlayers.{3...60}.mlp.shared_experts.up_proj.weight\\nnorm.weight\"\"\"\n+        config = AutoConfig.from_pretrained(\"deepseek-ai/DeepSeek-V3.1\")\n+        with torch.device(\"meta\"):\n+            model = AutoModel.from_config(config)\n+\n+        new_keys = \"\\n\".join(sorted(update_key_name(model.state_dict().keys())))\n+        self.assertEqual(new_keys, EXPECTED_KEYS)\n+\n     def test_can_generate(self):\n         \"\"\"Tests the behavior of `PreTrainedModel.can_generate` method.\"\"\"\n         logger = logging.get_logger(\"transformers.modeling_utils\")"
        }
    ],
    "stats": {
        "total": 57,
        "additions": 54,
        "deletions": 3
    }
}