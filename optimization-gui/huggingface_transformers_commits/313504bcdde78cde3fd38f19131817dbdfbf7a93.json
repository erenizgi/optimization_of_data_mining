{
    "author": "gante",
    "message": "ðŸš¨ [v5] remove deprecated `generate` classes (constraints and beam scorers) (#41223)\n\nrm",
    "sha": "313504bcdde78cde3fd38f19131817dbdfbf7a93",
    "files": [
        {
            "sha": "96ab4057171ce07219b15bebeda1e813abca74f1",
            "filename": "docs/source/en/internal/generation_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/313504bcdde78cde3fd38f19131817dbdfbf7a93/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/313504bcdde78cde3fd38f19131817dbdfbf7a93/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md?ref=313504bcdde78cde3fd38f19131817dbdfbf7a93",
            "patch": "@@ -193,28 +193,6 @@ A [`StoppingCriteria`] can be used to change when to stop generation (other than\n [[autodoc]] EosTokenCriteria\n     - __call__\n \n-## Constraints\n-\n-A [`Constraint`] can be used to force the generation to include specific tokens or sequences in the output. Please note that this is exclusively available to our PyTorch implementations.\n-\n-[[autodoc]] Constraint\n-\n-[[autodoc]] PhrasalConstraint\n-\n-[[autodoc]] DisjunctiveConstraint\n-\n-[[autodoc]] ConstraintListState\n-\n-## BeamSearch\n-\n-[[autodoc]] BeamScorer\n-    - process\n-    - finalize\n-\n-[[autodoc]] ConstrainedBeamSearchScorer\n-    - process\n-    - finalize\n-\n ## Streamers\n \n [[autodoc]] TextStreamer"
        },
        {
            "sha": "16e78cf2662b34627a0accb2a4cd03b3cde6fa43",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/313504bcdde78cde3fd38f19131817dbdfbf7a93/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/313504bcdde78cde3fd38f19131817dbdfbf7a93/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=313504bcdde78cde3fd38f19131817dbdfbf7a93",
            "patch": "@@ -389,12 +389,7 @@\n             \"AlternatingCodebooksLogitsProcessor\",\n             \"BayesianDetectorConfig\",\n             \"BayesianDetectorModel\",\n-            \"BeamScorer\",\n             \"ClassifierFreeGuidanceLogitsProcessor\",\n-            \"ConstrainedBeamSearchScorer\",\n-            \"Constraint\",\n-            \"ConstraintListState\",\n-            \"DisjunctiveConstraint\",\n             \"EncoderNoRepeatNGramLogitsProcessor\",\n             \"EncoderRepetitionPenaltyLogitsProcessor\",\n             \"EosTokenCriteria\",\n@@ -415,7 +410,6 @@\n             \"MinPLogitsWarper\",\n             \"NoBadWordsLogitsProcessor\",\n             \"NoRepeatNGramLogitsProcessor\",\n-            \"PhrasalConstraint\",\n             \"PrefixConstrainedLogitsProcessor\",\n             \"RepetitionPenaltyLogitsProcessor\",\n             \"SequenceBiasLogitsProcessor\",\n@@ -555,13 +549,8 @@\n     from .generation import AsyncTextIteratorStreamer as AsyncTextIteratorStreamer\n     from .generation import BayesianDetectorConfig as BayesianDetectorConfig\n     from .generation import BayesianDetectorModel as BayesianDetectorModel\n-    from .generation import BeamScorer as BeamScorer\n     from .generation import ClassifierFreeGuidanceLogitsProcessor as ClassifierFreeGuidanceLogitsProcessor\n     from .generation import CompileConfig as CompileConfig\n-    from .generation import ConstrainedBeamSearchScorer as ConstrainedBeamSearchScorer\n-    from .generation import Constraint as Constraint\n-    from .generation import ConstraintListState as ConstraintListState\n-    from .generation import DisjunctiveConstraint as DisjunctiveConstraint\n     from .generation import EncoderNoRepeatNGramLogitsProcessor as EncoderNoRepeatNGramLogitsProcessor\n     from .generation import EncoderRepetitionPenaltyLogitsProcessor as EncoderRepetitionPenaltyLogitsProcessor\n     from .generation import EosTokenCriteria as EosTokenCriteria\n@@ -583,7 +572,6 @@\n     from .generation import MinPLogitsWarper as MinPLogitsWarper\n     from .generation import NoBadWordsLogitsProcessor as NoBadWordsLogitsProcessor\n     from .generation import NoRepeatNGramLogitsProcessor as NoRepeatNGramLogitsProcessor\n-    from .generation import PhrasalConstraint as PhrasalConstraint\n     from .generation import PrefixConstrainedLogitsProcessor as PrefixConstrainedLogitsProcessor\n     from .generation import RepetitionPenaltyLogitsProcessor as RepetitionPenaltyLogitsProcessor\n     from .generation import SequenceBiasLogitsProcessor as SequenceBiasLogitsProcessor"
        },
        {
            "sha": "ccde5d8bc19c78aa484947af363d9e8c197e8a61",
            "filename": "src/transformers/generation/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/313504bcdde78cde3fd38f19131817dbdfbf7a93/src%2Ftransformers%2Fgeneration%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/313504bcdde78cde3fd38f19131817dbdfbf7a93/src%2Ftransformers%2Fgeneration%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2F__init__.py?ref=313504bcdde78cde3fd38f19131817dbdfbf7a93",
            "patch": "@@ -35,17 +35,6 @@\n except OptionalDependencyNotAvailable:\n     pass\n else:\n-    _import_structure[\"beam_constraints\"] = [\n-        \"Constraint\",\n-        \"ConstraintListState\",\n-        \"DisjunctiveConstraint\",\n-        \"PhrasalConstraint\",\n-    ]\n-    _import_structure[\"beam_search\"] = [\n-        \"BeamHypotheses\",\n-        \"BeamScorer\",\n-        \"ConstrainedBeamSearchScorer\",\n-    ]\n     _import_structure[\"candidate_generator\"] = [\n         \"AssistedCandidateGenerator\",\n         \"CandidateGenerator\",\n@@ -131,8 +120,6 @@\n     except OptionalDependencyNotAvailable:\n         pass\n     else:\n-        from .beam_constraints import Constraint, ConstraintListState, DisjunctiveConstraint, PhrasalConstraint\n-        from .beam_search import BeamHypotheses, BeamScorer, ConstrainedBeamSearchScorer\n         from .candidate_generator import (\n             AssistedCandidateGenerator,\n             CandidateGenerator,"
        },
        {
            "sha": "2a21cd3a620fcf1b58c4e078b33f575325acf70d",
            "filename": "src/transformers/generation/beam_constraints.py",
            "status": "removed",
            "additions": 0,
            "deletions": 533,
            "changes": 533,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f143006635effe2b38e95566a2f347b303916bb/src%2Ftransformers%2Fgeneration%2Fbeam_constraints.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f143006635effe2b38e95566a2f347b303916bb/src%2Ftransformers%2Fgeneration%2Fbeam_constraints.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fbeam_constraints.py?ref=8f143006635effe2b38e95566a2f347b303916bb",
            "patch": "@@ -1,533 +0,0 @@\n-from abc import ABC, abstractmethod\n-from typing import Optional\n-\n-from ..utils import logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-# TODO joao, manuel: remove in v4.58.0\n-class Constraint(ABC):\n-    r\"\"\"Abstract base class for all constraints that can be applied during generation.\n-    It must define how the constraint can be satisfied.\n-\n-    All classes that inherit Constraint must follow the requirement that\n-\n-    ```py\n-    completed = False\n-    while not completed:\n-        _, completed = constraint.update(constraint.advance())\n-    ```\n-\n-    will always terminate (halt).\n-    \"\"\"\n-\n-    def __init__(self):\n-        logger.warning_once(\n-            \"Importing `Constraint` classes is deprecated and will be removed in v4.58.0. Constrained beam search has been moved to the Hub: https://hf.co/transformers-community/constrained-beam-search. Please import using `from transformers.generation import Constraint` instead.\"\n-        )\n-        # test for the above condition\n-        self.test()\n-\n-    def test(self):\n-        \"\"\"\n-        Tests whether this constraint has been properly defined.\n-        \"\"\"\n-        counter = 0\n-        completed = False\n-        while not completed:\n-            if counter == 1:\n-                self.reset()\n-            advance = self.advance()\n-            if not self.does_advance(advance):\n-                raise Exception(\n-                    \"Custom Constraint is not defined correctly. self.does_advance(self.advance()) must be true.\"\n-                )\n-\n-            stepped, completed, reset = self.update(advance)\n-            counter += 1\n-\n-            if counter > 10000:\n-                raise Exception(\"update() does not fulfill the constraint.\")\n-\n-        if self.remaining() != 0:\n-            raise Exception(\"Custom Constraint is not defined correctly.\")\n-\n-    @abstractmethod\n-    def advance(self):\n-        \"\"\"\n-        When called, returns the token(s) that would take this constraint one step closer to being fulfilled.\n-\n-        Return:\n-            token_ids (Union[int, list[int], None]):\n-                - A single token ID (int) that advances the constraint, or\n-                - A list of token IDs that could advance the constraint\n-                - None if the constraint is completed or cannot be advanced\n-        \"\"\"\n-        raise NotImplementedError(\n-            f\"{self.__class__} is an abstract class. Only classes inheriting this class can be called.\"\n-        )\n-\n-    @abstractmethod\n-    def does_advance(self, token_id: int):\n-        \"\"\"\n-        Reads in a token and returns whether it creates progress.\n-        \"\"\"\n-        raise NotImplementedError(\n-            f\"{self.__class__} is an abstract class. Only classes inheriting this class can be called.\"\n-        )\n-\n-    @abstractmethod\n-    def update(self, token_id: int):\n-        \"\"\"\n-        Reads in a token and returns booleans that indicate the progress made by it. This function will update the\n-        state of this object unlikes `does_advance(self, token_id: int)`.\n-\n-        This isn't to test whether a certain token will advance the progress; it's to update its state as if it has\n-        been generated. This becomes important if token_id != desired token (refer to else statement in\n-        PhrasalConstraint)\n-\n-        Args:\n-            token_id(`int`):\n-                The id of a newly generated token in the beam search.\n-        Return:\n-            stepped(`bool`):\n-                Whether this constraint has become one step closer to being fulfuilled.\n-            completed(`bool`):\n-                Whether this constraint has been completely fulfilled by this token being generated.\n-            reset (`bool`):\n-                Whether this constraint has reset its progress by this token being generated.\n-        \"\"\"\n-        raise NotImplementedError(\n-            f\"{self.__class__} is an abstract class. Only classes inheriting this class can be called.\"\n-        )\n-\n-    @abstractmethod\n-    def reset(self):\n-        \"\"\"\n-        Resets the state of this constraint to its initialization. We would call this in cases where the fulfillment of\n-        a constraint is abrupted by an unwanted token.\n-        \"\"\"\n-        raise NotImplementedError(\n-            f\"{self.__class__} is an abstract class. Only classes inheriting this class can be called.\"\n-        )\n-\n-    @abstractmethod\n-    def remaining(self):\n-        \"\"\"\n-        Returns the number of remaining steps of `advance()` in order to complete this constraint.\n-        \"\"\"\n-        raise NotImplementedError(\n-            f\"{self.__class__} is an abstract class. Only classes inheriting this class can be called.\"\n-        )\n-\n-    @abstractmethod\n-    def copy(self, stateful=False):\n-        \"\"\"\n-        Creates a new instance of this constraint.\n-\n-        Args:\n-            stateful(`bool`): Whether to not only copy the constraint for new instance, but also its state.\n-\n-        Return:\n-            constraint(`Constraint`): The same constraint as the one being called from.\n-        \"\"\"\n-        raise NotImplementedError(\n-            f\"{self.__class__} is an abstract class. Only classes inheriting this class can be called.\"\n-        )\n-\n-\n-class PhrasalConstraint(Constraint):\n-    r\"\"\"\n-    [`Constraint`] enforcing that an ordered sequence of tokens is included in the output.\n-\n-    Args:\n-        token_ids (`list[int]`):\n-            The id of the token that must be generated by the output.\n-    \"\"\"\n-\n-    def __init__(self, token_ids: list[int]):\n-        super(Constraint, self).__init__()\n-\n-        if not isinstance(token_ids, list) or len(token_ids) == 0:\n-            raise ValueError(f\"`token_ids` has to be a non-empty list, but is {token_ids}.\")\n-        if any((not isinstance(token_id, int) or token_id < 0) for token_id in token_ids):\n-            raise ValueError(f\"Each list in `token_ids` has to be a list of positive integers, but is {token_ids}.\")\n-\n-        self.token_ids = token_ids\n-\n-        self.seqlen = len(self.token_ids)\n-        self.fulfilled_idx = -1  # the index of the currently fulfilled step\n-        self.completed = False\n-\n-    def advance(self):\n-        if self.completed:\n-            return None\n-        return self.token_ids[self.fulfilled_idx + 1]\n-\n-    def does_advance(self, token_id: int):\n-        if not isinstance(token_id, int):\n-            raise TypeError(f\"`token_id` has to be an `int`, but is {token_id} of type {type(token_id)}\")\n-\n-        if self.completed:\n-            return False\n-\n-        return token_id == self.token_ids[self.fulfilled_idx + 1]\n-\n-    def update(self, token_id: int):\n-        if not isinstance(token_id, int):\n-            raise TypeError(f\"`token_id` has to be an `int`, but is {token_id} of type {type(token_id)}\")\n-\n-        stepped = False\n-        completed = False\n-        reset = False\n-\n-        if self.does_advance(token_id):\n-            self.fulfilled_idx += 1\n-            stepped = True\n-            if self.fulfilled_idx == (self.seqlen - 1):\n-                completed = True\n-            self.completed = completed\n-        else:\n-            # failed to make progress.\n-            reset = True\n-            self.reset()\n-        return stepped, completed, reset\n-\n-    def reset(self):\n-        self.completed = False\n-        self.fulfilled_idx = 0\n-\n-    def remaining(self):\n-        return self.seqlen - (self.fulfilled_idx + 1)\n-\n-    def copy(self, stateful=False):\n-        new_constraint = PhrasalConstraint(self.token_ids)\n-\n-        if stateful:\n-            new_constraint.seq_len = self.seqlen\n-            new_constraint.fulfilled_idx = self.fulfilled_idx\n-            new_constraint.completed = self.completed\n-\n-        return new_constraint\n-\n-\n-class DisjunctiveTrie:\n-    def __init__(self, nested_token_ids: list[list[int]], no_subsets=True):\n-        r\"\"\"\n-        A helper class that builds a trie with the words represented in `nested_token_ids`.\n-        \"\"\"\n-        self.max_height = max([len(one) for one in nested_token_ids])\n-\n-        root = {}\n-        for token_ids in nested_token_ids:\n-            level = root\n-            for tidx, token_id in enumerate(token_ids):\n-                if token_id not in level:\n-                    level[token_id] = {}\n-\n-                level = level[token_id]\n-\n-        if no_subsets and self.has_subsets(root, nested_token_ids):\n-            raise ValueError(\n-                \"Each list in `nested_token_ids` can't be a complete subset of another list, but is\"\n-                f\" {nested_token_ids}.\"\n-            )\n-\n-        self.trie = root\n-\n-    def next_tokens(self, current_seq):\n-        \"\"\"\n-        The next possible tokens that will progress the trie, given the current sequence of tokens in `current_seq`.\n-        \"\"\"\n-        start = self.trie\n-\n-        for current_token in current_seq:\n-            start = start[current_token]\n-\n-        next_tokens = list(start.keys())\n-\n-        return next_tokens\n-\n-    def reached_leaf(self, current_seq):\n-        next_tokens = self.next_tokens(current_seq)\n-\n-        return len(next_tokens) == 0\n-\n-    def count_leaves(self, root):\n-        next_nodes = list(root.values())\n-        if len(next_nodes) == 0:\n-            return 1\n-        else:\n-            return sum([self.count_leaves(nn) for nn in next_nodes])\n-\n-    def has_subsets(self, trie, nested_token_ids):\n-        \"\"\"\n-        Returns whether # of leaves == # of words. Otherwise some word is a subset of another.\n-        \"\"\"\n-        leaf_count = self.count_leaves(trie)\n-        return len(nested_token_ids) != leaf_count\n-\n-\n-class DisjunctiveConstraint(Constraint):\n-    r\"\"\"\n-    A special [`Constraint`] that is fulfilled by fulfilling just one of several constraints.\n-\n-    Args:\n-        nested_token_ids (`list[list[int]]`):\n-            A list of words, where each word is a list of ids. This constraint is fulfilled by generating just one from\n-            the list of words.\n-    \"\"\"\n-\n-    def __init__(self, nested_token_ids: list[list[int]]):\n-        super(Constraint, self).__init__()\n-\n-        if not isinstance(nested_token_ids, list) or len(nested_token_ids) == 0:\n-            raise ValueError(f\"`nested_token_ids` has to be a non-empty list, but is {nested_token_ids}.\")\n-        if any(not isinstance(token_ids, list) for token_ids in nested_token_ids):\n-            raise ValueError(f\"`nested_token_ids` has to be a list of lists, but is {nested_token_ids}.\")\n-        if any(\n-            any((not isinstance(token_id, int) or token_id < 0) for token_id in token_ids)\n-            for token_ids in nested_token_ids\n-        ):\n-            raise ValueError(\n-                f\"Each list in `nested_token_ids` has to be a list of positive integers, but is {nested_token_ids}.\"\n-            )\n-\n-        self.trie = DisjunctiveTrie(nested_token_ids)\n-        self.token_ids = nested_token_ids\n-\n-        self.seqlen = self.trie.max_height\n-        self.current_seq = []\n-        self.completed = False\n-\n-    def advance(self):\n-        token_list = self.trie.next_tokens(self.current_seq)\n-\n-        if len(token_list) == 0:\n-            return None\n-        else:\n-            return token_list\n-\n-    def does_advance(self, token_id: int):\n-        if not isinstance(token_id, int):\n-            raise TypeError(f\"`token_id` is supposed to be type `int`, but is {token_id} of type {type(token_id)}\")\n-\n-        next_tokens = self.trie.next_tokens(self.current_seq)\n-\n-        return token_id in next_tokens\n-\n-    def update(self, token_id: int):\n-        if not isinstance(token_id, int):\n-            raise TypeError(f\"`token_id` is supposed to be type `int`, but is {token_id} of type {type(token_id)}\")\n-\n-        stepped = False\n-        completed = False\n-        reset = False\n-\n-        if self.does_advance(token_id):\n-            self.current_seq.append(token_id)\n-            stepped = True\n-        else:\n-            reset = True\n-            self.reset()\n-\n-        completed = self.trie.reached_leaf(self.current_seq)\n-        self.completed = completed\n-\n-        return stepped, completed, reset\n-\n-    def reset(self):\n-        self.completed = False\n-        self.current_seq = []\n-\n-    def remaining(self):\n-        if self.completed:\n-            # since this can be completed without reaching max height\n-            return 0\n-        else:\n-            return self.seqlen - len(self.current_seq)\n-\n-    def copy(self, stateful=False):\n-        new_constraint = DisjunctiveConstraint(self.token_ids)\n-\n-        if stateful:\n-            new_constraint.seq_len = self.seqlen\n-            new_constraint.current_seq = self.current_seq\n-            new_constraint.completed = self.completed\n-\n-        return new_constraint\n-\n-\n-class ConstraintListState:\n-    r\"\"\"\n-    A class for beam scorers to track its progress through a list of constraints.\n-\n-    Args:\n-        constraints (`list[Constraint]`):\n-            A list of [`Constraint`] objects that must be fulfilled by the beam scorer.\n-    \"\"\"\n-\n-    def __init__(self, constraints: list[Constraint]):\n-        self.constraints = constraints\n-\n-        # max # of steps required to fulfill a given constraint\n-        self.max_seqlen = max([c.seqlen for c in constraints])\n-        self.n_constraints = len(constraints)\n-        self.completed = False\n-\n-        self.init_state()\n-\n-    def init_state(self):\n-        self.complete_constraints = []\n-        self.inprogress_constraint = None\n-        self.pending_constraints = [constraint.copy(stateful=False) for constraint in self.constraints]\n-\n-    def get_bank(self):\n-        add = 0\n-        if self.inprogress_constraint:\n-            # extra points for having a constraint mid-fulfilled\n-            add += self.max_seqlen - self.inprogress_constraint.remaining()\n-\n-        return (len(self.complete_constraints) * self.max_seqlen) + add\n-\n-    def advance(self):\n-        \"\"\"The list of tokens to generate such that we can make progress.\n-        By \"list\" we don't mean the list of token that will fully fulfill a constraint.\n-\n-        Given constraints `c_i = {t_ij | j == # of tokens}`, If we're not in the middle of progressing through a\n-        specific constraint `c_i`, we return:\n-\n-        `[t_k1 for k in indices of unfulfilled constraints]`\n-\n-        If we are in the middle of a constraint, then we return:\n-            `[t_ij]`, where `i` is the index of the inprogress constraint, `j` is the next step for the constraint.\n-\n-        Though we don't care which constraint is fulfilled first, if we are in the progress of fulfilling a constraint,\n-        that's the only one we'll return.\n-        \"\"\"\n-        token_list = []\n-        if self.inprogress_constraint is None:\n-            for constraint in self.pending_constraints:  # \"pending\" == \"unfulfilled yet\"\n-                advance = constraint.advance()\n-                if isinstance(advance, int):\n-                    token_list.append(advance)\n-                elif isinstance(advance, list):\n-                    token_list.extend(advance)\n-        else:\n-            advance = self.inprogress_constraint.advance()\n-            if isinstance(advance, int):\n-                token_list.append(advance)\n-            elif isinstance(advance, list):\n-                token_list.extend(advance)\n-\n-        if len(token_list) == 0:\n-            return None\n-        else:\n-            return token_list\n-\n-    def reset(self, token_ids: Optional[list[int]]):\n-        \"\"\"\n-        token_ids: the tokens generated thus far to reset the state of the progress through constraints.\n-        \"\"\"\n-        self.init_state()\n-\n-        if token_ids is not None:\n-            for token in token_ids:\n-                # completes or steps **one** constraint\n-                complete, stepped = self.add(token)\n-\n-                # the entire list of constraints are fulfilled\n-                if self.completed:\n-                    break\n-\n-    def add(self, token_id: int):\n-        if not isinstance(token_id, int):\n-            raise TypeError(f\"`token_id` should be an `int`, but is `{token_id}`.\")\n-\n-        complete, stepped = False, False\n-\n-        if self.completed:\n-            complete = True\n-            stepped = False\n-            return complete, stepped\n-\n-        if self.inprogress_constraint is not None:\n-            # In the middle of fulfilling a constraint. If the `token_id` *does* makes an incremental progress to current\n-            # job, simply update the state\n-\n-            stepped, complete, reset = self.inprogress_constraint.update(token_id)\n-            if reset:\n-                # 1. If the next token breaks the progress, then we must restart.\n-                #     e.g. constraint = \"I love pies\" and sequence so far is \"I love\" but `token_id` == \"books\".\n-\n-                #     But that doesn't mean we self.init_state(), since we only reset the state for this particular\n-                #     constraint, not the full list of constraints.\n-\n-                self.pending_constraints.append(self.inprogress_constraint.copy(stateful=False))\n-                self.inprogress_constraint = None\n-\n-            if complete:\n-                # 2. If the next token completes the constraint, move it to completed list, set\n-                #     inprogress to None. If there are no pending constraints either, then this full list of constraints\n-                #     is complete.\n-\n-                self.complete_constraints.append(self.inprogress_constraint)\n-                self.inprogress_constraint = None\n-\n-                if len(self.pending_constraints) == 0:\n-                    # we're done!\n-                    self.completed = True\n-\n-        else:\n-            # Not in the middle of fulfilling a constraint. So does this `token_id` helps us step towards any of our list\n-            # of constraints?\n-\n-            for cidx, pending_constraint in enumerate(self.pending_constraints):\n-                if pending_constraint.does_advance(token_id):\n-                    stepped, complete, reset = pending_constraint.update(token_id)\n-\n-                    if not stepped:\n-                        raise Exception(\n-                            \"`constraint.update(token_id)` is not yielding incremental progress, \"\n-                            \"even though `constraint.does_advance(token_id)` is true.\"\n-                        )\n-\n-                    if complete:\n-                        self.complete_constraints.append(pending_constraint)\n-                        self.inprogress_constraint = None\n-\n-                    if not complete and stepped:\n-                        self.inprogress_constraint = pending_constraint\n-\n-                    if complete or stepped:\n-                        # If we made any progress at all, then it's at least not a \"pending constraint\".\n-\n-                        self.pending_constraints = (\n-                            self.pending_constraints[:cidx] + self.pending_constraints[cidx + 1 :]\n-                        )\n-\n-                        if len(self.pending_constraints) == 0 and self.inprogress_constraint is None:\n-                            # If there's no longer any pending after this and no inprogress either, then we must be\n-                            # complete.\n-\n-                            self.completed = True\n-\n-                        break  # prevent accidentally stepping through multiple constraints with just one token.\n-\n-        return complete, stepped\n-\n-    def copy(self, stateful=True):\n-        new_state = ConstraintListState(self.constraints)  # we actually never though self.constraints objects\n-        # throughout this process. So it's at initialization state.\n-\n-        if stateful:\n-            new_state.complete_constraints = [\n-                constraint.copy(stateful=True) for constraint in self.complete_constraints\n-            ]\n-            if self.inprogress_constraint is not None:\n-                new_state.inprogress_constraint = self.inprogress_constraint.copy(stateful=True)\n-            new_state.pending_constraints = [constraint.copy() for constraint in self.pending_constraints]\n-\n-        return new_state"
        },
        {
            "sha": "8510a02c803abcb0d8580bce473167fd09e8d2fb",
            "filename": "src/transformers/generation/beam_search.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1002,
            "changes": 1002,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f143006635effe2b38e95566a2f347b303916bb/src%2Ftransformers%2Fgeneration%2Fbeam_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f143006635effe2b38e95566a2f347b303916bb/src%2Ftransformers%2Fgeneration%2Fbeam_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fbeam_search.py?ref=8f143006635effe2b38e95566a2f347b303916bb",
            "patch": "@@ -1,1002 +0,0 @@\n-# coding=utf-8\n-# Copyright 2020 The HuggingFace Inc. team\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from abc import ABC, abstractmethod\n-from collections import UserDict\n-from typing import Optional, Union\n-\n-import numpy as np\n-import torch\n-\n-from ..utils import add_start_docstrings, logging\n-from .beam_constraints import Constraint, ConstraintListState\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-PROCESS_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size * num_beams, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using any class inheriting from [`PreTrainedTokenizer`]. See\n-            [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        next_scores (`torch.FloatTensor` of shape `(batch_size, 2 * num_beams)`):\n-            Current scores of the top `2 * num_beams` non-finished beam hypotheses.\n-        next_tokens (`torch.LongTensor` of shape `(batch_size, 2 * num_beams)`):\n-            `input_ids` of the tokens corresponding to the top `2 * num_beams` non-finished beam hypotheses.\n-        next_indices (`torch.LongTensor` of shape `(batch_size, 2 * num_beams)`):\n-            Beam indices indicating to which beam hypothesis the `next_tokens` correspond.\n-        pad_token_id (`int`, *optional*):\n-            The id of the *padding* token.\n-        eos_token_id (`Union[int, list[int]]`, *optional*):\n-            The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n-        beam_indices (`torch.LongTensor`, *optional*):\n-            Beam indices indicating to which beam hypothesis each token correspond.\n-\n-    Return:\n-        `UserDict`: A dictionary composed of the fields as defined above:\n-\n-            - **next_beam_scores** (`torch.FloatTensor` of shape `(batch_size * num_beams)`) -- Updated scores of all\n-              non-finished beams.\n-            - **next_beam_tokens** (`torch.FloatTensor` of shape `(batch_size * num_beams)`) -- Next tokens to be added\n-              to the non-finished beam_hypotheses.\n-            - **next_beam_indices** (`torch.FloatTensor` of shape `(batch_size * num_beams)`) -- Beam indices\n-              indicating to which beam the next tokens shall be added.\n-\n-\"\"\"\n-\n-FINALIZE_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size * num_beams, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using any class inheriting from [`PreTrainedTokenizer`]. See\n-            [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        final_beam_scores (`torch.FloatTensor` of shape `(batch_size * num_beams)`):\n-            The final scores of all non-finished beams.\n-        final_beam_tokens (`torch.FloatTensor` of shape `(batch_size * num_beams)`):\n-            The last tokens to be added to the non-finished beam_hypotheses.\n-        final_beam_indices (`torch.FloatTensor` of shape `(batch_size * num_beams)`):\n-            The beam indices indicating to which beam the `final_beam_tokens` shall be added.\n-        pad_token_id (`int`, *optional*):\n-            The id of the *padding* token.\n-        eos_token_id (`Union[int, list[int]]`, *optional*):\n-            The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n-\n-    Return:\n-        `torch.LongTensor` of shape `(batch_size * num_return_sequences, sequence_length)`: The generated sequences.\n-        The second dimension (sequence_length) is either equal to `max_length` or shorter if all batches finished early\n-        due to the `eos_token_id`.\n-\n-\"\"\"\n-\n-\n-class BeamScorer(ABC):\n-    \"\"\"\n-    Abstract base class for all beam scorers that are used for [`~PreTrainedModel.beam_search`] and\n-    [`~PreTrainedModel.beam_sample`].\n-    \"\"\"\n-\n-    @abstractmethod\n-    @add_start_docstrings(PROCESS_INPUTS_DOCSTRING)\n-    def process(\n-        self,\n-        input_ids: torch.LongTensor,\n-        next_scores: torch.FloatTensor,\n-        next_tokens: torch.LongTensor,\n-        next_indices: torch.LongTensor,\n-        **kwargs,\n-    ) -> tuple[torch.Tensor]:\n-        raise NotImplementedError(\"This is an abstract method.\")\n-\n-    @abstractmethod\n-    @add_start_docstrings(FINALIZE_INPUTS_DOCSTRING)\n-    def finalize(\n-        self,\n-        input_ids: torch.LongTensor,\n-        next_scores: torch.FloatTensor,\n-        next_tokens: torch.LongTensor,\n-        next_indices: torch.LongTensor,\n-        max_length: int,\n-        **kwargs,\n-    ) -> torch.LongTensor:\n-        raise NotImplementedError(\"This is an abstract method.\")\n-\n-\n-class BeamSearchScorer(BeamScorer):\n-    r\"\"\"\n-    [`BeamScorer`] implementing standard beam search decoding.\n-\n-    Adapted in part from [Facebook's XLM beam search\n-    code](https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529).\n-\n-    Reference for the diverse beam search algorithm and implementation [Ashwin Kalyan's DBS\n-    implementation](https://github.com/ashwinkalyan/dbs/blob/master/dbs/beam_utils.lua)\n-\n-    Args:\n-        batch_size (`int`):\n-            Batch Size of `input_ids` for which standard beam search decoding is run in parallel.\n-        num_beams (`int`):\n-            Number of beams for beam search.\n-        device (`torch.device`):\n-            Defines the device type (*e.g.*, `\"cpu\"` or `\"cuda\"`) on which this instance of `BeamSearchScorer` will be\n-            allocated.\n-        length_penalty (`float`, *optional*, defaults to 1.0):\n-            Exponential penalty to the length that is used with beam-based generation. It is applied as an exponent to\n-            the sequence length, which in turn is used to divide the score of the sequence. Since the score is the log\n-            likelihood of the sequence (i.e. negative), `length_penalty` > 0.0 promotes longer sequences, while\n-            `length_penalty` < 0.0 encourages shorter sequences.\n-        do_early_stopping (`bool` or `str`, *optional*, defaults to `False`):\n-            Controls the stopping condition for beam-based methods, like beam-search. It accepts the following values:\n-            `True`, where the generation stops as soon as there are `num_beams` complete candidates; `False`, where an\n-            heuristic is applied and the generation stops when is it very unlikely to find better candidates;\n-            `\"never\"`, where the beam search procedure only stops when there cannot be better candidates (canonical\n-            beam search algorithm).\n-        num_beam_hyps_to_keep (`int`, *optional*, defaults to 1):\n-            The number of beam hypotheses that shall be returned upon calling\n-            [`~transformers.BeamSearchScorer.finalize`].\n-        num_beam_groups (`int`, *optional*, defaults to 1):\n-            Number of groups to divide `num_beams` into in order to ensure diversity among different groups of beams.\n-            See [this paper](https://huggingface.co/papers/1610.02424) for more details.\n-        max_length (`int`, *optional*):\n-            The maximum length of the sequence to be generated.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        batch_size: int,\n-        num_beams: int,\n-        device: torch.device,\n-        length_penalty: float = 1.0,\n-        do_early_stopping: Union[bool, str] = False,\n-        num_beam_hyps_to_keep: int = 1,\n-        num_beam_groups: int = 1,\n-        max_length: Optional[int] = None,\n-    ):\n-        logger.warning_once(\n-            \"`BeamSearchScorer` is deprecated and will be removed in v4.62.0, as constrained beam search has been moved to the Hub: https://hf.co/transformers-community/constrained-beam-search.\"\n-        )\n-        self.num_beams = num_beams\n-        self.device = device\n-        self.length_penalty = length_penalty\n-        self.do_early_stopping = do_early_stopping\n-        self.num_beam_hyps_to_keep = num_beam_hyps_to_keep\n-        self.num_beam_groups = num_beam_groups\n-        self.group_size = self.num_beams // self.num_beam_groups\n-\n-        self._is_init = False\n-        # self._beam_hyps[i*self.num_beam_groups+j] is the beam_hyps of the j-th group in the i-th mini-batch.\n-        # If group_beam_search is not used, the list consists of `batch_size` beam_hyps.\n-        self._beam_hyps = [\n-            BeamHypotheses(\n-                num_beams=self.group_size,\n-                length_penalty=self.length_penalty,\n-                early_stopping=self.do_early_stopping,\n-                max_length=max_length,\n-            )\n-            for _ in range(batch_size * self.num_beam_groups)\n-        ]\n-        # self._done[i*self.num_beam_groups+j] indicates whether the generation of the beam_hyps of the j-th group\n-        # in the i-th mini-batch is complete.\n-        self._done = torch.tensor(\n-            [False for _ in range(batch_size * self.num_beam_groups)], dtype=torch.bool, device=self.device\n-        )\n-\n-        if not isinstance(num_beams, int) or num_beams <= 1:\n-            raise ValueError(\n-                f\"`num_beams` has to be an integer strictly greater than 1, but is {num_beams}. For `num_beams` == 1,\"\n-                \" one should make use of `greedy_search` instead.\"\n-            )\n-\n-        if not isinstance(num_beam_groups, int) or (num_beam_groups > num_beams) or (num_beams % num_beam_groups != 0):\n-            raise ValueError(\n-                \"`num_beam_groups` has to be an integer smaller or equal than `num_beams` and `num_beams` has to be\"\n-                f\" divisible by `num_beam_groups`, but is {num_beam_groups} with `num_beams` being {num_beams}.\"\n-            )\n-\n-    @property\n-    def is_done(self) -> bool:\n-        return self._done.all().item()\n-\n-    def process(\n-        self,\n-        input_ids: torch.LongTensor,\n-        next_scores: torch.FloatTensor,\n-        next_tokens: torch.LongTensor,\n-        next_indices: torch.LongTensor,\n-        pad_token_id: Optional[Union[int, torch.Tensor]] = None,\n-        eos_token_id: Optional[Union[int, list[int], torch.Tensor]] = None,\n-        beam_indices: Optional[torch.LongTensor] = None,\n-        group_index: int = 0,\n-        decoder_prompt_len: int = 0,\n-    ) -> dict[str, torch.Tensor]:\n-        # add up to the length which the next_scores is calculated on (including decoder prompt)\n-        cur_len = input_ids.shape[-1] + 1\n-        batch_size = len(self._beam_hyps) // self.num_beam_groups\n-\n-        if batch_size != (input_ids.shape[0] // self.group_size):\n-            if self.num_beam_groups > 1:\n-                raise ValueError(\n-                    f\"A group beam size of {input_ids.shape[0]} is used as the input, but a group beam \"\n-                    f\"size of {self.group_size} is expected by the beam scorer.\"\n-                )\n-            else:\n-                raise ValueError(\n-                    f\"A beam size of {input_ids.shape[0]} is used as the input, but a beam size of \"\n-                    f\"{self.group_size} is expected by the beam scorer.\"\n-                )\n-\n-        device = input_ids.device\n-        next_beam_scores = torch.zeros((batch_size, self.group_size), dtype=next_scores.dtype, device=device)\n-        next_beam_tokens = torch.zeros((batch_size, self.group_size), dtype=next_tokens.dtype, device=device)\n-        next_beam_indices = torch.zeros((batch_size, self.group_size), dtype=next_indices.dtype, device=device)\n-\n-        if eos_token_id is not None and not isinstance(eos_token_id, torch.Tensor):\n-            if isinstance(eos_token_id, int):\n-                eos_token_id = [eos_token_id]\n-            eos_token_id = torch.tensor(eos_token_id)\n-\n-        for batch_idx in range(batch_size):\n-            batch_group_idx = batch_idx * self.num_beam_groups + group_index\n-            if self._done[batch_group_idx]:\n-                if self.num_beams < len(self._beam_hyps[batch_group_idx]):\n-                    raise ValueError(f\"Batch can only be done if at least {self.num_beams} beams have been generated\")\n-                if eos_token_id is None or pad_token_id is None:\n-                    raise ValueError(\"Generated beams >= num_beams -> eos_token_id and pad_token have to be defined\")\n-                # pad the batch\n-                next_beam_scores[batch_idx, :] = 0\n-                next_beam_tokens[batch_idx, :] = pad_token_id\n-                next_beam_indices[batch_idx, :] = 0\n-                continue\n-\n-            # next tokens for this sentence\n-            beam_idx = 0\n-            for beam_token_rank, (next_token, next_score, next_index) in enumerate(\n-                zip(next_tokens[batch_idx], next_scores[batch_idx], next_indices[batch_idx])\n-            ):\n-                batch_beam_idx = batch_idx * self.group_size + next_index\n-                # add to generated hypotheses if end of sentence\n-                if (eos_token_id is not None) and (next_token.item() in eos_token_id):\n-                    # if beam_token does not belong to top num_beams tokens, it should not be added\n-                    is_beam_token_worse_than_top_num_beams = beam_token_rank >= self.group_size\n-                    if is_beam_token_worse_than_top_num_beams:\n-                        continue\n-                    if beam_indices is not None:\n-                        beam_index = beam_indices[batch_beam_idx]\n-                        beam_index = beam_index + (batch_beam_idx,)\n-                    else:\n-                        beam_index = None\n-\n-                    self._beam_hyps[batch_group_idx].add(\n-                        input_ids[batch_beam_idx].clone(),\n-                        next_score.item(),\n-                        beam_indices=beam_index,\n-                        generated_len=cur_len - decoder_prompt_len,\n-                    )\n-                else:\n-                    # add next predicted token since it is not eos_token\n-                    next_beam_scores[batch_idx, beam_idx] = next_score\n-                    next_beam_tokens[batch_idx, beam_idx] = next_token\n-                    next_beam_indices[batch_idx, beam_idx] = batch_beam_idx\n-                    beam_idx += 1\n-\n-                # once the beam for next step is full, don't add more tokens to it.\n-                if beam_idx == self.group_size:\n-                    break\n-\n-            if beam_idx < self.group_size:\n-                raise ValueError(\n-                    f\"At most {self.group_size} tokens in {next_tokens[batch_idx]} can be equal to `eos_token_id:\"\n-                    f\" {eos_token_id}`. Make sure {next_tokens[batch_idx]} are corrected.\"\n-                )\n-\n-            # Check if we are done so that we can save a pad step if all(done)\n-            self._done[batch_group_idx] = self._done[batch_group_idx] or self._beam_hyps[batch_group_idx].is_done(\n-                next_scores[batch_idx].max().item(), cur_len, decoder_prompt_len\n-            )\n-\n-        return UserDict(\n-            {\n-                \"next_beam_scores\": next_beam_scores.view(-1),\n-                \"next_beam_tokens\": next_beam_tokens.view(-1),\n-                \"next_beam_indices\": next_beam_indices.view(-1),\n-            }\n-        )\n-\n-    def finalize(\n-        self,\n-        input_ids: torch.LongTensor,\n-        final_beam_scores: torch.FloatTensor,\n-        final_beam_tokens: torch.LongTensor,\n-        final_beam_indices: torch.LongTensor,\n-        max_length: int,\n-        pad_token_id: Optional[Union[int, torch.Tensor]] = None,\n-        eos_token_id: Optional[Union[int, list[int], torch.Tensor]] = None,\n-        beam_indices: Optional[torch.LongTensor] = None,\n-        decoder_prompt_len: int = 0,\n-    ) -> tuple[torch.LongTensor]:\n-        batch_size = len(self._beam_hyps) // self.num_beam_groups\n-\n-        if eos_token_id is not None and not isinstance(eos_token_id, torch.Tensor):\n-            if isinstance(eos_token_id, int):\n-                eos_token_id = [eos_token_id]\n-            eos_token_id = torch.tensor(eos_token_id)\n-\n-        # finalize all open beam hypotheses and add to generated hypotheses\n-        for batch_group_idx, beam_hyp in enumerate(self._beam_hyps):\n-            if self._done[batch_group_idx]:\n-                continue\n-\n-            # all open beam hypotheses are added to the beam hypothesis\n-            # beam hypothesis class automatically keeps the best beams\n-            for index_per_group in range(self.group_size):\n-                batch_beam_idx = batch_group_idx * self.group_size + index_per_group\n-                final_score = final_beam_scores[batch_beam_idx].item()\n-                final_tokens = input_ids[batch_beam_idx]\n-                beam_index = beam_indices[batch_beam_idx] if beam_indices is not None else None\n-                generated_len = final_tokens.shape[-1] - decoder_prompt_len\n-                beam_hyp.add(final_tokens, final_score, beam_indices=beam_index, generated_len=generated_len)\n-\n-        # select the best hypotheses\n-        sent_lengths = input_ids.new(batch_size * self.num_beam_hyps_to_keep)\n-        best = []\n-        best_indices = []\n-        best_scores = torch.zeros(batch_size * self.num_beam_hyps_to_keep, device=self.device, dtype=torch.float32)\n-\n-        # retrieve best hypotheses\n-        for i in range(batch_size):\n-            beam_hyps_in_batch = self._beam_hyps[i * self.num_beam_groups : (i + 1) * self.num_beam_groups]\n-            candidate_beams = [beam for beam_hyp in beam_hyps_in_batch for beam in beam_hyp.beams]\n-            sorted_hyps = sorted(candidate_beams, key=lambda x: x[0])\n-            for j in range(self.num_beam_hyps_to_keep):\n-                best_hyp_tuple = sorted_hyps.pop()\n-                best_score = best_hyp_tuple[0]\n-                best_hyp = best_hyp_tuple[1]\n-                best_index = best_hyp_tuple[2]\n-                sent_lengths[self.num_beam_hyps_to_keep * i + j] = len(best_hyp)\n-\n-                # append hyp to lists\n-                best.append(best_hyp)\n-\n-                # append indices to list\n-                best_indices.append(best_index)\n-\n-                best_scores[i * self.num_beam_hyps_to_keep + j] = best_score\n-\n-        # prepare for adding eos\n-        sent_lengths_max = sent_lengths.max().item() + 1\n-        sent_max_len = min(sent_lengths_max, max_length) if max_length is not None else sent_lengths_max\n-        decoded: torch.LongTensor = input_ids.new(batch_size * self.num_beam_hyps_to_keep, sent_max_len)\n-\n-        if len(best_indices) > 0 and best_indices[0] is not None:\n-            indices: torch.LongTensor = input_ids.new(batch_size * self.num_beam_hyps_to_keep, sent_max_len)\n-        else:\n-            indices = None\n-\n-        # shorter batches are padded if needed\n-        if sent_lengths.min().item() != sent_lengths.max().item():\n-            if pad_token_id is None:\n-                raise ValueError(\"`pad_token_id` has to be defined\")\n-            decoded.fill_(pad_token_id)\n-\n-        if indices is not None:\n-            indices.fill_(-1)\n-\n-        # fill with hypotheses and eos_token_id if the latter fits in\n-        for i, (hypo, best_idx) in enumerate(zip(best, best_indices)):\n-            decoded[i, : sent_lengths[i]] = hypo\n-\n-            if indices is not None:\n-                indices[i, : len(best_idx)] = torch.tensor(best_idx)\n-\n-            if sent_lengths[i] < sent_max_len:\n-                # inserting only the first eos_token_id\n-                decoded[i, sent_lengths[i]] = eos_token_id[0]\n-\n-        return UserDict(\n-            {\n-                \"sequences\": decoded,\n-                \"sequence_scores\": best_scores,\n-                \"beam_indices\": indices,\n-            }\n-        )\n-\n-\n-class ConstrainedBeamSearchScorer(BeamScorer):\n-    r\"\"\"\n-    [`BeamScorer`] implementing constrained beam search decoding.\n-\n-\n-    Args:\n-        batch_size (`int`):\n-            Batch Size of `input_ids` for which standard beam search decoding is run in parallel.\n-        num_beams (`int`):\n-            Number of beams for beam search.\n-        constraints (`list[Constraint]`):\n-            A list of positive constraints represented as `Constraint` objects that must be fulfilled in the generation\n-            output. For more information, the documentation of [`Constraint`] should be read.\n-        device (`torch.device`):\n-            Defines the device type (*e.g.*, `\"cpu\"` or `\"cuda\"`) on which this instance of `BeamSearchScorer` will be\n-            allocated.\n-        length_penalty (`float`, *optional*, defaults to 1.0):\n-            Exponential penalty to the length that is used with beam-based generation. It is applied as an exponent to\n-            the sequence length, which in turn is used to divide the score of the sequence. Since the score is the log\n-            likelihood of the sequence (i.e. negative), `length_penalty` > 0.0 promotes longer sequences, while\n-            `length_penalty` < 0.0 encourages shorter sequences.\n-        do_early_stopping (`bool` or `str`, *optional*, defaults to `False`):\n-            Controls the stopping condition for beam-based methods, like beam-search. It accepts the following values:\n-            `True`, where the generation stops as soon as there are `num_beams` complete candidates; `False`, where an\n-            heuristic is applied and the generation stops when is it very unlikely to find better candidates;\n-            `\"never\"`, where the beam search procedure only stops when there cannot be better candidates (canonical\n-            beam search algorithm).\n-        num_beam_hyps_to_keep (`int`, *optional*, defaults to 1):\n-            The number of beam hypotheses that shall be returned upon calling\n-            [`~transformers.BeamSearchScorer.finalize`].\n-        max_length (`int`, *optional*):\n-            The maximum length of the sequence to be generated.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        batch_size: int,\n-        num_beams: int,\n-        constraints: list[Constraint],\n-        device: torch.device,\n-        length_penalty: float = 1.0,\n-        do_early_stopping: Union[bool, str] = False,\n-        num_beam_hyps_to_keep: int = 1,\n-        max_length: Optional[int] = None,\n-    ):\n-        logger.warning_once(\n-            \"`ConstrainedBeamSearchScorer` is deprecated and will be removed in v4.62.0, as constrained beam search has been moved to the Hub: https://hf.co/transformers-community/constrained-beam-search.\"\n-        )\n-        self.num_beams = num_beams\n-        self.device = device\n-        self.length_penalty = length_penalty\n-        self.do_early_stopping = do_early_stopping\n-        self.num_beam_hyps_to_keep = num_beam_hyps_to_keep\n-        self.constraints = constraints\n-\n-        self._is_init = False\n-        self._beam_hyps = [\n-            BeamHypotheses(\n-                num_beams=self.num_beams,\n-                length_penalty=self.length_penalty,\n-                early_stopping=self.do_early_stopping,\n-                max_length=max_length,\n-            )\n-            for _ in range(batch_size)\n-        ]\n-        self._done = torch.tensor([False for _ in range(batch_size)], dtype=torch.bool, device=self.device)\n-\n-        if not isinstance(num_beams, int) or num_beams <= 1:\n-            raise ValueError(\n-                f\"`num_beams` has to be an integer strictly greater than 1, but is {num_beams}. For `num_beams` == 1,\"\n-                \" one should make use of `greedy_search` instead.\"\n-            )\n-\n-    @property\n-    def is_done(self) -> bool:\n-        return self._done.all().item()\n-\n-    def make_constraint_states(self, n):\n-        return [ConstraintListState([constraint.copy() for constraint in self.constraints]) for _ in range(n)]\n-\n-    def check_completes_constraints(self, sequence):\n-        new_state = self.make_constraint_states(1)[0]\n-        new_state.reset(sequence)\n-        return new_state.completed\n-\n-    def process(\n-        self,\n-        input_ids: torch.LongTensor,\n-        next_scores: torch.FloatTensor,\n-        next_tokens: torch.LongTensor,\n-        next_indices: torch.LongTensor,\n-        scores_for_all_vocab: torch.FloatTensor,\n-        pad_token_id: Optional[Union[int, torch.Tensor]] = None,\n-        eos_token_id: Optional[Union[int, list[int], torch.Tensor]] = None,\n-        beam_indices: Optional[torch.LongTensor] = None,\n-        decoder_prompt_len: int = 0,\n-    ) -> tuple[torch.Tensor]:\n-        r\"\"\"\n-        Args:\n-            input_ids (`torch.LongTensor` of shape `(batch_size * num_beams, sequence_length)`):\n-                Indices of input sequence tokens in the vocabulary.\n-\n-                Indices can be obtained using any class inheriting from [`PreTrainedTokenizer`]. See\n-                [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.\n-\n-                [What are input IDs?](../glossary#input-ids)\n-            next_scores (`torch.FloatTensor` of shape `(batch_size, 2 * num_beams)`):\n-                Current scores of the top `2 * num_beams` non-finished beam hypotheses.\n-            next_tokens (`torch.LongTensor` of shape `(batch_size, 2 * num_beams)`):\n-                `input_ids` of the tokens corresponding to the top `2 * num_beams` non-finished beam hypotheses.\n-            next_indices (`torch.LongTensor` of shape `(batch_size, 2 * num_beams)`):\n-                Beam indices indicating to which beam hypothesis the `next_tokens` correspond.\n-            scores_for_all_vocab (`torch.FloatTensor` of shape `(batch_size * num_beams, sequence_length)`):\n-                The scores of all tokens in the vocabulary for each of the beam hypotheses.\n-            pad_token_id (`int`, *optional*):\n-                The id of the *padding* token.\n-            eos_token_id (`Union[int, list[int]]`, *optional*):\n-                The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n-            beam_indices (`torch.LongTensor`, *optional*):\n-                Beam indices indicating to which beam hypothesis each token correspond.\n-            decoder_prompt_len (`int`, *optional*):\n-                The length of prompt that is included in the input to decoder.\n-        Return:\n-            `UserDict`: A dictionary composed of the fields as defined above:\n-\n-                - **next_beam_scores** (`torch.FloatTensor` of shape `(batch_size * num_beams)`) -- Updated scores of\n-                  all\n-                non-finished beams.\n-\n-                - **next_beam_tokens** (`torch.FloatTensor` of shape `(batch_size * num_beams)`) -- Next tokens to be\n-                  added\n-                to the non-finished beam_hypotheses.\n-                - **next_beam_indices** (`torch.FloatTensor` of shape `(batch_size * num_beams)`) -- Beam indices\n-                indicating to which beam the next tokens shall be added.\n-        \"\"\"\n-\n-        # add up to the length which the next_scores is calculated on (including decoder prompt)\n-        cur_len = input_ids.shape[-1] + 1\n-        batch_size = len(self._beam_hyps)\n-\n-        device = input_ids.device\n-\n-        next_beam_scores = torch.zeros((batch_size, self.num_beams), dtype=next_scores.dtype, device=device)\n-        next_beam_tokens = torch.zeros((batch_size, self.num_beams), dtype=next_tokens.dtype, device=device)\n-        next_beam_indices = torch.zeros((batch_size, self.num_beams), dtype=next_indices.dtype, device=device)\n-\n-        if eos_token_id is not None and not isinstance(eos_token_id, torch.Tensor):\n-            if isinstance(eos_token_id, int):\n-                eos_token_id = [eos_token_id]\n-            eos_token_id = torch.tensor(eos_token_id)\n-\n-        for batch_idx, beam_hyp in enumerate(self._beam_hyps):\n-            if self._done[batch_idx]:\n-                if self.num_beams < len(beam_hyp):\n-                    raise ValueError(f\"Batch can only be done if at least {self.num_beams} beams have been generated\")\n-                if eos_token_id is None or pad_token_id is None:\n-                    raise ValueError(\"Generated beams >= num_beams -> eos_token_id and pad_token have to be defined\")\n-                # pad the batch\n-                next_beam_scores[batch_idx, :] = 0\n-                next_beam_tokens[batch_idx, :] = pad_token_id\n-                next_beam_indices[batch_idx, :] = 0\n-                continue\n-\n-            # next tokens for this sentence.\n-            beam_idx = 0\n-            for beam_token_rank, (next_token, next_score, next_index) in enumerate(\n-                zip(next_tokens[batch_idx], next_scores[batch_idx], next_indices[batch_idx])\n-            ):\n-                batch_beam_idx = batch_idx * self.num_beams + next_index\n-                # add to generated hypotheses if end of sentence\n-                if (eos_token_id is not None) and (next_token.item() in eos_token_id):\n-                    # if beam_token does not belong to top num_beams tokens, it should not be added\n-                    is_beam_token_worse_than_top_num_beams = beam_token_rank >= self.num_beams\n-                    if is_beam_token_worse_than_top_num_beams:\n-                        continue\n-\n-                    completes_constraint = self.check_completes_constraints(input_ids[batch_beam_idx].tolist())\n-                    if completes_constraint:\n-                        if beam_indices is not None:\n-                            beam_index = beam_indices[batch_beam_idx]\n-                            beam_index = beam_index + (batch_beam_idx,)\n-                        else:\n-                            beam_index = None\n-\n-                        beam_hyp.add(\n-                            input_ids[batch_beam_idx].clone(),\n-                            next_score.item(),\n-                            beam_indices=beam_index,\n-                            generated_len=cur_len - decoder_prompt_len,\n-                        )\n-                else:\n-                    # add next predicted token since it is not eos_token\n-                    next_beam_scores[batch_idx, beam_idx] = next_score\n-                    next_beam_tokens[batch_idx, beam_idx] = next_token\n-                    next_beam_indices[batch_idx, beam_idx] = batch_beam_idx\n-                    beam_idx += 1\n-\n-                # once the beam for next step is full, don't add more tokens to it.\n-                if beam_idx == self.num_beams:\n-                    break\n-\n-            new_scores, new_tokens, new_indices = self.step_sentence_constraint(\n-                batch_idx,\n-                input_ids,\n-                scores_for_all_vocab,\n-                next_beam_scores[batch_idx],\n-                next_beam_tokens[batch_idx],\n-                next_beam_indices[batch_idx],\n-            )\n-\n-            next_beam_scores[batch_idx] = new_scores\n-            next_beam_tokens[batch_idx] = new_tokens\n-            next_beam_indices[batch_idx] = new_indices\n-\n-            if beam_idx < self.num_beams:\n-                raise ValueError(\n-                    f\"At most {self.num_beams} tokens in {next_tokens[batch_idx]} can be equal to `eos_token_id:\"\n-                    f\" {eos_token_id}`. Make sure {next_tokens[batch_idx]} are corrected.\"\n-                )\n-\n-            # Check if we are done so that we can save a pad step if all(done)\n-            self._done[batch_idx] = self._done[batch_idx] or beam_hyp.is_done(\n-                next_scores[batch_idx].max().item(), cur_len, decoder_prompt_len\n-            )\n-\n-        return UserDict(\n-            {\n-                \"next_beam_scores\": next_beam_scores.view(-1),\n-                \"next_beam_tokens\": next_beam_tokens.view(-1),\n-                \"next_beam_indices\": next_beam_indices.view(-1),\n-            }\n-        )\n-\n-    def step_sentence_constraint(\n-        self,\n-        batch_idx: int,\n-        input_ids: torch.LongTensor,\n-        vocab_scores: torch.FloatTensor,\n-        sent_beam_scores: torch.FloatTensor,\n-        sent_beam_tokens: torch.LongTensor,\n-        sent_beam_indices: torch.LongTensor,\n-        push_progress: bool = False,\n-    ):\n-        # sent_beam_tokens are the next {num_beams} number of tokens that are under consideration for this beam\n-        # (candidate next tokens)\n-\n-        # 1. Adding \"advance_tokens\"\n-        #     using ConstraintStateList.advance(), we propose new tokens to be added into this \"candidate list\" that will\n-        #     advance us in fulfilling the constraints.\n-\n-        # 2. Selecting best candidates such that we end up with highest probable candidates\n-        #     that fulfill our constraints.\n-\n-        orig_len = sent_beam_indices.size(0)\n-        device = sent_beam_indices.device\n-\n-        # initialize states\n-        topk_constraint_states = self.make_constraint_states(orig_len)\n-        advance_constraint_states = self.make_constraint_states(orig_len)\n-\n-        sidx, eidx = batch_idx * orig_len, (batch_idx + 1) * orig_len\n-        this_batch_input_ids = input_ids[sidx:eidx]\n-        this_batch_token_scores = vocab_scores[sidx:eidx]\n-        full_hypotheses = torch.cat((input_ids[sent_beam_indices], sent_beam_tokens.unsqueeze(-1)), dim=-1)\n-\n-        # need to make new hypothesis that advance the constraints\n-        track_new = {\n-            \"new_seqs\": full_hypotheses.tolist(),\n-            \"new_states\": [],\n-            \"new_indices\": [],\n-            \"new_tokens\": [],\n-            \"new_scores\": [],\n-        }\n-        for seq_idx, pre_seq in enumerate(this_batch_input_ids):\n-            # pre_seq = ith sequence generated before this step.\n-\n-            # input_ids -> (topk) generic beam search best model next tokens\n-            #           -> (advance) constraints forcing the next token\n-            # either way, we need to sort them into \"banks\" later, so store a \"ConstraintListState\" for all types of\n-            # hypotheses.\n-\n-            topk_state = topk_constraint_states[seq_idx]\n-            topk_state.reset(full_hypotheses[seq_idx].tolist())\n-\n-            advance_state = advance_constraint_states[seq_idx]\n-            advance_state.reset(pre_seq.tolist())\n-\n-            if not advance_state.completed:\n-                advance_tokens = torch.tensor(advance_state.advance(), dtype=torch.long, device=device)\n-                for advance_token in advance_tokens:\n-                    # since adding each `advance_token` leads to a different hypothesis, create new state instance.\n-                    new_state = advance_state.copy(stateful=True)\n-                    new_state.add(advance_token.tolist())\n-\n-                    advance_seq = torch.cat((pre_seq, advance_token.unsqueeze(0)), -1).tolist()\n-                    if advance_seq not in track_new[\"new_seqs\"]:\n-                        # prevent duplicates, which are basically bound to happen in this process.\n-                        track_new[\"new_seqs\"].append(advance_seq)\n-                        track_new[\"new_indices\"].append(sidx + seq_idx)  # idx -> global idx across all the batches\n-                        track_new[\"new_tokens\"].append(advance_token)\n-                        track_new[\"new_scores\"].append(this_batch_token_scores[seq_idx].take(advance_token))\n-                        track_new[\"new_states\"].append(new_state)\n-            elif push_progress:\n-                # Basically, `sent_beam_indices` often chooses very little among `input_ids` the generated sequences that\n-                # actually fulfill our constraints. For example, let constraints == [\"loves pies\"] and\n-\n-                #     pre_seq_1 = \"The child loves pies and\" pre_seq_2 = \"The child plays in the playground and\"\n-\n-                # Without this step, if `sent_beam_indices` is something like [1,1], then\n-                #     1. `pre_seq_1` won't be added to the list of (topk) hypothesis since it's not in the indices and\n-                #     2.  it won't be added to the list of (advance) hypothesis since it's completed already. (this is\n-                #         the else part of `if constraints_completed[seq_idx]`)\n-                #     3. it ends up simply getting removed from consideration.\n-\n-                # #3 might be fine and actually desired, since it's likely that it's a low-probability output anyways,\n-                # especially if it's not in the list of `sent_beam_indices`. But this often leads to lengthened beam\n-                # search times, since completed sequences keep getting removed after all this effort for constrained\n-                # generation.\n-\n-                # Here, we basically take `pre_seq_1` and to \"push\" it into the considered list of hypotheses, by simply\n-                # appending the next likely token in the vocabulary and adding it to the list of hypotheses.\n-\n-                new_score, new_token = torch.max(this_batch_token_scores[seq_idx], 0)  # some next probable token\n-                advance_seq = torch.cat((pre_seq, new_token.unsqueeze(0)), -1)\n-\n-                advance_state = advance_constraint_states[seq_idx]\n-\n-                advance_seq = advance_seq.tolist()\n-\n-                advance_state.reset(advance_seq)\n-                if advance_seq not in track_new[\"new_seqs\"]:\n-                    # but still don't want to have duplicates\n-                    track_new[\"new_seqs\"].append(advance_seq)\n-                    track_new[\"new_indices\"].append(seq_idx)\n-                    track_new[\"new_tokens\"].append(new_token)\n-                    track_new[\"new_scores\"].append(new_score)\n-                    track_new[\"new_states\"].append(advance_state)\n-\n-        if len(track_new[\"new_indices\"]) > 0:\n-            new_indices = torch.tensor(track_new[\"new_indices\"], device=device)\n-            new_tokens = torch.stack(track_new[\"new_tokens\"]).to(device)\n-            new_scores = torch.stack(track_new[\"new_scores\"]).to(device)\n-\n-            all_states = topk_constraint_states + track_new[\"new_states\"]\n-            all_tokens = torch.cat((sent_beam_tokens, new_tokens), -1)\n-            all_scores = torch.cat((sent_beam_scores, new_scores), -1)\n-            all_banks = torch.tensor([one.get_bank() for one in all_states], device=device)\n-\n-            zipped = all_banks * 100 + all_scores\n-            indices = zipped.sort(descending=True).indices\n-            sorted_banks = all_banks[indices]\n-\n-            # Then we end up with {sorted among bank C}, {sorted among bank C-1}, ..., {sorted among bank 0}\n-\n-            counter = -1\n-            cur_bank = sorted_banks[0]\n-            increments = []\n-            for bank in sorted_banks:\n-                if bank == cur_bank:\n-                    counter += 1\n-                else:\n-                    counter = 0\n-                    cur_bank = bank\n-                increments.append(counter)\n-            rearrangers = torch.tensor(np.argsort(increments, kind=\"mergesort\"))\n-\n-            indices = indices[rearrangers][:orig_len]\n-\n-            sent_beam_scores = all_scores[indices]\n-            sent_beam_tokens = all_tokens[indices]\n-            sent_beam_indices = torch.cat((sent_beam_indices, new_indices))[indices]\n-\n-        return sent_beam_scores, sent_beam_tokens, sent_beam_indices\n-\n-    def finalize(\n-        self,\n-        input_ids: torch.LongTensor,\n-        final_beam_scores: torch.FloatTensor,\n-        final_beam_tokens: torch.LongTensor,\n-        final_beam_indices: torch.LongTensor,\n-        max_length: int,\n-        pad_token_id: Optional[Union[int, torch.Tensor]] = None,\n-        eos_token_id: Optional[Union[int, list[int], torch.Tensor]] = None,\n-        beam_indices: Optional[torch.LongTensor] = None,\n-        decoder_prompt_len: int = 0,\n-    ) -> tuple[torch.LongTensor]:\n-        batch_size = len(self._beam_hyps)\n-\n-        if eos_token_id is not None and not isinstance(eos_token_id, torch.Tensor):\n-            if isinstance(eos_token_id, int):\n-                eos_token_id = [eos_token_id]\n-            eos_token_id = torch.tensor(eos_token_id)\n-\n-        # finalize all open beam hypotheses and add to generated hypotheses\n-        for batch_idx, beam_hyp in enumerate(self._beam_hyps):\n-            if self._done[batch_idx]:\n-                continue\n-\n-            # all open beam hypotheses are added to the beam hypothesis\n-            # beam hypothesis class automatically keeps the best beams\n-\n-            ids_collect = []\n-            for beam_id in range(self.num_beams):\n-                batch_beam_idx = batch_idx * self.num_beams + beam_id\n-                final_score = final_beam_scores[batch_beam_idx].item()\n-                final_tokens = input_ids[batch_beam_idx]\n-\n-                completes_constraint = self.check_completes_constraints(final_tokens.tolist())\n-                if completes_constraint:\n-                    beam_index = beam_indices[batch_beam_idx] if beam_indices is not None else None\n-                    generated_len = final_tokens.shape[-1] - decoder_prompt_len\n-                    beam_hyp.add(final_tokens, final_score, beam_indices=beam_index, generated_len=generated_len)\n-                    ids_collect.append(beam_id)\n-\n-            # due to overly complex constraints or other factors, sometimes we can't guarantee a successful\n-            # generation. In these cases we simply return the highest scoring outputs.\n-            if len(ids_collect) < self.num_beam_hyps_to_keep:\n-                for beam_id in range(self.num_beams):\n-                    if beam_id not in ids_collect:\n-                        batch_beam_idx = batch_idx * self.num_beams + beam_id\n-                        final_score = final_beam_scores[batch_beam_idx].item()\n-                        final_tokens = input_ids[batch_beam_idx]\n-                        generated_len = final_tokens.shape[-1] - decoder_prompt_len\n-                        beam_hyp.add(final_tokens, final_score, generated_len=generated_len)\n-                    if len(ids_collect) >= self.num_beam_hyps_to_keep:\n-                        break\n-\n-        # select the best hypotheses\n-        sent_lengths = input_ids.new(batch_size * self.num_beam_hyps_to_keep)\n-        best = []\n-        best_indices = []\n-        best_scores = torch.zeros(batch_size * self.num_beam_hyps_to_keep, device=self.device, dtype=torch.float32)\n-\n-        # retrieve best hypotheses\n-        for i, beam_hyp in enumerate(self._beam_hyps):\n-            sorted_hyps = sorted(beam_hyp.beams, key=lambda x: x[0])\n-            for j in range(self.num_beam_hyps_to_keep):\n-                best_hyp_tuple = sorted_hyps.pop()\n-                best_score = best_hyp_tuple[0]\n-                best_hyp = best_hyp_tuple[1]\n-                best_index = best_hyp_tuple[2]\n-                sent_lengths[self.num_beam_hyps_to_keep * i + j] = len(best_hyp)\n-\n-                # append to lists\n-                best.append(best_hyp)\n-\n-                # append indices to list\n-                best_indices.append(best_index)\n-\n-                best_scores[i * self.num_beam_hyps_to_keep + j] = best_score\n-\n-        # prepare for adding eos\n-        sent_lengths_max = sent_lengths.max().item() + 1\n-\n-        sent_max_len = min(sent_lengths_max, max_length) if max_length is not None else sent_lengths_max\n-        decoded: torch.LongTensor = input_ids.new(batch_size * self.num_beam_hyps_to_keep, sent_max_len)\n-\n-        if len(best_indices) > 0 and best_indices[0] is not None:\n-            indices: torch.LongTensor = input_ids.new(batch_size * self.num_beam_hyps_to_keep, sent_max_len)\n-        else:\n-            indices = None\n-\n-        # shorter batches are padded if needed\n-        if sent_lengths.min().item() != sent_lengths.max().item():\n-            if pad_token_id is None:\n-                raise ValueError(\"`pad_token_id` has to be defined\")\n-            decoded.fill_(pad_token_id)\n-\n-        if indices is not None:\n-            indices.fill_(-1)\n-\n-        # fill with hypotheses and eos_token_id if the latter fits in\n-        for i, (hypo, best_idx) in enumerate(zip(best, best_indices)):\n-            decoded[i, : sent_lengths[i]] = hypo\n-\n-            if indices is not None:\n-                indices[i, : len(best_idx)] = torch.tensor(best_idx)\n-\n-            if sent_lengths[i] < sent_max_len:\n-                # inserting only the first eos_token_id\n-                decoded[i, sent_lengths[i]] = eos_token_id[0]\n-\n-        return UserDict(\n-            {\n-                \"sequences\": decoded,\n-                \"sequence_scores\": best_scores,\n-                \"beam_indices\": indices,\n-            }\n-        )\n-\n-\n-class BeamHypotheses:\n-    def __init__(\n-        self, num_beams: int, length_penalty: float, early_stopping: Union[bool, str], max_length: Optional[int] = None\n-    ):\n-        \"\"\"\n-        Initialize n-best list of hypotheses.\n-        \"\"\"\n-        logger.warning_once(\n-            \"`BeamHypotheses` is deprecated and will be removed in v4.62.0, as constrained beam search has been moved to the Hub: https://hf.co/transformers-community/constrained-beam-search.\"\n-        )\n-        self.length_penalty = length_penalty\n-        self.early_stopping = early_stopping\n-        self.max_length = max_length\n-        self.num_beams = num_beams\n-        self.beams = []\n-        self.worst_score = 1e9\n-\n-        if not isinstance(self.early_stopping, bool) and self.max_length is None:\n-            raise ValueError(\n-                \"When `do_early_stopping` is set to a string, `max_length` must be defined. Ensure it is passed to the\"\n-                \" BeamScorer class instance at initialization time.\"\n-            )\n-\n-    def __len__(self):\n-        \"\"\"\n-        Number of hypotheses in the list.\n-        \"\"\"\n-        return len(self.beams)\n-\n-    def add(\n-        self,\n-        hyp: torch.LongTensor,\n-        sum_logprobs: float,\n-        beam_indices: Optional[torch.LongTensor] = None,\n-        generated_len: Optional[int] = None,\n-    ):\n-        \"\"\"\n-        Add a new hypothesis to the list.\n-        \"\"\"\n-        if generated_len is not None:\n-            score = sum_logprobs / (generated_len**self.length_penalty)\n-        # This 'else' case exists for retrocompatibility\n-        else:\n-            score = sum_logprobs / (hyp.shape[-1] ** self.length_penalty)\n-\n-        if len(self) < self.num_beams or score > self.worst_score:\n-            self.beams.append((score, hyp, beam_indices))\n-            if len(self) > self.num_beams:\n-                sorted_next_scores = sorted([(s, idx) for idx, (s, _, _) in enumerate(self.beams)])\n-                del self.beams[sorted_next_scores[0][1]]\n-                self.worst_score = sorted_next_scores[1][0]\n-            else:\n-                self.worst_score = min(score, self.worst_score)\n-\n-    def is_done(self, best_sum_logprobs: float, cur_len: int, decoder_prompt_len: int = 0) -> bool:\n-        \"\"\"\n-        If there are enough hypotheses and that none of the hypotheses being generated can become better than the worst\n-        one in the heap, then we are done with this sentence.\n-        \"\"\"\n-\n-        if len(self) < self.num_beams:\n-            return False\n-\n-        # `True`: stop as soon as at least `num_beams` hypotheses are finished\n-        if self.early_stopping is True:\n-            return True\n-        # `False`: heuristic -- compute best possible score from `cur_len`, even though it is not entirely accurate\n-        #  when `length_penalty` is positive. See the discussion below for more details.\n-        # https://github.com/huggingface/transformers/pull/20901#issuecomment-1369845565\n-        elif self.early_stopping is False:\n-            highest_attainable_score = best_sum_logprobs / (cur_len - decoder_prompt_len) ** self.length_penalty\n-            ret = self.worst_score >= highest_attainable_score\n-            return ret\n-        # `\"never\"`: compute the best possible score, depending on the signal of `length_penalty`\n-        else:\n-            # `length_penalty` > 0.0 -> max denominator is obtaned from `max_length`, not from `cur_len` -> min\n-            # abs(`highest_attainable_score`) is obtained -> `highest_attainable_score` is negative, hence we obtain\n-            # its max this way\n-            if self.length_penalty > 0.0:\n-                if self.max_length <= decoder_prompt_len:\n-                    raise ValueError(\"max_length is not larger than decoder prompt length\")\n-                highest_attainable_score = (\n-                    best_sum_logprobs / (self.max_length - decoder_prompt_len) ** self.length_penalty\n-                )\n-            # the opposite logic applies here (max `highest_attainable_score` from `cur_len`)\n-            else:\n-                highest_attainable_score = best_sum_logprobs / (cur_len - decoder_prompt_len) ** self.length_penalty\n-            ret = self.worst_score >= highest_attainable_score\n-            return ret"
        },
        {
            "sha": "7f92e48acafbeee56e9e9a138bfeecf90748557b",
            "filename": "tests/generation/test_beam_constraints.py",
            "status": "removed",
            "additions": 0,
            "deletions": 114,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f143006635effe2b38e95566a2f347b303916bb/tests%2Fgeneration%2Ftest_beam_constraints.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f143006635effe2b38e95566a2f347b303916bb/tests%2Fgeneration%2Ftest_beam_constraints.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_beam_constraints.py?ref=8f143006635effe2b38e95566a2f347b303916bb",
            "patch": "@@ -1,114 +0,0 @@\n-# Copyright 2020 The HuggingFace Team Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a clone of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-import unittest\n-\n-from transformers import is_torch_available\n-from transformers.testing_utils import require_torch\n-\n-\n-if is_torch_available():\n-    import torch\n-\n-    from transformers.generation import DisjunctiveConstraint\n-\n-\n-@require_torch\n-class ConstraintTest(unittest.TestCase):\n-    def test_input_types(self):\n-        # For consistency across different places the DisjunctiveConstraint is called,\n-        # dc.token_ids is a list of integers. It is also initialized only by integers.\n-\n-        cset = [[1, 2, 4], [1, 2, 3, 4]]\n-        dc = DisjunctiveConstraint(cset)\n-        self.assertTrue(isinstance(dc.token_ids, list))\n-\n-        with self.assertRaises(ValueError):\n-            DisjunctiveConstraint(torch.LongTensor([[1, 2, 4], [1, 2, 3]]))\n-\n-        with self.assertRaises(ValueError):\n-            DisjunctiveConstraint([torch.LongTensor([1, 2, 4]), torch.LongTensor([1, 2, 3, 4, 5])])\n-\n-    def test_check_illegal_input(self):\n-        # We can't have constraints that are complete subsets of another. This leads to a perverse\n-        # interpretation of \"constraint fulfillment\": does generating [1,2,3] fulfill the constraint?\n-        # It would mean that it generated [1,2] which fulfills it, but it's in the middle of potentially\n-        # fulfilling [1,2,3,4]. If we believe that [1,2,3] does fulfill the constraint, then the algorithm\n-        # will necessarily never reach [1,2,3,4], giving users a false sense of control (better to just not allow it).\n-        cset = [[1, 2], [1, 2, 3, 4]]\n-\n-        with self.assertRaises(ValueError):\n-            DisjunctiveConstraint(cset)  # fails here\n-\n-    def test_example_progression(self):\n-        cset = [[1, 2, 3], [1, 2, 4]]\n-\n-        dc = DisjunctiveConstraint(cset)\n-\n-        stepped, completed, reset = dc.update(1)\n-        desired = stepped is True and completed is False and reset is False\n-        self.assertTrue(desired)\n-        self.assertTrue(not dc.completed)\n-        self.assertTrue(dc.current_seq == [1])\n-\n-        stepped, completed, reset = dc.update(2)\n-        desired = stepped is True and completed is False and reset is False\n-        self.assertTrue(desired)\n-        self.assertTrue(not dc.completed)\n-        self.assertTrue(dc.current_seq == [1, 2])\n-\n-        stepped, completed, reset = dc.update(3)\n-        desired = stepped is True and completed is True and reset is False\n-        self.assertTrue(desired)\n-        self.assertTrue(dc.completed)  # Completed!\n-        self.assertTrue(dc.current_seq == [1, 2, 3])\n-\n-    def test_example_progression_unequal_three_mid_and_reset(self):\n-        cset = [[1, 2, 3], [1, 2, 4, 5], [1, 2, 5]]\n-\n-        dc = DisjunctiveConstraint(cset)\n-\n-        stepped, completed, reset = dc.update(1)\n-        self.assertTrue(not dc.completed)\n-        self.assertTrue(dc.current_seq == [1])\n-\n-        stepped, completed, reset = dc.update(2)\n-        self.assertTrue(not dc.completed)\n-        self.assertTrue(dc.current_seq == [1, 2])\n-\n-        stepped, completed, reset = dc.update(4)\n-        self.assertTrue(not dc.completed)\n-        self.assertTrue(dc.current_seq == [1, 2, 4])\n-\n-        stepped, completed, reset = dc.update(5)\n-        self.assertTrue(dc.completed)  # Completed!\n-        self.assertTrue(dc.current_seq == [1, 2, 4, 5])\n-\n-        dc.reset()\n-\n-        stepped, completed, reset = dc.update(1)\n-        self.assertTrue(not dc.completed)\n-        self.assertTrue(dc.remaining() == 3)\n-        self.assertTrue(dc.current_seq == [1])\n-\n-        stepped, completed, reset = dc.update(2)\n-        self.assertTrue(not dc.completed)\n-        self.assertTrue(dc.remaining() == 2)\n-        self.assertTrue(dc.current_seq == [1, 2])\n-\n-        stepped, completed, reset = dc.update(5)\n-        self.assertTrue(dc.completed)  # Completed!\n-        self.assertTrue(dc.remaining() == 0)\n-        self.assertTrue(dc.current_seq == [1, 2, 5])"
        }
    ],
    "stats": {
        "total": 1696,
        "additions": 0,
        "deletions": 1696
    }
}