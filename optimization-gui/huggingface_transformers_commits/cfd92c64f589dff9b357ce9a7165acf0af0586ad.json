{
    "author": "aymeric-roucher",
    "message": "Add new documentation page for advanced agent usage (#33265)\n\n* Add new documentation page for advanced agent usage",
    "sha": "cfd92c64f589dff9b357ce9a7165acf0af0586ad",
    "files": [
        {
            "sha": "d4d88ff032e1a7711261ea2c0eedd3ff8ba7bcf2",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/cfd92c64f589dff9b357ce9a7165acf0af0586ad/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/cfd92c64f589dff9b357ce9a7165acf0af0586ad/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=cfd92c64f589dff9b357ce9a7165acf0af0586ad",
            "patch": "@@ -24,7 +24,9 @@\n   - local: model_sharing\n     title: Share your model\n   - local: agents\n-    title: Agents\n+    title: Agents 101\n+  - local: agents_advanced\n+    title: Agents, supercharged - Multi-agents, External tools, and more\n   - local: llm_tutorial\n     title: Generation with LLMs\n   - local: conversations"
        },
        {
            "sha": "b100e39f1c959106b349ab4c66d5912ad00dc382",
            "filename": "docs/source/en/agents.md",
            "status": "modified",
            "additions": 14,
            "deletions": 123,
            "changes": 137,
            "blob_url": "https://github.com/huggingface/transformers/blob/cfd92c64f589dff9b357ce9a7165acf0af0586ad/docs%2Fsource%2Fen%2Fagents.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cfd92c64f589dff9b357ce9a7165acf0af0586ad/docs%2Fsource%2Fen%2Fagents.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fagents.md?ref=cfd92c64f589dff9b357ce9a7165acf0af0586ad",
            "patch": "@@ -28,8 +28,8 @@ An agent is a system that uses an LLM as its engine, and it has access to functi\n These *tools* are functions for performing a task, and they contain all necessary description for the agent to properly use them.\n \n The agent can be programmed to:\n-- devise a series of actions/tools and run them all at once like the [`CodeAgent`] for example\n-- plan and execute actions/tools one by one and wait for the outcome of each action before launching the next one like the [`ReactJsonAgent`] for example\n+- devise a series of actions/tools and run them all at once,  like the [`CodeAgent`]\n+- plan and execute actions/tools one by one and wait for the outcome of each action before launching the next one, like the [`ReactJsonAgent`]\n \n ### Types of agents\n \n@@ -46,7 +46,18 @@ We implement two versions of ReactJsonAgent:\n - [`ReactCodeAgent`] is a new type of ReactJsonAgent that generates its tool calls as blobs of code, which works really well for LLMs that have strong coding performance.\n \n > [!TIP]\n-> Read [Open-source LLMs as LangChain Agents](https://huggingface.co/blog/open-source-llms-as-agents) blog post to learn more the ReAct agent.\n+> Read [Open-source LLMs as LangChain Agents](https://huggingface.co/blog/open-source-llms-as-agents) blog post to learn more about ReAct agents.\n+\n+<div class=\"flex justify-center\">\n+    <img\n+        class=\"block dark:hidden\"\n+        src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Agent_ManimCE.gif\"\n+    />\n+    <img\n+        class=\"hidden dark:block\"\n+        src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Agent_ManimCE.gif\"\n+    />\n+</div>\n \n ![Framework of a React Agent](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/open-source-llms-as-agents/ReAct.png)\n \n@@ -444,123 +455,3 @@ To speed up the start, tools are loaded only if called by the agent.\n This gets you this image:\n \n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rivers_and_lakes.png\">\n-\n-\n-### Use gradio-tools\n-\n-[gradio-tools](https://github.com/freddyaboulton/gradio-tools) is a powerful library that allows using Hugging\n-Face Spaces as tools. It supports many existing Spaces as well as custom Spaces.\n-\n-Transformers supports `gradio_tools` with the [`Tool.from_gradio`] method. For example, let's use the [`StableDiffusionPromptGeneratorTool`](https://github.com/freddyaboulton/gradio-tools/blob/main/gradio_tools/tools/prompt_generator.py) from `gradio-tools` toolkit for improving prompts to generate better images.\n-\n-Import and instantiate the tool, then pass it to the `Tool.from_gradio` method:\n-\n-```python\n-from gradio_tools import StableDiffusionPromptGeneratorTool\n-from transformers import Tool, load_tool, CodeAgent\n-\n-gradio_prompt_generator_tool = StableDiffusionPromptGeneratorTool()\n-prompt_generator_tool = Tool.from_gradio(gradio_prompt_generator_tool)\n-```\n-\n-Now you can use it just like any other tool. For example, let's improve the prompt  `a rabbit wearing a space suit`.\n-\n-```python\n-image_generation_tool = load_tool('huggingface-tools/text-to-image')\n-agent = CodeAgent(tools=[prompt_generator_tool, image_generation_tool], llm_engine=llm_engine)\n-\n-agent.run(\n-    \"Improve this prompt, then generate an image of it.\", prompt='A rabbit wearing a space suit'\n-)\n-```\n-\n-The model adequately leverages the tool:\n-```text\n-======== New task ========\n-Improve this prompt, then generate an image of it.\n-You have been provided with these initial arguments: {'prompt': 'A rabbit wearing a space suit'}.\n-==== Agent is executing the code below:\n-improved_prompt = StableDiffusionPromptGenerator(query=prompt)\n-while improved_prompt == \"QUEUE_FULL\":\n-    improved_prompt = StableDiffusionPromptGenerator(query=prompt)\n-print(f\"The improved prompt is {improved_prompt}.\")\n-image = image_generator(prompt=improved_prompt)\n-====\n-```\n-\n-Before finally generating the image:\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rabbit.png\">\n-\n-\n-> [!WARNING]\n-> gradio-tools require *textual* inputs and outputs even when working with different modalities like image and audio objects. Image and audio inputs and outputs are currently incompatible.\n-\n-### Use LangChain tools\n-\n-We love Langchain and think it has a very compelling suite of tools.\n-To import a tool from LangChain, use the `from_langchain()` method.\n-\n-Here is how you can use it to recreate the intro's search result using a LangChain web search tool.\n-\n-```python\n-from langchain.agents import load_tools\n-from transformers import Tool, ReactCodeAgent\n-\n-search_tool = Tool.from_langchain(load_tools([\"serpapi\"])[0])\n-\n-agent = ReactCodeAgent(tools=[search_tool])\n-\n-agent.run(\"How many more blocks (also denoted as layers) in BERT base encoder than the encoder from the architecture proposed in Attention is All You Need?\")\n-```\n-\n-## Gradio interface\n-\n-You can leverage `gradio.Chatbot`to display your agent's thoughts using `stream_to_gradio`, here is an example:\n-\n-```py\n-import gradio as gr\n-from transformers import (\n-    load_tool,\n-    ReactCodeAgent,\n-    HfApiEngine,\n-    stream_to_gradio,\n-)\n-\n-# Import tool from Hub\n-image_generation_tool = load_tool(\"m-ric/text-to-image\")\n-\n-llm_engine = HfApiEngine(\"meta-llama/Meta-Llama-3-70B-Instruct\")\n-\n-# Initialize the agent with the image generation tool\n-agent = ReactCodeAgent(tools=[image_generation_tool], llm_engine=llm_engine)\n-\n-\n-def interact_with_agent(task):\n-    messages = []\n-    messages.append(gr.ChatMessage(role=\"user\", content=task))\n-    yield messages\n-    for msg in stream_to_gradio(agent, task):\n-        messages.append(msg)\n-        yield messages + [\n-            gr.ChatMessage(role=\"assistant\", content=\"⏳ Task not finished yet!\")\n-        ]\n-    yield messages\n-\n-\n-with gr.Blocks() as demo:\n-    text_input = gr.Textbox(lines=1, label=\"Chat Message\", value=\"Make me a picture of the Statue of Liberty.\")\n-    submit = gr.Button(\"Run illustrator agent!\")\n-    chatbot = gr.Chatbot(\n-        label=\"Agent\",\n-        type=\"messages\",\n-        avatar_images=(\n-            None,\n-            \"https://em-content.zobj.net/source/twitter/53/robot-face_1f916.png\",\n-        ),\n-    )\n-    submit.click(interact_with_agent, [text_input], [chatbot])\n-\n-if __name__ == \"__main__\":\n-    demo.launch()\n-```\n\\ No newline at end of file"
        },
        {
            "sha": "e7469a310c41026a54ee98360e9feb16e8ebe361",
            "filename": "docs/source/en/agents_advanced.md",
            "status": "added",
            "additions": 182,
            "deletions": 0,
            "changes": 182,
            "blob_url": "https://github.com/huggingface/transformers/blob/cfd92c64f589dff9b357ce9a7165acf0af0586ad/docs%2Fsource%2Fen%2Fagents_advanced.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cfd92c64f589dff9b357ce9a7165acf0af0586ad/docs%2Fsource%2Fen%2Fagents_advanced.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fagents_advanced.md?ref=cfd92c64f589dff9b357ce9a7165acf0af0586ad",
            "patch": "@@ -0,0 +1,182 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+# Agents, supercharged - Multi-agents, External tools, and more\n+\n+[[open-in-colab]]\n+\n+### What is an agent?\n+\n+> [!TIP]\n+> If you're new to `transformers.agents`, make sure to first read the main [agents documentation](./agents).\n+\n+In this page we're going to highlight several advanced uses of `transformers.agents`.\n+\n+## Multi-agents\n+\n+Multi-agent has been introduced in Microsoft's framework [Autogen](https://huggingface.co/papers/2308.08155).\n+It simply means having several agents working together to solve your task instead of only one.\n+It empirically yields better performance on most benchmarks. The reason for this better performance is conceptually simple: for many tasks, rather than using a do-it-all system, you would prefer to specialize units on sub-tasks. Here, having agents with separate tool sets and memories allows to achieve efficient specialization.\n+\n+You can easily build hierarchical multi-agent systems with `transformers.agents`.\n+\n+To do so, encapsulate the agent in a [`ManagedAgent`] object. This object needs arguments `agent`, `name`, and a `description`, which will then be embedded in the manager agent's system prompt to let it know how to call this managed agent, as we also do for tools.\n+\n+Here's an example of making an agent that managed a specitif web search agent using our [`DuckDuckGoSearchTool`]:\n+\n+```py\n+from transformers.agents import ReactCodeAgent, HfApiEngine, DuckDuckGoSearchTool, ManagedAgent\n+\n+llm_engine = HfApiEngine()\n+\n+web_agent = ReactCodeAgent(tools=[DuckDuckGoSearchTool()], llm_engine=llm_engine)\n+\n+managed_web_agent = ManagedAgent(\n+    agent=web_agent,\n+    name=\"web_search\",\n+    description=\"Runs web searches for you. Give it your query as an argument.\"\n+)\n+\n+manager_agent = ReactCodeAgent(\n+    tools=[], llm_engine=llm_engine, managed_agents=[managed_web_agent]\n+)\n+\n+manager_agent.run(\"Who is the CEO of Hugging Face?\")\n+```\n+\n+> [!TIP]\n+> For an in-depth example of an efficient multi-agent implementation, see [how we pushed our multi-agent system to the top of the GAIA leaderboard](https://huggingface.co/blog/beating-gaia).\n+\n+\n+## Use tools from gradio or LangChain\n+\n+### Use gradio-tools\n+\n+[gradio-tools](https://github.com/freddyaboulton/gradio-tools) is a powerful library that allows using Hugging\n+Face Spaces as tools. It supports many existing Spaces as well as custom Spaces.\n+\n+Transformers supports `gradio_tools` with the [`Tool.from_gradio`] method. For example, let's use the [`StableDiffusionPromptGeneratorTool`](https://github.com/freddyaboulton/gradio-tools/blob/main/gradio_tools/tools/prompt_generator.py) from `gradio-tools` toolkit for improving prompts to generate better images.\n+\n+Import and instantiate the tool, then pass it to the `Tool.from_gradio` method:\n+\n+```python\n+from gradio_tools import StableDiffusionPromptGeneratorTool\n+from transformers import Tool, load_tool, CodeAgent\n+\n+gradio_prompt_generator_tool = StableDiffusionPromptGeneratorTool()\n+prompt_generator_tool = Tool.from_gradio(gradio_prompt_generator_tool)\n+```\n+\n+Now you can use it just like any other tool. For example, let's improve the prompt  `a rabbit wearing a space suit`.\n+\n+```python\n+image_generation_tool = load_tool('huggingface-tools/text-to-image')\n+agent = CodeAgent(tools=[prompt_generator_tool, image_generation_tool], llm_engine=llm_engine)\n+\n+agent.run(\n+    \"Improve this prompt, then generate an image of it.\", prompt='A rabbit wearing a space suit'\n+)\n+```\n+\n+The model adequately leverages the tool:\n+```text\n+======== New task ========\n+Improve this prompt, then generate an image of it.\n+You have been provided with these initial arguments: {'prompt': 'A rabbit wearing a space suit'}.\n+==== Agent is executing the code below:\n+improved_prompt = StableDiffusionPromptGenerator(query=prompt)\n+while improved_prompt == \"QUEUE_FULL\":\n+    improved_prompt = StableDiffusionPromptGenerator(query=prompt)\n+print(f\"The improved prompt is {improved_prompt}.\")\n+image = image_generator(prompt=improved_prompt)\n+====\n+```\n+\n+Before finally generating the image:\n+\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rabbit.png\">\n+\n+\n+> [!WARNING]\n+> gradio-tools require *textual* inputs and outputs even when working with different modalities like image and audio objects. Image and audio inputs and outputs are currently incompatible.\n+\n+### Use LangChain tools\n+\n+We love Langchain and think it has a very compelling suite of tools.\n+To import a tool from LangChain, use the `from_langchain()` method.\n+\n+Here is how you can use it to recreate the intro's search result using a LangChain web search tool.\n+\n+```python\n+from langchain.agents import load_tools\n+from transformers import Tool, ReactCodeAgent\n+\n+search_tool = Tool.from_langchain(load_tools([\"serpapi\"])[0])\n+\n+agent = ReactCodeAgent(tools=[search_tool])\n+\n+agent.run(\"How many more blocks (also denoted as layers) in BERT base encoder than the encoder from the architecture proposed in Attention is All You Need?\")\n+```\n+\n+## Display your agent run in a cool Gradio interface\n+\n+You can leverage `gradio.Chatbot`to display your agent's thoughts using `stream_to_gradio`, here is an example:\n+\n+```py\n+import gradio as gr\n+from transformers import (\n+    load_tool,\n+    ReactCodeAgent,\n+    HfApiEngine,\n+    stream_to_gradio,\n+)\n+\n+# Import tool from Hub\n+image_generation_tool = load_tool(\"m-ric/text-to-image\")\n+\n+llm_engine = HfApiEngine(\"meta-llama/Meta-Llama-3-70B-Instruct\")\n+\n+# Initialize the agent with the image generation tool\n+agent = ReactCodeAgent(tools=[image_generation_tool], llm_engine=llm_engine)\n+\n+\n+def interact_with_agent(task):\n+    messages = []\n+    messages.append(gr.ChatMessage(role=\"user\", content=task))\n+    yield messages\n+    for msg in stream_to_gradio(agent, task):\n+        messages.append(msg)\n+        yield messages + [\n+            gr.ChatMessage(role=\"assistant\", content=\"⏳ Task not finished yet!\")\n+        ]\n+    yield messages\n+\n+\n+with gr.Blocks() as demo:\n+    text_input = gr.Textbox(lines=1, label=\"Chat Message\", value=\"Make me a picture of the Statue of Liberty.\")\n+    submit = gr.Button(\"Run illustrator agent!\")\n+    chatbot = gr.Chatbot(\n+        label=\"Agent\",\n+        type=\"messages\",\n+        avatar_images=(\n+            None,\n+            \"https://em-content.zobj.net/source/twitter/53/robot-face_1f916.png\",\n+        ),\n+    )\n+    submit.click(interact_with_agent, [text_input], [chatbot])\n+\n+if __name__ == \"__main__\":\n+    demo.launch()\n+```\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 323,
        "additions": 199,
        "deletions": 124
    }
}