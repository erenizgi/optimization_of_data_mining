{
    "author": "yaswanth19",
    "message": "Add Janus model (#36053)\n\n* Iterative generation using input embeds\n\n* Add Janus model\n\n* discard changes\n\n* Janus imports\n\n* Refactor config and processor\n\n* Added Vision tower of Janus\n\n* Import Janus Image processor\n\n* Vision tower fixes\n\n* Refactor code\n\n* Added VQ Model\n\n* Complete model integration\n\n* temp conversion script\n\n* processor refactor\n\n* Adding files to facilitate pulling\n\n* Fixes after debugging\n\n* Skip test for these models\n\n* Add Janus Model\n\n* discard changes\n\n* Janus imports\n\n* Refactor config and processor\n\n* Added Vision tower of Janus\n\n* Import Janus Image processor\n\n* Vision tower fixes\n\n* Refactor code\n\n* Added VQ Model\n\n* Complete model integration\n\n* temp conversion script\n\n* processor refactor\n\n* Adding files to facilitate pulling\n\n* Fixes after debugging\n\n* Refactor to Text config\n\n* ‚ú® Added generate function\n\n* Saving intermediate convert file. Still need to read configs from the hub and convert them to our format.\n\n* Adding version that reads from the JSON files. Still have to tweak some parameters manually.\n\n* relative imports\n\n* Initial tests\n\n* Refactor image processor\n\n* Seemingly working version of the conversion script, will need to test further.\n\n* Adding command message\n\n* Fixing conflicting JanusTextConfig class\n\n* Incorporating some of the discussed changes.\n\n* Small fix to create dir.\n\n* Removing system from JINJA template\n\n* Adding draft processor tests\n\n* style fixes\n\n* Minor fixes and enhancement\n\n* added generation config\n\n* Initial tests\n\n* Small modifications, tests are now passing.\n\n* Small changes I noticed while reading code.\n\n* more fixes\n\n* Added JanusModel class\n\n* Small merge adaptations\n\n* Small merge adaptations\n\n* Image processing tests passing\n\n* More tests and fixes\n\n* Convert script updated and refactored\n\n* Tests and cleanup\n\n* make style\n\n* Postprocessing for image generation\n\n* generate refactor\n\n* fixes\n\n* - Passing tests that write a part of the model to cpu (e.g. test_cpu_offload)\n- Passing tests of dispatching SDPA\n- Only gradient checkpointing tests are left.\n\n* Removing temporary code\n\n* Changes\n\n* Writing change to modular\n\n* Added JanusVisionModel. SDPA dispatch tests pass more robustly. Gradient checkpoint tests are next\n\n* Gradient checkpoint tests passing\n\n* Removing debug code\n\n* Major generate refactor üòÆ‚Äçüí®\n\n* Temp changes for testing\n\n* Green quality CI\n\n* 2 out of 4 integration tests passing\n\n* breadcrumbs\n\n* Usage Examples\n\n* Regenerate modeling after merge\n\n* dirty code\n\n* JanusIntegrationTest are passing\n\n* breadcrumbs\n\n* happy CI\n\n* fixes\n\n* Changing template\n\n* nits\n\n* Text generation logits matching original codebase at 100% precision\n\n* Remove ./tmp from git tracking\n\n* Remove ./tmp from git tracking\n\n* Checkpointing changes after reviewing\n\n* Fixing code in docstrings\n\n* CHanging comments and small bug in convert file\n\n* Fixing bug in image_token_id for 7B version\n\n* Removing line that was added by both of us\n\n* Pushing changes after discussion. Only one left is to change the key mapping for convert file.\n\n* Updating module file\n\n* New convert file using dict. Tested that it is equivalent to the old one by:\n- comparing keys in a script\n- comparing checksums of the output files between version generated with the current convert script and those generated with the old script. This is a more reliable test.\n\n* revert changes\n\n* mistake\n\n* consistency change for CI\n\n* make style\n\n* doc fixes\n\n* more fixes\n\n* experimenting with masking out pad token\n\n* checkpoint\n\n* Batched generation with multi-images working for 1B models. Will test 7B next.\n\n* Device fix.\n\n* Writing changes to modular, previous ones were written to modeling just for quick testing.\n\n* Using passed processor attention mask (only in modeling for now)\n\n* Matching performance done in the non-standard way\n\n* Working version of batched generation. Will change how some args are passed to make it more similar to language case\n\n* More compliant version of the code\n\n* Removed duplicated `_prepare_4d_causal_attention_mask_with_cache_position`\n\n* Updating modular file, making masked filling with paddings more efficient\n\n* Slightly more efficient version\n\n* Modifying JanusVisionModel to be a wrapper\n\n* Fixing test to comply with new names\n\n* Modular overhaul\n\n* More refactoring\n\n* - Changing JanusVisionModel back\n- Changing forward pass\n- Adding boi token to the comparison\n\n* - Removing whole context model_ids\n- Using inherited implementation of prepare_inputs_for_generation\n\n* Moving the way boi token is passed to the model\n\n* Fixing sdpa test\n\n* Minor changes\n\n* testing changes\n\n* Minor fix\n\n* - Adding postprocessing test\n- checking values of generated image on integration test\n\n* changes\n\n* Removing pooled attention vision module, fixing convert script as a consequence\n\n* More changes\n\n* Fixes\n\n* Draft after merge\n\n* Bug fixes\n\n* More bug fix\n\n* Fixing docs\n\n* Nits\n\n* Refactor return dict\n\n* Moving image post processing test to main processor post process\n\n* Passing guidance_scale as kwarg\n\n* make style\n\n* üî• refactor\n\n* make style\n\n* Update and green CI\n\n* Nits and tests update\n\n* up\n\n* Added MID block\n\n* fix\n\n* Dead code\n\n* update testcase\n\n* update\n\n* model_id change\n\n* init_weight changes\n\n---------\n\nCo-authored-by: hsilva664 <metallic-silver@hotmail.com>",
    "sha": "a2ef3cf5379d35da68e58c54e7981f7998ab8f1b",
    "files": [
        {
            "sha": "b3f068ab2bad87fafd974b598b262fa9a8d2d374",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=a2ef3cf5379d35da68e58c54e7981f7998ab8f1b",
            "patch": "@@ -953,6 +953,8 @@\n         title: InstructBLIP\n       - local: model_doc/instructblipvideo\n         title: InstructBlipVideo\n+      - local: model_doc/janus\n+        title: Janus\n       - local: model_doc/kosmos-2\n         title: KOSMOS-2\n       - local: model_doc/layoutlm"
        },
        {
            "sha": "015f2910dfe450014e7a0441e188206024182a7b",
            "filename": "docs/source/en/model_doc/janus.md",
            "status": "added",
            "additions": 230,
            "deletions": 0,
            "changes": 230,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/docs%2Fsource%2Fen%2Fmodel_doc%2Fjanus.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/docs%2Fsource%2Fen%2Fmodel_doc%2Fjanus.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fjanus.md?ref=a2ef3cf5379d35da68e58c54e7981f7998ab8f1b",
            "patch": "@@ -0,0 +1,230 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Janus\n+\n+## Overview\n+\n+The Janus Model was originally proposed in [Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2410.13848) by DeepSeek AI team and later refined in [Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling](https://arxiv.org/abs/2501.17811). Janus is a vision-language model that can generate both image and text output, it can also take both images and text as input.\n+\n+> [!NOTE]\n+> The model doesn't generate both images and text in an interleaved format. The user has to pass a parameter indicating whether to generate text or image.\n+\n+The abstract from the original paper is the following:\n+\n+*In this paper, we introduce Janus, an autoregressive framework that unifies multimodal understanding and generation. Prior research often relies on a single visual encoder for both tasks, such as Chameleon. However, due to the differing levels of information granularity required by multimodal understanding and generation, this approach can lead to suboptimal performance, particularly in multimodal understanding. To address this issue, we decouple visual encoding into separate pathways, while still leveraging a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder's roles in understanding and generation, but also enhances the framework's flexibility. For instance, both the multimodal understanding and generation components can independently select their most suitable encoding methods. Experiments show that Janus surpasses previous unified model and matches or exceeds the performance of task-specific models. The simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models.*\n+\n+The abstract from the aforementioned `Janus-Pro` paper, released afterwards, is the following:\n+\n+*In this work, we introduce Janus-Pro, an advanced version of the previous work Janus. Specifically, Janus-Pro incorporates (1) an optimized training strate (2) expanded training data, and (3) scaling to larger model size. With these improvements, Janus-Pro achieves significant advancements in both multimodal understanding and text-to-image instruction-following capabilities, while also enhancing the stability of text-to-image generation. We hope this work will inspire further exploration in the field. Code and models are publicly available.*\n+\n+This model was contributed by [Yaswanth Gali](https://huggingface.co/yaswanthgali) and [Hugo Silva](https://huggingface.co/hugosilva664).\n+The original code can be found [here](https://github.com/deepseek-ai/Janus).\n+\n+## Usage Example\n+\n+### Single image inference\n+\n+Here is the example of visual understanding with a single image.\n+\n+> [!NOTE]\n+> Note that the model has been trained with a specific prompt format for chatting. Use `processor.apply_chat_template(my_conversation_dict)` to correctly format your prompts.\n+\n+```python\n+import torch  \n+from PIL import Image  \n+import requests  \n+\n+from transformers import JanusForConditionalGeneration, JanusProcessor  \n+\n+model_id = \"deepseek-community/Janus-Pro-1B\"\n+# Prepare Input for generation.\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {'type':'image', 'url': 'http://images.cocodataset.org/val2017/000000039769.jpg'},\n+            {'type':\"text\", \"text\":\"What do you see in this image?.\"}\n+        ]\n+    },\n+]\n+\n+# Set generation mode to `text` to perform text generation.\n+processor = JanusProcessor.from_pretrained(model_id)\n+model = JanusForConditionalGeneration.from_pretrained(model_id,     \n+        torch_dtype=torch.bfloat16,\n+        device_map=\"auto\")\n+\n+inputs = processor.apply_chat_template(\n+    messages,\n+    add_generation_prompt=True,\n+    generation_mode=\"text\",\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\",\n+).to(model.device, dtype=torch.bfloat16)\n+\n+output = model.generate(**inputs, max_new_tokens=40,generation_mode='text',do_sample=True)\n+text = processor.decode(output[0], skip_special_tokens=True)\n+print(text)\n+```\n+\n+### Multi image inference\n+\n+Janus can perform inference with multiple images as input, where images can belong to the same prompt or different prompts in batched inference, where the model processes many conversations in parallel. Here is how you can do it:\n+\n+```python\n+import torch\n+from PIL import Image\n+import requests\n+\n+from transformers import JanusForConditionalGeneration, JanusProcessor\n+\n+model_id = \"deepseek-community/Janus-Pro-1B\"\n+\n+image_urls = [\n+    \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n+    \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n+    \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg\"\n+]\n+\n+messages = [\n+    [\n+        {\n+            \"role\": \"user\",\n+            \"content\": [\n+                {\"type\": \"text\", \"text\": \"What‚Äôs the difference between\"},\n+                {\"type\": \"image\", \"url\": image_urls[0]},\n+                {\"type\": \"text\", \"text\": \" and \"},\n+                {\"type\": \"image\", \"url\": image_urls[1]}\n+            ]\n+        }\n+    ],\n+    [\n+        {\n+            \"role\": \"user\",\n+            \"content\": [\n+                {\"type\": \"image\", \"url\": image_urls[2]},\n+                {\"type\": \"text\", \"text\": \"What do you see in this image?\"}\n+            ]\n+        }\n+    ]\n+]\n+\n+# Load model and processor\n+processor = JanusProcessor.from_pretrained(model_id)\n+model = JanusForConditionalGeneration.from_pretrained(\n+    model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n+)\n+\n+inputs = processor.apply_chat_template(\n+    messages,\n+    add_generation_prompt=True,\n+    generation_mode=\"text\",\n+    tokenize=True,\n+    padding=True,\n+    return_dict=True,\n+    return_tensors=\"pt\"\n+).to(model.device, dtype=torch.bfloat16)\n+\n+# Generate response\n+output = model.generate(**inputs, max_new_tokens=40, generation_mode='text', do_sample=False)\n+text = processor.batch_decode(output, skip_special_tokens=True)\n+print(text)\n+```\n+\n+## Text to Image generation\n+\n+Janus can also generate images given a prompt.\n+\n+```python\n+import torch\n+from transformers import JanusForConditionalGeneration, JanusProcessor\n+\n+# Set generation mode to `image` to prepare inputs for image generation..\n+\n+model_id = \"deepseek-community/Janus-Pro-1B\"\n+processor = JanusProcessor.from_pretrained(model_id)\n+model = JanusForConditionalGeneration.from_pretrained(model_id,\n+        torch_dtype=torch.bfloat16,\n+        device_map=\"auto\")\n+\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"text\", \"text\": \"A dog running under the rain.\"},\n+        ],\n+     }\n+]\n+\n+prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n+inputs = processor(text=prompt,generation_mode=\"image\",return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n+\n+# Set num_return_sequence parameter to generate multiple images per prompt.\n+model.generation_config.num_return_sequences = 2\n+outputs = model.generate(**inputs,\n+                         generation_mode=\"image\",\n+                         do_sample=True,\n+                         use_cache=True,\n+                         )\n+# Perform post-processing on the generated token ids.\n+decoded_image = model.decode_image_tokens(outputs)\n+images = processor.postprocess(list(decoded_image.float()),return_tensors=\"PIL.Image.Image\")\n+# Save the image\n+for i, image in enumerate(images['pixel_values']):\n+    image.save(f\"result{i}.png\")\n+```\n+\n+## JanusConfig\n+\n+[[autodoc]] JanusConfig\n+\n+## JanusVisionConfig\n+\n+[[autodoc]] JanusVisionConfig\n+\n+## JanusVQVAEConfig\n+\n+[[autodoc]] JanusVQVAEConfig\n+\n+## JanusProcessor\n+\n+[[autodoc]] JanusProcessor\n+\n+## JanusImageProcessor\n+\n+[[autodoc]] JanusImageProcessor\n+\n+## JanusVisionModel\n+\n+[[autodoc]] JanusVisionModel\n+    - forward\n+\n+## JanusVQVAE\n+\n+[[autodoc]] JanusVQVAE\n+    - forward\n+\n+## JanusModel\n+\n+[[autodoc]] JanusModel\n+    - forward\n+\n+## JanusForConditionalGeneration\n+\n+[[autodoc]] JanusForConditionalGeneration\n+    - forward"
        },
        {
            "sha": "e0478603b27c3e3be1e4ead946e018ac5508e62a",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=a2ef3cf5379d35da68e58c54e7981f7998ab8f1b",
            "patch": "@@ -144,6 +144,7 @@\n     from .instructblip import *\n     from .instructblipvideo import *\n     from .jamba import *\n+    from .janus import *\n     from .jetmoe import *\n     from .kosmos2 import *\n     from .layoutlm import *"
        },
        {
            "sha": "731929313381959df99dd3fbc1949b19850207a4",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=a2ef3cf5379d35da68e58c54e7981f7998ab8f1b",
            "patch": "@@ -163,6 +163,7 @@\n         (\"instructblip\", \"InstructBlipConfig\"),\n         (\"instructblipvideo\", \"InstructBlipVideoConfig\"),\n         (\"jamba\", \"JambaConfig\"),\n+        (\"janus\", \"JanusConfig\"),\n         (\"jetmoe\", \"JetMoeConfig\"),\n         (\"jukebox\", \"JukeboxConfig\"),\n         (\"kosmos-2\", \"Kosmos2Config\"),\n@@ -517,6 +518,7 @@\n         (\"instructblip\", \"InstructBLIP\"),\n         (\"instructblipvideo\", \"InstructBlipVideo\"),\n         (\"jamba\", \"Jamba\"),\n+        (\"janus\", \"Janus\"),\n         (\"jetmoe\", \"JetMoe\"),\n         (\"jukebox\", \"Jukebox\"),\n         (\"kosmos-2\", \"KOSMOS-2\"),"
        },
        {
            "sha": "10ee95475eda74e3456e99df96f0f0f246d82660",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=a2ef3cf5379d35da68e58c54e7981f7998ab8f1b",
            "patch": "@@ -101,6 +101,7 @@\n             (\"imagegpt\", (\"ImageGPTImageProcessor\",)),\n             (\"instructblip\", (\"BlipImageProcessor\", \"BlipImageProcessorFast\")),\n             (\"instructblipvideo\", (\"InstructBlipVideoImageProcessor\",)),\n+            (\"janus\", (\"JanusImageProcessor\")),\n             (\"kosmos-2\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"layoutlmv2\", (\"LayoutLMv2ImageProcessor\", \"LayoutLMv2ImageProcessorFast\")),\n             (\"layoutlmv3\", (\"LayoutLMv3ImageProcessor\", \"LayoutLMv3ImageProcessorFast\")),"
        },
        {
            "sha": "1746d97fd0fb3cf96eb7eebf317b155aa3a3d0b3",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=a2ef3cf5379d35da68e58c54e7981f7998ab8f1b",
            "patch": "@@ -152,6 +152,7 @@\n         (\"imagegpt\", \"ImageGPTModel\"),\n         (\"informer\", \"InformerModel\"),\n         (\"jamba\", \"JambaModel\"),\n+        (\"janus\", \"JanusModel\"),\n         (\"jetmoe\", \"JetMoeModel\"),\n         (\"jukebox\", \"JukeboxModel\"),\n         (\"kosmos-2\", \"Kosmos2Model\"),\n@@ -359,6 +360,7 @@\n         (\"idefics\", \"IdeficsForVisionText2Text\"),\n         (\"idefics2\", \"Idefics2ForConditionalGeneration\"),\n         (\"idefics3\", \"Idefics3ForConditionalGeneration\"),\n+        (\"janus\", \"JanusForConditionalGeneration\"),\n         (\"layoutlm\", \"LayoutLMForMaskedLM\"),\n         (\"llava\", \"LlavaForConditionalGeneration\"),\n         (\"llava_next\", \"LlavaNextForConditionalGeneration\"),\n@@ -858,6 +860,7 @@\n         (\"idefics2\", \"Idefics2ForConditionalGeneration\"),\n         (\"idefics3\", \"Idefics3ForConditionalGeneration\"),\n         (\"instructblip\", \"InstructBlipForConditionalGeneration\"),\n+        (\"janus\", \"JanusForConditionalGeneration\"),\n         (\"kosmos-2\", \"Kosmos2ForConditionalGeneration\"),\n         (\"llama4\", \"Llama4ForConditionalGeneration\"),\n         (\"llava\", \"LlavaForConditionalGeneration\"),"
        },
        {
            "sha": "c55a4ab2129102080965e16f76e7daff8c2c68e5",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=a2ef3cf5379d35da68e58c54e7981f7998ab8f1b",
            "patch": "@@ -75,6 +75,7 @@\n         (\"idefics3\", \"Idefics3Processor\"),\n         (\"instructblip\", \"InstructBlipProcessor\"),\n         (\"instructblipvideo\", \"InstructBlipVideoProcessor\"),\n+        (\"janus\", \"JanusProcessor\"),\n         (\"kosmos-2\", \"Kosmos2Processor\"),\n         (\"layoutlmv2\", \"LayoutLMv2Processor\"),\n         (\"layoutlmv3\", \"LayoutLMv3Processor\"),"
        },
        {
            "sha": "8496588acb03b7b07a8e38fe56be37ac8e724b53",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=a2ef3cf5379d35da68e58c54e7981f7998ab8f1b",
            "patch": "@@ -265,6 +265,7 @@\n                     \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n                 ),\n             ),\n+            (\"janus\", (None, \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n             (\n                 \"jetmoe\",\n                 ("
        },
        {
            "sha": "1c83ddea5a7e1a746442ff55d47340c0558fc77a",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=a2ef3cf5379d35da68e58c54e7981f7998ab8f1b",
            "patch": "@@ -755,7 +755,6 @@ def __init__(self, config):\n         self.beta = getattr(config, \"beta\", 0.25)\n \n         self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n-        self.re_embed = self.num_embeddings\n \n     def forward(self, hidden_state: torch.Tensor):\n         hidden_state = hidden_state.permute(0, 2, 3, 1).contiguous()"
        },
        {
            "sha": "06bc90cd938a628f75b9f954dee63531b8e06bbe",
            "filename": "src/transformers/models/janus/__init__.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fjanus%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fjanus%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2F__init__.py?ref=a2ef3cf5379d35da68e58c54e7981f7998ab8f1b",
            "patch": "@@ -0,0 +1,29 @@\n+# Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_janus import *\n+    from .image_processing_janus import *\n+    from .modeling_janus import *\n+    from .processing_janus import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "de727ab6d07fbfa89cd72b2e8b2f4673917fe705",
            "filename": "src/transformers/models/janus/configuration_janus.py",
            "status": "added",
            "additions": 314,
            "deletions": 0,
            "changes": 314,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fjanus%2Fconfiguration_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fjanus%2Fconfiguration_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fconfiguration_janus.py?ref=a2ef3cf5379d35da68e58c54e7981f7998ab8f1b",
            "patch": "@@ -0,0 +1,314 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/janus/modular_janus.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_janus.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# coding=utf-8\n+# Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import List\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class JanusVisionConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`JanusVisionModel`]. It is used to instantiate a\n+    `JanusVisionModel` according to the specified arguments, defining the model architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 1024):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        num_hidden_layers (`int`, *optional*, defaults to 24):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            The number of input channels.\n+        patch_size (`int`, *optional*, defaults to 16):\n+            The size (resolution) of each patch.\n+        image_size (`int`, *optional*, defaults to 384):\n+            The size (resolution) of each image.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            Dropout probability for attention weights.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"`, and `\"gelu_new\"` are supported.\n+        mlp_ratio (`float`, *optional*, defaults to 4.0):\n+            Ratio of MLP hidden dimensionality to embedding dimensionality.\n+        attention_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to add a bias to the queries, keys, and values in the attention layers.\n+        hidden_dropout_rate (`float`, *optional*, defaults to 0.0):\n+            The dropout probability for fully connected layers in the encoder.\n+        projection_dim (`int`, *optional*, defaults to 2048):\n+            Dimensionality of the MLP projection head.\n+        projection_dropout (`float`, *optional*, defaults to 0.0):\n+            Dropout probability for the projection layer.\n+        use_qk_norm (`bool`, *optional*, defaults to `False`):\n+            Whether to normalize the query and key matrices.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated normal initializer for initializing all weight matrices.\n+        depth (`int`, *optional*, defaults to 2):\n+            Number of hidden layers in the aligner module.\n+        num_image_tokens (`int`, *optional*, defaults to 576):\n+            Number of image tokens.\n+    \"\"\"\n+\n+    model_type = \"janus_vision_model\"\n+    base_config_key = \"vision_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=1024,\n+        num_hidden_layers=24,\n+        num_attention_heads=16,\n+        num_channels=3,\n+        patch_size=16,\n+        image_size=384,\n+        attention_dropout=0.0,\n+        layer_norm_eps=1e-6,\n+        hidden_act=\"gelu\",\n+        mlp_ratio=4.0,\n+        attention_bias=True,\n+        hidden_dropout_rate=0.0,\n+        projection_dim=2048,\n+        projection_dropout=0.0,\n+        use_qk_norm=False,\n+        initializer_range=0.02,\n+        depth=2,\n+        num_image_tokens=576,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_channels = num_channels\n+        self.patch_size = patch_size\n+        self.image_size = image_size\n+        self.attention_dropout = attention_dropout\n+        self.layer_norm_eps = layer_norm_eps\n+        self.hidden_act = hidden_act\n+\n+        self.mlp_ratio = mlp_ratio\n+        self.attention_bias = attention_bias\n+        self.hidden_dropout_rate = hidden_dropout_rate\n+        self.projection_dim = projection_dim\n+        self.projection_dropout = projection_dropout\n+        self.use_qk_norm = use_qk_norm\n+        self.initializer_range = initializer_range\n+        self.depth = depth\n+        self.num_image_tokens = num_image_tokens\n+\n+\n+class JanusVQVAEConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`JanusVQVAEModel`]. It is used to instantiate a\n+    `JanusVQVAEModel` according to the specified arguments, defining the model architecture.\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information. Instantiating a\n+    configuration with the defaults will yield a similar configuration to the VQModel of the\n+    [deepseek-community/Janus-Pro-1B](https://huggingface.co/deepseek-community/Janus-Pro-1B).\n+\n+    Args:\n+        embed_dim (`int`, *optional*, defaults to 8):\n+            Dimensionality of each embedding vector.\n+        num_embeddings (`int`, *optional*, defaults to 16384):\n+            Number of codebook embeddings.\n+        double_latent (`bool`, *optional*, defaults to `False`):\n+            Whether to use double z channels.\n+        latent_channels (`int`, *optional*, defaults to 256):\n+            Number of channels for the latent space.\n+        num_patches (`int`, *optional*, defaults to 32):\n+            Num of patches the input images can be divided into.\n+        in_channels (`int`, *optional*, defaults to 3):\n+            Number of input channels.\n+        out_channels (`int`, *optional*, defaults to 3):\n+            Number of out channels.\n+        base_channels (`int`, *optional*, defaults to 128):\n+            Base channel count.\n+        channel_multiplier (`List[int]`, *optional*, defaults to `[1, 1, 2, 2, 4]`):\n+            Channel multipliers for each resolution.\n+        num_res_blocks (`int`, *optional*, defaults to 2):\n+            Number of residual blocks.\n+        dropout (`float`, *optional*, defaults to 0.0):\n+            Dropout rate.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        projection_dim (`int`, *optional*, defaults to 2048):\n+            Dimensionality of the MLP projection head.\n+        num_hidden_layers (`int`, *optional*, defaults to 2):\n+            Number of hidden layers in VAVAE MLP Connecter module.\n+        hidden_act (`str` or `Callable`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n+        image_token_embed_dim (`int`, *optional*, defaults to 2048):\n+            Dimension of image embeddings. It should be same as the dimensionality of text embeddings.\n+    \"\"\"\n+\n+    model_type = \"janus_vqgan\"\n+    base_config_key = \"vq_config\"\n+\n+    def __init__(\n+        self,\n+        embed_dim: int = 8,\n+        num_embeddings: int = 16384,\n+        double_latent: bool = False,\n+        latent_channels: int = 256,\n+        num_patches: int = 32,\n+        in_channels: int = 3,\n+        out_channels: int = 3,\n+        base_channels: int = 128,\n+        channel_multiplier: List[int] = [1, 1, 2, 2, 4],\n+        num_res_blocks: int = 2,\n+        dropout: float = 0.0,\n+        initializer_range=0.02,\n+        projection_dim=2048,\n+        num_hidden_layers=2,\n+        hidden_act=\"gelu\",\n+        image_token_embed_dim=2048,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.embed_dim = embed_dim\n+        self.num_embeddings = num_embeddings\n+        self.double_latent = double_latent\n+        self.latent_channels = latent_channels\n+        self.in_channels = in_channels\n+        self.base_channels = base_channels\n+        self.channel_multiplier = channel_multiplier\n+        self.num_res_blocks = num_res_blocks\n+        self.dropout = dropout\n+        self.initializer_range = initializer_range\n+        self.num_patches = num_patches\n+        self.out_channels = out_channels\n+        self.projection_dim = projection_dim\n+        self.num_hidden_layers = num_hidden_layers\n+        self.hidden_act = hidden_act\n+        self.image_token_embed_dim = image_token_embed_dim\n+\n+\n+class JanusConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`JanusModel`]. It is used to instantiate an\n+    Janus model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the Janus-1B or Janus-7B models.\n+\n+    e.g. [deepseek-community/Janus-Pro-1B](https://huggingface.co/deepseek-community/Janus-Pro-1B) or\n+    [deepseek-community/Janus-Pro-7B](https://huggingface.co/deepseek-community/Janus-Pro-7B)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `LlamaConfig`):\n+            The config object or dictionary of the text backbone.\n+        vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `JanusVisionConfig`):\n+            The config object or dictionary of the vision backbone.\n+        vq_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `JanusVQVAEConfig`):\n+            The config object or dictionary of the VQVAE backbone.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import JanusForConditionalGeneration, JanusConfig, JanusVisionConfig, JanusVQVAEConfig, LlamaConfig\n+\n+    >>> # Initializing a Janus vision config\n+    >>> vision_config = JanusVisionConfig()\n+\n+    >>> # Initializing a Llama config\n+    >>> text_config = LlamaConfig()\n+\n+    >>> # Initializing a VQ config\n+    >>> vq_config = JanusVQVAEConfig()\n+\n+    >>> # Initializing a Janus Pro 1B style configuration\n+    >>> configuration = JanusConfig(vision_config=vision_config, text_config=text_config, vq_config=vq_config)\n+\n+    >>> # Initializing a model from the Janus Pro 1B style configuration\n+    >>> model = JanusForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"janus\"\n+    sub_configs = {\n+        \"text_config\": AutoConfig,\n+        \"vision_config\": JanusVisionConfig,\n+        \"vq_config\": JanusVQVAEConfig,\n+    }\n+\n+    def __init__(self, text_config=None, vision_config=None, vq_config=None, **kwargs):\n+        if isinstance(text_config, dict):\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"llama\")\n+            self.text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n+\n+        elif text_config is None:\n+            logger.info(\"`text_config` is None. Initializing with default values\")\n+            self.text_config = CONFIG_MAPPING[\"llama\"]()\n+        elif isinstance(text_config, PretrainedConfig):\n+            self.text_config = text_config\n+        else:\n+            raise ValueError(\n+                f\"Invalid type for `text_config`. Must be either `dict` or `LlamaConfig`.\"\n+                f\" Type found: {type(text_config)}\"\n+            )\n+\n+        if vision_config is None:\n+            logger.info(\"`vision_config` is None. Initializing with default JanusVisionConfig values\")\n+            self.vision_config = JanusVisionConfig()\n+        elif isinstance(vision_config, dict):\n+            self.vision_config = JanusVisionConfig(**vision_config)\n+        elif isinstance(vision_config, JanusVisionConfig):\n+            self.vision_config = vision_config\n+        else:\n+            raise ValueError(\n+                f\"Invalid type for `vision_config`. Must be either `dict` or `JanusVisionConfig`.\"\n+                f\" Type found: {type(vision_config)}\"\n+            )\n+\n+        if vq_config is None:\n+            logger.info(\"`vq_config` is None. Initializing with default JanusVQVAEConfig values\")\n+            self.vq_config = JanusVQVAEConfig()\n+        elif isinstance(vq_config, dict):\n+            self.vq_config = JanusVQVAEConfig(**vq_config)\n+        elif isinstance(vq_config, JanusVQVAEConfig):\n+            self.vq_config = vq_config\n+        else:\n+            raise ValueError(\n+                f\"Invalid type for `vq_config`. Must be either `dict` or `JanusVQVAEConfig`.\"\n+                f\" Type found: {type(vq_config)}\"\n+            )\n+\n+        # This dimension is required when decoding discrete image tokens to continuous input.\n+        self.vq_config.num_patches = self.vision_config.image_size // self.vision_config.patch_size\n+        # The default is only the index for the 1B model, 7B uses a different one\n+        self.image_token_index = kwargs.get(\"image_token_index\", 100581)\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"JanusVQVAEConfig\", \"JanusVisionConfig\", \"JanusConfig\"]"
        },
        {
            "sha": "32e16780bbe7b04d93ad9d4101bf20dbadb11753",
            "filename": "src/transformers/models/janus/convert_janus_weights_to_hf.py",
            "status": "added",
            "additions": 501,
            "deletions": 0,
            "changes": 501,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fjanus%2Fconvert_janus_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fjanus%2Fconvert_janus_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fconvert_janus_weights_to_hf.py?ref=a2ef3cf5379d35da68e58c54e7981f7998ab8f1b",
            "patch": "@@ -0,0 +1,501 @@\n+# coding=utf-8\n+# Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"\n+Example of run command (run from root):\n+\n+python src/transformers/models/janus/convert_janus_weights_to_hf.py --repo_id deepseek-ai/Janus-Pro-1B --local_dir tmp/hub_code_in --output_dir tmp/hub_code_out --safe_serialization\n+Using provided local directory: tmp/hub_code_in\n+\"\"\"\n+\n+import argparse\n+import gc\n+import json\n+import os\n+import re\n+\n+import torch\n+from accelerate import init_empty_weights\n+from huggingface_hub import snapshot_download\n+\n+from transformers import (\n+    AutoTokenizer,\n+    JanusConfig,\n+    JanusForConditionalGeneration,\n+    JanusVisionConfig,\n+    JanusVQVAEConfig,\n+    LlamaConfig,\n+)\n+from transformers.models.janus.image_processing_janus import JanusImageProcessor\n+from transformers.models.janus.processing_janus import JanusProcessor\n+\n+\n+# Mappings\n+MAPPINGS = {\n+    # Vision model\n+    r\"(?<!gen_)vision_model\\.vision_tower\\.blocks\\.(\\d+)\\.attn\": r\"model.vision_model.encoder.layers.\\1.self_attn\",\n+    r\"(?<!gen_)vision_model.vision_tower.blocks\": \"model.vision_model.encoder.layers\",\n+    r\"(?<!gen_)vision_model.vision_tower.pos_embed\": \"model.vision_model.embeddings.position_embedding.weight\",\n+    r\"(?<!gen_)vision_model.vision_tower.patch_embed.proj\": \"model.vision_model.embeddings.patch_embedding\",\n+    r\"(?<!gen_)vision_model.vision_tower.norm\": \"model.vision_model.post_layernorm\",\n+    r\"(?P<pre>\\b(vision_model|model\\.vision_model)\\b.*\\.)proj(?=\\.|\\s|$)\": r\"\\g<pre>projection_layer\",\n+    r\"(?P<pre>\\b(vision_model|model\\.vision_model)\\b.*\\.)norm(?=\\.|\\s|$)\": r\"\\g<pre>layer_norm\",\n+    r\"(?P<pre>\\b(vision_model|model\\.vision_model)\\b.*\\.)norm1(?=\\.|\\s|$)\": r\"\\g<pre>layer_norm1\",\n+    r\"(?P<pre>\\b(vision_model|model\\.vision_model)\\b.*\\.)norm2(?=\\.|\\s|$)\": r\"\\g<pre>layer_norm2\",\n+    r\"\\bvision_model\\.vision_tower\\.attn_pool\\.[^\\s$]*\": None,\n+    # VQ Model\n+    r\"gen_vision_model\": \"model.vqmodel\",\n+    r\"(?P<pre>\\b(gen_vision_model|model\\.vqmodel)\\b.*\\.)decoder\\.conv_blocks(?=\\.|\\s|$)\": r\"\\g<pre>decoder.up\",\n+    r\"(?P<pre>\\b(gen_vision_model|model\\.vqmodel)\\b.*\\.)encoder\\.conv_blocks(?=\\.|\\s|$)\": r\"\\g<pre>encoder.down\",\n+    r\"(?P<pre>\\b(gen_vision_model|model\\.vqmodel)\\b.*\\.)res(?=\\.|\\s|$)\": r\"\\g<pre>block\",\n+    r\"(?P<pre>\\b(gen_vision_model|model\\.vqmodel)\\b.*\\.)mid\\.0(?=\\.|\\s|$)\": r\"\\g<pre>mid.block_1\",\n+    r\"(?P<pre>\\b(gen_vision_model|model\\.vqmodel)\\b.*\\.)mid\\.1(?=\\.|\\s|$)\": r\"\\g<pre>mid.attn_1\",\n+    r\"(?P<pre>\\b(gen_vision_model|model\\.vqmodel)\\b.*\\.)mid\\.2(?=\\.|\\s|$)\": r\"\\g<pre>mid.block_2\",\n+    # Aligner Modules\n+    r\"(gen_aligner)\\.layers\\.0\": r\"model.generation_aligner.fc1\",\n+    r\"(gen_aligner)\\.layers\\.2\": r\"model.generation_aligner.hidden_layers.0\",\n+    r\"(?<!gen_)(aligner)\\.layers\\.0\": r\"model.aligner.fc1\",\n+    r\"(?<!gen_)(aligner)\\.layers\\.2\": r\"model.aligner.hidden_layers.0\",\n+    \"gen_head.output_mlp_projector\": \"model.generation_head.proj_out\",\n+    r\"(\\s|^)gen_embed\": r\"\\1model.generation_embeddings\",\n+    r\"(\\s|^)gen_head\": r\"\\1model.generation_head\",\n+    r\"\\b(gen_vision_model|model\\.vqmodel)\\.quantize\\.codebook_used\": None,\n+    # Language model\n+    r\"(\\s|^)language_model\\.model\": r\"\\1model.language_model\",\n+    r\"\\b(model\\.language_model|(?<!model\\.)language_model)\\.lm_head\\.weight\": \"lm_head.weight\",\n+}\n+\n+CHAT_TEMPLATE = (\n+    \"{%set seps=['\\n\\n','<\\uff5cend\\u2581of\\u2581sentence\\uff5c>']%}\"\n+    \"{%set i=0%}\"\n+    \"{%for message in messages%}\"\n+    \"{%if message['role']|lower=='user'%}\"\n+    \"<|User|>: \"\n+    \"{%elif message['role']|lower=='assistant'%}\"\n+    \"<|Assistant|>:{%if not (loop.last and not add_generation_prompt and message['content'][0]['type']=='text' and message['content'][0]['text']=='')%} {%endif%}\"\n+    \"{%else%}\"\n+    \"{{message['role'].capitalize()}}: \"\n+    \"{%endif%}\"\n+    \"{%for content in message['content']%}\"\n+    \"{%if content['type']=='image'%}\"\n+    \"{%if not loop.first%}{{'\\n'}}{%endif%}\"\n+    \"<image_placeholder>\"\n+    \"{%if not loop.last%}{{'\\n'}}{%endif%}\"\n+    \"{%elif content['type']=='text'%}\"\n+    \"{%set text=content['text']%}\"\n+    \"{%if loop.first%}{%set text=text.lstrip()%}{%endif%}\"\n+    \"{%if loop.last%}{%set text=text.rstrip()%}{%endif%}\"\n+    \"{%if not loop.first and message['content'][loop.index0-1]['type']=='text'%}\"\n+    \"{{' '+text}}\"\n+    \"{%else%}\"\n+    \"{{text}}\"\n+    \"{%endif%}\"\n+    \"{%endif%}\"\n+    \"{%endfor%}\"\n+    \"{%if not loop.last or add_generation_prompt%}\"\n+    \"{%if message['role']|lower=='user'%}\"\n+    \"{{seps[0]}}\"\n+    \"{%else%}\"\n+    \"{{seps[1]}}\"\n+    \"{%endif%}\"\n+    \"{%endif%}\"\n+    \"{%endfor%}\"\n+    \"{%if add_generation_prompt%}<|Assistant|>:{%endif%}\"\n+)\n+\n+\n+def convert_old_keys_to_new_keys(state_dict):\n+    keys_as_text = \"\\n\".join(state_dict.keys())\n+    new_keys_as_text = keys_as_text\n+    for old, repl in MAPPINGS.items():\n+        if repl is None:\n+            new_keys_as_text = re.sub(old, \"\", new_keys_as_text)\n+        else:\n+            new_keys_as_text = re.sub(old, repl, new_keys_as_text)\n+    output_dict = dict(zip(keys_as_text.split(\"\\n\"), new_keys_as_text.split(\"\\n\")))\n+    return output_dict\n+\n+\n+def split_tensor(tensor, key):\n+    \"\"\"Splits a merged tensor (qkv or kv) into separate tensors and creates keys for each part.\"\"\"\n+\n+    if \"qkv\" in key:\n+        prefix_to_replace = \"qkv\"\n+        num_splits = 3\n+        new_keys = [\"q_proj\", \"k_proj\", \"v_proj\"]\n+    elif \"kv\" in key:\n+        prefix_to_replace = \"kv\"\n+        num_splits = 2\n+        new_keys = [\"k_proj\", \"v_proj\"]\n+    else:\n+        raise ValueError(f\"Unrecognized tensor type in key: {key}\")\n+\n+    split_size = tensor.shape[0] // num_splits\n+    tensors = torch.split(tensor, split_size, dim=0)\n+    return {key.replace(prefix_to_replace, new_keys[i]): tensors[i] for i in range(num_splits)}\n+\n+\n+def convert_state_dict_to_hf(state_dict):\n+    \"\"\"Convert state dict keys to HF format.\"\"\"\n+    conversion_dict = convert_old_keys_to_new_keys(state_dict)\n+    converted_state_dict = {}\n+\n+    for old_key, new_key in conversion_dict.items():\n+        if new_key:\n+            if \"qkv\" in new_key or \"kv\" in new_key:  # Detect merged attention keys and split them.\n+                qkv_split_dict = split_tensor(state_dict[old_key], new_key)\n+                converted_state_dict.update(qkv_split_dict)\n+            else:\n+                converted_state_dict[new_key] = state_dict[old_key]\n+\n+    # Embeddings will not have initial dimension\n+    pos_embed_key = \"model.vision_model.embeddings.position_embedding.weight\"\n+    converted_state_dict[pos_embed_key] = converted_state_dict[pos_embed_key].squeeze(0)\n+\n+    return converted_state_dict\n+\n+\n+def ensure_model_downloaded(repo_id: str = None, revision: str = None, local_dir: str = None) -> str:\n+    \"\"\"\n+    Ensures model files are downloaded locally, downloads them if not.\n+    Returns path to local files.\n+\n+    Args:\n+        repo_id: The Hugging Face model repo ID (required if local_dir not provided)\n+        revision: Optional git revision to use\n+        local_dir: Optional local directory path where model files should be stored/found\n+    \"\"\"\n+    if local_dir is not None:\n+        if os.path.exists(local_dir):\n+            print(f\"Using provided local directory: {local_dir}\")\n+        else:\n+            # Create the local directory if it doesn't exist\n+            os.makedirs(local_dir, exist_ok=True)\n+            print(f\"Created local directory: {local_dir}\")\n+\n+    if repo_id is None:\n+        raise ValueError(\"Either repo_id or local_dir must be provided\")\n+\n+    print(f\"Ensuring {repo_id} (revision: {revision or 'latest'}) is downloaded...\")\n+\n+    try:\n+        # First try to find files locally\n+        download_dir = snapshot_download(repo_id, revision=revision, local_files_only=True, local_dir=local_dir)\n+        print(f\"Found model files locally at {download_dir}\")\n+        return download_dir\n+    except Exception:\n+        # If files not found locally, download them\n+        print(f\"Downloading model files for {repo_id}...\")\n+        download_dir = snapshot_download(repo_id, revision=revision, local_files_only=False, local_dir=local_dir)\n+        print(f\"Downloaded model files to {download_dir}\")\n+        return download_dir\n+\n+\n+def load_model_state_dict(input_path: str) -> dict:\n+    \"\"\"\n+    Load model state dict, handling both single and sharded files.\n+    \"\"\"\n+    index_path = os.path.join(input_path, \"pytorch_model.bin.index.json\")\n+    single_file_path = os.path.join(input_path, \"pytorch_model.bin\")\n+\n+    # Check if we have a sharded model\n+    if os.path.exists(index_path):\n+        print(\"Loading sharded model...\")\n+        state_dict = {}\n+        with open(index_path, \"r\") as f:\n+            index = json.load(f)\n+\n+        # Get unique shard files and load each one only once\n+        unique_shard_files = sorted(set(index[\"weight_map\"].values()))\n+        for shard_file in unique_shard_files:\n+            print(f\"Loading shard {shard_file}...\")\n+            shard_path = os.path.join(input_path, shard_file)\n+            shard_dict = torch.load(shard_path, map_location=\"cpu\")\n+            state_dict.update(shard_dict)\n+\n+        return state_dict\n+\n+    # Single file model\n+    elif os.path.exists(single_file_path):\n+        print(\"Loading single file model...\")\n+        return torch.load(single_file_path, map_location=\"cpu\")\n+\n+    else:\n+        raise ValueError(f\"No model files found in {input_path}\")\n+\n+\n+def convert_model(\n+    repo_id=None,\n+    local_dir=None,\n+    text_model_id=None,\n+    output_dir=None,\n+    output_hub_path=None,\n+    safe_serialization=True,\n+    revision=None,\n+):\n+    \"\"\"Convert and save the model weights, processor, and configuration.\"\"\"\n+    if output_dir is None and output_hub_path is None:\n+        raise ValueError(\"At least one of output_dir or output_hub_path must be specified\")\n+\n+    if repo_id is None and local_dir is None:\n+        raise ValueError(\"Either repo_id or local_dir must be specified\")\n+\n+    # Create output directory if specified\n+    if output_dir:\n+        os.makedirs(output_dir, exist_ok=True)\n+        print(f\"Created/verified output directory: {output_dir}\")\n+\n+    torch.set_default_dtype(torch.float16)\n+\n+    # Download or locate model files\n+    input_path = ensure_model_downloaded(repo_id=repo_id, revision=revision, local_dir=local_dir)\n+\n+    # Load configuration files\n+    required_files = [\"config.json\", \"preprocessor_config.json\", \"special_tokens_map.json\", \"tokenizer_config.json\"]\n+\n+    missing_files = [f for f in required_files if not os.path.exists(os.path.join(input_path, f))]\n+    if missing_files:\n+        raise ValueError(\n+            f\"The following required configuration files are missing from {input_path}: {', '.join(missing_files)}. \"\n+            \"Please ensure you have downloaded all necessary model files.\"\n+        )\n+\n+    with open(os.path.join(input_path, \"config.json\"), \"r\") as f:\n+        config_data = json.load(f)\n+    with open(os.path.join(input_path, \"preprocessor_config.json\"), \"r\") as f:\n+        preprocessor_config = json.load(f)\n+    with open(os.path.join(input_path, \"special_tokens_map.json\"), \"r\") as f:\n+        special_tokens_map = json.load(f)\n+    with open(os.path.join(input_path, \"tokenizer_config.json\"), \"r\") as f:\n+        tokenizer_config = json.load(f)\n+\n+    # Create tokenizer directly from tokenizer.json if it exists\n+    tokenizer_json_path = os.path.join(input_path, \"tokenizer.json\")\n+    special_image_tokens = {\n+        \"image_token\": \"<image_placeholder>\",\n+        \"boi_token\": \"<begin_of_image>\",\n+        \"eoi_token\": \"<end_of_image>\",\n+    }\n+\n+    if os.path.exists(tokenizer_json_path) and not text_model_id:\n+        tokenizer = AutoTokenizer.from_pretrained(\n+            input_path,  # This will load tokenizer.json directly\n+            model_max_length=tokenizer_config[\"model_max_length\"],\n+            extra_special_tokens=special_image_tokens,\n+        )\n+    else:\n+        # Fallback to creating from text_model_id with special tokens\n+        tokenizer = AutoTokenizer.from_pretrained(\n+            text_model_id,\n+            bos_token=special_tokens_map[\"bos_token\"],\n+            eos_token=special_tokens_map[\"eos_token\"],\n+            pad_token=special_tokens_map[\"pad_token\"],\n+            additional_special_tokens=special_tokens_map[\"additional_special_tokens\"],\n+            model_max_length=tokenizer_config[\"model_max_length\"],\n+            extra_special_tokens=special_image_tokens,\n+        )\n+\n+    # Create image processor from config\n+    image_processor_kwargs = {}\n+    for key in [\"do_normalize\", \"image_mean\", \"image_std\", \"min_size\", \"rescale_factor\"]:\n+        if key in preprocessor_config:\n+            image_processor_kwargs[key] = preprocessor_config[key]\n+\n+    if \"image_size\" in preprocessor_config:\n+        image_processor_kwargs[\"size\"] = {\n+            \"height\": preprocessor_config[\"image_size\"],\n+            \"width\": preprocessor_config[\"image_size\"],\n+        }\n+\n+    image_processor = JanusImageProcessor(**image_processor_kwargs)\n+\n+    # Create processor with chat template\n+    processor = JanusProcessor(\n+        image_processor=image_processor,\n+        tokenizer=tokenizer,\n+        chat_template=CHAT_TEMPLATE,\n+        use_default_system_prompt=True,\n+    )\n+\n+    if output_dir:\n+        print(f\"Saving processor to {output_dir}...\")\n+        processor.save_pretrained(output_dir)\n+    if output_hub_path:\n+        print(f\"Pushing processor to hub at {output_hub_path}...\")\n+        processor.push_to_hub(output_hub_path)\n+\n+    # Create model configurations\n+    text_config_kwargs = {}\n+    for key in [\n+        \"vocab_size\",\n+        \"hidden_size\",\n+        \"intermediate_size\",\n+        \"num_hidden_layers\",\n+        \"num_attention_heads\",\n+        \"num_key_value_heads\",\n+        \"hidden_act\",\n+        \"max_position_embeddings\",\n+        \"torch_dtype\",\n+    ]:\n+        if key in config_data[\"language_config\"]:\n+            text_config_kwargs[key] = config_data[\"language_config\"][key]\n+\n+    # Add token IDs from tokenizer\n+    text_config_kwargs.update(\n+        {\n+            \"pad_token_id\": tokenizer.pad_token_id,\n+            \"bos_token_id\": tokenizer.bos_token_id,\n+            \"eos_token_id\": tokenizer.eos_token_id,\n+        }\n+    )\n+\n+    text_config = LlamaConfig(**text_config_kwargs)\n+\n+    # Create vision config\n+    vision_config_kwargs = {}\n+    if \"image_size\" in config_data[\"vision_config\"][\"params\"]:\n+        vision_config_kwargs[\"image_size\"] = config_data[\"vision_config\"][\"params\"][\"image_size\"]\n+\n+    # Add aligner params if present\n+    if \"aligner_config\" in config_data and \"params\" in config_data[\"aligner_config\"]:\n+        if \"n_embed\" in config_data[\"aligner_config\"][\"params\"]:\n+            vision_config_kwargs[\"projection_dim\"] = config_data[\"aligner_config\"][\"params\"][\"n_embed\"]\n+        if \"depth\" in config_data[\"aligner_config\"][\"params\"]:\n+            vision_config_kwargs[\"depth\"] = config_data[\"aligner_config\"][\"params\"][\"depth\"]\n+\n+    vision_config = JanusVisionConfig(**vision_config_kwargs)\n+\n+    vq_config = JanusVQVAEConfig(\n+        embed_dim=config_data[\"gen_vision_config\"][\"params\"][\"n_embed\"],\n+        num_embeddings=config_data[\"gen_vision_config\"][\"params\"][\"image_token_size\"],\n+        projection_dim=config_data[\"gen_aligner_config\"][\"params\"][\"n_embed\"],\n+        depth=config_data[\"gen_aligner_config\"][\"params\"][\"depth\"],\n+        image_token_embed_dim=config_data[\"gen_head_config\"][\"params\"][\"image_token_embed\"],\n+    )\n+\n+    # Create the main config\n+    config = JanusConfig(\n+        text_config=text_config,\n+        vision_config=vision_config,\n+        vq_config=vq_config,\n+        image_token_index=tokenizer.vocab.get(\"<image_placeholder>\"),\n+    )\n+\n+    # Save the config\n+    if output_dir:\n+        config.save_pretrained(output_dir)\n+    if output_hub_path:\n+        config.push_to_hub(output_hub_path)\n+\n+    # Initialize model with empty weights\n+    print(\"Creating empty model...\")\n+    with init_empty_weights():\n+        model = JanusForConditionalGeneration(config)\n+\n+    model.generation_config.temperature = 1\n+    model.generation_config.guidance_scale = 5\n+    model.generation_config.pad_token_id = tokenizer.vocab.get(\"<\\uff5c\\u2581pad\\u2581\\uff5c>\")\n+    model.generation_config.generation_kwargs[\"boi_token_id\"] = tokenizer.vocab.get(\"<begin_of_image>\")\n+\n+    # Load and convert state dict\n+    print(\"Loading state dict...\")\n+    state_dict = load_model_state_dict(input_path)\n+    state_dict = convert_state_dict_to_hf(state_dict)\n+\n+    # Load converted state dict\n+    print(\"Loading converted weights into model...\")\n+    model.load_state_dict(state_dict, strict=True, assign=True)\n+\n+    # Tie weights before any device mapping\n+    print(\"Tying weights...\")\n+    model.tie_weights()\n+\n+    # Save the model\n+    if output_dir:\n+        print(f\"Saving model to {output_dir}...\")\n+        model.save_pretrained(output_dir, safe_serialization=safe_serialization)\n+    if output_hub_path:\n+        print(f\"Pushing model to hub at {output_hub_path}...\")\n+        model.push_to_hub(output_hub_path, safe_serialization=safe_serialization)\n+\n+    del state_dict, model\n+    gc.collect()\n+\n+    # Validate the saved model if saved locally\n+    if output_dir:\n+        print(\"Reloading the local model to check if it's saved correctly...\")\n+        # TODO: warning about weights not being tied is raised here regardless of model.tie_weights() above\n+        JanusForConditionalGeneration.from_pretrained(output_dir, device_map=\"auto\")\n+        print(\"Local model reloaded successfully.\")\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--repo_id\",\n+        help=\"HuggingFace Hub repo ID for the model\",\n+        default=None,\n+    )\n+    parser.add_argument(\n+        \"--local_dir\",\n+        help=\"Local directory containing the model files\",\n+        default=None,\n+    )\n+    parser.add_argument(\n+        \"--revision\",\n+        help=\"Specific revision to download from the Hub\",\n+        default=None,\n+    )\n+    parser.add_argument(\n+        \"--output_dir\",\n+        help=\"Location to write HF model locally\",\n+        default=None,\n+    )\n+    parser.add_argument(\n+        \"--output_hub_path\",\n+        help=\"Repository ID to push model to hub (e.g. 'username/model-name')\",\n+        default=None,\n+    )\n+    parser.add_argument(\n+        \"--text_model_id\",\n+        help=\"Hub ID of the text model to get tokenizer from. Optional if tokenizer.json exists in the model directory.\",\n+        required=False,\n+    )\n+    parser.add_argument(\n+        \"--safe_serialization\",\n+        action=\"store_true\",\n+        help=\"Whether to save using safetensors\",\n+    )\n+    args = parser.parse_args()\n+\n+    if args.output_dir is None and args.output_hub_path is None:\n+        raise ValueError(\"At least one of --output_dir or --output_hub_path must be specified\")\n+\n+    if args.repo_id is None and args.local_dir is None:\n+        raise ValueError(\"Either --repo_id or --local_dir must be specified\")\n+\n+    convert_model(\n+        repo_id=args.repo_id,\n+        local_dir=args.local_dir,\n+        text_model_id=args.text_model_id,\n+        output_dir=args.output_dir,\n+        output_hub_path=args.output_hub_path,\n+        safe_serialization=args.safe_serialization,\n+        revision=args.revision,\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "1dac2fec481b91f0cec6080fd59620e6a4299a85",
            "filename": "src/transformers/models/janus/image_processing_janus.py",
            "status": "added",
            "additions": 508,
            "deletions": 0,
            "changes": 508,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py?ref=a2ef3cf5379d35da68e58c54e7981f7998ab8f1b",
            "patch": "@@ -0,0 +1,508 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/janus/modular_janus.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_janus.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# coding=utf-8\n+# Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Dict, Iterable, List, Optional, Tuple, Union\n+\n+import numpy as np\n+\n+from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n+from ...image_transforms import convert_to_rgb, resize, to_channel_dimension_format\n+from ...image_utils import (\n+    OPENAI_CLIP_MEAN,\n+    OPENAI_CLIP_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    get_image_size,\n+    infer_channel_dimension_format,\n+    is_scaled_image,\n+    make_flat_list_of_images,\n+    make_list_of_images,\n+    to_numpy_array,\n+    valid_images,\n+    validate_preprocess_arguments,\n+)\n+from ...utils import (\n+    TensorType,\n+    filter_out_non_signature_kwargs,\n+    is_vision_available,\n+    logging,\n+)\n+\n+\n+if is_vision_available():\n+    import PIL\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class JanusImageProcessor(BaseImageProcessor):\n+    r\"\"\"\n+    Constructs a JANUS image processor.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by the\n+            `do_resize` parameter in the `preprocess` method.\n+        size (`dict`, *optional*, defaults to `{\"height\": 384, \"width\": 384}`):\n+            Size of the output image after resizing. Can be overridden by the `size` parameter in the `preprocess`\n+            method.\n+        min_size (`int`, *optional*, defaults to 14):\n+            The minimum allowed size for the resized image. Ensures that neither the height nor width\n+            falls below this value after resizing.\n+        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n+            Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`. Can be\n+            overridden by the `resample` parameter in the `preprocess` method.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by the\n+            `do_rescale` parameter in the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Only has an effect if `do_rescale` is set to `True`. Can be\n+            overridden by the `rescale_factor` parameter in the `preprocess` method.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`\n+            method. Can be overridden by the `do_normalize` parameter in the `preprocess` method.\n+        image_mean (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n+            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n+            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method. Can be\n+            overridden by the `image_mean` parameter in the `preprocess` method.\n+        image_std (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n+            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n+            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n+            Can be overridden by the `image_std` parameter in the `preprocess` method.\n+        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n+            Whether to convert the image to RGB.\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values\"]\n+\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        size: Dict[str, int] = None,\n+        min_size: int = 14,\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n+        do_normalize: bool = True,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_convert_rgb: bool = None,\n+        **kwargs,\n+    ) -> None:\n+        super().__init__(**kwargs)\n+        size = size if size is not None else {\"height\": 384, \"width\": 384}\n+        size = get_size_dict(size, default_to_square=True)\n+\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.resample = resample\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean if image_mean is not None else OPENAI_CLIP_MEAN\n+        self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n+        self.do_convert_rgb = do_convert_rgb\n+\n+        self.min_size = min_size\n+        if image_mean is None:\n+            self.background_color = (127, 127, 127)\n+        else:\n+            self.background_color = tuple([int(x * 255) for x in image_mean])\n+\n+    def resize(\n+        self,\n+        image: np.ndarray,\n+        size: Union[Dict[str, int], int],\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        **kwargs,\n+    ) -> np.ndarray:\n+        \"\"\"\n+        Resize an image to dynamically calculated size.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                Image to resize.\n+            resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n+                `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BICUBIC`.\n+            data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the output image. If unset, the channel dimension format of the input\n+                image is used. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `None`: will be inferred from input\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+\n+        Returns:\n+            `np.ndarray`: The resized image.\n+        \"\"\"\n+        if input_data_format is None:\n+            input_data_format = infer_channel_dimension_format(image)\n+\n+        height, width = get_image_size(image, input_data_format)\n+        max_size = max(height, width)\n+\n+        size = get_size_dict(size, default_to_square=True)\n+        if size[\"height\"] != size[\"width\"]:\n+            raise ValueError(\n+                f\"Output height and width must be the same. Got height={size['height']} and width={size['width']}\"\n+            )\n+        size = size[\"height\"]\n+\n+        delta = size / max_size\n+        # Largest side becomes `size` and the other side is scaled according to the aspect ratio.\n+        output_size_nonpadded = [\n+            max(int(height * delta), self.min_size),\n+            max(int(width * delta), self.min_size),\n+        ]\n+\n+        image = resize(\n+            image,\n+            size=output_size_nonpadded,\n+            resample=resample,\n+            data_format=data_format,\n+            input_data_format=input_data_format,\n+            **kwargs,\n+        )\n+        # Expand and pad the images to obtain a square image of dimensions `size x size`\n+        image = self.pad_to_square(\n+            image=image,\n+            background_color=self.background_color,\n+            input_data_format=input_data_format,\n+        )\n+        return image\n+\n+    @filter_out_non_signature_kwargs()\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        do_resize: Optional[bool] = None,\n+        size: Optional[Dict[str, int]] = None,\n+        resample: PILImageResampling = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_convert_rgb: Optional[bool] = None,\n+        data_format: ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> PIL.Image.Image:\n+        \"\"\"\n+        Preprocess an image or batch of images.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n+                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+                Controls the size of the image after `resize`. The shortest edge of the image is resized to\n+                `size[\"shortest_edge\"]` whilst preserving the aspect ratio. If the longest edge of this resized image\n+                is > `int(size[\"shortest_edge\"] * (1333 / 800))`, then the image is resized again to make the longest\n+                edge equal to `int(size[\"shortest_edge\"] * (1333 / 800))`.\n+            resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image values between [0 - 1].\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+                Image mean to normalize the image by if `do_normalize` is set to `True`.\n+            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+                Image standard deviation to normalize the image by if `do_normalize` is set to `True`.\n+            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n+                Whether to convert the image to RGB.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                    - Unset: Return a list of `np.ndarray`.\n+                    - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n+                    - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                    - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+                    - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        \"\"\"\n+        do_resize = do_resize if do_resize is not None else self.do_resize\n+        resample = resample if resample is not None else self.resample\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+\n+        size = size if size is not None else self.size\n+        size = get_size_dict(size, default_to_square=False)\n+        images = make_flat_list_of_images(images)\n+\n+        if not valid_images(images):\n+            raise ValueError(\n+                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+\n+        validate_preprocess_arguments(\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+        )\n+        # PIL RGBA images are converted to RGB\n+        if do_convert_rgb:\n+            images = [convert_to_rgb(image) for image in images]\n+\n+        # All transformations expect numpy arrays.\n+        images = [to_numpy_array(image) for image in images]\n+\n+        if do_rescale and is_scaled_image(images[0]):\n+            logger.warning_once(\n+                \"It looks like you are trying to rescale already rescaled images. If the input\"\n+                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+            )\n+\n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(images[0])\n+\n+        if do_resize:\n+            images = [\n+                self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n+                for image in images\n+            ]\n+\n+        if do_rescale:\n+            images = [\n+                self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+                for image in images\n+            ]\n+\n+        if do_normalize:\n+            images = [\n+                self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n+                for image in images\n+            ]\n+\n+        images = [\n+            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n+        ]\n+\n+        encoded_outputs = BatchFeature(data={\"pixel_values\": images}, tensor_type=return_tensors)\n+\n+        return encoded_outputs\n+\n+    def pad_to_square(\n+        self,\n+        image: np.ndarray,\n+        background_color: Union[int, Tuple[int, int, int]] = 0,\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> np.array:\n+        \"\"\"\n+        Pads an image to a square based on the longest edge.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                The image to pad.\n+            background_color (`int` or `Tuple[int, int, int]`, *optional*, defaults to 0):\n+                The color to use for the padding. Can be an integer for single channel or a\n+                tuple of integers representing for multi-channel images. If passed as integer\n+                in mutli-channel mode, it will default to `0` in subsequent channels.\n+            data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the output image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                If unset, will use same as the input image.\n+            input_data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the input image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+\n+        Returns:\n+            `np.ndarray`: The padded image.\n+        \"\"\"\n+        height, width = get_image_size(image, input_data_format)\n+        num_channels = image.shape[0] if input_data_format == ChannelDimension.FIRST else image.shape[-1]\n+\n+        if height == width:\n+            image = (\n+                to_channel_dimension_format(image, data_format, input_data_format)\n+                if data_format is not None\n+                else image\n+            )\n+            return image\n+\n+        max_dim = max(height, width)\n+\n+        # Ensure background_color is the correct shape\n+        if isinstance(background_color, int):\n+            background_color = [background_color]\n+        elif len(background_color) != num_channels:\n+            raise ValueError(\n+                f\"background_color must have no more than {num_channels} elements to match the number of channels\"\n+            )\n+\n+        if input_data_format == ChannelDimension.FIRST:\n+            result = np.zeros((num_channels, max_dim, max_dim), dtype=image.dtype)\n+            for i, color in enumerate(background_color):\n+                result[i, :, :] = color\n+            if width > height:\n+                start = (max_dim - height) // 2\n+                result[:, start : start + height, :] = image\n+            else:\n+                start = (max_dim - width) // 2\n+                result[:, :, start : start + width] = image\n+        else:\n+            result = np.zeros((max_dim, max_dim, num_channels), dtype=image.dtype)\n+            for i, color in enumerate(background_color):\n+                result[:, :, i] = color\n+            if width > height:\n+                start = (max_dim - height) // 2\n+                result[start : start + height, :, :] = image\n+            else:\n+                start = (max_dim - width) // 2\n+                result[:, start : start + width, :] = image\n+\n+        return result\n+\n+    def postprocess(\n+        self,\n+        images: ImageInput,\n+        do_rescale: bool = None,\n+        rescale_factor: float = None,\n+        do_normalize: bool = None,\n+        image_mean: List[float] = None,\n+        image_std: List[float] = None,\n+        input_data_format: str = None,\n+        return_tensors: str = None,\n+    ):\n+        \"\"\"Applies post-processing to the decoded image tokens by reversing transformations applied during preprocessing.\"\"\"\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = 1.0 / self.rescale_factor if rescale_factor is None else rescale_factor\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+\n+        images = make_list_of_images(images)  # Ensures input is a list\n+\n+        if isinstance(images[0], PIL.Image.Image):\n+            return images if len(images) > 1 else images[0]\n+\n+        if input_data_format is None:\n+            input_data_format = infer_channel_dimension_format(images[0])  # Determine format dynamically\n+\n+        pixel_values = []\n+\n+        for image in images:\n+            image = to_numpy_array(image)  # Ensure NumPy format\n+\n+            if do_normalize:\n+                image = self.unnormalize(\n+                    image=image, image_mean=image_mean, image_std=image_std, input_data_format=input_data_format\n+                )\n+\n+            if do_rescale:\n+                image = self.rescale(image, scale=rescale_factor, input_data_format=input_data_format)\n+                image = image.clip(0, 255).astype(np.uint8)\n+\n+            if do_normalize and do_rescale and return_tensors == \"PIL.Image.Image\":\n+                image = to_channel_dimension_format(image, ChannelDimension.LAST, input_channel_dim=input_data_format)\n+                image = PIL.Image.fromarray(image)\n+\n+            pixel_values.append(image)\n+\n+        data = {\"pixel_values\": pixel_values}\n+        return_tensors = return_tensors if return_tensors != \"PIL.Image.Image\" else None\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+    def unnormalize(\n+        self,\n+        image: np.array,\n+        image_mean: Union[float, Iterable[float]],\n+        image_std: Union[float, Iterable[float]],\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> np.array:\n+        \"\"\"\n+        Unnormalizes `image` using the mean and standard deviation specified by `mean` and `std`.\n+        image = (image * image_std) + image_mean\n+        Args:\n+            image (`torch.Tensor` of shape `(batch_size, num_channels, image_size, image_size)` or `(num_channels, image_size, image_size)`):\n+                Batch of pixel values to postprocess.\n+            image_mean (`float` or `Iterable[float]`):\n+                The mean to use for unnormalization.\n+            image_std (`float` or `Iterable[float]`):\n+                The standard deviation to use for unnormalization.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        \"\"\"\n+        num_channels = 3\n+\n+        if isinstance(image_mean, Iterable):\n+            if len(image_mean) != num_channels:\n+                raise ValueError(f\"mean must have {num_channels} elements if it is an iterable, got {len(image_mean)}\")\n+        else:\n+            image_mean = [image_mean] * num_channels\n+\n+        if isinstance(image_std, Iterable):\n+            if len(image_std) != num_channels:\n+                raise ValueError(f\"std must have {num_channels} elements if it is an iterable, got {len(image_std)}\")\n+        else:\n+            image_std = [image_std] * num_channels\n+\n+        rev_image_mean = tuple(-mean / std for mean, std in zip(image_mean, image_std))\n+        rev_image_std = tuple(1 / std for std in image_std)\n+        image = self.normalize(\n+            image=image, mean=rev_image_mean, std=rev_image_std, input_data_format=input_data_format\n+        )\n+        return image\n+\n+\n+__all__ = [\"JanusImageProcessor\"]"
        },
        {
            "sha": "a4c7937bc459941ae45b8df1af9fd5864cec4e5d",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "added",
            "additions": 1660,
            "deletions": 0,
            "changes": 1660,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=a2ef3cf5379d35da68e58c54e7981f7998ab8f1b",
            "patch": "@@ -0,0 +1,1660 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/janus/modular_janus.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_janus.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# coding=utf-8\n+# Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import copy\n+from dataclasses import dataclass\n+from typing import Callable, List, Optional, Tuple, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache\n+from ...generation import ClassifierFreeGuidanceLogitsProcessor, GenerationMixin, GenerationMode, LogitsProcessorList\n+from ...generation.utils import GenerateDecoderOnlyOutput\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ModelOutput\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n+    is_torch_available,\n+    logging,\n+    replace_return_docstrings,\n+    torch_int,\n+)\n+from ..auto import AutoModel\n+from .configuration_janus import JanusConfig, JanusVisionConfig, JanusVQVAEConfig\n+\n+\n+if is_torch_available():\n+    import torch.nn.functional as F\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+# General docstring\n+_CONFIG_FOR_DOC = \"JanusConfig\"\n+\n+\n+JANUS_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`JanusConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare Janus Model outputting raw hidden-states without any specific head on top.\",\n+    JANUS_START_DOCSTRING,\n+)\n+class JanusPreTrainedModel(PreTrainedModel):\n+    config_class = JanusConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"LlamaDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_quantized_cache = True\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n+    _supports_param_buffer_assignment = False\n+\n+    def _init_weights(self, module):\n+        std = (\n+            self.config.vision_config.initializer_range\n+            if hasattr(self.config, \"vision_config\")\n+            else self.config.initializer_range\n+        )\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, (nn.GroupNorm, nn.LayerNorm)):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+\n+@dataclass\n+class JanusVQVAEOutput(ModelOutput):\n+    \"\"\"\n+    Base class for Janus VQ-VAE mode model outputs.\n+    Args:\n+        decoded_pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+            Reconstructed pixel values after encoding and decoding the input.\n+        embedding_loss (`torch.FloatTensor`):\n+            Embedding loss.\n+    \"\"\"\n+\n+    decoded_pixel_values: Optional[torch.FloatTensor] = None\n+    embedding_loss: torch.FloatTensor = None\n+\n+\n+@dataclass\n+class JanusBaseModelOutputWithPast(ModelOutput):\n+    \"\"\"\n+    Base class for Janus model's outputs that may also contain a past key/values (to speed up sequential decoding).\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+\n+            If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n+            hidden_size)` is output.\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n+            `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n+            encoder_sequence_length, embed_size_per_head)`.\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n+            `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n+            input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+            Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n+            sequence_length, hidden_size)`.\n+\n+            image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n+    \"\"\"\n+\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+\n+\n+@dataclass\n+class JanusCausalLMOutputWithPast(ModelOutput):\n+    \"\"\"\n+    Base class for Janus causal language model (or autoregressive) outputs.\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+            Language modeling loss (for next-token prediction).\n+        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+            Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n+            sequence_length, hidden_size)`.\n+\n+            image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[List[torch.FloatTensor]] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+\n+\n+class JanusVisionEmbeddings(nn.Module):\n+    def __init__(self, config: JanusVisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.image_size = config.image_size\n+        self.patch_size = config.patch_size\n+\n+        self.patch_embedding = nn.Conv2d(\n+            in_channels=config.num_channels,\n+            out_channels=self.embed_dim,\n+            kernel_size=self.patch_size,\n+            stride=self.patch_size,\n+            padding=\"valid\",\n+        )\n+\n+        self.num_patches = (self.image_size // self.patch_size) ** 2\n+        self.num_positions = self.num_patches\n+        self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n+        self.register_buffer(\"position_ids\", torch.arange(self.num_positions).expand((1, -1)), persistent=False)\n+\n+    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+        \"\"\"\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing and no class embeddings.\n+\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n+        \"\"\"\n+\n+        num_patches = embeddings.shape[1]\n+        num_positions = self.position_embedding.weight.shape[0]\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embedding(self.position_ids)\n+\n+        patch_pos_embed = self.position_embedding.weight.unsqueeze(0)\n+\n+        dim = embeddings.shape[-1]\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n+        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n+        patch_pos_embed = nn.functional.interpolate(\n+            patch_pos_embed,\n+            size=(new_height, new_width),\n+            mode=\"bicubic\",\n+            align_corners=False,\n+        )\n+\n+        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n+        return patch_pos_embed\n+\n+    def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n+        _, _, height, width = pixel_values.shape\n+        target_dtype = self.patch_embedding.weight.dtype\n+        patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]\n+        embeddings = patch_embeds.flatten(2).transpose(1, 2)\n+\n+        if interpolate_pos_encoding:\n+            pos_embeds = self.interpolate_pos_encoding(embeddings, height, width)\n+        else:\n+            pos_embeds = self.position_embedding(self.position_ids)\n+\n+        embeddings = embeddings + pos_embeds\n+\n+        return embeddings\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class JanusVisionAttention(nn.Module):\n+    \"\"\"Attention Class for Janus Vision Encoder\"\"\"\n+\n+    def __init__(self, config: JanusVisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = self.embed_dim // self.num_heads\n+        if self.head_dim * self.num_heads != self.embed_dim:\n+            raise ValueError(\n+                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n+                f\" {self.num_heads}).\"\n+            )\n+        self.scale = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        proj_dropout = config.projection_dropout\n+        qk_norm = config.use_qk_norm\n+\n+        # Janus has no MHA, hence for `eager_attention_forward` call setting `num_key_value_groups` to 1.\n+        self.num_key_value_groups = 1\n+\n+        self.q_proj = nn.Linear(self.embed_dim, self.num_heads * self.head_dim, bias=config.attention_bias)\n+        self.k_proj = nn.Linear(self.embed_dim, self.num_heads * self.head_dim, bias=config.attention_bias)\n+        self.v_proj = nn.Linear(self.embed_dim, self.num_heads * self.head_dim, bias=config.attention_bias)\n+        self.projection_layer = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.projection_dropout = nn.Dropout(proj_dropout) if proj_dropout > 0 else nn.Identity()\n+\n+        self.q_norm = nn.LayerNorm(self.embed_dim) if qk_norm else nn.Identity()\n+        self.k_norm = nn.LayerNorm(self.embed_dim) if qk_norm else nn.Identity()\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ):\n+        batch_size, seq_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.reshape(-1, self.num_heads, self.head_dim)\n+        query_states = self.q_norm(query_states)\n+\n+        key_states = key_states.reshape(-1, self.num_heads, self.head_dim)\n+        key_states = self.k_norm(key_states)\n+\n+        query_states = query_states.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scale,\n+            is_causal=False,\n+            **kwargs,\n+        )\n+        attn_output = attn_output.reshape(batch_size, seq_len, self.embed_dim)\n+\n+        output = self.projection_layer(attn_output)\n+        output = self.projection_dropout(output)\n+\n+        outputs = (output, attn_weights) if output_attentions else (output, None)\n+        return outputs\n+\n+\n+class JanusVisionMLP(nn.Module):\n+    def __init__(self, config: JanusVisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.intermediate_size = int(config.hidden_size * config.mlp_ratio)\n+        self.activation_fn = ACT2FN[config.hidden_act]  # Gelu act\n+        self.fc1 = nn.Linear(config.hidden_size, self.intermediate_size)\n+        self.fc2 = nn.Linear(self.intermediate_size, config.hidden_size)\n+        self.dropout1 = nn.Dropout(config.hidden_dropout_rate)\n+        self.dropout2 = nn.Dropout(config.hidden_dropout_rate)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states = self.activation_fn(hidden_states)\n+        hidden_states = self.dropout1(hidden_states)\n+        hidden_states = self.fc2(hidden_states)\n+        hidden_states = self.dropout2(hidden_states)\n+        return hidden_states\n+\n+\n+class JanusVisionEncoderLayer(nn.Module):\n+    def __init__(self, config: JanusVisionConfig):\n+        super().__init__()\n+        self.embed_dim = config.hidden_size\n+        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n+        self.self_attn = JanusVisionAttention(config)\n+        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n+        self.mlp = JanusVisionMLP(config)\n+        self.config = config\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: torch.Tensor,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.FloatTensor]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`):\n+                Input to the layer of shape `(batch, seq_len, embed_dim)`.\n+            attention_mask (`torch.FloatTensor`):\n+                Attention mask of shape `(batch, 1, q_len, k_v_seq_len)` where padding elements are indicated by very large negative values.\n+            output_attentions (`bool`, *optional*, defaults to `False`):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.layer_norm1(hidden_states)\n+        hidden_states, attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        residual = hidden_states\n+        hidden_states = self.layer_norm2(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (attn_weights,)\n+\n+        return outputs\n+\n+\n+class JanusVisionEncoder(nn.Module):\n+    \"\"\"\n+    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n+    [`JanusVisionEncoderLayer`].\n+\n+    Args:\n+        config: JanusVisionConfig\n+    \"\"\"\n+\n+    def __init__(self, config: JanusVisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.layers = nn.ModuleList([JanusVisionEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.gradient_checkpointing = False\n+\n+    # Ignore copy\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        inputs_embeds,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> BaseModelOutput:\n+        r\"\"\"\n+        Args:\n+            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n+                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n+                than the model's internal embedding lookup matrix.\n+            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+                - 1 for tokens that are **not masked**,\n+                - 0 for tokens that are **masked**.\n+\n+                [What are attention masks?](../glossary#attention-mask)\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n+                for more detail.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        encoder_states = () if output_hidden_states else None\n+        all_attentions = () if output_attentions else None\n+\n+        hidden_states = inputs_embeds\n+        for encoder_layer in self.layers:\n+            if output_hidden_states:\n+                encoder_states = encoder_states + (hidden_states,)\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    encoder_layer.__call__,\n+                    hidden_states,\n+                    attention_mask,\n+                    output_attentions,\n+                )\n+            else:\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    output_attentions=output_attentions,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_attentions = all_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            encoder_states = encoder_states + (hidden_states,)\n+\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=encoder_states,\n+            attentions=all_attentions,\n+        )\n+\n+\n+JANUS_VISION_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using [`JanusProcessor`]. See [`JanusProcessor.__call__`] for\n+            details.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n+            Whether to interpolate the pre-trained position encodings.\n+\"\"\"\n+\n+\n+class JanusVisionModel(JanusPreTrainedModel):\n+    main_input_name = \"pixel_values\"\n+    config_class = JanusVisionConfig\n+\n+    def __init__(self, config: JanusVisionConfig):\n+        super().__init__(config)\n+        self.config = config\n+        embed_dim = config.hidden_size\n+\n+        self.embeddings = JanusVisionEmbeddings(config)\n+        self.encoder = JanusVisionEncoder(config)\n+        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(JANUS_VISION_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=JanusVisionConfig)\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n+    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+        r\"\"\"\n+        Returns:\n+\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n+\n+        encoder_outputs = self.encoder(\n+            inputs_embeds=hidden_states,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = self.post_layernorm(last_hidden_state)\n+\n+        pooled_output = last_hidden_state[:, 0, :]\n+        pooled_output = self.post_layernorm(pooled_output)\n+\n+        if not return_dict:\n+            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooled_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+    def get_input_embeddings(self):\n+        return self.embeddings\n+\n+\n+class JanusVisionAlignerMLP(nn.Module):\n+    def __init__(self, config: JanusVisionConfig):\n+        super().__init__()\n+\n+        self.fc1 = nn.Linear(config.hidden_size, config.projection_dim)\n+        self.hidden_layers = nn.ModuleList(\n+            [nn.Linear(config.projection_dim, config.projection_dim) for _ in range(1, config.depth)]\n+        )\n+        self.activation_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.fc1(hidden_states)\n+        for layer in self.hidden_layers:\n+            hidden_states = self.activation_fn(hidden_states)\n+            hidden_states = layer(hidden_states)\n+        return hidden_states\n+\n+\n+class JanusVQVAEVectorQuantizer(nn.Module):\n+    \"\"\"\n+    A module for vector quantization using learned embedding vectors.\n+\n+    This module implements the quantization process similar to te one described in\n+    the VQ-VAE (Vector Quantized Variational AutoEncoder) paper. It quantizes continuous\n+    input vectors into discrete codebook vectors, which are learned during training.\n+    Current implementation improves over previous ones by avoiding costly matrix multiplications\n+    and allowing for post-hoc remapping of indices.\n+    \"\"\"\n+\n+    def __init__(self, config: JanusVQVAEConfig):\n+        super().__init__()\n+        self.num_embeddings = config.num_embeddings\n+        self.embedding_dim = config.embed_dim\n+        self.beta = getattr(config, \"beta\", 0.25)\n+\n+        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n+        self.quant_state_dims = [config.num_patches] * 2\n+\n+    def forward(self, hidden_state: torch.Tensor):\n+        hidden_state = hidden_state.permute(0, 2, 3, 1).contiguous()\n+        hidden_state_flattened = hidden_state.view(-1, self.embedding_dim)\n+\n+        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n+        distances = (\n+            torch.sum(hidden_state_flattened**2, dim=1, keepdim=True)\n+            + torch.sum(self.embedding.weight**2, dim=1)\n+            - 2 * torch.einsum(\"bd,dn->bn\", hidden_state_flattened, self.embedding.weight.transpose(0, 1))\n+        )\n+\n+        min_encoding_indices = torch.argmin(distances, dim=1)\n+        hidden_state_quant = self.embedding(min_encoding_indices).view(hidden_state.shape)\n+\n+        # compute loss for embedding\n+        loss = torch.mean((hidden_state_quant.detach() - hidden_state) ** 2) + self.beta * torch.mean(\n+            (hidden_state_quant - hidden_state.detach()) ** 2\n+        )\n+\n+        # preserve gradients\n+        hidden_state_quant = hidden_state + (hidden_state_quant - hidden_state).detach()\n+\n+        # reshape back to match original input shape\n+        hidden_state_quant = hidden_state_quant.permute(0, 3, 1, 2).contiguous()\n+\n+        return hidden_state_quant, loss, min_encoding_indices\n+\n+    def get_codebook_entry(self, image_tokens: torch.LongTensor) -> torch.FloatTensor:\n+        batch_size = image_tokens.shape[0]\n+        emb_dim: int = self.embedding.weight.shape[-1]\n+\n+        # get quantized latent vectors\n+        hidden_state_quant = self.embedding(image_tokens)\n+        # l2 normalization on the last dimension\n+        hidden_state_quant = F.normalize(hidden_state_quant, p=2, dim=-1)\n+\n+        # reshape back to match original input shape\n+        hidden_state_quant = hidden_state_quant.view((batch_size, *self.quant_state_dims, emb_dim))\n+        hidden_state_quant = hidden_state_quant.permute(0, 3, 1, 2).contiguous()\n+\n+        return hidden_state_quant\n+\n+\n+class JanusVQVAEResnetBlock(nn.Module):\n+    def __init__(\n+        self,\n+        config,\n+        in_channels,\n+        out_channels=None,\n+        conv_shortcut=False,\n+    ):\n+        super().__init__()\n+        self.in_channels = in_channels\n+        self.out_channels = in_channels if out_channels is None else out_channels\n+        self.use_conv_shortcut = conv_shortcut\n+\n+        self.norm1 = torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n+        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n+        self.norm2 = torch.nn.GroupNorm(num_groups=32, num_channels=out_channels, eps=1e-6, affine=True)\n+        self.dropout = torch.nn.Dropout(config.dropout)\n+        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n+        if self.in_channels != self.out_channels:\n+            if self.use_conv_shortcut:\n+                self.conv_shortcut = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n+            else:\n+                self.nin_shortcut = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n+\n+    def forward(self, hidden_states):\n+        residual = hidden_states\n+        hidden_states = self.norm1(hidden_states)\n+        hidden_states *= torch.sigmoid(hidden_states)\n+        hidden_states = self.conv1(hidden_states)\n+\n+        hidden_states = self.norm2(hidden_states)\n+        hidden_states *= torch.sigmoid(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+        hidden_states = self.conv2(hidden_states)\n+\n+        if self.in_channels != self.out_channels:\n+            if self.use_conv_shortcut:\n+                residual = self.conv_shortcut(residual)\n+            else:\n+                residual = self.nin_shortcut(residual)\n+\n+        return residual + hidden_states\n+\n+\n+class JanusVQVAEAttnBlock(nn.Module):\n+    def __init__(self, in_channels):\n+        super().__init__()\n+        self.in_channels = in_channels\n+\n+        self.norm = torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n+        self.q = torch.nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n+        self.k = torch.nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n+        self.v = torch.nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n+        self.proj_out = torch.nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n+\n+    def forward(self, hidden_states):\n+        residual = hidden_states\n+        hidden_states = self.norm(hidden_states)\n+        query_states = self.q(hidden_states)\n+        key_states = self.k(hidden_states)\n+        value_states = self.v(hidden_states)\n+\n+        # compute attention\n+        batch_size, channels, height, width = query_states.shape\n+        query_states = query_states.reshape(batch_size, channels, height * width).permute(0, 2, 1)\n+        key_states = key_states.reshape(batch_size, channels, height * width)\n+        attn_weights = torch.bmm(query_states, key_states)\n+        attn_weights = attn_weights * (int(channels) ** (-0.5))\n+        attn_weights = F.softmax(attn_weights, dim=2)\n+\n+        # attend to values\n+        value_states = value_states.reshape(batch_size, channels, height * width)\n+        attn_weights = attn_weights.permute(0, 2, 1)\n+        attn_output = torch.bmm(value_states, attn_weights).reshape(batch_size, channels, height, width)\n+\n+        attn_output = self.proj_out(attn_output)\n+        return residual + attn_output\n+\n+\n+class JanusVQVAEConvDownsample(nn.Module):\n+    def __init__(self, in_channels):\n+        super().__init__()\n+        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=2, padding=0)\n+\n+    def forward(self, hidden_states):\n+        # no asymmetric padding in torch conv, must do it ourselves\n+        hidden_states = F.pad(hidden_states, pad=(0, 1, 0, 1), mode=\"constant\", value=0)\n+        hidden_states = self.conv(hidden_states)\n+        return hidden_states\n+\n+\n+class JanusVQVAEConvUpsample(nn.Module):\n+    def __init__(self, in_channels):\n+        super().__init__()\n+        self.conv = torch.nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n+\n+    def forward(self, hidden_states):\n+        hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n+        hidden_states = self.conv(hidden_states)\n+        return hidden_states\n+\n+\n+class JanusVQVAEMidBlock(nn.Module):\n+    def __init__(self, config: JanusVQVAEConfig, channels: int):\n+        super().__init__()\n+        self.block_1 = JanusVQVAEResnetBlock(\n+            config=config,\n+            in_channels=channels,\n+            out_channels=channels,\n+        )\n+        self.attn_1 = JanusVQVAEAttnBlock(channels)\n+        self.block_2 = JanusVQVAEResnetBlock(\n+            config=config,\n+            in_channels=channels,\n+            out_channels=channels,\n+        )\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.block_1(hidden_states)\n+        hidden_states = self.attn_1(hidden_states)\n+        hidden_states = self.block_2(hidden_states)\n+        return hidden_states\n+\n+\n+class JanusVQVAEEncoder(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+\n+        self.num_resolutions = len(config.channel_multiplier)\n+        self.num_res_blocks = config.num_res_blocks\n+        base_channels = config.base_channels\n+        in_channels = config.in_channels\n+        double_latent = config.double_latent\n+        latent_channels = config.latent_channels\n+        channel_multiplier = config.channel_multiplier\n+\n+        self.conv_in = torch.nn.Conv2d(in_channels, base_channels, kernel_size=3, stride=1, padding=1)\n+\n+        in_channel_multiplier = (1,) + tuple(channel_multiplier)\n+        self.in_channel_multiplier = in_channel_multiplier\n+        self.down = nn.ModuleList()\n+        for i_level in range(self.num_resolutions):\n+            block = nn.ModuleList()\n+            attn = nn.ModuleList()\n+            block_in = base_channels * in_channel_multiplier[i_level]\n+            block_out = base_channels * channel_multiplier[i_level]\n+            for i_block in range(self.num_res_blocks):\n+                block.append(\n+                    JanusVQVAEResnetBlock(\n+                        config=config,\n+                        in_channels=block_in,\n+                        out_channels=block_out,\n+                    )\n+                )\n+                block_in = block_out\n+                if i_level == self.num_resolutions - 1:\n+                    attn.append(JanusVQVAEAttnBlock(block_in))\n+\n+            down = nn.Module()\n+            down.block = block\n+            down.attn = attn\n+            if i_level != self.num_resolutions - 1:\n+                down.downsample = JanusVQVAEConvDownsample(block_in)\n+            self.down.append(down)\n+\n+        self.mid = JanusVQVAEMidBlock(config, block_in)\n+\n+        self.norm_out = torch.nn.GroupNorm(num_groups=32, num_channels=block_in, eps=1e-6, affine=True)\n+        self.conv_out = torch.nn.Conv2d(\n+            block_in,\n+            2 * latent_channels if double_latent else latent_channels,\n+            kernel_size=3,\n+            stride=1,\n+            padding=1,\n+        )\n+\n+    def forward(self, pixel_values: torch.LongTensor):\n+        # downsampling\n+        hidden_states = [self.conv_in(pixel_values)]\n+        for i_level in range(self.num_resolutions):\n+            for i_block in range(self.num_res_blocks):\n+                hidden_state = self.down[i_level].block[i_block](\n+                    hidden_states[-1],\n+                )\n+                if len(self.down[i_level].attn) > 0:\n+                    hidden_state = self.down[i_level].attn[i_block](hidden_state)\n+                hidden_states.append(hidden_state)\n+            if i_level != self.num_resolutions - 1:\n+                hidden_states.append(self.down[i_level].downsample(hidden_states[-1]))\n+\n+        # middle\n+        last_hidden_state = hidden_states[-1]\n+        last_hidden_state = self.mid(last_hidden_state)\n+\n+        # end\n+        last_hidden_state = self.norm_out(last_hidden_state)\n+        last_hidden_state *= torch.sigmoid(last_hidden_state)\n+        last_hidden_state = self.conv_out(last_hidden_state)\n+        return last_hidden_state\n+\n+\n+class JanusVQVAEDecoder(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+\n+        self.num_resolutions = len(config.channel_multiplier)\n+        self.num_res_blocks = config.num_res_blocks\n+        base_channels = config.base_channels\n+        latent_channels = config.latent_channels\n+        out_channels = config.out_channels\n+\n+        # compute in_ch_mult, block_in and curr_res at lowest res\n+        block_in = base_channels * config.channel_multiplier[self.num_resolutions - 1]\n+\n+        # z to block_in\n+        self.conv_in = torch.nn.Conv2d(latent_channels, block_in, kernel_size=3, stride=1, padding=1)\n+\n+        # middle\n+        self.mid = JanusVQVAEMidBlock(config, block_in)\n+\n+        # upsampling\n+        self.up = nn.ModuleList()\n+        for i_level in reversed(range(self.num_resolutions)):\n+            block = nn.ModuleList()\n+            attn = nn.ModuleList()\n+            block_out = base_channels * config.channel_multiplier[i_level]\n+            for i_block in range(self.num_res_blocks + 1):\n+                block.append(\n+                    JanusVQVAEResnetBlock(\n+                        config=config,\n+                        in_channels=block_in,\n+                        out_channels=block_out,\n+                    )\n+                )\n+                block_in = block_out\n+                if i_level == self.num_resolutions - 1:\n+                    attn.append(JanusVQVAEAttnBlock(block_in))\n+            up = nn.Module()\n+            up.block = block\n+            up.attn = attn\n+            if i_level != 0:\n+                up.upsample = JanusVQVAEConvUpsample(block_in)\n+            self.up.append(up)\n+\n+        # end\n+        self.norm_out = torch.nn.GroupNorm(num_groups=32, num_channels=block_in, eps=1e-6, affine=True)\n+        self.conv_out = torch.nn.Conv2d(block_in, out_channels, kernel_size=3, stride=1, padding=1)\n+\n+    def forward(self, hidden_state: torch.FloatTensor) -> torch.FloatTensor:\n+        hidden_state = self.conv_in(hidden_state)\n+\n+        # middle\n+        hidden_state = self.mid(hidden_state)\n+\n+        # upsampling\n+        for i_level in range(self.num_resolutions):\n+            for i_block in range(self.num_res_blocks + 1):\n+                hidden_state = self.up[i_level].block[i_block](hidden_state)\n+                if len(self.up[i_level].attn) > 0:\n+                    hidden_state = self.up[i_level].attn[i_block](hidden_state)\n+            if i_level != self.num_resolutions - 1:\n+                hidden_state = self.up[i_level].upsample(hidden_state)\n+\n+        hidden_state = self.norm_out(hidden_state)\n+        hidden_state *= torch.sigmoid(hidden_state)\n+        hidden_state = self.conv_out(hidden_state)\n+        return hidden_state\n+\n+\n+JANUS_VQ_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`JanusVQVAEConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The VQ-VAE model used in Janus for encoding/decoding images into discrete tokens.\n+    This model follows the \"Make-a-scene: Scene-based text-to-image generation with human priors\" paper from\n+    [ Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman](https://arxiv.org/abs/2203.13131).\n+    \"\"\",\n+    JANUS_VQ_START_DOCSTRING,\n+)\n+class JanusVQVAE(JanusPreTrainedModel):\n+    \"\"\"Vision Transformer-based VQ-VAE model for encoding and decoding pixel values.\"\"\"\n+\n+    config_class = JanusVQVAEConfig\n+\n+    _no_split_modules = [\n+        \"JanusVQVAEAttnBlock\",\n+        \"JanusVQVAEResnetBlock\",\n+        \"JanusVQVAEVectorQuantizer\",\n+    ]\n+    main_input_name = \"pixel_values\"\n+\n+    def __init__(self, config: JanusVQVAEConfig):\n+        super().__init__(config)\n+\n+        self.encoder = JanusVQVAEEncoder(config)\n+        self.quantize = JanusVQVAEVectorQuantizer(config)\n+        self.quant_conv = torch.nn.Conv2d(config.latent_channels, config.embed_dim, 1)\n+        self.post_quant_conv = torch.nn.Conv2d(config.embed_dim, config.latent_channels, 1)\n+        self.eval()  # Janus's VQ model is frozen\n+        self.decoder = JanusVQVAEDecoder(config)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize the VQVAE model.\n+        self.post_init()\n+\n+    def encode(self, pixel_values: torch.LongTensor):\n+        hidden_states = self.encoder(pixel_values)\n+        hidden_states = self.quant_conv(hidden_states)\n+        quant, emb_loss, indices = self.quantize(hidden_states)\n+        return quant, emb_loss, indices\n+\n+    def decode(self, image_tokens: torch.LongTensor) -> torch.FloatTensor:\n+        \"\"\"\n+        Decodes quantized token IDs into pixel values.\n+        Args:\n+            image_tokens (torch.LongTensor): Batch of token IDs.\n+        Returns:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                Pixel values decoded from the token IDs.\n+        \"\"\"\n+        if image_tokens.shape[1] != self.quantize.quant_state_dims[0] * self.quantize.quant_state_dims[1]:\n+            raise ValueError(\n+                f\"Expected `image_tokens` to have shape `(batch_size, {self.quantize.quant_state_dims[0] * self.quantize.quant_state_dims[1]})`, \"\n+                f\"but got shape `{image_tokens.shape}`.\"\n+            )\n+        codebook_entry = self.quantize.get_codebook_entry(image_tokens)\n+        hidden_states = self.post_quant_conv(codebook_entry)\n+        pixel_values = self.decoder(hidden_states)\n+        return pixel_values\n+\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+    ) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n+        \"\"\"\n+        Encodes pixel values into quantized tokens and decodes them back.\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n+                The tensors corresponding to the input images.\n+        Returns:\n+            decoded_pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                Reconstructed pixel values after encoding and decoding the input.\n+            embedding_loss (`torch.FloatTensor`): Embedding loss.\n+        \"\"\"\n+\n+        batch_size = pixel_values.shape[0]\n+        quant, embedding_loss, indices = self.encode(pixel_values)\n+        decoded_pixel_values = self.decode(indices.view(batch_size, -1))\n+        output = JanusVQVAEOutput(decoded_pixel_values, embedding_loss)\n+\n+        return output\n+\n+\n+class JanusVQVAEAlignerMLP(nn.Module):\n+    def __init__(self, config: JanusVQVAEConfig):\n+        super().__init__()\n+\n+        self.fc1 = nn.Linear(config.embed_dim, config.projection_dim)\n+        self.hidden_layers = nn.ModuleList(\n+            [nn.Linear(config.projection_dim, config.projection_dim) for _ in range(1, config.num_hidden_layers)]\n+        )\n+        self.activation_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.fc1(hidden_states)\n+        for layer in self.hidden_layers:\n+            hidden_states = self.activation_fn(hidden_states)\n+            hidden_states = layer(hidden_states)\n+        return hidden_states\n+\n+\n+class JanusVQVAEHead(nn.Module):\n+    \"\"\"Head used for sampling tokens in image generation, replacing the usual lm head.\"\"\"\n+\n+    def __init__(self, config: JanusVQVAEConfig):\n+        super().__init__()\n+        self.proj_out = nn.Linear(config.image_token_embed_dim, config.projection_dim)\n+        self.activation_fn = ACT2FN[config.hidden_act]\n+        self.vision_head = nn.Linear(config.projection_dim, config.num_embeddings)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.tensor:\n+        hidden_states = self.proj_out(hidden_states)\n+        hidden_states = self.activation_fn(hidden_states)\n+        hidden_states = self.vision_head(hidden_states)\n+        return hidden_states\n+\n+\n+JANUS_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n+            The tensors corresponding to the input images. Pixel values can be obtained using\n+            [`AutoImageProcessor`].\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n+            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n+            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n+            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The Janus model which consists of a siglip vision backbone, a Llama language model and a VQ model.\"\"\",\n+    JANUS_START_DOCSTRING,\n+)\n+class JanusModel(JanusPreTrainedModel):\n+    def __init__(self, config: JanusConfig):\n+        super().__init__(config)\n+        self.config = config\n+        # This is necessary for backward compatibility, see SiglipModel initialization\n+        self.vision_model = JanusVisionModel._from_config(config.vision_config)\n+        self.aligner = JanusVisionAlignerMLP(self.vision_model.config)\n+\n+        self.vqmodel = JanusVQVAE._from_config(config.vq_config)\n+\n+        # Below generation_* modules are used for Image generation.\n+        # Embeddings used for image generation, instead of Janus vision embeddings.\n+        self.generation_embeddings = nn.Embedding(self.vqmodel.config.num_embeddings, self.vqmodel.config.embed_dim)\n+        self.generation_aligner = JanusVQVAEAlignerMLP(self.vqmodel.config)\n+        self.generation_head = JanusVQVAEHead(self.vqmodel.config)\n+\n+        self.language_model = AutoModel.from_config(config=config.text_config)\n+\n+        self.gradient_checkpointing = False\n+        # Initialize weights and apply final processing.\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def get_image_features(self, pixel_values):\n+        image_embeds = self.vision_model(pixel_values)\n+        image_embeds = self.aligner(image_embeds.last_hidden_state)\n+        return image_embeds\n+\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(JANUS_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n+    ):\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\n+                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        if pixel_values is not None and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_embeds = self.get_image_features(pixel_values)\n+            image_attention_mask = input_ids == self.config.image_token_index\n+\n+            embed_dim = inputs_embeds.shape[-1]\n+            image_features = image_embeds.reshape(-1, embed_dim)\n+            image_attention_mask = image_attention_mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n+\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(image_attention_mask, image_features)\n+\n+        lm_output = self.language_model(\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        output = JanusBaseModelOutputWithPast(\n+            last_hidden_state=lm_output.last_hidden_state,\n+            past_key_values=lm_output.past_key_values,\n+            hidden_states=lm_output.hidden_states,\n+            attentions=lm_output.attentions,\n+            image_hidden_states=image_embeds if pixel_values is not None else None,\n+        )\n+\n+        return output\n+\n+\n+class JanusForConditionalGeneration(JanusPreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"model.language_model.embed_tokens.weight\", \"lm_head.weight\"]\n+    _supports_static_cache = True\n+\n+    def __init__(self, config: JanusConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.model = JanusModel(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing.\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.language_model.set_input_embeddings(value)\n+\n+    def prepare_embeddings_for_image_generation(self, inputs: torch.Tensor) -> torch.Tensor:\n+        hidden_state = self.model.generation_embeddings(inputs)\n+        hidden_state = self.model.generation_aligner(hidden_state)\n+        return hidden_state\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    def set_decoder(self, decoder):\n+        self.model = decoder\n+\n+    def get_decoder(self):\n+        return self.model\n+\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(JANUS_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=JanusCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n+    ):\n+        r\"\"\"\n+        Args:\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n+\n+        Returns:\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+\n+        output = JanusCausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=outputs.image_hidden_states,\n+        )\n+        return output\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        pixel_values=None,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- extra custom processing\n+\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+        # Otherwise we need pixel values to be passed to model\n+        if cache_position[0] == 0:\n+            model_inputs[\"pixel_values\"] = pixel_values\n+\n+        return model_inputs\n+\n+    def decode_image_tokens(self, image_tokens: torch.Tensor):\n+        \"\"\"\n+        Decodes generated image tokens from language model to continuous pixel values\n+        with VQGAN module via upsampling.\n+        Args:\n+            image_tokens (`torch.LongTensor` of shape `(batch_size, num_of_tokens)`):\n+                The tensors corresponding to the input images.\n+        \"\"\"\n+        decoded_image = self.model.vqmodel.decode(image_tokens)\n+        decoded_image = decoded_image.permute(0, 2, 3, 1)\n+        return decoded_image\n+\n+    @torch.no_grad\n+    def generate(\n+        self,\n+        inputs: torch.Tensor = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        logits_processor: Optional[LogitsProcessorList] = None,\n+        **kwargs,\n+    ):\n+        # 1. Handle generation config and model kwargs\n+        generation_config = kwargs.pop(\"generation_config\", self.generation_config)\n+        generation_config = copy.deepcopy(generation_config)\n+\n+        # Default to \"text\" generation if mode isn't provided\n+        generation_mode = kwargs.pop(\"generation_mode\", \"text\")\n+        if generation_mode == \"text\":\n+            # Set guidance_scale=None to prevent running UnbatchedCFG processor.\n+            return super().generate(\n+                inputs=inputs,\n+                attention_mask=attention_mask,\n+                generation_config=generation_config,\n+                guidance_scale=None,\n+                **kwargs,\n+            )\n+\n+        model_kwargs = generation_config.update(**kwargs)  # All unused kwargs must be model kwargs\n+\n+        # Validate generation mode\n+        if generation_config.get_generation_mode() not in (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n+            raise ValueError(\n+                \"Got incompatible mode for Image Generation, should be one of greedy or sampling. \"\n+                \"Ensure that beam search is de-activated by setting `num_beams=1` and `num_beam_groups=1`.\"\n+            )\n+\n+        # Validate the configuration and model kwargs\n+        generation_config.validate()\n+        self._validate_model_kwargs(model_kwargs.copy())\n+\n+        # 2. Initialize logit processors\n+        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n+\n+        # Set `use_cache=True` as we will be using input embeds for generation.\n+        model_kwargs[\"use_cache\"] = True\n+\n+        if generation_config.guidance_scale is None:\n+            logger.warning(\"`guidance_scale` is required for CFG but not provided. Setting to default value of 5.\")\n+            generation_config.guidance_scale = 5\n+        model_kwargs[\"guidance_scale\"] = generation_config.guidance_scale\n+\n+        # 3. Prepare model inputs\n+        input_ids, model_input_name, model_kwargs = self._prepare_model_inputs(\n+            inputs, generation_config.bos_token_id, model_kwargs\n+        )\n+        dtype, device = input_ids.dtype, input_ids.device\n+\n+        if len(input_ids.shape) != 2:\n+            raise ValueError(\n+                f\"Expected input ids of shape (batch_size, seq_len), but got {input_ids.shape}\"\n+                \"Passing `inputs embeds` is not supported currently.\"\n+            )\n+\n+        # Prepare special tokens which will be used generate internally.\n+        kwargs_has_attention_mask = attention_mask is not None\n+        self._prepare_special_tokens(generation_config, kwargs_has_attention_mask, device=input_ids.device)\n+\n+        # 4. Add CFG processor along with user passed logit processor.\n+        if generation_config.guidance_scale and generation_config.guidance_scale > 1:\n+            logits_processor.append(ClassifierFreeGuidanceLogitsProcessor(generation_config.guidance_scale))\n+            generation_config.guidance_scale = None  # Reset to prevent processor duplication.\n+\n+        # 5. Prepare logits processor\n+        logits_processor = self._get_logits_processor(\n+            generation_config=generation_config,\n+            input_ids_seq_length=input_ids.shape[1],\n+            encoder_input_ids=input_ids,\n+            prefix_allowed_tokens_fn=None,\n+            logits_processor=logits_processor,\n+            device=device,\n+        )\n+\n+        # 6. Expand inputs for multiple image generations per prompt.\n+        input_ids, model_kwargs = self._expand_inputs_for_generation(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            expand_size=generation_config.num_return_sequences,\n+            **model_kwargs,\n+        )\n+\n+        # 7. Prepare input and model caches\n+        num_image_tokens = self.model.vision_model.config.num_image_tokens\n+        batch_size, seq_len = input_ids.shape\n+\n+        input_tokens = input_ids.repeat(2, 1)  # Double batch size for conditional/unconditional logits\n+        attention_mask = model_kwargs.pop(\"attention_mask\", None)\n+        attention_mask = attention_mask.repeat(2, 1)\n+        model_kwargs[\"attention_mask\"] = attention_mask\n+\n+        # Mask all the tokens that are neither BOS nor BOI with pad token in the unconditional logits.\n+        mask = (input_tokens[batch_size:, :] != generation_config.bos_token_id) & (\n+            input_tokens[batch_size:, :] != generation_config.generation_kwargs[\"boi_token_id\"]\n+        )\n+        input_tokens[batch_size:, :].masked_fill_(mask, generation_config.pad_token_id)\n+\n+        inputs_embeds = self.get_input_embeddings()(input_tokens)\n+\n+        model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n+\n+        if model_kwargs.get(\"past_key_values\", None) is None:\n+            # Prepare cache if not provided.\n+            model_kwargs[\"past_key_values\"] = self._get_cache(\n+                cache_implementation=generation_config.cache_implementation or \"static\",\n+                # batch_size should account for both conditional/unconditional input; hence multiplied by 2.\n+                batch_size=batch_size * 2,\n+                # we should have at least a cache len of seq_len + num_image_tokens.\n+                max_cache_len=max(generation_config.max_length, num_image_tokens + seq_len),\n+                device=device,\n+                model_kwargs=model_kwargs,\n+            )\n+\n+        # Placeholder for generated tokens.\n+        generated_tokens = torch.zeros((batch_size, num_image_tokens), dtype=dtype, device=device)\n+\n+        # 8. init attention / hidden states / scores tuples\n+        output_attentions = generation_config.output_attentions\n+        output_hidden_states = generation_config.output_hidden_states\n+        output_scores = generation_config.output_scores\n+        output_logits = generation_config.output_logits\n+        return_dict_in_generate = generation_config.return_dict_in_generate\n+\n+        raw_scores = () if (return_dict_in_generate and output_scores) else None\n+        raw_logits = () if (return_dict_in_generate and output_logits) else None\n+        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n+        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n+\n+        for i in range(num_image_tokens):\n+            model_inputs = self.prepare_inputs_for_generation(\n+                inputs_embeds=inputs_embeds, input_ids=input_tokens, **model_kwargs\n+            )\n+\n+            model_inputs[\"attention_mask\"] = model_inputs[\"attention_mask\"].to(inputs_embeds.device)\n+            model_inputs[\"cache_position\"] = model_inputs[\"cache_position\"].to(inputs_embeds.device)\n+\n+            outputs = self.model.language_model(\n+                **model_inputs,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+            )\n+\n+            # Update model_kwargs like cache_position for next generation.\n+            model_kwargs = self._update_model_kwargs_for_generation(outputs, model_kwargs)\n+            hidden_state = outputs.last_hidden_state[:, -1, :].clone()\n+\n+            # Generate scores using the generation head (Not using above defined LM Head)\n+            scores = self.model.generation_head(hidden_state)\n+            next_token_scores = logits_processor(input_ids, scores)\n+\n+            # Sample next token.\n+            if generation_config.do_sample:\n+                probs = torch.softmax(next_token_scores, dim=-1)\n+                next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n+            else:\n+                next_token = torch.argmax(next_token_scores, dim=-1)\n+\n+            generated_tokens[:, i] = next_token\n+\n+            # Prepare embeddings for the next step.\n+            next_token = torch.cat([next_token, next_token])\n+            next_token = next_token.unsqueeze(-1)\n+\n+            inputs_embeds = self.prepare_embeddings_for_image_generation(next_token)\n+\n+        if return_dict_in_generate:\n+            if output_scores:\n+                raw_scores += (scores,)\n+            if output_logits:\n+                raw_logits += (hidden_state.float(),)\n+            if output_attentions:\n+                decoder_attentions += outputs.attentions\n+            if output_hidden_states:\n+                decoder_hidden_states += outputs.hidden_states\n+\n+        if return_dict_in_generate:\n+            return GenerateDecoderOnlyOutput(\n+                sequences=generated_tokens,\n+                scores=scores,\n+                logits=raw_logits,\n+                attentions=decoder_attentions,\n+                hidden_states=decoder_hidden_states,\n+                past_key_values=outputs.past_key_values,\n+            )\n+        else:\n+            return generated_tokens\n+\n+\n+__all__ = [\"JanusPreTrainedModel\", \"JanusForConditionalGeneration\", \"JanusModel\", \"JanusVQVAE\", \"JanusVisionModel\"]"
        },
        {
            "sha": "03e3a05a27a733551b5acb7ef6fdaabfaad5fb2c",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "added",
            "additions": 1770,
            "deletions": 0,
            "changes": 1770,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=a2ef3cf5379d35da68e58c54e7981f7998ab8f1b",
            "patch": "@@ -0,0 +1,1770 @@\n+# coding=utf-8\n+# Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import copy\n+from dataclasses import dataclass\n+from typing import Callable, Dict, Iterable, List, Optional, Tuple, Union\n+\n+import numpy as np\n+import torch\n+from torch import nn\n+\n+from transformers.models.blip.image_processing_blip import BlipImageProcessor\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache\n+from ...generation import (\n+    ClassifierFreeGuidanceLogitsProcessor,\n+    GenerationMixin,\n+    GenerationMode,\n+    LogitsProcessorList,\n+)\n+from ...generation.utils import GenerateDecoderOnlyOutput\n+from ...image_processing_utils import BatchFeature, get_size_dict\n+from ...image_transforms import (\n+    resize,\n+    to_channel_dimension_format,\n+)\n+from ...image_utils import (\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    get_image_size,\n+    infer_channel_dimension_format,\n+    make_list_of_images,\n+    to_numpy_array,\n+)\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import ModelOutput\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n+    is_torch_available,\n+    is_vision_available,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from ..auto import AutoModel\n+from ..blip_2.modeling_blip_2 import Blip2VisionModel\n+from ..chameleon.configuration_chameleon import ChameleonVQVAEConfig\n+from ..chameleon.modeling_chameleon import (\n+    ChameleonVQVAE,\n+    ChameleonVQVAEEncoder,\n+    ChameleonVQVAEEncoderAttnBlock,\n+    ChameleonVQVAEEncoderConvDownsample,\n+    ChameleonVQVAEEncoderResnetBlock,\n+    ChameleonVQVAEVectorQuantizer,\n+)\n+from ..idefics.modeling_idefics import IdeficsBaseModelOutputWithPast, IdeficsCausalLMOutputWithPast\n+from ..llama.modeling_llama import eager_attention_forward\n+from ..siglip.configuration_siglip import SiglipVisionConfig\n+from ..siglip.modeling_siglip import (\n+    SiglipEncoder,\n+    SiglipEncoderLayer,\n+    SiglipVisionEmbeddings,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+    import torch.nn as nn\n+    import torch.nn.functional as F\n+    import torch.utils.checkpoint\n+\n+if is_vision_available():\n+    import PIL\n+\n+\n+from ...configuration_utils import PretrainedConfig\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+# General docstring\n+_CONFIG_FOR_DOC = \"JanusConfig\"\n+\n+\n+class JanusVisionConfig(SiglipVisionConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`JanusVisionModel`]. It is used to instantiate a\n+    `JanusVisionModel` according to the specified arguments, defining the model architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 1024):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        num_hidden_layers (`int`, *optional*, defaults to 24):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            The number of input channels.\n+        patch_size (`int`, *optional*, defaults to 16):\n+            The size (resolution) of each patch.\n+        image_size (`int`, *optional*, defaults to 384):\n+            The size (resolution) of each image.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            Dropout probability for attention weights.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"`, and `\"gelu_new\"` are supported.\n+        mlp_ratio (`float`, *optional*, defaults to 4.0):\n+            Ratio of MLP hidden dimensionality to embedding dimensionality.\n+        attention_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to add a bias to the queries, keys, and values in the attention layers.\n+        hidden_dropout_rate (`float`, *optional*, defaults to 0.0):\n+            The dropout probability for fully connected layers in the encoder.\n+        projection_dim (`int`, *optional*, defaults to 2048):\n+            Dimensionality of the MLP projection head.\n+        projection_dropout (`float`, *optional*, defaults to 0.0):\n+            Dropout probability for the projection layer.\n+        use_qk_norm (`bool`, *optional*, defaults to `False`):\n+            Whether to normalize the query and key matrices.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated normal initializer for initializing all weight matrices.\n+        depth (`int`, *optional*, defaults to 2):\n+            Number of hidden layers in the aligner module.\n+        num_image_tokens (`int`, *optional*, defaults to 576):\n+            Number of image tokens.\n+    \"\"\"\n+\n+    model_type = \"janus_vision_model\"\n+    base_config_key = \"vision_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=1024,\n+        num_hidden_layers=24,\n+        num_attention_heads=16,\n+        num_channels=3,\n+        patch_size=16,\n+        image_size=384,\n+        attention_dropout=0.0,\n+        layer_norm_eps=1e-6,\n+        hidden_act=\"gelu\",\n+        mlp_ratio=4.0,\n+        attention_bias=True,\n+        hidden_dropout_rate=0.0,\n+        projection_dim=2048,\n+        projection_dropout=0.0,\n+        use_qk_norm=False,\n+        initializer_range=0.02,\n+        depth=2,\n+        num_image_tokens=576,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            hidden_size=hidden_size,\n+            num_hidden_layers=num_hidden_layers,\n+            num_attention_heads=num_attention_heads,\n+            num_channels=num_channels,\n+            patch_size=patch_size,\n+            image_size=image_size,\n+            attention_dropout=attention_dropout,\n+            layer_norm_eps=layer_norm_eps,\n+            hidden_act=hidden_act,\n+            **kwargs,\n+        )\n+        del self.intermediate_size\n+\n+        self.mlp_ratio = mlp_ratio\n+        self.attention_bias = attention_bias\n+        self.hidden_dropout_rate = hidden_dropout_rate\n+        self.projection_dim = projection_dim\n+        self.projection_dropout = projection_dropout\n+        self.use_qk_norm = use_qk_norm\n+        self.initializer_range = initializer_range\n+        self.depth = depth\n+        self.num_image_tokens = num_image_tokens\n+\n+\n+class JanusVQVAEConfig(ChameleonVQVAEConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`JanusVQVAEModel`]. It is used to instantiate a\n+    `JanusVQVAEModel` according to the specified arguments, defining the model architecture.\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information. Instantiating a\n+    configuration with the defaults will yield a similar configuration to the VQModel of the\n+    [deepseek-community/Janus-Pro-1B](https://huggingface.co/deepseek-community/Janus-Pro-1B).\n+\n+    Args:\n+        embed_dim (`int`, *optional*, defaults to 8):\n+            Dimensionality of each embedding vector.\n+        num_embeddings (`int`, *optional*, defaults to 16384):\n+            Number of codebook embeddings.\n+        double_latent (`bool`, *optional*, defaults to `False`):\n+            Whether to use double z channels.\n+        latent_channels (`int`, *optional*, defaults to 256):\n+            Number of channels for the latent space.\n+        num_patches (`int`, *optional*, defaults to 32):\n+            Num of patches the input images can be divided into.\n+        in_channels (`int`, *optional*, defaults to 3):\n+            Number of input channels.\n+        out_channels (`int`, *optional*, defaults to 3):\n+            Number of out channels.\n+        base_channels (`int`, *optional*, defaults to 128):\n+            Base channel count.\n+        channel_multiplier (`List[int]`, *optional*, defaults to `[1, 1, 2, 2, 4]`):\n+            Channel multipliers for each resolution.\n+        num_res_blocks (`int`, *optional*, defaults to 2):\n+            Number of residual blocks.\n+        dropout (`float`, *optional*, defaults to 0.0):\n+            Dropout rate.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        projection_dim (`int`, *optional*, defaults to 2048):\n+            Dimensionality of the MLP projection head.\n+        num_hidden_layers (`int`, *optional*, defaults to 2):\n+            Number of hidden layers in VAVAE MLP Connecter module.\n+        hidden_act (`str` or `Callable`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n+        image_token_embed_dim (`int`, *optional*, defaults to 2048):\n+            Dimension of image embeddings. It should be same as the dimensionality of text embeddings.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        embed_dim: int = 8,\n+        num_embeddings: int = 16384,\n+        double_latent: bool = False,\n+        latent_channels: int = 256,\n+        num_patches: int = 32,\n+        in_channels: int = 3,\n+        out_channels: int = 3,\n+        base_channels: int = 128,\n+        channel_multiplier: List[int] = [1, 1, 2, 2, 4],\n+        num_res_blocks: int = 2,\n+        dropout: float = 0.0,\n+        initializer_range=0.02,\n+        projection_dim=2048,\n+        num_hidden_layers=2,\n+        hidden_act=\"gelu\",\n+        image_token_embed_dim=2048,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            embed_dim=embed_dim,\n+            num_embeddings=num_embeddings,\n+            double_latent=double_latent,\n+            latent_channels=latent_channels,\n+            in_channels=in_channels,\n+            base_channels=base_channels,\n+            channel_multiplier=channel_multiplier,\n+            num_res_blocks=num_res_blocks,\n+            dropout=dropout,\n+            initializer_range=initializer_range,\n+            **kwargs,\n+        )\n+        self.num_patches = num_patches\n+        self.out_channels = out_channels\n+        self.projection_dim = projection_dim\n+        self.num_hidden_layers = num_hidden_layers\n+        self.hidden_act = hidden_act\n+        self.image_token_embed_dim = image_token_embed_dim\n+\n+        del self.resolution\n+        del self.attn_resolutions\n+        del self.attn_type\n+\n+\n+class JanusConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`JanusModel`]. It is used to instantiate an\n+    Janus model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the Janus-1B or Janus-7B models.\n+\n+    e.g. [deepseek-community/Janus-Pro-1B](https://huggingface.co/deepseek-community/Janus-Pro-1B) or\n+    [deepseek-community/Janus-Pro-7B](https://huggingface.co/deepseek-community/Janus-Pro-7B)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `LlamaConfig`):\n+            The config object or dictionary of the text backbone.\n+        vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `JanusVisionConfig`):\n+            The config object or dictionary of the vision backbone.\n+        vq_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `JanusVQVAEConfig`):\n+            The config object or dictionary of the VQVAE backbone.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import JanusForConditionalGeneration, JanusConfig, JanusVisionConfig, JanusVQVAEConfig, LlamaConfig\n+\n+    >>> # Initializing a Janus vision config\n+    >>> vision_config = JanusVisionConfig()\n+\n+    >>> # Initializing a Llama config\n+    >>> text_config = LlamaConfig()\n+\n+    >>> # Initializing a VQ config\n+    >>> vq_config = JanusVQVAEConfig()\n+\n+    >>> # Initializing a Janus Pro 1B style configuration\n+    >>> configuration = JanusConfig(vision_config=vision_config, text_config=text_config, vq_config=vq_config)\n+\n+    >>> # Initializing a model from the Janus Pro 1B style configuration\n+    >>> model = JanusForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"janus\"\n+    sub_configs = {\n+        \"text_config\": AutoConfig,\n+        \"vision_config\": JanusVisionConfig,\n+        \"vq_config\": JanusVQVAEConfig,\n+    }\n+\n+    def __init__(self, text_config=None, vision_config=None, vq_config=None, **kwargs):\n+        if isinstance(text_config, dict):\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"llama\")\n+            self.text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n+\n+        elif text_config is None:\n+            logger.info(\"`text_config` is None. Initializing with default values\")\n+            self.text_config = CONFIG_MAPPING[\"llama\"]()\n+        elif isinstance(text_config, PretrainedConfig):\n+            self.text_config = text_config\n+        else:\n+            raise ValueError(\n+                f\"Invalid type for `text_config`. Must be either `dict` or `LlamaConfig`.\"\n+                f\" Type found: {type(text_config)}\"\n+            )\n+\n+        if vision_config is None:\n+            logger.info(\"`vision_config` is None. Initializing with default JanusVisionConfig values\")\n+            self.vision_config = JanusVisionConfig()\n+        elif isinstance(vision_config, dict):\n+            self.vision_config = JanusVisionConfig(**vision_config)\n+        elif isinstance(vision_config, JanusVisionConfig):\n+            self.vision_config = vision_config\n+        else:\n+            raise ValueError(\n+                f\"Invalid type for `vision_config`. Must be either `dict` or `JanusVisionConfig`.\"\n+                f\" Type found: {type(vision_config)}\"\n+            )\n+\n+        if vq_config is None:\n+            logger.info(\"`vq_config` is None. Initializing with default JanusVQVAEConfig values\")\n+            self.vq_config = JanusVQVAEConfig()\n+        elif isinstance(vq_config, dict):\n+            self.vq_config = JanusVQVAEConfig(**vq_config)\n+        elif isinstance(vq_config, JanusVQVAEConfig):\n+            self.vq_config = vq_config\n+        else:\n+            raise ValueError(\n+                f\"Invalid type for `vq_config`. Must be either `dict` or `JanusVQVAEConfig`.\"\n+                f\" Type found: {type(vq_config)}\"\n+            )\n+\n+        # This dimension is required when decoding discrete image tokens to continuous input.\n+        self.vq_config.num_patches = self.vision_config.image_size // self.vision_config.patch_size\n+        # The default is only the index for the 1B model, 7B uses a different one\n+        self.image_token_index = kwargs.get(\"image_token_index\", 100581)\n+        super().__init__(**kwargs)\n+\n+\n+JANUS_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`JanusConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare Janus Model outputting raw hidden-states without any specific head on top.\",\n+    JANUS_START_DOCSTRING,\n+)\n+class JanusPreTrainedModel(PreTrainedModel):\n+    config_class = JanusConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"LlamaDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_quantized_cache = True\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n+    _supports_param_buffer_assignment = False\n+\n+    def _init_weights(self, module):\n+        std = (\n+            self.config.vision_config.initializer_range\n+            if hasattr(self.config, \"vision_config\")\n+            else self.config.initializer_range\n+        )\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, (nn.GroupNorm, nn.LayerNorm)):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+\n+@dataclass\n+class JanusVQVAEOutput(ModelOutput):\n+    \"\"\"\n+    Base class for Janus VQ-VAE mode model outputs.\n+    Args:\n+        decoded_pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+            Reconstructed pixel values after encoding and decoding the input.\n+        embedding_loss (`torch.FloatTensor`):\n+            Embedding loss.\n+    \"\"\"\n+\n+    decoded_pixel_values: Optional[torch.FloatTensor] = None\n+    embedding_loss: torch.FloatTensor = None\n+\n+\n+@dataclass\n+class JanusBaseModelOutputWithPast(IdeficsBaseModelOutputWithPast):\n+    pass\n+\n+\n+@dataclass\n+class JanusCausalLMOutputWithPast(IdeficsCausalLMOutputWithPast):\n+    pass\n+\n+\n+class JanusVisionEmbeddings(SiglipVisionEmbeddings):\n+    def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n+        _, _, height, width = pixel_values.shape\n+        target_dtype = self.patch_embedding.weight.dtype\n+        patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]\n+        embeddings = patch_embeds.flatten(2).transpose(1, 2)\n+\n+        if interpolate_pos_encoding:\n+            pos_embeds = self.interpolate_pos_encoding(embeddings, height, width)\n+        else:\n+            pos_embeds = self.position_embedding(self.position_ids)\n+\n+        embeddings = embeddings + pos_embeds\n+\n+        return embeddings\n+\n+\n+class JanusVisionAttention(nn.Module):\n+    \"\"\"Attention Class for Janus Vision Encoder\"\"\"\n+\n+    def __init__(self, config: JanusVisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = self.embed_dim // self.num_heads\n+        if self.head_dim * self.num_heads != self.embed_dim:\n+            raise ValueError(\n+                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n+                f\" {self.num_heads}).\"\n+            )\n+        self.scale = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        proj_dropout = config.projection_dropout\n+        qk_norm = config.use_qk_norm\n+\n+        # Janus has no MHA, hence for `eager_attention_forward` call setting `num_key_value_groups` to 1.\n+        self.num_key_value_groups = 1\n+\n+        self.q_proj = nn.Linear(self.embed_dim, self.num_heads * self.head_dim, bias=config.attention_bias)\n+        self.k_proj = nn.Linear(self.embed_dim, self.num_heads * self.head_dim, bias=config.attention_bias)\n+        self.v_proj = nn.Linear(self.embed_dim, self.num_heads * self.head_dim, bias=config.attention_bias)\n+        self.projection_layer = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.projection_dropout = nn.Dropout(proj_dropout) if proj_dropout > 0 else nn.Identity()\n+\n+        self.q_norm = nn.LayerNorm(self.embed_dim) if qk_norm else nn.Identity()\n+        self.k_norm = nn.LayerNorm(self.embed_dim) if qk_norm else nn.Identity()\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ):\n+        batch_size, seq_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.reshape(-1, self.num_heads, self.head_dim)\n+        query_states = self.q_norm(query_states)\n+\n+        key_states = key_states.reshape(-1, self.num_heads, self.head_dim)\n+        key_states = self.k_norm(key_states)\n+\n+        query_states = query_states.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scale,\n+            is_causal=False,\n+            **kwargs,\n+        )\n+        attn_output = attn_output.reshape(batch_size, seq_len, self.embed_dim)\n+\n+        output = self.projection_layer(attn_output)\n+        output = self.projection_dropout(output)\n+\n+        outputs = (output, attn_weights) if output_attentions else (output, None)\n+        return outputs\n+\n+\n+class JanusVisionMLP(nn.Module):\n+    def __init__(self, config: JanusVisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.intermediate_size = int(config.hidden_size * config.mlp_ratio)\n+        self.activation_fn = ACT2FN[config.hidden_act]  # Gelu act\n+        self.fc1 = nn.Linear(config.hidden_size, self.intermediate_size)\n+        self.fc2 = nn.Linear(self.intermediate_size, config.hidden_size)\n+        self.dropout1 = nn.Dropout(config.hidden_dropout_rate)\n+        self.dropout2 = nn.Dropout(config.hidden_dropout_rate)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states = self.activation_fn(hidden_states)\n+        hidden_states = self.dropout1(hidden_states)\n+        hidden_states = self.fc2(hidden_states)\n+        hidden_states = self.dropout2(hidden_states)\n+        return hidden_states\n+\n+\n+class JanusVisionEncoderLayer(SiglipEncoderLayer):\n+    def __init__(self, config: JanusVisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.self_attn = JanusVisionAttention(config)\n+        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n+        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n+        self.mlp = JanusVisionMLP(config)\n+\n+\n+class JanusVisionEncoder(SiglipEncoder):\n+    def __init__(self, config: JanusVisionConfig):\n+        super().__init__(config)\n+        self.layers = nn.ModuleList([JanusVisionEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n+\n+\n+class JanusVisionModel(Blip2VisionModel):\n+    def __init__(self, config: JanusVisionConfig):\n+        super().__init__(config)\n+        self.encoder = JanusVisionEncoder(config)\n+\n+\n+class JanusVisionAlignerMLP(nn.Module):\n+    def __init__(self, config: JanusVisionConfig):\n+        super().__init__()\n+\n+        self.fc1 = nn.Linear(config.hidden_size, config.projection_dim)\n+        self.hidden_layers = nn.ModuleList(\n+            [nn.Linear(config.projection_dim, config.projection_dim) for _ in range(1, config.depth)]\n+        )\n+        self.activation_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.fc1(hidden_states)\n+        for layer in self.hidden_layers:\n+            hidden_states = self.activation_fn(hidden_states)\n+            hidden_states = layer(hidden_states)\n+        return hidden_states\n+\n+\n+class JanusVQVAEVectorQuantizer(ChameleonVQVAEVectorQuantizer):\n+    def __init__(self, config: JanusVQVAEConfig):\n+        super().__init__(config)\n+        self.quant_state_dims = [config.num_patches] * 2\n+\n+    def get_codebook_entry(self, image_tokens: torch.LongTensor) -> torch.FloatTensor:\n+        batch_size = image_tokens.shape[0]\n+        emb_dim: int = self.embedding.weight.shape[-1]\n+\n+        # get quantized latent vectors\n+        hidden_state_quant = self.embedding(image_tokens)\n+        # l2 normalization on the last dimension\n+        hidden_state_quant = F.normalize(hidden_state_quant, p=2, dim=-1)\n+\n+        # reshape back to match original input shape\n+        hidden_state_quant = hidden_state_quant.view((batch_size, *self.quant_state_dims, emb_dim))\n+        hidden_state_quant = hidden_state_quant.permute(0, 3, 1, 2).contiguous()\n+\n+        return hidden_state_quant\n+\n+\n+class JanusVQVAEResnetBlock(ChameleonVQVAEEncoderResnetBlock):\n+    pass\n+\n+\n+class JanusVQVAEAttnBlock(ChameleonVQVAEEncoderAttnBlock):\n+    pass\n+\n+\n+class JanusVQVAEConvDownsample(ChameleonVQVAEEncoderConvDownsample):\n+    pass\n+\n+\n+class JanusVQVAEConvUpsample(nn.Module):\n+    def __init__(self, in_channels):\n+        super().__init__()\n+        self.conv = torch.nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n+\n+    def forward(self, hidden_states):\n+        hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n+        hidden_states = self.conv(hidden_states)\n+        return hidden_states\n+\n+\n+class JanusVQVAEMidBlock(nn.Module):\n+    def __init__(self, config: JanusVQVAEConfig, channels: int):\n+        super().__init__()\n+        self.block_1 = JanusVQVAEResnetBlock(\n+            config=config,\n+            in_channels=channels,\n+            out_channels=channels,\n+        )\n+        self.attn_1 = JanusVQVAEAttnBlock(channels)\n+        self.block_2 = JanusVQVAEResnetBlock(\n+            config=config,\n+            in_channels=channels,\n+            out_channels=channels,\n+        )\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.block_1(hidden_states)\n+        hidden_states = self.attn_1(hidden_states)\n+        hidden_states = self.block_2(hidden_states)\n+        return hidden_states\n+\n+\n+class JanusVQVAEEncoder(ChameleonVQVAEEncoder, nn.Module):\n+    def __init__(self, config):\n+        nn.Module.__init__()\n+\n+        self.num_resolutions = len(config.channel_multiplier)\n+        self.num_res_blocks = config.num_res_blocks\n+        base_channels = config.base_channels\n+        in_channels = config.in_channels\n+        double_latent = config.double_latent\n+        latent_channels = config.latent_channels\n+        channel_multiplier = config.channel_multiplier\n+\n+        self.conv_in = torch.nn.Conv2d(in_channels, base_channels, kernel_size=3, stride=1, padding=1)\n+\n+        in_channel_multiplier = (1,) + tuple(channel_multiplier)\n+        self.in_channel_multiplier = in_channel_multiplier\n+        self.down = nn.ModuleList()\n+        for i_level in range(self.num_resolutions):\n+            block = nn.ModuleList()\n+            attn = nn.ModuleList()\n+            block_in = base_channels * in_channel_multiplier[i_level]\n+            block_out = base_channels * channel_multiplier[i_level]\n+            for i_block in range(self.num_res_blocks):\n+                block.append(\n+                    JanusVQVAEResnetBlock(\n+                        config=config,\n+                        in_channels=block_in,\n+                        out_channels=block_out,\n+                    )\n+                )\n+                block_in = block_out\n+                if i_level == self.num_resolutions - 1:\n+                    attn.append(JanusVQVAEAttnBlock(block_in))\n+\n+            down = nn.Module()\n+            down.block = block\n+            down.attn = attn\n+            if i_level != self.num_resolutions - 1:\n+                down.downsample = JanusVQVAEConvDownsample(block_in)\n+            self.down.append(down)\n+\n+        self.mid = JanusVQVAEMidBlock(config, block_in)\n+\n+        self.norm_out = torch.nn.GroupNorm(num_groups=32, num_channels=block_in, eps=1e-6, affine=True)\n+        self.conv_out = torch.nn.Conv2d(\n+            block_in,\n+            2 * latent_channels if double_latent else latent_channels,\n+            kernel_size=3,\n+            stride=1,\n+            padding=1,\n+        )\n+\n+    def forward(self, pixel_values: torch.LongTensor):\n+        # downsampling\n+        hidden_states = [self.conv_in(pixel_values)]\n+        for i_level in range(self.num_resolutions):\n+            for i_block in range(self.num_res_blocks):\n+                hidden_state = self.down[i_level].block[i_block](\n+                    hidden_states[-1],\n+                )\n+                if len(self.down[i_level].attn) > 0:\n+                    hidden_state = self.down[i_level].attn[i_block](hidden_state)\n+                hidden_states.append(hidden_state)\n+            if i_level != self.num_resolutions - 1:\n+                hidden_states.append(self.down[i_level].downsample(hidden_states[-1]))\n+\n+        # middle\n+        last_hidden_state = hidden_states[-1]\n+        last_hidden_state = self.mid(last_hidden_state)\n+\n+        # end\n+        last_hidden_state = self.norm_out(last_hidden_state)\n+        last_hidden_state *= torch.sigmoid(last_hidden_state)\n+        last_hidden_state = self.conv_out(last_hidden_state)\n+        return last_hidden_state\n+\n+\n+class JanusVQVAEDecoder(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+\n+        self.num_resolutions = len(config.channel_multiplier)\n+        self.num_res_blocks = config.num_res_blocks\n+        base_channels = config.base_channels\n+        latent_channels = config.latent_channels\n+        out_channels = config.out_channels\n+\n+        # compute in_ch_mult, block_in and curr_res at lowest res\n+        block_in = base_channels * config.channel_multiplier[self.num_resolutions - 1]\n+\n+        # z to block_in\n+        self.conv_in = torch.nn.Conv2d(latent_channels, block_in, kernel_size=3, stride=1, padding=1)\n+\n+        # middle\n+        self.mid = JanusVQVAEMidBlock(config, block_in)\n+\n+        # upsampling\n+        self.up = nn.ModuleList()\n+        for i_level in reversed(range(self.num_resolutions)):\n+            block = nn.ModuleList()\n+            attn = nn.ModuleList()\n+            block_out = base_channels * config.channel_multiplier[i_level]\n+            for i_block in range(self.num_res_blocks + 1):\n+                block.append(\n+                    JanusVQVAEResnetBlock(\n+                        config=config,\n+                        in_channels=block_in,\n+                        out_channels=block_out,\n+                    )\n+                )\n+                block_in = block_out\n+                if i_level == self.num_resolutions - 1:\n+                    attn.append(JanusVQVAEAttnBlock(block_in))\n+            up = nn.Module()\n+            up.block = block\n+            up.attn = attn\n+            if i_level != 0:\n+                up.upsample = JanusVQVAEConvUpsample(block_in)\n+            self.up.append(up)\n+\n+        # end\n+        self.norm_out = torch.nn.GroupNorm(num_groups=32, num_channels=block_in, eps=1e-6, affine=True)\n+        self.conv_out = torch.nn.Conv2d(block_in, out_channels, kernel_size=3, stride=1, padding=1)\n+\n+    def forward(self, hidden_state: torch.FloatTensor) -> torch.FloatTensor:\n+        hidden_state = self.conv_in(hidden_state)\n+\n+        # middle\n+        hidden_state = self.mid(hidden_state)\n+\n+        # upsampling\n+        for i_level in range(self.num_resolutions):\n+            for i_block in range(self.num_res_blocks + 1):\n+                hidden_state = self.up[i_level].block[i_block](hidden_state)\n+                if len(self.up[i_level].attn) > 0:\n+                    hidden_state = self.up[i_level].attn[i_block](hidden_state)\n+            if i_level != self.num_resolutions - 1:\n+                hidden_state = self.up[i_level].upsample(hidden_state)\n+\n+        hidden_state = self.norm_out(hidden_state)\n+        hidden_state *= torch.sigmoid(hidden_state)\n+        hidden_state = self.conv_out(hidden_state)\n+        return hidden_state\n+\n+\n+class JanusVQVAE(ChameleonVQVAE):\n+    \"\"\"Vision Transformer-based VQ-VAE model for encoding and decoding pixel values.\"\"\"\n+\n+    _no_split_modules = [\n+        \"JanusVQVAEAttnBlock\",\n+        \"JanusVQVAEResnetBlock\",\n+        \"JanusVQVAEVectorQuantizer\",\n+    ]\n+    main_input_name = \"pixel_values\"\n+\n+    def __init__(self, config: JanusVQVAEConfig):\n+        super().__init__(config)\n+        self.decoder = JanusVQVAEDecoder(config)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize the VQVAE model.\n+        self.post_init()\n+\n+    def decode(self, image_tokens: torch.LongTensor) -> torch.FloatTensor:\n+        \"\"\"\n+        Decodes quantized token IDs into pixel values.\n+        Args:\n+            image_tokens (torch.LongTensor): Batch of token IDs.\n+        Returns:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                Pixel values decoded from the token IDs.\n+        \"\"\"\n+        if image_tokens.shape[1] != self.quantize.quant_state_dims[0] * self.quantize.quant_state_dims[1]:\n+            raise ValueError(\n+                f\"Expected `image_tokens` to have shape `(batch_size, {self.quantize.quant_state_dims[0] * self.quantize.quant_state_dims[1]})`, \"\n+                f\"but got shape `{image_tokens.shape}`.\"\n+            )\n+        codebook_entry = self.quantize.get_codebook_entry(image_tokens)\n+        hidden_states = self.post_quant_conv(codebook_entry)\n+        pixel_values = self.decoder(hidden_states)\n+        return pixel_values\n+\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+    ) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n+        \"\"\"\n+        Encodes pixel values into quantized tokens and decodes them back.\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n+                The tensors corresponding to the input images.\n+        Returns:\n+            decoded_pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                Reconstructed pixel values after encoding and decoding the input.\n+            embedding_loss (`torch.FloatTensor`): Embedding loss.\n+        \"\"\"\n+\n+        batch_size = pixel_values.shape[0]\n+        quant, embedding_loss, indices = self.encode(pixel_values)\n+        decoded_pixel_values = self.decode(indices.view(batch_size, -1))\n+        output = JanusVQVAEOutput(decoded_pixel_values, embedding_loss)\n+\n+        return output\n+\n+\n+class JanusVQVAEAlignerMLP(nn.Module):\n+    def __init__(self, config: JanusVQVAEConfig):\n+        super().__init__()\n+\n+        self.fc1 = nn.Linear(config.embed_dim, config.projection_dim)\n+        self.hidden_layers = nn.ModuleList(\n+            [nn.Linear(config.projection_dim, config.projection_dim) for _ in range(1, config.num_hidden_layers)]\n+        )\n+        self.activation_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.fc1(hidden_states)\n+        for layer in self.hidden_layers:\n+            hidden_states = self.activation_fn(hidden_states)\n+            hidden_states = layer(hidden_states)\n+        return hidden_states\n+\n+\n+class JanusVQVAEHead(nn.Module):\n+    \"\"\"Head used for sampling tokens in image generation, replacing the usual lm head.\"\"\"\n+\n+    def __init__(self, config: JanusVQVAEConfig):\n+        super().__init__()\n+        self.proj_out = nn.Linear(config.image_token_embed_dim, config.projection_dim)\n+        self.activation_fn = ACT2FN[config.hidden_act]\n+        self.vision_head = nn.Linear(config.projection_dim, config.num_embeddings)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.tensor:\n+        hidden_states = self.proj_out(hidden_states)\n+        hidden_states = self.activation_fn(hidden_states)\n+        hidden_states = self.vision_head(hidden_states)\n+        return hidden_states\n+\n+\n+JANUS_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n+            The tensors corresponding to the input images. Pixel values can be obtained using\n+            [`AutoImageProcessor`].\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n+            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n+            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n+            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The Janus model which consists of a siglip vision backbone, a Llama language model and a VQ model.\"\"\",\n+    JANUS_START_DOCSTRING,\n+)\n+class JanusModel(JanusPreTrainedModel):\n+    def __init__(self, config: JanusConfig):\n+        super().__init__(config)\n+        self.config = config\n+        # This is necessary for backward compatibility, see SiglipModel initialization\n+        self.vision_model = JanusVisionModel._from_config(config.vision_config)\n+        self.aligner = JanusVisionAlignerMLP(self.vision_model.config)\n+\n+        self.vqmodel = JanusVQVAE._from_config(config.vq_config)\n+\n+        # Below generation_* modules are used for Image generation.\n+        # Embeddings used for image generation, instead of Janus vision embeddings.\n+        self.generation_embeddings = nn.Embedding(self.vqmodel.config.num_embeddings, self.vqmodel.config.embed_dim)\n+        self.generation_aligner = JanusVQVAEAlignerMLP(self.vqmodel.config)\n+        self.generation_head = JanusVQVAEHead(self.vqmodel.config)\n+\n+        self.language_model = AutoModel.from_config(config=config.text_config)\n+\n+        self.gradient_checkpointing = False\n+        # Initialize weights and apply final processing.\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def get_image_features(self, pixel_values):\n+        image_embeds = self.vision_model(pixel_values)\n+        image_embeds = self.aligner(image_embeds.last_hidden_state)\n+        return image_embeds\n+\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(JANUS_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n+    ):\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\n+                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        if pixel_values is not None and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_embeds = self.get_image_features(pixel_values)\n+            image_attention_mask = input_ids == self.config.image_token_index\n+\n+            embed_dim = inputs_embeds.shape[-1]\n+            image_features = image_embeds.reshape(-1, embed_dim)\n+            image_attention_mask = image_attention_mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n+\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(image_attention_mask, image_features)\n+\n+        lm_output = self.language_model(\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        output = JanusBaseModelOutputWithPast(\n+            last_hidden_state=lm_output.last_hidden_state,\n+            past_key_values=lm_output.past_key_values,\n+            hidden_states=lm_output.hidden_states,\n+            attentions=lm_output.attentions,\n+            image_hidden_states=image_embeds if pixel_values is not None else None,\n+        )\n+\n+        return output\n+\n+\n+class JanusForConditionalGeneration(JanusPreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"model.language_model.embed_tokens.weight\", \"lm_head.weight\"]\n+    _supports_static_cache = True\n+\n+    def __init__(self, config: JanusConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.model = JanusModel(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing.\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.language_model.set_input_embeddings(value)\n+\n+    def prepare_embeddings_for_image_generation(self, inputs: torch.Tensor) -> torch.Tensor:\n+        hidden_state = self.model.generation_embeddings(inputs)\n+        hidden_state = self.model.generation_aligner(hidden_state)\n+        return hidden_state\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    def set_decoder(self, decoder):\n+        self.model = decoder\n+\n+    def get_decoder(self):\n+        return self.model\n+\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(JANUS_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=JanusCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n+    ):\n+        r\"\"\"\n+        Args:\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n+\n+        Returns:\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+\n+        output = JanusCausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=outputs.image_hidden_states,\n+        )\n+        return output\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        pixel_values=None,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- extra custom processing\n+\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+        # Otherwise we need pixel values to be passed to model\n+        if cache_position[0] == 0:\n+            model_inputs[\"pixel_values\"] = pixel_values\n+\n+        return model_inputs\n+\n+    def decode_image_tokens(self, image_tokens: torch.Tensor):\n+        \"\"\"\n+        Decodes generated image tokens from language model to continuous pixel values\n+        with VQGAN module via upsampling.\n+        Args:\n+            image_tokens (`torch.LongTensor` of shape `(batch_size, num_of_tokens)`):\n+                The tensors corresponding to the input images.\n+        \"\"\"\n+        decoded_image = self.model.vqmodel.decode(image_tokens)\n+        decoded_image = decoded_image.permute(0, 2, 3, 1)\n+        return decoded_image\n+\n+    @torch.no_grad\n+    def generate(\n+        self,\n+        inputs: torch.Tensor = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        logits_processor: Optional[LogitsProcessorList] = None,\n+        **kwargs,\n+    ):\n+        # 1. Handle generation config and model kwargs\n+        generation_config = kwargs.pop(\"generation_config\", self.generation_config)\n+        generation_config = copy.deepcopy(generation_config)\n+\n+        # Default to \"text\" generation if mode isn't provided\n+        generation_mode = kwargs.pop(\"generation_mode\", \"text\")\n+        if generation_mode == \"text\":\n+            # Set guidance_scale=None to prevent running UnbatchedCFG processor.\n+            return super().generate(\n+                inputs=inputs,\n+                attention_mask=attention_mask,\n+                generation_config=generation_config,\n+                guidance_scale=None,\n+                **kwargs,\n+            )\n+\n+        model_kwargs = generation_config.update(**kwargs)  # All unused kwargs must be model kwargs\n+\n+        # Validate generation mode\n+        if generation_config.get_generation_mode() not in (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n+            raise ValueError(\n+                \"Got incompatible mode for Image Generation, should be one of greedy or sampling. \"\n+                \"Ensure that beam search is de-activated by setting `num_beams=1` and `num_beam_groups=1`.\"\n+            )\n+\n+        # Validate the configuration and model kwargs\n+        generation_config.validate()\n+        self._validate_model_kwargs(model_kwargs.copy())\n+\n+        # 2. Initialize logit processors\n+        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n+\n+        # Set `use_cache=True` as we will be using input embeds for generation.\n+        model_kwargs[\"use_cache\"] = True\n+\n+        if generation_config.guidance_scale is None:\n+            logger.warning(\"`guidance_scale` is required for CFG but not provided. Setting to default value of 5.\")\n+            generation_config.guidance_scale = 5\n+        model_kwargs[\"guidance_scale\"] = generation_config.guidance_scale\n+\n+        # 3. Prepare model inputs\n+        input_ids, model_input_name, model_kwargs = self._prepare_model_inputs(\n+            inputs, generation_config.bos_token_id, model_kwargs\n+        )\n+        dtype, device = input_ids.dtype, input_ids.device\n+\n+        if len(input_ids.shape) != 2:\n+            raise ValueError(\n+                f\"Expected input ids of shape (batch_size, seq_len), but got {input_ids.shape}\"\n+                \"Passing `inputs embeds` is not supported currently.\"\n+            )\n+\n+        # Prepare special tokens which will be used generate internally.\n+        kwargs_has_attention_mask = attention_mask is not None\n+        self._prepare_special_tokens(generation_config, kwargs_has_attention_mask, device=input_ids.device)\n+\n+        # 4. Add CFG processor along with user passed logit processor.\n+        if generation_config.guidance_scale and generation_config.guidance_scale > 1:\n+            logits_processor.append(ClassifierFreeGuidanceLogitsProcessor(generation_config.guidance_scale))\n+            generation_config.guidance_scale = None  # Reset to prevent processor duplication.\n+\n+        # 5. Prepare logits processor\n+        logits_processor = self._get_logits_processor(\n+            generation_config=generation_config,\n+            input_ids_seq_length=input_ids.shape[1],\n+            encoder_input_ids=input_ids,\n+            prefix_allowed_tokens_fn=None,\n+            logits_processor=logits_processor,\n+            device=device,\n+        )\n+\n+        # 6. Expand inputs for multiple image generations per prompt.\n+        input_ids, model_kwargs = self._expand_inputs_for_generation(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            expand_size=generation_config.num_return_sequences,\n+            **model_kwargs,\n+        )\n+\n+        # 7. Prepare input and model caches\n+        num_image_tokens = self.model.vision_model.config.num_image_tokens\n+        batch_size, seq_len = input_ids.shape\n+\n+        input_tokens = input_ids.repeat(2, 1)  # Double batch size for conditional/unconditional logits\n+        attention_mask = model_kwargs.pop(\"attention_mask\", None)\n+        attention_mask = attention_mask.repeat(2, 1)\n+        model_kwargs[\"attention_mask\"] = attention_mask\n+\n+        # Mask all the tokens that are neither BOS nor BOI with pad token in the unconditional logits.\n+        mask = (input_tokens[batch_size:, :] != generation_config.bos_token_id) & (\n+            input_tokens[batch_size:, :] != generation_config.generation_kwargs[\"boi_token_id\"]\n+        )\n+        input_tokens[batch_size:, :].masked_fill_(mask, generation_config.pad_token_id)\n+\n+        inputs_embeds = self.get_input_embeddings()(input_tokens)\n+\n+        model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n+\n+        if model_kwargs.get(\"past_key_values\", None) is None:\n+            # Prepare cache if not provided.\n+            model_kwargs[\"past_key_values\"] = self._get_cache(\n+                cache_implementation=generation_config.cache_implementation or \"static\",\n+                # batch_size should account for both conditional/unconditional input; hence multiplied by 2.\n+                batch_size=batch_size * 2,\n+                # we should have at least a cache len of seq_len + num_image_tokens.\n+                max_cache_len=max(generation_config.max_length, num_image_tokens + seq_len),\n+                device=device,\n+                model_kwargs=model_kwargs,\n+            )\n+\n+        # Placeholder for generated tokens.\n+        generated_tokens = torch.zeros((batch_size, num_image_tokens), dtype=dtype, device=device)\n+\n+        # 8. init attention / hidden states / scores tuples\n+        output_attentions = generation_config.output_attentions\n+        output_hidden_states = generation_config.output_hidden_states\n+        output_scores = generation_config.output_scores\n+        output_logits = generation_config.output_logits\n+        return_dict_in_generate = generation_config.return_dict_in_generate\n+\n+        raw_scores = () if (return_dict_in_generate and output_scores) else None\n+        raw_logits = () if (return_dict_in_generate and output_logits) else None\n+        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n+        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n+\n+        for i in range(num_image_tokens):\n+            model_inputs = self.prepare_inputs_for_generation(\n+                inputs_embeds=inputs_embeds, input_ids=input_tokens, **model_kwargs\n+            )\n+\n+            model_inputs[\"attention_mask\"] = model_inputs[\"attention_mask\"].to(inputs_embeds.device)\n+            model_inputs[\"cache_position\"] = model_inputs[\"cache_position\"].to(inputs_embeds.device)\n+\n+            outputs = self.model.language_model(\n+                **model_inputs,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+            )\n+\n+            # Update model_kwargs like cache_position for next generation.\n+            model_kwargs = self._update_model_kwargs_for_generation(outputs, model_kwargs)\n+            hidden_state = outputs.last_hidden_state[:, -1, :].clone()\n+\n+            # Generate scores using the generation head (Not using above defined LM Head)\n+            scores = self.model.generation_head(hidden_state)\n+            next_token_scores = logits_processor(input_ids, scores)\n+\n+            # Sample next token.\n+            if generation_config.do_sample:\n+                probs = torch.softmax(next_token_scores, dim=-1)\n+                next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n+            else:\n+                next_token = torch.argmax(next_token_scores, dim=-1)\n+\n+            generated_tokens[:, i] = next_token\n+\n+            # Prepare embeddings for the next step.\n+            next_token = torch.cat([next_token, next_token])\n+            next_token = next_token.unsqueeze(-1)\n+\n+            inputs_embeds = self.prepare_embeddings_for_image_generation(next_token)\n+\n+        if return_dict_in_generate:\n+            if output_scores:\n+                raw_scores += (scores,)\n+            if output_logits:\n+                raw_logits += (hidden_state.float(),)\n+            if output_attentions:\n+                decoder_attentions += outputs.attentions\n+            if output_hidden_states:\n+                decoder_hidden_states += outputs.hidden_states\n+\n+        if return_dict_in_generate:\n+            return GenerateDecoderOnlyOutput(\n+                sequences=generated_tokens,\n+                scores=scores,\n+                logits=raw_logits,\n+                attentions=decoder_attentions,\n+                hidden_states=decoder_hidden_states,\n+                past_key_values=outputs.past_key_values,\n+            )\n+        else:\n+            return generated_tokens\n+\n+\n+class JanusImageProcessor(BlipImageProcessor):\n+    r\"\"\"\n+    Constructs a JANUS image processor.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by the\n+            `do_resize` parameter in the `preprocess` method.\n+        size (`dict`, *optional*, defaults to `{\"height\": 384, \"width\": 384}`):\n+            Size of the output image after resizing. Can be overridden by the `size` parameter in the `preprocess`\n+            method.\n+        min_size (`int`, *optional*, defaults to 14):\n+            The minimum allowed size for the resized image. Ensures that neither the height nor width\n+            falls below this value after resizing.\n+        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n+            Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`. Can be\n+            overridden by the `resample` parameter in the `preprocess` method.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by the\n+            `do_rescale` parameter in the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Only has an effect if `do_rescale` is set to `True`. Can be\n+            overridden by the `rescale_factor` parameter in the `preprocess` method.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`\n+            method. Can be overridden by the `do_normalize` parameter in the `preprocess` method.\n+        image_mean (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n+            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n+            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method. Can be\n+            overridden by the `image_mean` parameter in the `preprocess` method.\n+        image_std (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n+            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n+            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n+            Can be overridden by the `image_std` parameter in the `preprocess` method.\n+        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n+            Whether to convert the image to RGB.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        size: Dict[str, int] = None,\n+        min_size: int = 14,\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n+        do_normalize: bool = True,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_convert_rgb: bool = None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.min_size = min_size\n+        if image_mean is None:\n+            self.background_color = (127, 127, 127)\n+        else:\n+            self.background_color = tuple([int(x * 255) for x in image_mean])\n+\n+    def pad_to_square(\n+        self,\n+        image: np.ndarray,\n+        background_color: Union[int, Tuple[int, int, int]] = 0,\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> np.array:\n+        \"\"\"\n+        Pads an image to a square based on the longest edge.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                The image to pad.\n+            background_color (`int` or `Tuple[int, int, int]`, *optional*, defaults to 0):\n+                The color to use for the padding. Can be an integer for single channel or a\n+                tuple of integers representing for multi-channel images. If passed as integer\n+                in mutli-channel mode, it will default to `0` in subsequent channels.\n+            data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the output image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                If unset, will use same as the input image.\n+            input_data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the input image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+\n+        Returns:\n+            `np.ndarray`: The padded image.\n+        \"\"\"\n+        height, width = get_image_size(image, input_data_format)\n+        num_channels = image.shape[0] if input_data_format == ChannelDimension.FIRST else image.shape[-1]\n+\n+        if height == width:\n+            image = (\n+                to_channel_dimension_format(image, data_format, input_data_format)\n+                if data_format is not None\n+                else image\n+            )\n+            return image\n+\n+        max_dim = max(height, width)\n+\n+        # Ensure background_color is the correct shape\n+        if isinstance(background_color, int):\n+            background_color = [background_color]\n+        elif len(background_color) != num_channels:\n+            raise ValueError(\n+                f\"background_color must have no more than {num_channels} elements to match the number of channels\"\n+            )\n+\n+        if input_data_format == ChannelDimension.FIRST:\n+            result = np.zeros((num_channels, max_dim, max_dim), dtype=image.dtype)\n+            for i, color in enumerate(background_color):\n+                result[i, :, :] = color\n+            if width > height:\n+                start = (max_dim - height) // 2\n+                result[:, start : start + height, :] = image\n+            else:\n+                start = (max_dim - width) // 2\n+                result[:, :, start : start + width] = image\n+        else:\n+            result = np.zeros((max_dim, max_dim, num_channels), dtype=image.dtype)\n+            for i, color in enumerate(background_color):\n+                result[:, :, i] = color\n+            if width > height:\n+                start = (max_dim - height) // 2\n+                result[start : start + height, :, :] = image\n+            else:\n+                start = (max_dim - width) // 2\n+                result[:, start : start + width, :] = image\n+\n+        return result\n+\n+    def resize(\n+        self,\n+        image: np.ndarray,\n+        size: Union[Dict[str, int], int],\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        **kwargs,\n+    ) -> np.ndarray:\n+        \"\"\"\n+        Resize an image to dynamically calculated size.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                Image to resize.\n+            resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n+                `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BICUBIC`.\n+            data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the output image. If unset, the channel dimension format of the input\n+                image is used. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `None`: will be inferred from input\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+\n+        Returns:\n+            `np.ndarray`: The resized image.\n+        \"\"\"\n+        if input_data_format is None:\n+            input_data_format = infer_channel_dimension_format(image)\n+\n+        height, width = get_image_size(image, input_data_format)\n+        max_size = max(height, width)\n+\n+        size = get_size_dict(size, default_to_square=True)\n+        if size[\"height\"] != size[\"width\"]:\n+            raise ValueError(\n+                f\"Output height and width must be the same. Got height={size['height']} and width={size['width']}\"\n+            )\n+        size = size[\"height\"]\n+\n+        delta = size / max_size\n+        # Largest side becomes `size` and the other side is scaled according to the aspect ratio.\n+        output_size_nonpadded = [\n+            max(int(height * delta), self.min_size),\n+            max(int(width * delta), self.min_size),\n+        ]\n+\n+        image = resize(\n+            image,\n+            size=output_size_nonpadded,\n+            resample=resample,\n+            data_format=data_format,\n+            input_data_format=input_data_format,\n+            **kwargs,\n+        )\n+        # Expand and pad the images to obtain a square image of dimensions `size x size`\n+        image = self.pad_to_square(\n+            image=image,\n+            background_color=self.background_color,\n+            input_data_format=input_data_format,\n+        )\n+        return image\n+\n+    def postprocess(\n+        self,\n+        images: ImageInput,\n+        do_rescale: bool = None,\n+        rescale_factor: float = None,\n+        do_normalize: bool = None,\n+        image_mean: List[float] = None,\n+        image_std: List[float] = None,\n+        input_data_format: str = None,\n+        return_tensors: str = None,\n+    ):\n+        \"\"\"Applies post-processing to the decoded image tokens by reversing transformations applied during preprocessing.\"\"\"\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = 1.0 / self.rescale_factor if rescale_factor is None else rescale_factor\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+\n+        images = make_list_of_images(images)  # Ensures input is a list\n+\n+        if isinstance(images[0], PIL.Image.Image):\n+            return images if len(images) > 1 else images[0]\n+\n+        if input_data_format is None:\n+            input_data_format = infer_channel_dimension_format(images[0])  # Determine format dynamically\n+\n+        pixel_values = []\n+\n+        for image in images:\n+            image = to_numpy_array(image)  # Ensure NumPy format\n+\n+            if do_normalize:\n+                image = self.unnormalize(\n+                    image=image, image_mean=image_mean, image_std=image_std, input_data_format=input_data_format\n+                )\n+\n+            if do_rescale:\n+                image = self.rescale(image, scale=rescale_factor, input_data_format=input_data_format)\n+                image = image.clip(0, 255).astype(np.uint8)\n+\n+            if do_normalize and do_rescale and return_tensors == \"PIL.Image.Image\":\n+                image = to_channel_dimension_format(image, ChannelDimension.LAST, input_channel_dim=input_data_format)\n+                image = PIL.Image.fromarray(image)\n+\n+            pixel_values.append(image)\n+\n+        data = {\"pixel_values\": pixel_values}\n+        return_tensors = return_tensors if return_tensors != \"PIL.Image.Image\" else None\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+    def unnormalize(\n+        self,\n+        image: np.array,\n+        image_mean: Union[float, Iterable[float]],\n+        image_std: Union[float, Iterable[float]],\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> np.array:\n+        \"\"\"\n+        Unnormalizes `image` using the mean and standard deviation specified by `mean` and `std`.\n+        image = (image * image_std) + image_mean\n+        Args:\n+            image (`torch.Tensor` of shape `(batch_size, num_channels, image_size, image_size)` or `(num_channels, image_size, image_size)`):\n+                Batch of pixel values to postprocess.\n+            image_mean (`float` or `Iterable[float]`):\n+                The mean to use for unnormalization.\n+            image_std (`float` or `Iterable[float]`):\n+                The standard deviation to use for unnormalization.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        \"\"\"\n+        num_channels = 3\n+\n+        if isinstance(image_mean, Iterable):\n+            if len(image_mean) != num_channels:\n+                raise ValueError(f\"mean must have {num_channels} elements if it is an iterable, got {len(image_mean)}\")\n+        else:\n+            image_mean = [image_mean] * num_channels\n+\n+        if isinstance(image_std, Iterable):\n+            if len(image_std) != num_channels:\n+                raise ValueError(f\"std must have {num_channels} elements if it is an iterable, got {len(image_std)}\")\n+        else:\n+            image_std = [image_std] * num_channels\n+\n+        rev_image_mean = tuple(-mean / std for mean, std in zip(image_mean, image_std))\n+        rev_image_std = tuple(1 / std for std in image_std)\n+        image = self.normalize(\n+            image=image, mean=rev_image_mean, std=rev_image_std, input_data_format=input_data_format\n+        )\n+        return image\n+\n+\n+__all__ = [\n+    \"JanusImageProcessor\",\n+    \"JanusPreTrainedModel\",\n+    \"JanusForConditionalGeneration\",\n+    \"JanusModel\",\n+    \"JanusVQVAE\",\n+    \"JanusVisionModel\",\n+    \"JanusVQVAEConfig\",\n+    \"JanusVisionConfig\",\n+    \"JanusConfig\",\n+]"
        },
        {
            "sha": "4132ca8f43d6918175c083f7e98fa7fe6438cdf4",
            "filename": "src/transformers/models/janus/processing_janus.py",
            "status": "added",
            "additions": 188,
            "deletions": 0,
            "changes": 188,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py?ref=a2ef3cf5379d35da68e58c54e7981f7998ab8f1b",
            "patch": "@@ -0,0 +1,188 @@\n+# coding=utf-8\n+# Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"\n+Processor class for Janus.\n+\"\"\"\n+\n+from typing import List, Union\n+\n+from ...feature_extraction_utils import BatchFeature\n+from ...image_utils import ImageInput\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n+from ...tokenization_utils_base import (\n+    PreTokenizedInput,\n+    TextInput,\n+)\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+DEFAULT_SYSTEM_PROMPT = (\n+    \"You are a helpful language and vision assistant. \"\n+    \"You are able to understand the visual content that the user provides, \"\n+    \"and assist the user with a variety of tasks using natural language.\\n\\n\"\n+)\n+\n+\n+class JanusTextKwargs(TextKwargs, total=False):\n+    generation_mode: str\n+\n+\n+class JanusProcessorKwargs(ProcessingKwargs, total=False):\n+    text_kwargs: JanusTextKwargs\n+    _defaults = {\n+        \"text_kwargs\": {\"padding\": False, \"generation_mode\": \"text\"},\n+        \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+    }\n+\n+\n+class JanusProcessor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a Janus processor which wraps a Janus Image Processor and a Llama tokenizer into a single processor.\n+\n+    [`JanusProcessor`] offers all the functionalities of [`JanusImageProcessor`] and [`LlamaTokenizerFast`]. See the\n+    [`~JanusProcessor.__call__`] and [`~JanusProcessor.decode`] for more information.\n+\n+    Args:\n+        image_processor ([`JanusImageProcessor`]):\n+            The image processor is a required input.\n+        tokenizer ([`LlamaTokenizerFast`]):\n+            The tokenizer is a required input.\n+        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n+            in a chat into a tokenizable string.\n+        use_default_system_prompt (`str`, *optional*, defaults to `False`):\n+            Use default system prompt for Text Generation.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"tokenizer\"]\n+    valid_kwargs = [\"chat_template\", \"use_default_system_prompt\"]\n+    image_processor_class = \"JanusImageProcessor\"\n+    tokenizer_class = \"LlamaTokenizerFast\"\n+\n+    def __init__(self, image_processor, tokenizer, chat_template=None, use_default_system_prompt=False, **kwargs):\n+        self.num_image_tokens = 576\n+        self.image_token = tokenizer.image_token\n+        self.image_start_token = tokenizer.boi_token\n+        self.image_end_token = tokenizer.eoi_token\n+        self.use_default_system_prompt = use_default_system_prompt\n+\n+        super().__init__(image_processor, tokenizer, chat_template=chat_template)\n+\n+    def __call__(\n+        self,\n+        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        images: ImageInput = None,\n+        videos=None,\n+        audio=None,\n+        **kwargs: Unpack[JanusProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n+        and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n+        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        JanusImageProcessor's [`~JanusImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        of the above two methods for more information.\n+\n+        Args:\n+            text (`str`, `List[str]`, `List[List[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+                - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+              `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+        \"\"\"\n+\n+        output_kwargs = self._merge_kwargs(\n+            JanusProcessorKwargs, tokenizer_init_kwargs=self.tokenizer.init_kwargs, **kwargs\n+        )\n+\n+        if text is None and images is None:\n+            raise ValueError(\"You must specify either text or images.\")\n+\n+        if text is not None:\n+            if isinstance(text, str):\n+                text = [text]\n+            elif not (isinstance(text, (list, tuple)) and all(isinstance(t, str) for t in text)):\n+                raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+\n+        generation_mode = output_kwargs[\"text_kwargs\"].pop(\"generation_mode\")\n+\n+        # Replace the image token with expanded image tokens.\n+        prompt_strings = []\n+        one_img_tokens = self.image_start_token + (self.image_token * self.num_image_tokens) + self.image_end_token\n+        for prompt in text:\n+            prompt = prompt.replace(self.image_token, one_img_tokens)\n+            if self.use_default_system_prompt and generation_mode == \"text\":\n+                prompt = DEFAULT_SYSTEM_PROMPT + prompt\n+            if generation_mode == \"image\":\n+                prompt += self.image_start_token\n+            prompt_strings.append(prompt)\n+\n+        data = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n+\n+        # Process images if pixel values are provided.\n+        if images is not None and generation_mode != \"image\":\n+            data[\"pixel_values\"] = self.image_processor(images=images, **output_kwargs[\"images_kwargs\"])[\n+                \"pixel_values\"\n+            ]\n+\n+        return BatchFeature(data=data)\n+\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    def postprocess(self, images: ImageInput, **kwargs):\n+        \"\"\"\n+        Forwards all arguments to the image processor's `postprocess` method.\n+        Refer to the original method's docstring for more details.\n+        \"\"\"\n+        return self.image_processor.postprocess(images, **kwargs)\n+\n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+\n+\n+__all__ = [\"JanusProcessor\"]"
        },
        {
            "sha": "0672589769ad4f553a7b410a90e98a6441dbdef5",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=a2ef3cf5379d35da68e58c54e7981f7998ab8f1b",
            "patch": "@@ -126,6 +126,7 @@\n     \"qwen2vl\",\n     \"qwen2_5_vl\",\n     \"ayavision\",\n+    \"janus\",\n     \"gemma3\",\n     \"mistral3\",\n     \"chameleon\","
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/janus/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/tests%2Fmodels%2Fjanus%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/tests%2Fmodels%2Fjanus%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjanus%2F__init__.py?ref=a2ef3cf5379d35da68e58c54e7981f7998ab8f1b"
        },
        {
            "sha": "184f669e6a5817734da4741cd5adc6dc5a5d1ba9",
            "filename": "tests/models/janus/test_image_processing_janus.py",
            "status": "added",
            "additions": 188,
            "deletions": 0,
            "changes": 188,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/tests%2Fmodels%2Fjanus%2Ftest_image_processing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/tests%2Fmodels%2Fjanus%2Ftest_image_processing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjanus%2Ftest_image_processing_janus.py?ref=a2ef3cf5379d35da68e58c54e7981f7998ab8f1b",
            "patch": "@@ -0,0 +1,188 @@\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import numpy as np\n+\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import JanusImageProcessor\n+\n+\n+class JanusImageProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        image_size=384,\n+        min_resolution=30,\n+        max_resolution=200,\n+        do_resize=True,\n+        size=None,\n+        do_normalize=True,\n+        image_mean=[1.0, 1.0, 1.0],\n+        image_std=[1.0, 1.0, 1.0],\n+        do_convert_rgb=True,\n+    ):\n+        size = size if size is not None else {\"height\": 384, \"width\": 384}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_convert_rgb = do_convert_rgb\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_convert_rgb\": self.do_convert_rgb,\n+        }\n+\n+    # Copied from tests.models.clip.test_image_processing_clip.CLIPImageProcessingTester.prepare_image_inputs\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        return prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+\n+\n+@require_torch\n+@require_vision\n+class JanusImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    image_processing_class = JanusImageProcessor if is_vision_available() else None\n+\n+    # Copied from tests.models.clip.test_image_processing_clip.CLIPImageProcessingTest.setUp with CLIP->Janus\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = JanusImageProcessingTester(self)\n+\n+    @property\n+    # Copied from tests.models.clip.test_image_processing_clip.CLIPImageProcessingTest.image_processor_dict\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    def test_image_processor_properties(self):\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+        self.assertTrue(hasattr(image_processing, \"size\"))\n+        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+        self.assertTrue(hasattr(image_processing, \"image_std\"))\n+        self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n+\n+    def test_image_processor_from_dict_with_kwargs(self):\n+        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n+        self.assertEqual(image_processor.size, {\"height\": 384, \"width\": 384})\n+        self.assertEqual(image_processor.image_mean, [1.0, 1.0, 1.0])\n+\n+        image_processor = self.image_processing_class.from_dict(\n+            self.image_processor_dict, size=42, image_mean=[1.0, 2.0, 1.0]\n+        )\n+        self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+        self.assertEqual(image_processor.image_mean, [1.0, 2.0, 1.0])\n+\n+    def test_call_pil(self):\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n+        for image in image_inputs:\n+            self.assertIsInstance(image, Image.Image)\n+\n+        # Test Non batched input\n+        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = (1, 3, 384, 384)\n+        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+        # Test batched\n+        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = (7, 3, 384, 384)\n+        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+    def test_call_numpy(self):\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, numpify=True)\n+        for image in image_inputs:\n+            self.assertIsInstance(image, np.ndarray)\n+\n+        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = (1, 3, 384, 384)\n+        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = (7, 3, 384, 384)\n+        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+    def test_call_pytorch(self):\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n+\n+        for image in image_inputs:\n+            self.assertIsInstance(image, torch.Tensor)\n+\n+        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = (1, 3, 384, 384)\n+        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = (7, 3, 384, 384)\n+        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+    def test_nested_input(self):\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n+\n+        # Test batched as a list of images.\n+        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = (7, 3, 384, 384)\n+        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+        # Test batched as a nested list of images, where each sublist is one batch.\n+        image_inputs_nested = [image_inputs[:3], image_inputs[3:]]\n+        encoded_images_nested = image_processing(image_inputs_nested, return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = (7, 3, 384, 384)\n+        self.assertEqual(tuple(encoded_images_nested.shape), expected_output_image_shape)\n+\n+        # Image processor should return same pixel values, independently of input format.\n+        self.assertTrue((encoded_images_nested == encoded_images).all())\n+\n+    @unittest.skip(reason=\"Not supported\")\n+    def test_call_numpy_4_channels(self):\n+        pass"
        },
        {
            "sha": "03208f388e840996474787fb90b39a070924bdd2",
            "filename": "tests/models/janus/test_modeling_janus.py",
            "status": "added",
            "additions": 553,
            "deletions": 0,
            "changes": 553,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py?ref=a2ef3cf5379d35da68e58c54e7981f7998ab8f1b",
            "patch": "@@ -0,0 +1,553 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Janus model.\"\"\"\n+\n+import re\n+import tempfile\n+import unittest\n+from functools import reduce\n+\n+import numpy as np\n+import requests\n+\n+from transformers import (\n+    AutoProcessor,\n+    JanusConfig,\n+    JanusForConditionalGeneration,\n+    JanusModel,\n+    JanusVQVAE,\n+    JanusVQVAEConfig,\n+    is_torch_available,\n+    is_vision_available,\n+)\n+from transformers.models.auto import get_values\n+from transformers.models.auto.modeling_auto import MODEL_FOR_BACKBONE_MAPPING_NAMES, MODEL_MAPPING_NAMES\n+from transformers.testing_utils import (\n+    require_torch,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+\n+class JanusVisionText2TextModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        image_token_index=0,\n+        seq_length=25,\n+        initializer_range=0.02,\n+        text_config={\n+            \"model_type\": \"llama\",\n+            \"seq_length\": 7,\n+            \"is_training\": True,\n+            \"use_input_mask\": True,\n+            \"use_token_type_ids\": False,\n+            \"use_labels\": True,\n+            \"vocab_size\": 99,\n+            \"hidden_size\": 32,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 4,\n+            \"intermediate_size\": 37,\n+            \"hidden_act\": \"gelu\",\n+            \"hidden_dropout_prob\": 0.1,\n+            \"attention_probs_dropout_prob\": 0.1,\n+            \"max_position_embeddings\": 512,\n+            \"type_vocab_size\": 16,\n+            \"type_sequence_label_size\": 2,\n+            \"initializer_range\": 0.02,\n+            \"num_labels\": 3,\n+            \"num_choices\": 4,\n+            \"pad_token_id\": 1,\n+        },\n+        is_training=True,\n+        vision_config={\n+            \"use_labels\": True,\n+            \"image_size\": 20,\n+            \"patch_size\": 5,\n+            \"num_image_tokens\": 4,\n+            \"num_channels\": 3,\n+            \"is_training\": True,\n+            \"hidden_size\": 32,\n+            \"projection_dim\": 32,\n+            \"num_key_value_heads\": 1,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 4,\n+            \"mlp_ratio\": 2,\n+            \"dropout\": 0.1,\n+            \"attention_dropout\": 0.1,\n+            \"initializer_range\": 0.02,\n+            \"vision_feature_select_strategy\": \"default\",\n+            \"vision_feature_layer\": -1,\n+        },\n+        use_cache=False,\n+        vq_num_embeds=12,\n+        vq_embed_dim=12,\n+        vq_channel_multiplier=[1, 1],\n+    ):\n+        self.parent = parent\n+        self.initializer_range = initializer_range\n+        # `image_token_index` is set to 0 to pass \"resize_embeddings\" test, do not modify\n+        self.image_token_index = image_token_index\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.seq_length = seq_length\n+        self.pad_token_id = text_config[\"pad_token_id\"]\n+\n+        self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n+        self.vocab_size = text_config[\"vocab_size\"]\n+        self.hidden_size = text_config[\"hidden_size\"]\n+        self.num_attention_heads = text_config[\"num_attention_heads\"]\n+        self.is_training = is_training\n+\n+        self.batch_size = 3\n+        self.num_channels = vision_config[\"num_channels\"]\n+        self.image_size = vision_config[\"image_size\"]\n+        self.num_image_tokens = vision_config[\"num_image_tokens\"]\n+        self.use_cache = use_cache\n+\n+        # vq model params\n+        self.vq_num_embeds = vq_num_embeds\n+        self.vq_embed_dim = vq_embed_dim\n+        self.vq_channel_multiplier = vq_channel_multiplier\n+\n+    def get_vq_config(self):\n+        return {\n+            \"embed_dim\": self.vq_embed_dim,\n+            \"num_embeddings\": self.vq_num_embeds,\n+            \"latent_channels\": self.vq_embed_dim,\n+            \"in_channels\": 3,\n+            \"base_channels\": 32,  # we have a GroupNorm of 32 groups, so can't do less\n+            \"channel_multiplier\": self.vq_channel_multiplier,\n+            \"initializer_range\": self.initializer_range,\n+            \"projection_dim\": 10,\n+            \"image_token_embed_dim\": 32,  # Same as text model hidden size\n+        }\n+\n+    def get_config(self):\n+        return JanusConfig(\n+            text_config=self.text_config,\n+            vision_config=self.vision_config,\n+            vq_config=self.get_vq_config(),\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        config = self.get_config()\n+        pixel_values = floats_tensor(\n+            [\n+                self.batch_size,\n+                3,\n+                self.image_size,\n+                self.image_size,\n+            ]\n+        )\n+        return config, pixel_values\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size - 1) + 1\n+        attention_mask = input_ids.ne(self.pad_token_id).to(torch_device)\n+\n+        # set the 16 first tokens to be image, and ensure that no other tokens are image tokens\n+        # do not change this unless you modified image size or patch size\n+        input_ids[input_ids == self.image_token_index] = self.pad_token_id\n+        input_ids[:, : self.num_image_tokens] = self.image_token_index\n+        inputs_dict = {\n+            \"pixel_values\": pixel_values,\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+            \"labels\": input_ids,\n+            \"generation_mode\": \"text\",  # Required to perform text generation instead of image generation.\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class JanusVisionText2TextModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+    all_model_classes = (JanusModel, JanusForConditionalGeneration) if is_torch_available() else ()\n+    all_generative_model_classes = (JanusForConditionalGeneration,) if is_torch_available() else ()\n+    fx_compatible = False\n+    test_pruning = False\n+    test_head_masking = False\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = JanusVisionText2TextModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=JanusConfig, has_text_modality=False)\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n+    def test_inputs_embeds(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+            del inputs[\"generation_mode\"]\n+\n+            wte = model.get_input_embeddings()\n+            inputs[\"inputs_embeds\"] = wte(input_ids)\n+\n+            with torch.no_grad():\n+                model(**inputs)\n+\n+    # Overwrite inputs_embeds tests because we need to delete \"pixel values\" for VLMs.\n+    def test_inputs_embeds_matches_input_ids(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+            del inputs[\"generation_mode\"]\n+\n+            inputs_embeds = model.get_input_embeddings()(input_ids)\n+\n+            with torch.no_grad():\n+                out_ids = model(input_ids=input_ids, **inputs)[0]\n+                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n+            torch.testing.assert_close(out_embeds, out_ids)\n+\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+\n+                # Load the model with SDPA\n+                model_sdpa = model_class.from_pretrained(tmpdirname)\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                # Load model with eager attention\n+                model_eager = model_class.from_pretrained(\n+                    tmpdirname,\n+                    attn_implementation=\"eager\",\n+                )\n+                model_eager = model_eager.eval().to(torch_device)\n+\n+            # SigLip has one shared cls attr for all models, so we assign both submodels heer\n+            vision_attn = language_attn = \"sdpa\" if model._supports_sdpa else \"eager\"\n+\n+            if hasattr(model_sdpa, \"vision_model\") and hasattr(model_sdpa, \"language_model\"):\n+                self.assertTrue(model_sdpa.vision_model.config._attn_implementation == vision_attn)\n+                self.assertTrue(model_sdpa.language_model.config._attn_implementation == language_attn)\n+                self.assertTrue(model_eager.vision_model.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.language_model.config._attn_implementation == \"eager\")\n+\n+            self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+            self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+\n+            for name, submodule in model_eager.named_modules():\n+                class_name = submodule.__class__.__name__\n+                if any(re.finditer(r\"Attention(?!Pool)\", class_name)):\n+                    self.assertTrue(submodule.config._attn_implementation == \"eager\")\n+\n+            for name, submodule in model_sdpa.named_modules():\n+                class_name = submodule.__class__.__name__\n+                if any(re.finditer(r\"Attention(?!Pool)\", class_name)):\n+                    self.assertTrue(submodule.config._attn_implementation == \"sdpa\")\n+\n+    def check_training_gradient_checkpointing(self, gradient_checkpointing_kwargs=None):\n+        if not self.model_tester.is_training:\n+            self.skipTest(reason=\"ModelTester is not configured to run training tests\")\n+        \"\"\"\n+        We skip some parameters when checking for gradient checkpointing:\n+        - VQ model, as its training is not supported.\n+        - A few other modules used for image generation.\n+        \"\"\"\n+        skip_patterns = [\"vqmodel\", \"generation_embeddings\", \"generation_aligner\", \"generation_head\"]\n+\n+        for model_class in self.all_model_classes:\n+            with self.subTest(model_class.__name__):\n+                if (\n+                    model_class.__name__\n+                    in [\n+                        *get_values(MODEL_MAPPING_NAMES),\n+                        *get_values(MODEL_FOR_BACKBONE_MAPPING_NAMES),\n+                    ]\n+                    or not model_class.supports_gradient_checkpointing\n+                ):\n+                    # TODO (ydshieh): use `skipTest` once pytest-dev/pytest-subtests/pull/169 is merged\n+                    # self.skipTest(reason=f\"`supports_gradient_checkpointing` is False for {model_class.__name__}.\")\n+                    continue\n+\n+                config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+                config.use_cache = False\n+                config.return_dict = True\n+                model = model_class(config)\n+\n+                model.to(torch_device)\n+                model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n+                model.train()\n+\n+                # unfreeze additional layers\n+                for p in model.parameters():\n+                    p.requires_grad_(True)\n+\n+                optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n+\n+                inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+                loss = model(**inputs).loss\n+                loss.backward()\n+                optimizer.step()\n+\n+                if self.test_all_params_have_gradient:\n+                    for k, v in model.named_parameters():\n+                        if v.requires_grad and not reduce(lambda t, s: t | (s in k), skip_patterns, False):\n+                            self.assertTrue(v.grad is not None, f\"{k} in {model_class.__name__} has no gradient!\")\n+                        else:\n+                            pass\n+\n+\n+class JanusVQModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=5,\n+        is_training=False,\n+        initializer_range=0.02,\n+        image_size=30,\n+        num_embeds=12,\n+        base_channels=32,  # we have a GroupNorm of 32 groups, so can't do less\n+        embed_dim=12,\n+        channel_multiplier=[1, 2],\n+        patch_size=2,\n+        scope=None,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.is_training = is_training\n+        self.initializer_range = initializer_range\n+        self.image_size = image_size\n+        self.base_channels = base_channels\n+        self.num_embeds = num_embeds\n+        self.embed_dim = embed_dim\n+        self.channel_multiplier = channel_multiplier\n+        self.num_patches = image_size // patch_size\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, 3, self.image_size, self.image_size])\n+        config = self.get_config()\n+        return config, pixel_values\n+\n+    def get_config(self):\n+        return JanusVQVAEConfig(\n+            embed_dim=self.embed_dim,\n+            num_embeddings=self.num_embeds,\n+            latent_channels=self.embed_dim,\n+            in_channels=3,\n+            base_channels=self.base_channels,\n+            channel_multiplier=self.channel_multiplier,\n+            initializer_range=self.initializer_range,\n+            resolution=self.image_size,\n+            num_patches=self.num_patches,\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class JanusVQModelTest(ModelTesterMixin, unittest.TestCase):\n+    all_model_classes = (JanusVQVAE,) if is_torch_available() else ()\n+    test_head_masking = False\n+    test_pruning = False\n+    fx_compatible = False\n+    has_attentions = False\n+    test_resize_embeddings = False\n+\n+    def setUp(self):\n+        self.model_tester = JanusVQModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self,\n+            config_class=JanusVQVAEConfig,\n+            has_text_modality=False,\n+            common_properties=[\"embed_dim\", \"num_embeddings\"],\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    @unittest.skip(\"Janus VQ module cannot offload due to using `self.weight` directly\")\n+    def test_cpu_offload(self):\n+        pass\n+\n+    @unittest.skip(\"Janus VQ module cannot offload due to using `self.weight` directly\")\n+    def test_disk_offload_bin(self):\n+        pass\n+\n+    @unittest.skip(\"Janus VQ module cannot offload due to using `self.weight` directly\")\n+    def test_disk_offload_safetensors(self):\n+        pass\n+\n+    @unittest.skip(\"Janus VQ module has no hidden states\")\n+    def test_hidden_states_output(self):\n+        pass\n+\n+    @unittest.skip(\"Janus VQ module has no hidden states\")\n+    def test_model_outputs_equivalence(self):\n+        pass\n+\n+    @unittest.skip(\"Janus VQ module has no get/set embeddings method\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(\"Janus VQ module has no hidden states\")\n+    def test_retain_grad_hidden_states_attentions(self):\n+        pass\n+\n+\n+class JanusIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        self.model_id = \"deepseek-community/Janus-Pro-1B\"\n+\n+    @slow\n+    def test_model_text_generation(self):\n+        model = JanusForConditionalGeneration.from_pretrained(self.model_id, device_map=\"auto\")\n+        model.eval()\n+        processor = AutoProcessor.from_pretrained(self.model_id)\n+        image = Image.open(\n+            requests.get(\"https://nineplanets.org/wp-content/uploads/2020/12/the-big-dipper-1.jpg\", stream=True).raw\n+        )\n+        prompt = \"<image_placeholder>\\nDescribe what do you see here and tell me about the history behind it?\"\n+        inputs = processor(images=image, text=prompt, generation_mode=\"text\", return_tensors=\"pt\").to(model.device)\n+\n+        output = model.generate(**inputs, max_new_tokens=20, generation_mode=\"text\", do_sample=False)\n+        EXPECTED_DECODED_TEXT = 'You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\\n\\n\\nDescribe what do you see here and tell me about the history behind it?\\n\\nThe image depicts the constellation of Leo, which is often referred to as the \"Lion\"'  # fmt: skip\n+        text = processor.decode(output[0], skip_special_tokens=True)\n+        self.assertEqual(\n+            text,\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    @slow\n+    def test_model_text_generation_batched(self):\n+        model = JanusForConditionalGeneration.from_pretrained(self.model_id, device_map=\"auto\")\n+        processor = AutoProcessor.from_pretrained(self.model_id)\n+\n+        image_1 = Image.open(\n+            requests.get(\"https://nineplanets.org/wp-content/uploads/2020/12/the-big-dipper-1.jpg\", stream=True).raw\n+        )\n+        image_2 = Image.open(\n+            requests.get(\"https://www.kxan.com/wp-content/uploads/sites/40/2020/10/ORION.jpg\", stream=True).raw\n+        )\n+        prompts = [\n+            \"<image_placeholder>\\nDescribe what do you see here and tell me about the history behind it?\",\n+            \"What constellation is this image showing?<image_placeholder>\\n\",\n+        ]\n+\n+        inputs = processor(\n+            images=[image_1, image_2], text=prompts, generation_mode=\"text\", padding=True, return_tensors=\"pt\"\n+        ).to(model.device, torch.float16)\n+\n+        EXPECTED_TEXT_COMPLETION = [\n+            'You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\\n\\n\\nDescribe what do you see here and tell me about the history behind it?\\n\\nThe image depicts the constellation of Leo, which is often referred to as the \"Lion\"',\n+            \"You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\\n\\nWhat constellation is this image showing?\\n\\nThe image shows a constellation that is shaped like a stylized figure with a long tail. This\",\n+        ]\n+        generated_ids = model.generate(**inputs, max_new_tokens=20, generation_mode=\"text\", do_sample=False)\n+        text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n+\n+    @slow\n+    def test_model_text_generation_with_multi_image(self):\n+        model = JanusForConditionalGeneration.from_pretrained(self.model_id, device_map=\"auto\")\n+        processor = AutoProcessor.from_pretrained(self.model_id)\n+\n+        image_1 = Image.open(\n+            requests.get(\"https://nineplanets.org/wp-content/uploads/2020/12/the-big-dipper-1.jpg\", stream=True).raw\n+        )\n+        image_2 = Image.open(\n+            requests.get(\"https://www.kxan.com/wp-content/uploads/sites/40/2020/10/ORION.jpg\", stream=True).raw\n+        )\n+        prompt = \"What do these two images <image_placeholder> and <image_placeholder> have in common?\"\n+\n+        inputs = processor(images=[image_1, image_2], text=prompt, generation_mode=\"text\", return_tensors=\"pt\").to(\n+            model.device, torch.float16\n+        )\n+\n+        EXPECTED_TEXT_COMPLETION = ['You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\\n\\nWhat do these two images  and  have in common?\\n\\nThe two images you provided are of the same constellation. The first image shows the constellation of Leo, and the second image shows the constellation of Ursa Major. Both constellations are part of']  # fmt: skip\n+        generated_ids = model.generate(**inputs, max_new_tokens=40, do_sample=False)\n+        text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n+\n+    @slow\n+    def test_model_generate_images(self):\n+        model = JanusForConditionalGeneration.from_pretrained(self.model_id, device_map=\"auto\")\n+        processor = AutoProcessor.from_pretrained(self.model_id)\n+\n+        inputs = processor(\n+            text=[\"A portrait of young girl. masterpiece, film grained, best quality.\"],\n+            padding=True,\n+            generation_mode=\"image\",\n+            return_tensors=\"pt\",\n+        ).to(model.device)\n+\n+        self.assertTrue(inputs.input_ids.shape[1] == 17)\n+\n+        out = model.generate(\n+            **inputs,\n+            generation_mode=\"image\",\n+            do_sample=False,\n+        )\n+\n+        # It should run for num_image_tokens in this case 576.\n+        self.assertTrue(out.shape[1] == 576)\n+\n+        # fmt: off\n+        expected_tokens = torch.tensor([4484,  4015, 15750,   506,  3758, 11651,  8597,  5739,  4861,   971,\n+         14985, 14834, 15438,  7548,  1820,  1465, 13529, 12761, 10503, 12761,\n+         14303,  6155,  4015, 11766,   705, 15736, 14146, 10417,  1951,  7713,\n+         14305, 15617,  6169,  2706,  8006, 14893,  3855, 10188, 15652,  6297,\n+          1097, 12108, 15038,   311, 14998, 15165,   897,  4044,  1762,  4676,\n+        ]).to(model.device)\n+        # fmt: on\n+\n+        # Compare the first 50 generated tokens.\n+        self.assertTrue(torch.allclose(expected_tokens, out[0][:50]))\n+\n+        # Decode generated tokens to pixel values and postprocess them.\n+        decoded_pixel_values = model.decode_image_tokens(out)\n+        images = processor.postprocess(list(decoded_pixel_values.float()), return_tensors=\"np\")\n+\n+        self.assertTrue(images[\"pixel_values\"].shape == (1, 384, 384, 3))\n+        self.assertTrue(isinstance(images[\"pixel_values\"], np.ndarray))"
        },
        {
            "sha": "8b664bb7432f8eaec36dce55744d58f45c58e19a",
            "filename": "tests/models/janus/test_processor_janus.py",
            "status": "added",
            "additions": 455,
            "deletions": 0,
            "changes": 455,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/tests%2Fmodels%2Fjanus%2Ftest_processor_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/tests%2Fmodels%2Fjanus%2Ftest_processor_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjanus%2Ftest_processor_janus.py?ref=a2ef3cf5379d35da68e58c54e7981f7998ab8f1b",
            "patch": "@@ -0,0 +1,455 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Janus model.\"\"\"\n+\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+\n+from transformers import AutoProcessor, AutoTokenizer, JanusProcessor\n+from transformers.models.janus.convert_janus_weights_to_hf import CHAT_TEMPLATE\n+from transformers.utils import is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_vision_available():\n+    pass\n+\n+\n+class JanusProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = JanusProcessor\n+\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+        special_image_tokens = {\n+            \"image_token\": \"<image_placeholder>\",\n+            \"boi_token\": \"<begin_of_image>\",\n+            \"eoi_token\": \"<end_of_image>\",\n+        }\n+\n+        processor = self.processor_class.from_pretrained(\n+            \"deepseek-community/Janus-Pro-1B\",\n+            extra_special_tokens=special_image_tokens,\n+        )\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    def prepare_processor_dict(self):\n+        # similar to Emu3 and Qwen2VLProcessorTest, but keep the template in the convert script to avoid duplicated code\n+        return {\n+            \"chat_template\": CHAT_TEMPLATE,\n+        }\n+\n+    def test_chat_template_single(self):\n+        \"\"\"\n+        Tests that the chat template matches the original implementation when applied to a single message.\n+        \"\"\"\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        # Single image message\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                        {\"type\": \"image\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        correct_prompt = [\"<|User|>: What is shown in this image?\\n<image_placeholder>\\n\\n<|Assistant|>:\"]\n+\n+        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n+        self.assertEqual(formatted_prompt, correct_prompt)\n+\n+        # Single image message with capitalization\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"User\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                        {\"type\": \"image\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        correct_prompt = [\"<|User|>: What is shown in this image?\\n<image_placeholder>\\n\\n<|Assistant|>:\"]\n+\n+        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n+        self.assertEqual(formatted_prompt, correct_prompt)\n+\n+        # Single image message with uppercase\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"USER\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                        {\"type\": \"image\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        correct_prompt = [\"<|User|>: What is shown in this image?\\n<image_placeholder>\\n\\n<|Assistant|>:\"]\n+\n+        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n+        self.assertEqual(formatted_prompt, correct_prompt)\n+\n+        \"\"\"\n+        Warning: normally, the other models have a test comparing chat template+tokenization as two separate steps\n+        versus as a single step (i.e. processor.apply_chat_template(..., tokenize=True)). However, our processor has\n+        some extra steps other than simply applying prompt to tokenizer. These include prepending the default system\n+        prompts and, following the implementation from the Janus codebase, expanding the image token.\n+        \"\"\"\n+\n+        # Checking the output dict keys\n+        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n+        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n+\n+        # Now test the ability to return dict\n+        messages[0][0][\"content\"][1].update(\n+            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n+        )\n+        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n+        self.assertTrue(self.images_input_name in out_dict)\n+        # should always have input_ids and attention_mask\n+        self.assertEqual(len(out_dict[\"input_ids\"]), 1)\n+        self.assertEqual(len(out_dict[\"attention_mask\"]), 1)\n+        self.assertEqual(len(out_dict[self.images_input_name]), 1)\n+\n+        # Passing generation prompt explicitly\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                        {\"type\": \"image\"},\n+                    ],\n+                },\n+                {\n+                    \"role\": \"assistant\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=False)\n+        self.assertEqual(formatted_prompt, correct_prompt)\n+\n+        # Single prompt with multiple images\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"Compare this image\"},\n+                        {\"type\": \"image\"},\n+                        {\"type\": \"text\", \"text\": \"with this image\"},\n+                        {\"type\": \"image\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        correct_prompt = [\n+            \"<|User|>: Compare this image\\n<image_placeholder>\\nwith this image\\n<image_placeholder>\\n\\n<|Assistant|>:\"\n+        ]\n+        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n+        self.assertEqual(formatted_prompt, correct_prompt)\n+\n+        # Multiple turns and multiple images\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"Compare this image\"},\n+                        {\"type\": \"image\"},\n+                        {\"type\": \"text\", \"text\": \"with this image\"},\n+                        {\"type\": \"image\"},\n+                    ],\n+                },\n+                {\n+                    \"role\": \"assistant\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"The first image is an equation, the second is a pie chart.\"},\n+                    ],\n+                },\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"image\"},\n+                        {\n+                            \"type\": \"text\",\n+                            \"text\": \"What about this third image? To which of the previous to is it more similar?\",\n+                        },\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        correct_prompt = [\n+            \"<|User|>: Compare this image\\n<image_placeholder>\\nwith this image\\n<image_placeholder>\\n\\n<|Assistant|>: The first image is an equation, the second is a pie chart.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><|User|>: <image_placeholder>\\nWhat about this third image? To which of the previous to is it more similar?\\n\\n<|Assistant|>:\"\n+        ]\n+        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n+        self.assertEqual(formatted_prompt, correct_prompt)\n+\n+    def test_chat_template_batched(self):\n+        \"\"\"\n+        Tests that the chat template matches the original implementation when applied to a batch of messages.\n+        \"\"\"\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        # Test 1: Simple single image per message batch\n+        batched_messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                        {\"type\": \"image\"},\n+                    ],\n+                },\n+            ],\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                        {\"type\": \"image\"},\n+                    ],\n+                },\n+            ],\n+        ]\n+\n+        correct_prompts = [\n+            \"<|User|>: What is shown in this image?\\n<image_placeholder>\\n\\n<|Assistant|>:\",\n+            \"<|User|>: What is shown in this image?\\n<image_placeholder>\\n\\n<|Assistant|>:\",\n+        ]\n+\n+        formatted_prompts = processor.apply_chat_template(batched_messages, add_generation_prompt=True)\n+        self.assertEqual(formatted_prompts, correct_prompts)\n+\n+        # Similarly to the single case, no test for chat template+tokenization as two separate steps versus as a single step\n+\n+        # Checking the output dict keys\n+        out_dict = processor.apply_chat_template(\n+            batched_messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            padding=True,\n+        )\n+        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n+\n+        # Verify image inputs are included in the output dict\n+        batched_messages[0][0][\"content\"][1].update(\n+            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n+        )\n+        batched_messages[1][0][\"content\"][1].update(\n+            {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"}\n+        )\n+        out_dict = processor.apply_chat_template(\n+            batched_messages, add_generation_prompt=True, tokenize=True, return_dict=True, padding=True\n+        )\n+        self.assertTrue(self.images_input_name in out_dict)\n+        self.assertEqual(len(out_dict[\"input_ids\"]), 2)  # Batch size for text\n+        self.assertEqual(len(out_dict[\"attention_mask\"]), 2)  # Batch size for attention mask\n+        self.assertEqual(len(out_dict[self.images_input_name]), 2)  # Batch size for images\n+\n+        # Test 2: Two images per message batch with different prompts\n+        batched_messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"Compare this image\"},\n+                        {\"type\": \"image\"},\n+                        {\"type\": \"text\", \"text\": \"with this image\"},\n+                        {\"type\": \"image\"},\n+                    ],\n+                },\n+            ],\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"image\"},\n+                        {\"type\": \"text\", \"text\": \"Describe how the previous image compares to the following\"},\n+                        {\"type\": \"image\"},\n+                    ],\n+                },\n+            ],\n+        ]\n+\n+        correct_prompts = [\n+            \"<|User|>: Compare this image\\n<image_placeholder>\\nwith this image\\n<image_placeholder>\\n\\n<|Assistant|>:\",\n+            \"<|User|>: <image_placeholder>\\nDescribe how the previous image compares to the following\\n<image_placeholder>\\n\\n<|Assistant|>:\",\n+        ]\n+        formatted_prompts = processor.apply_chat_template(batched_messages, add_generation_prompt=True)\n+        self.assertEqual(formatted_prompts, correct_prompts)\n+\n+        # Test 3: Multi-turn conversations with multiple images\n+        batched_messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"Compare this image\"},\n+                        {\"type\": \"image\"},\n+                        {\"type\": \"text\", \"text\": \"with this image\"},\n+                        {\"type\": \"image\"},\n+                    ],\n+                },\n+                {\n+                    \"role\": \"assistant\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"The first image is an equation, the second is a pie chart.\"},\n+                    ],\n+                },\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"image\"},\n+                        {\n+                            \"type\": \"text\",\n+                            \"text\": \"What about this third image? To which of the previous to is it more similar?\",\n+                        },\n+                    ],\n+                },\n+            ],\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"image\"},\n+                        {\"type\": \"text\", \"text\": \"Describe how the previous image compares to the following\"},\n+                        {\"type\": \"image\"},\n+                    ],\n+                },\n+                {\n+                    \"role\": \"assistant\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"The first image is a formula, the second is a plot.\"},\n+                    ],\n+                },\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"Which of them is closer to the following?\"},\n+                        {\"type\": \"image\"},\n+                    ],\n+                },\n+            ],\n+        ]\n+\n+        correct_prompts = [\n+            \"<|User|>: Compare this image\\n<image_placeholder>\\nwith this image\\n<image_placeholder>\\n\\n<|Assistant|>: The first image is an equation, the second is a pie chart.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><|User|>: <image_placeholder>\\nWhat about this third image? To which of the previous to is it more similar?\\n\\n<|Assistant|>:\",\n+            \"<|User|>: <image_placeholder>\\nDescribe how the previous image compares to the following\\n<image_placeholder>\\n\\n<|Assistant|>: The first image is a formula, the second is a plot.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><|User|>: Which of them is closer to the following?\\n<image_placeholder>\\n\\n<|Assistant|>:\",\n+        ]\n+        formatted_prompts = processor.apply_chat_template(batched_messages, add_generation_prompt=True)\n+        self.assertEqual(formatted_prompts, correct_prompts)\n+\n+    def test_chat_template_accepts_processing_kwargs(self):\n+        \"\"\"Tests that the chat template correctly handles additional processing arguments.\"\"\"\n+        # Get processor and skip if it doesn't have a chat template\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        # Create a simple text message for testing\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        # Test 1: Padding to max_length\n+        # PS: we have to override the parent max_length of 50 to 80 because the output is already 51 tokens\n+        formatted_prompt_tokenized = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            padding=\"max_length\",\n+            max_length=80,\n+        )\n+        self.assertEqual(len(formatted_prompt_tokenized[0]), 80)\n+\n+        # Test 2: Truncation\n+        # Verify that the output is truncated to exactly 5 tokens\n+        formatted_prompt_tokenized = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            truncation=True,\n+            max_length=5,\n+        )\n+        self.assertEqual(len(formatted_prompt_tokenized[0]), 5)\n+\n+        # Test 3: Image processing kwargs\n+        # Add an image and test image processing parameters\n+        messages[0][0][\"content\"].append(\n+            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n+        )\n+        # Process with image rescaling and verify the pixel values are negative\n+        out_dict = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            do_rescale=True,\n+            rescale_factor=-1,\n+            return_tensors=\"np\",\n+        )\n+        self.assertLessEqual(out_dict[self.images_input_name][0][0].mean(), 0)\n+\n+    def test_processor_postprocess(self):\n+        processor_components = self.prepare_components()\n+        processor = self.processor_class(**processor_components)\n+\n+        input_str = \"lower newer\"\n+        orig_image_input = self.prepare_image_inputs()\n+        orig_image = np.array(orig_image_input).transpose(2, 0, 1)\n+\n+        inputs = processor(text=input_str, images=orig_image, do_resize=False, return_tensors=\"np\")\n+        normalized_image_input = inputs.pixel_values\n+        unnormalized_images = processor.postprocess(normalized_image_input, return_tensors=\"np\")[\"pixel_values\"]\n+\n+        # For an image where pixels go from 0 to 255 the diff can be 1 due to some numerical precision errors when scaling and unscaling\n+        self.assertTrue(np.abs(orig_image - unnormalized_images).max() >= 1)"
        },
        {
            "sha": "95dac3d6b76776df6ae00d63de268e770a4f8027",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2ef3cf5379d35da68e58c54e7981f7998ab8f1b/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=a2ef3cf5379d35da68e58c54e7981f7998ab8f1b",
            "patch": "@@ -156,6 +156,7 @@\n         \"Llama4VisionModel\",  # Building part of bigger (tested) model. # TODO: add tests\n         \"Emu3VQVAE\",  # Building part of bigger (tested) model\n         \"Emu3TextModel\",  # Building part of bigger (tested) model\n+        \"JanusVisionModel\",  # Building part of bigger (tested) model\n         \"TimesFmModel\",  # Building part of bigger (tested) model\n     ]\n )\n@@ -356,6 +357,8 @@\n     \"MoshiForConditionalGeneration\",  # no auto class for speech-to-speech\n     \"Emu3VQVAE\",  # no autoclass for VQ-VAE models\n     \"Emu3TextModel\",  # Building part of bigger (tested) model\n+    \"JanusVQVAE\",  # no autoclass for VQ-VAE models\n+    \"JanusVisionModel\",  # Building part of bigger (tested) model\n     \"Qwen2_5OmniTalkerForConditionalGeneration\",  # Building part of a bigger model\n     \"Qwen2_5OmniTalkerModel\",  # Building part of a bigger model\n     \"Qwen2_5OmniThinkerForConditionalGeneration\",  # Building part of a bigger model"
        }
    ],
    "stats": {
        "total": 6412,
        "additions": 6411,
        "deletions": 1
    }
}