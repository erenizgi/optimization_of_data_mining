{
    "author": "guangy10",
    "message": "Export T5 (encoder-decoder) to ExecuTorch (#36486)\n\nCo-authored-by: Guang Yang <guangyang@fb.com>",
    "sha": "3b07ca78bb696825feee3e976795fab58f2b6d0c",
    "files": [
        {
            "sha": "09fd0c387fd4310cabf3269a76647ec0a2056470",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 179,
            "deletions": 0,
            "changes": 179,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b07ca78bb696825feee3e976795fab58f2b6d0c/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b07ca78bb696825feee3e976795fab58f2b6d0c/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=3b07ca78bb696825feee3e976795fab58f2b6d0c",
            "patch": "@@ -12,6 +12,8 @@\n \n import torch\n \n+from transformers.generation.configuration_utils import GenerationConfig\n+\n from ..utils.import_utils import is_torch_available\n \n \n@@ -216,3 +218,180 @@ def convert_and_export_with_cache(\n             strict=True,\n         )\n         return exported_program\n+\n+\n+class Seq2SeqLMEncoderExportableModule(torch.nn.Module):\n+    \"\"\"\n+    A wrapper module designed to make a Seq2Seq LM encoder exportable with `torch.export`.\n+    This module ensures that the exported encoder model is compatible with ExecuTorch.\n+    \"\"\"\n+\n+    def __init__(self, encoder_model):\n+        super().__init__()\n+        self.encoder = encoder_model\n+\n+    def forward(self, input_ids):\n+        return self.encoder(input_ids=input_ids).last_hidden_state\n+\n+\n+class Seq2SeqLMDecoderExportableModuleWithStaticCache(torch.nn.Module):\n+    \"\"\"\n+    A wrapper module designed to make a Seq2Seq LM decoder exportable with `torch.export`,\n+    specifically for use with static caching. This module ensures the exported decoder\n+    is compatible with ExecuTorch.\n+    \"\"\"\n+\n+    def __init__(self, model, max_static_cache_length, batch_size):\n+        super().__init__()\n+\n+        # Get the decoder component\n+        self.decoder = model.get_decoder()\n+        self.lm_head = model.lm_head\n+        self.config = model.config\n+\n+        # Initialize static cache\n+        self.static_cache = StaticCache(\n+            config=self.config,\n+            max_batch_size=batch_size,\n+            max_cache_len=max_static_cache_length,\n+            device=\"cpu\",\n+            dtype=torch.float32,\n+        )\n+\n+        # Register cache buffers to make them exportable\n+        for i in range(len(self.static_cache.key_cache)):\n+            self.register_buffer(f\"key_cache_{i}\", self.static_cache.key_cache[i], persistent=False)\n+            self.register_buffer(f\"value_cache_{i}\", self.static_cache.value_cache[i], persistent=False)\n+\n+    def forward(self, decoder_input_ids, encoder_hidden_states, cache_position):\n+        # Get outputs from decoder\n+        outputs = self.decoder(\n+            input_ids=decoder_input_ids,\n+            encoder_hidden_states=encoder_hidden_states,\n+            past_key_values=self.static_cache,\n+            use_cache=True,\n+            cache_position=cache_position,\n+        )\n+\n+        # Apply language model head\n+        lm_logits = self.lm_head(outputs[0])\n+\n+        return lm_logits\n+\n+\n+class Seq2SeqLMExportableModule(torch.nn.Module):\n+    def __init__(\n+        self, model, batch_size=1, max_hidden_seq_length=4096, cache_implementation=\"static\", max_cache_length=1024\n+    ):\n+        super().__init__()\n+\n+        self.full_model = model\n+        self.encoder = model.get_encoder()\n+        self.config = model.config\n+        self.max_hidden_seq_length = max_hidden_seq_length\n+        self.generation_config = GenerationConfig(\n+            use_cache=True,\n+            max_length=max_cache_length,\n+            cache_implementation=cache_implementation,\n+            cache_config={\n+                \"batch_size\": batch_size,\n+                \"max_cache_len\": max_cache_length,\n+            },\n+        )\n+        self.exported_encoder = None\n+        self.exported_decoder = None\n+\n+    def _export_encoder(self, encoder_input_ids):\n+        wrapped_encoder = Seq2SeqLMEncoderExportableModule(self.encoder).to(\"cpu\").eval()\n+\n+        # Define dynamic sequence length for encoder\n+        seq_len_dim = torch.export.Dim(\"encoder_seq_length\", max=self.max_hidden_seq_length)\n+\n+        # Export the encoder\n+        with torch.no_grad():\n+            exported_encoder = torch.export.export(\n+                wrapped_encoder, (encoder_input_ids,), dynamic_shapes={\"input_ids\": {1: seq_len_dim}}, strict=True\n+            )\n+\n+        return exported_encoder\n+\n+    def _export_decoder(self, decoder_input_ids, encoder_hidden_states, cache_position):\n+        wrapped_decoder = (\n+            Seq2SeqLMDecoderExportableModuleWithStaticCache(\n+                model=self.full_model,\n+                max_static_cache_length=self.generation_config.cache_config.max_cache_len,\n+                batch_size=self.generation_config.cache_config.batch_size,\n+            )\n+            .to(\"cpu\")\n+            .eval()\n+        )\n+\n+        # Define dynamic dimension for encoder output sequence length\n+        encoder_seq_len_dim = torch.export.Dim(\"encoder_hidden_seq_length\", max=self.max_hidden_seq_length)\n+\n+        # Export the decoder\n+        with torch.no_grad():\n+            exported_decoder = torch.export.export(\n+                wrapped_decoder,\n+                (decoder_input_ids, encoder_hidden_states, cache_position),\n+                dynamic_shapes={\n+                    \"decoder_input_ids\": None,\n+                    \"encoder_hidden_states\": {1: encoder_seq_len_dim},\n+                    \"cache_position\": None,\n+                },\n+                strict=True,\n+            )\n+\n+        return exported_decoder\n+\n+    def export(self, encoder_input_ids=None, decoder_input_ids=None, encoder_hidden_states=None, cache_position=None):\n+        example_encoder_input_ids = (\n+            encoder_input_ids if encoder_input_ids is not None else torch.ones((1, 10), dtype=torch.long)\n+        )\n+        example_decoder_input_ids = (\n+            decoder_input_ids if decoder_input_ids is not None else torch.tensor([[0]], dtype=torch.long)\n+        )  # Start token\n+        example_cache_position = cache_position if cache_position is not None else torch.tensor([0], dtype=torch.long)\n+        example_encoder_hidden_states = (\n+            encoder_hidden_states\n+            if encoder_hidden_states is not None\n+            else torch.zeros(\n+                (self.generation_config.cache_config.batch_size, 10, self.config.d_model), dtype=torch.float32\n+            )\n+        )\n+        self.exported_encoder = self._export_encoder(example_encoder_input_ids)\n+        self.exported_decoder = self._export_decoder(\n+            example_decoder_input_ids, example_encoder_hidden_states, example_cache_position\n+        )\n+\n+        # Return self to allow chaining\n+        return self\n+\n+    def generate(self, prompt_token_ids, max_new_tokens):\n+        with torch.no_grad():\n+            # Run encoder\n+            encoder_output = self.exported_encoder.module()(prompt_token_ids)\n+\n+            # Initialize with start token (0 for T5)\n+            decoder_input_ids = torch.tensor([[0]], dtype=torch.long)\n+            generated_ids = [0]\n+\n+            # Generate tokens one by one\n+            for i in range(max_new_tokens - 1):\n+                # Run decoder for next token prediction\n+                logits = self.exported_decoder.module()(\n+                    decoder_input_ids, encoder_output, torch.tensor([i], dtype=torch.long)\n+                )\n+\n+                # Get next token\n+                next_token = torch.argmax(logits[:, -1, :], dim=-1).item()\n+                generated_ids.append(next_token)\n+\n+                # Update input for next iteration\n+                decoder_input_ids = torch.tensor([[next_token]], dtype=torch.long)\n+\n+                # Check if EOS token\n+                if next_token == self.config.eos_token_id:\n+                    break\n+\n+            return generated_ids"
        },
        {
            "sha": "48fe5e8942a594fa439ac215bb9bd536c04fa627",
            "filename": "tests/models/t5/test_modeling_t5.py",
            "status": "modified",
            "additions": 145,
            "deletions": 0,
            "changes": 145,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b07ca78bb696825feee3e976795fab58f2b6d0c/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b07ca78bb696825feee3e976795fab58f2b6d0c/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py?ref=3b07ca78bb696825feee3e976795fab58f2b6d0c",
            "patch": "@@ -22,6 +22,7 @@\n \n from transformers import T5Config, is_torch_available\n from transformers.models.auto.modeling_auto import MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES\n+from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_4\n from transformers.testing_utils import (\n     require_accelerate,\n     require_sentencepiece,\n@@ -1698,6 +1699,150 @@ def test_compile_static_cache_encoder(self):\n         logits_compiled = model(**inputs)\n         torch.testing.assert_close(logits[0][:, -3:, -3], logits_compiled[0][:, -3:, -3], rtol=1e-5, atol=1e-5)\n \n+    @slow\n+    def test_export_encoder(self):\n+        \"\"\"Test exporting T5EncoderModel to torch export format.\"\"\"\n+        if not is_torch_greater_or_equal_than_2_4:\n+            self.skipTest(\"This test requires torch >= 2.4 to run.\")\n+\n+        from transformers.integrations.executorch import Seq2SeqLMEncoderExportableModule\n+\n+        model_id = \"google-t5/t5-small\"\n+        device = \"cpu\"\n+        example_input_ids = torch.ones((1, 10), dtype=torch.long).to(device)\n+\n+        # Load model\n+        model = T5EncoderModel.from_pretrained(model_id).to(device=device).eval()\n+\n+        # Get original output for comparison\n+        with torch.no_grad():\n+            original_output = model(input_ids=example_input_ids).last_hidden_state\n+\n+        encoder_model = Seq2SeqLMEncoderExportableModule(model)\n+\n+        # Export the encoder_model\n+        with torch.no_grad():\n+            seq_len_dim = torch.export.Dim(\"sequence_length\", max=4096)\n+\n+            exported_program = torch.export.export(\n+                encoder_model, (example_input_ids,), dynamic_shapes={\"input_ids\": {1: seq_len_dim}}, strict=True\n+            )\n+\n+        # Test the exported model\n+        with torch.no_grad():\n+            exported_output = exported_program.module()(example_input_ids)\n+\n+        # Verify outputs are close enough\n+        self.assertTrue(torch.allclose(original_output, exported_output, atol=1e-5))\n+\n+    @slow\n+    def test_export_decoder(self):\n+        \"\"\"Test exporting T5 decoder with static cache to torch export format.\"\"\"\n+        if not is_torch_greater_or_equal_than_2_4:\n+            self.skipTest(\"This test requires torch >= 2.4 to run.\")\n+\n+        from transformers import AutoModelForSeq2SeqLM, T5ForConditionalGeneration\n+        from transformers.integrations.executorch import Seq2SeqLMDecoderExportableModuleWithStaticCache\n+\n+        model_id = \"google-t5/t5-small\"\n+\n+        # Configuration for static cache\n+        batch_size = 1\n+        max_cache_len = 123\n+        device = \"cpu\"\n+\n+        full_model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(device)\n+        self.assertIsInstance(full_model, T5ForConditionalGeneration)\n+        decoder_model = (\n+            Seq2SeqLMDecoderExportableModuleWithStaticCache(full_model, max_cache_len, batch_size).to(device).eval()\n+        )\n+\n+        # Prepare test inputs\n+        example_decoder_input_ids = torch.tensor([[0]], dtype=torch.long)  # Start token\n+        example_cache_position = torch.tensor([0], dtype=torch.long)\n+\n+        # For T5-small, hidden size is 512\n+        example_encoder_hidden_states = torch.zeros((batch_size, 10, 512), dtype=torch.float32)\n+\n+        # Export the model\n+        with torch.no_grad():\n+            encoder_sequence_length_dim = torch.export.Dim(\"encoder_sequence_length\", max=4096)\n+\n+            exported_program = torch.export.export(\n+                decoder_model,\n+                (example_decoder_input_ids, example_encoder_hidden_states, example_cache_position),\n+                dynamic_shapes={\n+                    \"decoder_input_ids\": None,\n+                    \"encoder_hidden_states\": {1: encoder_sequence_length_dim},\n+                    \"cache_position\": None,\n+                },\n+                strict=True,\n+            )\n+\n+        # We won't directly verify outputs here as it's complicated with caching,\n+        # but we'll check the export was successful\n+        self.assertIsNotNone(exported_program)\n+\n+        # Verify cache buffers existence and shapes\n+        cache_buffers = [\n+            (name, buffer)\n+            for name, buffer in exported_program.named_buffers()\n+            if name.startswith(\"key_cache_\") or name.startswith(\"value_cache_\")\n+        ]\n+\n+        # Verify cache buffers\n+        self.assertTrue(len(cache_buffers) > 0, \"No cache buffers found in exported model\")\n+        for name, buffer in cache_buffers:\n+            # Verify cache buffers are 3D\n+            self.assertEqual(buffer.shape[2], max_cache_len)\n+\n+    @slow\n+    def test_export_t5_summarization(self):\n+        \"\"\"Test composing exported T5 encoder and decoder for summarization.\"\"\"\n+        if not is_torch_greater_or_equal_than_2_4:\n+            self.skipTest(\"This test requires torch >= 2.4 to run.\")\n+\n+        from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, T5ForConditionalGeneration\n+        from transformers.integrations.executorch import Seq2SeqLMExportableModule\n+\n+        device = \"cpu\"\n+        batch_size = 1\n+        max_cache_length = 1234\n+        max_hidden_seq_length = 5678\n+        model_id = \"google-t5/t5-small\"\n+\n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n+        full_model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(device).eval()\n+        self.assertIsInstance(full_model, T5ForConditionalGeneration)\n+        wrapped_model = Seq2SeqLMExportableModule(\n+            full_model,\n+            batch_size=batch_size,\n+            max_hidden_seq_length=max_hidden_seq_length,\n+            max_cache_length=max_cache_length,\n+        )\n+\n+        exported_t5 = wrapped_model.export()\n+\n+        # Test Summarization with Composed Models\n+        prompts = [\n+            \"summarize: Simply put, the theory of relativity states that 1) the speed of light is constant in all inertial \"\n+            \"reference frames, and 2) the laws of physics are the same for all inertial reference frames.\\nThe \"\n+            \"theory of relativity is not hard to grasp.\"\n+        ]\n+        input_ids = tokenizer(prompts, return_tensors=\"pt\").input_ids\n+\n+        generated_ids = exported_t5.generate(prompt_token_ids=input_ids, max_new_tokens=max_cache_length)\n+        generated_summary = tokenizer.decode(generated_ids, skip_special_tokens=True)\n+\n+        # Also run original model for comparison\n+        original_model = T5ForConditionalGeneration.from_pretrained(model_id).eval()\n+        with torch.no_grad():\n+            original_outputs = original_model.generate(input_ids, max_length=50, num_beams=1)\n+        original_summary = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n+\n+        # Basic verification that we got a reasonable summary\n+        self.assertEqual(generated_summary, original_summary)\n+\n \n @require_torch\n class TestAsymmetricT5(unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 324,
        "additions": 324,
        "deletions": 0
    }
}