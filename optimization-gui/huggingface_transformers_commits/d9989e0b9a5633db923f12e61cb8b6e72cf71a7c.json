{
    "author": "BenjaminBossan",
    "message": "[PEFT] Add warning for missing key in LoRA adapter (#34068)\n\nWhen loading a LoRA adapter, so far, there was only a warning when there\r\nwere unexpected keys in the checkpoint. Now, there is also a warning\r\nwhen there are missing keys.\r\n\r\nThis change is consistent with\r\nhttps://github.com/huggingface/peft/pull/2118 in PEFT and the planned PR\r\nhttps://github.com/huggingface/diffusers/pull/9622 in diffusers.\r\n\r\nApart from this change, the error message for unexpected keys was\r\nslightly altered for consistency (it should be more readable now). Also,\r\nbesides adding a test for the missing keys warning, a test for\r\nunexpected keys warning was also added, as it was missing so far.",
    "sha": "d9989e0b9a5633db923f12e61cb8b6e72cf71a7c",
    "files": [
        {
            "sha": "8afff36eb08625b33eb98dcddc3948817739ba91",
            "filename": "src/transformers/integrations/peft.py",
            "status": "modified",
            "additions": 20,
            "deletions": 4,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/d9989e0b9a5633db923f12e61cb8b6e72cf71a7c/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d9989e0b9a5633db923f12e61cb8b6e72cf71a7c/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fpeft.py?ref=d9989e0b9a5633db923f12e61cb8b6e72cf71a7c",
            "patch": "@@ -235,13 +235,29 @@ def load_adapter(\n         )\n \n         if incompatible_keys is not None:\n-            # check only for unexpected keys\n+            err_msg = \"\"\n+            origin_name = peft_model_id if peft_model_id is not None else \"state_dict\"\n+            # Check for unexpected keys.\n             if hasattr(incompatible_keys, \"unexpected_keys\") and len(incompatible_keys.unexpected_keys) > 0:\n-                logger.warning(\n-                    f\"Loading adapter weights from {peft_model_id} led to unexpected keys not found in the model: \"\n-                    f\" {incompatible_keys.unexpected_keys}. \"\n+                err_msg = (\n+                    f\"Loading adapter weights from {origin_name} led to unexpected keys not found in the model: \"\n+                    f\"{', '.join(incompatible_keys.unexpected_keys)}. \"\n                 )\n \n+            # Check for missing keys.\n+            missing_keys = getattr(incompatible_keys, \"missing_keys\", None)\n+            if missing_keys:\n+                # Filter missing keys specific to the current adapter, as missing base model keys are expected.\n+                lora_missing_keys = [k for k in missing_keys if \"lora_\" in k and adapter_name in k]\n+                if lora_missing_keys:\n+                    err_msg += (\n+                        f\"Loading adapter weights from {origin_name} led to missing keys in the model: \"\n+                        f\"{', '.join(lora_missing_keys)}\"\n+                    )\n+\n+            if err_msg:\n+                logger.warning(err_msg)\n+\n         # Re-dispatch model and hooks in case the model is offloaded to CPU / Disk.\n         if (\n             (getattr(self, \"hf_device_map\", None) is not None)"
        },
        {
            "sha": "aebf2b295267c4167bf95a8575e36073277106b1",
            "filename": "tests/peft_integration/test_peft_integration.py",
            "status": "modified",
            "additions": 76,
            "deletions": 2,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/d9989e0b9a5633db923f12e61cb8b6e72cf71a7c/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d9989e0b9a5633db923f12e61cb8b6e72cf71a7c/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpeft_integration%2Ftest_peft_integration.py?ref=d9989e0b9a5633db923f12e61cb8b6e72cf71a7c",
            "patch": "@@ -20,8 +20,9 @@\n from huggingface_hub import hf_hub_download\n from packaging import version\n \n-from transformers import AutoModelForCausalLM, OPTForCausalLM\n+from transformers import AutoModelForCausalLM, OPTForCausalLM, logging\n from transformers.testing_utils import (\n+    CaptureLogger,\n     require_bitsandbytes,\n     require_peft,\n     require_torch,\n@@ -72,9 +73,15 @@ def test_peft_from_pretrained(self):\n         This checks if we pass a remote folder that contains an adapter config and adapter weights, it\n         should correctly load a model that has adapters injected on it.\n         \"\"\"\n+        logger = logging.get_logger(\"transformers.integrations.peft\")\n+\n         for model_id in self.peft_test_model_ids:\n             for transformers_class in self.transformers_test_model_classes:\n-                peft_model = transformers_class.from_pretrained(model_id).to(torch_device)\n+                with CaptureLogger(logger) as cl:\n+                    peft_model = transformers_class.from_pretrained(model_id).to(torch_device)\n+                # ensure that under normal circumstances, there  are no warnings about keys\n+                self.assertNotIn(\"unexpected keys\", cl.out)\n+                self.assertNotIn(\"missing keys\", cl.out)\n \n                 self.assertTrue(self._check_lora_correctly_converted(peft_model))\n                 self.assertTrue(peft_model._hf_peft_config_loaded)\n@@ -548,3 +555,70 @@ def test_peft_from_pretrained_hub_kwargs(self):\n \n         model = OPTForCausalLM.from_pretrained(peft_model_id, adapter_kwargs=adapter_kwargs)\n         self.assertTrue(self._check_lora_correctly_converted(model))\n+\n+    def test_peft_from_pretrained_unexpected_keys_warning(self):\n+        \"\"\"\n+        Test for warning when loading a PEFT checkpoint with unexpected keys.\n+        \"\"\"\n+        from peft import LoraConfig\n+\n+        logger = logging.get_logger(\"transformers.integrations.peft\")\n+\n+        for model_id, peft_model_id in zip(self.transformers_test_model_ids, self.peft_test_model_ids):\n+            for transformers_class in self.transformers_test_model_classes:\n+                model = transformers_class.from_pretrained(model_id).to(torch_device)\n+\n+                peft_config = LoraConfig()\n+                state_dict_path = hf_hub_download(peft_model_id, \"adapter_model.bin\")\n+                dummy_state_dict = torch.load(state_dict_path)\n+\n+                # add unexpected key\n+                dummy_state_dict[\"foobar\"] = next(iter(dummy_state_dict.values()))\n+\n+                with CaptureLogger(logger) as cl:\n+                    model.load_adapter(\n+                        adapter_state_dict=dummy_state_dict, peft_config=peft_config, low_cpu_mem_usage=False\n+                    )\n+\n+                msg = \"Loading adapter weights from state_dict led to unexpected keys not found in the model: foobar\"\n+                self.assertIn(msg, cl.out)\n+\n+    def test_peft_from_pretrained_missing_keys_warning(self):\n+        \"\"\"\n+        Test for warning when loading a PEFT checkpoint with missing keys.\n+        \"\"\"\n+        from peft import LoraConfig\n+\n+        logger = logging.get_logger(\"transformers.integrations.peft\")\n+\n+        for model_id, peft_model_id in zip(self.transformers_test_model_ids, self.peft_test_model_ids):\n+            for transformers_class in self.transformers_test_model_classes:\n+                model = transformers_class.from_pretrained(model_id).to(torch_device)\n+\n+                peft_config = LoraConfig()\n+                state_dict_path = hf_hub_download(peft_model_id, \"adapter_model.bin\")\n+                dummy_state_dict = torch.load(state_dict_path)\n+\n+                # remove a key so that we have missing keys\n+                key = next(iter(dummy_state_dict.keys()))\n+                del dummy_state_dict[key]\n+\n+                with CaptureLogger(logger) as cl:\n+                    model.load_adapter(\n+                        adapter_state_dict=dummy_state_dict,\n+                        peft_config=peft_config,\n+                        low_cpu_mem_usage=False,\n+                        adapter_name=\"other\",\n+                    )\n+\n+                # Here we need to adjust the key name a bit to account for PEFT-specific naming.\n+                # 1. Remove PEFT-specific prefix\n+                # If merged after dropping Python 3.8, we can use: key = key.removeprefix(peft_prefix)\n+                peft_prefix = \"base_model.model.\"\n+                key = key[len(peft_prefix) :]\n+                # 2. Insert adapter name\n+                prefix, _, suffix = key.rpartition(\".\")\n+                key = f\"{prefix}.other.{suffix}\"\n+\n+                msg = f\"Loading adapter weights from state_dict led to missing keys in the model: {key}\"\n+                self.assertIn(msg, cl.out)"
        }
    ],
    "stats": {
        "total": 102,
        "additions": 96,
        "deletions": 6
    }
}