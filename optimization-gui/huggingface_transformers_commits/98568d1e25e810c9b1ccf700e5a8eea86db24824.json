{
    "author": "islemyakoubi",
    "message": "Fix incorrect bbox_embed initialization when decoder_bbox_embed_share=False in GroundingDINO (#38238)\n\n* A shallow copy in groundingdino\nFixes #37333\n\n* Supprimer une ligne vide dans la classe GroundingDinoForObjectDetection\n\n* Translate comments in the GroundingDinoForObjectDetection class from French to English",
    "sha": "98568d1e25e810c9b1ccf700e5a8eea86db24824",
    "files": [
        {
            "sha": "27ac107f6790b0d38781f6fee9e7237602f2a26f",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 16,
            "deletions": 7,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/98568d1e25e810c9b1ccf700e5a8eea86db24824/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/98568d1e25e810c9b1ccf700e5a8eea86db24824/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=98568d1e25e810c9b1ccf700e5a8eea86db24824",
            "patch": "@@ -2454,16 +2454,25 @@ def __init__(self, config: GroundingDinoConfig):\n         _class_embed = GroundingDinoContrastiveEmbedding(config)\n \n         if config.decoder_bbox_embed_share:\n-            _bbox_embed = GroundingDinoMLPPredictionHead(\n+            # a single shared instance\n+            shared_head = GroundingDinoMLPPredictionHead(\n                 input_dim=config.d_model, hidden_dim=config.d_model, output_dim=4, num_layers=3\n             )\n-            self.bbox_embed = nn.ModuleList([_bbox_embed for _ in range(config.decoder_layers)])\n+            self.bbox_embed = nn.ModuleList([shared_head] * config.decoder_layers)\n         else:\n-            for _ in range(config.decoder_layers):\n-                _bbox_embed = GroundingDinoMLPPredictionHead(\n-                    input_dim=config.d_model, hidden_dim=config.d_model, output_dim=4, num_layers=3\n-                )\n-                self.bbox_embed = nn.ModuleList([_bbox_embed for _ in range(config.decoder_layers)])\n+            # each layer has its own head (implicit deep copy through a new instance)\n+            self.bbox_embed = nn.ModuleList(\n+                [\n+                    GroundingDinoMLPPredictionHead(\n+                        input_dim=config.d_model,\n+                        hidden_dim=config.d_model,\n+                        output_dim=4,\n+                        num_layers=3,\n+                    )\n+                    for _ in range(config.decoder_layers)\n+                ]\n+            )\n+\n         self.class_embed = nn.ModuleList([_class_embed for _ in range(config.decoder_layers)])\n         # hack for box-refinement\n         self.model.decoder.bbox_embed = self.bbox_embed"
        }
    ],
    "stats": {
        "total": 23,
        "additions": 16,
        "deletions": 7
    }
}