{
    "author": "zucchini-nlp",
    "message": "Attn implementation for composite models (#32238)\n\n* first try\r\n\r\n* codestyle\r\n\r\n* idefics2 is happy\r\n\r\n* [run-slow] llava, llava_next, video_llava, vipllava, llava_next_video, idefics, idefics2, kosmos2, fuyu, blip, blip_2, instructblip, instructblipvideo, paligemma\r\n\r\n* fix-copies\r\n\r\n* [run-slow] llava, llava_next, video_llava, vipllava, llava_next_video, idefics, idefics2, kosmos2, fuyu, blip, blip_2, instructblip, instructblipvideo\r\n\r\n* blip-2 needs to init vision from config\r\n\r\n* when was this removed O_o\r\n\r\n* minor fix\r\n\r\n* tests\r\n\r\n* this way?\r\n\r\n* tests\r\n\r\n* model-agnostic code\r\n\r\n* codestyle\r\n\r\n* add tests for idefics\r\n\r\n* modify general test for VLMs\r\n\r\n* no generation test for vlm yet!\r\n\r\n* no generation test here also\r\n\r\n* wanr in VIT-SDPA if output attn\r\n\r\n* add more tests\r\n\r\n* user can pass dict as attn impl\r\n\r\n* repo consistency\r\n\r\n* update\r\n\r\n* muicgen\r\n\r\n* no prints\r\n\r\n* forgot speech enc-dec and clip\r\n\r\n* how many composite models we have?\r\n\r\n* musicgen meelody is same as mudicgen\r\n\r\n* +siglip\r\n\r\n* fix tests + add some more\r\n\r\n* remove idefics custom overriden code\r\n\r\n* make idefics2 automappable\r\n\r\n* nits\r\n\r\n* skip tests\r\n\r\n* doctests\r\n\r\n* Update src/transformers/models/idefics2/configuration_idefics2.py\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* Update tests/models/clip/test_modeling_clip.py\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* Update tests/models/idefics2/test_modeling_idefics2.py\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* Update tests/models/idefics2/test_modeling_idefics2.py\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* Update src/transformers/configuration_utils.py\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* major update, no need for automap\r\n\r\n* clean up\r\n\r\n* add FA2 test\r\n\r\n* more tests\r\n\r\n* style\r\n\r\n* skip tests\r\n\r\n* why did these started failing now?\r\n\r\n* no attributes for FA2 needed\r\n\r\n* one tiny test\r\n\r\n* address comment about FA2 false warning\r\n\r\n* style\r\n\r\n* add new models and resolve conflicts\r\n\r\n* fix copies\r\n\r\n* let it be this way for now, come back tomorrow to review\r\n\r\n* some more fixes\r\n\r\n* update\r\n\r\n* more updates\r\n\r\n* update\r\n\r\n* fix copies\r\n\r\n* style and tests\r\n\r\n* another big update\r\n\r\n* fix tests\r\n\r\n* fix tests\r\n\r\n* update\r\n\r\n* another update\r\n\r\n* fix tests\r\n\r\n* fix copies\r\n\r\n* fix tests\r\n\r\n---------\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>",
    "sha": "21d5025826857e11a75ef7b23ac15a607be4fc54",
    "files": [
        {
            "sha": "67bd31fdaeede599cee710a09bce8fdd3756f82a",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -79,6 +79,7 @@ FlashAttention-2 is currently supported for the following architectures:\n * [OLMo](https://huggingface.co/docs/transformers/model_doc/olmo#transformers.OlmoModel)\n * [OLMoE](https://huggingface.co/docs/transformers/model_doc/olmoe#transformers.OlmoeModel)\n * [OPT](https://huggingface.co/docs/transformers/model_doc/opt#transformers.OPTModel)\n+* [PaliGemma](https://huggingface.co/docs/transformers/model_doc/paligemma#transformers.PaliGemmaForConditionalGeneration)\n * [Phi](https://huggingface.co/docs/transformers/model_doc/phi#transformers.PhiModel)\n * [Phi3](https://huggingface.co/docs/transformers/model_doc/phi3#transformers.Phi3Model)\n * [PhiMoE](https://huggingface.co/docs/transformers/model_doc/phimoe#transformers.PhimoeModel)\n@@ -88,6 +89,10 @@ FlashAttention-2 is currently supported for the following architectures:\n * [Qwen2Audio](https://huggingface.co/docs/transformers/model_doc/qwen2_audio#transformers.Qwen2AudioEncoder)\n * [Qwen2MoE](https://huggingface.co/docs/transformers/model_doc/qwen2_moe#transformers.Qwen2MoeModel)\n * [Qwen2VL](https://huggingface.co/docs/transformers/model_doc/qwen2_vl#transformers.Qwen2VLModel)\n+* [RAG](https://huggingface.co/docs/transformers/model_doc/rag#transformers.RagModel)\n+* [SpeechEncoderDecoder](https://huggingface.co/docs/transformers/model_doc/speech_encoder_decoder#transformers.SpeechEncoderDecoderModel)\n+* [VisionEncoderDecoder](https://huggingface.co/docs/transformers/model_doc/vision_encoder_decoder#transformers.VisionEncoderDecoderModel)\n+* [VisionTextDualEncoder](https://huggingface.co/docs/transformers/model_doc/vision_text_dual_encoder#transformers.VisionTextDualEncoderModel)\n * [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperModel)\n * [Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2#transformers.Wav2Vec2Model)\n * [Hubert](https://huggingface.co/docs/transformers/model_doc/hubert#transformers.HubertModel)\n@@ -225,6 +230,7 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [Dinov2](https://huggingface.co/docs/transformers/en/model_doc/dinov2)\n * [DistilBert](https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertModel)\n * [Dpr](https://huggingface.co/docs/transformers/model_doc/dpr#transformers.DprReader)\n+* [EncoderDecoder](https://huggingface.co/docs/transformers/model_doc/encoder_decoder#transformers.EncoderDecoderModel)\n * [Falcon](https://huggingface.co/docs/transformers/model_doc/falcon#transformers.FalconModel)\n * [Gemma](https://huggingface.co/docs/transformers/model_doc/gemma#transformers.GemmaModel)\n * [Gemma2](https://huggingface.co/docs/transformers/model_doc/gemma2#transformers.Gemma2Model)\n@@ -233,11 +239,16 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [GPTNeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox#transformers.GPTNeoXModel)\n * [Hubert](https://huggingface.co/docs/transformers/model_doc/hubert#transformers.HubertModel)\n * [Idefics](https://huggingface.co/docs/transformers/model_doc/idefics#transformers.IdeficsModel)\n+* [Idefics2](https://huggingface.co/docs/transformers/model_doc/idefics2#transformers.Idefics2Model)\n+* [Idefics3](https://huggingface.co/docs/transformers/model_doc/idefics3#transformers.Idefics3Model)\n * [Granite](https://huggingface.co/docs/transformers/model_doc/granite#transformers.GraniteModel)\n * [GraniteMoe](https://huggingface.co/docs/transformers/model_doc/granitemoe#transformers.GraniteMoeModel)\n * [JetMoe](https://huggingface.co/docs/transformers/model_doc/jetmoe#transformers.JetMoeModel)\n * [Jamba](https://huggingface.co/docs/transformers/model_doc/jamba#transformers.JambaModel)\n * [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)\n+* [Llava](https://huggingface.co/docs/transformers/model_doc/llava)\n+* [Llava-NeXT](https://huggingface.co/docs/transformers/model_doc/llava_next)\n+* [Llava-NeXT-Video](https://huggingface.co/docs/transformers/model_doc/llava_next_video)\n * [LLaVA-Onevision](https://huggingface.co/docs/transformers/model_doc/llava_onevision)\n * [M2M100](https://huggingface.co/docs/transformers/model_doc/m2m_100#transformers.M2M100Model)\n * [Mimi](https://huggingface.co/docs/transformers/model_doc/mimi)\n@@ -277,10 +288,15 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [Musicgen](https://huggingface.co/docs/transformers/model_doc/musicgen#transformers.MusicgenModel)\n * [MusicGen Melody](https://huggingface.co/docs/transformers/model_doc/musicgen_melody#transformers.MusicgenMelodyModel)\n * [Nemotron](https://huggingface.co/docs/transformers/model_doc/nemotron)\n+* [SpeechEncoderDecoder](https://huggingface.co/docs/transformers/model_doc/speech_encoder_decoder#transformers.SpeechEncoderDecoderModel)\n+* [VideoLlava](https://huggingface.co/docs/transformers/model_doc/video_llava)\n+* [VipLlava](https://huggingface.co/docs/transformers/model_doc/vipllava)\n+* [VisionEncoderDecoder](https://huggingface.co/docs/transformers/model_doc/vision_encoder_decoder#transformers.VisionEncoderDecoderModel)\n * [ViT](https://huggingface.co/docs/transformers/model_doc/vit#transformers.ViTModel)\n * [ViTHybrid](https://huggingface.co/docs/transformers/model_doc/vit_hybrid#transformers.ViTHybridModel)\n * [ViTMAE](https://huggingface.co/docs/transformers/model_doc/vit_mae#transformers.ViTMAEModel)\n * [ViTMSN](https://huggingface.co/docs/transformers/model_doc/vit_msn#transformers.ViTMSNModel)\n+* [VisionTextDualEncoder](https://huggingface.co/docs/transformers/model_doc/vision_text_dual_encoder#transformers.VisionTextDualEncoderModel)\n * [VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae#transformers.VideoMAEModell)\n * [ViViT](https://huggingface.co/docs/transformers/model_doc/vivit#transformers.VivitModel)\n * [wav2vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2#transformers.Wav2Vec2Model)"
        },
        {
            "sha": "1d892c49a231fc83f6c0a6373e0e55b558f0da21",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -296,6 +296,7 @@ def __init__(self, **kwargs):\n \n         # Attention implementation to use, if relevant.\n         self._attn_implementation_internal = kwargs.pop(\"attn_implementation\", None)\n+        self._attn_implementation_autoset = False\n \n         # Drop the transformers version info\n         self.transformers_version = kwargs.pop(\"transformers_version\", None)\n@@ -776,6 +777,10 @@ def __eq__(self, other):\n     def __repr__(self):\n         return f\"{self.__class__.__name__} {self.to_json_string()}\"\n \n+    def __iter__(self):\n+        for attr in self.__dict__:\n+            yield attr\n+\n     def to_diff_dict(self) -> Dict[str, Any]:\n         \"\"\"\n         Removes all attributes from config which correspond to the default config attributes for better readability and"
        },
        {
            "sha": "a6fbd7b1a9145308caf9564275d935ece5037218",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 45,
            "deletions": 13,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -1420,9 +1420,10 @@ def __init__(self, config: PretrainedConfig, *inputs, **kwargs):\n                 f\"`model = {self.__class__.__name__}.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n             )\n         # Save config and origin of the pretrained weights if given in model\n-        config = self._autoset_attn_implementation(\n-            config, torch_dtype=torch.get_default_dtype(), check_device_map=False\n-        )\n+        if not getattr(config, \"_attn_implementation_autoset\", False):\n+            config = self._autoset_attn_implementation(\n+                config, torch_dtype=torch.get_default_dtype(), check_device_map=False\n+            )\n         self.config = config\n \n         self.name_or_path = config.name_or_path\n@@ -1500,6 +1501,9 @@ def _from_config(cls, config, **kwargs):\n             torch_dtype (`torch.dtype`, *optional*):\n                 Override the default `torch.dtype` and load the model under this dtype.\n         \"\"\"\n+        # when we init a model from within another model (e.g. VLMs) and dispatch on FA2\n+        # a warning is raised that dtype should be fp16. Since we never pass dtype from within\n+        # modeling code, we can try to infer it here same way as done in `from_pretrained`\n         torch_dtype = kwargs.pop(\"torch_dtype\", torch.get_default_dtype())\n         use_flash_attention_2 = kwargs.pop(\"use_flash_attention_2\", False)\n \n@@ -1518,12 +1522,13 @@ def _from_config(cls, config, **kwargs):\n             attn_implementation = None\n \n         config._attn_implementation = kwargs.pop(\"attn_implementation\", attn_implementation)\n-        config = cls._autoset_attn_implementation(\n-            config,\n-            use_flash_attention_2=use_flash_attention_2,\n-            check_device_map=False,\n-            torch_dtype=torch_dtype,\n-        )\n+        if not getattr(config, \"_attn_implementation_autoset\", False):\n+            config = cls._autoset_attn_implementation(\n+                config,\n+                use_flash_attention_2=use_flash_attention_2,\n+                check_device_map=False,\n+                torch_dtype=torch_dtype,\n+            )\n \n         if is_deepspeed_zero3_enabled():\n             import deepspeed\n@@ -1570,7 +1575,11 @@ def _autoset_attn_implementation(\n                     ' We recommend to just use `attn_implementation=\"flash_attention_2\"` when loading the model.'\n                 )\n \n-            if config._attn_implementation not in [\"eager\", \"sdpa\", \"flash_attention_2\"]:\n+            if not isinstance(config._attn_implementation, dict) and config._attn_implementation not in [\n+                \"eager\",\n+                \"sdpa\",\n+                \"flash_attention_2\",\n+            ]:\n                 message = f'Specified `attn_implementation=\"{config._attn_implementation}\"` is not supported. The only possible arguments are `attn_implementation=\"eager\"` (manual attention implementation)'\n                 if cls._supports_flash_attn_2:\n                     message += ', `\"attn_implementation=flash_attention_2\"` (implementation using flash attention 2)'\n@@ -1581,6 +1590,22 @@ def _autoset_attn_implementation(\n             # If a config is passed with a preset attn_implementation, we skip the automatic dispatch and use the user-provided config, with hard checks that the requested attention implementation is available.\n             requested_attn_implementation = config._attn_implementation_internal\n \n+        # Composite models consisting of several PretrainedModels have to specify attention impl as a dict\n+        # where keys are sub-config names. But most people will specify one `str` which means that should dispatch it\n+        # for all sub-models.\n+        # Below we check if a config is composite and manually prepare a dict of attn impl if not already passed as a dict.\n+        # Later each sub-module will dispatch with its own attn impl, by calling `XXXModel._from_config(config.text_config)`\n+        # If any of sub-modules doesn't support requested attn, an error will be raised. See https://github.com/huggingface/transformers/pull/32238\n+        for key in config:\n+            if isinstance(getattr(config, key), PretrainedConfig):\n+                sub_config = getattr(config, key)\n+                curr_attn_implementation = (\n+                    requested_attn_implementation\n+                    if not isinstance(requested_attn_implementation, dict)\n+                    else requested_attn_implementation.get(key, None)\n+                )\n+                sub_config._attn_implementation_internal = curr_attn_implementation\n+\n         if use_flash_attention_2:\n             logger.warning_once(\n                 'The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.'\n@@ -1611,9 +1636,12 @@ def _autoset_attn_implementation(\n                     \"Using the `SDPA` attention implementation on multi-gpu setup with ROCM may lead to performance issues due to the FA backend. Disabling it to use alternative backends.\"\n                 )\n                 torch.backends.cuda.enable_flash_sdp(False)\n+        elif isinstance(requested_attn_implementation, dict):\n+            config._attn_implementation = None\n         else:\n             config._attn_implementation = \"eager\"\n \n+        config._attn_implementation_autoset = True\n         return config\n \n     @classmethod\n@@ -2771,6 +2799,9 @@ def save_pretrained(\n         # Attach architecture to the config\n         model_to_save.config.architectures = [model_to_save.__class__.__name__]\n \n+        # Unset attn implementation so it can be set to another one when loading back\n+        model_to_save.config._attn_implementation_autoset = False\n+\n         # If we have a custom model, we copy the file defining it in the folder and set the attributes so it can be\n         # loaded from the Hub.\n         if self._auto_class is not None:\n@@ -4055,9 +4086,10 @@ def from_pretrained(\n             init_contexts.append(init_empty_weights())\n \n         config = copy.deepcopy(config)  # We do not want to modify the config inplace in from_pretrained.\n-        config = cls._autoset_attn_implementation(\n-            config, use_flash_attention_2=use_flash_attention_2, torch_dtype=torch_dtype, device_map=device_map\n-        )\n+        if not getattr(config, \"_attn_implementation_autoset\", False):\n+            config = cls._autoset_attn_implementation(\n+                config, use_flash_attention_2=use_flash_attention_2, torch_dtype=torch_dtype, device_map=device_map\n+            )\n \n         with ContextManagers(init_contexts):\n             # Let's make sure we don't run the init function of buffer modules"
        },
        {
            "sha": "491c6ce164611a2391da90c7771ee53926b16b30",
            "filename": "src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 17,
            "deletions": 1,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -176,8 +176,24 @@ def __init__(self, config: ASTConfig) -> None:\n         self.attention_probs_dropout_prob = config.attention_probs_dropout_prob\n \n     def forward(\n-        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n+        self,\n+        hidden_states: torch.FloatTensor,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n     ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n+        if output_attentions or head_mask is not None:\n+            logger.warning_once(\n+                \"`ASTSdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n+                \"`output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but \"\n+                \"specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n+                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                head_mask=head_mask,\n+                output_attentions=output_attentions,\n+            )\n+\n         mixed_query_layer = self.query(hidden_states)\n \n         key_layer = self.transpose_for_scores(self.key(hidden_states))"
        },
        {
            "sha": "eba82cd1b3c8e4ad8b5dc17495e06e5e88199312",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 12,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -410,6 +410,7 @@ class Blip2PreTrainedModel(PreTrainedModel):\n     config_class = Blip2Config\n     base_model_prefix = \"blip\"\n     supports_gradient_checkpointing = True\n+\n     _no_split_modules = [\n         \"Blip2Attention\",\n         \"Blip2QFormerMultiHeadAttention\",\n@@ -1455,13 +1456,9 @@ def __init__(self, config: Blip2Config):\n \n         self.language_projection = nn.Linear(config.qformer_config.hidden_size, config.text_config.hidden_size)\n         if config.use_decoder_only_language_model:\n-            language_model = AutoModelForCausalLM.from_config(\n-                config.text_config, attn_implementation=config._attn_implementation\n-            )\n+            language_model = AutoModelForCausalLM.from_config(config.text_config)\n         else:\n-            language_model = AutoModelForSeq2SeqLM.from_config(\n-                config.text_config, attn_implementation=config._attn_implementation\n-            )\n+            language_model = AutoModelForSeq2SeqLM.from_config(config.text_config)\n \n         # Update _tied_weights_keys using the base model used.\n         if language_model._tied_weights_keys is not None:\n@@ -2020,13 +2017,9 @@ def __init__(self, config: Blip2Config):\n \n         self.language_projection = nn.Linear(config.qformer_config.hidden_size, config.text_config.hidden_size)\n         if config.use_decoder_only_language_model:\n-            language_model = AutoModelForCausalLM.from_config(\n-                config.text_config, attn_implementation=config._attn_implementation\n-            )\n+            language_model = AutoModelForCausalLM.from_config(config.text_config)\n         else:\n-            language_model = AutoModelForSeq2SeqLM.from_config(\n-                config.text_config, attn_implementation=config._attn_implementation\n-            )\n+            language_model = AutoModelForSeq2SeqLM.from_config(config.text_config)\n \n         # Update _tied_weights_keys using the base model used.\n         if language_model._tied_weights_keys is not None:"
        },
        {
            "sha": "04a3a73de0455e66a5db3be9ab4b1ade058ffa86",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -1204,10 +1204,10 @@ def __init__(self, config: CLIPConfig):\n         self.text_embed_dim = text_config.hidden_size\n         self.vision_embed_dim = vision_config.hidden_size\n \n-        text_model = CLIPTextModel._from_config(text_config, attn_implementation=config._attn_implementation)\n+        text_model = CLIPTextModel._from_config(text_config)\n         self.text_model = text_model.text_model\n \n-        vision_model = CLIPVisionModel._from_config(vision_config, attn_implementation=config._attn_implementation)\n+        vision_model = CLIPVisionModel._from_config(vision_config)\n         self.vision_model = vision_model.vision_model\n \n         self.visual_projection = nn.Linear(self.vision_embed_dim, self.projection_dim, bias=False)\n@@ -1590,9 +1590,7 @@ def __init__(self, config: CLIPConfig) -> None:\n         super().__init__(config)\n \n         self.num_labels = config.num_labels\n-        vision_model = CLIPVisionModel._from_config(\n-            config.vision_config, attn_implementation=config._attn_implementation\n-        )\n+        vision_model = CLIPVisionModel._from_config(config.vision_config)\n         self.vision_model = vision_model.vision_model\n \n         # Classifier head"
        },
        {
            "sha": "e0b053e43906b893eeed4cb23824688aa54f378f",
            "filename": "src/transformers/models/deit/modeling_deit.py",
            "status": "modified",
            "additions": 17,
            "deletions": 1,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -248,8 +248,24 @@ def __init__(self, config: DeiTConfig) -> None:\n         self.attention_probs_dropout_prob = config.attention_probs_dropout_prob\n \n     def forward(\n-        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n+        self,\n+        hidden_states: torch.FloatTensor,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n     ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n+        if output_attentions or head_mask is not None:\n+            logger.warning_once(\n+                \"`DeiTSdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n+                \"`output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but \"\n+                \"specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n+                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                head_mask=head_mask,\n+                output_attentions=output_attentions,\n+            )\n+\n         mixed_query_layer = self.query(hidden_states)\n \n         key_layer = self.transpose_for_scores(self.key(hidden_states))"
        },
        {
            "sha": "9ebedce07fb83353a4f08cfe670cd2b05c2e1ed1",
            "filename": "src/transformers/models/encoder_decoder/modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -180,6 +180,8 @@ class EncoderDecoderModel(PreTrainedModel, GenerationMixin):\n     main_input_name = \"input_ids\"\n     supports_gradient_checkpointing = True\n     _supports_param_buffer_assignment = False\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n \n     def __init__(\n         self,\n@@ -210,12 +212,12 @@ def __init__(\n         if encoder is None:\n             from ..auto.modeling_auto import AutoModel\n \n-            encoder = AutoModel.from_config(config.encoder, attn_implementation=config._attn_implementation)\n+            encoder = AutoModel.from_config(config.encoder)\n \n         if decoder is None:\n             from ..auto.modeling_auto import AutoModelForCausalLM\n \n-            decoder = AutoModelForCausalLM.from_config(config.decoder, attn_implementation=config._attn_implementation)\n+            decoder = AutoModelForCausalLM.from_config(config.decoder)\n \n         self.encoder = encoder\n         self.decoder = decoder\n@@ -233,6 +235,9 @@ def __init__(\n \n         # make sure that the individual model's config refers to the shared config\n         # so that the updates to the config will be synced\n+        # update `_attn_implementation` because the attn is set in a deepcopied config within PreTrainedModel\n+        self.config.encoder._attn_implementation = self.encoder.config._attn_implementation\n+        self.config.decoder._attn_implementation = self.decoder.config._attn_implementation\n         self.encoder.config = self.config.encoder\n         self.decoder.config = self.config.decoder\n "
        },
        {
            "sha": "8bd24728b03885ad26155cdbd6dbc81329e7e6d6",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -933,18 +933,6 @@ def _init_weights(self, module):\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n \n-    # Adapted from transformers.modeling_utils.PreTrainedModel._check_and_enable_sdpa\n-    @classmethod\n-    def _check_and_enable_sdpa(cls, config, hard_check_only: bool = False) -> PretrainedConfig:\n-        # We remove the checks on `is_torch_sdpa_available()` and `cls._supports_sdpa` as Falcon supports SDPA from torch==2.0.0 (no requirement on 2.1).\n-        _is_bettertransformer = getattr(cls, \"use_bettertransformer\", False)\n-        if _is_bettertransformer:\n-            return config\n-\n-        if not hard_check_only:\n-            config._attn_implementation = \"sdpa\"\n-        return config\n-\n \n LLAMA_INPUTS_DOCSTRING = r\"\"\"\n     Args:"
        },
        {
            "sha": "64743d1cd470e77519ac8f4fc079d66b4e8f61be",
            "filename": "src/transformers/models/idefics2/configuration_idefics2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 1,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fidefics2%2Fconfiguration_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fidefics2%2Fconfiguration_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fconfiguration_idefics2.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -57,7 +57,7 @@ class Idefics2VisionConfig(PretrainedConfig):\n             The epsilon used by the layer normalization layers.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n-        intializer_range (`float`, *optional*, defaults to 0.02):\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation for initializing all weight matrices in the model.\n \n     Example:\n@@ -134,6 +134,10 @@ class Idefics2PerceiverConfig(PretrainedConfig):\n     Args:\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the perceiver block.\n+        hidden_size (`int`, *optional*, defaults to 4096):\n+            Dimension of the hidden representations.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n         resampler_n_latents (`int`, *optional*, defaults to 64):\n             Number of latent embeddings to resample (\"compress\") the input sequence to (usually < 128).\n         resampler_depth (`int`, *optional*, defaults to 3):\n@@ -153,6 +157,8 @@ class Idefics2PerceiverConfig(PretrainedConfig):\n     def __init__(\n         self,\n         hidden_act=\"silu\",\n+        hidden_size=4096,\n+        rms_norm_eps=1e-06,\n         resampler_n_latents=64,\n         resampler_depth=3,\n         resampler_n_heads=16,\n@@ -162,6 +168,8 @@ def __init__(\n         **kwargs,\n     ):\n         self.hidden_act = hidden_act\n+        self.hidden_size = hidden_size\n+        self.rms_norm_eps = rms_norm_eps\n         self.resampler_n_latents = resampler_n_latents\n         self.resampler_depth = resampler_depth\n         self.resampler_n_heads = resampler_n_heads\n@@ -258,5 +266,12 @@ def __init__(\n             )\n \n         self.text_config = text_config\n+        if self.text_config.hidden_size != self.perceiver_config.hidden_size:\n+            self.perceiver_config.hidden_size = self.text_config.hidden_size\n+            self.perceiver_config.rms_norm_eps = self.text_config.rms_norm_eps\n+            logger.warning_once(\n+                \"Perceiver config has a different `hidden_size` than text config, which means default values were used. \"\n+                \"In your model's config on the hub, add `hidden_size` and `rms_norm_eps` keys under the `perceiver_config` dict. \"\n+            )\n \n         super().__init__(**kwargs, tie_word_embeddings=tie_word_embeddings)"
        },
        {
            "sha": "3d46c3bd82e788c42efffa8f7d7f5aca23ff9a0c",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 129,
            "deletions": 109,
            "changes": 238,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -16,7 +16,7 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Dict, List, Optional, Tuple, Union\n+from typing import List, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -38,7 +38,7 @@\n     replace_return_docstrings,\n )\n from ..auto import AutoModel\n-from .configuration_idefics2 import Idefics2Config, Idefics2VisionConfig\n+from .configuration_idefics2 import Idefics2Config, Idefics2PerceiverConfig, Idefics2VisionConfig\n \n \n if is_flash_attn_2_available():\n@@ -572,9 +572,86 @@ def forward(\n         )\n \n \n-class Idefics2VisionTransformer(nn.Module):\n+IDEFICS2_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`Idefics2Config`] or [`Idefics2VisionConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare Idefics2 Model outputting raw hidden-states without any specific head on top.\",\n+    IDEFICS2_START_DOCSTRING,\n+)\n+class Idefics2PreTrainedModel(PreTrainedModel):\n+    config_class = Idefics2Config\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"Idefics2VisionAttention\", \"Idefics2MLP\", \"Idefics2PerceiverLayer\", \"Idefics2DecoderLayer\"]\n+    _skip_keys_device_placement = \"past_key_values\"\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_cache_class = True\n+\n+    def _init_weights(self, module):\n+        std = (\n+            self.config.text_config.initializer_range\n+            if hasattr(self.config, \"initializer_range\")\n+            else self.config.text_config.initializer_range\n+        )\n+\n+        if hasattr(module, \"class_embedding\"):\n+            module.class_embedding.data.normal_(mean=0.0, std=std)\n+\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+\n+IDEFICS2_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n+            The tensors corresponding to the input images. Pixel values can be obtained using\n+            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details ([]`LlavaProcessor`] uses\n+            [`CLIPImageProcessor`] for processing images).\n+        pixel_attention_mask (`torch.Tensor` of shape `(batch_size, image_size, image_size)`, *optional*):\n+            Mask to avoid performing attention on padding pixel indices.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"\"\"Idefics2 vision encoder model that returnss raw image embeddings.\"\"\",\n+    IDEFICS2_START_DOCSTRING,\n+)\n+class Idefics2VisionTransformer(Idefics2PreTrainedModel):\n+    _supports_sdpa = False\n+    config_class = Idefics2VisionConfig\n+\n     def __init__(self, config: Idefics2VisionConfig):\n-        super().__init__()\n+        super().__init__(config)\n         embed_dim = config.hidden_size\n \n         self.config = config\n@@ -687,12 +764,12 @@ def __init__(self, config, layer_idx: Optional[int] = None) -> None:\n         super().__init__()\n \n         self.layer_idx = None\n-        self.hidden_size = config.text_config.hidden_size\n-        self.num_heads = config.perceiver_config.resampler_n_heads\n-        self.head_dim = config.perceiver_config.resampler_head_dim\n-        self.num_key_value_heads = config.perceiver_config.num_key_value_heads\n+        self.hidden_size = config.hidden_size\n+        self.num_heads = config.resampler_n_heads\n+        self.head_dim = config.resampler_head_dim\n+        self.num_key_value_heads = config.num_key_value_heads\n         self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.attention_dropout = config.perceiver_config.attention_dropout\n+        self.attention_dropout = config.attention_dropout\n \n         self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n         self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n@@ -918,20 +995,20 @@ def forward(\n class Idefics2PerceiverLayer(nn.Module):\n     def __init__(self, config, layer_idx: int):\n         super().__init__()\n-        self.hidden_size = config.text_config.hidden_size\n-        self.n_latents = config.perceiver_config.resampler_n_latents\n-        self.depth = config.perceiver_config.resampler_depth\n-        self.rms_norm_eps = config.text_config.rms_norm_eps\n+        self.hidden_size = config.hidden_size\n+        self.n_latents = config.resampler_n_latents\n+        self.depth = config.resampler_depth\n+        self.rms_norm_eps = config.rms_norm_eps\n \n         self.input_latents_norm = Idefics2RMSNorm(self.hidden_size, eps=self.rms_norm_eps)\n         self.input_context_norm = Idefics2RMSNorm(self.hidden_size, eps=self.rms_norm_eps)\n         self.self_attn = IDEFICS2_PERCEIVER_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx=layer_idx)\n         self.post_attention_layernorm = Idefics2RMSNorm(self.hidden_size, eps=self.rms_norm_eps)\n         self.mlp = Idefics2MLP(\n-            hidden_size=config.text_config.hidden_size,\n-            intermediate_size=config.text_config.hidden_size * 4,\n-            output_size=config.text_config.hidden_size,\n-            hidden_act=config.perceiver_config.hidden_act,\n+            hidden_size=config.hidden_size,\n+            intermediate_size=config.hidden_size * 4,\n+            output_size=config.hidden_size,\n+            hidden_act=config.hidden_act,\n         )\n \n     def forward(\n@@ -987,20 +1064,37 @@ def forward(\n         return outputs\n \n \n-class Idefics2PerceiverResampler(nn.Module):\n+IDEFICS2_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        context (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_dim)`):\n+            The hidden states of the image after vision encoder and modality projection.\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"Idefics2 perceiver resampler model that performs `depth` blocks of cross-attention with a fixed \",\n+    \"`n_latents` inputs to decrease embedding sequence length. The Resampler acts as a form of learned pooling and \",\n+    \"is derived from [Perceiver: General Perception with Iterative Attention](https://arxiv.org/abs/2103.03206)\",\n+    IDEFICS2_START_DOCSTRING,\n+)\n+class Idefics2PerceiverResampler(Idefics2PreTrainedModel):\n+    _supports_sdpa = False\n+    config_class = Idefics2PerceiverConfig\n+\n     def __init__(self, config) -> None:\n-        \"\"\"\n-        Instantiates a Perceiver Resampler that operates over a sequence of embeddings (say from a ResNet or ViT or\n-        MAE) of a given dimension, performs `depth` blocks of cross-attention with a fixed `n_latents` inputs, then\n-        returns a Tensor of shape [bsz, n_latents, embed_dim]. The Resampler acts as a form of learned pooling and\n-        is derived from [Perceiver: General Perception with Iterative Attention](https://arxiv.org/abs/2103.03206).\n-        \"\"\"\n-        super().__init__()\n-        self.hidden_size = config.text_config.hidden_size\n-        self.hidden_act = config.perceiver_config.hidden_act\n-        self.n_latents = config.perceiver_config.resampler_n_latents\n-        self.depth = config.perceiver_config.resampler_depth\n-        self.rms_norm_eps = config.text_config.rms_norm_eps\n+        super().__init__(config)\n+        self.hidden_size = config.hidden_size\n+        self.hidden_act = config.hidden_act\n+        self.n_latents = config.resampler_n_latents\n+        self.depth = config.resampler_depth\n+        self.rms_norm_eps = config.rms_norm_eps\n \n         # Create Latents for Perceiver\n         self.latents = nn.Parameter(torch.ones(self.n_latents, self.hidden_size))\n@@ -1014,7 +1108,7 @@ def __init__(self, config) -> None:\n     def forward(\n         self,\n         context: torch.Tensor,\n-        attention_mask,\n+        attention_mask: torch.Tensor,\n     ) -> torch.Tensor:\n         # seq embed -> bsz seq embed\n         latents = self.latents.unsqueeze(0).expand((context.shape[0], *self.latents.size()))\n@@ -1057,88 +1151,14 @@ def __init__(self, config):\n             output_size=config.text_config.hidden_size,\n             hidden_act=config.text_config.hidden_act,\n         )\n-        self.perceiver_resampler = Idefics2PerceiverResampler(config)\n+        self.perceiver_resampler = Idefics2PerceiverResampler._from_config(config.perceiver_config)\n \n     def forward(self, image_hidden_states, attention_mask):\n         image_hidden_states = self.modality_projection(image_hidden_states)\n         image_hidden_states = self.perceiver_resampler(context=image_hidden_states, attention_mask=attention_mask)\n         return image_hidden_states\n \n \n-IDEFICS2_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`Idefics2Config`] or [`Idefics2VisionConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Idefics2 Model outputting raw hidden-states without any specific head on top.\",\n-    IDEFICS2_START_DOCSTRING,\n-)\n-class Idefics2PreTrainedModel(PreTrainedModel):\n-    config_class = Idefics2Config\n-    base_model_prefix = \"model\"\n-    supports_gradient_checkpointing = True\n-    _no_split_modules = [\"Idefics2VisionAttention\", \"Idefics2MLP\", \"Idefics2PerceiverLayer\", \"Idefics2DecoderLayer\"]\n-    _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_cache_class = True\n-\n-    def _init_weights(self, module):\n-        std = (\n-            self.config.initializer_range\n-            if hasattr(self.config, \"initializer_range\")\n-            else self.config.text_config.initializer_range\n-        )\n-\n-        if hasattr(module, \"class_embedding\"):\n-            module.class_embedding.data.normal_(mean=0.0, std=std)\n-\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-\n-    @classmethod\n-    def _autoset_attn_implementation(\n-        cls,\n-        config,\n-        use_flash_attention_2: bool = False,\n-        torch_dtype: Optional[torch.dtype] = None,\n-        device_map: Optional[Union[str, Dict[str, int]]] = None,\n-        check_device_map: bool = True,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Overrides the method in `PreTrainedModel` to update the vision config with the correct attention implementation\n-        \"\"\"\n-        config = super()._autoset_attn_implementation(\n-            config=config,\n-            use_flash_attention_2=use_flash_attention_2,\n-            torch_dtype=torch_dtype,\n-            device_map=device_map,\n-            check_device_map=check_device_map,\n-            **kwargs,\n-        )\n-        config.vision_config._attn_implementation = config._attn_implementation\n-        return config\n-\n-\n IDEFICS2_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1219,14 +1239,14 @@ def __init__(self, config: Idefics2Config):\n         self.padding_idx = self.config.text_config.pad_token_id\n         self.vocab_size = self.config.text_config.vocab_size\n \n-        self.vision_model = Idefics2VisionTransformer(config.vision_config)\n+        self.vision_model = Idefics2VisionTransformer._from_config(config.vision_config)\n         self.connector = Idefics2Connector(config)\n-        self.text_model = AutoModel.from_config(config.text_config, attn_implementation=config._attn_implementation)\n+        self.text_model = AutoModel.from_config(config.text_config)\n \n         self.image_seq_len = config.perceiver_config.resampler_n_latents\n         self.image_token_id = self.config.image_token_id\n \n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n+        self._use_flash_attention_2 = config.text_config._attn_implementation == \"flash_attention_2\"\n \n         self.post_init()\n "
        },
        {
            "sha": "31d43948fbd5659dcbf3941e79b4fedf353d6e34",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -621,12 +621,13 @@ class Idefics3PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Idefics3VisionAttention\", \"Idefics3DecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n     _supports_cache_class = True\n \n     # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2PreTrainedModel._init_weights\n     def _init_weights(self, module):\n         std = (\n-            self.config.initializer_range\n+            self.config.text_config.initializer_range\n             if hasattr(self.config, \"initializer_range\")\n             else self.config.text_config.initializer_range\n         )\n@@ -667,6 +668,7 @@ def _init_weights(self, module):\n )\n class Idefics3VisionTransformer(Idefics3PreTrainedModel):\n     config_class = Idefics3VisionConfig\n+    _supports_sdpa = False\n \n     def __init__(self, config: Idefics3VisionConfig):\n         super().__init__(config)\n@@ -824,18 +826,16 @@ def __init__(self, config: Idefics3Config):\n         self.padding_idx = self.config.text_config.pad_token_id\n         self.vocab_size = self.config.text_config.vocab_size\n \n-        self.vision_model = Idefics3VisionTransformer._from_config(\n-            config.vision_config, attn_implementation=config._attn_implementation\n-        )\n+        self.vision_model = Idefics3VisionTransformer._from_config(config.vision_config)\n         self.connector = Idefics3Connector(config)\n-        self.text_model = AutoModel.from_config(config.text_config, attn_implementation=config._attn_implementation)\n+        self.text_model = AutoModel.from_config(config.text_config)\n \n         self.image_seq_len = int(\n             ((config.vision_config.image_size // config.vision_config.patch_size) ** 2) / (config.scale_factor**2)\n         )\n         self.image_token_id = self.config.image_token_id\n \n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n+        self._use_flash_attention_2 = config.text_config._attn_implementation == \"flash_attention_2\"\n \n         self.post_init()\n "
        },
        {
            "sha": "5cce774ce0716a37eec227ee14010dfa0cebcb96",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -315,6 +315,7 @@ class InstructBlipPreTrainedModel(PreTrainedModel):\n     config_class = InstructBlipConfig\n     base_model_prefix = \"blip\"\n     supports_gradient_checkpointing = True\n+\n     _no_split_modules = [\n         \"InstructBlipQFormerEmbeddings\",\n         \"InstructBlipAttention\",\n@@ -1298,13 +1299,9 @@ def __init__(self, config: InstructBlipConfig):\n         self.language_projection = nn.Linear(config.qformer_config.hidden_size, config.text_config.hidden_size)\n \n         if config.use_decoder_only_language_model:\n-            language_model = AutoModelForCausalLM.from_config(\n-                config.text_config, attn_implementation=config._attn_implementation\n-            )\n+            language_model = AutoModelForCausalLM.from_config(config.text_config)\n         else:\n-            language_model = AutoModelForSeq2SeqLM.from_config(\n-                config.text_config, attn_implementation=config._attn_implementation\n-            )\n+            language_model = AutoModelForSeq2SeqLM.from_config(config.text_config)\n \n         if language_model._no_split_modules is not None:\n             self._no_split_modules.extend(language_model._no_split_modules)"
        },
        {
            "sha": "c9f12391666c2257ee354dd37ec7170f70b1622b",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -317,6 +317,7 @@ class InstructBlipVideoPreTrainedModel(PreTrainedModel):\n     config_class = InstructBlipVideoConfig\n     base_model_prefix = \"blip\"\n     supports_gradient_checkpointing = True\n+\n     _no_split_modules = [\n         \"InstructBlipVideoQFormerEmbeddings\",\n         \"InstructBlipVideoAttention\",\n@@ -1292,13 +1293,9 @@ def __init__(self, config: InstructBlipVideoConfig):\n         self.language_projection = nn.Linear(config.qformer_config.hidden_size, config.text_config.hidden_size)\n \n         if config.use_decoder_only_language_model:\n-            language_model = AutoModelForCausalLM.from_config(\n-                config.text_config, attn_implementation=config._attn_implementation\n-            )\n+            language_model = AutoModelForCausalLM.from_config(config.text_config)\n         else:\n-            language_model = AutoModelForSeq2SeqLM.from_config(\n-                config.text_config, attn_implementation=config._attn_implementation\n-            )\n+            language_model = AutoModelForSeq2SeqLM.from_config(config.text_config)\n \n         if language_model._no_split_modules is not None:\n             self._no_split_modules.extend(language_model._no_split_modules)"
        },
        {
            "sha": "c17d35296a9c772c835a288d4bf5dbd7887ce27f",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 12,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -125,8 +125,9 @@ class LlavaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlavaVisionAttention\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n     _supports_cache_class = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n \n     def _init_weights(self, module):\n         # important: this ported version of Llava isn't meant for training from scratch - only\n@@ -150,14 +151,6 @@ def _init_weights(self, module):\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n \n-    @property\n-    def _supports_sdpa(self):\n-        \"\"\"\n-        Retrieve language_model's attribute to check whether the model supports\n-        SDPA or not.\n-        \"\"\"\n-        return self.language_model._supports_sdpa\n-\n \n LLAVA_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n@@ -245,9 +238,7 @@ def __init__(self, config: LlavaConfig):\n \n         self.multi_modal_projector = LlavaMultiModalProjector(config)\n         self.vocab_size = config.text_config.vocab_size\n-        self.language_model = AutoModelForCausalLM.from_config(\n-            config.text_config, attn_implementation=config._attn_implementation\n-        )\n+        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n         self.post_init()\n "
        },
        {
            "sha": "04ff098170b7a3e84534290ad3f4dc5d756687e2",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 3,
            "deletions": 12,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -234,8 +234,9 @@ class LlavaNextPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlavaNextVisionAttention\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n     _supports_cache_class = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n \n     def _init_weights(self, module):\n         # important: this ported version of LlavaNext isn't meant for training from scratch - only\n@@ -259,14 +260,6 @@ def _init_weights(self, module):\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n \n-    @property\n-    def _supports_sdpa(self):\n-        \"\"\"\n-        Retrieve language_model's attribute to check whether the model supports\n-        SDPA or not.\n-        \"\"\"\n-        return self.language_model._supports_sdpa\n-\n \n LLAVA_NEXT_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n@@ -360,9 +353,7 @@ def __init__(self, config: LlavaNextConfig):\n         self.image_newline = nn.Parameter(torch.randn(config.text_config.hidden_size, dtype=self.dtype) * embed_std)\n \n         self.vocab_size = config.text_config.vocab_size\n-        self.language_model = AutoModelForCausalLM.from_config(\n-            config.text_config, attn_implementation=config._attn_implementation\n-        )\n+        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n         self._padding_side = \"left\"  # set it to left by default, user can use setter to change padding_sides\n         self.post_init()"
        },
        {
            "sha": "8d3bfb1efa4e85c84ca4760b91bcb99e3c5c7ae4",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 3,
            "deletions": 12,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -277,8 +277,9 @@ class LlavaNextVideoPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlavaNextVideoVisionAttention\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n     _supports_cache_class = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n \n     def _init_weights(self, module):\n         # important: this ported version of LlavaNextVideo isn't meant for training from scratch - only\n@@ -302,14 +303,6 @@ def _init_weights(self, module):\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n \n-    @property\n-    def _supports_sdpa(self):\n-        \"\"\"\n-        Retrieve language_model's attribute to check whether the model supports\n-        SDPA or not.\n-        \"\"\"\n-        return self.language_model._supports_sdpa\n-\n \n LLAVA_NEXT_VIDEO_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n@@ -406,9 +399,7 @@ def __init__(\n         self.image_newline = nn.Parameter(torch.randn(config.text_config.hidden_size, dtype=self.dtype) * embed_std)\n \n         self.vocab_size = config.text_config.vocab_size\n-        self.language_model = AutoModelForCausalLM.from_config(\n-            config.text_config, attn_implementation=config._attn_implementation\n-        )\n+        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n         self._padding_side = \"left\"  # set it to left by default, user can use setter to change padding_sides\n         self.vision_resampler = LlavaNextVideoPooler(config)"
        },
        {
            "sha": "2c5fa511467aff15d0ee4fd790df5c741c7a163a",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -363,18 +363,14 @@ def _init_weights(self, module):\n class LlavaOnevisionForConditionalGeneration(LlavaOnevisionPreTrainedModel, GenerationMixin):\n     def __init__(self, config: LlavaOnevisionConfig):\n         super().__init__(config)\n-        self.vision_tower = AutoModel.from_config(\n-            config.vision_config, attn_implementation=config._attn_implementation\n-        )\n+        self.vision_tower = AutoModel.from_config(config.vision_config)\n \n         self.multi_modal_projector = LlavaOnevisionMultiModalProjector(config)\n         embed_std = 1 / math.sqrt(config.text_config.hidden_size)\n         self.image_newline = nn.Parameter(torch.randn(config.text_config.hidden_size, dtype=self.dtype) * embed_std)\n \n         self.vocab_size = config.text_config.vocab_size\n-        self.language_model = AutoModelForCausalLM.from_config(\n-            config.text_config, attn_implementation=config._attn_implementation\n-        )\n+        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n         self.post_init()\n \n     # Copied from transformers.models.llava_next.modeling_llava_next.LlavaNextForConditionalGeneration.get_input_embeddings"
        },
        {
            "sha": "c5ae615a12b5cce4665ad0c4e66be4d1d5638a06",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -1979,12 +1979,8 @@ def __init__(self, config: MllamaConfig):\n         self.vision_output_dim = config.vision_config.vision_output_dim\n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n \n-        self.vision_model = MllamaVisionModel._from_config(\n-            config.vision_config, attn_implementation=config._attn_implementation\n-        )\n-        self.language_model = MllamaForCausalLM._from_config(\n-            config.text_config, attn_implementation=config._attn_implementation\n-        )\n+        self.vision_model = MllamaVisionModel._from_config(config.vision_config)\n+        self.language_model = MllamaForCausalLM._from_config(config.text_config)\n         self.multi_modal_projector = nn.Linear(\n             config.vision_config.vision_output_dim,\n             config.text_config.hidden_size,"
        },
        {
            "sha": "0d282355defa96c9167eae46cbdb668ae6afbe36",
            "filename": "src/transformers/models/musicgen/configuration_musicgen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fconfiguration_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fconfiguration_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fconfiguration_musicgen.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -236,20 +236,3 @@ def from_sub_models_config(\n     # This is a property because you might want to change the codec model on the fly\n     def sampling_rate(self):\n         return self.audio_encoder.sampling_rate\n-\n-    @property\n-    def _attn_implementation(self):\n-        # This property is made private for now (as it cannot be changed and a PreTrainedModel.use_attn_implementation method needs to be implemented.)\n-        if hasattr(self, \"_attn_implementation_internal\"):\n-            if self._attn_implementation_internal is None:\n-                # `config.attn_implementation` should never be None, for backward compatibility.\n-                return \"eager\"\n-            else:\n-                return self._attn_implementation_internal\n-        else:\n-            return \"eager\"\n-\n-    @_attn_implementation.setter\n-    def _attn_implementation(self, value):\n-        self._attn_implementation_internal = value\n-        self.decoder._attn_implementation = value"
        },
        {
            "sha": "c18e1d1c9d86b11361695a8ed090b49f4f3ee675",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -1713,7 +1713,7 @@ def __init__(\n             audio_encoder = AutoModel.from_config(config.audio_encoder)\n \n         if decoder is None:\n-            decoder = MusicgenForCausalLM(config.decoder)\n+            decoder = MusicgenForCausalLM._from_config(config.decoder)\n \n         self.text_encoder = text_encoder\n         self.audio_encoder = audio_encoder\n@@ -1737,6 +1737,9 @@ def __init__(\n \n         # make sure that the individual model's config refers to the shared config\n         # so that the updates to the config will be synced\n+        self.config.text_encoder._attn_implementation = self.text_encoder.config._attn_implementation\n+        self.config.audio_encoder._attn_implementation = self.audio_encoder.config._attn_implementation\n+        self.config.decoder._attn_implementation = self.decoder.config._attn_implementation\n         self.text_encoder.config = self.config.text_encoder\n         self.audio_encoder.config = self.config.audio_encoder\n         self.decoder.config = self.config.decoder"
        },
        {
            "sha": "8a77cea0252234116fd8861a47fc703070683f4b",
            "filename": "src/transformers/models/musicgen_melody/configuration_musicgen_melody.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fconfiguration_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fconfiguration_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fconfiguration_musicgen_melody.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -250,20 +250,3 @@ def from_sub_models_config(\n     # This is a property because you might want to change the codec model on the fly\n     def sampling_rate(self):\n         return self.audio_encoder.sampling_rate\n-\n-    @property\n-    def _attn_implementation(self):\n-        # This property is made private for now (as it cannot be changed and a PreTrainedModel.use_attn_implementation method needs to be implemented.)\n-        if hasattr(self, \"_attn_implementation_internal\"):\n-            if self._attn_implementation_internal is None:\n-                # `config.attn_implementation` should never be None, for backward compatibility.\n-                return \"eager\"\n-            else:\n-                return self._attn_implementation_internal\n-        else:\n-            return \"eager\"\n-\n-    @_attn_implementation.setter\n-    def _attn_implementation(self, value):\n-        self._attn_implementation_internal = value\n-        self.decoder._attn_implementation = value"
        },
        {
            "sha": "d2f339afc41451e47c3d87ef5dbf481319d0ff0c",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -1628,14 +1628,17 @@ def __init__(\n             audio_encoder = AutoModel.from_config(config.audio_encoder)\n \n         if decoder is None:\n-            decoder = MusicgenMelodyForCausalLM(config.decoder)\n+            decoder = MusicgenMelodyForCausalLM._from_config(config.decoder)\n \n         self.text_encoder = text_encoder\n         self.audio_encoder = audio_encoder\n         self.decoder = decoder\n \n         # make sure that the individual model's config refers to the shared config\n         # so that the updates to the config will be synced\n+        self.config.text_encoder._attn_implementation = self.text_encoder.config._attn_implementation\n+        self.config.audio_encoder._attn_implementation = self.audio_encoder.config._attn_implementation\n+        self.config.decoder._attn_implementation = self.decoder.config._attn_implementation\n         self.text_encoder.config = self.config.text_encoder\n         self.audio_encoder.config = self.config.audio_encoder\n         self.decoder.config = self.config.decoder"
        },
        {
            "sha": "0f44e4bd40208c71b58a46114d80dd9202906520",
            "filename": "src/transformers/models/omdet_turbo/modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -288,7 +288,7 @@ def put(self, key, value) -> None:\n class OmDetTurboLanguageBackbone(nn.Module):\n     def __init__(self, config: OmDetTurboConfig):\n         super().__init__()\n-        self.model = AutoModel.from_config(config.text_config, attn_implementation=config._attn_implementation)\n+        self.model = AutoModel.from_config(config.text_config)\n         self.text_projection = nn.Parameter(torch.zeros(config.text_projection_in_dim, config.text_projection_out_dim))\n \n     def forward(self, hidden_states, mask=None, encode_type=\"task\"):"
        },
        {
            "sha": "ffb4b7435f2a2ac4beff263c1e9ee70866f349f9",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 8,
            "deletions": 14,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -193,12 +193,12 @@ class PaliGemmaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"PaliGemmaMultiModalProjector\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = False\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n-    _supports_sdpa = True\n     _supports_cache_class = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n \n     def _init_weights(self, module):\n         # important: this ported version of PaliGemmaisn't meant for training from scratch - only\n@@ -221,14 +221,6 @@ def _init_weights(self, module):\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n \n-    @property\n-    def _supports_sdpa(self):\n-        \"\"\"\n-        Retrieve language_model's attribute to check whether the model supports\n-        SDPA or not.\n-        \"\"\"\n-        return self.language_model._supports_sdpa\n-\n \n PALIGEMMA_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n@@ -310,11 +302,8 @@ def __init__(self, config: PaliGemmaConfig):\n         self.vision_tower = AutoModel.from_config(config=config.vision_config)\n         self.multi_modal_projector = PaliGemmaMultiModalProjector(config)\n         self.vocab_size = config.text_config.vocab_size\n-        self._attn_implementation = config._attn_implementation\n \n-        language_model = AutoModelForCausalLM.from_config(\n-            config=config.text_config, attn_implementation=self._attn_implementation\n-        )\n+        language_model = AutoModelForCausalLM.from_config(config=config.text_config)\n \n         if language_model._tied_weights_keys is not None:\n             self._tied_weights_keys = [f\"language_model.{k}\" for k in language_model._tied_weights_keys]\n@@ -354,6 +343,11 @@ def tie_weights(self):\n     def _update_causal_mask(\n         self, attention_mask, token_type_ids, inputs_embeds, past_key_values, cache_position, is_training: bool = False\n     ):\n+        if self.config.text_config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and 0.0 in attention_mask:\n+                return attention_mask\n+            return None\n+\n         using_static_cache = isinstance(past_key_values, StaticCache)\n         dtype = inputs_embeds.dtype\n         min_dtype = torch.finfo(dtype).min"
        },
        {
            "sha": "ce0e427048cf232b1aef3c19c2c60497638ce2a8",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 3,
            "deletions": 12,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -544,6 +544,7 @@ class Qwen2AudioPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Qwen2AudioAttention\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n \n     def _init_weights(self, module):\n         # important: this ported version of Qwen2Audio isn't meant for training from scratch - only\n@@ -559,14 +560,6 @@ def _init_weights(self, module):\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n \n-    @property\n-    def _supports_sdpa(self):\n-        \"\"\"\n-        Retrieve language_model's attribute to check whether the model supports\n-        SDPA or not.\n-        \"\"\"\n-        return self.language_model._supports_sdpa\n-\n \n QWEN2AUDIOENCODER_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n@@ -859,13 +852,11 @@ def forward(self, audio_features):\n class Qwen2AudioForConditionalGeneration(Qwen2AudioPreTrainedModel, GenerationMixin):\n     def __init__(self, config: Qwen2AudioConfig):\n         super().__init__(config)\n-        self.audio_tower = AutoModel.from_config(config.audio_config, attn_implementation=config._attn_implementation)\n+        self.audio_tower = AutoModel.from_config(config.audio_config)\n \n         self.multi_modal_projector = Qwen2AudioMultiModalProjector(config)\n         self.vocab_size = config.text_config.vocab_size\n-        self.language_model = AutoModelForCausalLM.from_config(\n-            config.text_config, attn_implementation=config._attn_implementation\n-        )\n+        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n         self._padding_side = \"left\"  # set it to left by default, user can use setter to change padding_sides\n         self.post_init()"
        },
        {
            "sha": "07531248f63b1d5820985d5a0c6964e153ded5c1",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -1443,9 +1443,7 @@ class Qwen2VLForConditionalGeneration(Qwen2VLPreTrainedModel, GenerationMixin):\n \n     def __init__(self, config):\n         super().__init__(config)\n-        self.visual = Qwen2VisionTransformerPretrainedModel._from_config(\n-            config.vision_config, attn_implementation=config._attn_implementation\n-        )\n+        self.visual = Qwen2VisionTransformerPretrainedModel._from_config(config.vision_config)\n         self.model = Qwen2VLModel(config)\n         self.vocab_size = config.vocab_size\n         self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)"
        },
        {
            "sha": "dfc2664b78a3dc6f80a0910a7923b77646c0f60d",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -232,6 +232,8 @@ class RagPreTrainedModel(PreTrainedModel):\n \n     config_class = RagConfig\n     base_model_prefix = \"rag\"\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n \n     @classmethod\n     def from_pretrained(cls, *args, **kwargs):\n@@ -506,16 +508,12 @@ def __init__(\n         if question_encoder is None:\n             from ..auto.modeling_auto import AutoModel\n \n-            question_encoder = AutoModel.from_config(\n-                config.question_encoder, attn_implementation=config._attn_implementation\n-            )\n+            question_encoder = AutoModel.from_config(config.question_encoder)\n \n         if generator is None:\n             from ..auto.modeling_auto import AutoModelForSeq2SeqLM\n \n-            generator = AutoModelForSeq2SeqLM.from_config(\n-                config.generator, attn_implementation=config._attn_implementation\n-            )\n+            generator = AutoModelForSeq2SeqLM.from_config(config.generator)\n \n         self.retriever = retriever\n         if self.retriever is not None:"
        },
        {
            "sha": "a3d06cbb4792b445ad6cb9e1bd3fba8af7740c86",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -669,6 +669,7 @@ class SiglipPreTrainedModel(PreTrainedModel):\n     config_class = SiglipConfig\n     base_model_prefix = \"siglip\"\n     supports_gradient_checkpointing = True\n+\n     _no_split_modules = [\n         \"SiglipTextEmbeddings\",\n         \"SiglipEncoderLayer\",\n@@ -1218,8 +1219,8 @@ def __init__(self, config: SiglipConfig):\n         vision_config = config.vision_config\n \n         # First, initialize the text and vision models with proper attention implementation\n-        text_model = SiglipTextModel._from_config(text_config, attn_implementation=config._attn_implementation)\n-        vision_model = SiglipVisionModel._from_config(vision_config, attn_implementation=config._attn_implementation)\n+        text_model = SiglipTextModel._from_config(text_config)\n+        vision_model = SiglipVisionModel._from_config(vision_config)\n \n         # Second, get the text and vision submodules (for backward compatibility)\n         self.text_model = text_model.text_model\n@@ -1454,9 +1455,7 @@ def __init__(self, config: SiglipConfig) -> None:\n \n         # Create the vision model with proper attention\n         # and take only vision_model submodule (for backward compatibility)\n-        vision_model = SiglipVisionModel._from_config(\n-            config.vision_config, attn_implementation=config._attn_implementation\n-        )\n+        vision_model = SiglipVisionModel._from_config(config.vision_config)\n         self.vision_model = vision_model.vision_model\n \n         # Classifier head"
        },
        {
            "sha": "0d2b911bebe582daf1cdd469b3366219330d128c",
            "filename": "src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -183,6 +183,8 @@ class SpeechEncoderDecoderModel(PreTrainedModel, GenerationMixin):\n     main_input_name = \"inputs\"\n     supports_gradient_checkpointing = True\n     _supports_param_buffer_assignment = False\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n \n     def __init__(\n         self,\n@@ -213,10 +215,10 @@ def __init__(\n         super().__init__(config)\n \n         if encoder is None:\n-            encoder = AutoModel.from_config(config.encoder, attn_implementation=config._attn_implementation)\n+            encoder = AutoModel.from_config(config.encoder)\n \n         if decoder is None:\n-            decoder = AutoModelForCausalLM.from_config(config.decoder, attn_implementation=config._attn_implementation)\n+            decoder = AutoModelForCausalLM.from_config(config.decoder)\n \n         self.encoder = encoder\n         self.decoder = decoder\n@@ -234,6 +236,8 @@ def __init__(\n \n         # make sure that the individual model's config refers to the shared config\n         # so that the updates to the config will be synced\n+        self.config.encoder._attn_implementation = self.encoder.config._attn_implementation\n+        self.config.decoder._attn_implementation = self.decoder.config._attn_implementation\n         self.encoder.config = self.config.encoder\n         self.decoder.config = self.config.decoder\n "
        },
        {
            "sha": "b455040059e6535c3a82f21cbd80cfbb391504f5",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 12,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -126,8 +126,9 @@ class VideoLlavaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"VideoLlavaVisionAttention\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n     _supports_cache_class = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n \n     def _init_weights(self, module):\n         std = (\n@@ -148,14 +149,6 @@ def _init_weights(self, module):\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n \n-    @property\n-    def _supports_sdpa(self):\n-        \"\"\"\n-        Retrieve language_model's attribute to check whether the model supports\n-        SDPA or not.\n-        \"\"\"\n-        return self.language_model._supports_sdpa\n-\n \n VIDEO_LLAVA_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n@@ -248,9 +241,7 @@ def __init__(self, config: VideoLlavaConfig):\n \n         self.multi_modal_projector = VideoLlavaMultiModalProjector(config)\n         self.vocab_size = config.text_config.vocab_size\n-        self.language_model = AutoModelForCausalLM.from_config(\n-            config.text_config, attn_implementation=config._attn_implementation\n-        )\n+        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n         self.post_init()\n "
        },
        {
            "sha": "10935c0b63e076d5f554adb325b69edb53f17a68",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 12,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -132,8 +132,9 @@ class VipLlavaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"VipLlavaVisionAttention\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n     _supports_cache_class = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n \n     def _init_weights(self, module):\n         # important: this ported version of VipLlava isn't meant for training from scratch - only\n@@ -157,14 +158,6 @@ def _init_weights(self, module):\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n \n-    @property\n-    def _supports_sdpa(self):\n-        \"\"\"\n-        Retrieve language_model's attribute to check whether the model supports\n-        SDPA or not.\n-        \"\"\"\n-        return self.language_model._supports_sdpa\n-\n \n VIPLLAVA_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n@@ -248,9 +241,7 @@ def __init__(self, config: VipLlavaConfig):\n \n         self.multi_modal_projector = VipLlavaMultiModalProjector(config)\n         self.vocab_size = config.text_config.vocab_size\n-        self.language_model = AutoModelForCausalLM.from_config(\n-            config.text_config, attn_implementation=config._attn_implementation\n-        )\n+        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n         self.post_init()\n "
        },
        {
            "sha": "152a96014033019457782996b6d2b22fa8657bbc",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -161,6 +161,8 @@ class VisionEncoderDecoderModel(PreTrainedModel, GenerationMixin):\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n     _supports_param_buffer_assignment = False\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n \n     def __init__(\n         self,\n@@ -191,10 +193,10 @@ def __init__(\n         super().__init__(config)\n \n         if encoder is None:\n-            encoder = AutoModel.from_config(config.encoder, attn_implementation=config._attn_implementation)\n+            encoder = AutoModel.from_config(config.encoder)\n \n         if decoder is None:\n-            decoder = AutoModelForCausalLM.from_config(config.decoder, attn_implementation=config._attn_implementation)\n+            decoder = AutoModelForCausalLM.from_config(config.decoder)\n \n         self.encoder = encoder\n         self.decoder = decoder\n@@ -212,6 +214,8 @@ def __init__(\n \n         # make sure that the individual model's config refers to the shared config\n         # so that the updates to the config will be synced\n+        self.config.encoder._attn_implementation = self.encoder.config._attn_implementation\n+        self.config.decoder._attn_implementation = self.decoder.config._attn_implementation\n         self.encoder.config = self.config.encoder\n         self.decoder.config = self.config.decoder\n "
        },
        {
            "sha": "4b39de3df1c882f58f9891eceb4824145cb2d285",
            "filename": "src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -161,6 +161,8 @@ def clip_loss(similarity: torch.Tensor) -> torch.Tensor:\n class VisionTextDualEncoderModel(PreTrainedModel):\n     config_class = VisionTextDualEncoderConfig\n     base_model_prefix = \"vision_text_dual_encoder\"\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n \n     def __init__(\n         self,\n@@ -184,18 +186,18 @@ def __init__(\n             if isinstance(config.vision_config, CLIPVisionConfig):\n                 vision_model = CLIPVisionModel(config.vision_config)\n             else:\n-                vision_model = AutoModel.from_config(\n-                    config.vision_config, attn_implementation=config._attn_implementation\n-                )\n+                vision_model = AutoModel.from_config(config.vision_config)\n \n         if text_model is None:\n-            text_model = AutoModel.from_config(config.text_config, attn_implementation=config._attn_implementation)\n+            text_model = AutoModel.from_config(config.text_config)\n \n         self.vision_model = vision_model\n         self.text_model = text_model\n \n         # make sure that the individual model's config refers to the shared config\n         # so that the updates to the config will be synced\n+        self.config.vision_config._attn_implementation = self.vision_model.config._attn_implementation\n+        self.config.text_config._attn_implementation = self.text_model.config._attn_implementation\n         self.vision_model.config = self.config.vision_config\n         self.text_model.config = self.config.text_config\n "
        },
        {
            "sha": "bb08acfc0bba670fbdb0f09cc41f56f51d6613bf",
            "filename": "src/transformers/models/vit/modeling_vit.py",
            "status": "modified",
            "additions": 17,
            "deletions": 1,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -250,8 +250,24 @@ def __init__(self, config: ViTConfig) -> None:\n         self.attention_probs_dropout_prob = config.attention_probs_dropout_prob\n \n     def forward(\n-        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n+        self,\n+        hidden_states: torch.FloatTensor,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n     ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n+        if output_attentions or head_mask is not None:\n+            logger.warning_once(\n+                \"`ViTSdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n+                \"`output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but \"\n+                \"specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n+                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                head_mask=head_mask,\n+                output_attentions=output_attentions,\n+            )\n+\n         mixed_query_layer = self.query(hidden_states)\n \n         key_layer = self.transpose_for_scores(self.key(hidden_states))"
        },
        {
            "sha": "e319f2f655aabf0596baf6d5509887e368b68a9a",
            "filename": "src/transformers/models/vit_mae/modeling_vit_mae.py",
            "status": "modified",
            "additions": 17,
            "deletions": 1,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -424,8 +424,24 @@ def __init__(self, config: ViTMAEConfig) -> None:\n         self.attention_probs_dropout_prob = config.attention_probs_dropout_prob\n \n     def forward(\n-        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n+        self,\n+        hidden_states: torch.FloatTensor,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n     ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n+        if output_attentions or head_mask is not None:\n+            logger.warning_once(\n+                \"`ViTMAESdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n+                \"`output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but \"\n+                \"specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n+                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                head_mask=head_mask,\n+                output_attentions=output_attentions,\n+            )\n+\n         mixed_query_layer = self.query(hidden_states)\n \n         key_layer = self.transpose_for_scores(self.key(hidden_states))"
        },
        {
            "sha": "39274dd28fef5b05ab1e562df2cb54fdfbb8239c",
            "filename": "src/transformers/models/vit_msn/modeling_vit_msn.py",
            "status": "modified",
            "additions": 17,
            "deletions": 1,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -241,8 +241,24 @@ def __init__(self, config: ViTMSNConfig) -> None:\n         self.attention_probs_dropout_prob = config.attention_probs_dropout_prob\n \n     def forward(\n-        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n+        self,\n+        hidden_states: torch.FloatTensor,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n     ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n+        if output_attentions or head_mask is not None:\n+            logger.warning_once(\n+                \"`ViTMSNSdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n+                \"`output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but \"\n+                \"specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n+                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                head_mask=head_mask,\n+                output_attentions=output_attentions,\n+            )\n+\n         mixed_query_layer = self.query(hidden_states)\n \n         key_layer = self.transpose_for_scores(self.key(hidden_states))"
        },
        {
            "sha": "f7ef3e55f5f7998cda126428143a526d2586d1d6",
            "filename": "src/transformers/models/yolos/modeling_yolos.py",
            "status": "modified",
            "additions": 17,
            "deletions": 1,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -299,8 +299,24 @@ def __init__(self, config: YolosConfig) -> None:\n         self.attention_probs_dropout_prob = config.attention_probs_dropout_prob\n \n     def forward(\n-        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n+        self,\n+        hidden_states: torch.FloatTensor,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n     ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n+        if output_attentions or head_mask is not None:\n+            logger.warning_once(\n+                \"`YolosSdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n+                \"`output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but \"\n+                \"specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n+                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                head_mask=head_mask,\n+                output_attentions=output_attentions,\n+            )\n+\n         mixed_query_layer = self.query(hidden_states)\n \n         key_layer = self.transpose_for_scores(self.key(hidden_states))"
        },
        {
            "sha": "e5d04bd85a34043f6b000129a65f20af1dd8cc55",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 123,
            "deletions": 0,
            "changes": 123,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -27,6 +27,7 @@\n     require_torch_fp16,\n     require_torch_gpu,\n     require_torch_multi_accelerator,\n+    require_torch_sdpa,\n     require_vision,\n     slow,\n     torch_device,\n@@ -456,6 +457,7 @@ class Blip2ForConditionalGenerationDecoderOnlyTest(ModelTesterMixin, GenerationT\n     test_resize_embeddings = False\n     test_attention_outputs = False\n     test_torchscript = False\n+    _is_composite = True\n \n     def setUp(self):\n         self.model_tester = Blip2ForConditionalGenerationDecoderOnlyModelTester(self)\n@@ -488,6 +490,66 @@ def test_save_load_fast_init_from_base(self):\n     def test_save_load_fast_init_to_base(self):\n         pass\n \n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        \"\"\"\n+        Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n+        This tests only by looking at layer names, as usually SDPA layers are calles \"SDPAAttention\".\n+        In contrast to the above test, this one checks if the \"config._attn_implamentation\" is a dict after the model\n+        is loaded, because we manually replicate requested attn implementation on each sub-config when loading.\n+        See https://github.com/huggingface/transformers/pull/32238 for more info\n+\n+        The test tries to cover most general cases of composite models, VLMs with vision and text configs. Any model\n+        that has a different set of sub-configs has to overwrite this test.\n+        \"\"\"\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        if not self._is_composite:\n+            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_sdpa = model_class.from_pretrained(tmpdirname)\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                text_attn = \"sdpa\" if model.language_model._supports_sdpa else \"eager\"\n+                vision_attn = \"sdpa\" if model.vision_model._supports_sdpa else \"eager\"\n+                qformer_attn = \"sdpa\" if model.qformer._supports_sdpa else \"eager\"\n+\n+                # `None` as it is the requested one which will be assigned to each sub-config\n+                # Sub-model will dispatch to SDPA if it can (checked below that `SDPA` layers are present)\n+                self.assertTrue(model.language_model.config._attn_implementation == text_attn)\n+                self.assertTrue(model.vision_model.config._attn_implementation == vision_attn)\n+                self.assertTrue(model.qformer.config._attn_implementation == qformer_attn)\n+\n+                model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n+                model_eager = model_eager.eval().to(torch_device)\n+                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.language_model.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.vision_model.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.qformer.config._attn_implementation == \"eager\")\n+\n+                for name, submodule in model_eager.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+                has_sdpa = False\n+                for name, submodule in model_sdpa.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        has_sdpa = True\n+                        break\n+                if not has_sdpa and any(\n+                    module_attn == \"sdpa\" for module_attn in [text_attn, vision_attn, qformer_attn]\n+                ):\n+                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n+\n     def test_forward_signature(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -715,6 +777,7 @@ class Blip2ModelTest(ModelTesterMixin, PipelineTesterMixin, GenerationTesterMixi\n     test_resize_embeddings = False\n     test_attention_outputs = False\n     test_torchscript = False\n+    _is_composite = True\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n@@ -768,6 +831,66 @@ def test_save_load_fast_init_to_base(self):\n     def test_cpu_offload(self):\n         pass\n \n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        \"\"\"\n+        Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n+        This tests only by looking at layer names, as usually SDPA layers are calles \"SDPAAttention\".\n+        In contrast to the above test, this one checks if the \"config._attn_implamentation\" is a dict after the model\n+        is loaded, because we manually replicate requested attn implementation on each sub-config when loading.\n+        See https://github.com/huggingface/transformers/pull/32238 for more info\n+\n+        The test tries to cover most general cases of composite models, VLMs with vision and text configs. Any model\n+        that has a different set of sub-configs has to overwrite this test.\n+        \"\"\"\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        if not self._is_composite:\n+            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_sdpa = model_class.from_pretrained(tmpdirname)\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                text_attn = \"sdpa\" if model.language_model._supports_sdpa else \"eager\"\n+                vision_attn = \"sdpa\" if model.vision_model._supports_sdpa else \"eager\"\n+                qformer_attn = \"sdpa\" if model.qformer._supports_sdpa else \"eager\"\n+\n+                # `None` as it is the requested one which will be assigned to each sub-config\n+                # Sub-model will dispatch to SDPA if it can (checked below that `SDPA` layers are present)\n+                self.assertTrue(model.language_model.config._attn_implementation == text_attn)\n+                self.assertTrue(model.vision_model.config._attn_implementation == vision_attn)\n+                self.assertTrue(model.qformer.config._attn_implementation == qformer_attn)\n+\n+                model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n+                model_eager = model_eager.eval().to(torch_device)\n+                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.language_model.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.vision_model.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.qformer.config._attn_implementation == \"eager\")\n+\n+                for name, submodule in model_eager.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+                has_sdpa = False\n+                for name, submodule in model_sdpa.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        has_sdpa = True\n+                        break\n+                if not has_sdpa and any(\n+                    module_attn == \"sdpa\" for module_attn in [text_attn, vision_attn, qformer_attn]\n+                ):\n+                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n+\n     def test_forward_signature(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "a7c8c8ef8410e84ba5f0f1355676bde34635cbfc",
            "filename": "tests/models/clip/test_modeling_clip.py",
            "status": "modified",
            "additions": 65,
            "deletions": 18,
            "changes": 83,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -191,6 +191,53 @@ class CLIPModelTesterMixin(ModelTesterMixin):\n     different output logits, and are not supposed to be used or tested with padding_side=\"left\".\n     \"\"\"\n \n+    def test_sdpa_can_dispatch_composite_models(self):\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+\n+                # Load the model with SDPA\n+                model_sdpa = model_class.from_pretrained(tmpdirname)\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                # Load model with eager attention\n+                model_eager = model_class.from_pretrained(\n+                    tmpdirname,\n+                    attn_implementation=\"eager\",\n+                )\n+                model_eager = model_eager.eval().to(torch_device)\n+\n+            # SigLip has one shared cls attr for all models, so we assign both submodels heer\n+            vision_attn = text_attn = \"sdpa\" if model._supports_sdpa else \"eager\"\n+\n+            # `None` as it is the requested one which will be assigned to each sub-config\n+            # Sub-model will dispatch to SDPA if it can (checked below that `SDPA` layers are present)\n+            if hasattr(model_sdpa, \"vision_model\") and hasattr(model_sdpa, \"text_model\"):\n+                self.assertTrue(model_sdpa.vision_model.config._attn_implementation == vision_attn)\n+                self.assertTrue(model_sdpa.text_model.config._attn_implementation == text_attn)\n+                self.assertTrue(model_eager.vision_model.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.text_model.config._attn_implementation == \"eager\")\n+\n+            self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+            self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+\n+            for name, submodule in model_eager.named_modules():\n+                class_name = submodule.__class__.__name__\n+                if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                    raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+            has_sdpa = False\n+            for name, submodule in model_sdpa.named_modules():\n+                class_name = submodule.__class__.__name__\n+                if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                    has_sdpa = True\n+                    break\n+            if not has_sdpa and model_sdpa.config.model_type != \"falcon\":\n+                raise ValueError(\"The SDPA model should have SDPA attention layers\")\n+\n     def test_eager_matches_sdpa_inference(\n         self,\n         torch_dtype: str,\n@@ -252,24 +299,6 @@ def get_mean_reldiff(msg, current_case, x, ref, atol, rtol):\n                 )\n                 model_eager = model_eager.eval().to(torch_device)\n \n-            self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n-            self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n-\n-            for name, submodule in model_eager.named_modules():\n-                class_name = submodule.__class__.__name__\n-                if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                    raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-            has_sdpa = False\n-            for name, submodule in model_sdpa.named_modules():\n-                class_name = submodule.__class__.__name__\n-                if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                    has_sdpa = True\n-                    break\n-\n-            if not has_sdpa:\n-                raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n             # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving the model each time,\n             # but it would be nicer to have an efficient way to use parameterized.expand\n             cases = [\n@@ -461,6 +490,10 @@ def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n             use_attention_mask_options=(None,),\n         )\n \n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        super().test_sdpa_can_dispatch_composite_models()\n+\n \n class CLIPTextModelTester:\n     def __init__(\n@@ -639,6 +672,10 @@ def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n             use_attention_mask_options=(None, \"right\"),  # \"left\" is not supported for text model\n         )\n \n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        super().test_sdpa_can_dispatch_composite_models()\n+\n     @require_torch_sdpa\n     def test_sdpa_can_dispatch_on_flash(self):\n         self.skipTest(reason=\"CLIPTextModel has two attention masks: `causal_attention_mask` and `attention_mask`\")\n@@ -704,6 +741,7 @@ class CLIPModelTest(CLIPModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n     test_pruning = False\n     test_resize_embeddings = False\n     test_attention_outputs = False\n+    _is_composite = True\n \n     def setUp(self):\n         self.model_tester = CLIPModelTester(self)\n@@ -975,6 +1013,10 @@ def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n             use_attention_mask_options=(None, \"right\"),  # \"left\" is not supported for text model\n         )\n \n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        super().test_sdpa_can_dispatch_composite_models()\n+\n     @require_torch_sdpa\n     def test_sdpa_can_dispatch_on_flash(self):\n         self.skipTest(reason=\"CLIP text tower has two attention masks: `causal_attention_mask` and `attention_mask`\")\n@@ -1104,6 +1146,7 @@ class CLIPForImageClassificationModelTest(CLIPModelTesterMixin, PipelineTesterMi\n     test_pruning = False\n     test_resize_embeddings = False\n     test_attention_outputs = False\n+    _is_composite = True\n \n     def setUp(self):\n         self.model_tester = CLIPForImageClassificationModelTester(self)\n@@ -1143,6 +1186,10 @@ def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n             use_attention_mask_options=(None,),\n         )\n \n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        super().test_sdpa_can_dispatch_composite_models()\n+\n \n # We will verify our results on an image of cute cats\n def prepare_img():"
        },
        {
            "sha": "0ee4b75ed803e34e4e4b2b1da688f2a8ade1f750",
            "filename": "tests/models/encoder_decoder/test_modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 73,
            "deletions": 1,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -18,7 +18,14 @@\n import unittest\n \n from transformers import is_torch_available, logging\n-from transformers.testing_utils import CaptureLogger, require_deterministic_for_xpu, require_torch, slow, torch_device\n+from transformers.testing_utils import (\n+    CaptureLogger,\n+    require_deterministic_for_xpu,\n+    require_torch,\n+    require_torch_sdpa,\n+    slow,\n+    torch_device,\n+)\n \n from ...test_modeling_common import ids_tensor\n from ..bart.test_modeling_bart import BartStandaloneDecoderModelTester\n@@ -54,6 +61,8 @@\n \n @require_torch\n class EncoderDecoderMixin:\n+    supports_sdpa = False\n+\n     def get_encoder_decoder_model(self, config, decoder_config):\n         raise NotImplementedError\n \n@@ -670,6 +679,67 @@ def test_real_model_save_load_from_pretrained(self):\n                 max_diff = np.amax(np.abs(out_1 - out_2))\n                 self.assertLessEqual(max_diff, 1e-5)\n \n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        if not self.supports_sdpa:\n+            self.skipTest(\"SDPA is not supported\")\n+\n+        inputs_dict = self.prepare_config_and_inputs()\n+        encoder_config, decoder_config = inputs_dict[\"config\"], inputs_dict[\"decoder_config\"]\n+        config = EncoderDecoderConfig.from_encoder_decoder_configs(\n+            encoder_config=encoder_config, decoder_config=decoder_config\n+        )\n+        model = EncoderDecoderModel(config=config)\n+\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            model.save_pretrained(tmpdirname)\n+            model_sdpa = EncoderDecoderModel.from_pretrained(tmpdirname)\n+            model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+            # see https://github.com/huggingface/transformers/pull/32238\n+            # Sub-model will dispatch to SDPA if it can (checked below that `SDPA` layers are present)\n+            encoder_attn = \"sdpa\" if model.encoder._supports_sdpa else \"eager\"\n+            decoder_attn = \"sdpa\" if model.decoder._supports_sdpa else \"eager\"\n+            self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+            self.assertTrue(model_sdpa.encoder.config._attn_implementation == encoder_attn)\n+            self.assertTrue(model_sdpa.decoder.config._attn_implementation == decoder_attn)\n+\n+            # Also test that nothing break if we request SDPA explicitly, when both sub-parts support it.\n+            # If the model supports sdpa (i.e. all of sub-models supports it) we'll dispatch safely\n+            # Otherwise we should raise error that SDPA is not supported, as some of the sub-models doesn't support\n+            if encoder_attn == \"sdpa\" and decoder_attn == \"sdpa\":\n+                model_sdpa_explicit = EncoderDecoderModel.from_pretrained(tmpdirname, attn_implementation=\"sdpa\")\n+                model_sdpa_explicit = model_sdpa_explicit.eval().to(torch_device)\n+\n+                self.assertTrue(model_sdpa_explicit.config._attn_implementation == \"sdpa\")\n+            else:\n+                with self.assertRaises(ValueError):\n+                    model_sdpa_explicit = EncoderDecoderModel.from_pretrained(tmpdirname, attn_implementation=\"sdpa\")\n+\n+            model_eager = EncoderDecoderModel.from_pretrained(\n+                tmpdirname,\n+                attn_implementation=\"eager\",\n+            )\n+            model_eager = model_eager.eval().to(torch_device)\n+\n+            self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+            self.assertTrue(model_eager.encoder.config._attn_implementation == \"eager\")\n+            self.assertTrue(model_eager.decoder.config._attn_implementation == \"eager\")\n+\n+            for name, submodule in model_eager.named_modules():\n+                class_name = submodule.__class__.__name__\n+                if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                    raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+            has_sdpa = False\n+            for name, submodule in model_sdpa.named_modules():\n+                class_name = submodule.__class__.__name__\n+                if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                    has_sdpa = True\n+                    break\n+            if not has_sdpa:\n+                raise ValueError(\"The SDPA model should have SDPA attention layers\")\n+\n \n @require_torch\n class BertEncoderDecoderModelTest(EncoderDecoderMixin, unittest.TestCase):\n@@ -949,6 +1019,8 @@ def get_pretrained_model(self):\n \n @require_torch\n class GPT2EncoderDecoderModelTest(EncoderDecoderMixin, unittest.TestCase):\n+    supports_sdpa = True\n+\n     def get_encoder_decoder_model(self, config, decoder_config):\n         encoder_model = BertModel(config)\n         decoder_model = GPT2LMHeadModel(decoder_config)"
        },
        {
            "sha": "94670803daa99810dca25b6a72e5a8c0e7fa142d",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -88,6 +88,10 @@ def setUp(self):\n     def test_model_outputs_equivalence(self, **kwargs):\n         pass\n \n+    @unittest.skip(\"Gemma2's forcefully disables sdpa due to softcapping\")\n+    def test_sdpa_can_dispatch_non_composite_models(self):\n+        pass\n+\n     @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n     @unittest.skip(\"Gemma2's eager attn/sdpa attn outputs are expected to be different\")\n     def test_eager_matches_sdpa_inference(self):"
        },
        {
            "sha": "bbade169550f8c34849cb3379ad2b41297fc81d8",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -580,11 +580,9 @@ def test_model_from_pretrained(self):\n         model = IdeficsModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    @require_torch_sdpa\n-    @slow\n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        self.skipTest(reason=\"Idefics has a hard requirement on SDPA, skipping this test\")\n+    @unittest.skip(\"Idefics has a hard requirement on SDPA\")\n+    def test_sdpa_can_dispatch_non_composite_models(self):\n+        pass\n \n \n @unittest.skipIf(not is_torch_greater_or_equal_than_2_0, reason=\"pytorch 2.0 or higher is required\")\n@@ -806,6 +804,10 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n+    @unittest.skip(\"Idefics has a hard requirement on SDPA\")\n+    def test_sdpa_can_dispatch_non_composite_models(self):\n+        pass\n+\n \n @unittest.skipIf(not is_torch_greater_or_equal_than_2_0, reason=\"pytorch 2.0 or higher is required\")\n @require_torch"
        },
        {
            "sha": "854b8b934578e06dd32522408169fac8cbc019ac",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 40,
            "deletions": 0,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -16,6 +16,7 @@\n \n import copy\n import gc\n+import tempfile\n import unittest\n from io import BytesIO\n \n@@ -36,6 +37,7 @@\n     require_torch,\n     require_torch_gpu,\n     require_torch_multi_gpu,\n+    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -180,6 +182,7 @@ class Idefics2ModelTest(ModelTesterMixin, unittest.TestCase):\n     test_pruning = False\n     test_resize_embeddings = True\n     test_head_masking = False\n+    _is_composite = True\n \n     def setUp(self):\n         self.model_tester = Idefics2VisionText2TextModelTester(self)\n@@ -327,6 +330,43 @@ def test_resize_embeddings_untied(self):\n             # Check that the model can still do a forward pass successfully (every parameter should be resized)\n             model(**self._prepare_for_class(inputs_dict, model_class))\n \n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_sdpa = model_class.from_pretrained(tmpdirname)\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                vision_attn = None if model.vision_model._supports_sdpa else \"eager\"\n+                perceiver_attn = None if model.connector.perceiver_resampler._supports_sdpa else \"eager\"\n+                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(model_sdpa.vision_model.config._attn_implementation == vision_attn)\n+                self.assertTrue(model_sdpa.connector.perceiver_resampler.config._attn_implementation == perceiver_attn)\n+\n+                model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n+                model_eager = model_eager.eval().to(torch_device)\n+                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.vision_model.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_sdpa.connector.perceiver_resampler.config._attn_implementation == \"eager\")\n+\n+                for name, submodule in model_eager.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+                has_sdpa = False\n+                for name, submodule in model_sdpa.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        has_sdpa = True\n+                        break\n+                if not has_sdpa and model_sdpa.config.model_type != \"falcon\":\n+                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n+\n \n @require_torch\n class Idefics2ForConditionalGenerationModelTest(GenerationTesterMixin, ModelTesterMixin, unittest.TestCase):"
        },
        {
            "sha": "5182ac20cd993e90f54a38f76d5ec5ee3541a771",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 62,
            "deletions": 0,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -32,6 +32,7 @@\n     require_accelerate,\n     require_bitsandbytes,\n     require_torch,\n+    require_torch_sdpa,\n     require_vision,\n     slow,\n     torch_device,\n@@ -460,6 +461,7 @@ class InstructBlipForConditionalGenerationDecoderOnlyTest(ModelTesterMixin, Gene\n     test_resize_embeddings = False\n     test_attention_outputs = False\n     test_torchscript = False\n+    _is_composite = True\n \n     def setUp(self):\n         self.model_tester = InstructBlipForConditionalGenerationDecoderOnlyModelTester(self)\n@@ -529,6 +531,66 @@ def test_model_from_pretrained(self):\n         model = InstructBlipForConditionalGeneration.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        \"\"\"\n+        Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n+        This tests only by looking at layer names, as usually SDPA layers are calles \"SDPAAttention\".\n+        In contrast to the above test, this one checks if the \"config._attn_implamentation\" is a dict after the model\n+        is loaded, because we manually replicate requested attn implementation on each sub-config when loading.\n+        See https://github.com/huggingface/transformers/pull/32238 for more info\n+\n+        The test tries to cover most general cases of composite models, VLMs with vision and text configs. Any model\n+        that has a different set of sub-configs has to overwrite this test.\n+        \"\"\"\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        if not self._is_composite:\n+            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_sdpa = model_class.from_pretrained(tmpdirname)\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                text_attn = \"sdpa\" if model.language_model._supports_sdpa else \"eager\"\n+                vision_attn = \"sdpa\" if model.vision_model._supports_sdpa else \"eager\"\n+                qformer_attn = \"sdpa\" if model.qformer._supports_sdpa else \"eager\"\n+\n+                # `None` as it is the requested one which will be assigned to each sub-config\n+                # Sub-model will dispatch to SDPA if it can (checked below that `SDPA` layers are present)\n+                self.assertTrue(model.language_model.config._attn_implementation == text_attn)\n+                self.assertTrue(model.vision_model.config._attn_implementation == vision_attn)\n+                self.assertTrue(model.qformer.config._attn_implementation == qformer_attn)\n+\n+                model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n+                model_eager = model_eager.eval().to(torch_device)\n+                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.language_model.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.vision_model.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.qformer.config._attn_implementation == \"eager\")\n+\n+                for name, submodule in model_eager.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+                has_sdpa = False\n+                for name, submodule in model_sdpa.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        has_sdpa = True\n+                        break\n+                if not has_sdpa and any(\n+                    module_attn == \"sdpa\" for module_attn in [text_attn, vision_attn, qformer_attn]\n+                ):\n+                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n+\n \n # We will verify our results on an image of cute cats\n def prepare_img():"
        },
        {
            "sha": "298c7a8d7ff46f6e5a09a8759e252f56a457e173",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 62,
            "deletions": 0,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -32,6 +32,7 @@\n     require_accelerate,\n     require_bitsandbytes,\n     require_torch,\n+    require_torch_sdpa,\n     require_vision,\n     slow,\n     torch_device,\n@@ -481,6 +482,7 @@ class InstructBlipVideoForConditionalGenerationDecoderOnlyTest(\n     test_resize_embeddings = False\n     test_attention_outputs = False\n     test_torchscript = False\n+    _is_composite = True\n \n     def setUp(self):\n         self.model_tester = InstructBlipVideoForConditionalGenerationDecoderOnlyModelTester(self)\n@@ -550,6 +552,66 @@ def test_model_from_pretrained(self):\n         model = InstructBlipVideoForConditionalGeneration.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        \"\"\"\n+        Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n+        This tests only by looking at layer names, as usually SDPA layers are calles \"SDPAAttention\".\n+        In contrast to the above test, this one checks if the \"config._attn_implamentation\" is a dict after the model\n+        is loaded, because we manually replicate requested attn implementation on each sub-config when loading.\n+        See https://github.com/huggingface/transformers/pull/32238 for more info\n+\n+        The test tries to cover most general cases of composite models, VLMs with vision and text configs. Any model\n+        that has a different set of sub-configs has to overwrite this test.\n+        \"\"\"\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        if not self._is_composite:\n+            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_sdpa = model_class.from_pretrained(tmpdirname)\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                text_attn = \"sdpa\" if model.language_model._supports_sdpa else \"eager\"\n+                vision_attn = \"sdpa\" if model.vision_model._supports_sdpa else \"eager\"\n+                qformer_attn = \"sdpa\" if model.qformer._supports_sdpa else \"eager\"\n+\n+                # `None` as it is the requested one which will be assigned to each sub-config\n+                # Sub-model will dispatch to SDPA if it can (checked below that `SDPA` layers are present)\n+                self.assertTrue(model.language_model.config._attn_implementation == text_attn)\n+                self.assertTrue(model.vision_model.config._attn_implementation == vision_attn)\n+                self.assertTrue(model.qformer.config._attn_implementation == qformer_attn)\n+\n+                model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n+                model_eager = model_eager.eval().to(torch_device)\n+                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.language_model.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.vision_model.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.qformer.config._attn_implementation == \"eager\")\n+\n+                for name, submodule in model_eager.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+                has_sdpa = False\n+                for name, submodule in model_sdpa.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        has_sdpa = True\n+                        break\n+                if not has_sdpa and any(\n+                    module_attn == \"sdpa\" for module_attn in [text_attn, vision_attn, qformer_attn]\n+                ):\n+                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n+\n \n # We will verify our results on an image of cute cats\n def prepare_video():"
        },
        {
            "sha": "de6c0b15d661f91cf43a42f040174a12f277ba80",
            "filename": "tests/models/kosmos2/test_modeling_kosmos2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 2,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -25,8 +25,17 @@\n \n from transformers import AutoModelForImageTextToText, AutoProcessor, Kosmos2Config\n from transformers.models.kosmos2.configuration_kosmos2 import Kosmos2TextConfig, Kosmos2VisionConfig\n-from transformers.testing_utils import IS_ROCM_SYSTEM, require_torch, require_vision, slow, torch_device\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.testing_utils import (\n+    IS_ROCM_SYSTEM,\n+    require_torch,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import (\n+    is_torch_available,\n+    is_vision_available,\n+)\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n@@ -257,6 +266,7 @@ class Kosmos2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n     test_pruning = False\n     test_resize_embeddings = False\n     test_attention_outputs = False\n+    _is_composite = True\n \n     # TODO: `image-to-text` pipeline for this model needs Processor.\n     def is_pipeline_test_to_skip("
        },
        {
            "sha": "405fad1bd31c8d3321b46df5aece19c8e9b0c5d3",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -186,6 +186,7 @@ class LlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterM\n     pipeline_model_mapping = {\"image-to-text\": LlavaForConditionalGeneration} if is_torch_available() else {}\n     test_pruning = False\n     test_head_masking = False\n+    _is_composite = True\n \n     def setUp(self):\n         self.model_tester = LlavaVisionText2TextModelTester(self)\n@@ -260,6 +261,16 @@ def test_sdpa_can_compile_dynamic(self):\n     def test_sdpa_can_dispatch_on_flash(self):\n         pass\n \n+    @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n+    def test_flash_attn_2_fp32_ln(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"VLMs need lots of steps to prepare images/mask correctly to get pad-free inputs. Can be tested as part of LLM test\"\n+    )\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n \n @require_torch\n class LlavaForConditionalGenerationIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "6589bf14d24c658662647f27db9b19ef130dd737",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -218,6 +218,7 @@ class LlavaNextForConditionalGenerationModelTest(ModelTesterMixin, GenerationTes\n     all_generative_model_classes = (LlavaNextForConditionalGeneration,) if is_torch_available() else ()\n     test_pruning = False\n     test_head_masking = False\n+    _is_composite = True\n \n     def setUp(self):\n         self.model_tester = LlavaNextVisionText2TextModelTester(self)\n@@ -316,6 +317,16 @@ def test_sdpa_can_compile_dynamic(self):\n     def test_sdpa_can_dispatch_on_flash(self):\n         pass\n \n+    @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n+    def test_flash_attn_2_fp32_ln(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"VLMs need lots of steps to prepare images/mask correctly to get pad-free inputs. Can be tested as part of LLM test\"\n+    )\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n \n @require_torch\n class LlavaNextForConditionalGenerationIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "05fc8a49e1e9b946251b1a43951983adfc7014b7",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -236,6 +236,7 @@ class LlavaNextVideoForConditionalGenerationModelTest(ModelTesterMixin, Generati\n     all_generative_model_classes = (LlavaNextVideoForConditionalGeneration,) if is_torch_available() else ()\n     test_pruning = False\n     test_head_masking = False\n+    _is_composite = True\n \n     def setUp(self):\n         self.model_tester = LlavaNextVideoVisionText2TextModelTester(self)\n@@ -340,6 +341,16 @@ def test_sdpa_can_compile_dynamic(self):\n     def test_sdpa_can_dispatch_on_flash(self):\n         pass\n \n+    @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n+    def test_flash_attn_2_fp32_ln(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"VLMs need lots of steps to prepare images/mask correctly to get pad-free inputs. Can be tested as part of LLM test\"\n+    )\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n \n @require_torch\n class LlavaNextVideoForConditionalGenerationIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "0a33898b63072b3a97bf6240aa8eec955973ac61",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -219,6 +219,7 @@ class LlavaOnevisionForConditionalGenerationModelTest(ModelTesterMixin, Generati\n     all_generative_model_classes = (LlavaOnevisionForConditionalGeneration,) if is_torch_available() else ()\n     test_pruning = False\n     test_head_masking = False\n+    _is_composite = True\n \n     def setUp(self):\n         self.model_tester = LlavaOnevisionVisionText2TextModelTester(self)\n@@ -306,6 +307,16 @@ def test_training_gradient_checkpointing_use_reentrant_false(self):\n     def test_assisted_decoding_with_num_logits_to_keep(self):\n         pass\n \n+    @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n+    def test_flash_attn_2_fp32_ln(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"VLMs need lots of steps to prepare images/mask correctly to get pad-free inputs. Can be tested as part of LLM test\"\n+    )\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n \n @require_torch\n class LlavaOnevisionForConditionalGenerationIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "fafa2f71331ba31086dcce57592baa82c092286c",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -274,6 +274,7 @@ class MllamaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTester\n     test_pruning = False\n     test_head_masking = False\n     test_torchscript = False\n+    _is_composite = True\n \n     def setUp(self):\n         self.model_tester = MllamaVisionText2TextModelTester(self)"
        },
        {
            "sha": "438178bfc6faa2e5e946374ff73894dcb72d41f6",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 143,
            "deletions": 42,
            "changes": 185,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -654,29 +654,13 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n                 model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)\n                 model_sdpa = model_sdpa.eval().to(torch_device)\n \n-                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n-\n                 model_eager = model_class.from_pretrained(\n                     tmpdirname,\n                     torch_dtype=torch_dtype,\n                     attn_implementation=\"eager\",\n                 )\n                 model_eager = model_eager.eval().to(torch_device)\n \n-                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n-\n-                for name, submodule in model_eager.named_modules():\n-                    if \"SdpaAttention\" in submodule.__class__.__name__:\n-                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-                has_sdpa = False\n-                for name, submodule in model_sdpa.named_modules():\n-                    if \"SdpaAttention\" in submodule.__class__.__name__:\n-                        has_sdpa = True\n-                        break\n-                if not has_sdpa and model_sdpa.config.model_type != \"falcon\":\n-                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n                 # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving 8 times the model,\n                 # but it would be nicer to have an efficient way to use parameterized.expand\n                 fail_cases = []\n@@ -1042,6 +1026,7 @@ class MusicgenTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n     # not to test torchscript as the model tester doesn't prepare `input_values` and `padding_mask`\n     # (and `torchscript` hates `None` values).\n     test_torchscript = False\n+    _is_composite = True\n \n     def setUp(self):\n         self.model_tester = MusicgenTester(self)\n@@ -1420,7 +1405,7 @@ def test_save_load_fast_init_from_base(self):\n     @require_torch_gpu\n     @mark.flash_attn_test\n     @slow\n-    # Copied from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_inference_equivalence\n+    # Adapted from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_inference_equivalence\n     def test_flash_attn_2_inference_equivalence(self):\n         for model_class in self.all_model_classes:\n             if not model_class._supports_flash_attn_2:\n@@ -1432,7 +1417,9 @@ def test_flash_attn_2_inference_equivalence(self):\n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n                 model_fa = model_class.from_pretrained(\n-                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n+                    tmpdirname,\n+                    torch_dtype=torch.bfloat16,\n+                    attn_implementation={\"decoder\": \"flash_attention_2\", \"audio_encoder\": None, \"text_encoder\": None},\n                 )\n                 model_fa.to(torch_device)\n \n@@ -1505,7 +1492,88 @@ def test_flash_attn_2_inference_equivalence(self):\n     @require_torch_gpu\n     @mark.flash_attn_test\n     @slow\n-    # Copied from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_inference_equivalence_right_padding\n+    def test_flash_attn_2_conversion(self):\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            if not model_class._supports_flash_attn_2:\n+                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n+\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model = model_class.from_pretrained(\n+                    tmpdirname,\n+                    torch_dtype=torch.float16,\n+                    attn_implementation={\"decoder\": \"flash_attention_2\", \"audio_encoder\": None, \"text_encoder\": None},\n+                ).to(torch_device)\n+\n+                for _, module in model.named_modules():\n+                    if \"FlashAttention\" in module.__class__.__name__:\n+                        return\n+\n+                self.assertTrue(False, \"FlashAttention2 modules not found in model\")\n+\n+    @require_torch_sdpa\n+    @require_torch_gpu\n+    @slow\n+    def test_sdpa_can_dispatch_on_flash(self):\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        torch.compiler.reset()\n+        compute_capability = torch.cuda.get_device_capability()\n+        major, _ = compute_capability\n+\n+        if not torch.version.cuda or major < 8:\n+            self.skipTest(reason=\"This test requires an NVIDIA GPU with compute capability >= 8.0\")\n+\n+        for model_class in self.all_model_classes:\n+            if not model_class._supports_sdpa:\n+                self.skipTest(f\"{model_class.__name__} does not support SDPA\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n+            if config.model_type in [\"llava\", \"llava_next\", \"vipllava\", \"video_llava\"]:\n+                self.skipTest(\n+                    reason=\"Llava-like models currently (transformers==4.39.1) requires an attention_mask input\"\n+                )\n+            if config.model_type in [\"paligemma\"]:\n+                self.skipTest(\n+                    \"PaliGemma-like models currently (transformers==4.41.0) requires an attention_mask input\"\n+                )\n+            if config.model_type in [\"idefics\", \"idefics2\", \"idefics3\"]:\n+                self.skipTest(reason=\"Idefics currently (transformers==4.39.1) requires an image_attention_mask input\")\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model = model_class.from_pretrained(\n+                    tmpdirname,\n+                    torch_dtype=torch.float16,\n+                    attn_implementation={\"decoder\": \"sdpa\", \"audio_encoder\": None, \"text_encoder\": None},\n+                )\n+                model.to(torch_device)\n+\n+                inputs_dict.pop(\"attention_mask\", None)\n+                inputs_dict.pop(\"decoder_attention_mask\", None)\n+\n+                for name, inp in inputs_dict.items():\n+                    if isinstance(inp, torch.Tensor) and inp.dtype in [torch.float32, torch.float16]:\n+                        inputs_dict[name] = inp.to(torch.float16)\n+\n+                with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n+                    _ = model(**inputs_dict)\n+\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    @slow\n+    # Adapted from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_inference_equivalence_right_padding\n     def test_flash_attn_2_inference_equivalence_right_padding(self):\n         for model_class in self.all_model_classes:\n             if not model_class._supports_flash_attn_2:\n@@ -1517,7 +1585,9 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n                 model_fa = model_class.from_pretrained(\n-                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n+                    tmpdirname,\n+                    torch_dtype=torch.bfloat16,\n+                    attn_implementation={\"decoder\": \"flash_attention_2\", \"audio_encoder\": None, \"text_encoder\": None},\n                 )\n                 model_fa.to(torch_device)\n \n@@ -1587,7 +1657,7 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n     @require_torch_gpu\n     @mark.flash_attn_test\n     @slow\n-    # Copied from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_generate_left_padding\n+    # Adapted from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_generate_left_padding\n     def test_flash_attn_2_generate_left_padding(self):\n         # Ignore copy\n         for model_class in self.greedy_sample_model_classes:\n@@ -1622,7 +1692,7 @@ def test_flash_attn_2_generate_left_padding(self):\n                 model = model_class.from_pretrained(\n                     tmpdirname,\n                     torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n+                    attn_implementation={\"decoder\": \"flash_attention_2\", \"audio_encoder\": None, \"text_encoder\": None},\n                     low_cpu_mem_usage=True,\n                 ).to(torch_device)\n \n@@ -1636,7 +1706,7 @@ def test_flash_attn_2_generate_left_padding(self):\n     @require_torch_gpu\n     @mark.flash_attn_test\n     @slow\n-    # Copied from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_generate_padding_right\n+    # Adapted from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_generate_padding_right\n     def test_flash_attn_2_generate_padding_right(self):\n         # Ignore copy\n         for model_class in self.greedy_sample_model_classes:\n@@ -1670,7 +1740,7 @@ def test_flash_attn_2_generate_padding_right(self):\n                 model = model_class.from_pretrained(\n                     tmpdirname,\n                     torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n+                    attn_implementation={\"decoder\": \"flash_attention_2\", \"audio_encoder\": None, \"text_encoder\": None},\n                     low_cpu_mem_usage=True,\n                 ).to(torch_device)\n \n@@ -1684,7 +1754,7 @@ def test_flash_attn_2_generate_padding_right(self):\n     @require_torch_gpu\n     @mark.flash_attn_test\n     @slow\n-    # Copied from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_generate_use_cache\n+    # Adapted from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_generate_use_cache\n     def test_flash_attn_2_generate_use_cache(self):\n         max_new_tokens = 30\n \n@@ -1713,7 +1783,7 @@ def test_flash_attn_2_generate_use_cache(self):\n                 model = model_class.from_pretrained(\n                     tmpdirname,\n                     torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n+                    attn_implementation={\"decoder\": \"flash_attention_2\", \"audio_encoder\": None, \"text_encoder\": None},\n                     low_cpu_mem_usage=True,\n                 ).to(torch_device)\n \n@@ -1726,6 +1796,53 @@ def test_flash_attn_2_generate_use_cache(self):\n                     use_cache=True,\n                 )\n \n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        if not self._is_composite:\n+            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_sdpa = model_class.from_pretrained(tmpdirname)\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                audio_encoder_attn = \"sdpa\" if model.audio_encoder._supports_sdpa else \"eager\"\n+                text_encoder_attn = \"sdpa\" if model.text_encoder._supports_sdpa else \"eager\"\n+                decoder_attn = \"sdpa\" if model.decoder._supports_sdpa else \"eager\"\n+\n+                # `None` as it is the requested one which will be assigned to each sub-config\n+                # Sub-model will dispatch to SDPA if it can (checked below that `SDPA` layers are present)\n+                self.assertTrue(model_sdpa.audio_encoder.config._attn_implementation == audio_encoder_attn)\n+                self.assertTrue(model_sdpa.text_encoder.config._attn_implementation == text_encoder_attn)\n+                self.assertTrue(model_sdpa.decoder.config._attn_implementation == decoder_attn)\n+                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+                model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n+                model_eager = model_eager.eval().to(torch_device)\n+\n+                self.assertTrue(model_eager.audio_encoder.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.text_encoder.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.decoder.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+\n+                for name, submodule in model_eager.named_modules():\n+                    if \"SdpaAttention\" in submodule.__class__.__name__:\n+                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+                has_sdpa = False\n+                for name, submodule in model_sdpa.named_modules():\n+                    if \"SdpaAttention\" in submodule.__class__.__name__:\n+                        has_sdpa = True\n+                        break\n+                if not has_sdpa and model_sdpa.config.model_type != \"falcon\":\n+                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n+\n     @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n     @require_torch_sdpa\n     @slow\n@@ -1792,29 +1909,13 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n                 model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)\n                 model_sdpa = model_sdpa.eval().to(torch_device)\n \n-                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n-\n                 model_eager = model_class.from_pretrained(\n                     tmpdirname,\n                     torch_dtype=torch_dtype,\n                     attn_implementation=\"eager\",\n                 )\n                 model_eager = model_eager.eval().to(torch_device)\n \n-                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n-\n-                for name, submodule in model_eager.named_modules():\n-                    if \"SdpaAttention\" in submodule.__class__.__name__:\n-                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-                has_sdpa = False\n-                for name, submodule in model_sdpa.named_modules():\n-                    if \"SdpaAttention\" in submodule.__class__.__name__:\n-                        has_sdpa = True\n-                        break\n-                if not has_sdpa and model_sdpa.config.model_type != \"falcon\":\n-                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n                 # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving 8 times the model,\n                 # but it would be nicer to have an efficient way to use parameterized.expand\n                 fail_cases = []"
        },
        {
            "sha": "f53fc21ba80c093a60970a9e3dd1997155f18c76",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 150,
            "deletions": 251,
            "changes": 401,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -311,7 +311,9 @@ def test_flash_attn_2_inference_equivalence(self):\n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n                 model_fa = model_class.from_pretrained(\n-                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n+                    tmpdirname,\n+                    torch_dtype=torch.bfloat16,\n+                    attn_implementation=\"flash_attention_2\",\n                 )\n                 model_fa.to(torch_device)\n \n@@ -391,7 +393,9 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n                 model_fa = model_class.from_pretrained(\n-                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n+                    tmpdirname,\n+                    torch_dtype=torch.bfloat16,\n+                    attn_implementation=\"flash_attention_2\",\n                 )\n                 model_fa.to(torch_device)\n \n@@ -454,148 +458,10 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n \n                 assert torch.allclose(logits_fa[:-1], logits[:-1], atol=4e-2, rtol=4e-2)\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    # Copied from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_generate_left_padding\n-    def test_flash_attn_2_generate_left_padding(self):\n-        # Ignore copy\n-        for model_class in self.greedy_sample_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(\n-                    torch_device\n-                )\n-\n-                dummy_input = inputs_dict[model.main_input_name]\n-                if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                    dummy_input = dummy_input.to(torch.float16)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-                # make sure we do left padding\n-                dummy_attention_mask[:, :-1] = 0\n-                dummy_attention_mask[:, -1:] = 1\n-\n-                out = model.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=8, do_sample=False\n-                )\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                out_fa = model.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=8, do_sample=False\n-                )\n-\n-                self.assertTrue(torch.allclose(out, out_fa))\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    # Copied from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_generate_padding_right\n-    def test_flash_attn_2_generate_padding_right(self):\n-        # Ignore copy\n-        for model_class in self.greedy_sample_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(\n-                    torch_device\n-                )\n-\n-                dummy_input = inputs_dict[model.main_input_name]\n-                if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                    dummy_input = dummy_input.to(torch.float16)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-                # make sure we do right padding\n-                dummy_attention_mask[:, :-1] = 1\n-                dummy_attention_mask[:, -1:] = 0\n-\n-                out = model.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=8, do_sample=False\n-                )\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                out_fa = model.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=8, do_sample=False\n-                )\n-\n-                self.assertTrue(torch.allclose(out, out_fa))\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    # Copied from tests.models.musicgen.test_modeling_musicgen.MusicgenDecoderTest.test_flash_attn_2_generate_use_cache\n-    def test_flash_attn_2_generate_use_cache(self):\n-        max_new_tokens = 30\n-\n-        # Ignore copy\n-        for model_class in self.greedy_sample_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                # Just test that a large cache works as expected\n-                _ = model.generate(\n-                    dummy_input,\n-                    attention_mask=dummy_attention_mask,\n-                    max_new_tokens=max_new_tokens,\n-                    do_sample=False,\n-                    use_cache=True,\n-                )\n-\n     @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n     @require_torch_sdpa\n     @slow\n-    # Copied from tests.models.musicgen.test_modeling_musicgen.MusicgenDecoderTest.test_eager_matches_sdpa_inference\n+    # Copied from tests.test_modeling_common.ModelTesterMixin.test_eager_matches_sdpa_inference\n     def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n@@ -658,29 +524,13 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n                 model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)\n                 model_sdpa = model_sdpa.eval().to(torch_device)\n \n-                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n-\n                 model_eager = model_class.from_pretrained(\n                     tmpdirname,\n                     torch_dtype=torch_dtype,\n                     attn_implementation=\"eager\",\n                 )\n                 model_eager = model_eager.eval().to(torch_device)\n \n-                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n-\n-                for name, submodule in model_eager.named_modules():\n-                    if \"SdpaAttention\" in submodule.__class__.__name__:\n-                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-                has_sdpa = False\n-                for name, submodule in model_sdpa.named_modules():\n-                    if \"SdpaAttention\" in submodule.__class__.__name__:\n-                        has_sdpa = True\n-                        break\n-                if not has_sdpa and model_sdpa.config.model_type != \"falcon\":\n-                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n                 # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving 8 times the model,\n                 # but it would be nicer to have an efficient way to use parameterized.expand\n                 fail_cases = []\n@@ -839,74 +689,6 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n \n                 self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n \n-    @require_torch_sdpa\n-    @slow\n-    # Copied from tests.models.musicgen.test_modeling_musicgen.MusicgenDecoderTest.test_eager_matches_sdpa_generate\n-    def test_eager_matches_sdpa_generate(self):\n-        max_new_tokens = 30\n-\n-        # Ignore copy\n-        for model_class in self.greedy_sample_model_classes:\n-            if not model_class._supports_sdpa:\n-                self.skipTest(f\"{model_class.__name__} does not support SDPA\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-\n-                model_sdpa = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n-\n-                model_eager = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    low_cpu_mem_usage=True,\n-                    attn_implementation=\"eager\",\n-                ).to(torch_device)\n-\n-                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n-\n-                for name, submodule in model_eager.named_modules():\n-                    if \"SdpaAttention\" in submodule.__class__.__name__:\n-                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-                has_sdpa = False\n-                for name, submodule in model_sdpa.named_modules():\n-                    if \"SdpaAttention\" in submodule.__class__.__name__:\n-                        has_sdpa = True\n-                        break\n-                if not has_sdpa:\n-                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n-                # Just test that a large cache works as expected\n-                res_eager = model_eager.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=max_new_tokens, do_sample=False\n-                )\n-\n-                res_sdpa = model_sdpa.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=max_new_tokens, do_sample=False\n-                )\n-\n-                self.assertTrue(torch.allclose(res_eager, res_sdpa))\n-\n \n def prepare_musicgen_melody_inputs_dict(\n     config,\n@@ -1048,6 +830,7 @@ class MusicgenMelodyTest(ModelTesterMixin, GenerationTesterMixin, PipelineTester\n     # not to test torchscript as the model tester doesn't prepare `input_features` and `padding_mask`\n     # (and `torchscript` hates `None` values).\n     test_torchscript = False\n+    _is_composite = True\n \n     def setUp(self):\n         self.model_tester = MusicgenMelodyTester(self)\n@@ -1406,7 +1189,7 @@ def test_save_load_fast_init_from_base(self):\n     @require_torch_gpu\n     @mark.flash_attn_test\n     @slow\n-    # Copied from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_inference_equivalence\n+    # Adapted from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_inference_equivalence\n     def test_flash_attn_2_inference_equivalence(self):\n         for model_class in self.all_model_classes:\n             if not model_class._supports_flash_attn_2:\n@@ -1418,7 +1201,9 @@ def test_flash_attn_2_inference_equivalence(self):\n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n                 model_fa = model_class.from_pretrained(\n-                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n+                    tmpdirname,\n+                    torch_dtype=torch.bfloat16,\n+                    attn_implementation={\"decoder\": \"flash_attention_2\", \"audio_encoder\": None, \"text_encoder\": None},\n                 )\n                 model_fa.to(torch_device)\n \n@@ -1491,7 +1276,88 @@ def test_flash_attn_2_inference_equivalence(self):\n     @require_torch_gpu\n     @mark.flash_attn_test\n     @slow\n-    # Copied from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_inference_equivalence_right_padding\n+    def test_flash_attn_2_conversion(self):\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            if not model_class._supports_flash_attn_2:\n+                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n+\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model = model_class.from_pretrained(\n+                    tmpdirname,\n+                    torch_dtype=torch.float16,\n+                    attn_implementation={\"decoder\": \"flash_attention_2\", \"audio_encoder\": None, \"text_encoder\": None},\n+                ).to(torch_device)\n+\n+                for _, module in model.named_modules():\n+                    if \"FlashAttention\" in module.__class__.__name__:\n+                        return\n+\n+                self.assertTrue(False, \"FlashAttention2 modules not found in model\")\n+\n+    @require_torch_sdpa\n+    @require_torch_gpu\n+    @slow\n+    def test_sdpa_can_dispatch_on_flash(self):\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        torch.compiler.reset()\n+        compute_capability = torch.cuda.get_device_capability()\n+        major, _ = compute_capability\n+\n+        if not torch.version.cuda or major < 8:\n+            self.skipTest(reason=\"This test requires an NVIDIA GPU with compute capability >= 8.0\")\n+\n+        for model_class in self.all_model_classes:\n+            if not model_class._supports_sdpa:\n+                self.skipTest(f\"{model_class.__name__} does not support SDPA\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n+            if config.model_type in [\"llava\", \"llava_next\", \"vipllava\", \"video_llava\"]:\n+                self.skipTest(\n+                    reason=\"Llava-like models currently (transformers==4.39.1) requires an attention_mask input\"\n+                )\n+            if config.model_type in [\"paligemma\"]:\n+                self.skipTest(\n+                    \"PaliGemma-like models currently (transformers==4.41.0) requires an attention_mask input\"\n+                )\n+            if config.model_type in [\"idefics\", \"idefics2\", \"idefics3\"]:\n+                self.skipTest(reason=\"Idefics currently (transformers==4.39.1) requires an image_attention_mask input\")\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model = model_class.from_pretrained(\n+                    tmpdirname,\n+                    torch_dtype=torch.float16,\n+                    attn_implementation={\"decoder\": \"sdpa\", \"audio_encoder\": None, \"text_encoder\": None},\n+                )\n+                model.to(torch_device)\n+\n+                inputs_dict.pop(\"attention_mask\", None)\n+                inputs_dict.pop(\"decoder_attention_mask\", None)\n+\n+                for name, inp in inputs_dict.items():\n+                    if isinstance(inp, torch.Tensor) and inp.dtype in [torch.float32, torch.float16]:\n+                        inputs_dict[name] = inp.to(torch.float16)\n+\n+                with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n+                    _ = model(**inputs_dict)\n+\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    @slow\n+    # Adapted from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_inference_equivalence_right_padding\n     def test_flash_attn_2_inference_equivalence_right_padding(self):\n         for model_class in self.all_model_classes:\n             if not model_class._supports_flash_attn_2:\n@@ -1503,7 +1369,9 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n                 model_fa = model_class.from_pretrained(\n-                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n+                    tmpdirname,\n+                    torch_dtype=torch.bfloat16,\n+                    attn_implementation={\"decoder\": \"flash_attention_2\", \"audio_encoder\": None, \"text_encoder\": None},\n                 )\n                 model_fa.to(torch_device)\n \n@@ -1573,7 +1441,7 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n     @require_torch_gpu\n     @mark.flash_attn_test\n     @slow\n-    # Copied from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_generate_left_padding\n+    # Adapted from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_generate_left_padding\n     def test_flash_attn_2_generate_left_padding(self):\n         # Ignore copy\n         for model_class in self.greedy_sample_model_classes:\n@@ -1608,7 +1476,7 @@ def test_flash_attn_2_generate_left_padding(self):\n                 model = model_class.from_pretrained(\n                     tmpdirname,\n                     torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n+                    attn_implementation={\"decoder\": \"flash_attention_2\", \"audio_encoder\": None, \"text_encoder\": None},\n                     low_cpu_mem_usage=True,\n                 ).to(torch_device)\n \n@@ -1622,7 +1490,7 @@ def test_flash_attn_2_generate_left_padding(self):\n     @require_torch_gpu\n     @mark.flash_attn_test\n     @slow\n-    # Copied from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_generate_padding_right\n+    # Adapted from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_generate_padding_right\n     def test_flash_attn_2_generate_padding_right(self):\n         # Ignore copy\n         for model_class in self.greedy_sample_model_classes:\n@@ -1656,7 +1524,7 @@ def test_flash_attn_2_generate_padding_right(self):\n                 model = model_class.from_pretrained(\n                     tmpdirname,\n                     torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n+                    attn_implementation={\"decoder\": \"flash_attention_2\", \"audio_encoder\": None, \"text_encoder\": None},\n                     low_cpu_mem_usage=True,\n                 ).to(torch_device)\n \n@@ -1670,7 +1538,7 @@ def test_flash_attn_2_generate_padding_right(self):\n     @require_torch_gpu\n     @mark.flash_attn_test\n     @slow\n-    # Copied from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_generate_use_cache\n+    # Adapted from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_generate_use_cache\n     def test_flash_attn_2_generate_use_cache(self):\n         max_new_tokens = 30\n \n@@ -1699,7 +1567,7 @@ def test_flash_attn_2_generate_use_cache(self):\n                 model = model_class.from_pretrained(\n                     tmpdirname,\n                     torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n+                    attn_implementation={\"decoder\": \"flash_attention_2\", \"audio_encoder\": None, \"text_encoder\": None},\n                     low_cpu_mem_usage=True,\n                 ).to(torch_device)\n \n@@ -1712,6 +1580,53 @@ def test_flash_attn_2_generate_use_cache(self):\n                     use_cache=True,\n                 )\n \n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        if not self._is_composite:\n+            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_sdpa = model_class.from_pretrained(tmpdirname)\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                audio_encoder_attn = \"sdpa\" if model.audio_encoder._supports_sdpa else \"eager\"\n+                text_encoder_attn = \"sdpa\" if model.text_encoder._supports_sdpa else \"eager\"\n+                decoder_attn = \"sdpa\" if model.decoder._supports_sdpa else \"eager\"\n+\n+                # `None` as it is the requested one which will be assigned to each sub-config\n+                # Sub-model will dispatch to SDPA if it can (checked below that `SDPA` layers are present)\n+                self.assertTrue(model_sdpa.audio_encoder.config._attn_implementation == audio_encoder_attn)\n+                self.assertTrue(model_sdpa.text_encoder.config._attn_implementation == text_encoder_attn)\n+                self.assertTrue(model_sdpa.decoder.config._attn_implementation == decoder_attn)\n+                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+                model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n+                model_eager = model_eager.eval().to(torch_device)\n+\n+                self.assertTrue(model_eager.audio_encoder.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.text_encoder.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.decoder.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+\n+                for name, submodule in model_eager.named_modules():\n+                    if \"SdpaAttention\" in submodule.__class__.__name__:\n+                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+                has_sdpa = False\n+                for name, submodule in model_sdpa.named_modules():\n+                    if \"SdpaAttention\" in submodule.__class__.__name__:\n+                        has_sdpa = True\n+                        break\n+                if not has_sdpa and model_sdpa.config.model_type != \"falcon\":\n+                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n+\n     @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n     @require_torch_sdpa\n     @slow\n@@ -1775,29 +1690,13 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n                 model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)\n                 model_sdpa = model_sdpa.eval().to(torch_device)\n \n-                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n-\n                 model_eager = model_class.from_pretrained(\n                     tmpdirname,\n                     torch_dtype=torch_dtype,\n                     attn_implementation=\"eager\",\n                 )\n                 model_eager = model_eager.eval().to(torch_device)\n \n-                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n-\n-                for name, submodule in model_eager.named_modules():\n-                    if \"SdpaAttention\" in submodule.__class__.__name__:\n-                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-                has_sdpa = False\n-                for name, submodule in model_sdpa.named_modules():\n-                    if \"SdpaAttention\" in submodule.__class__.__name__:\n-                        has_sdpa = True\n-                        break\n-                if not has_sdpa and model_sdpa.config.model_type != \"falcon\":\n-                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n                 # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving 8 times the model,\n                 # but it would be nicer to have an efficient way to use parameterized.expand\n                 fail_cases = []"
        },
        {
            "sha": "cfc2a2c29b1d70d0a8f8a27fc234355e4270ad88",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -187,6 +187,7 @@ class PaliGemmaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTes\n     test_pruning = False\n     test_torchscript = False\n     test_head_masking = False\n+    _is_composite = True\n \n     def setUp(self):\n         self.model_tester = PaliGemmaVisionText2TextModelTester(self)\n@@ -319,6 +320,16 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n     def test_static_cache_matches_dynamic(self):\n         pass\n \n+    @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n+    def test_flash_attn_2_fp32_ln(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"VLMs need lots of steps to prepare images/mask correctly to get pad-free inputs. Can be tested as part of LLM test\"\n+    )\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n \n @slow\n @require_torch"
        },
        {
            "sha": "314f870f5d90963f74769f87c275dcb0dc6555a9",
            "filename": "tests/models/qwen2_audio/test_modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 50,
            "deletions": 0,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -15,6 +15,7 @@\n \"\"\"Testing suite for the PyTorch Qwen2Audio model.\"\"\"\n \n import gc\n+import tempfile\n import unittest\n from io import BytesIO\n from urllib.request import urlopen\n@@ -29,6 +30,7 @@\n )\n from transformers.testing_utils import (\n     require_torch,\n+    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -152,6 +154,7 @@ class Qwen2AudioForConditionalGenerationModelTest(ModelTesterMixin, unittest.Tes\n     all_model_classes = (Qwen2AudioForConditionalGeneration,) if is_torch_available() else ()\n     test_pruning = False\n     test_head_masking = False\n+    _is_composite = True\n \n     def setUp(self):\n         self.model_tester = Qwen2AudioModelTester(self)\n@@ -165,6 +168,53 @@ def test_sdpa_can_compile_dynamic(self):\n     def test_sdpa_can_dispatch_on_flash(self):\n         pass\n \n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        # overwrite because Qwen2 is audio+text model (not vision+text)\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        if not self._is_composite:\n+            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_sdpa = model_class.from_pretrained(tmpdirname)\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                text_attn = \"sdpa\" if model.language_model._supports_sdpa else \"eager\"\n+                vision_attn = \"sdpa\" if model.audio_tower._supports_sdpa else \"eager\"\n+\n+                # `None` as it is the requested one which will be assigned to each sub-config\n+                # Sub-model will dispatch to SDPA if it can (checked below that `SDPA` layers are present)\n+                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(model.language_model.config._attn_implementation == text_attn)\n+                self.assertTrue(model.audio_tower.config._attn_implementation == vision_attn)\n+\n+                model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n+                model_eager = model_eager.eval().to(torch_device)\n+                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.language_model.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.audio_tower.config._attn_implementation == \"eager\")\n+\n+                for name, submodule in model_eager.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+                has_sdpa = False\n+                for name, submodule in model_sdpa.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        has_sdpa = True\n+                        break\n+                if not has_sdpa and model_sdpa.config.model_type != \"falcon\":\n+                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n+\n \n @require_torch\n class Qwen2AudioForConditionalGenerationIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "2fe06b1511a471d45a9a43e89aeb65bc252b3b82",
            "filename": "tests/models/siglip/test_modeling_siglip.py",
            "status": "modified",
            "additions": 63,
            "deletions": 17,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -71,6 +71,51 @@\n \n \n class SiglipModelTesterMixin(ModelTesterMixin):\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+\n+                # Load the model with SDPA\n+                model_sdpa = model_class.from_pretrained(tmpdirname)\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                # Load model with eager attention\n+                model_eager = model_class.from_pretrained(\n+                    tmpdirname,\n+                    attn_implementation=\"eager\",\n+                )\n+                model_eager = model_eager.eval().to(torch_device)\n+\n+            # SigLip has one shared cls attr for all models, so we assign both submodels heer\n+            vision_attn = text_attn = \"sdpa\" if model._supports_sdpa else \"eager\"\n+\n+            if hasattr(model_sdpa, \"vision_model\") and hasattr(model_sdpa, \"text_model\"):\n+                self.assertTrue(model_sdpa.vision_model.config._attn_implementation == vision_attn)\n+                self.assertTrue(model_sdpa.text_model.config._attn_implementation == text_attn)\n+                self.assertTrue(model_eager.vision_model.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.text_model.config._attn_implementation == \"eager\")\n+\n+            self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+            self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+\n+            for name, submodule in model_eager.named_modules():\n+                class_name = submodule.__class__.__name__\n+                if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                    raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+            has_sdpa = False\n+            for name, submodule in model_sdpa.named_modules():\n+                class_name = submodule.__class__.__name__\n+                if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                    has_sdpa = True\n+                    break\n+            if not has_sdpa and model_sdpa.config.model_type != \"falcon\":\n+                raise ValueError(\"The SDPA model should have SDPA attention layers\")\n+\n     def test_eager_matches_sdpa_inference(\n         self,\n         torch_dtype: str,\n@@ -132,23 +177,6 @@ def get_mean_reldiff(msg, current_case, x, ref, atol, rtol):\n                 )\n                 model_eager = model_eager.eval().to(torch_device)\n \n-            self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n-            self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n-\n-            for name, submodule in model_eager.named_modules():\n-                class_name = submodule.__class__.__name__\n-                if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                    raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-            has_sdpa = False\n-            for name, submodule in model_sdpa.named_modules():\n-                class_name = submodule.__class__.__name__\n-                if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                    has_sdpa = True\n-                    break\n-            if not has_sdpa and model_sdpa.config.model_type != \"falcon\":\n-                raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n             # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving the model each time,\n             # but it would be nicer to have an efficient way to use parameterized.expand\n             cases = [\n@@ -400,6 +428,10 @@ def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n             use_attention_mask_options=(False,),\n         )\n \n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        super().test_sdpa_can_dispatch_composite_models()\n+\n \n class SiglipTextModelTester:\n     def __init__(\n@@ -562,6 +594,10 @@ def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n             use_attention_mask_options=(False, True),\n         )\n \n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        super().test_sdpa_can_dispatch_composite_models()\n+\n \n class SiglipModelTester:\n     def __init__(self, parent, text_kwargs=None, vision_kwargs=None, is_training=True):\n@@ -629,6 +665,7 @@ class SiglipModelTest(SiglipModelTesterMixin, PipelineTesterMixin, unittest.Test\n     test_cpu_offload = False\n     test_disk_offload_safetensors = False\n     test_disk_offload_bin = False\n+    _is_composite = True\n \n     # Copied from tests.models.clip.test_modeling_clip.CLIPModelTest.setUp with CLIP->Siglip\n     def setUp(self):\n@@ -851,6 +888,10 @@ def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n             use_attention_mask_options=(False, True),\n         )\n \n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        super().test_sdpa_can_dispatch_composite_models()\n+\n \n class SiglipForImageClassificationModelTester(SiglipModelTester):\n     def __init__(self, parent):\n@@ -888,6 +929,7 @@ class SiglipForImageClassificationModelTest(SiglipModelTesterMixin, PipelineTest\n     test_cpu_offload = False\n     test_disk_offload_safetensors = False\n     test_disk_offload_bin = False\n+    _is_composite = True\n \n     def setUp(self):\n         self.model_tester = SiglipForImageClassificationModelTester(self)\n@@ -925,6 +967,10 @@ def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n             torch_dtype=torch_dtype, logit_keys=(\"logits\",), use_attention_mask_options=(False,)\n         )\n \n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        super().test_sdpa_can_dispatch_composite_models()\n+\n \n # We will verify our results on an image of cute cats\n def prepare_img():"
        },
        {
            "sha": "6e0b7fa9782fbc401ea70d50eec69ba0239c2fe5",
            "filename": "tests/models/speech_encoder_decoder/test_modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 67,
            "deletions": 1,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_speech_encoder_decoder.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -18,7 +18,13 @@\n import unittest\n \n from transformers import is_torch_available\n-from transformers.testing_utils import require_deterministic_for_xpu, require_torch, slow, torch_device\n+from transformers.testing_utils import (\n+    require_deterministic_for_xpu,\n+    require_torch,\n+    require_torch_sdpa,\n+    slow,\n+    torch_device,\n+)\n \n from ...test_modeling_common import floats_tensor, ids_tensor, random_attention_mask\n from ..bert.test_modeling_bert import BertModelTester\n@@ -441,6 +447,66 @@ def test_real_model_save_load_from_pretrained(self):\n                 max_diff = np.amax(np.abs(out_1 - out_2))\n                 self.assertLessEqual(max_diff, 1e-5)\n \n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        inputs_dict = self.prepare_config_and_inputs()\n+        encoder_config, decoder_config = inputs_dict[\"config\"], inputs_dict[\"decoder_config\"]\n+        config = SpeechEncoderDecoderConfig.from_encoder_decoder_configs(\n+            encoder_config=encoder_config, decoder_config=decoder_config\n+        )\n+        model = SpeechEncoderDecoderModel(config=config)\n+\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            model.save_pretrained(tmpdirname)\n+            model_sdpa = SpeechEncoderDecoderModel.from_pretrained(tmpdirname)\n+            model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+            # see https://github.com/huggingface/transformers/pull/32238\n+            # Sub-model will dispatch to SDPA if it can (checked below that `SDPA` layers are present)\n+            encoder_attn = \"sdpa\" if model.encoder._supports_sdpa else \"eager\"\n+            decoder_attn = \"sdpa\" if model.decoder._supports_sdpa else \"eager\"\n+            self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+            self.assertTrue(model_sdpa.encoder.config._attn_implementation == encoder_attn)\n+            self.assertTrue(model_sdpa.decoder.config._attn_implementation == decoder_attn)\n+\n+            # Also test that nothing break if we request SDPA explicitly, when both sub-parts support it.\n+            # If the model supports sdpa (i.e. all of sub-models supports it) we'll dispatch safely\n+            # Otherwise we should raise error that SDPA is not supported, as some of the sub-models doesn't support\n+            if encoder_attn == \"sdpa\" and decoder_attn == \"sdpa\":\n+                model_sdpa_explicit = SpeechEncoderDecoderModel.from_pretrained(tmpdirname, attn_implementation=\"sdpa\")\n+                model_sdpa_explicit = model_sdpa_explicit.eval().to(torch_device)\n+\n+                self.assertTrue(model_sdpa_explicit.config._attn_implementation == \"sdpa\")\n+            else:\n+                with self.assertRaises(ValueError):\n+                    model_sdpa_explicit = SpeechEncoderDecoderModel.from_pretrained(\n+                        tmpdirname, attn_implementation=\"sdpa\"\n+                    )\n+\n+            model_eager = SpeechEncoderDecoderModel.from_pretrained(\n+                tmpdirname,\n+                attn_implementation=\"eager\",\n+            )\n+            model_eager = model_eager.eval().to(torch_device)\n+\n+            self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+            self.assertTrue(model_eager.encoder.config._attn_implementation == \"eager\")\n+            self.assertTrue(model_eager.decoder.config._attn_implementation == \"eager\")\n+\n+            for name, submodule in model_eager.named_modules():\n+                class_name = submodule.__class__.__name__\n+                if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                    raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+            has_sdpa = False\n+            for name, submodule in model_sdpa.named_modules():\n+                class_name = submodule.__class__.__name__\n+                if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                    has_sdpa = True\n+                    break\n+            if not has_sdpa:\n+                raise ValueError(\"The SDPA model should have SDPA attention layers\")\n+\n \n @require_torch\n class Wav2Vec2BertModelTest(EncoderDecoderMixin, unittest.TestCase):"
        },
        {
            "sha": "1bd01843981deb2b779c6c427cff73997f963dc9",
            "filename": "tests/models/video_llava/test_modeling_video_llava.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -206,6 +206,7 @@ class VideoLlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTe\n     test_pruning = False\n     test_resize_embeddings = True\n     test_head_masking = False\n+    _is_composite = True\n \n     def setUp(self):\n         self.model_tester = VideoLlavaVisionText2TextModelTester(self)\n@@ -237,6 +238,16 @@ def test_sdpa_can_compile_dynamic(self):\n     def test_sdpa_can_dispatch_on_flash(self):\n         pass\n \n+    @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n+    def test_flash_attn_2_fp32_ln(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"VLMs need lots of steps to prepare images/mask correctly to get pad-free inputs. Can be tested as part of LLM test\"\n+    )\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n     @unittest.skip(\n         reason=\"After #33533, this still passes, but many subsequential tests fail with `device-side assert triggered`\"\n     )"
        },
        {
            "sha": "2c241c23f26158a5b6f9b9bdcc5bce86ec097c2e",
            "filename": "tests/models/vipllava/test_modeling_vipllava.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -168,6 +168,7 @@ class VipLlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTest\n     test_pruning = False\n     test_resize_embeddings = True\n     test_head_masking = False\n+    _is_composite = True\n \n     def setUp(self):\n         self.model_tester = VipLlavaVisionText2TextModelTester(self)\n@@ -242,6 +243,16 @@ def test_sdpa_can_compile_dynamic(self):\n     def test_sdpa_can_dispatch_on_flash(self):\n         pass\n \n+    @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n+    def test_flash_attn_2_fp32_ln(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"VLMs need lots of steps to prepare images/mask correctly to get pad-free inputs. Can be tested as part of LLM test\"\n+    )\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n \n @require_torch\n class VipLlavaForConditionalGenerationIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "7def8a9ac965076f94c0953854f3da28c85f4429",
            "filename": "tests/models/vision_encoder_decoder/test_modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 313,
            "deletions": 1,
            "changes": 314,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -27,17 +27,24 @@\n     require_nltk,\n     require_sentencepiece,\n     require_torch,\n+    require_torch_sdpa,\n     require_vision,\n     slow,\n     to_2tuple,\n     torch_device,\n )\n-from transformers.utils import cached_property, is_torch_available, is_vision_available\n+from transformers.utils import (\n+    cached_property,\n+    is_torch_available,\n+    is_vision_available,\n+)\n \n from ...test_modeling_common import floats_tensor, ids_tensor, random_attention_mask\n from ..bart.test_modeling_bart import BartModelTester\n from ..bert.test_modeling_bert import BertModelTester\n from ..deit.test_modeling_deit import DeiTModelTester\n+from ..donut.test_modeling_donut_swin import DonutSwinModelTester\n+from ..gpt2.test_modeling_gpt2 import GPT2ModelTester\n from ..layoutlmv3.test_modeling_layoutlmv3 import LayoutLMv3ModelTester\n from ..swin.test_modeling_swin import SwinModelTester\n from ..trocr.test_modeling_trocr import TrOCRStandaloneDecoderModelTester\n@@ -53,6 +60,8 @@\n         BartForCausalLM,\n         BertLMHeadModel,\n         DeiTModel,\n+        DonutSwinModel,\n+        GPT2LMHeadModel,\n         LayoutLMv3Model,\n         SwinModel,\n         TrOCRForCausalLM,\n@@ -72,6 +81,8 @@\n \n @require_torch\n class EncoderDecoderMixin:\n+    supports_sdpa = False\n+\n     def get_encoder_decoder_model(self, config, decoder_config):\n         pass\n \n@@ -374,6 +385,69 @@ def test_real_model_save_load_from_pretrained(self):\n                 max_diff = np.amax(np.abs(out_1 - out_2))\n                 self.assertLessEqual(max_diff, 1e-5)\n \n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        if not self.supports_sdpa:\n+            self.skipTest(\"SDPA is not supported\")\n+\n+        inputs_dict = self.prepare_config_and_inputs()\n+        encoder_config, decoder_config = inputs_dict[\"config\"], inputs_dict[\"decoder_config\"]\n+        config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(\n+            encoder_config=encoder_config, decoder_config=decoder_config\n+        )\n+        model = VisionEncoderDecoderModel(config=config)\n+\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            model.save_pretrained(tmpdirname)\n+            model_sdpa = VisionEncoderDecoderModel.from_pretrained(tmpdirname)\n+            model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+            # see https://github.com/huggingface/transformers/pull/32238\n+            # Sub-model will dispatch to SDPA if it can (checked below that `SDPA` layers are present)\n+            encoder_attn = \"sdpa\" if model.encoder._supports_sdpa else \"eager\"\n+            decoder_attn = \"sdpa\" if model.decoder._supports_sdpa else \"eager\"\n+            self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+            self.assertTrue(model_sdpa.encoder.config._attn_implementation == encoder_attn)\n+            self.assertTrue(model_sdpa.decoder.config._attn_implementation == decoder_attn)\n+\n+            # Also test that nothing break if we request SDPA explicitly, when both sub-parts support it.\n+            # If the model supports sdpa (i.e. all of sub-models supports it) we'll dispatch safely\n+            # Otherwise we should raise error that SDPA is not supported, as some of the sub-models doesn't support\n+            if encoder_attn == \"sdpa\" and decoder_attn == \"sdpa\":\n+                model_sdpa_explicit = VisionEncoderDecoderModel.from_pretrained(tmpdirname, attn_implementation=\"sdpa\")\n+                model_sdpa_explicit = model_sdpa_explicit.eval().to(torch_device)\n+\n+                self.assertTrue(model_sdpa_explicit.config._attn_implementation == \"sdpa\")\n+            else:\n+                with self.assertRaises(ValueError):\n+                    model_sdpa_explicit = VisionEncoderDecoderModel.from_pretrained(\n+                        tmpdirname, attn_implementation=\"sdpa\"\n+                    )\n+\n+            model_eager = VisionEncoderDecoderModel.from_pretrained(\n+                tmpdirname,\n+                attn_implementation=\"eager\",\n+            )\n+            model_eager = model_eager.eval().to(torch_device)\n+\n+            self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+            self.assertTrue(model_eager.encoder.config._attn_implementation == \"eager\")\n+            self.assertTrue(model_eager.decoder.config._attn_implementation == \"eager\")\n+\n+            for name, submodule in model_eager.named_modules():\n+                class_name = submodule.__class__.__name__\n+                if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                    raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+            has_sdpa = False\n+            for name, submodule in model_sdpa.named_modules():\n+                class_name = submodule.__class__.__name__\n+                if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                    has_sdpa = True\n+                    break\n+            if not has_sdpa:\n+                raise ValueError(\"The SDPA model should have SDPA attention layers\")\n+\n \n @require_torch\n class DeiT2RobertaModelTest(EncoderDecoderMixin, unittest.TestCase):\n@@ -497,6 +571,8 @@ def prepare_config_and_inputs(self):\n \n @require_torch\n class ViT2BertModelTest(EncoderDecoderMixin, unittest.TestCase):\n+    supports_sdpa = True  # one submodel support SDPA\n+\n     def get_pretrained_model_and_inputs(self):\n         model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n             \"hf-internal-testing/tiny-random-vit\", \"hf-internal-testing/tiny-bert\"\n@@ -649,6 +725,8 @@ def test_real_model_save_load_from_pretrained(self):\n \n @require_torch\n class ViT2TrOCR(EncoderDecoderMixin, unittest.TestCase):\n+    supports_sdpa = True  # one submodel support SDPA\n+\n     def get_encoder_decoder_model(self, config, decoder_config):\n         encoder_model = ViTModel(config).eval()\n         decoder_model = TrOCRForCausalLM(decoder_config).eval()\n@@ -804,6 +882,240 @@ def test_real_model_save_load_from_pretrained(self):\n         pass\n \n \n+@require_torch\n+class VIT2GPT2Test(EncoderDecoderMixin, unittest.TestCase):\n+    supports_sdpa = True  # both submodels support SDPA\n+\n+    def get_encoder_decoder_model(self, config, decoder_config):\n+        encoder_model = ViTModel(config).eval()\n+        decoder_model = GPT2LMHeadModel(decoder_config).eval()\n+        return encoder_model, decoder_model\n+\n+    def prepare_config_and_inputs(self):\n+        model_tester_encoder = ViTModelTester(self, batch_size=13)\n+        model_tester_decoder = GPT2ModelTester(self, batch_size=13, hidden_size=32, max_position_embeddings=512)\n+        encoder_config_and_inputs = model_tester_encoder.prepare_config_and_inputs()\n+        decoder_config_and_inputs = model_tester_decoder.prepare_config_and_inputs()\n+        config, pixel_values, labels = encoder_config_and_inputs\n+        (\n+            decoder_config,\n+            decoder_input_ids,\n+            decoder_attention_mask,\n+            decoder_head_mask,\n+            decoder_token_type_ids,\n+            mc_token_ids,\n+            sequence_labels,\n+            token_labels,\n+            choice_labels,\n+        ) = decoder_config_and_inputs\n+\n+        # make sure that cross attention layers are added\n+        decoder_config.add_cross_attention = True\n+        #  disable cache for now\n+        decoder_config.use_cache = False\n+        return {\n+            \"config\": config,\n+            \"pixel_values\": pixel_values,\n+            \"decoder_config\": decoder_config,\n+            \"decoder_input_ids\": decoder_input_ids,\n+            \"decoder_attention_mask\": decoder_attention_mask,\n+            \"decoder_head_mask\": decoder_head_mask,\n+            \"labels\": decoder_input_ids,\n+        }\n+\n+    def check_encoder_decoder_model_output_attentions(\n+        self,\n+        config,\n+        decoder_config,\n+        decoder_input_ids,\n+        decoder_attention_mask,\n+        pixel_values,\n+        labels=None,\n+        **kwargs,\n+    ):\n+        # make the decoder inputs a different shape from the encoder inputs to harden the test\n+        decoder_input_ids = decoder_input_ids[:, :-1]\n+        decoder_attention_mask = decoder_attention_mask[:, :-1]\n+        encoder_model, decoder_model = self.get_encoder_decoder_model(config, decoder_config)\n+        enc_dec_model = VisionEncoderDecoderModel(encoder=encoder_model, decoder=decoder_model)\n+        enc_dec_model.to(torch_device)\n+        outputs_encoder_decoder = enc_dec_model(\n+            pixel_values=pixel_values,\n+            decoder_input_ids=decoder_input_ids,\n+            decoder_attention_mask=decoder_attention_mask,\n+            output_attentions=True,\n+            **kwargs,\n+        )\n+\n+        encoder_attentions = outputs_encoder_decoder[\"encoder_attentions\"]\n+        self.assertEqual(len(encoder_attentions), config.num_hidden_layers)\n+\n+        seq_len = (encoder_model.config.image_size // encoder_model.config.patch_size) ** 2 + 1\n+\n+        decoder_attentions = outputs_encoder_decoder[\"decoder_attentions\"]\n+        num_decoder_layers = (\n+            decoder_config.num_decoder_layers\n+            if hasattr(decoder_config, \"num_decoder_layers\")\n+            else decoder_config.num_hidden_layers\n+        )\n+        self.assertEqual(len(decoder_attentions), num_decoder_layers)\n+\n+        self.assertEqual(\n+            decoder_attentions[0].shape[-3:],\n+            (decoder_config.num_attention_heads, decoder_input_ids.shape[-1], decoder_input_ids.shape[-1]),\n+        )\n+\n+        cross_attentions = outputs_encoder_decoder[\"cross_attentions\"]\n+        self.assertEqual(len(cross_attentions), num_decoder_layers)\n+\n+        cross_attention_input_seq_len = decoder_input_ids.shape[-1]\n+        self.assertEqual(\n+            cross_attentions[0].shape[-3:],\n+            (decoder_config.num_attention_heads, cross_attention_input_seq_len, seq_len),  # 4 6 16\n+        )\n+\n+    def check_encoder_decoder_model_generate(self, config, decoder_config, pixel_values=None, **kwargs):\n+        encoder_model, decoder_model = self.get_encoder_decoder_model(config, decoder_config)\n+        enc_dec_model = VisionEncoderDecoderModel(encoder=encoder_model, decoder=decoder_model)\n+\n+        # Generate until max length\n+        if hasattr(enc_dec_model.config, \"eos_token_id\"):\n+            enc_dec_model.config.eos_token_id = None\n+        if hasattr(enc_dec_model.config, \"decoder\") and hasattr(enc_dec_model.config.decoder, \"eos_token_id\"):\n+            enc_dec_model.config.decoder.eos_token_id = None\n+        if hasattr(enc_dec_model.generation_config, \"eos_token_id\"):\n+            enc_dec_model.generation_config.eos_token_id = None\n+        enc_dec_model.to(torch_device)\n+\n+        generated_output = enc_dec_model.generate(\n+            pixel_values=pixel_values,\n+            decoder_start_token_id=enc_dec_model.config.decoder.bos_token_id,\n+            **kwargs,\n+        )\n+        self.assertEqual(generated_output.shape, (pixel_values.shape[0],) + (decoder_config.max_length,))\n+\n+    @unittest.skip(reason=\"VIT2GPT2 also has an integration test for testinf save-load\")\n+    def test_real_model_save_load_from_pretrained(self):\n+        pass\n+\n+\n+@require_torch\n+class Donut2GPT2Test(EncoderDecoderMixin, unittest.TestCase):\n+    supports_sdpa = True  # one submodel (GPT2) support SDPA\n+\n+    def get_encoder_decoder_model(self, config, decoder_config):\n+        encoder_model = DonutSwinModel(config).eval()\n+        decoder_model = GPT2LMHeadModel(decoder_config).eval()\n+        return encoder_model, decoder_model\n+\n+    def prepare_config_and_inputs(self):\n+        model_tester_encoder = DonutSwinModelTester(self, batch_size=13)\n+        model_tester_decoder = GPT2ModelTester(self, batch_size=13, hidden_size=32, max_position_embeddings=512)\n+        encoder_config_and_inputs = model_tester_encoder.prepare_config_and_inputs()\n+        decoder_config_and_inputs = model_tester_decoder.prepare_config_and_inputs()\n+        config, pixel_values, labels = encoder_config_and_inputs\n+        (\n+            decoder_config,\n+            decoder_input_ids,\n+            decoder_attention_mask,\n+            decoder_head_mask,\n+            decoder_token_type_ids,\n+            mc_token_ids,\n+            sequence_labels,\n+            token_labels,\n+            choice_labels,\n+        ) = decoder_config_and_inputs\n+\n+        # make sure that cross attention layers are added\n+        decoder_config.add_cross_attention = True\n+        #  disable cache for now\n+        decoder_config.use_cache = False\n+        return {\n+            \"config\": config,\n+            \"pixel_values\": pixel_values,\n+            \"decoder_config\": decoder_config,\n+            \"decoder_input_ids\": decoder_input_ids,\n+            \"decoder_attention_mask\": decoder_attention_mask,\n+            \"decoder_head_mask\": decoder_head_mask,\n+            \"labels\": decoder_input_ids,\n+        }\n+\n+    def check_encoder_decoder_model_output_attentions(\n+        self,\n+        config,\n+        decoder_config,\n+        decoder_input_ids,\n+        decoder_attention_mask,\n+        pixel_values,\n+        labels=None,\n+        **kwargs,\n+    ):\n+        # make the decoder inputs a different shape from the encoder inputs to harden the test\n+        decoder_input_ids = decoder_input_ids[:, :-1]\n+        decoder_attention_mask = decoder_attention_mask[:, :-1]\n+        encoder_model, decoder_model = self.get_encoder_decoder_model(config, decoder_config)\n+        enc_dec_model = VisionEncoderDecoderModel(encoder=encoder_model, decoder=decoder_model)\n+        enc_dec_model.to(torch_device)\n+        outputs_encoder_decoder = enc_dec_model(\n+            pixel_values=pixel_values,\n+            decoder_input_ids=decoder_input_ids,\n+            decoder_attention_mask=decoder_attention_mask,\n+            output_attentions=True,\n+            **kwargs,\n+        )\n+\n+        encoder_attentions = outputs_encoder_decoder[\"encoder_attentions\"]\n+        self.assertEqual(len(encoder_attentions), config.num_hidden_layers)\n+\n+        seq_len = encoder_model.config.image_size // encoder_model.config.patch_size\n+\n+        decoder_attentions = outputs_encoder_decoder[\"decoder_attentions\"]\n+        num_decoder_layers = (\n+            decoder_config.num_decoder_layers\n+            if hasattr(decoder_config, \"num_decoder_layers\")\n+            else decoder_config.num_hidden_layers\n+        )\n+        self.assertEqual(len(decoder_attentions), num_decoder_layers)\n+\n+        self.assertEqual(\n+            decoder_attentions[0].shape[-3:],\n+            (decoder_config.num_attention_heads, decoder_input_ids.shape[-1], decoder_input_ids.shape[-1]),\n+        )\n+\n+        cross_attentions = outputs_encoder_decoder[\"cross_attentions\"]\n+        self.assertEqual(len(cross_attentions), num_decoder_layers)\n+\n+        cross_attention_input_seq_len = decoder_input_ids.shape[-1]\n+        self.assertEqual(\n+            cross_attentions[0].shape[-3:],\n+            (decoder_config.num_attention_heads, cross_attention_input_seq_len, seq_len),  # 4 6 16\n+        )\n+\n+    def check_encoder_decoder_model_generate(self, config, decoder_config, pixel_values=None, **kwargs):\n+        encoder_model, decoder_model = self.get_encoder_decoder_model(config, decoder_config)\n+        enc_dec_model = VisionEncoderDecoderModel(encoder=encoder_model, decoder=decoder_model)\n+\n+        # Generate until max length\n+        if hasattr(enc_dec_model.config, \"eos_token_id\"):\n+            enc_dec_model.config.eos_token_id = None\n+        if hasattr(enc_dec_model.config, \"decoder\") and hasattr(enc_dec_model.config.decoder, \"eos_token_id\"):\n+            enc_dec_model.config.decoder.eos_token_id = None\n+        if hasattr(enc_dec_model.generation_config, \"eos_token_id\"):\n+            enc_dec_model.generation_config.eos_token_id = None\n+        enc_dec_model.to(torch_device)\n+\n+        generated_output = enc_dec_model.generate(\n+            pixel_values=pixel_values,\n+            decoder_start_token_id=enc_dec_model.config.decoder.bos_token_id,\n+            **kwargs,\n+        )\n+        self.assertEqual(generated_output.shape, (pixel_values.shape[0],) + (decoder_config.max_length,))\n+\n+    @unittest.skip(reason=\"Donut has an Integration test for that\")\n+    def test_real_model_save_load_from_pretrained(self):\n+        pass\n+\n+\n @require_vision\n @require_torch\n class TrOCRModelIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "dec1482f562a330a7bdd62d8db9903ceebb1556b",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 203,
            "deletions": 39,
            "changes": 242,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -207,6 +207,7 @@ class ModelTesterMixin:\n     test_model_parallel = False\n     is_encoder_decoder = False\n     has_attentions = True\n+    _is_composite = False\n     model_split_percents = [0.5, 0.7, 0.9]\n \n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n@@ -3006,6 +3007,7 @@ def test_inputs_embeds_matches_input_ids_with_generate(self):\n                 *get_values(MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES),\n             ]:\n                 continue\n+\n             model = model_class(config)\n             model.to(torch_device)\n             model.eval()\n@@ -3950,6 +3952,147 @@ def test_flash_attn_2_generate_padding_right(self):\n \n                 self.assertTrue(torch.allclose(out, out_fa))\n \n+    def test_attn_implementation_composite_models(self):\n+        \"\"\"\n+        Tests if composite models can receive a dict object as attn_implementation, where each key should be\n+        one of the sub-configs from the model's config.\n+        \"\"\"\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        for model_class in self.all_model_classes:\n+            if not self._is_composite:\n+                self.skipTest(\"Model is not a composite model.\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            sub_configs = {\n+                key: getattr(config, key) for key in config if isinstance(getattr(config, key), PretrainedConfig)\n+            }\n+\n+            # set eager as it will be the one supported in all models\n+            # we just need to test if passing 'attn_implementation' as a dict fails or not\n+            attn_implementation_per_subconfig = {}\n+            for key, sub_config in sub_configs.items():\n+                attn_implementation_per_subconfig[key] = \"eager\"\n+\n+            config._attn_implementation = attn_implementation_per_subconfig\n+            model = model_class(config)\n+            for key in model.config:\n+                if isinstance(getattr(model.config, key), PretrainedConfig):\n+                    sub_config = getattr(model.config, key)\n+                    self.assertTrue(sub_config._attn_implementation == \"eager\")\n+\n+            for name, submodule in model.named_modules():\n+                class_name = submodule.__class__.__name__\n+                if (\n+                    \"SdpaAttention\" in class_name\n+                    or \"SdpaSelfAttention\" in class_name\n+                    or \"FlashAttention\" in class_name\n+                ):\n+                    raise ValueError(\"The eager model should not have SDPA/FA2 attention layers\")\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_non_composite_models(self):\n+        \"\"\"\n+        Tests if non-composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n+        This tests only by looking at layer names, as usually SDPA layers are calles \"SDPAAttention\".\n+        \"\"\"\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        if not self.all_model_classes[0]._supports_sdpa or self._is_composite:\n+            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_sdpa = model_class.from_pretrained(tmpdirname)\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+\n+                model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n+                model_eager = model_eager.eval().to(torch_device)\n+                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+\n+                for name, submodule in model_eager.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+                has_sdpa = False\n+                for name, submodule in model_sdpa.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        has_sdpa = True\n+                        break\n+                if not has_sdpa and model_sdpa.config.model_type != \"falcon\":\n+                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        \"\"\"\n+        Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n+        This tests only by looking at layer names, as usually SDPA layers are calles \"SDPAAttention\".\n+        In contrast to the above test, this one checks if the \"config._attn_implamentation\" is a dict after the model\n+        is loaded, because we manually replicate requested attn implementation on each sub-config when loading.\n+        See https://github.com/huggingface/transformers/pull/32238 for more info\n+\n+        The test tries to cover most general cases of composite models, VLMs with vision and text configs. Any model\n+        that has a different set of sub-configs has to overwrite this test.\n+        \"\"\"\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        if not self._is_composite:\n+            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_sdpa = model_class.from_pretrained(tmpdirname)\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                vision_model_names = {\"visual\", \"image_tower\", \"vision_tower\", \"vision_model\"}\n+                language_model_names = {\"language_model\", \"model\", \"text_model\"}\n+                vision_model_name = [name for name in vision_model_names if hasattr(model_sdpa, name)][0]\n+                language_model_name = [name for name in language_model_names if hasattr(model_sdpa, name)][0]\n+\n+                vision_model_sdpa = getattr(model, vision_model_name)\n+                language_model_sdpa = getattr(model, language_model_name)\n+                text_attn = \"sdpa\" if language_model_sdpa._supports_sdpa else \"eager\"\n+                vision_attn = \"sdpa\" if vision_model_sdpa._supports_sdpa else \"eager\"\n+\n+                # `None` as it is the requested one which will be assigned to each sub-config\n+                # Sub-model will dispatch to SDPA if it can (checked below that `SDPA` layers are present)\n+                self.assertTrue(language_model_sdpa.config._attn_implementation == text_attn)\n+                self.assertTrue(vision_model_sdpa.config._attn_implementation == vision_attn)\n+\n+                model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n+                model_eager = model_eager.eval().to(torch_device)\n+                self.assertTrue(getattr(model_eager, language_model_name).config._attn_implementation == \"eager\")\n+                self.assertTrue(getattr(model_eager, vision_model_name).config._attn_implementation == \"eager\")\n+\n+                for name, submodule in model_eager.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+                has_sdpa = False\n+                for name, submodule in model_sdpa.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        has_sdpa = True\n+                        break\n+                if not has_sdpa and any(module_attn == \"sdpa\" for module_attn in [text_attn, vision_attn]):\n+                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n+\n     @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n     @require_torch_sdpa\n     @slow\n@@ -4012,39 +4155,20 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n             # This means that the class needs to be instantiated much later, after `use_mask` is set, which means a significant refactor of the code.\n             # However masking there is not done at any layers that matters (i.e self-attention), therefore we can safely deactivate it.\n             deactivate_mask = \"use_mask_token\" in inspect.signature(model_class).parameters\n-\n             is_encoder_decoder = model.config.is_encoder_decoder\n \n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n                 model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)\n                 model_sdpa = model_sdpa.eval().to(torch_device)\n \n-                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n-\n                 model_eager = model_class.from_pretrained(\n                     tmpdirname,\n                     torch_dtype=torch_dtype,\n                     attn_implementation=\"eager\",\n                 )\n                 model_eager = model_eager.eval().to(torch_device)\n \n-                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n-\n-                for name, submodule in model_eager.named_modules():\n-                    class_name = submodule.__class__.__name__\n-                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-                has_sdpa = False\n-                for name, submodule in model_sdpa.named_modules():\n-                    class_name = submodule.__class__.__name__\n-                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                        has_sdpa = True\n-                        break\n-                if not has_sdpa and model_sdpa.config.model_type != \"falcon\":\n-                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n                 # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving 16 times the model,\n                 # but it would be nicer to have an efficient way to use parameterized.expand\n                 fail_cases = []\n@@ -4279,7 +4403,7 @@ def test_sdpa_can_dispatch_on_flash(self):\n                 self.skipTest(\n                     \"PaliGemma-like models currently (transformers==4.41.0) requires an attention_mask input\"\n                 )\n-            if config.model_type in [\"idefics\"]:\n+            if config.model_type in [\"idefics\", \"idefics2\", \"idefics3\"]:\n                 self.skipTest(reason=\"Idefics currently (transformers==4.39.1) requires an image_attention_mask input\")\n             model = model_class(config)\n \n@@ -4382,31 +4506,13 @@ def test_eager_matches_sdpa_generate(self):\n                     low_cpu_mem_usage=True,\n                 ).to(torch_device)\n \n-                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n-\n                 model_eager = model_class.from_pretrained(\n                     tmpdirname,\n                     torch_dtype=torch.float16,\n                     low_cpu_mem_usage=True,\n                     attn_implementation=\"eager\",\n                 ).to(torch_device)\n \n-                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n-\n-                for name, submodule in model_eager.named_modules():\n-                    class_name = submodule.__class__.__name__\n-                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-                has_sdpa = False\n-                for name, submodule in model_sdpa.named_modules():\n-                    class_name = submodule.__class__.__name__\n-                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                        has_sdpa = True\n-                        break\n-                if not has_sdpa:\n-                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n                 # Just test that a large cache works as expected\n                 res_eager = model_eager.generate(\n                     dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=max_new_tokens, do_sample=False\n@@ -4429,6 +4535,8 @@ def test_sdpa_matches_eager_sliding_window(self):\n             self.skipTest(f\"No generative model classes for {self.__class__.__name__}\")\n \n         for model_class in self.all_generative_model_classes:\n+            if model_class._supports_sdpa:\n+                self.skipTest(reason=\"Model architecture does not support attentions\")\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n             if config.model_type not in WINDOW_ATTENTION_MODELS:\n@@ -4531,6 +4639,62 @@ def test_flash_attn_2_generate_use_cache(self):\n                     use_cache=True,\n                 )\n \n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    def test_flash_attn_2_can_dispatch_composite_models(self):\n+        \"\"\"\n+        Tests if composite models can dispatch on FA2 if the sub-models support FA2.\n+        The tests is needed as we handle differently composite models and we cannot check them\n+        with above tests. If any of the sub-models does not support FA2, we'll raise an error when dispatching\n+        that particular sub-model. Otherwise we dispatch safely in all sub-models, where \"sub-models\" are specific\n+        backbone models (LM/vision/audio/etc)\n+        \"\"\"\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        if not is_torch_fp16_available_on_device(torch_device):\n+            self.skipTest(f\"float16 not supported on {torch_device} (on the specific device currently used)\")\n+\n+        torch_dtype = torch.float16\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+            if not self._is_composite:\n+                self.skipTest(\"This model is not a composte model!\")\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)\n+\n+                supports_fa2_all_modules = all(\n+                    module._supports_flash_attn_2\n+                    for name, module in model.named_modules()\n+                    if isinstance(module, PreTrainedModel) and name != \"\"\n+                )\n+                if not supports_fa2_all_modules:\n+                    with self.assertRaises(ValueError):\n+                        model_fa2 = model_class.from_pretrained(\n+                            tmpdirname, torch_dtype=torch_dtype, attn_implementation=\"flash_attention_2\"\n+                        )\n+                else:\n+                    model_fa2 = model_class.from_pretrained(\n+                        tmpdirname, torch_dtype=torch_dtype, attn_implementation=\"flash_attention_2\"\n+                    )\n+                    for key in model_fa2.config:\n+                        if isinstance(getattr(model_fa2.config, key), PretrainedConfig):\n+                            sub_config = getattr(model_fa2.config, key)\n+                            self.assertTrue(sub_config._attn_implementation == \"flash_attention_2\")\n+\n+                    has_fa2 = False\n+                    for name, submodule in model_fa2.named_modules():\n+                        class_name = submodule.__class__.__name__\n+                        if \"FlashAttention\" in class_name:\n+                            has_fa2 = True\n+                            break\n+                    if not has_fa2:\n+                        raise ValueError(\"The FA2 model should have FA2 layers\")\n+\n     @require_flash_attn\n     @require_torch_gpu\n     @mark.flash_attn_test\n@@ -4679,7 +4843,7 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n                 if 0 in inputs_dict[\"attention_mask\"][:, -1]:\n                     inputs_dict[\"attention_mask\"] = inputs_dict[\"attention_mask\"].flip(1)\n                 dummy_attention_mask = inputs_dict[\"attention_mask\"]\n-                inputs_dict[\"input_ids\"][~dummy_attention_mask.bool()] = config.pad_token_id\n+                inputs_dict[\"input_ids\"][~dummy_attention_mask.bool()] = config.get_text_config().pad_token_id\n \n                 model = (\n                     model_class.from_pretrained("
        },
        {
            "sha": "35a651d0e598739fd65fc9aa801dfb06f7a03b48",
            "filename": "tests/utils/test_configuration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Futils%2Ftest_configuration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/tests%2Futils%2Ftest_configuration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_configuration_utils.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -228,6 +228,7 @@ def test_config_common_kwargs_is_complete(self):\n                 \"_name_or_path\",\n                 \"_commit_hash\",\n                 \"_attn_implementation_internal\",\n+                \"_attn_implementation_autoset\",\n                 \"transformers_version\",\n             ],\n         )"
        },
        {
            "sha": "10be5cdcd262307ae1cbcfe7ba7db6f923c829d5",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/21d5025826857e11a75ef7b23ac15a607be4fc54/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21d5025826857e11a75ef7b23ac15a607be4fc54/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=21d5025826857e11a75ef7b23ac15a607be4fc54",
            "patch": "@@ -82,6 +82,8 @@\n     \"SeamlessM4Tv2TextToUnitModel\",\n     \"SeamlessM4Tv2CodeHifiGan\",\n     \"SeamlessM4Tv2TextToUnitForConditionalGeneration\",\n+    \"Idefics2PerceiverResampler\",\n+    \"Idefics2VisionTransformer\",\n     \"Idefics3VisionTransformer\",\n ]\n \n@@ -225,7 +227,6 @@\n     \"BeitForMaskedImageModeling\",\n     \"ChineseCLIPTextModel\",\n     \"ChineseCLIPVisionModel\",\n-    \"CLIPTextModel\",\n     \"CLIPTextModelWithProjection\",\n     \"CLIPVisionModelWithProjection\",\n     \"ClvpForCausalLM\",\n@@ -327,6 +328,7 @@\n     \"SiglipVisionModel\",\n     \"SiglipTextModel\",\n     \"ChameleonVQVAE\",  # no autoclass for VQ-VAE models\n+    \"CLIPTextModel\",\n     \"MoshiForConditionalGeneration\",  # no auto class for speech-to-speech\n ]\n "
        }
    ],
    "stats": {
        "total": 2638,
        "additions": 1925,
        "deletions": 713
    }
}