{
    "author": "qubvel",
    "message": "Fix docs typo (#40167)\n\n* DINOv3 model\n\n* working version\n\n* linter revert\n\n* linter revert\n\n* linter revert\n\n* fix init\n\n* remove flex and add convert to hf script\n\n* DINOv3 convnext\n\n* working version of convnext\n\n* adding to auto\n\n* Dinov3 -> DINOv3\n\n* PR feedback\n\n* complete convert checkpoint\n\n* fix assertion\n\n* bf16 -> fp32\n\n* add fast image processor\n\n* fixup\n\n* change conversion script\n\n* Use Pixtral attention\n\n* minor renaming\n\n* simplify intermediates capturing\n\n* refactor DINOv3ViTPatchEmbeddings\n\n* Refactor DINOv3ViTEmbeddings\n\n* [WIP] rope: remove unused params\n\n* [WIP] rope: rename period -> inv_freq for consistency\n\n* [WIP] rope: move augs\n\n* change inv_freq init (not persistent anymore)\n\n* [WIP] rope: move coords to init\n\n* rope - done!\n\n* use default LayerScale\n\n* conversion: truncate expected outputs\n\n* remove commented code\n\n* Refactor MLP layers\n\n* nit\n\n* clean up config params\n\n* nit docs\n\n* simplify embeddings\n\n* simplify compile compat lru_cache\n\n* fixup\n\n* dynamic patch coords\n\n* move augmentation\n\n* Fix docs\n\n* fixup and type hints\n\n* fix output capturing\n\n* fix tests\n\n* fixup\n\n* fix auto mappings\n\n* Add draft docs\n\n* fix dtype cast issue\n\n* add push to hub\n\n* add image processor tests\n\n* fixup\n\n* add modular\n\n* update modular\n\n* convert and test convnext\n\n* update conversion script\n\n* update prefix\n\n* Update LayerNorm\n\n* refactor DINOv3ConvNextLayer\n\n* rename\n\n* refactor convnext model\n\n* fix doc check\n\n* fix docs\n\n* fix convnext config\n\n* tmp fix for check docstring\n\n* remove unused arg\n\n* fix tests\n\n* (nit) change init\n\n* standardize gated MLP\n\n* clear namings and sat493m\n\n* fix tensors on different devices\n\n* revert linter\n\n* pr\n\n* pr feedbak ruff format\n\n* missing headers\n\n* fix code snippet and collection link in docs\n\n* DINOv3 description\n\n* fix checkpoints in tests\n\n* not doc fixes in configs\n\n* output_hidden_states\n\n* x -> features\n\n* remove sequential\n\n---------\n\nCo-authored-by: Cijo Jose <cijose@meta.com>",
    "sha": "6f259bc83e518c281877cfc90efe61bf8a79bba0",
    "files": [
        {
            "sha": "c44a55c62226d62086c2b0000c41e051a3c34d05",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0",
            "patch": "@@ -763,6 +763,8 @@\n         title: DINOV2\n       - local: model_doc/dinov2_with_registers\n         title: DINOv2 with Registers\n+      - local: model_doc/dinov3\n+        title: DINOv3\n       - local: model_doc/dit\n         title: DiT\n       - local: model_doc/dpt"
        },
        {
            "sha": "b3f2067fe92dfee76d8d90034a854577c220792c",
            "filename": "docs/source/en/model_doc/dinov3.md",
            "status": "added",
            "additions": 181,
            "deletions": 0,
            "changes": 181,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov3.md?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0",
            "patch": "@@ -0,0 +1,181 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+-->\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n+\n+# DINOv3\n+\n+DINOv3 is a family of versatile vision foundation models that outperforms the specialized state of the art across a broad range of settings, without fine-tuning. DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models.\n+\n+You can find all the original DINOv3 checkpoints under the [DINOv3](https://huggingface.co/collections/facebook/dinov3-68924841bd6b561778e31009) collection.\n+\n+> [!TIP]\n+> Click on the DINOv3 models in the right sidebar for more examples of how to apply DINOv3 to different vision tasks.\n+\n+The example below demonstrates how to obtain an image embedding with [`Pipeline`] or the [`AutoModel`] class.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+import torch\n+from transformers import pipeline\n+\n+pipe = pipeline(\n+    task=\"image-feature-extraction\", \n+    model=\"facebook/dinov3-vits16-pretrain-lvd1689m\",\n+    torch_dtype=torch.bfloat16,\n+)\n+\n+pipe(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import AutoImageProcessor, AutoModel\n+from transformers.image_utils import load_image\n+\n+url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+image = load_image(url)\n+\n+processor = AutoImageProcessor.from_pretrained(\"facebook/dinov3-vits16-pretrain-lvd1689m\")\n+model = AutoModel.from_pretrained(\n+    \"facebook/dinov3-vits16-pretrain-lvd1689m\", \n+    torch_dtype=torch.float16, \n+    device_map=\"auto\", \n+    attn_implementation=\"sdpa\"\n+)\n+\n+inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n+with torch.inference_mode():\n+    outputs = model(**inputs)\n+\n+pooled_output = outputs.pooler_output\n+print(\"Pooled output shape:\", pooled_output.shape)\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n+\n+The example below uses [torchao](../quantization/torchao) to only quantize the weights to int4.\n+\n+```py\n+# pip install torchao\n+import torch\n+from transformers import TorchAoConfig, AutoImageProcessor, AutoModel\n+from torchao.quantization import Int4WeightOnlyConfig\n+from transformers.image_utils import load_image\n+\n+\n+url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+image = load_image(url)\n+\n+processor = AutoImageProcessor.from_pretrained(\"facebook/dinov3-vitsplus-pretrain-lvd1689m\")\n+\n+quant_type = Int4WeightOnlyConfig(group_size=128)\n+quantization_config = TorchAoConfig(quant_type=quant_type)\n+\n+model = AutoModel.from_pretrained(\n+    \"facebook/dinov3-vit7b16-pretrain-lvd1689m\",\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"auto\",\n+    quantization_config=quantization_config\n+)\n+\n+inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n+with torch.inference_mode():\n+    outputs = model(**inputs)\n+\n+pooled_output = outputs.pooler_output\n+print(\"Pooled output shape:\", pooled_output.shape)\n+```\n+\n+## Notes\n+\n+- The example below shows how to split the output tensor into:\n+  - one embedding for the whole image, commonly referred to as a `CLS` token,\n+    useful for classification and retrieval\n+  - register tokens - learnable embeddings that act as dedicated “memory slots” for global information, \n+    they reduce high-norm artifacts in patch tokens, yielding cleaner attention maps and better \n+    performance on dense prediction tasks.\n+  - a set of local embeddings, one for each `16x16` patch of the input image,\n+    useful for dense tasks, such as semantic segmentation\n+\n+  ```py\n+  import torch\n+  from transformers import AutoImageProcessor, AutoModel\n+  from transformers.image_utils import load_image \n+  \n+  url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+  image = load_image(url)\n+  print(\"Image size:\", image.height, image.width)  # [480, 640]\n+  \n+  processor = AutoImageProcessor.from_pretrained(\"facebook/dinov3-vits16-pretrain-lvd1689m\")\n+  model = AutoModel.from_pretrained(\"facebook/dinov3-vits16-pretrain-lvd1689m\")\n+  patch_size = model.config.patch_size\n+  print(\"Patch size:\", patch_size) # 16\n+  print(\"Num register tokens:\", model.config.num_register_tokens) # 4\n+\n+  inputs = processor(images=image, return_tensors=\"pt\")\n+  print(\"Preprocessed image size:\", inputs.pixel_values.shape)  # [1, 3, 224, 224]\n+\n+  batch_size, _, img_height, img_width = inputs.pixel_values.shape\n+  num_patches_height, num_patches_width = img_height // patch_size, img_width // patch_size\n+  num_patches_flat = num_patches_height * num_patches_width\n+  \n+  with torch.inference_mode():\n+    outputs = model(**inputs)\n+\n+  last_hidden_states = outputs.last_hidden_state\n+  print(last_hidden_states.shape)  # [1, 1 + 4 + 256, 384]\n+  assert last_hidden_states.shape == (batch_size, 1 + model.config.num_register_tokens + num_patches_flat, model.config.hidden_size)\n+\n+  cls_token = last_hidden_states[:, 0, :]\n+  patch_features_flat = last_hidden_states[:, 1 + model.config.num_register_tokens:, :]\n+  patch_features = patch_features_flat.unflatten(1, (num_patches_height, num_patches_width))\n+  ```\n+\n+## DINOv3ViTConfig\n+\n+[[autodoc]] DINOv3ViTConfig\n+\n+## DINOv3ConvNextConfig\n+\n+[[autodoc]] DINOv3ConvNextConfig\n+\n+## DINOv3ViTModel\n+\n+[[autodoc]] DINOv3ViTModel\n+    - forward\n+\n+## DINOv3ConvNextModel\n+\n+[[autodoc]] DINOv3ConvNextModel\n+    - forward\n+\n+## DINOv3ViTImageProcessorFast\n+\n+[[autodoc]] DINOv3ViTImageProcessorFast\n+    - preprocess"
        },
        {
            "sha": "ea1d5488a6151ba989fd959275faf5e1b889af03",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0",
            "patch": "@@ -99,6 +99,8 @@\n     from .dinat import *\n     from .dinov2 import *\n     from .dinov2_with_registers import *\n+    from .dinov3_convnext import *\n+    from .dinov3_vit import *\n     from .distilbert import *\n     from .dit import *\n     from .donut import *"
        },
        {
            "sha": "57f074590e422709f3af325b107502a6caa4cd11",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0",
            "patch": "@@ -117,6 +117,8 @@\n         (\"dinat\", \"DinatConfig\"),\n         (\"dinov2\", \"Dinov2Config\"),\n         (\"dinov2_with_registers\", \"Dinov2WithRegistersConfig\"),\n+        (\"dinov3_convnext\", \"DINOv3ConvNextConfig\"),\n+        (\"dinov3_vit\", \"DINOv3ViTConfig\"),\n         (\"distilbert\", \"DistilBertConfig\"),\n         (\"doge\", \"DogeConfig\"),\n         (\"donut-swin\", \"DonutSwinConfig\"),\n@@ -525,6 +527,8 @@\n         (\"dinat\", \"DiNAT\"),\n         (\"dinov2\", \"DINOv2\"),\n         (\"dinov2_with_registers\", \"DINOv2 with Registers\"),\n+        (\"dinov3_convnext\", \"DINOv3 ConvNext\"),\n+        (\"dinov3_vit\", \"DINOv3 ViT\"),\n         (\"distilbert\", \"DistilBERT\"),\n         (\"dit\", \"DiT\"),\n         (\"doge\", \"Doge\"),"
        },
        {
            "sha": "1e3e0aa178d1404faef546bb02fb2a81bbfc025c",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0",
            "patch": "@@ -88,6 +88,7 @@\n             (\"detr\", (\"DetrImageProcessor\", \"DetrImageProcessorFast\")),\n             (\"dinat\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"dinov2\", (\"BitImageProcessor\", \"BitImageProcessorFast\")),\n+            (\"dinov3_vit\", (None, \"DINOv3ViTImageProcessorFast\")),\n             (\"donut-swin\", (\"DonutImageProcessor\", \"DonutImageProcessorFast\")),\n             (\"dpt\", (\"DPTImageProcessor\", \"DPTImageProcessorFast\")),\n             (\"efficientformer\", (\"EfficientFormerImageProcessor\", None)),"
        },
        {
            "sha": "7d8d2157122a23142ed7e94233504ac0c3de3094",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0",
            "patch": "@@ -121,6 +121,8 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"dinat\", \"DinatModel\"),\n         (\"dinov2\", \"Dinov2Model\"),\n         (\"dinov2_with_registers\", \"Dinov2WithRegistersModel\"),\n+        (\"dinov3_convnext\", \"DINOv3ConvNextModel\"),\n+        (\"dinov3_vit\", \"DINOv3ViTModel\"),\n         (\"distilbert\", \"DistilBertModel\"),\n         (\"doge\", \"DogeModel\"),\n         (\"donut-swin\", \"DonutSwinModel\"),\n@@ -746,6 +748,8 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"dinat\", \"DinatModel\"),\n         (\"dinov2\", \"Dinov2Model\"),\n         (\"dinov2_with_registers\", \"Dinov2WithRegistersModel\"),\n+        (\"dinov3_convnext\", \"DINOv3ConvNextModel\"),\n+        (\"dinov3_vit\", \"DINOv3ViTModel\"),\n         (\"dpt\", \"DPTModel\"),\n         (\"efficientformer\", \"EfficientFormerModel\"),\n         (\"efficientnet\", \"EfficientNetModel\"),"
        },
        {
            "sha": "8839dc7cec78158801a1b623c27dd46281aadb0d",
            "filename": "src/transformers/models/dinov3_convnext/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2F__init__.py?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_dinov3_convnext import *\n+    from .modeling_dinov3_convnext import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "fa593e10ec1a685091234ee7966873570716276e",
            "filename": "src/transformers/models/dinov3_convnext/configuration_dinov3_convnext.py",
            "status": "added",
            "additions": 103,
            "deletions": 0,
            "changes": 103,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fconfiguration_dinov3_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fconfiguration_dinov3_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fconfiguration_dinov3_convnext.py?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0",
            "patch": "@@ -0,0 +1,103 @@\n+# coding=utf-8\n+# Copyright 2025 Meta Platforms, Inc. and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"ConvNeXT model configuration\"\"\"\n+\n+from typing import Optional\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class DINOv3ConvNextConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`DINOv3ConvNextModel`]. It is used to instantiate an\n+    DINOv3ConvNext model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the DINOv3ConvNext\n+    [facebook/dinov3-convnext-tiny-pretrain-lvd1689m](https://huggingface.co/facebook/dinov3-convnext-tiny-pretrain-lvd1689m) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        num_channels (`int`, *optional*, defaults to 3):\n+            The number of input channels.\n+        hidden_sizes (`list[int]`, *optional*, defaults to [96, 192, 384, 768]):\n+            Dimensionality (hidden size) at each stage.\n+        depths (`list[int]`, *optional*, defaults to [3, 3, 9, 3]):\n+            The number of layers for each stage.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in each block. If string, `\"gelu\"`, `\"relu\"`,\n+            `\"selu\"` and `\"gelu_new\"` are supported.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        layer_scale_init_value (`float`, *optional*, defaults to 1e-06):\n+            The initial value for the layer scale.\n+        drop_path_rate (`float`, *optional*, defaults to 0.0):\n+            The drop rate for stochastic depth.\n+        image_size (`int`, *optional*, defaults to 224):\n+            The size (resolution) of input images.\n+\n+    Example:\n+    ```python\n+    >>> from transformers import DINOv3ConvNextConfig, DINOv3ConvNextModel\n+\n+    >>> # Initializing a DINOv3ConvNext (tiny variant) style configuration\n+    >>> config = DINOv3ConvNextConfig()\n+\n+    >>> # Initializing a model (with random weights)\n+    >>> model = DINOv3ConvNextModel(config)\n+\n+    >>> # Accessing the model config\n+    >>> config = model.config\n+    ```\"\"\"\n+\n+    model_type = \"dinov3_convnext\"\n+\n+    def __init__(\n+        self,\n+        num_channels: int = 3,\n+        hidden_sizes: Optional[list[int]] = None,\n+        depths: Optional[list[int]] = None,\n+        hidden_act: str = \"gelu\",\n+        initializer_range: float = 0.02,\n+        layer_norm_eps: float = 1e-6,\n+        layer_scale_init_value: float = 1e-6,\n+        drop_path_rate: float = 0.0,\n+        image_size: int = 224,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.num_channels = num_channels\n+        self.hidden_sizes = [96, 192, 384, 768] if hidden_sizes is None else hidden_sizes\n+        self.depths = [3, 3, 9, 3] if depths is None else depths\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.layer_norm_eps = layer_norm_eps\n+        self.layer_scale_init_value = layer_scale_init_value\n+        self.drop_path_rate = drop_path_rate\n+        self.image_size = image_size\n+\n+    @property\n+    def num_stages(self) -> int:\n+        return len(self.hidden_sizes)\n+\n+\n+__all__ = [\"DINOv3ConvNextConfig\"]"
        },
        {
            "sha": "0ba200936ebe02e62a894beaf9a23eb39e5ed746",
            "filename": "src/transformers/models/dinov3_convnext/convert_dinov3_convnext_to_hf.py",
            "status": "added",
            "additions": 234,
            "deletions": 0,
            "changes": 234,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fconvert_dinov3_convnext_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fconvert_dinov3_convnext_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fconvert_dinov3_convnext_to_hf.py?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0",
            "patch": "@@ -0,0 +1,234 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Convert DINOv3 checkpoints from the original repository.\n+\n+URL: https://github.com/facebookresearch/dinov3/tree/main\n+\"\"\"\n+\n+import argparse\n+import os\n+import re\n+from typing import Optional\n+\n+import requests\n+import torch\n+from huggingface_hub import HfApi, hf_hub_download\n+from PIL import Image\n+from torchvision import transforms\n+\n+from transformers import DINOv3ConvNextConfig, DINOv3ConvNextModel, DINOv3ViTImageProcessorFast\n+\n+\n+HUB_MODELS = {\n+    \"convnext_tiny\": \"facebook/dinov3-convnext-tiny-pretrain-lvd1689m\",\n+    \"convnext_small\": \"facebook/dinov3-convnext-small-pretrain-lvd1689m\",\n+    \"convnext_base\": \"facebook/dinov3-convnext-base-pretrain-lvd1689m\",\n+    \"convnext_large\": \"facebook/dinov3-convnext-large-pretrain-lvd1689m\",\n+}\n+\n+HUB_CHECKPOINTS = {\n+    \"convnext_tiny\": \"dinov3_convnext_tiny_pretrain_lvd1689m-21b726bb.pth\",\n+    \"convnext_small\": \"dinov3_convnext_small_pretrain_lvd1689m-296db49d.pth\",\n+    \"convnext_base\": \"dinov3_convnext_base_pretrain_lvd1689m-801f2ba9.pth\",\n+    \"convnext_large\": \"dinov3_convnext_large_pretrain_lvd1689m-61fa432d.pth\",\n+}\n+\n+# fmt: off\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    r\"dwconv\":                              r\"depthwise_conv\",\n+    r\"pwconv\":                              r\"pointwise_conv\",\n+    r\"norm\":                                r\"layer_norm\",\n+    r\"stages.(\\d+).(\\d+)\":                  r\"stages.\\1.layers.\\2\",\n+    r\"downsample_layers.(\\d+).(\\d+)\":       r\"stages.\\1.downsample_layers.\\2\",\n+}\n+# fmt: on\n+\n+\n+def get_dinov3_config(model_name: str) -> DINOv3ConvNextConfig:\n+    # size of the architecture\n+    if model_name == \"convnext_tiny\":\n+        return DINOv3ConvNextConfig(\n+            depths=[3, 3, 9, 3],\n+            hidden_sizes=[96, 192, 384, 768],\n+        )\n+    elif model_name == \"convnext_small\":\n+        return DINOv3ConvNextConfig(\n+            depths=[3, 3, 27, 3],\n+            hidden_sizes=[96, 192, 384, 768],\n+        )\n+    elif model_name == \"convnext_base\":\n+        return DINOv3ConvNextConfig(\n+            depths=[3, 3, 27, 3],\n+            hidden_sizes=[128, 256, 512, 1024],\n+        )\n+    elif model_name == \"convnext_large\":\n+        return DINOv3ConvNextConfig(\n+            depths=[3, 3, 27, 3],\n+            hidden_sizes=[192, 384, 768, 1536],\n+        )\n+    else:\n+        raise ValueError(\"Model not supported\")\n+\n+\n+def prepare_img():\n+    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n+    return image\n+\n+\n+def get_transform(resize_size: int = 224):\n+    to_tensor = transforms.ToTensor()\n+    resize = transforms.Resize((resize_size, resize_size), antialias=True)\n+    normalize = transforms.Normalize(\n+        mean=(0.485, 0.456, 0.406),\n+        std=(0.229, 0.224, 0.225),\n+    )\n+    return transforms.Compose([to_tensor, resize, normalize])\n+\n+\n+def get_image_processor(resize_size: int = 224):\n+    return DINOv3ViTImageProcessorFast(\n+        do_resize=True,\n+        size={\"height\": resize_size, \"width\": resize_size},\n+        resample=2,  # BILINEAR\n+    )\n+\n+\n+def convert_old_keys_to_new_keys(state_dict_keys: Optional[dict] = None):\n+    \"\"\"\n+    This function should be applied only once, on the concatenated keys to efficiently rename using\n+    the key mappings.\n+    \"\"\"\n+    output_dict = {}\n+    if state_dict_keys is not None:\n+        old_text = \"\\n\".join(state_dict_keys)\n+        new_text = old_text\n+        for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING.items():\n+            if replacement is None:\n+                new_text = re.sub(pattern, \"\", new_text)  # an empty line\n+                continue\n+            new_text = re.sub(pattern, replacement, new_text)\n+        output_dict = dict(zip(old_text.split(\"\\n\"), new_text.split(\"\\n\")))\n+    return output_dict\n+\n+\n+@torch.no_grad()\n+def convert_and_test_dinov3_checkpoint(args):\n+    expected_outputs = {\n+        \"convnext_tiny_cls\": [-6.372119, 1.300791, 2.074303, -0.079975, 0.607205],\n+        \"convnext_tiny_patch\": [0.490530, -3.713466, 1.848513, -1.040319, -1.090818],\n+        \"convnext_small_cls\": [-0.903914, 1.412183, 0.287465, 0.175296, -2.397940],\n+        \"convnext_small_patch\": [-1.081114, 0.637362, 3.748765, 0.170179, 1.445153],\n+        \"convnext_base_cls\": [0.155366, -0.378771, -0.735157, -2.818718, 0.015095],\n+        \"convnext_base_patch\": [3.039118, 0.778155, -1.961322, -1.607147, -2.411941],\n+        \"convnext_large_cls\": [-2.219094, -0.594451, -2.300294, -0.957415, -0.520473],\n+        \"convnext_large_patch\": [-1.477349, -0.217038, -3.128137, 0.418962, 0.334949],\n+    }\n+    model_name = args.model_name\n+    config = get_dinov3_config(model_name)\n+    # print(config)\n+\n+    model = DINOv3ConvNextModel(config).eval()\n+    state_dict_path = hf_hub_download(repo_id=HUB_MODELS[model_name], filename=HUB_CHECKPOINTS[model_name])\n+    original_state_dict = torch.load(state_dict_path)\n+    original_keys = list(original_state_dict.keys())\n+    new_keys = convert_old_keys_to_new_keys(original_keys)\n+\n+    converted_state_dict = {}\n+    for key in original_keys:\n+        new_key = new_keys[key]\n+        weight_tensor = original_state_dict[key]\n+        if key == \"norms.3.weight\" or key == \"norms.3.bias\":\n+            continue\n+        converted_state_dict[new_key] = weight_tensor\n+    model.load_state_dict(converted_state_dict, strict=True)\n+    model = model.eval()\n+\n+    transform = get_transform()\n+    image_processor = get_image_processor()\n+    image = prepare_img()\n+\n+    # check preprocessing\n+    original_pixel_values = transform(image).unsqueeze(0)  # add batch dimension\n+    inputs = image_processor(image, return_tensors=\"pt\")\n+\n+    torch.testing.assert_close(original_pixel_values, inputs[\"pixel_values\"], atol=1e-6, rtol=1e-6)\n+    print(\"Preprocessing looks ok!\")\n+\n+    with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.float):\n+        model_output = model(**inputs)\n+\n+    last_layer_class_token = model_output.pooler_output\n+    last_layer_patch_tokens = model_output.last_hidden_state[:, 1:]\n+\n+    actual_outputs = {}\n+    actual_outputs[f\"{model_name}_cls\"] = last_layer_class_token[0, :5].tolist()\n+    actual_outputs[f\"{model_name}_patch\"] = last_layer_patch_tokens[0, 0, :5].tolist()\n+\n+    print(\"Actual:  \", [round(x, 6) for x in actual_outputs[f\"{model_name}_cls\"]])\n+    print(\"Expected:\", expected_outputs[f\"{model_name}_cls\"])\n+\n+    torch.testing.assert_close(\n+        torch.Tensor(actual_outputs[f\"{model_name}_cls\"]),\n+        torch.Tensor(expected_outputs[f\"{model_name}_cls\"]),\n+        atol=1e-3,\n+        rtol=1e-3,\n+    )\n+    print(\"Actual:  \", [round(x, 6) for x in actual_outputs[f\"{model_name}_patch\"]])\n+    print(\"Expected:\", expected_outputs[f\"{model_name}_patch\"])\n+\n+    torch.testing.assert_close(\n+        torch.Tensor(actual_outputs[f\"{model_name}_patch\"]),\n+        torch.Tensor(expected_outputs[f\"{model_name}_patch\"]),\n+        atol=1e-3,\n+        rtol=1e-3,\n+    )\n+    print(\"Forward pass looks ok!\")\n+\n+    save_dir = os.path.join(args.save_dir, model_name)\n+    os.makedirs(save_dir, exist_ok=True)\n+    model.save_pretrained(save_dir)\n+    image_processor.save_pretrained(save_dir)\n+    print(f\"Model saved to {save_dir}\")\n+\n+    if args.push_to_hub:\n+        api = HfApi()\n+        repo = HUB_MODELS[model_name]\n+        api.upload_folder(folder_path=save_dir, repo_id=repo, repo_type=\"model\")\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    # Required parameters\n+    parser.add_argument(\n+        \"--model-name\",\n+        default=\"convnext_tiny\",\n+        type=str,\n+        choices=[\"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"],\n+        help=\"Name of the model you'd like to convert.\",\n+    )\n+    parser.add_argument(\n+        \"--save-dir\",\n+        default=\"converted_models\",\n+        type=str,\n+        help=\"Directory to save the converted model.\",\n+    )\n+    parser.add_argument(\n+        \"--push-to-hub\",\n+        action=\"store_true\",\n+        help=\"Push the converted model to the Hugging Face Hub.\",\n+    )\n+    args = parser.parse_args()\n+    convert_and_test_dinov3_checkpoint(args)"
        },
        {
            "sha": "2318faf1482494fa9da612784fe01db83d6efbec",
            "filename": "src/transformers/models/dinov3_convnext/modeling_dinov3_convnext.py",
            "status": "added",
            "additions": 261,
            "deletions": 0,
            "changes": 261,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fmodeling_dinov3_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fmodeling_dinov3_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fmodeling_dinov3_convnext.py?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0",
            "patch": "@@ -0,0 +1,261 @@\n+# coding=utf-8\n+# Copyright 2025 Meta Platforms, Inc. and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch ConvNext model.\"\"\"\n+\n+from typing import Optional\n+\n+import numpy as np\n+import torch\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...modeling_outputs import (\n+    BaseModelOutputWithPoolingAndNoAttention,\n+)\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import auto_docstring, logging\n+from ...utils.generic import can_return_tuple\n+from .configuration_dinov3_convnext import DINOv3ConvNextConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+# Copied from transformers.models.beit.modeling_beit.drop_path\n+def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:\n+    \"\"\"\n+    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n+\n+    Comment by Ross Wightman: This is the same as the DropConnect impl I created for EfficientNet, etc networks,\n+    however, the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n+    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for changing the\n+    layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use 'survival rate' as the\n+    argument.\n+    \"\"\"\n+    if drop_prob == 0.0 or not training:\n+        return input\n+    keep_prob = 1 - drop_prob\n+    shape = (input.shape[0],) + (1,) * (input.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n+    random_tensor = keep_prob + torch.rand(shape, dtype=input.dtype, device=input.device)\n+    random_tensor.floor_()  # binarize\n+    output = input.div(keep_prob) * random_tensor\n+    return output\n+\n+\n+# Copied from transformers.models.convnext.modeling_convnext.ConvNextDropPath with ConvNext->DINOv3ConvNext\n+class DINOv3ConvNextDropPath(nn.Module):\n+    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n+\n+    def __init__(self, drop_prob: Optional[float] = None) -> None:\n+        super().__init__()\n+        self.drop_prob = drop_prob\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        return drop_path(hidden_states, self.drop_prob, self.training)\n+\n+    def extra_repr(self) -> str:\n+        return f\"p={self.drop_prob}\"\n+\n+\n+class DINOv3ConvNextLayerNorm(nn.LayerNorm):\n+    r\"\"\"LayerNorm that supports two data formats: channels_last (default) or channels_first.\n+    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch_size, height,\n+    width, channels) while channels_first corresponds to inputs with shape (batch_size, channels, height, width).\n+    \"\"\"\n+\n+    def __init__(self, *args, data_format=\"channels_last\", **kwargs):\n+        super().__init__(*args, **kwargs)\n+        if data_format not in [\"channels_last\", \"channels_first\"]:\n+            raise NotImplementedError(f\"Unsupported data format: {data_format}\")\n+        self.data_format = data_format\n+\n+    def forward(self, features: torch.Tensor) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            features: Tensor of shape (batch_size, channels, height, width) OR (batch_size, height, width, channels)\n+        \"\"\"\n+        if self.data_format == \"channels_first\":\n+            features = features.permute(0, 2, 3, 1)\n+            features = super().forward(features)\n+            features = features.permute(0, 3, 1, 2)\n+        else:\n+            features = super().forward(features)\n+        return features\n+\n+\n+class DINOv3ConvNextLayer(nn.Module):\n+    \"\"\"This corresponds to the `Block` class in the original implementation.\n+\n+    There are two equivalent implementations:\n+     1) DwConv, LayerNorm (channels_first), Conv, GELU, Conv (all in (N, C, H, W) format)\n+     2) DwConv, Permute, LayerNorm (channels_last), Linear, GELU, Linear, Permute\n+\n+    The authors used (2) as they find it slightly faster in PyTorch.\n+\n+    Args:\n+        config ([`DINOv3ConvNextConfig`]):\n+            Model config.\n+        channels (`int`):\n+            Number of input (and output) channels.\n+        drop_path (`float`):\n+            Drop path rate. Default: 0.0.\n+    \"\"\"\n+\n+    def __init__(self, config: DINOv3ConvNextConfig, channels: int, drop_path: float = 0.0):\n+        super().__init__()\n+        self.depthwise_conv = nn.Conv2d(channels, channels, kernel_size=7, padding=3, groups=channels)\n+        self.layer_norm = DINOv3ConvNextLayerNorm(channels, eps=config.layer_norm_eps)\n+        self.pointwise_conv1 = nn.Linear(channels, 4 * channels)  # can be seen as a 1x1 conv\n+        self.activation_fn = ACT2FN[config.hidden_act]\n+        self.pointwise_conv2 = nn.Linear(4 * channels, channels)  # can be seen as a 1x1 conv\n+        self.gamma = nn.Parameter(torch.full((channels,), config.layer_scale_init_value), requires_grad=True)\n+        self.drop_path = DINOv3ConvNextDropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n+\n+    def forward(self, features: torch.Tensor) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            features: Tensor of shape (batch_size, channels, height, width)\n+        \"\"\"\n+        residual = features\n+        features = self.depthwise_conv(features)\n+        features = features.permute(0, 2, 3, 1)  # to channels last\n+        features = self.layer_norm(features)\n+        features = self.pointwise_conv1(features)\n+        features = self.activation_fn(features)\n+        features = self.pointwise_conv2(features)\n+        features = features * self.gamma\n+        features = features.permute(0, 3, 1, 2)  # back to channels first\n+        features = residual + self.drop_path(features)\n+        return features\n+\n+\n+class DINOv3ConvNextStage(nn.Module):\n+    \"\"\" \"\"\"\n+\n+    def __init__(self, config: DINOv3ConvNextConfig, stage_idx: int):\n+        super().__init__()\n+\n+        in_channels = config.hidden_sizes[stage_idx - 1] if stage_idx > 0 else config.num_channels\n+        out_channels = config.hidden_sizes[stage_idx]\n+\n+        if stage_idx == 0:\n+            self.downsample_layers = nn.ModuleList(\n+                [\n+                    nn.Conv2d(config.num_channels, out_channels, kernel_size=4, stride=4),\n+                    DINOv3ConvNextLayerNorm(out_channels, eps=config.layer_norm_eps, data_format=\"channels_first\"),\n+                ]\n+            )\n+        else:\n+            self.downsample_layers = nn.ModuleList(\n+                [\n+                    DINOv3ConvNextLayerNorm(in_channels, eps=config.layer_norm_eps, data_format=\"channels_first\"),\n+                    nn.Conv2d(in_channels, out_channels, kernel_size=2, stride=2),\n+                ]\n+            )\n+\n+        num_stage_layers = config.depths[stage_idx]\n+        num_previous_layers = sum(config.depths[:stage_idx])\n+        num_total_layers = sum(config.depths)\n+        drop_path_rates = np.linspace(0, config.drop_path_rate, num_total_layers).tolist()\n+\n+        self.layers = nn.ModuleList(\n+            [\n+                DINOv3ConvNextLayer(config, channels=out_channels, drop_path=drop_path_rates[i])\n+                for i in range(num_previous_layers, num_previous_layers + num_stage_layers)\n+            ]\n+        )\n+\n+    def forward(self, features: torch.Tensor) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            features: Tensor of shape (batch_size, channels, height, width)\n+        \"\"\"\n+        for layer in self.downsample_layers:\n+            features = layer(features)\n+        for layer in self.layers:\n+            features = layer(features)\n+        return features\n+\n+\n+@auto_docstring\n+class DINOv3ConvNextPreTrainedModel(PreTrainedModel):\n+    config: DINOv3ConvNextConfig\n+    base_model_prefix = \"dinov3_convnext\"\n+    main_input_name = \"pixel_values\"\n+    _no_split_modules = [\"DINOv3ConvNextLayer\"]\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            # Slightly different from the TF version which uses truncated_normal for initialization\n+            # cf https://github.com/pytorch/pytorch/pull/5617\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, (nn.LayerNorm, DINOv3ConvNextLayerNorm)):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, DINOv3ConvNextLayer):\n+            if module.gamma is not None:\n+                module.gamma.data.fill_(self.config.layer_scale_init_value)\n+\n+\n+@auto_docstring\n+class DINOv3ConvNextModel(DINOv3ConvNextPreTrainedModel):\n+    def __init__(self, config: DINOv3ConvNextConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.stages = nn.ModuleList([DINOv3ConvNextStage(config, stage_idx) for stage_idx in range(config.num_stages)])\n+        self.layer_norm = nn.LayerNorm(config.hidden_sizes[-1], eps=config.layer_norm_eps)  # final norm layer\n+        self.pool = nn.AdaptiveAvgPool2d(1)\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self, pixel_values: torch.FloatTensor, output_hidden_states: Optional[bool] = None\n+    ) -> BaseModelOutputWithPoolingAndNoAttention:\n+        hidden_states = pixel_values\n+\n+        output_hidden_states = output_hidden_states or self.config.output_hidden_states\n+        all_hidden_states = [hidden_states] if output_hidden_states else []\n+\n+        for stage in self.stages:\n+            hidden_states = stage(hidden_states)\n+\n+            # store intermediate stage outputs\n+            if output_hidden_states:\n+                all_hidden_states.append(hidden_states)\n+\n+        # make global representation, a.k.a [CLS] token\n+        pooled_output = self.pool(hidden_states)\n+\n+        # (batch_size, channels, height, width) -> (batch_size, height * width, channels)\n+        pooled_output = pooled_output.flatten(2).transpose(1, 2)\n+        hidden_states = hidden_states.flatten(2).transpose(1, 2)\n+\n+        # concat \"cls\" and \"patch tokens\" as (batch_size, 1 + height * width, channels)\n+        hidden_states = torch.cat([pooled_output, hidden_states], dim=1)\n+        hidden_states = self.layer_norm(hidden_states)\n+\n+        return BaseModelOutputWithPoolingAndNoAttention(\n+            last_hidden_state=hidden_states,\n+            pooler_output=hidden_states[:, 0],\n+            hidden_states=tuple(all_hidden_states) if output_hidden_states else None,\n+        )\n+\n+\n+__all__ = [\"DINOv3ConvNextModel\", \"DINOv3ConvNextPreTrainedModel\"]"
        },
        {
            "sha": "a74878b2053cf43fabe19a7fd72e020a0879f8e6",
            "filename": "src/transformers/models/dinov3_vit/__init__.py",
            "status": "added",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2F__init__.py?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0",
            "patch": "@@ -0,0 +1,28 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_dinov3_vit import *\n+    from .image_processing_dinov3_vit_fast import *\n+    from .modeling_dinov3_vit import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "78cbd200ce612e6c778392c85d2f8c97a7d19c82",
            "filename": "src/transformers/models/dinov3_vit/configuration_dinov3_vit.py",
            "status": "added",
            "additions": 166,
            "deletions": 0,
            "changes": 166,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fconfiguration_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fconfiguration_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fconfiguration_dinov3_vit.py?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0",
            "patch": "@@ -0,0 +1,166 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"DINOv3 model configuration\"\"\"\n+\n+from typing import Optional\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class DINOv3ViTConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`DINOv3Model`]. It is used to instantiate an\n+    DINOv3 model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the DINOv3\n+    [facebook/dinov3-vits16-pretrain-lvd1689m](https://huggingface.co/facebook/dinov3-vits16-pretrain-lvd1689m) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        patch_size (`int`, *optional*, defaults to 16):\n+            The size (resolution) of each patch.\n+        hidden_size (`int`, *optional*, defaults to 384):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        intermediate_size (`int`, *optional*, defaults to 1536):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer.\n+        num_hidden_layers (`int`, *optional*, defaults to 12):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 6):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` are supported.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the layer normalization layers.\n+        rope_theta (`float`, *optional*, defaults to 100.0):\n+            The base period of the RoPE embeddings.\n+        image_size (`int`, *optional*, defaults to 224):\n+            The size (resolution) of each image.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            The number of input channels.\n+        query_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to add a bias to the query projection.\n+        key_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to add a bias to the key projection.\n+        value_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to add a bias to the value projection.\n+        proj_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to add a bias to the output projection.\n+        mlp_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to add a bias to the MLP layers.\n+        layerscale_value (`float`, *optional*, defaults to 1.0):\n+            Initial value to use for layer scale.\n+        drop_path_rate (`float`, *optional*, defaults to 0.0):\n+            Stochastic depth rate per sample (when applied in the main path of residual layers).\n+        use_gated_mlp (`bool`, *optional*, defaults to `False`):\n+            Whether to use the SwiGLU feedforward neural network.\n+        num_register_tokens (`int`, *optional*, defaults to 0):\n+            The number of register tokens.\n+        pos_embed_shift (`float`, *optional*):\n+            Amount to randomly shift position embedding coordinates in [-shift, shift],\n+            applied only in training mode if not `None`.\n+        pos_embed_jitter (`float`, *optional*):\n+            Amount to randomly jitter position embedding coordinates in log-uniform value in [1/jitter, jitter],\n+            applied only in training mode if not `None`.\n+        pos_embed_rescale (`float`, *optional*, defaults to 2.0):\n+            Amount to randomly rescale position embedding coordinates in log-uniform value in [1/rescale, rescale],\n+            applied only in training mode if not `None`.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import DINOv3ViTConfig, DINOv3ViTModel\n+\n+    >>> # Initializing a DINOv3 ViT-small style configuration\n+    >>> config = DINOv3ViTConfig()\n+\n+    >>> # Initializing a model (with random weights) from the config\n+    >>> model = DINOv3ViTModel(config)\n+\n+    >>> # Accessing the model config\n+    >>> config = model.config\n+    ```\"\"\"\n+\n+    model_type = \"dinov3_vit\"\n+\n+    def __init__(\n+        self,\n+        patch_size: int = 16,\n+        hidden_size: int = 384,\n+        intermediate_size: int = 1536,\n+        num_hidden_layers: int = 12,\n+        num_attention_heads: int = 6,\n+        hidden_act: str = \"gelu\",\n+        attention_dropout: float = 0.0,\n+        initializer_range: float = 0.02,\n+        layer_norm_eps: float = 1e-5,\n+        rope_theta: float = 100.0,\n+        image_size: int = 224,\n+        num_channels: int = 3,\n+        query_bias: bool = True,\n+        key_bias: bool = False,\n+        value_bias: bool = True,\n+        proj_bias: bool = True,\n+        mlp_bias: bool = True,\n+        layerscale_value: float = 1.0,\n+        drop_path_rate: float = 0.0,\n+        use_gated_mlp: bool = False,\n+        num_register_tokens: int = 0,\n+        # train augs\n+        pos_embed_shift: Optional[float] = None,\n+        pos_embed_jitter: Optional[float] = None,\n+        pos_embed_rescale: Optional[float] = 2.0,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.hidden_act = hidden_act\n+        self.attention_dropout = attention_dropout\n+        self.initializer_range = initializer_range\n+        self.layer_norm_eps = layer_norm_eps\n+        self.layerscale_value = layerscale_value\n+        self.drop_path_rate = drop_path_rate\n+        self.use_gated_mlp = use_gated_mlp\n+        self.rope_theta = rope_theta\n+        self.query_bias = query_bias\n+        self.key_bias = key_bias\n+        self.value_bias = value_bias\n+        self.proj_bias = proj_bias\n+        self.mlp_bias = mlp_bias\n+        self.num_register_tokens = num_register_tokens\n+\n+        # train augs\n+        self.pos_embed_shift = pos_embed_shift\n+        self.pos_embed_jitter = pos_embed_jitter\n+        self.pos_embed_rescale = pos_embed_rescale\n+\n+\n+__all__ = [\"DINOv3ViTConfig\"]"
        },
        {
            "sha": "b6589e089d95977e7de94a3be261e8c91d0d32b6",
            "filename": "src/transformers/models/dinov3_vit/convert_dinov3_vit_to_hf.py",
            "status": "added",
            "additions": 337,
            "deletions": 0,
            "changes": 337,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fconvert_dinov3_vit_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fconvert_dinov3_vit_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fconvert_dinov3_vit_to_hf.py?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0",
            "patch": "@@ -0,0 +1,337 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Convert DINOv3 checkpoints from the original repository.\n+\n+URL: https://github.com/facebookresearch/dinov3/tree/main\n+\"\"\"\n+\n+import argparse\n+import os\n+import re\n+from typing import Optional\n+\n+import requests\n+import torch\n+from huggingface_hub import HfApi, hf_hub_download\n+from PIL import Image\n+from torchvision import transforms\n+\n+from transformers import DINOv3ViTConfig, DINOv3ViTImageProcessorFast, DINOv3ViTModel\n+\n+\n+HUB_MODELS = {\n+    \"vits16_lvd1689m\": \"facebook/dinov3-vits16-pretrain-lvd1689m\",\n+    \"vits16plus_lvd1689m\": \"facebook/dinov3-vits16plus-pretrain-lvd1689m\",\n+    \"vitb16_lvd1689m\": \"facebook/dinov3-vitb16-pretrain-lvd1689m\",\n+    \"vitl16_lvd1689m\": \"facebook/dinov3-vitl16-pretrain-lvd1689m\",\n+    \"vitl16_sat493m\": \"facebook/dinov3-vitl16-pretrain-sat493m\",\n+    \"vith16plus_lvd1689m\": \"facebook/dinov3-vith16plus-pretrain-lvd1689m\",\n+    \"vit7b16_lvd1689m\": \"facebook/dinov3-vit7b16-pretrain-lvd1689m\",\n+    \"vit7b16_sat493m\": \"facebook/dinov3-vit7b16-pretrain-sat493m\",\n+}\n+\n+HUB_CHECKPOINTS = {\n+    \"vits16_lvd1689m\": \"dinov3_vits16_pretrain_lvd1689m-08c60483.pth\",\n+    \"vits16plus_lvd1689m\": \"dinov3_vits16plus_pretrain_lvd1689m-4057cbaa.pth\",\n+    \"vitb16_lvd1689m\": \"dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth\",\n+    \"vitl16_lvd1689m\": \"dinov3_vitl16_pretrain_lvd1689m-8aa4cbdd.pth\",\n+    \"vitl16_sat493m\": \"dinov3_vitl16_pretrain_sat493m-eadcf0ff.pth\",\n+    \"vith16plus_lvd1689m\": \"dinov3_vith16plus_pretrain_lvd1689m-7c1da9a5.pth\",\n+    \"vit7b16_lvd1689m\": \"dinov3_vit7b16_pretrain_lvd1689m-a955f4ea.pth\",\n+    \"vit7b16_sat493m\": \"dinov3_vit7b16_pretrain_sat493m-a6675841.pth\",\n+}\n+\n+# fmt: off\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    r\"cls_token\":                   r\"embeddings.cls_token\",\n+    r\"mask_token\":                  r\"embeddings.mask_token\",\n+    r\"storage_tokens\":              r\"embeddings.register_tokens\",\n+    r\"patch_embed.proj\":            r\"embeddings.patch_embeddings\",\n+    r\"periods\":                     r\"inv_freq\",\n+    r\"rope_embed\":                  r\"rope_embeddings\",\n+    r\"blocks.(\\d+).attn.proj\":      r\"layer.\\1.attention.o_proj\",\n+    r\"blocks.(\\d+).attn.\":          r\"layer.\\1.attention.\",\n+    r\"blocks.(\\d+).ls(\\d+).gamma\":  r\"layer.\\1.layer_scale\\2.lambda1\",\n+    r\"blocks.(\\d+).mlp.fc1\":        r\"layer.\\1.mlp.up_proj\",\n+    r\"blocks.(\\d+).mlp.fc2\":        r\"layer.\\1.mlp.down_proj\",\n+    r\"blocks.(\\d+).mlp\":            r\"layer.\\1.mlp\",\n+    r\"blocks.(\\d+).norm\":           r\"layer.\\1.norm\",\n+    r\"w1\":                          r\"gate_proj\",\n+    r\"w2\":                          r\"up_proj\",\n+    r\"w3\":                          r\"down_proj\",\n+}\n+# fmt: on\n+\n+\n+def convert_old_keys_to_new_keys(state_dict_keys: Optional[dict] = None):\n+    \"\"\"\n+    This function should be applied only once, on the concatenated keys to efficiently rename using\n+    the key mappings.\n+    \"\"\"\n+    output_dict = {}\n+    if state_dict_keys is not None:\n+        old_text = \"\\n\".join(state_dict_keys)\n+        new_text = old_text\n+        for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING.items():\n+            if replacement is None:\n+                new_text = re.sub(pattern, \"\", new_text)  # an empty line\n+                continue\n+            new_text = re.sub(pattern, replacement, new_text)\n+        output_dict = dict(zip(old_text.split(\"\\n\"), new_text.split(\"\\n\")))\n+    return output_dict\n+\n+\n+def split_qkv(state_dict: dict):\n+    keys = [x for x in state_dict.keys() if \"qkv\" in x]\n+    for key in keys:\n+        qkv = state_dict.pop(key)\n+        q, k, v = torch.chunk(qkv, 3, dim=0)\n+        state_dict[key.replace(\"qkv\", \"q_proj\")] = q\n+        state_dict[key.replace(\"qkv\", \"k_proj\")] = k\n+        state_dict[key.replace(\"qkv\", \"v_proj\")] = v\n+    return state_dict\n+\n+\n+def get_dinov3_config(model_name: str) -> DINOv3ViTConfig:\n+    # size of the architecture\n+    if model_name == \"vits16_lvd1689m\":\n+        return DINOv3ViTConfig(\n+            patch_size=16,\n+            hidden_size=384,\n+            intermediate_size=1536,\n+            num_hidden_layers=12,\n+            num_attention_heads=6,\n+            proj_bias=True,\n+            num_register_tokens=4,\n+            use_gated_mlp=False,\n+            hidden_act=\"gelu\",\n+        )\n+    elif model_name == \"vits16plus_lvd1689m\":\n+        return DINOv3ViTConfig(\n+            patch_size=16,\n+            hidden_size=384,\n+            intermediate_size=1536,\n+            num_hidden_layers=12,\n+            num_attention_heads=6,\n+            num_register_tokens=4,\n+            use_gated_mlp=True,\n+            hidden_act=\"silu\",\n+        )\n+    elif model_name == \"vitb16_lvd1689m\":\n+        return DINOv3ViTConfig(\n+            patch_size=16,\n+            hidden_size=768,\n+            intermediate_size=3072,\n+            num_hidden_layers=12,\n+            num_attention_heads=12,\n+            proj_bias=True,\n+            num_register_tokens=4,\n+            use_gated_mlp=False,\n+            hidden_act=\"gelu\",\n+        )\n+    elif model_name in (\"vitl16_lvd1689m\", \"vitl16_sat493m\"):\n+        return DINOv3ViTConfig(\n+            patch_size=16,\n+            hidden_size=1024,\n+            intermediate_size=4096,\n+            num_hidden_layers=24,\n+            num_attention_heads=16,\n+            num_register_tokens=4,\n+            use_gated_mlp=False,\n+            hidden_act=\"gelu\",\n+        )\n+    elif model_name == \"vith16plus_lvd1689m\":\n+        return DINOv3ViTConfig(\n+            patch_size=16,\n+            hidden_size=1280,\n+            intermediate_size=5120,\n+            num_hidden_layers=32,\n+            num_attention_heads=20,\n+            num_register_tokens=4,\n+            use_gated_mlp=True,\n+            hidden_act=\"silu\",\n+        )\n+    elif model_name in (\"vit7b16_lvd1689m\", \"vit7b16_sat493m\"):\n+        return DINOv3ViTConfig(\n+            patch_size=16,\n+            hidden_size=4096,\n+            intermediate_size=8192,\n+            num_hidden_layers=40,\n+            num_attention_heads=32,\n+            query_bias=False,\n+            value_bias=False,\n+            num_register_tokens=4,\n+            use_gated_mlp=True,\n+            hidden_act=\"silu\",\n+        )\n+    else:\n+        raise ValueError(\"Model not supported\")\n+\n+\n+def prepare_img():\n+    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n+    return image\n+\n+\n+def get_transform(resize_size: int = 224):\n+    to_tensor = transforms.ToTensor()\n+    resize = transforms.Resize((resize_size, resize_size), antialias=True)\n+    normalize = transforms.Normalize(\n+        mean=(0.485, 0.456, 0.406),\n+        std=(0.229, 0.224, 0.225),\n+    )\n+    return transforms.Compose([to_tensor, resize, normalize])\n+\n+\n+def get_image_processor(resize_size: int = 224):\n+    return DINOv3ViTImageProcessorFast(\n+        do_resize=True,\n+        size={\"height\": resize_size, \"width\": resize_size},\n+        resample=2,  # BILINEAR\n+    )\n+\n+\n+@torch.no_grad()\n+def convert_and_test_dinov3_checkpoint(args):\n+    expected_outputs = {\n+        \"vits16_lvd1689m_cls\": [0.463561, -0.415609, 0.408236, -0.126613, -0.286636],\n+        \"vits16_lvd1689m_patch\": [-0.038754, -0.250895, -0.016392, -0.455473, 0.571582],\n+        \"vits16plus_lvd1689m_cls\": [-0.471349, -1.365778, -0.317983, 0.377219, -0.769085],\n+        \"vits16plus_lvd1689m_patch\": [0.144551, -0.388117, -0.393433, -0.157695, -0.600380],\n+        \"vitb16_lvd1689m_cls\": [1.034643, -0.180609, -0.341018, -0.066376, -0.011383],\n+        \"vitb16_lvd1689m_patch\": [-0.082523, -0.456272, -0.728029, -0.430680, -0.152880],\n+        \"vitl16_lvd1689m_cls\": [0.484527, -0.582214, 0.480636, 0.592040, 0.945166],\n+        \"vitl16_lvd1689m_patch\": [-0.211367, -0.490863, -0.257131, 0.101763, 0.154511],\n+        \"vith16plus_lvd1689m_cls\": [-0.064575, -0.148866, -0.621524, 0.634878, 0.152695],\n+        \"vith16plus_lvd1689m_patch\": [-0.093817, 0.287407, -0.050036, 0.428043, 0.094561],\n+        \"vit7b16_lvd1689m_cls\": [0.275439, -0.261353, 0.067772, 0.049936, -0.158747],\n+        \"vit7b16_lvd1689m_patch\": [0.044442, -0.052542, 0.070777, -0.065111, -0.026546],\n+        \"vitl16_sat493m_cls\": [-0.33235, 0.34052, -0.22087, 0.21434, 0.09003],\n+        \"vitl16_sat493m_patch\": [0.18488, 0.30309, -0.20689, 0.12848, 0.06207],\n+        \"vit7b16_sat493m_cls\": [-0.19779, 0.11819, -0.00581, -0.21055, -0.03971],\n+        \"vit7b16_sat493m_patch\": [-0.12423, 0.07879, -0.10057, 0.02835, -0.11727],\n+    }\n+\n+    model_name = args.model_name\n+    config = get_dinov3_config(model_name)\n+\n+    model = DINOv3ViTModel(config).eval()\n+    state_dict_path = hf_hub_download(repo_id=HUB_MODELS[model_name], filename=HUB_CHECKPOINTS[model_name])\n+    original_state_dict = torch.load(state_dict_path, mmap=True)\n+\n+    original_state_dict = split_qkv(original_state_dict)\n+    original_keys = list(original_state_dict.keys())\n+    new_keys = convert_old_keys_to_new_keys(original_keys)\n+\n+    converted_state_dict = {}\n+    for key in original_keys:\n+        new_key = new_keys[key]\n+        weight_tensor = original_state_dict[key]\n+\n+        if \"bias_mask\" in key or \"attn.k_proj.bias\" in key or \"local_cls_norm\" in key:\n+            continue\n+        if \"embeddings.mask_token\" in new_key:\n+            weight_tensor = weight_tensor.unsqueeze(1)\n+        if \"inv_freq\" in new_key:\n+            continue\n+\n+        converted_state_dict[new_key] = weight_tensor\n+\n+    model.load_state_dict(converted_state_dict, strict=True)\n+    model = model.eval()\n+\n+    transform = get_transform()\n+    image_processor = get_image_processor()\n+    image = prepare_img()\n+\n+    # check preprocessing\n+    original_pixel_values = transform(image).unsqueeze(0)  # add batch dimension\n+    inputs = image_processor(image, return_tensors=\"pt\")\n+\n+    torch.testing.assert_close(original_pixel_values, inputs[\"pixel_values\"], atol=1e-6, rtol=1e-6)\n+    print(\"Preprocessing looks ok!\")\n+\n+    with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.float):\n+        model_output = model(**inputs)\n+\n+    last_layer_class_token = model_output.pooler_output\n+    last_layer_patch_tokens = model_output.last_hidden_state[:, config.num_register_tokens + 1 :]\n+\n+    actual_outputs = {}\n+    actual_outputs[f\"{model_name}_cls\"] = last_layer_class_token[0, :5].tolist()\n+    actual_outputs[f\"{model_name}_patch\"] = last_layer_patch_tokens[0, 0, :5].tolist()\n+\n+    print(\"Actual:  \", [round(x, 6) for x in actual_outputs[f\"{model_name}_cls\"]])\n+    print(\"Expected:\", expected_outputs[f\"{model_name}_cls\"])\n+\n+    torch.testing.assert_close(\n+        torch.Tensor(actual_outputs[f\"{model_name}_cls\"]),\n+        torch.Tensor(expected_outputs[f\"{model_name}_cls\"]),\n+        atol=1e-3,\n+        rtol=1e-3,\n+    )\n+    torch.testing.assert_close(\n+        torch.Tensor(actual_outputs[f\"{model_name}_patch\"]),\n+        torch.Tensor(expected_outputs[f\"{model_name}_patch\"]),\n+        atol=1e-3,\n+        rtol=1e-3,\n+    )\n+    print(\"Forward pass looks ok!\")\n+\n+    save_dir = os.path.join(args.save_dir, model_name)\n+    os.makedirs(save_dir, exist_ok=True)\n+    model.save_pretrained(save_dir)\n+    image_processor.save_pretrained(save_dir)\n+    print(f\"Model saved to {save_dir}\")\n+\n+    if args.push_to_hub:\n+        api = HfApi()\n+        repo = HUB_MODELS[model_name]\n+        api.upload_folder(folder_path=save_dir, repo_id=repo, repo_type=\"model\")\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    # Required parameters\n+    parser.add_argument(\n+        \"--model-name\",\n+        default=\"vith16plus_lvd1689m\",\n+        type=str,\n+        choices=[\n+            \"vits16_lvd1689m\",\n+            \"vits16plus_lvd1689m\",\n+            \"vitb16_lvd1689m\",\n+            \"vitl16_lvd1689m\",\n+            \"vitl16_sat493m\",\n+            \"vith16plus_lvd1689m\",\n+            \"vit7b16_lvd1689m\",\n+            \"vit7b16_sat493m\",\n+        ],\n+        help=\"Name of the model you'd like to convert.\",\n+    )\n+    parser.add_argument(\n+        \"--save-dir\",\n+        default=\"converted_models\",\n+        type=str,\n+        help=\"Directory to save the converted model.\",\n+    )\n+    parser.add_argument(\n+        \"--push-to-hub\",\n+        action=\"store_true\",\n+        help=\"Push the converted model to the Hugging Face Hub.\",\n+    )\n+    args = parser.parse_args()\n+    convert_and_test_dinov3_checkpoint(args)"
        },
        {
            "sha": "3664bdd20ae8f9ffd5419f2a95d80a2a80ebef6e",
            "filename": "src/transformers/models/dinov3_vit/image_processing_dinov3_vit_fast.py",
            "status": "added",
            "additions": 104,
            "deletions": 0,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fimage_processing_dinov3_vit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fimage_processing_dinov3_vit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fimage_processing_dinov3_vit_fast.py?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0",
            "patch": "@@ -0,0 +1,104 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for DINOv3.\"\"\"\n+\n+from typing import Optional, Union\n+\n+from transformers.image_processing_base import BatchFeature\n+from transformers.image_processing_utils_fast import BaseImageProcessorFast, group_images_by_shape, reorder_images\n+from transformers.image_utils import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, PILImageResampling, SizeDict\n+from transformers.utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    logging,\n+)\n+from transformers.utils.import_utils import requires\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_v2_available():\n+    from torchvision.transforms.v2 import functional as F\n+elif is_torchvision_available():\n+    from torchvision.transforms import functional as F\n+\n+\n+@auto_docstring\n+@requires(backends=(\"torchvision\", \"torch\"))\n+class DINOv3ViTImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_DEFAULT_MEAN\n+    image_std = IMAGENET_DEFAULT_STD\n+    size = {\"height\": 224, \"width\": 224}\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+\n+    # Overriden for DINOv3 to preserve order of transforms\n+    # rescale -> resize -> normalize\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_center_crop: bool,\n+        crop_size: SizeDict,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n+    ) -> BatchFeature:\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_rescale:\n+                stacked_images = self.rescale(stacked_images, rescale_factor)\n+            if do_resize:\n+                stacked_images = self.resize(\n+                    image=stacked_images, size=size, interpolation=interpolation, antialias=True\n+                )\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_center_crop:\n+                stacked_images = self.center_crop(stacked_images, crop_size)\n+            if do_normalize:\n+                stacked_images = self.normalize(stacked_images, image_mean, image_std)\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"DINOv3ViTImageProcessorFast\"]"
        },
        {
            "sha": "dbea73e6caf50a31aab79a522b8a9713ff035de8",
            "filename": "src/transformers/models/dinov3_vit/modeling_dinov3_vit.py",
            "status": "added",
            "additions": 538,
            "deletions": 0,
            "changes": 538,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0",
            "patch": "@@ -0,0 +1,538 @@\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+#           This file was automatically generated from src/transformers/models/dinov3_vit/modular_dinov3_vit.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_dinov3_vit.py file directly. One of our CI enforces this.\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+# coding=utf-8\n+# Copyright 2025 Meta AI and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from typing import Callable, Optional\n+\n+import numpy as np\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutputWithPooling\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...pytorch_utils import compile_compatible_method_lru_cache\n+from ...utils import TransformersKwargs, auto_docstring\n+from ...utils.generic import check_model_inputs\n+from .configuration_dinov3_vit import DINOv3ViTConfig\n+\n+\n+class DINOv3ViTEmbeddings(nn.Module):\n+    \"\"\"\n+    Construct the CLS token, mask token, position and patch embeddings.\n+    \"\"\"\n+\n+    def __init__(self, config: DINOv3ViTConfig):\n+        super().__init__()\n+        self.config = config\n+        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_size))\n+        self.mask_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n+        self.register_tokens = nn.Parameter(torch.empty(1, config.num_register_tokens, config.hidden_size))\n+        self.patch_embeddings = nn.Conv2d(\n+            config.num_channels, config.hidden_size, kernel_size=config.patch_size, stride=config.patch_size\n+        )\n+\n+    def forward(self, pixel_values: torch.Tensor, bool_masked_pos: Optional[torch.Tensor] = None) -> torch.Tensor:\n+        batch_size = pixel_values.shape[0]\n+        target_dtype = self.patch_embeddings.weight.dtype\n+\n+        # (batch_size, num_channels, height, width) -> (batch_size, num_patches, hidden_size)\n+        patch_embeddings = self.patch_embeddings(pixel_values.to(dtype=target_dtype))\n+        patch_embeddings = patch_embeddings.flatten(2).transpose(1, 2)\n+\n+        if bool_masked_pos is not None:\n+            mask_token = self.mask_token.to(patch_embeddings.dtype)\n+            patch_embeddings = torch.where(bool_masked_pos.unsqueeze(-1), mask_token, patch_embeddings)\n+\n+        # Add CLS and register tokens\n+        cls_token = self.cls_token.expand(batch_size, -1, -1)\n+        register_tokens = self.register_tokens.expand(batch_size, -1, -1)\n+        embeddings = torch.cat([cls_token, register_tokens, patch_embeddings], dim=1)\n+\n+        return embeddings\n+\n+\n+@compile_compatible_method_lru_cache(maxsize=32)\n+def get_patches_center_coordinates(\n+    num_patches_h: int, num_patches_w: int, dtype: torch.dtype, device: torch.device\n+) -> torch.Tensor:\n+    \"\"\"\n+    Computes the 2D coordinates of the centers of image patches, normalized to the range [-1, +1].\n+    The center of each patch is exactly halfway between its top-left and bottom-right corners.\n+\n+    Args:\n+        num_patches_h (int): Number of patches along the vertical (height) axis.\n+        num_patches_w (int): Number of patches along the horizontal (width) axis.\n+        dtype (torch.dtype): The desired data type of the returned tensor.\n+\n+    Returns:\n+        torch.Tensor: A tensor of shape (height * width, 2), where each row contains the (y, x)\n+            coordinates of a patch center, normalized to [-1, +1].\n+    \"\"\"\n+    coords_h = torch.arange(0.5, num_patches_h, dtype=dtype, device=device)\n+    coords_w = torch.arange(0.5, num_patches_w, dtype=dtype, device=device)\n+    coords_h = coords_h / num_patches_h\n+    coords_w = coords_w / num_patches_w\n+    # (height, width, 2) -> (height * width, 2)\n+    coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing=\"ij\"), dim=-1)\n+    coords = coords.flatten(0, 1)\n+    # Shift range [0, 1] to [-1, +1]\n+    coords = 2.0 * coords - 1.0\n+    return coords\n+\n+\n+def augment_patches_center_coordinates(\n+    coords: torch.Tensor,\n+    shift: Optional[float] = None,\n+    jitter: Optional[float] = None,\n+    rescale: Optional[float] = None,\n+) -> torch.Tensor:\n+    # Shift coords by adding a uniform value in [-shift, shift]\n+    if shift is not None:\n+        shift_hw = torch.empty((1, 2), device=coords.device, dtype=coords.dtype)\n+        shift_hw = shift_hw.uniform_(-shift, shift)\n+        coords = coords + shift_hw\n+\n+    # Jitter coords by multiplying the range [-1, 1] by a log-uniform value in [1/jitter, jitter]\n+    if jitter is not None:\n+        jitter_range = np.log(jitter)\n+        jitter_hw = torch.empty((1, 2), device=coords.device, dtype=coords.dtype)\n+        jitter_hw = jitter_hw.uniform_(-jitter_range, jitter_range).exp()\n+        coords = coords * jitter_hw\n+\n+    # Rescale coords by multiplying the range [-1, 1] by a log-uniform value in [1/rescale, rescale]\n+    if rescale is not None:\n+        rescale_range = np.log(rescale)\n+        rescale_hw = torch.empty(1, device=coords.device, dtype=coords.dtype)\n+        rescale_hw = rescale_hw.uniform_(-rescale_range, rescale_range).exp()\n+        coords = coords * rescale_hw\n+\n+    return coords\n+\n+\n+class DINOv3ViTRopePositionEmbedding(nn.Module):\n+    inv_freq: torch.Tensor\n+\n+    def __init__(self, config: DINOv3ViTConfig):\n+        super().__init__()\n+\n+        self.config = config\n+        self.base = config.rope_theta\n+        self.head_dim = config.hidden_size // config.num_attention_heads\n+        self.num_patches_h = config.image_size // config.patch_size\n+        self.num_patches_w = config.image_size // config.patch_size\n+\n+        inv_freq = 1 / self.base ** torch.arange(0, 1, 4 / self.head_dim, dtype=torch.float32)  # (head_dim / 4,)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+\n+    def forward(self, pixel_values: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n+        _, _, height, width = pixel_values.shape\n+        num_patches_h = height // self.config.patch_size\n+        num_patches_w = width // self.config.patch_size\n+\n+        device = pixel_values.device\n+        device_type = device.type if isinstance(device.type, str) and device.type != \"mps\" else \"cpu\"\n+\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            # Although we could precompute static patch_coords from image_size and patch_size in the config,\n+            # the model was trained with random_scale, so it can process images of varying sizes.\n+            # Therefore, it's better to compute patch_coords dynamically (with lru_cache).\n+            patch_coords = get_patches_center_coordinates(\n+                num_patches_h, num_patches_w, dtype=torch.float32, device=device\n+            )\n+            if self.training:\n+                patch_coords = augment_patches_center_coordinates(\n+                    patch_coords,\n+                    shift=self.config.pos_embed_shift,\n+                    jitter=self.config.pos_embed_jitter,\n+                    rescale=self.config.pos_embed_rescale,\n+                )\n+\n+            # (height * width, 2, head_dim / 4) -> (height * width, head_dim / 2) -> (height * width, head_dim)\n+            angles = 2 * math.pi * patch_coords[:, :, None] * self.inv_freq[None, None, :]\n+            angles = angles.flatten(1, 2)\n+            angles = angles.tile(2)\n+\n+            cos = torch.cos(angles)\n+            sin = torch.sin(angles)\n+\n+        dtype = pixel_values.dtype\n+        return cos.to(dtype=dtype), sin.to(dtype=dtype)\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+\n+    # Normalize the attention scores to probabilities.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+\n+    # This is actually dropping out entire tokens to attend to, which might\n+    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    # Mask heads if we want to\n+    if attention_mask is not None:\n+        attn_weights = attn_weights * attention_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+def apply_rotary_pos_emb(\n+    q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor, **kwargs\n+) -> tuple[torch.Tensor, torch.Tensor]:\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors, but only to the patch tokens,\n+    ignoring the prefix tokens (cls token and register tokens).\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+\n+    num_tokens = q.shape[-2]\n+    num_patches = sin.shape[-2]\n+    num_prefix_tokens = num_tokens - num_patches  # cls token + register tokens\n+\n+    q_prefix_tokens, q_patches = q.split((num_prefix_tokens, num_patches), dim=-2)\n+    k_prefix_tokens, k_patches = k.split((num_prefix_tokens, num_patches), dim=-2)\n+\n+    # apply rope only to patch tokens\n+    q_patches = (q_patches * cos) + (rotate_half(q_patches) * sin)\n+    k_patches = (k_patches * cos) + (rotate_half(k_patches) * sin)\n+\n+    q = torch.cat((q_prefix_tokens, q_patches), dim=-2)\n+    k = torch.cat((k_prefix_tokens, k_patches), dim=-2)\n+\n+    return q, k\n+\n+\n+class DINOv3ViTAttention(nn.Module):\n+    \"\"\"\n+    Multi-headed attention compatible with ALL_ATTENTION_FUNCTIONS.\n+    \"\"\"\n+\n+    def __init__(self, config: DINOv3ViTConfig):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = self.embed_dim // self.num_heads\n+        self.is_causal = False\n+\n+        self.scaling = self.head_dim**-0.5\n+        self.is_causal = False\n+\n+        self.dropout = config.attention_dropout\n+        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.key_bias)\n+        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.value_bias)\n+\n+        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.query_bias)\n+        self.o_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.proj_bias)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+\n+        batch_size, patches, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(batch_size, patches, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(batch_size, patches, self.num_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(batch_size, patches, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(batch_size, patches, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+\n+        return attn_output, attn_weights\n+\n+\n+class DINOv3ViTLayerScale(nn.Module):\n+    def __init__(self, config) -> None:\n+        super().__init__()\n+        self.lambda1 = nn.Parameter(config.layerscale_value * torch.ones(config.hidden_size))\n+\n+    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n+        return hidden_state * self.lambda1\n+\n+\n+def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:\n+    \"\"\"\n+    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n+\n+    Comment by Ross Wightman: This is the same as the DropConnect impl I created for EfficientNet, etc networks,\n+    however, the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n+    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for changing the\n+    layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use 'survival rate' as the\n+    argument.\n+    \"\"\"\n+    if drop_prob == 0.0 or not training:\n+        return input\n+    keep_prob = 1 - drop_prob\n+    shape = (input.shape[0],) + (1,) * (input.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n+    random_tensor = keep_prob + torch.rand(shape, dtype=input.dtype, device=input.device)\n+    random_tensor.floor_()  # binarize\n+    output = input.div(keep_prob) * random_tensor\n+    return output\n+\n+\n+class DINOv3ViTDropPath(nn.Module):\n+    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n+\n+    def __init__(self, drop_prob: Optional[float] = None) -> None:\n+        super().__init__()\n+        self.drop_prob = drop_prob\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        return drop_path(hidden_states, self.drop_prob, self.training)\n+\n+    def extra_repr(self) -> str:\n+        return f\"p={self.drop_prob}\"\n+\n+\n+class DINOv3ViTMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        return self.down_proj(self.act_fn(self.up_proj(x)))\n+\n+\n+class DINOv3ViTGatedMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+class DINOv3ViTLayer(GradientCheckpointingLayer):\n+    \"\"\"This corresponds to the Block class in the original implementation.\"\"\"\n+\n+    def __init__(self, config: DINOv3ViTConfig):\n+        super().__init__()\n+\n+        self.norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.attention = DINOv3ViTAttention(config)\n+        self.layer_scale1 = DINOv3ViTLayerScale(config)\n+        self.drop_path = DINOv3ViTDropPath(config.drop_path_rate) if config.drop_path_rate > 0.0 else nn.Identity()\n+\n+        self.norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+        if config.use_gated_mlp:\n+            self.mlp = DINOv3ViTGatedMLP(config)\n+        else:\n+            self.mlp = DINOv3ViTMLP(config)\n+        self.layer_scale2 = DINOv3ViTLayerScale(config)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+    ) -> torch.Tensor:\n+        # Attention with residual connection\n+        residual = hidden_states\n+        hidden_states = self.norm1(hidden_states)\n+        hidden_states, _ = self.attention(\n+            hidden_states,\n+            attention_mask=attention_mask,\n+            position_embeddings=position_embeddings,\n+        )\n+        hidden_states = self.layer_scale1(hidden_states)\n+        hidden_states = self.drop_path(hidden_states) + residual\n+\n+        # MLP with residual connection\n+        residual = hidden_states\n+        hidden_states = self.norm2(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = self.layer_scale2(hidden_states)\n+        hidden_states = self.drop_path(hidden_states) + residual\n+\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class DINOv3ViTPreTrainedModel(PreTrainedModel):\n+    config: DINOv3ViTConfig\n+    base_model_prefix = \"dinov3_vit\"\n+    main_input_name = \"pixel_values\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"DINOv3ViTLayer\"]\n+    _supports_sdpa = True\n+    _supports_flash_attn = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": DINOv3ViTLayer,\n+        \"attentions\": DINOv3ViTAttention,\n+    }\n+\n+    def _init_weights(self, module) -> None:\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n+            # `trunc_normal_cpu` not implemented in `half` issues\n+            module.weight.data = nn.init.trunc_normal_(\n+                module.weight.data.to(torch.float32),\n+                mean=0.0,\n+                std=self.config.initializer_range,\n+            ).to(module.weight.dtype)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, DINOv3ViTEmbeddings):\n+            module.cls_token.data = nn.init.trunc_normal_(\n+                module.cls_token.data.to(torch.float32),\n+                mean=0.0,\n+                std=self.config.initializer_range,\n+            ).to(module.cls_token.dtype)\n+            if module.config.num_register_tokens > 0:\n+                module.register_tokens.data = nn.init.trunc_normal_(\n+                    module.register_tokens.data.to(torch.float32),\n+                    mean=0.0,\n+                    std=self.config.initializer_range,\n+                ).to(module.register_tokens.dtype)\n+            module.mask_token.data.zero_()\n+        elif isinstance(module, DINOv3ViTLayerScale):\n+            module.lambda1.data.fill_(self.config.layerscale_value)\n+\n+\n+@auto_docstring\n+class DINOv3ViTModel(DINOv3ViTPreTrainedModel):\n+    def __init__(self, config: DINOv3ViTConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.embeddings = DINOv3ViTEmbeddings(config)\n+        self.rope_embeddings = DINOv3ViTRopePositionEmbedding(config)\n+        self.layer = nn.ModuleList([DINOv3ViTLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.gradient_checkpointing = False\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embeddings.patch_embeddings\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: torch.Tensor,\n+        bool_masked_pos: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutputWithPooling:\n+        r\"\"\"\n+        bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, sequence_length)`):\n+            Boolean masked positions. Indicates which patches are masked (1) and which aren't (0). Only relevant for\n+            pre-training.\n+        \"\"\"\n+\n+        pixel_values = pixel_values.to(self.embeddings.patch_embeddings.weight.dtype)\n+        hidden_states = self.embeddings(pixel_values, bool_masked_pos=bool_masked_pos)\n+        position_embeddings = self.rope_embeddings(pixel_values)\n+\n+        for i, layer_module in enumerate(self.layer):\n+            layer_head_mask = head_mask[i] if head_mask is not None else None\n+            hidden_states = layer_module(\n+                hidden_states,\n+                attention_mask=layer_head_mask,\n+                position_embeddings=position_embeddings,\n+            )\n+\n+        sequence_output = self.norm(hidden_states)\n+        pooled_output = sequence_output[:, 0, :]\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=sequence_output,\n+            pooler_output=pooled_output,\n+        )\n+\n+\n+__all__ = [\"DINOv3ViTModel\", \"DINOv3ViTPreTrainedModel\"]"
        },
        {
            "sha": "f4a1e69beaacea50504d3865f7fc5bfdea1dfdc5",
            "filename": "src/transformers/models/dinov3_vit/modular_dinov3_vit.py",
            "status": "added",
            "additions": 429,
            "deletions": 0,
            "changes": 429,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodular_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodular_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodular_dinov3_vit.py?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0",
            "patch": "@@ -0,0 +1,429 @@\n+# coding=utf-8\n+# Copyright 2025 Meta AI and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch DINOv3 model.\"\"\"\n+\n+import math\n+from typing import Callable, Optional\n+\n+import numpy as np\n+import torch\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from transformers.models.arcee.modeling_arcee import ArceeMLP\n+from transformers.models.dinov2.modeling_dinov2 import (\n+    Dinov2DropPath,\n+    Dinov2LayerScale,\n+    Dinov2PreTrainedModel,\n+    eager_attention_forward,\n+)\n+from transformers.models.llama.modeling_llama import LlamaMLP\n+from transformers.models.pixtral.modeling_pixtral import PixtralAttention, rotate_half\n+\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutputWithPooling\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import Unpack\n+from ...pytorch_utils import compile_compatible_method_lru_cache\n+from ...utils import TransformersKwargs, auto_docstring, logging\n+from ...utils.generic import check_model_inputs\n+from .configuration_dinov3_vit import DINOv3ViTConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class DINOv3ViTEmbeddings(nn.Module):\n+    \"\"\"\n+    Construct the CLS token, mask token, position and patch embeddings.\n+    \"\"\"\n+\n+    def __init__(self, config: DINOv3ViTConfig):\n+        super().__init__()\n+        self.config = config\n+        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_size))\n+        self.mask_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n+        self.register_tokens = nn.Parameter(torch.empty(1, config.num_register_tokens, config.hidden_size))\n+        self.patch_embeddings = nn.Conv2d(\n+            config.num_channels, config.hidden_size, kernel_size=config.patch_size, stride=config.patch_size\n+        )\n+\n+    def forward(self, pixel_values: torch.Tensor, bool_masked_pos: Optional[torch.Tensor] = None) -> torch.Tensor:\n+        batch_size = pixel_values.shape[0]\n+        target_dtype = self.patch_embeddings.weight.dtype\n+\n+        # (batch_size, num_channels, height, width) -> (batch_size, num_patches, hidden_size)\n+        patch_embeddings = self.patch_embeddings(pixel_values.to(dtype=target_dtype))\n+        patch_embeddings = patch_embeddings.flatten(2).transpose(1, 2)\n+\n+        if bool_masked_pos is not None:\n+            mask_token = self.mask_token.to(patch_embeddings.dtype)\n+            patch_embeddings = torch.where(bool_masked_pos.unsqueeze(-1), mask_token, patch_embeddings)\n+\n+        # Add CLS and register tokens\n+        cls_token = self.cls_token.expand(batch_size, -1, -1)\n+        register_tokens = self.register_tokens.expand(batch_size, -1, -1)\n+        embeddings = torch.cat([cls_token, register_tokens, patch_embeddings], dim=1)\n+\n+        return embeddings\n+\n+\n+@compile_compatible_method_lru_cache(maxsize=32)\n+def get_patches_center_coordinates(\n+    num_patches_h: int, num_patches_w: int, dtype: torch.dtype, device: torch.device\n+) -> torch.Tensor:\n+    \"\"\"\n+    Computes the 2D coordinates of the centers of image patches, normalized to the range [-1, +1].\n+    The center of each patch is exactly halfway between its top-left and bottom-right corners.\n+\n+    Args:\n+        num_patches_h (int): Number of patches along the vertical (height) axis.\n+        num_patches_w (int): Number of patches along the horizontal (width) axis.\n+        dtype (torch.dtype): The desired data type of the returned tensor.\n+\n+    Returns:\n+        torch.Tensor: A tensor of shape (height * width, 2), where each row contains the (y, x)\n+            coordinates of a patch center, normalized to [-1, +1].\n+    \"\"\"\n+    coords_h = torch.arange(0.5, num_patches_h, dtype=dtype, device=device)\n+    coords_w = torch.arange(0.5, num_patches_w, dtype=dtype, device=device)\n+    coords_h = coords_h / num_patches_h\n+    coords_w = coords_w / num_patches_w\n+    # (height, width, 2) -> (height * width, 2)\n+    coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing=\"ij\"), dim=-1)\n+    coords = coords.flatten(0, 1)\n+    # Shift range [0, 1] to [-1, +1]\n+    coords = 2.0 * coords - 1.0\n+    return coords\n+\n+\n+def augment_patches_center_coordinates(\n+    coords: torch.Tensor,\n+    shift: Optional[float] = None,\n+    jitter: Optional[float] = None,\n+    rescale: Optional[float] = None,\n+) -> torch.Tensor:\n+    # Shift coords by adding a uniform value in [-shift, shift]\n+    if shift is not None:\n+        shift_hw = torch.empty((1, 2), device=coords.device, dtype=coords.dtype)\n+        shift_hw = shift_hw.uniform_(-shift, shift)\n+        coords = coords + shift_hw\n+\n+    # Jitter coords by multiplying the range [-1, 1] by a log-uniform value in [1/jitter, jitter]\n+    if jitter is not None:\n+        jitter_range = np.log(jitter)\n+        jitter_hw = torch.empty((1, 2), device=coords.device, dtype=coords.dtype)\n+        jitter_hw = jitter_hw.uniform_(-jitter_range, jitter_range).exp()\n+        coords = coords * jitter_hw\n+\n+    # Rescale coords by multiplying the range [-1, 1] by a log-uniform value in [1/rescale, rescale]\n+    if rescale is not None:\n+        rescale_range = np.log(rescale)\n+        rescale_hw = torch.empty(1, device=coords.device, dtype=coords.dtype)\n+        rescale_hw = rescale_hw.uniform_(-rescale_range, rescale_range).exp()\n+        coords = coords * rescale_hw\n+\n+    return coords\n+\n+\n+class DINOv3ViTRopePositionEmbedding(nn.Module):\n+    inv_freq: torch.Tensor\n+\n+    def __init__(self, config: DINOv3ViTConfig):\n+        super().__init__()\n+\n+        self.config = config\n+        self.base = config.rope_theta\n+        self.head_dim = config.hidden_size // config.num_attention_heads\n+        self.num_patches_h = config.image_size // config.patch_size\n+        self.num_patches_w = config.image_size // config.patch_size\n+\n+        inv_freq = 1 / self.base ** torch.arange(0, 1, 4 / self.head_dim, dtype=torch.float32)  # (head_dim / 4,)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+\n+    def forward(self, pixel_values: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n+        _, _, height, width = pixel_values.shape\n+        num_patches_h = height // self.config.patch_size\n+        num_patches_w = width // self.config.patch_size\n+\n+        device = pixel_values.device\n+        device_type = device.type if isinstance(device.type, str) and device.type != \"mps\" else \"cpu\"\n+\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            # Although we could precompute static patch_coords from image_size and patch_size in the config,\n+            # the model was trained with random_scale, so it can process images of varying sizes.\n+            # Therefore, it's better to compute patch_coords dynamically (with lru_cache).\n+            patch_coords = get_patches_center_coordinates(\n+                num_patches_h, num_patches_w, dtype=torch.float32, device=device\n+            )\n+            if self.training:\n+                patch_coords = augment_patches_center_coordinates(\n+                    patch_coords,\n+                    shift=self.config.pos_embed_shift,\n+                    jitter=self.config.pos_embed_jitter,\n+                    rescale=self.config.pos_embed_rescale,\n+                )\n+\n+            # (height * width, 2, head_dim / 4) -> (height * width, head_dim / 2) -> (height * width, head_dim)\n+            angles = 2 * math.pi * patch_coords[:, :, None] * self.inv_freq[None, None, :]\n+            angles = angles.flatten(1, 2)\n+            angles = angles.tile(2)\n+\n+            cos = torch.cos(angles)\n+            sin = torch.sin(angles)\n+\n+        dtype = pixel_values.dtype\n+        return cos.to(dtype=dtype), sin.to(dtype=dtype)\n+\n+\n+def apply_rotary_pos_emb(\n+    q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor, **kwargs\n+) -> tuple[torch.Tensor, torch.Tensor]:\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors, but only to the patch tokens,\n+    ignoring the prefix tokens (cls token and register tokens).\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+\n+    num_tokens = q.shape[-2]\n+    num_patches = sin.shape[-2]\n+    num_prefix_tokens = num_tokens - num_patches  # cls token + register tokens\n+\n+    q_prefix_tokens, q_patches = q.split((num_prefix_tokens, num_patches), dim=-2)\n+    k_prefix_tokens, k_patches = k.split((num_prefix_tokens, num_patches), dim=-2)\n+\n+    # apply rope only to patch tokens\n+    q_patches = (q_patches * cos) + (rotate_half(q_patches) * sin)\n+    k_patches = (k_patches * cos) + (rotate_half(k_patches) * sin)\n+\n+    q = torch.cat((q_prefix_tokens, q_patches), dim=-2)\n+    k = torch.cat((k_prefix_tokens, k_patches), dim=-2)\n+\n+    return q, k\n+\n+\n+class DINOv3ViTAttention(PixtralAttention):\n+    def __init__(self, config: DINOv3ViTConfig):\n+        super().__init__(config)\n+\n+        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.query_bias)\n+        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.key_bias)\n+        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.value_bias)\n+        self.o_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.proj_bias)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+\n+        batch_size, patches, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(batch_size, patches, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(batch_size, patches, self.num_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(batch_size, patches, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(batch_size, patches, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+\n+        return attn_output, attn_weights\n+\n+\n+class DINOv3ViTLayerScale(Dinov2LayerScale):\n+    pass\n+\n+\n+class DINOv3ViTDropPath(Dinov2DropPath):\n+    pass\n+\n+\n+class DINOv3ViTMLP(ArceeMLP):\n+    pass\n+\n+\n+class DINOv3ViTGatedMLP(LlamaMLP):\n+    pass\n+\n+\n+class DINOv3ViTLayer(GradientCheckpointingLayer):\n+    \"\"\"This corresponds to the Block class in the original implementation.\"\"\"\n+\n+    def __init__(self, config: DINOv3ViTConfig):\n+        super().__init__()\n+\n+        self.norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.attention = DINOv3ViTAttention(config)\n+        self.layer_scale1 = DINOv3ViTLayerScale(config)\n+        self.drop_path = DINOv3ViTDropPath(config.drop_path_rate) if config.drop_path_rate > 0.0 else nn.Identity()\n+\n+        self.norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+        if config.use_gated_mlp:\n+            self.mlp = DINOv3ViTGatedMLP(config)\n+        else:\n+            self.mlp = DINOv3ViTMLP(config)\n+        self.layer_scale2 = DINOv3ViTLayerScale(config)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+    ) -> torch.Tensor:\n+        # Attention with residual connection\n+        residual = hidden_states\n+        hidden_states = self.norm1(hidden_states)\n+        hidden_states, _ = self.attention(\n+            hidden_states,\n+            attention_mask=attention_mask,\n+            position_embeddings=position_embeddings,\n+        )\n+        hidden_states = self.layer_scale1(hidden_states)\n+        hidden_states = self.drop_path(hidden_states) + residual\n+\n+        # MLP with residual connection\n+        residual = hidden_states\n+        hidden_states = self.norm2(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = self.layer_scale2(hidden_states)\n+        hidden_states = self.drop_path(hidden_states) + residual\n+\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class DINOv3ViTPreTrainedModel(Dinov2PreTrainedModel):\n+    _can_record_outputs = {\n+        \"hidden_states\": DINOv3ViTLayer,\n+        \"attentions\": DINOv3ViTAttention,\n+    }\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n+            # `trunc_normal_cpu` not implemented in `half` issues\n+            module.weight.data = nn.init.trunc_normal_(\n+                module.weight.data.to(torch.float32),\n+                mean=0.0,\n+                std=self.config.initializer_range,\n+            ).to(module.weight.dtype)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, DINOv3ViTEmbeddings):\n+            module.cls_token.data = nn.init.trunc_normal_(\n+                module.cls_token.data.to(torch.float32),\n+                mean=0.0,\n+                std=self.config.initializer_range,\n+            ).to(module.cls_token.dtype)\n+            if module.config.num_register_tokens > 0:\n+                module.register_tokens.data = nn.init.trunc_normal_(\n+                    module.register_tokens.data.to(torch.float32),\n+                    mean=0.0,\n+                    std=self.config.initializer_range,\n+                ).to(module.register_tokens.dtype)\n+            module.mask_token.data.zero_()\n+        elif isinstance(module, DINOv3ViTLayerScale):\n+            module.lambda1.data.fill_(self.config.layerscale_value)\n+\n+\n+@auto_docstring\n+class DINOv3ViTModel(DINOv3ViTPreTrainedModel):\n+    def __init__(self, config: DINOv3ViTConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.embeddings = DINOv3ViTEmbeddings(config)\n+        self.rope_embeddings = DINOv3ViTRopePositionEmbedding(config)\n+        self.layer = nn.ModuleList([DINOv3ViTLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.gradient_checkpointing = False\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embeddings.patch_embeddings\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: torch.Tensor,\n+        bool_masked_pos: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutputWithPooling:\n+        r\"\"\"\n+        bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, sequence_length)`):\n+            Boolean masked positions. Indicates which patches are masked (1) and which aren't (0). Only relevant for\n+            pre-training.\n+        \"\"\"\n+\n+        pixel_values = pixel_values.to(self.embeddings.patch_embeddings.weight.dtype)\n+        hidden_states = self.embeddings(pixel_values, bool_masked_pos=bool_masked_pos)\n+        position_embeddings = self.rope_embeddings(pixel_values)\n+\n+        for i, layer_module in enumerate(self.layer):\n+            layer_head_mask = head_mask[i] if head_mask is not None else None\n+            hidden_states = layer_module(\n+                hidden_states,\n+                attention_mask=layer_head_mask,\n+                position_embeddings=position_embeddings,\n+            )\n+\n+        sequence_output = self.norm(hidden_states)\n+        pooled_output = sequence_output[:, 0, :]\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=sequence_output,\n+            pooler_output=pooled_output,\n+        )\n+\n+\n+__all__ = [\"DINOv3ViTModel\", \"DINOv3ViTPreTrainedModel\"]"
        },
        {
            "sha": "60548bbc7e998bd6a77b4871341ee6b2fe3ef1a0",
            "filename": "src/transformers/pytorch_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 11,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fpytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Fpytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpytorch_utils.py?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0",
            "patch": "@@ -339,26 +339,22 @@ def isin_mps_friendly(elements: torch.Tensor, test_elements: torch.Tensor | int)\n         return torch.isin(elements, test_elements)\n \n \n+@wraps(lru_cache)\n def compile_compatible_method_lru_cache(*lru_args, **lru_kwargs):\n     \"\"\"\n     LRU cache decorator from standard functools library, but with a workaround to disable\n     caching when torchdynamo is compiling. Expected to work with class methods.\n     \"\"\"\n \n     def decorator(func):\n+        func_with_cache = lru_cache(*lru_args, **lru_kwargs)(func)\n+\n         @wraps(func)\n-        def wrapper(self, *args, **kwargs):\n-            if not is_torchdynamo_compiling():\n-                # Cache the function only if the model is not being compiled\n-                # check if the function is already cached, otherwise create it\n-                if not hasattr(self, f\"_cached_{func.__name__}\"):\n-                    self.__setattr__(\n-                        f\"_cached_{func.__name__}\", lru_cache(*lru_args, **lru_kwargs)(func.__get__(self))\n-                    )\n-                return self.__getattribute__(f\"_cached_{func.__name__}\")(*args, **kwargs)\n+        def wrapper(*args, **kwargs):\n+            if is_torchdynamo_compiling():\n+                return func(*args, **kwargs)\n             else:\n-                # Otherwise, just call the original function\n-                return func(self, *args, **kwargs)\n+                return func_with_cache(*args, **kwargs)\n \n         return wrapper\n "
        },
        {
            "sha": "13cdd6bb3f47ad661335c78954db8eeebbca955a",
            "filename": "src/transformers/utils/fx.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Futils%2Ffx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/src%2Ftransformers%2Futils%2Ffx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Ffx.py?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0",
            "patch": "@@ -129,6 +129,8 @@ def _generate_supported_model_class_names(\n     \"deberta\",\n     \"deberta-v2\",\n     \"dinov2\",\n+    \"dinov3_convnext\",\n+    \"dinov3_vit\",\n     \"distilbert\",\n     \"donut-swin\",\n     \"electra\","
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/dinov3_convnext/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/tests%2Fmodels%2Fdinov3_convnext%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/tests%2Fmodels%2Fdinov3_convnext%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov3_convnext%2F__init__.py?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0"
        },
        {
            "sha": "a34aacbd8e97b978d5306b1757c5ca071d1aaf57",
            "filename": "tests/models/dinov3_convnext/test_modeling_dinov3_convnext.py",
            "status": "added",
            "additions": 242,
            "deletions": 0,
            "changes": 242,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/tests%2Fmodels%2Fdinov3_convnext%2Ftest_modeling_dinov3_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/tests%2Fmodels%2Fdinov3_convnext%2Ftest_modeling_dinov3_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov3_convnext%2Ftest_modeling_dinov3_convnext.py?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0",
            "patch": "@@ -0,0 +1,242 @@\n+# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch ConvNext model.\"\"\"\n+\n+import unittest\n+\n+from transformers import DINOv3ConvNextConfig\n+from transformers.testing_utils import require_torch, require_vision, slow, torch_device\n+from transformers.utils import cached_property, is_torch_available, is_vision_available\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import DINOv3ConvNextModel\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import AutoImageProcessor\n+\n+\n+class DINOv3ConvNextModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=13,\n+        image_size=32,\n+        num_channels=3,\n+        hidden_sizes=[10, 20, 30, 40],\n+        depths=[2, 2, 3, 2],\n+        is_training=False,\n+        use_labels=True,\n+        intermediate_size=37,\n+        hidden_act=\"gelu\",\n+        num_labels=10,\n+        initializer_range=0.02,\n+        scope=None,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.image_size = image_size\n+        self.num_channels = num_channels\n+        self.hidden_sizes = hidden_sizes\n+        self.depths = depths\n+        self.is_training = is_training\n+        self.use_labels = use_labels\n+        self.intermediate_size = intermediate_size\n+        self.hidden_act = hidden_act\n+        self.num_labels = num_labels\n+        self.initializer_range = initializer_range\n+        self.scope = scope\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+\n+        labels = None\n+        if self.use_labels:\n+            labels = ids_tensor([self.batch_size], self.num_labels)\n+\n+        config = self.get_config()\n+        return config, pixel_values, labels\n+\n+    def get_config(self):\n+        return DINOv3ConvNextConfig(\n+            num_channels=self.num_channels,\n+            hidden_sizes=self.hidden_sizes,\n+            depths=self.depths,\n+            hidden_act=self.hidden_act,\n+            is_decoder=False,\n+            initializer_range=self.initializer_range,\n+            num_labels=self.num_labels,\n+        )\n+\n+    def create_and_check_model(self, config, pixel_values, labels):\n+        model = DINOv3ConvNextModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+        # expected last hidden states: B, C, H // 32, W // 32\n+        self.parent.assertEqual(\n+            result.last_hidden_state.shape,\n+            (\n+                self.batch_size,\n+                1 + self.image_size // 32 * self.image_size // 32,\n+                self.hidden_sizes[-1],\n+            ),\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values, labels = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class DINOv3ConvNextModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Here we also overwrite some of the tests of test_modeling_common.py, as ConvNext does not use input_ids, inputs_embeds,\n+    attention_mask and seq_length.\n+    \"\"\"\n+\n+    all_model_classes = (DINOv3ConvNextModel,) if is_torch_available() else ()\n+    pipeline_model_mapping = {\"image-feature-extraction\": DINOv3ConvNextModel} if is_torch_available() else {}\n+\n+    fx_compatible = False\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+    has_attentions = False\n+    test_torch_exportable = True\n+\n+    def setUp(self):\n+        self.model_tester = DINOv3ConvNextModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self,\n+            config_class=DINOv3ConvNextConfig,\n+            has_text_modality=False,\n+            hidden_size=37,\n+            common_properties=[\"num_channels\", \"hidden_sizes\"],\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    @unittest.skip(reason=\"DINOv3ConvNext does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"DINOv3ConvNext does not support input and output embeddings\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"DINOv3ConvNext does not use feedforward chunking\")\n+    def test_feed_forward_chunking(self):\n+        pass\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_hidden_states_output(self):\n+        def check_hidden_states_output(inputs_dict, config, model_class):\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            hidden_states = outputs.encoder_hidden_states if config.is_encoder_decoder else outputs.hidden_states\n+\n+            self.assertEqual(len(hidden_states), 5)\n+\n+            # DINOv3ConvNext's feature maps are of shape (batch_size, num_channels, height, width)\n+            self.assertListEqual(\n+                list(hidden_states[1].shape[-2:]),\n+                [self.model_tester.image_size // 4, self.model_tester.image_size // 4],\n+            )\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_hidden_states\"] = True\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+            # check that output_hidden_states also work using config\n+            del inputs_dict[\"output_hidden_states\"]\n+            config.output_hidden_states = True\n+\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+    @slow\n+    def test_model_from_pretrained(self):\n+        model_name = \"facebook/dinov3-convnext-tiny-pretrain-lvd1689m\"\n+        model = DINOv3ConvNextModel.from_pretrained(model_name)\n+        self.assertIsNotNone(model)\n+\n+    @unittest.skip(reason=\"DINOv3ConvNext does not retain grads for first hidden state (original pixel_values)\")\n+    def test_retain_grad_hidden_states_attentions(self):\n+        pass\n+\n+\n+# We will verify our results on an image of cute cats\n+def prepare_img():\n+    image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+    return image\n+\n+\n+@require_torch\n+@require_vision\n+class DINOv3ConvNextModelIntegrationTest(unittest.TestCase):\n+    @cached_property\n+    def default_image_processor(self):\n+        return (\n+            AutoImageProcessor.from_pretrained(\"facebook/dinov3-convnext-tiny-pretrain-lvd1689m\")\n+            if is_vision_available()\n+            else None\n+        )\n+\n+    @slow\n+    def test_inference_no_head(self):\n+        model = DINOv3ConvNextModel.from_pretrained(\"facebook/dinov3-convnext-tiny-pretrain-lvd1689m\").to(torch_device)\n+\n+        image_processor = self.default_image_processor\n+        image = prepare_img()\n+        inputs = image_processor(image, return_tensors=\"pt\").to(torch_device)\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+\n+        # verify the last hidden states\n+        _, _, height, width = inputs[\"pixel_values\"].shape\n+        expected_seq_length = (height * width) // 4 ** (model.config.num_stages + 1) + 1  # +1 for the \"CLS\" token\n+        expected_shape = torch.Size((1, expected_seq_length, model.config.hidden_sizes[-1]))\n+        self.assertEqual(outputs.last_hidden_state.shape, expected_shape)\n+\n+        last_layer_cls_token = outputs.pooler_output\n+        expected_slice = torch.tensor([-6.3721, 1.3008, 2.0743, -0.0800, 0.6072], device=torch_device)\n+        torch.testing.assert_close(last_layer_cls_token[0, :5], expected_slice, rtol=1e-4, atol=1e-4)\n+\n+        last_layer_patch_tokens = outputs.last_hidden_state[:, 1:]\n+        expected_slice = torch.tensor([0.4905, -3.7135, 1.8485, -1.0403, -1.0908], device=torch_device)\n+        torch.testing.assert_close(last_layer_patch_tokens[0, 0, :5], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/dinov3_vit/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/tests%2Fmodels%2Fdinov3_vit%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/tests%2Fmodels%2Fdinov3_vit%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov3_vit%2F__init__.py?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0"
        },
        {
            "sha": "552d5220953d737b619eb7d95c8ccf22f27d4989",
            "filename": "tests/models/dinov3_vit/test_image_processing_dinov3_vit_fast.py",
            "status": "added",
            "additions": 127,
            "deletions": 0,
            "changes": 127,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/tests%2Fmodels%2Fdinov3_vit%2Ftest_image_processing_dinov3_vit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/tests%2Fmodels%2Fdinov3_vit%2Ftest_image_processing_dinov3_vit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov3_vit%2Ftest_image_processing_dinov3_vit_fast.py?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0",
            "patch": "@@ -0,0 +1,127 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torchvision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+\n+\n+if is_torchvision_available():\n+    from transformers import DINOv3ViTImageProcessorFast\n+\n+\n+class DINOv3ViTImageProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=400,\n+        do_resize=True,\n+        size=None,\n+        do_center_crop=True,\n+        crop_size=None,\n+        do_normalize=True,\n+        image_mean=[0.48145466, 0.4578275, 0.40821073],\n+        image_std=[0.26862954, 0.26130258, 0.27577711],\n+        do_convert_rgb=True,\n+    ):\n+        super().__init__()\n+        size = size if size is not None else {\"shortest_edge\": 20}\n+        crop_size = crop_size if crop_size is not None else {\"height\": 18, \"width\": 18}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_center_crop = do_center_crop\n+        self.crop_size = crop_size\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_convert_rgb = do_convert_rgb\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"do_center_crop\": self.do_center_crop,\n+            \"crop_size\": self.crop_size,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_convert_rgb\": self.do_convert_rgb,\n+        }\n+\n+    def expected_output_image_shape(self, images):\n+        return self.num_channels, self.crop_size[\"height\"], self.crop_size[\"width\"]\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        return prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+\n+\n+@require_torch\n+@require_vision\n+class DINOv3ViTImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    image_processing_class = None\n+    fast_image_processing_class = DINOv3ViTImageProcessorFast if is_torchvision_available() else None\n+    test_slow_image_processor = False\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = DINOv3ViTImageProcessingTester(self)\n+\n+    @property\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    def test_image_processor_properties(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n+            self.assertTrue(hasattr(image_processing, \"center_crop\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n+\n+    def test_image_processor_from_dict_with_kwargs(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 20})\n+            self.assertEqual(image_processor.crop_size, {\"height\": 18, \"width\": 18})\n+\n+            image_processor = image_processing_class.from_dict(\n+                self.image_processor_dict, size={\"height\": 42, \"width\": 42}\n+            )\n+            self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})"
        },
        {
            "sha": "00e078739d88e537e8e12fd815141ebc9a6546aa",
            "filename": "tests/models/dinov3_vit/test_modeling_dinov3_vit.py",
            "status": "added",
            "additions": 278,
            "deletions": 0,
            "changes": 278,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/tests%2Fmodels%2Fdinov3_vit%2Ftest_modeling_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/tests%2Fmodels%2Fdinov3_vit%2Ftest_modeling_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov3_vit%2Ftest_modeling_dinov3_vit.py?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0",
            "patch": "@@ -0,0 +1,278 @@\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch DINOv3 model.\"\"\"\n+\n+import unittest\n+\n+from transformers import DINOv3ViTConfig\n+from transformers.testing_utils import require_torch, require_vision, slow, torch_device\n+from transformers.utils import cached_property, is_torch_available, is_vision_available\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+    from transformers import DINOv3ViTModel\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import AutoImageProcessor\n+\n+\n+class DINOv3ViTModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=13,\n+        image_size=30,\n+        patch_size=2,\n+        num_channels=3,\n+        is_training=False,\n+        use_labels=True,\n+        hidden_size=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        intermediate_size=37,\n+        hidden_act=\"gelu\",\n+        hidden_dropout_prob=0.1,\n+        attention_probs_dropout_prob=0.1,\n+        type_sequence_label_size=10,\n+        initializer_range=0.02,\n+        num_register_tokens=2,\n+        mask_ratio=0.5,\n+        scope=None,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.is_training = is_training\n+        self.use_labels = use_labels\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.hidden_act = hidden_act\n+        self.hidden_dropout_prob = hidden_dropout_prob\n+        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n+        self.type_sequence_label_size = type_sequence_label_size\n+        self.initializer_range = initializer_range\n+        self.num_register_tokens = num_register_tokens\n+        self.scope = scope\n+\n+        num_patches = (image_size // patch_size) ** 2\n+        self.seq_length = num_patches + 1 + self.num_register_tokens\n+        self.mask_ratio = mask_ratio\n+        self.num_masks = int(mask_ratio * self.seq_length)\n+        self.mask_length = num_patches\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+\n+        labels = None\n+        if self.use_labels:\n+            labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n+\n+        config = self.get_config()\n+\n+        return config, pixel_values, labels\n+\n+    def get_config(self):\n+        return DINOv3ViTConfig(\n+            image_size=self.image_size,\n+            patch_size=self.patch_size,\n+            num_channels=self.num_channels,\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            hidden_act=self.hidden_act,\n+            hidden_dropout_prob=self.hidden_dropout_prob,\n+            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n+            is_decoder=False,\n+            initializer_range=self.initializer_range,\n+            num_register_tokens=self.num_register_tokens,\n+        )\n+\n+    def create_and_check_model(self, config, pixel_values, labels):\n+        model = DINOv3ViTModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+        self.parent.assertEqual(\n+            result.last_hidden_state.shape,\n+            (self.batch_size, self.seq_length, self.hidden_size),\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        (\n+            config,\n+            pixel_values,\n+            labels,\n+        ) = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class Dinov3ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Here we also overwrite some of the tests of test_modeling_common.py, as Dinov3 does not use input_ids, inputs_embeds,\n+    attention_mask and seq_length.\n+    \"\"\"\n+\n+    all_model_classes = (DINOv3ViTModel,) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\n+            \"image-feature-extraction\": DINOv3ViTModel,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    fx_compatible = False\n+\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+    test_torch_exportable = True\n+\n+    def setUp(self):\n+        self.model_tester = DINOv3ViTModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=DINOv3ViTConfig, has_text_modality=False, hidden_size=37)\n+\n+    def test_initialization(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        configs_no_init = _config_zero_init(config)\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            for name, param in model.named_parameters():\n+                if param.requires_grad and \"register_tokens\" not in name:\n+                    # See PR #38607 (to avoid flakiness)\n+                    data = torch.flatten(param.data)\n+                    n_elements = torch.numel(data)\n+                    # skip 2.5% of elements on each side to avoid issues caused by `nn.init.trunc_normal_` described in\n+                    # https://github.com/huggingface/transformers/pull/27906#issuecomment-1846951332\n+                    n_elements_to_skip_on_each_side = int(n_elements * 0.025)\n+                    data_to_check = torch.sort(data).values\n+                    if n_elements_to_skip_on_each_side > 0:\n+                        data_to_check = data_to_check[n_elements_to_skip_on_each_side:-n_elements_to_skip_on_each_side]\n+                    self.assertIn(\n+                        ((data_to_check.mean() * 1e9).round() / 1e9).item(),\n+                        [0.0, 1.0],\n+                        msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                    )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    @unittest.skip(reason=\"Dinov3 does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    def test_model_get_set_embeddings(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            self.assertIsInstance(model.get_input_embeddings(), (nn.Module))\n+            x = model.get_output_embeddings()\n+            self.assertTrue(x is None or isinstance(x, nn.Linear))\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    @unittest.skip(reason=\"Dinov3 does not support feedforward chunking yet\")\n+    def test_feed_forward_chunking(self):\n+        pass\n+\n+    @slow\n+    def test_model_from_pretrained(self):\n+        model_name = \"facebook/dinov3-vits16-pretrain-lvd1689m\"\n+        model = DINOv3ViTModel.from_pretrained(model_name)\n+        self.assertIsNotNone(model)\n+\n+\n+# We will verify our results on an image of cute cats\n+def prepare_img():\n+    image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+    return image\n+\n+\n+@require_torch\n+@require_vision\n+class DINOv3ViTModelIntegrationTest(unittest.TestCase):\n+    @cached_property\n+    def default_image_processor(self):\n+        return (\n+            AutoImageProcessor.from_pretrained(\"facebook/dinov3-vits16-pretrain-lvd1689m\")\n+            if is_vision_available()\n+            else None\n+        )\n+\n+    @slow\n+    def test_inference_no_head(self):\n+        model = DINOv3ViTModel.from_pretrained(\"facebook/dinov3-vits16-pretrain-lvd1689m\").to(torch_device)\n+\n+        image_processor = self.default_image_processor\n+        image = prepare_img()\n+        inputs = image_processor(image, return_tensors=\"pt\").to(torch_device)\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+\n+        # verify the last hidden states\n+        # in DINOv3 with Registers, the seq length equals the number of patches + 1 + num_register_tokens (we add 1 for the [CLS] token)\n+        _, _, height, width = inputs[\"pixel_values\"].shape\n+        num_patches = (height // model.config.patch_size) * (width // model.config.patch_size)\n+        expected_seq_length = num_patches + 1 + model.config.num_register_tokens\n+        expected_shape = torch.Size((1, expected_seq_length, model.config.hidden_size))\n+        self.assertEqual(outputs.last_hidden_state.shape, expected_shape)\n+\n+        last_layer_cls_token = outputs.pooler_output\n+        expected_slice = torch.tensor([0.4637, -0.4160, 0.4086, -0.1265, -0.2865], device=torch_device)\n+        torch.testing.assert_close(last_layer_cls_token[0, :5], expected_slice, rtol=1e-4, atol=1e-4)\n+\n+        last_layer_patch_tokens = outputs.last_hidden_state[:, model.config.num_register_tokens + 1 :]\n+        expected_slice = torch.tensor([-0.0386, -0.2509, -0.0161, -0.4556, 0.5716], device=torch_device)\n+        torch.testing.assert_close(last_layer_patch_tokens[0, 0, :5], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "3904b850b6008d6187bb7fd18a1311762373a8d1",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0",
            "patch": "@@ -169,6 +169,8 @@\n     \"DetrConfig\",\n     \"DetrImageProcessor\",\n     \"DinatModel\",\n+    \"DINOv3ConvNextConfig\",\n+    \"DINOv3ViTConfig\",\n     \"DistilBertConfig\",\n     \"DistilBertTokenizerFast\",\n     \"DocumentQuestionAnsweringPipeline\","
        },
        {
            "sha": "cf4fd6c76237e9cb290e2d91d7629e344ce759e8",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f259bc83e518c281877cfc90efe61bf8a79bba0/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f259bc83e518c281877cfc90efe61bf8a79bba0/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=6f259bc83e518c281877cfc90efe61bf8a79bba0",
            "patch": "@@ -411,6 +411,8 @@\n         (\"data2vec-audio\", \"data2vec\"),\n         (\"data2vec-vision\", \"data2vec\"),\n         (\"donut-swin\", \"donut\"),\n+        (\"dinov3_convnext\", \"dinov3\"),\n+        (\"dinov3_vit\", \"dinov3\"),\n     ]\n )\n "
        }
    ],
    "stats": {
        "total": 3092,
        "additions": 3081,
        "deletions": 11
    }
}