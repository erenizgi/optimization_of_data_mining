{
    "author": "zucchini-nlp",
    "message": "Generation tests: don't rely on main input name (#34228)\n\n* don't rely on main input name\r\n\r\n* update",
    "sha": "ca541bd4f4d932f486a4116deba833b4ffaebd15",
    "files": [
        {
            "sha": "996d95eb80ff9b1311b1c85f7afba6c6f018efb9",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 43,
            "deletions": 26,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca541bd4f4d932f486a4116deba833b4ffaebd15/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca541bd4f4d932f486a4116deba833b4ffaebd15/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=ca541bd4f4d932f486a4116deba833b4ffaebd15",
            "patch": "@@ -410,15 +410,14 @@ def _contrastive_generate(\n     def test_greedy_generate(self):\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            main_input = inputs_dict[model_class.main_input_name]\n \n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._greedy_generate(model=model, inputs_dict=inputs_dict)\n \n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n+                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n \n     @pytest.mark.generate\n     def test_greedy_generate_dict_outputs(self):\n@@ -444,7 +443,9 @@ def test_greedy_generate_dict_outputs(self):\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, GreedySearchEncoderDecoderOutput)\n             else:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n+                self.assertTrue(\n+                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n+                )\n                 self.assertIsInstance(output_generate, GenerateDecoderOnlyOutput)\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, GreedySearchDecoderOnlyOutput)\n@@ -478,23 +479,24 @@ def test_greedy_generate_dict_outputs_use_cache(self):\n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n+                self.assertTrue(\n+                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n+                )\n \n             self._check_outputs(output_generate, main_input, model.config, use_cache=True)\n \n     @pytest.mark.generate\n     def test_sample_generate(self):\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            main_input = inputs_dict[model_class.main_input_name]\n \n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._sample_generate(model=model, inputs_dict=inputs_dict, num_return_sequences=1)\n \n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n+                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n \n     @pytest.mark.generate\n     def test_sample_generate_dict_output(self):\n@@ -521,7 +523,9 @@ def test_sample_generate_dict_output(self):\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, SampleEncoderDecoderOutput)\n             else:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n+                self.assertTrue(\n+                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n+                )\n                 self.assertIsInstance(output_generate, GenerateDecoderOnlyOutput)\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, SampleDecoderOnlyOutput)\n@@ -532,7 +536,6 @@ def test_sample_generate_dict_output(self):\n     def test_beam_search_generate(self):\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            main_input = inputs_dict[model_class.main_input_name]\n \n             model = model_class(config).to(torch_device).eval()\n \n@@ -542,7 +545,7 @@ def test_beam_search_generate(self):\n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n+                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n \n     @pytest.mark.generate\n     def test_beam_search_generate_dict_output(self):\n@@ -569,7 +572,9 @@ def test_beam_search_generate_dict_output(self):\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, BeamSearchEncoderDecoderOutput)\n             else:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n+                self.assertTrue(\n+                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n+                )\n                 self.assertIsInstance(output_generate, GenerateBeamDecoderOnlyOutput)\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, BeamSearchDecoderOnlyOutput)\n@@ -609,7 +614,9 @@ def test_beam_search_generate_dict_outputs_use_cache(self):\n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n+                self.assertTrue(\n+                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n+                )\n \n             self._check_outputs(\n                 output_generate,\n@@ -647,7 +654,6 @@ def test_model_parallel_beam_search(self):\n     def test_beam_sample_generate(self):\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            main_input = inputs_dict[model_class.main_input_name]\n \n             model = model_class(config).to(torch_device).eval()\n             beam_kwargs = self._get_beam_kwargs()\n@@ -660,7 +666,7 @@ def test_beam_sample_generate(self):\n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n+                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n \n             # for VLMs inputs embeds won't match input ids unless images are encoded and merged with ids properly\n             # no quick fix available, since obtaining image embeddings step is very model-specific\n@@ -712,7 +718,9 @@ def test_beam_sample_generate_dict_output(self):\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, BeamSampleEncoderDecoderOutput)\n             else:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n+                self.assertTrue(\n+                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n+                )\n                 self.assertIsInstance(output_generate, GenerateBeamDecoderOnlyOutput)\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, BeamSampleDecoderOnlyOutput)\n@@ -746,7 +754,6 @@ def test_generate_without_input_ids(self):\n     def test_group_beam_search_generate(self):\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            main_input = inputs_dict[model_class.main_input_name]\n \n             model = model_class(config).to(torch_device).eval()\n             # check `generate()` and `group_beam_search()` are equal\n@@ -759,7 +766,7 @@ def test_group_beam_search_generate(self):\n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n+                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n \n             # check `group_beam_search` for higher than 1 `num_return_sequences`\n             num_return_sequences = 2\n@@ -772,7 +779,7 @@ def test_group_beam_search_generate(self):\n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n+                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n \n     @pytest.mark.generate\n     def test_group_beam_search_generate_dict_output(self):\n@@ -799,7 +806,9 @@ def test_group_beam_search_generate_dict_output(self):\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, BeamSearchEncoderDecoderOutput)\n             else:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n+                self.assertTrue(\n+                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n+                )\n                 self.assertIsInstance(output_generate, GenerateBeamDecoderOnlyOutput)\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, BeamSearchDecoderOnlyOutput)\n@@ -814,7 +823,6 @@ def test_group_beam_search_generate_dict_output(self):\n     def test_constrained_beam_search_generate(self):\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            main_input = inputs_dict[model_class.main_input_name]\n \n             model = model_class(config).to(torch_device).eval()\n \n@@ -838,7 +846,7 @@ def test_constrained_beam_search_generate(self):\n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n+                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n \n             for generation_output in output_generate:\n                 self._check_sequence_inside_sequence(force_tokens, generation_output)\n@@ -862,7 +870,7 @@ def test_constrained_beam_search_generate(self):\n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n+                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n \n             for generation_output in output_generate:\n                 self._check_sequence_inside_sequence(force_tokens, generation_output)\n@@ -903,7 +911,9 @@ def test_constrained_beam_search_generate_dict_output(self):\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, BeamSearchEncoderDecoderOutput)\n             else:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n+                self.assertTrue(\n+                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n+                )\n                 self.assertIsInstance(output_generate, GenerateBeamDecoderOnlyOutput)\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, BeamSearchDecoderOnlyOutput)\n@@ -923,7 +933,6 @@ def test_contrastive_generate(self):\n                 self.skipTest(reason=\"Won't fix: old model with different cache format\")\n \n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            main_input = inputs_dict[model_class.main_input_name]\n \n             # NOTE: contrastive search only works with cache on at the moment.\n             if not hasattr(config, \"use_cache\"):\n@@ -940,7 +949,7 @@ def test_contrastive_generate(self):\n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n+                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n \n     @pytest.mark.generate\n     def test_contrastive_generate_dict_outputs_use_cache(self):\n@@ -975,7 +984,9 @@ def test_contrastive_generate_dict_outputs_use_cache(self):\n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n             else:\n-                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + main_input.shape[-1])\n+                self.assertTrue(\n+                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n+                )\n \n             self._check_outputs(output_generate, main_input, model.config, use_cache=True)\n \n@@ -2035,8 +2046,14 @@ def test_inherits_generation_mixin(self):\n             self.assertTrue(\"GenerationMixin\" in str(model_class.__bases__))\n \n     def _check_outputs(self, output, main_input, config, use_cache=False, num_return_sequences=1):\n+        # we can be sure what is batch size from main input but seq length depends on model type and whether input is text/audio/image\n+        # so we infer actual text seq length from model_tester, same was as it is done in `test_modeling_common.py` tests`\n         batch_size = main_input.shape[0]\n-        seq_length = main_input.shape[-1]\n+\n+        seq_length = getattr(self.model_tester, \"seq_length\", None)\n+        seq_length = getattr(self.model_tester, \"encoder_seq_length\", seq_length)\n+        seq_length = getattr(self.model_tester, \"text_seq_length\", seq_length)\n+\n         config = config.text_config if hasattr(config, \"text_config\") else config\n         num_sequences_in_output = batch_size * num_return_sequences\n "
        },
        {
            "sha": "25b28477a145ec460d7653713d5d3f77dea29b64",
            "filename": "tests/models/reformer/test_modeling_reformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca541bd4f4d932f486a4116deba833b4ffaebd15/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca541bd4f4d932f486a4116deba833b4ffaebd15/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py?ref=ca541bd4f4d932f486a4116deba833b4ffaebd15",
            "patch": "@@ -53,6 +53,7 @@ def __init__(\n         parent,\n         batch_size=13,\n         seq_length=32,\n+        text_seq_length=None,\n         is_training=True,\n         is_decoder=True,\n         use_input_mask=True,\n@@ -128,6 +129,7 @@ def __init__(\n         self.attn_layers = attn_layers\n         self.pad_token_id = pad_token_id\n         self.hash_seed = hash_seed\n+        self.text_seq_length = text_seq_length or seq_length\n \n         attn_chunk_length = local_attn_chunk_length if local_attn_chunk_length is not None else lsh_attn_chunk_length\n         num_chunks_after = local_num_chunks_after if local_num_chunks_after is not None else lsh_num_chunks_after\n@@ -608,7 +610,7 @@ class ReformerLocalAttnModelTest(ReformerTesterMixin, GenerationTesterMixin, Mod\n     test_sequence_classification_problem_types = True\n \n     def setUp(self):\n-        self.model_tester = ReformerModelTester(self)\n+        self.model_tester = ReformerModelTester(self, text_seq_length=16)\n         self.config_tester = ConfigTester(self, config_class=ReformerConfig, hidden_size=37)\n \n     @slow\n@@ -689,7 +691,7 @@ def prepare_config_and_inputs_for_generate(self, *args, **kwargs):\n         # decreasing the seq_length in tester causes errors for \"training_tests\", those need exactly max seq length\n         # NOTE: seq_length has to be multiple of 4, otherwise it fails for other tests\n         original_sequence_length = self.model_tester.seq_length\n-        self.model_tester.seq_length = 16\n+        self.model_tester.seq_length = self.model_tester.text_seq_length\n         test_inputs = super().prepare_config_and_inputs_for_generate(*args, **kwargs)\n         self.model_tester.seq_length = original_sequence_length\n         return test_inputs"
        },
        {
            "sha": "253cda7e49cb142310f7a5cfbce7b40e2cb83cf6",
            "filename": "tests/models/speech_to_text/test_modeling_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca541bd4f4d932f486a4116deba833b4ffaebd15/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca541bd4f4d932f486a4116deba833b4ffaebd15/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py?ref=ca541bd4f4d932f486a4116deba833b4ffaebd15",
            "patch": "@@ -618,14 +618,6 @@ def test_resize_embeddings_untied(self):\n     def test_generate_without_input_ids(self):\n         pass\n \n-    def _check_outputs(self, output, main_input, config, use_cache=False, num_return_sequences=1):\n-        # In this model, the index of `batch_size` and `sequence_length`` in `main_input` is different: they are the\n-        # first two dimensions of the tensor.\n-        main_input = main_input[:, :, 0]\n-        super()._check_outputs(\n-            output, main_input, config, use_cache=use_cache, num_return_sequences=num_return_sequences\n-        )\n-\n     def _create_and_check_torchscript(self, config, inputs_dict):\n         if not self.test_torchscript:\n             self.skipTest(reason=\"test_torchscript is set to False\")"
        }
    ],
    "stats": {
        "total": 83,
        "additions": 47,
        "deletions": 36
    }
}