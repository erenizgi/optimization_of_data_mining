{
    "author": "faaany",
    "message": "[tests] enable autoawq tests on XPU  (#36327)\n\nadd autoawq\n\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "c3700b0eee573276ac65bd28c85171a768cfc3b1",
    "files": [
        {
            "sha": "5238c29a9c2c067c029e101a11251a93aab0605d",
            "filename": "tests/quantization/autoawq/test_awq.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/c3700b0eee573276ac65bd28c85171a768cfc3b1/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c3700b0eee573276ac65bd28c85171a768cfc3b1/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fautoawq%2Ftest_awq.py?ref=c3700b0eee573276ac65bd28c85171a768cfc3b1",
            "patch": "@@ -19,9 +19,11 @@\n \n from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AwqConfig, OPTForCausalLM\n from transformers.testing_utils import (\n+    backend_empty_cache,\n     require_accelerate,\n     require_auto_awq,\n     require_intel_extension_for_pytorch,\n+    require_torch_accelerator,\n     require_torch_gpu,\n     require_torch_multi_gpu,\n     slow,\n@@ -37,8 +39,9 @@\n     from accelerate import init_empty_weights\n \n \n-@require_torch_gpu\n+@require_torch_accelerator\n class AwqConfigTest(unittest.TestCase):\n+    @require_torch_gpu\n     def test_wrong_backend(self):\n         \"\"\"\n         Simple test that checks if a user passes a wrong backend an error is raised\n@@ -90,7 +93,7 @@ def test_from_dict(self):\n \n \n @slow\n-@require_torch_gpu\n+@require_torch_accelerator\n @require_auto_awq\n @require_accelerate\n class AwqTest(unittest.TestCase):\n@@ -107,7 +110,7 @@ class AwqTest(unittest.TestCase):\n         \"Hello my name is Katie and I am a 20 year old student from the UK. I am currently studying for a degree in English Literature and History at the University of York. I am a very out\",\n         \"Hello my name is Katie and I am a 20 year old student from the UK. I am currently studying for a degree in English Literature and History at the University of York. I am a very creative\",\n     ]\n-    device_map = \"cuda\"\n+    device_map = torch_device\n \n     # called only once for all test in this class\n     @classmethod\n@@ -120,7 +123,7 @@ def setUpClass(cls):\n \n     def tearDown(self):\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n         gc.collect()\n \n     def test_quantized_model_conversion(self):\n@@ -475,7 +478,7 @@ def test_generation_mixtral_fused(self):\n \n \n @slow\n-@require_torch_gpu\n+@require_torch_accelerator\n @require_auto_awq\n @require_accelerate\n class AwqScaleTest(unittest.TestCase):\n@@ -488,7 +491,7 @@ def test_load_quantized_model(self):\n         Simple test that checks if the scales have been replaced in the quantized model\n         \"\"\"\n         quantized_model = AutoModelForCausalLM.from_pretrained(\n-            \"TechxGenus/starcoder2-3b-AWQ\", torch_dtype=torch.float16, device_map=\"cuda\"\n+            \"TechxGenus/starcoder2-3b-AWQ\", torch_dtype=torch.float16, device_map=torch_device\n         )\n         self.assertTrue(isinstance(quantized_model.model.layers[0].mlp.act, ScaledActivation))\n "
        }
    ],
    "stats": {
        "total": 15,
        "additions": 9,
        "deletions": 6
    }
}