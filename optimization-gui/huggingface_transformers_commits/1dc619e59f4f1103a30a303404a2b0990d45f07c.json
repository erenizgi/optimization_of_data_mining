{
    "author": "vasqu",
    "message": "[`FlexAttn`] Fix models with unique characteristics (#38433)\n\n* fix\n\n* style\n\n* check\n\n* check 2\n\n* add deepseek workaround",
    "sha": "1dc619e59f4f1103a30a303404a2b0990d45f07c",
    "files": [
        {
            "sha": "7677b909c3beec05ce904d73baa5640b9abe608f",
            "filename": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/1dc619e59f4f1103a30a303404a2b0990d45f07c/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1dc619e59f4f1103a30a303404a2b0990d45f07c/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py?ref=1dc619e59f4f1103a30a303404a2b0990d45f07c",
            "patch": "@@ -24,6 +24,7 @@\n     require_read_token,\n     require_torch,\n     require_torch_accelerator,\n+    require_torch_gpu,\n     require_torch_large_accelerator,\n     require_torch_sdpa,\n     slow,\n@@ -494,6 +495,35 @@ def test_eager_matches_sdpa_generate(self):\n                     msg=f\"\\n{tokenizer.batch_decode(res_eager)} \\nvs\\n{tokenizer.batch_decode(res_sdpa)}\",\n                 )\n \n+    @require_torch_gpu\n+    def test_flex_attention_with_grads(self):\n+        \"\"\"\n+        Overwriting as the namings/functionality on the attention part are different; for now it's more of a unique model.\n+        Original issue is also due to dimensionalities, here specifically due to dims not being a multiple of 2.\n+        \"\"\"\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            config._attn_implementation = \"flex_attention\"\n+\n+            # Disable dropout\n+            config.attention_dropout = 0.0\n+\n+            # Deepseek 3 specific - manipulate nope and adjust calculated total head dim\n+            config.qk_nope_head_dim = 16\n+            config.qk_head_dim = config.qk_nope_head_dim + config.qk_rope_head_dim\n+\n+            model = model_class(config).to(device=torch_device)\n+            self.assertTrue(model.config._attn_implementation == \"flex_attention\")\n+\n+            # Elaborate workaround for encoder-decoder models as some do not specify their main input\n+            dummy_inputs = {model.main_input_name: inputs_dict[model.main_input_name].to(torch_device)}\n+            if config.is_encoder_decoder:\n+                dummy_inputs[\"decoder_input_ids\"] = inputs_dict[\"decoder_input_ids\"].to(torch_device)\n+                dummy_inputs[\"decoder_attention_mask\"] = inputs_dict[\"decoder_attention_mask\"].to(torch_device)\n+\n+            # If this does not raise an error, the test passes (see https://github.com/huggingface/transformers/pull/35605)\n+            _ = model(**dummy_inputs)\n+\n \n @require_torch_accelerator\n class DeepseekV3IntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "4c12ac4732344c3ca84c5754cac6b47fcf351893",
            "filename": "tests/models/zamba2/test_modeling_zamba2.py",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/1dc619e59f4f1103a30a303404a2b0990d45f07c/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1dc619e59f4f1103a30a303404a2b0990d45f07c/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py?ref=1dc619e59f4f1103a30a303404a2b0990d45f07c",
            "patch": "@@ -578,6 +578,28 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n     def test_new_cache_format(self, num_beams, do_sample):\n         pass\n \n+    @require_torch_gpu\n+    def test_flex_attention_with_grads(self):\n+        \"\"\"\n+        Overwriting as the base hidden size is big enough for compile.\n+        Manipulation of dims causes issues due to other constraints not being satisfied anymore.\n+        \"\"\"\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            config._attn_implementation = \"flex_attention\"\n+\n+            model = model_class(config).to(device=torch_device)\n+            self.assertTrue(model.config._attn_implementation == \"flex_attention\")\n+\n+            # Elaborate workaround for encoder-decoder models as some do not specify their main input\n+            dummy_inputs = {model.main_input_name: inputs_dict[model.main_input_name].to(torch_device)}\n+            if config.is_encoder_decoder:\n+                dummy_inputs[\"decoder_input_ids\"] = inputs_dict[\"decoder_input_ids\"].to(torch_device)\n+                dummy_inputs[\"decoder_attention_mask\"] = inputs_dict[\"decoder_attention_mask\"].to(torch_device)\n+\n+            # If this does not raise an error, the test passes (see https://github.com/huggingface/transformers/pull/35605)\n+            _ = model(**dummy_inputs)\n+\n \n @require_torch\n class Zamba2ModelIntegrationTest(unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 52,
        "additions": 52,
        "deletions": 0
    }
}