{
    "author": "fxmarty-amd",
    "message": "Support loading Quark quantized models in Transformers (#36372)\n\n* add quark quantizer\n\n* add quark doc\n\n* clean up doc\n\n* fix tests\n\n* make style\n\n* more style fixes\n\n* cleanup imports\n\n* cleaning\n\n* precise install\n\n* Update docs/source/en/quantization/quark.md\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* Update tests/quantization/quark_integration/test_quark.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* Update src/transformers/utils/quantization_config.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* remove import guard as suggested\n\n* update copyright headers\n\n* add quark to transformers-quantization-latest-gpu Dockerfile\n\n* make tests pass on transformers main + quark==0.7\n\n* add missing F8_E4M3 and F8_E5M2 keys from str_to_torch_dtype\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\nCo-authored-by: Bowen Bao <bowenbao@amd.com>\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "1a374799cedcf7a7df14256226a7576c324a42fa",
    "files": [
        {
            "sha": "33d8b10b02ee790b5613d68b4f83359087ecc099",
            "filename": "docker/transformers-quantization-latest-gpu/Dockerfile",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a374799cedcf7a7df14256226a7576c324a42fa/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a374799cedcf7a7df14256226a7576c324a42fa/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile?ref=1a374799cedcf7a7df14256226a7576c324a42fa",
            "patch": "@@ -79,6 +79,9 @@ RUN git clone https://github.com/NetEase-FuXi/EETQ.git && cd EETQ/ && git submod\n # Add compressed-tensors for quantization testing\n RUN python3 -m pip install --no-cache-dir compressed-tensors\n \n+# Add AMD Quark for quantization testing\n+RUN python3 -m pip install --no-cache-dir amd-quark\n+\n # Add transformers in editable mode\n RUN python3 -m pip install --no-cache-dir -e ./transformers[dev-torch]\n "
        },
        {
            "sha": "5a5f344454322be5bfc21a051aa6e7cb1530cc17",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a374799cedcf7a7df14256226a7576c324a42fa/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a374799cedcf7a7df14256226a7576c324a42fa/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=1a374799cedcf7a7df14256226a7576c324a42fa",
            "patch": "@@ -187,6 +187,8 @@\n     title: Optimum\n   - local: quantization/quanto\n     title: Quanto\n+  - local: quantization/quark\n+    title: Quark\n   - local: quantization/torchao\n     title: torchao\n   - local: quantization/spqr"
        },
        {
            "sha": "fb42e886bace79c94d3964ecdd2c45d0e9ce8e70",
            "filename": "docs/source/en/main_classes/quantization.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a374799cedcf7a7df14256226a7576c324a42fa/docs%2Fsource%2Fen%2Fmain_classes%2Fquantization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a374799cedcf7a7df14256226a7576c324a42fa/docs%2Fsource%2Fen%2Fmain_classes%2Fquantization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fquantization.md?ref=1a374799cedcf7a7df14256226a7576c324a42fa",
            "patch": "@@ -88,3 +88,7 @@ Learn how to quantize models in the [Quantization](../quantization) guide.\n ## FineGrainedFP8Config\n \n [[autodoc]] FineGrainedFP8Config\n+\n+## QuarkConfig\n+\n+[[autodoc]] QuarkConfig"
        },
        {
            "sha": "ac6fdb4a3dabeab86115cfe89e7d048681e30f4c",
            "filename": "docs/source/en/quantization/overview.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a374799cedcf7a7df14256226a7576c324a42fa/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a374799cedcf7a7df14256226a7576c324a42fa/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Foverview.md?ref=1a374799cedcf7a7df14256226a7576c324a42fa",
            "patch": "@@ -40,6 +40,7 @@ Use the Space below to help you pick a quantization method depending on your har\n | [VPTQ](./vptq)                             | 游댮                   | 游댮              |     游릭     | 游리        | 游댮                                 | 游댮              | 游릭              | 1/8         | 游댮               | 游릭                          | 游릭                      | https://github.com/microsoft/VPTQ            |\n | [FINEGRAINED_FP8](./finegrained_fp8)                 | 游릭                   | 游댮              | 游릭        | 游댮        | 游댮                                 | 游댮              | 游댮              | 8             | 游댮               | 游릭                          | 游릭                      |        |\n | [SpQR](./spqr)                          | 游댮                       |  游댮   | 游릭        | 游댮              |    游댮    | 游댮         |         游릭              | 3              |              游댮                     | 游릭           | 游릭                      | https://github.com/Vahe1994/SpQR/       |\n+| [Quark](./quark.md)                           | 游댮                       | 游릭 | 游릭      | 游릭      | 游릭                   | 游릭       | ?               | 2/4/6/8/9/16 | 游댮                | 游댮                               | 游릭                       | https://quark.docs.amd.com/latest/                      |\n \n ## Resources\n \n@@ -55,4 +56,4 @@ If you are looking for a user-friendly quantization experience, you can use the\n * [Bitsandbytes Space](https://huggingface.co/spaces/bnb-community/bnb-my-repo)\n * [GGUF Space](https://huggingface.co/spaces/ggml-org/gguf-my-repo)\n * [MLX Space](https://huggingface.co/spaces/mlx-community/mlx-my-repo)\n-* [AuoQuant Notebook](https://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4?usp=sharing#scrollTo=ZC9Nsr9u5WhN)\n\\ No newline at end of file\n+* [AuoQuant Notebook](https://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4?usp=sharing#scrollTo=ZC9Nsr9u5WhN)"
        },
        {
            "sha": "8d60affbc28098f943d75c94fc4171b86ead3716",
            "filename": "docs/source/en/quantization/quark.md",
            "status": "added",
            "additions": 84,
            "deletions": 0,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a374799cedcf7a7df14256226a7576c324a42fa/docs%2Fsource%2Fen%2Fquantization%2Fquark.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a374799cedcf7a7df14256226a7576c324a42fa/docs%2Fsource%2Fen%2Fquantization%2Fquark.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fquark.md?ref=1a374799cedcf7a7df14256226a7576c324a42fa",
            "patch": "@@ -0,0 +1,84 @@\n+<!--Copyright 2025 Advanced Micro Devices, Inc. and The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+丘멆잺 Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Quark\n+\n+[Quark](https://quark.docs.amd.com/latest/) is a deep learning quantization toolkit designed to be agnostic to specific data types, algorithms, and hardware. Different pre-processing strategies, algorithms and data-types can be combined in Quark.\n+\n+The PyTorch support integrated through 游뱅 Transformers primarily targets AMD CPUs and GPUs, and is primarily meant to be used for evaluation purposes. For example, it is possible to use [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) with 游뱅 Transformers backend and evaluate a wide range of models quantized through Quark seamlessly.\n+\n+Users interested in Quark can refer to its [documentation](https://quark.docs.amd.com/latest/) to get started quantizing models and using them in supported open-source libraries!\n+\n+Although Quark has its own checkpoint / [configuration format](https://huggingface.co/amd/Llama-3.1-8B-Instruct-FP8-KV-Quark-test/blob/main/config.json#L26), the library also supports producing models with a serialization layout compliant with other quantization/runtime implementations ([AutoAWQ](https://huggingface.co/docs/transformers/quantization/awq), [native fp8 in 游뱅 Transformers](https://huggingface.co/docs/transformers/quantization/finegrained_fp8)).\n+\n+To be able to load Quark quantized models in Transformers, the library first needs to be installed:\n+\n+```bash\n+pip install amd-quark\n+```\n+\n+## Support matrix\n+\n+Models quantized through Quark support a large range of features, that can be combined together. All quantized models independently of their configuration can seamlessly be reloaded through `PretrainedModel.from_pretrained`.\n+\n+The table below shows a few features supported by Quark:\n+\n+| **Feature**                     | **Supported subset in Quark**                                                                             |   |\n+|---------------------------------|-----------------------------------------------------------------------------------------------------------|---|\n+| Data types                      | int8, int4, int2, bfloat16, float16, fp8_e5m2, fp8_e4m3, fp6_e3m2, fp6_e2m3, fp4, OCP MX, MX6, MX9, bfp16 |   |\n+| Pre-quantization transformation | SmoothQuant, QuaRot, SpinQuant, AWQ                                                                       |   |\n+| Quantization algorithm          | GPTQ                                                                                                      |   |\n+| Supported operators             | ``nn.Linear``, ``nn.Conv2d``, ``nn.ConvTranspose2d``, ``nn.Embedding``, ``nn.EmbeddingBag``               |   |\n+| Granularity                     | per-tensor, per-channel, per-block, per-layer, per-layer type                                             |   |\n+| KV cache                        | fp8                                                                                                       |   |\n+| Activation calibration          | MinMax / Percentile / MSE                                                                                 |   |\n+| Quantization strategy           | weight-only, static, dynamic, with or without output quantization                                         |   |\n+\n+## Models on Hugging Face Hub\n+\n+Public models using Quark native serialization can be found at https://huggingface.co/models?other=quark.\n+\n+Although Quark also supports [models using `quant_method=\"fp8\"`](https://huggingface.co/models?other=fp8) and [models using `quant_method=\"awq\"`](https://huggingface.co/models?other=awq), Transformers loads these models rather through [AutoAWQ](https://huggingface.co/docs/transformers/quantization/awq) or uses the [native fp8 support in 游뱅 Transformers](https://huggingface.co/docs/transformers/quantization/finegrained_fp8).\n+\n+## Using Quark models in Transformers\n+\n+Here is an example of how one can load a Quark model in Transformers:\n+\n+```python\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+model_id = \"EmbeddedLLM/Llama-3.1-8B-Instruct-w_fp8_per_channel_sym\"\n+model = AutoModelForCausalLM.from_pretrained(model_id)\n+model = model.to(\"cuda\")\n+\n+print(model.model.layers[0].self_attn.q_proj)\n+# QParamsLinear(\n+#   (weight_quantizer): ScaledRealQuantizer()\n+#   (input_quantizer): ScaledRealQuantizer()\n+#   (output_quantizer): ScaledRealQuantizer()\n+# )\n+\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+inp = tokenizer(\"Where is a good place to cycle around Tokyo?\", return_tensors=\"pt\")\n+inp = inp.to(\"cuda\")\n+\n+res = model.generate(**inp, min_new_tokens=50, max_new_tokens=100)\n+\n+print(tokenizer.batch_decode(res)[0])\n+# <|begin_of_text|>Where is a good place to cycle around Tokyo? There are several places in Tokyo that are suitable for cycling, depending on your skill level and interests. Here are a few suggestions:\n+# 1. Yoyogi Park: This park is a popular spot for cycling and has a wide, flat path that's perfect for beginners. You can also visit the Meiji Shrine, a famous Shinto shrine located in the park.\n+# 2. Imperial Palace East Garden: This beautiful garden has a large, flat path that's perfect for cycling. You can also visit the\n+```\n\\ No newline at end of file"
        },
        {
            "sha": "fa3f7ac29aba6026bbf50ade44c842ec00e90138",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a374799cedcf7a7df14256226a7576c324a42fa/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a374799cedcf7a7df14256226a7576c324a42fa/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=1a374799cedcf7a7df14256226a7576c324a42fa",
            "patch": "@@ -1046,6 +1046,7 @@\n         \"HiggsConfig\",\n         \"HqqConfig\",\n         \"QuantoConfig\",\n+        \"QuarkConfig\",\n         \"SpQRConfig\",\n         \"TorchAoConfig\",\n         \"VptqConfig\",\n@@ -6287,6 +6288,7 @@\n         HiggsConfig,\n         HqqConfig,\n         QuantoConfig,\n+        QuarkConfig,\n         SpQRConfig,\n         TorchAoConfig,\n         VptqConfig,"
        },
        {
            "sha": "d916e6aaad65c22a8d3d60e7d4d81d642bf313f4",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a374799cedcf7a7df14256226a7576c324a42fa/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a374799cedcf7a7df14256226a7576c324a42fa/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=1a374799cedcf7a7df14256226a7576c324a42fa",
            "patch": "@@ -536,6 +536,10 @@ def load_sharded_checkpoint(model, folder, strict=True, prefer_safe=True):\n     str_to_torch_dtype[\"U32\"] = torch.uint32\n     str_to_torch_dtype[\"U64\"] = torch.uint64\n \n+if is_torch_greater_or_equal(\"2.1.0\"):\n+    str_to_torch_dtype[\"F8_E4M3\"] = torch.float8_e4m3fn\n+    str_to_torch_dtype[\"F8_E5M2\"] = torch.float8_e5m2\n+\n \n def load_state_dict(\n     checkpoint_file: Union[str, os.PathLike],\n@@ -3675,6 +3679,10 @@ def to(self, *args, **kwargs):\n \n         if getattr(self, \"quantization_method\", None) == QuantizationMethod.HQQ:\n             raise ValueError(\"`.to` is not supported for HQQ-quantized models.\")\n+\n+        if dtype_present_in_args and getattr(self, \"quantization_method\", None) == QuantizationMethod.QUARK:\n+            raise ValueError(\"Casting a Quark quantized model to a new `dtype` is not supported.\")\n+\n         # Checks if the model has been loaded in 4-bit or 8-bit with BNB\n         if getattr(self, \"quantization_method\", None) == QuantizationMethod.BITS_AND_BYTES:\n             if dtype_present_in_args:"
        },
        {
            "sha": "9d24b3539530b5d1ae03972de776f1d610c703d9",
            "filename": "src/transformers/quantizers/auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a374799cedcf7a7df14256226a7576c324a42fa/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a374799cedcf7a7df14256226a7576c324a42fa/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fauto.py?ref=1a374799cedcf7a7df14256226a7576c324a42fa",
            "patch": "@@ -1,4 +1,5 @@\n # Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+# Modifications Copyright (C) 2025, Advanced Micro Devices, Inc. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -31,6 +32,7 @@\n     QuantizationConfigMixin,\n     QuantizationMethod,\n     QuantoConfig,\n+    QuarkConfig,\n     SpQRConfig,\n     TorchAoConfig,\n     VptqConfig,\n@@ -49,6 +51,7 @@\n from .quantizer_higgs import HiggsHfQuantizer\n from .quantizer_hqq import HqqHfQuantizer\n from .quantizer_quanto import QuantoHfQuantizer\n+from .quantizer_quark import QuarkHfQuantizer\n from .quantizer_spqr import SpQRHfQuantizer\n from .quantizer_torchao import TorchAoHfQuantizer\n from .quantizer_vptq import VptqHfQuantizer\n@@ -61,6 +64,7 @@\n     \"gptq\": GptqHfQuantizer,\n     \"aqlm\": AqlmHfQuantizer,\n     \"quanto\": QuantoHfQuantizer,\n+    \"quark\": QuarkHfQuantizer,\n     \"eetq\": EetqHfQuantizer,\n     \"higgs\": HiggsHfQuantizer,\n     \"hqq\": HqqHfQuantizer,\n@@ -81,6 +85,7 @@\n     \"gptq\": GPTQConfig,\n     \"aqlm\": AqlmConfig,\n     \"quanto\": QuantoConfig,\n+    \"quark\": QuarkConfig,\n     \"hqq\": HqqConfig,\n     \"compressed-tensors\": CompressedTensorsConfig,\n     \"fbgemm_fp8\": FbgemmFp8Config,"
        },
        {
            "sha": "374360b1cb8f20724d74fe3d59cb78b0b47b2912",
            "filename": "src/transformers/quantizers/quantizer_quark.py",
            "status": "added",
            "additions": 113,
            "deletions": 0,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a374799cedcf7a7df14256226a7576c324a42fa/src%2Ftransformers%2Fquantizers%2Fquantizer_quark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a374799cedcf7a7df14256226a7576c324a42fa/src%2Ftransformers%2Fquantizers%2Fquantizer_quark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_quark.py?ref=1a374799cedcf7a7df14256226a7576c324a42fa",
            "patch": "@@ -0,0 +1,113 @@\n+# coding=utf-8\n+# Copyright 2025 Advanced Micro Devices, Inc. and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import TYPE_CHECKING, Any, Dict\n+\n+from ..file_utils import is_torch_available\n+from .base import HfQuantizer\n+\n+\n+if TYPE_CHECKING:\n+    from ..modeling_utils import PreTrainedModel\n+\n+    if is_torch_available():\n+        import torch\n+\n+from ..utils import is_accelerate_available, is_quark_available, logging\n+\n+\n+if is_accelerate_available():\n+    from accelerate.utils import set_module_tensor_to_device\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+CHECKPOINT_KEYS = {\n+    \"weight_scale\": \"weight_quantizer.scale\",\n+    \"bias_scale\": \"bias_quantizer.scale\",\n+    \"input_scale\": \"input_quantizer.scale\",\n+    \"output_scale\": \"output_quantizer.scale\",\n+    \"weight_zero_point\": \"weight_quantizer.zero_point\",\n+    \"bias_zero_point\": \"bias_quantizer.zero_point\",\n+    \"input_zero_point\": \"input_quantizer.zero_point\",\n+    \"output_zero_point\": \"output_quantizer.zero_point\",\n+}\n+\n+\n+class QuarkHfQuantizer(HfQuantizer):\n+    \"\"\"\n+    Quark quantizer (https://quark.docs.amd.com/latest/).\n+    \"\"\"\n+\n+    requires_calibration = True  # On-the-fly quantization with quark is not supported for now.\n+    required_packages = [\"quark\"]\n+\n+    # Checkpoints are expected to be already quantized when loading a quark model. However, as some keys from\n+    # the checkpoint might mismatch the model parameters keys, we use the `create_quantized_param` method\n+    # to load the checkpoints, remapping the keys.\n+    requires_parameters_quantization = True\n+\n+    def __init__(self, quantization_config, **kwargs):\n+        super().__init__(quantization_config, **kwargs)\n+\n+        self.json_export_config = quantization_config.json_export_config\n+\n+    def validate_environment(self, *args, **kwargs):\n+        if not is_quark_available():\n+            raise ImportError(\n+                \"Loading a Quark quantized model requires the `quark` library but it was not found in the environment. Please refer to https://quark.docs.amd.com/latest/install.html.\"\n+            )\n+\n+    def _process_model_before_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n+        from quark.torch.export.api import _map_to_quark\n+\n+        _map_to_quark(\n+            model,\n+            self.quantization_config.quant_config,\n+            pack_method=self.json_export_config.pack_method,\n+            custom_mode=self.quantization_config.custom_mode,\n+        )\n+\n+        return model\n+\n+    def check_quantized_param(\n+        self,\n+        model: \"PreTrainedModel\",\n+        param_value: \"torch.Tensor\",\n+        param_name: str,\n+        state_dict: Dict[str, Any],\n+        **kwargs,\n+    ) -> bool:\n+        return True\n+\n+    def create_quantized_param(\n+        self, model, param, param_name, param_device, state_dict, unexpected_keys\n+    ) -> \"torch.nn.Parameter\":\n+        postfix = param_name.split(\".\")[-1]\n+\n+        if postfix in CHECKPOINT_KEYS:\n+            param_name = param_name.replace(postfix, CHECKPOINT_KEYS[postfix])\n+\n+        set_module_tensor_to_device(model, param_name, param_device, value=param)\n+\n+    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n+        return model\n+\n+    def is_serializable(self, safe_serialization=None):\n+        return False\n+\n+    @property\n+    def is_trainable(self):\n+        return False"
        },
        {
            "sha": "83d1b6105d38a7215554fd02c48bd4a212393bcf",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a374799cedcf7a7df14256226a7576c324a42fa/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a374799cedcf7a7df14256226a7576c324a42fa/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=1a374799cedcf7a7df14256226a7576c324a42fa",
            "patch": "@@ -116,6 +116,7 @@\n     is_pytesseract_available,\n     is_pytest_available,\n     is_pytorch_quantization_available,\n+    is_quark_available,\n     is_rjieba_available,\n     is_sacremoses_available,\n     is_safetensors_available,\n@@ -1299,6 +1300,13 @@ def require_fbgemm_gpu(test_case):\n     return unittest.skipUnless(is_fbgemm_gpu_available(), \"test requires fbgemm-gpu\")(test_case)\n \n \n+def require_quark(test_case):\n+    \"\"\"\n+    Decorator for quark dependency\n+    \"\"\"\n+    return unittest.skipUnless(is_quark_available(), \"test requires quark\")(test_case)\n+\n+\n def require_flute_hadamard(test_case):\n     \"\"\"\n     Decorator marking a test that requires higgs and hadamard"
        },
        {
            "sha": "a549af2928da5aacef3023f578b346b0c255c409",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a374799cedcf7a7df14256226a7576c324a42fa/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a374799cedcf7a7df14256226a7576c324a42fa/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=1a374799cedcf7a7df14256226a7576c324a42fa",
            "patch": "@@ -181,6 +181,7 @@\n     is_pytesseract_available,\n     is_pytest_available,\n     is_pytorch_quantization_available,\n+    is_quark_available,\n     is_rich_available,\n     is_rjieba_available,\n     is_sacremoses_available,"
        },
        {
            "sha": "b6eb2be5db4ec86d0b455b8906a9245cde460b81",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a374799cedcf7a7df14256226a7576c324a42fa/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a374799cedcf7a7df14256226a7576c324a42fa/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=1a374799cedcf7a7df14256226a7576c324a42fa",
            "patch": "@@ -45,6 +45,11 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n     package_version = \"N/A\"\n     if package_exists:\n         try:\n+            # TODO: Once python 3.9 support is dropped, `importlib.metadata.packages_distributions()`\n+            # should be used here to map from package name to distribution names\n+            # e.g. PIL -> Pillow, Pillow-SIMD; quark -> amd-quark; onnxruntime -> onnxruntime-gpu.\n+            # `importlib.metadata.packages_distributions()` is not available in Python 3.9.\n+\n             # Primary method to get the package version\n             package_version = importlib.metadata.version(pkg_name)\n         except importlib.metadata.PackageNotFoundError:\n@@ -62,6 +67,12 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n                 except ImportError:\n                     # If the package can't be imported, it's not available\n                     package_exists = False\n+            elif pkg_name == \"quark\":\n+                # TODO: remove once `importlib.metadata.packages_distributions()` is supported.\n+                try:\n+                    package_version = importlib.metadata.version(\"amd-quark\")\n+                except Exception:\n+                    package_exists = False\n             else:\n                 # For packages other than \"torch\", don't attempt the fallback and set as not available\n                 package_exists = False\n@@ -150,6 +161,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n _gptqmodel_available = _is_package_available(\"gptqmodel\")\n # `importlib.metadata.version` doesn't work with `awq`\n _auto_awq_available = importlib.util.find_spec(\"awq\") is not None\n+_quark_available = _is_package_available(\"quark\")\n _is_optimum_quanto_available = False\n try:\n     importlib.metadata.version(\"optimum_quanto\")\n@@ -1118,6 +1130,10 @@ def is_optimum_quanto_available():\n     return _is_optimum_quanto_available\n \n \n+def is_quark_available():\n+    return _quark_available\n+\n+\n def is_compressed_tensors_available():\n     return _compressed_tensors_available\n "
        },
        {
            "sha": "6d859264005b4b35a911cb411729b5bc5f661564",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 41,
            "deletions": 0,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a374799cedcf7a7df14256226a7576c324a42fa/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a374799cedcf7a7df14256226a7576c324a42fa/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=1a374799cedcf7a7df14256226a7576c324a42fa",
            "patch": "@@ -2,6 +2,7 @@\n # coding=utf-8\n \n # Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n+# Modifications Copyright (C) 2025, Advanced Micro Devices, Inc. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -31,6 +32,7 @@\n     is_compressed_tensors_available,\n     is_gptqmodel_available,\n     is_hqq_available,\n+    is_quark_available,\n     is_torch_available,\n     is_torchao_available,\n     logging,\n@@ -60,6 +62,7 @@ class QuantizationMethod(str, Enum):\n     BITNET = \"bitnet\"\n     SPQR = \"spqr\"\n     FP8 = \"fp8\"\n+    QUARK = \"quark\"\n \n \n class AWQLinearVersion(str, Enum):\n@@ -1772,3 +1775,41 @@ def post_init(self):\n             raise ValueError(\"weight_block_size must be a tuple of two integers\")\n         if self.weight_block_size[0] <= 0 or self.weight_block_size[1] <= 0:\n             raise ValueError(\"weight_block_size must be a tuple of two positive integers\")\n+\n+\n+class QuarkConfig(QuantizationConfigMixin):\n+    def __init__(\n+        self,\n+        **kwargs,\n+    ):\n+        if is_torch_available() and is_quark_available():\n+            from quark import __version__ as quark_version\n+            from quark.torch.export.config.config import JsonExporterConfig\n+            from quark.torch.export.main_export.quant_config_parser import QuantConfigParser\n+            from quark.torch.quantization.config.config import Config\n+\n+        # This might be e.g. `\"fp8\"` or `\"awq\"`.\n+        self.custom_mode = kwargs[\"quant_method\"]\n+        self.legacy = \"export\" not in kwargs\n+\n+        if self.custom_mode in [\"awq\", \"fp8\"]:\n+            # Legacy (quark<1.0) or custom export.\n+            self.quant_config = QuantConfigParser.from_custom_config(kwargs, is_bias_quantized=False)\n+            self.json_export_config = JsonExporterConfig()\n+        else:\n+            self.quant_config = Config.from_dict(kwargs)\n+\n+            if \"export\" in kwargs:\n+                # TODO: Remove this check once configuration version is handled natively by Quark.\n+                if \"min_kv_scale\" in kwargs[\"export\"] and version.parse(quark_version) < version.parse(\"0.8\"):\n+                    min_kv_scale = kwargs[\"export\"].pop(\"min_kv_scale\")\n+                    logger.warning(\n+                        f\"The parameter `min_kv_scale={min_kv_scale}` was found in the model config.json's `quantization_config.export` configuration, but this parameter is supported only for quark>=0.8. Ignoring this configuration parameter. Please update the `amd-quark` package.\"\n+                    )\n+\n+                self.json_export_config = JsonExporterConfig(**kwargs[\"export\"])\n+            else:\n+                # Legacy (quark<1.0) or custom export.\n+                self.json_export_config = JsonExporterConfig()\n+\n+        self.quant_method = QuantizationMethod.QUARK"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/quantization/quark_integration/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a374799cedcf7a7df14256226a7576c324a42fa/tests%2Fquantization%2Fquark_integration%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a374799cedcf7a7df14256226a7576c324a42fa/tests%2Fquantization%2Fquark_integration%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fquark_integration%2F__init__.py?ref=1a374799cedcf7a7df14256226a7576c324a42fa"
        },
        {
            "sha": "32a9f6a6d8fd71d2f355bc1513603d3a01f03734",
            "filename": "tests/quantization/quark_integration/test_quark.py",
            "status": "added",
            "additions": 143,
            "deletions": 0,
            "changes": 143,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a374799cedcf7a7df14256226a7576c324a42fa/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a374799cedcf7a7df14256226a7576c324a42fa/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py?ref=1a374799cedcf7a7df14256226a7576c324a42fa",
            "patch": "@@ -0,0 +1,143 @@\n+# coding=utf-8\n+# Copyright 2025 Advanced Micro Devices, Inc. and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, QuarkConfig\n+from transformers.testing_utils import (\n+    is_torch_available,\n+    require_accelerate,\n+    require_quark,\n+    require_torch_gpu,\n+    require_torch_multi_gpu,\n+    slow,\n+)\n+from transformers.utils.import_utils import is_quark_available\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_quark_available():\n+    from quark.torch.export.nn.modules.qparamslinear import QParamsLinear\n+\n+\n+class QuarkConfigTest(unittest.TestCase):\n+    def test_commmon_args(self):\n+        config = AutoConfig.from_pretrained(\"amd/Llama-3.1-8B-Instruct-w-int8-a-int8-sym-test\")\n+        QuarkConfig(**config.quantization_config)\n+\n+\n+@slow\n+@require_quark\n+@require_torch_gpu\n+class QuarkTest(unittest.TestCase):\n+    reference_model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n+    quantized_model_name = \"amd/Llama-3.1-8B-Instruct-w-int8-a-int8-sym-test\"\n+\n+    input_text = \"Today I am in Paris and\"\n+\n+    EXPECTED_OUTPUTS = set()\n+    EXPECTED_OUTPUTS.add(\"Today I am in Paris and I am not in Paris, France\\nToday I am in Paris, Illinois\")\n+    EXPECTED_OUTPUTS.add(\"Today I am in Paris and I am enjoying the city of light. I am not just any ordinary Paris\")\n+    EXPECTED_OUTPUTS.add(\"Today I am in Paris and I am enjoying my day off! The sun is shining, the birds are\")\n+\n+    EXPECTED_RELATIVE_DIFFERENCE = 1.66\n+    device_map = None\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        \"\"\"\n+        Setup reference & quantized model\n+        \"\"\"\n+        cls.model_fp16 = AutoModelForCausalLM.from_pretrained(\n+            cls.reference_model_name, torch_dtype=torch.float16, device_map=cls.device_map\n+        )\n+        cls.mem_fp16 = cls.model_fp16.get_memory_footprint()\n+\n+        cls.tokenizer = AutoTokenizer.from_pretrained(cls.reference_model_name, use_fast=True)\n+\n+        cls.quantized_model = AutoModelForCausalLM.from_pretrained(\n+            cls.quantized_model_name,\n+            torch_dtype=torch.float16,\n+            device_map=cls.device_map,\n+        )\n+\n+    def test_memory_footprint(self):\n+        mem_quantized = self.quantized_model.get_memory_footprint()\n+\n+        self.assertTrue(self.mem_fp16 / mem_quantized > self.EXPECTED_RELATIVE_DIFFERENCE)\n+\n+    def test_device_and_dtype_assignment(self):\n+        r\"\"\"\n+        Test whether trying to cast (or assigning a device to) a model after quantization will throw an error.\n+        Checks also if other models are casted correctly.\n+        \"\"\"\n+        # This should work\n+        if self.device_map is None:\n+            _ = self.quantized_model.to(0)\n+\n+        with self.assertRaises(ValueError):\n+            # Tries with a `dtype``\n+            self.quantized_model.to(torch.float16)\n+\n+    def test_original_dtype(self):\n+        r\"\"\"\n+        A simple test to check if the model succesfully stores the original dtype\n+        \"\"\"\n+        self.assertTrue(hasattr(self.quantized_model.config, \"_pre_quantization_dtype\"))\n+        self.assertFalse(hasattr(self.model_fp16.config, \"_pre_quantization_dtype\"))\n+        self.assertTrue(self.quantized_model.config._pre_quantization_dtype == torch.float16)\n+\n+        self.assertTrue(isinstance(self.quantized_model.model.layers[0].mlp.gate_proj, QParamsLinear))\n+\n+    def check_inference_correctness(self, model):\n+        r\"\"\"\n+        Test the generation quality of the quantized model and see that we are matching the expected output.\n+        Given that we are operating on small numbers + the testing model is relatively small, we might not get\n+        the same output across GPUs. So we'll generate few tokens (5-10) and check their output.\n+        \"\"\"\n+        # Check that inference pass works on the model\n+        encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\")\n+\n+        gen_config = GenerationConfig(\n+            max_new_tokens=15,\n+            min_new_tokens=15,\n+            use_cache=True,\n+            num_beams=1,\n+            do_sample=False,\n+        )\n+\n+        # Check the exactness of the results\n+        output_sequences = model.generate(input_ids=encoded_input[\"input_ids\"].to(0), generation_config=gen_config)\n+\n+        # Get the generation\n+        self.assertIn(self.tokenizer.decode(output_sequences[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n+\n+    def test_generate_quality(self):\n+        \"\"\"\n+        Simple test to check the quality of the model by comparing the generated tokens with the expected tokens\n+        \"\"\"\n+        if self.device_map is None:\n+            self.check_inference_correctness(self.quantized_model.to(0))\n+        else:\n+            self.check_inference_correctness(self.quantized_model)\n+\n+\n+@require_accelerate\n+@require_torch_multi_gpu\n+@require_quark\n+class QuarkTestDeviceMap(QuarkTest):\n+    device_map = \"auto\""
        }
    ],
    "stats": {
        "total": 433,
        "additions": 432,
        "deletions": 1
    }
}