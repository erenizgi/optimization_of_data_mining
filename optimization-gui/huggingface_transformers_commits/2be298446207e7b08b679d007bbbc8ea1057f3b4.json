{
    "author": "qubvel",
    "message": "Fix pytorch defomr attn path (#36923)\n\n* Fix pytorch path for DeformableAttention\n\n* Apply for GroundingDino",
    "sha": "2be298446207e7b08b679d007bbbc8ea1057f3b4",
    "files": [
        {
            "sha": "8db75abc0a772c545f1fe3bca2a7917616f3f9e2",
            "filename": "src/transformers/models/deformable_detr/modeling_deformable_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2be298446207e7b08b679d007bbbc8ea1057f3b4/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2be298446207e7b08b679d007bbbc8ea1057f3b4/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py?ref=2be298446207e7b08b679d007bbbc8ea1057f3b4",
            "patch": "@@ -70,10 +70,10 @@ def forward(\n     ):\n         batch_size, _, num_heads, hidden_dim = value.shape\n         _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n-        value_list = value.split([height * width for height, width in value_spatial_shapes], dim=1)\n+        value_list = value.split([height * width for height, width in value_spatial_shapes_list], dim=1)\n         sampling_grids = 2 * sampling_locations - 1\n         sampling_value_list = []\n-        for level_id, (height, width) in enumerate(value_spatial_shapes):\n+        for level_id, (height, width) in enumerate(value_spatial_shapes_list):\n             # batch_size, height*width, num_heads, hidden_dim\n             # -> batch_size, height*width, num_heads*hidden_dim\n             # -> batch_size, num_heads*hidden_dim, height*width"
        },
        {
            "sha": "0b3b2899c145c0a631f615757c60e37001a903d7",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 24,
            "deletions": 6,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/2be298446207e7b08b679d007bbbc8ea1057f3b4/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2be298446207e7b08b679d007bbbc8ea1057f3b4/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=2be298446207e7b08b679d007bbbc8ea1057f3b4",
            "patch": "@@ -51,8 +51,8 @@\n _CHECKPOINT_FOR_DOC = \"IDEA-Research/grounding-dino-tiny\"\n \n \n-# Copied from models.deformable_detr.MultiScaleDeformableAttention\n @use_kernel_forward_from_hub(\"MultiScaleDeformableAttention\")\n+# Copied from transformers.models.deformable_detr.modeling_deformable_detr.MultiScaleDeformableAttention\n class MultiScaleDeformableAttention(nn.Module):\n     def forward(\n         self,\n@@ -66,10 +66,10 @@ def forward(\n     ):\n         batch_size, _, num_heads, hidden_dim = value.shape\n         _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n-        value_list = value.split([height * width for height, width in value_spatial_shapes], dim=1)\n+        value_list = value.split([height * width for height, width in value_spatial_shapes_list], dim=1)\n         sampling_grids = 2 * sampling_locations - 1\n         sampling_value_list = []\n-        for level_id, (height, width) in enumerate(value_spatial_shapes):\n+        for level_id, (height, width) in enumerate(value_spatial_shapes_list):\n             # batch_size, height*width, num_heads, hidden_dim\n             # -> batch_size, height*width, num_heads*hidden_dim\n             # -> batch_size, num_heads*hidden_dim, height*width\n@@ -1015,6 +1015,7 @@ def forward(\n         position_embeddings: torch.Tensor = None,\n         reference_points=None,\n         spatial_shapes=None,\n+        spatial_shapes_list=None,\n         level_start_index=None,\n         output_attentions: bool = False,\n     ):\n@@ -1030,6 +1031,8 @@ def forward(\n                 Reference points.\n             spatial_shapes (`torch.LongTensor`, *optional*):\n                 Spatial shapes of the backbone feature maps.\n+            spatial_shapes_list (`List[Tuple[int, int]]`, *optional*):\n+                Spatial shapes of the backbone feature maps (but as list for export compatibility).\n             level_start_index (`torch.LongTensor`, *optional*):\n                 Level start index.\n             output_attentions (`bool`, *optional*):\n@@ -1047,6 +1050,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             reference_points=reference_points,\n             spatial_shapes=spatial_shapes,\n+            spatial_shapes_list=spatial_shapes_list,\n             level_start_index=level_start_index,\n             output_attentions=output_attentions,\n         )\n@@ -1147,6 +1151,7 @@ def forward(\n         vision_features: Tensor,\n         vision_position_embedding: Tensor,\n         spatial_shapes: Tensor,\n+        spatial_shapes_list: List[Tuple[int, int]],\n         level_start_index: Tensor,\n         key_padding_mask: Tensor,\n         reference_points: Tensor,\n@@ -1179,6 +1184,7 @@ def forward(\n             position_embeddings=vision_position_embedding,\n             reference_points=reference_points,\n             spatial_shapes=spatial_shapes,\n+            spatial_shapes_list=spatial_shapes_list,\n             level_start_index=level_start_index,\n         )\n \n@@ -1295,6 +1301,7 @@ def forward(\n         position_embeddings: Optional[torch.Tensor] = None,\n         reference_points=None,\n         spatial_shapes=None,\n+        spatial_shapes_list=None,\n         level_start_index=None,\n         vision_encoder_hidden_states: Optional[torch.Tensor] = None,\n         vision_encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -1347,6 +1354,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             reference_points=reference_points,\n             spatial_shapes=spatial_shapes,\n+            spatial_shapes_list=spatial_shapes_list,\n             level_start_index=level_start_index,\n             output_attentions=output_attentions,\n         )\n@@ -1594,6 +1602,7 @@ def forward(\n         vision_attention_mask: Tensor,\n         vision_position_embedding: Tensor,\n         spatial_shapes: Tensor,\n+        spatial_shapes_list: List[Tuple[int, int]],\n         level_start_index: Tensor,\n         valid_ratios=None,\n         text_features: Optional[Tensor] = None,\n@@ -1618,6 +1627,8 @@ def forward(\n                 Position embeddings that are added to the queries and keys in each self-attention layer.\n             spatial_shapes (`torch.LongTensor` of shape `(num_feature_levels, 2)`):\n                 Spatial shapes of each feature map.\n+            spatial_shapes_list (`List[Tuple[int, int]]`):\n+                Spatial shapes of each feature map (but as list for export compatibility).\n             level_start_index (`torch.LongTensor` of shape `(num_feature_levels)`):\n                 Starting index of each feature map.\n             valid_ratios (`torch.FloatTensor` of shape `(batch_size, num_feature_levels, 2)`):\n@@ -1670,6 +1681,7 @@ def forward(\n                 vision_features=vision_features,\n                 vision_position_embedding=vision_position_embedding,\n                 spatial_shapes=spatial_shapes,\n+                spatial_shapes_list=spatial_shapes_list,\n                 level_start_index=level_start_index,\n                 key_padding_mask=vision_attention_mask,\n                 reference_points=reference_points,\n@@ -1748,6 +1760,7 @@ def forward(\n         text_encoder_attention_mask=None,\n         reference_points=None,\n         spatial_shapes=None,\n+        spatial_shapes_list=None,\n         level_start_index=None,\n         valid_ratios=None,\n         self_attn_mask=None,\n@@ -1775,6 +1788,8 @@ def forward(\n                 Reference point in range `[0, 1]`, top-left (0,0), bottom-right (1, 1), including padding area.\n             spatial_shapes (`torch.FloatTensor` of shape `(num_feature_levels, 2)`):\n                 Spatial shapes of the feature maps.\n+            spatial_shapes_list (`List[Tuple[int, int]]`):\n+                Spatial shapes of the feature maps (but as list for export compatibility).\n             level_start_index (`torch.LongTensor` of shape `(num_feature_levels)`, *optional*):\n                 Indexes for the start of each feature level. In range `[0, sequence_length]`.\n             valid_ratios (`torch.FloatTensor` of shape `(batch_size, num_feature_levels, 2)`, *optional*):\n@@ -1867,6 +1882,7 @@ def custom_forward(*inputs):\n                     position_embeddings=query_pos,\n                     reference_points=reference_points_input,\n                     spatial_shapes=spatial_shapes,\n+                    spatial_shapes_list=spatial_shapes_list,\n                     level_start_index=level_start_index,\n                     vision_encoder_hidden_states=vision_encoder_hidden_states,\n                     vision_encoder_attention_mask=vision_encoder_attention_mask,\n@@ -2248,11 +2264,11 @@ def forward(\n         source_flatten = []\n         mask_flatten = []\n         lvl_pos_embed_flatten = []\n-        spatial_shapes = []\n+        spatial_shapes_list = []\n         for level, (source, mask, pos_embed) in enumerate(zip(feature_maps, masks, position_embeddings_list)):\n             batch_size, num_channels, height, width = source.shape\n             spatial_shape = (height, width)\n-            spatial_shapes.append(spatial_shape)\n+            spatial_shapes_list.append(spatial_shape)\n             source = source.flatten(2).transpose(1, 2)\n             mask = mask.flatten(1)\n             pos_embed = pos_embed.flatten(2).transpose(1, 2)\n@@ -2263,7 +2279,7 @@ def forward(\n         source_flatten = torch.cat(source_flatten, 1)\n         mask_flatten = torch.cat(mask_flatten, 1)\n         lvl_pos_embed_flatten = torch.cat(lvl_pos_embed_flatten, 1)\n-        spatial_shapes = torch.as_tensor(spatial_shapes, dtype=torch.long, device=source_flatten.device)\n+        spatial_shapes = torch.as_tensor(spatial_shapes_list, dtype=torch.long, device=source_flatten.device)\n         level_start_index = torch.cat((spatial_shapes.new_zeros((1,)), spatial_shapes.prod(1).cumsum(0)[:-1]))\n         valid_ratios = torch.stack([self.get_valid_ratio(m) for m in masks], 1)\n         valid_ratios = valid_ratios.float()\n@@ -2276,6 +2292,7 @@ def forward(\n                 vision_attention_mask=~mask_flatten,\n                 vision_position_embedding=lvl_pos_embed_flatten,\n                 spatial_shapes=spatial_shapes,\n+                spatial_shapes_list=spatial_shapes_list,\n                 level_start_index=level_start_index,\n                 valid_ratios=valid_ratios,\n                 text_features=text_features,\n@@ -2352,6 +2369,7 @@ def forward(\n             text_encoder_attention_mask=~text_token_mask,\n             reference_points=reference_points,\n             spatial_shapes=spatial_shapes,\n+            spatial_shapes_list=spatial_shapes_list,\n             level_start_index=level_start_index,\n             valid_ratios=valid_ratios,\n             self_attn_mask=None,"
        },
        {
            "sha": "570c8cc3a32b683e612251e97b87a58da060fa2b",
            "filename": "src/transformers/models/omdet_turbo/modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2be298446207e7b08b679d007bbbc8ea1057f3b4/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2be298446207e7b08b679d007bbbc8ea1057f3b4/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py?ref=2be298446207e7b08b679d007bbbc8ea1057f3b4",
            "patch": "@@ -172,8 +172,8 @@ class OmDetTurboObjectDetectionOutput(ModelOutput):\n     classes_structure: Optional[torch.LongTensor] = None\n \n \n-# Copied from models.deformable_detr.MultiScaleDeformableAttention\n @use_kernel_forward_from_hub(\"MultiScaleDeformableAttention\")\n+# Copied from transformers.models.deformable_detr.modeling_deformable_detr.MultiScaleDeformableAttention\n class MultiScaleDeformableAttention(nn.Module):\n     def forward(\n         self,\n@@ -187,10 +187,10 @@ def forward(\n     ):\n         batch_size, _, num_heads, hidden_dim = value.shape\n         _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n-        value_list = value.split([height * width for height, width in value_spatial_shapes], dim=1)\n+        value_list = value.split([height * width for height, width in value_spatial_shapes_list], dim=1)\n         sampling_grids = 2 * sampling_locations - 1\n         sampling_value_list = []\n-        for level_id, (height, width) in enumerate(value_spatial_shapes):\n+        for level_id, (height, width) in enumerate(value_spatial_shapes_list):\n             # batch_size, height*width, num_heads, hidden_dim\n             # -> batch_size, height*width, num_heads*hidden_dim\n             # -> batch_size, num_heads*hidden_dim, height*width"
        },
        {
            "sha": "d7305d42652767bb73cb2a92b92e5f4a448eede3",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/2be298446207e7b08b679d007bbbc8ea1057f3b4/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2be298446207e7b08b679d007bbbc8ea1057f3b4/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py?ref=2be298446207e7b08b679d007bbbc8ea1057f3b4",
            "patch": "@@ -50,8 +50,8 @@\n _CHECKPOINT_FOR_DOC = \"PekingU/rtdetr_r50vd\"\n \n \n-# Copied from models.deformable_detr.MultiScaleDeformableAttention\n @use_kernel_forward_from_hub(\"MultiScaleDeformableAttention\")\n+# Copied from transformers.models.deformable_detr.modeling_deformable_detr.MultiScaleDeformableAttention\n class MultiScaleDeformableAttention(nn.Module):\n     def forward(\n         self,\n@@ -65,10 +65,10 @@ def forward(\n     ):\n         batch_size, _, num_heads, hidden_dim = value.shape\n         _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n-        value_list = value.split([height * width for height, width in value_spatial_shapes], dim=1)\n+        value_list = value.split([height * width for height, width in value_spatial_shapes_list], dim=1)\n         sampling_grids = 2 * sampling_locations - 1\n         sampling_value_list = []\n-        for level_id, (height, width) in enumerate(value_spatial_shapes):\n+        for level_id, (height, width) in enumerate(value_spatial_shapes_list):\n             # batch_size, height*width, num_heads, hidden_dim\n             # -> batch_size, height*width, num_heads*hidden_dim\n             # -> batch_size, num_heads*hidden_dim, height*width\n@@ -1998,7 +1998,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         outputs = self.model("
        },
        {
            "sha": "f707f5af27cb4d2a07c2d1c44fa839c56104d730",
            "filename": "src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2be298446207e7b08b679d007bbbc8ea1057f3b4/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2be298446207e7b08b679d007bbbc8ea1057f3b4/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py?ref=2be298446207e7b08b679d007bbbc8ea1057f3b4",
            "patch": "@@ -1997,7 +1997,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         outputs = self.model("
        }
    ],
    "stats": {
        "total": 48,
        "additions": 32,
        "deletions": 16
    }
}