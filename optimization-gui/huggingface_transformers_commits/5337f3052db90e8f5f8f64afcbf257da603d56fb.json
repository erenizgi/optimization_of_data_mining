{
    "author": "gante",
    "message": "ðŸš¨ðŸš¨  [generate] ignore `cache_implementation=\"hybrid\"` hub defaults (#40135)\n\n* working?\n\n* fix tests",
    "sha": "5337f3052db90e8f5f8f64afcbf257da603d56fb",
    "files": [
        {
            "sha": "8d1ed8baec5c0c48fe82b21039a9fa05361309c5",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5337f3052db90e8f5f8f64afcbf257da603d56fb/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5337f3052db90e8f5f8f64afcbf257da603d56fb/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=5337f3052db90e8f5f8f64afcbf257da603d56fb",
            "patch": "@@ -1742,6 +1742,13 @@ def _prepare_generation_config(\n             generation_config = self.generation_config\n             using_model_generation_config = True\n \n+            # Related to #40039: prior to this PR, models with sliding window attention were forced to have\n+            # `cache_implementation=\"hybrid\"` (the static sliding window cache). For these models, we now want to use\n+            # the dynamic sliding window cache by default, so we UNSET `cache_implementation` if it is a default value.\n+            # (if we're inside this branch, then it is because we're using default values from the Hub)\n+            if generation_config.cache_implementation == \"hybrid\":\n+                generation_config.cache_implementation = None\n+\n         # `torch.export.export` usually raises an exception if it is called\n         # with ``strict=True``. deepcopy can only be processed if ``strict=False``.\n         generation_config = copy.deepcopy(generation_config)\n@@ -1954,10 +1961,6 @@ def _prepare_cache_for_generation(\n             )\n             generation_config.cache_implementation = None\n \n-        generation_config.cache_implementation = generation_config.cache_implementation or getattr(\n-            self.config.get_text_config(decoder=True), \"cache_implementation\", None\n-        )\n-\n         # assisted decoding and contrastive search need to roll-back the Cache, which is not supported if\n         # it has sliding layers - so if we use any of those 2, do not pass the config to DynamicCache, which\n         # will result in creating a Cache with only full layers even if model uses sliding window"
        },
        {
            "sha": "f9a19646e7e026a2e6d07f6c991f41b52fc34856",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 53,
            "deletions": 8,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/5337f3052db90e8f5f8f64afcbf257da603d56fb/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5337f3052db90e8f5f8f64afcbf257da603d56fb/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=5337f3052db90e8f5f8f64afcbf257da603d56fb",
            "patch": "@@ -500,7 +500,8 @@ def test_model_4b_bf16(self):\n             add_generation_prompt=True,\n         ).to(torch_device)\n \n-        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+        # cache_implementation=\"hybrid\" an in the original transformers implementation\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False, cache_implementation=\"hybrid\")\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n \n         EXPECTED_TEXTS = Expectations(\n@@ -545,7 +546,8 @@ def test_model_4b_batch(self):\n             add_generation_prompt=True,\n         ).to(torch_device)\n \n-        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+        # cache_implementation=\"hybrid\" an in the original transformers implementation\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False, cache_implementation=\"hybrid\")\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n \n         EXPECTED_TEXTS = Expectations(\n@@ -599,7 +601,8 @@ def test_model_4b_crops(self):\n             **crop_config,\n         ).to(torch_device)\n \n-        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+        # cache_implementation=\"hybrid\" an in the original transformers implementation\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False, cache_implementation=\"hybrid\")\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n \n         EXPECTED_NUM_IMAGES = 3  # one for the origin image and two crops of images\n@@ -654,7 +657,8 @@ def test_model_4b_batch_crops(self):\n             **crop_config,\n         ).to(torch_device)\n \n-        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+        # cache_implementation=\"hybrid\" an in the original transformers implementation\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False, cache_implementation=\"hybrid\")\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n         EXPECTED_NUM_IMAGES = 9  # 3 * (one for the origin image and two crops of images) = 9\n         EXPECTED_TEXTS = Expectations(\n@@ -708,7 +712,8 @@ def test_model_4b_multiimage(self):\n             add_generation_prompt=True,\n         ).to(torch_device)\n \n-        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+        # cache_implementation=\"hybrid\" an in the original transformers implementation\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False, cache_implementation=\"hybrid\")\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n         EXPECTED_TEXTS = Expectations(\n             {\n@@ -729,7 +734,8 @@ def test_model_1b_text_only(self):\n         tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n         inputs = tokenizer(\"Write a poem about Machine Learning.\", return_tensors=\"pt\").to(torch_device)\n \n-        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+        # cache_implementation=\"hybrid\" an in the original transformers implementation\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False, cache_implementation=\"hybrid\")\n         output_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n \n         EXPECTED_TEXTS = Expectations(\n@@ -763,7 +769,8 @@ def test_model_4b_flash_attn(self):\n             add_generation_prompt=True,\n         ).to(torch_device)\n \n-        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+        # cache_implementation=\"hybrid\" an in the original transformers implementation\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False, cache_implementation=\"hybrid\")\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n \n         EXPECTED_TEXTS = Expectations(\n@@ -803,7 +810,10 @@ def test_generation_beyond_sliding_window(self, attn_implementation: str):\n         input_size = inputs.input_ids.shape[-1]\n         self.assertTrue(input_size > model.config.sliding_window)\n \n-        out = model.generate(**inputs, max_new_tokens=20, do_sample=False)[:, input_size:]\n+        # cache_implementation=\"hybrid\" an in the original transformers implementation\n+        out = model.generate(**inputs, max_new_tokens=20, do_sample=False, cache_implementation=\"hybrid\")[\n+            :, input_size:\n+        ]\n         output_text = tokenizer.batch_decode(out)\n \n         EXPECTED_COMPLETIONS = [\" and I'm going to take a walk.\\n\\nI really enjoy the scenery, and I'\", \", green, yellow, orange, purple, brown, black, white, gray.\\n\\nI'\"]  # fmt: skip\n@@ -844,9 +854,44 @@ def test_export_text_only_with_hybrid_cache(self):\n                 **input_text,\n                 max_new_tokens=max_new_tokens_to_generate,\n                 do_sample=False,  # Use greedy decoding to match the exported model\n+                cache_implementation=\"hybrid\",\n             )\n \n         eager_generated_text = tokenizer.decode(eager_outputs[0], skip_special_tokens=True)\n         logging.info(f\"\\nEager generated texts: '{eager_generated_text}'\")\n \n         self.assertEqual(export_generated_text, eager_generated_text)\n+\n+    def test_dynamic_sliding_window_is_default(self):\n+        \"\"\"\n+        Test that the dynamic sliding window cache (added in #40039) is the default cache implementation for Gemma3\n+        models, despite the fact that Hub checkpoints may have `cache_implementation=\"hybrid\"` (static sliding window).\n+        \"\"\"\n+        model_id = \"google/gemma-3-1b-it\"\n+        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n+\n+        # the default cache is static sliding window\n+        self.assertEqual(model.config.cache_implementation, \"hybrid\")\n+        self.assertEqual(model.generation_config.cache_implementation, \"hybrid\")\n+\n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n+        prompt = \"What is the capital of France?\"\n+        model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n+\n+        foward_outputs = model(**model_inputs)\n+        self.assertIn(\"DynamicSlidingWindowLayer\", str(foward_outputs.past_key_values))\n+\n+        generate_outputs = model.generate(\n+            **model_inputs, max_new_tokens=2, do_sample=False, return_dict_in_generate=True\n+        )\n+        self.assertIn(\"DynamicSlidingWindowLayer\", str(generate_outputs.past_key_values))\n+\n+        # If we manually specify the cache implementation = \"hybrid\", it will use the static sliding window cache\n+        generate_outputs = model.generate(\n+            **model_inputs,\n+            max_new_tokens=2,\n+            do_sample=False,\n+            return_dict_in_generate=True,\n+            cache_implementation=\"hybrid\",\n+        )\n+        self.assertNotIn(\"DynamicSlidingWindowLayer\", str(generate_outputs.past_key_values))"
        }
    ],
    "stats": {
        "total": 72,
        "additions": 60,
        "deletions": 12
    }
}