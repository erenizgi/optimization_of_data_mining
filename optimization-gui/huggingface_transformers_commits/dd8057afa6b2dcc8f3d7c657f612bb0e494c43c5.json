{
    "author": "SunMarc",
    "message": "Update `param_element_size` (#42818)\n\n* clean\n\n* int\n\n* check\n\n* better\n\n* working\n\n* remove unrelated stuff\n\n* rm print\n\n* torchao\n\n* Fix\n\n* added\n\n* fix quanto\n\n* revert\n\n* reverted\n\n* rm comment\n\n* fix",
    "sha": "dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5",
    "files": [
        {
            "sha": "d5e444bc7fcc81246383a9044926db27fa78e5f3",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 45,
            "deletions": 39,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5",
            "patch": "@@ -4056,6 +4056,7 @@ def _load_pretrained_model(\n \n         # Model's definition arriving here is final (TP hooks added, quantized layers replaces)\n         expected_keys = list(model.state_dict().keys())\n+\n         if logger.level >= logging.WARNING:\n             verify_tp_plan(expected_keys, getattr(model, \"_tp_plan\", None))\n \n@@ -4398,8 +4399,7 @@ def _move_missing_keys_from_meta_to_cpu(\n             # Buffers are not initialized on the meta device, so we still need this check to avoid overwriting them\n             if param.device == torch.device(\"meta\"):\n                 value = torch.empty_like(param, dtype=dtype, device=\"cpu\")\n-                if not is_quantized or not hf_quantizer.param_needs_quantization(self, key):\n-                    _load_parameter_into_model(self, key, value)\n+                _load_parameter_into_model(self, key, value)\n \n     def _initialize_missing_keys(self, is_quantized: bool) -> None:\n         \"\"\"\n@@ -4539,6 +4539,46 @@ def is_accelerator_device(device: Union[str, int, torch.device]) -> bool:\n         return torch.device(device).type not in [\"meta\", \"cpu\"]\n \n \n+def get_total_byte_count(\n+    model: PreTrainedModel, accelerator_device_map: dict, hf_quantizer: Optional[HfQuantizer] = None\n+):\n+    \"\"\"\n+    This utility function calculates the total bytes count needed to load the model on each device.\n+    This is useful for caching_allocator_warmup as we want to know how much cache we need to pre-allocate.\n+    \"\"\"\n+\n+    total_byte_count = defaultdict(lambda: 0)\n+    tied_param_names = model.all_tied_weights_keys.keys()\n+\n+    tp_plan = getattr(model, \"_tp_plan\", []) or []\n+    tp_plan_regex = (\n+        re.compile(\"|\".join([re.escape(plan) for plan in tp_plan]))\n+        if _torch_distributed_available and torch.distributed.is_initialized()\n+        else None\n+    )\n+\n+    for param_name, device in accelerator_device_map.items():\n+        # Skip if the parameter has already been accounted for (tied weights)\n+        if param_name in tied_param_names:\n+            continue\n+\n+        param = model.get_parameter_or_buffer(param_name)\n+\n+        if hf_quantizer is not None:\n+            dtype_size = hf_quantizer.param_element_size(model, param_name, param)\n+        else:\n+            dtype_size = param.element_size()\n+\n+        param_byte_count = param.numel() * dtype_size\n+\n+        if tp_plan_regex is not None:\n+            generic_name = re.sub(r\"\\.\\d+\\.\", \".*.\", param_name)\n+            param_byte_count //= torch.distributed.get_world_size() if tp_plan_regex.search(generic_name) else 1\n+\n+        total_byte_count[device] += param_byte_count\n+    return total_byte_count\n+\n+\n def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: dict, hf_quantizer: Optional[HfQuantizer]):\n     \"\"\"This function warm-ups the caching allocator based on the size of the model tensors that will reside on each\n     device. It allows to have one large call to Malloc, instead of recursively calling it later when loading\n@@ -4558,7 +4598,7 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: dict,\n     - Loading speed bottleneck is now almost only tensor copy (i.e. changing the dtype) and moving the tensors to the devices.\n     However, we cannot really improve on those aspects obviously, as the data needs to be moved/copied in the end.\n     \"\"\"\n-    factor = 2 if hf_quantizer is None else hf_quantizer.get_accelerator_warm_up_factor()\n+    factor = 2\n \n     # Remove disk, cpu and meta devices, and cast to proper torch.device\n     accelerator_device_map = {\n@@ -4567,40 +4607,7 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: dict,\n     if not accelerator_device_map:\n         return\n \n-    tp_plan = getattr(model, \"_tp_plan\", []) or []\n-    tp_plan_regex = (\n-        re.compile(\"|\".join([re.escape(plan) for plan in tp_plan]))\n-        if _torch_distributed_available and torch.distributed.is_initialized()\n-        else None\n-    )\n-    total_byte_count = defaultdict(lambda: 0)\n-    tied_param_names = model.all_tied_weights_keys.keys()\n-    for param_name, device in accelerator_device_map.items():\n-        # Skip if the parameter has already been accounted for (tied weights)\n-        if param_name in tied_param_names:\n-            continue\n-\n-        # For example in the case of MXFP4 quantization, we need to update the param name to the original param name\n-        # because the checkpoint contains blocks, and scales, but since we are dequantizing, we need to use the original param name\n-        if hf_quantizer is not None:\n-            param_name = hf_quantizer.get_param_name(param_name)\n-\n-        try:\n-            param = model.get_parameter_or_buffer(param_name)\n-        except AttributeError:\n-            # TODO: for now let's skip if we can't find the parameters\n-            if hf_quantizer is not None:\n-                continue\n-            raise AttributeError(f\"Parameter {param_name} not found in model\")\n-\n-        # The dtype of different parameters may be different with composite models or `keep_in_fp32_modules`\n-        param_byte_count = param.numel() * param.element_size()\n-\n-        if tp_plan_regex is not None:\n-            generic_name = re.sub(r\"\\.\\d+\\.\", \".*.\", param_name)\n-            param_byte_count //= torch.distributed.get_world_size() if tp_plan_regex.search(generic_name) else 1\n-\n-        total_byte_count[device] += param_byte_count\n+    total_byte_count = get_total_byte_count(model, accelerator_device_map, hf_quantizer)\n \n     # This will kick off the caching allocator to avoid having to Malloc afterwards\n     for device, byte_count in total_byte_count.items():\n@@ -4620,8 +4627,7 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: dict,\n             unused_memory = torch_accelerator_module.memory_reserved(\n                 index\n             ) - torch_accelerator_module.memory_allocated(index)\n-            byte_count = max(0, byte_count - unused_memory)\n-        # Allocate memory\n+            byte_count = int(max(0, byte_count - unused_memory))\n         _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)\n \n "
        },
        {
            "sha": "c07ce78b3e82f3b6f5635dfc8c2da0903573962e",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5",
            "patch": "@@ -108,33 +108,7 @@ def update_device_map(self, device_map: dict[str, Any] | None) -> dict[str, Any]\n         \"\"\"\n         return device_map\n \n-    def adjust_target_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        \"\"\"\n-        Override this method if you want to adjust the `target_dtype` variable used in `from_pretrained`\n-        to compute the device_map in case the device_map is a `str`. E.g. for bitsandbytes we force-set `target_dtype`\n-        to `torch.int8` and for 4-bit we pass a custom enum `accelerate.CustomDtype.int4`.\n-\n-        Args:\n-            dtype (`torch.dtype`, *optional*):\n-                The dtype that is used to compute the device_map.\n-        \"\"\"\n-        return dtype\n-\n     def param_element_size(self, model: \"PreTrainedModel\", param_name: str, param: \"torch.Tensor\") -> float:\n-        \"Return the element size (in bytes) for `param_name`.\"\n-\n-        if self.param_needs_quantization(model, param_name):\n-            from accelerate.utils import CustomDtype\n-\n-            mapping = {\n-                torch.int8: 1,\n-                CustomDtype.INT4: 0.5,\n-                CustomDtype.FP8: 1,\n-                CustomDtype.INT2: 0.25,\n-            }\n-            # The value passed is actually not used when the method is overridden\n-            if (custom_dtype := self.adjust_target_dtype(torch.float16)) in mapping:\n-                return mapping[custom_dtype]\n         return param.element_size()\n \n     def adjust_max_memory(self, max_memory: dict[str, int | str]) -> dict[str, int | str]:\n@@ -231,17 +205,6 @@ def dequantize(self, model, dtype=None):\n \n         return model\n \n-    def get_accelerator_warm_up_factor(self):\n-        \"\"\"\n-        The factor to be used in `caching_allocator_warmup` to get the number of bytes to pre-allocate to warm up accelerator.\n-        A factor of 2 means we allocate all bytes in the empty model (since we allocate in fp16), a factor of 4 means\n-        we allocate half the memory of the weights residing in the empty model, etc...\n-        \"\"\"\n-        # By default we return 4, i.e. half the model size (this corresponds to the case where the model is not\n-        # really pre-processed, i.e. we do not have the info that weights are going to be 8 bits before actual\n-        # weight loading)\n-        return 4\n-\n     def _dequantize(self, model, dtype=None):\n         raise NotImplementedError(\n             f\"{self.quantization_config.quant_method} has no implementation of `dequantize`, please raise an issue on GitHub.\""
        },
        {
            "sha": "c56cca664dd37b03f42922488a42697b78353458",
            "filename": "src/transformers/quantizers/quantizer_bitnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py?ref=dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5",
            "patch": "@@ -86,10 +86,6 @@ def adjust_max_memory(self, max_memory: dict[str, int | str]) -> dict[str, int |\n         max_memory = {key: val * 0.90 for key, val in max_memory.items()}\n         return max_memory\n \n-    def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        target_dtype = torch.int8\n-        return target_dtype\n-\n     def is_serializable(self):\n         return True\n "
        },
        {
            "sha": "48c7de9dea4bcf57f34a4447b13cc7621a89d971",
            "filename": "src/transformers/quantizers/quantizer_bnb_4bit.py",
            "status": "modified",
            "additions": 6,
            "deletions": 31,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py?ref=dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5",
            "patch": "@@ -51,15 +51,6 @@ class Bnb4BitHfQuantizer(HfQuantizer):\n     def __init__(self, quantization_config, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n \n-        # This describes the additional items that are saved on the state dict (on the params themselves)\n-        self.bnb_keys = [\n-            f\"quant_state.bitsandbytes__{self.quantization_config.bnb_4bit_quant_type}\",\n-            \"absmax\",\n-            \"quant_map\",\n-        ]\n-        if self.quantization_config.bnb_4bit_use_double_quant:\n-            self.bnb_keys.extend([\"nested_absmax\", \"nested_quant_map\"])\n-\n     def validate_environment(self, *args, **kwargs):\n         if not is_accelerate_available():\n             raise ImportError(\n@@ -87,36 +78,20 @@ def validate_environment(self, *args, **kwargs):\n                     \"for more details. \"\n                 )\n \n-    def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        from accelerate.utils import CustomDtype\n+    def param_element_size(self, model: \"PreTrainedModel\", param_name: str, param: \"torch.Tensor\") -> float:\n+        \"Return the element size (in bytes) for `param_name`.\"\n+        if self.param_needs_quantization(model, param_name):\n+            # 4 bit\n+            return 0.5\n \n-        if target_dtype != torch.int8:\n-            logger.info(\"target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\")\n-        return CustomDtype.INT4\n+        return super().param_element_size(model, param_name, param)\n \n     def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         import bitsandbytes as bnb\n \n-        # TODO: maybe remove\n-        # # They are on the params themselves, so we cannot easily extract the module from the name\n-        if any(param_name.endswith(x) for x in self.bnb_keys):\n-            return True\n         module, name = get_module_from_name(model, param_name)\n         return isinstance(module, bnb.nn.Linear4bit) and name != \"bias\"\n \n-    def get_param_name(self, param_name: str) -> str:\n-        \"\"\"\n-        Get the right param_name in order to get the module associated with the param.\n-        This is useful for quantized stats lile absmax or quant_map as we need to update the param_name to get the module as they are stored in ...weight.absmax.\n-        \"\"\"\n-        if self.pre_quantized:\n-            # We need to get the param name of quantized weights and not its components. Otherwise, we won't be able to get the nn.Module associated.\n-            if any(param_name.endswith(x) for x in self.bnb_keys):\n-                param_name = (\n-                    param_name.rsplit(\".\", 1)[0] if \"quant_state.\" not in param_name else param_name.rsplit(\".\", 2)[0]\n-                )\n-        return param_name\n-\n     def adjust_max_memory(self, max_memory: dict[str, int | str]) -> dict[str, int | str]:\n         # need more space for buffers that are created during quantization\n         max_memory = {key: val * 0.90 for key, val in max_memory.items()}"
        },
        {
            "sha": "3cdd9c020229395537fdb07053c84ddef19f33e0",
            "filename": "src/transformers/quantizers/quantizer_bnb_8bit.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py?ref=dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5",
            "patch": "@@ -102,8 +102,12 @@ def update_device_map(self, device_map):\n             )\n         return device_map\n \n-    def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        return torch.int8\n+    def param_element_size(self, model: \"PreTrainedModel\", param_name: str, param: \"torch.Tensor\") -> float:\n+        \"Return the element size (in bytes) for `param_name`.\"\n+        if self.param_needs_quantization(model, param_name):\n+            # 8-bit\n+            return 1\n+        return super().param_element_size(model, param_name, param)\n \n     def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         import bitsandbytes as bnb"
        },
        {
            "sha": "6646b03817a64df7ae4e740224c1ca9d77c8e0b5",
            "filename": "src/transformers/quantizers/quantizer_finegrained_fp8.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py?ref=dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5",
            "patch": "@@ -82,6 +82,13 @@ def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **\n                 return True\n         return False\n \n+    def param_element_size(self, model: \"PreTrainedModel\", param_name: str, param: \"torch.Tensor\") -> float:\n+        \"Return the element size (in bytes) for `param_name`.\"\n+        if self.param_needs_quantization(model, param_name):\n+            # 8 bit, this is neeed as when `pre_quantized`` is False, we don't set the dtype of the FP8Linear in order to correctly load the weights\n+            return 1\n+        return super().param_element_size(model, param_name, param)\n+\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n@@ -136,10 +143,6 @@ def is_serializable(self):\n     def is_trainable(self) -> bool:\n         return False\n \n-    def get_accelerator_warm_up_factor(self):\n-        # Pre-processing is done cleanly, so we can allocate everything here\n-        return 2\n-\n     def get_quantize_ops(self):\n         from ..integrations.finegrained_fp8 import Fp8Quantize\n "
        },
        {
            "sha": "811a862c1ce5c72ab256c7e1d107a5fb92a53256",
            "filename": "src/transformers/quantizers/quantizer_mxfp4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py?ref=dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5",
            "patch": "@@ -202,19 +202,6 @@ def update_ep_plan(self, config):\n                 )\n         return config\n \n-    def get_param_name(self, param_name: str) -> str:\n-        if self.quantization_config.dequantize:\n-            if \"_blocks\" in param_name:\n-                return param_name.replace(\"_blocks\", \"\")\n-            elif \"_scales\" in param_name:\n-                return param_name.replace(\"_scales\", \"\")\n-        elif not self.pre_quantized:\n-            if param_name.endswith(\"gate_up_proj\"):\n-                return param_name.replace(\"gate_up_proj\", \"gate_up_proj_blocks\")\n-            if param_name.endswith(\"down_proj\"):\n-                return param_name.replace(\"down_proj\", \"down_proj_blocks\")\n-        return param_name\n-\n     def get_state_dict_and_metadata(self, model):\n         from ..integrations import Mxfp4GptOssExperts\n "
        },
        {
            "sha": "1e01e5a593c873d65220be91725f2d4c7877ec76",
            "filename": "src/transformers/quantizers/quantizer_quanto.py",
            "status": "modified",
            "additions": 12,
            "deletions": 10,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py?ref=dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5",
            "patch": "@@ -44,6 +44,13 @@ class QuantoHfQuantizer(HfQuantizer):\n \n     def __init__(self, quantization_config: QuantoConfig, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n+        map_to_param_size = {\n+            \"int8\": 1,\n+            \"float8\": 1,\n+            \"int4\": 0.5,\n+            \"int2\": 0.25,\n+        }\n+        self.quantized_param_size = map_to_param_size.get(self.quantization_config.weights, None)\n \n     def validate_environment(self, *args, **kwargs):\n         if not is_optimum_quanto_available():\n@@ -83,17 +90,12 @@ def adjust_max_memory(self, max_memory: dict[str, int | str]) -> dict[str, int |\n         max_memory = {key: val * 0.90 for key, val in max_memory.items()}\n         return max_memory\n \n-    def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        from accelerate.utils import CustomDtype\n+    def param_element_size(self, model: \"PreTrainedModel\", param_name: str, param: \"torch.Tensor\") -> float:\n+        \"Return the element size (in bytes) for `param_name`.\"\n+        if self.param_needs_quantization(model, param_name) and self.quantized_param_size is not None:\n+            return self.quantized_param_size\n \n-        mapping = {\n-            \"int8\": torch.int8,\n-            \"float8\": CustomDtype.FP8,\n-            \"int4\": CustomDtype.INT4,\n-            \"int2\": CustomDtype.INT2,\n-        }\n-        target_dtype = mapping[self.quantization_config.weights]\n-        return target_dtype\n+        return super().param_element_size(model, param_name, param)\n \n     def _process_model_before_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n         from ..integrations import replace_with_quanto_layers"
        },
        {
            "sha": "cf7dee95a52e9cbbcc813137d6668158e082c846",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 20,
            "deletions": 89,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5",
            "patch": "@@ -18,7 +18,7 @@\n from packaging import version\n \n from .base import HfQuantizer\n-from .quantizers_utils import get_module_from_name\n+from .quantizers_utils import get_module_from_name, should_convert_module\n \n \n if TYPE_CHECKING:\n@@ -94,19 +94,19 @@ class TorchAoHfQuantizer(HfQuantizer):\n     def __init__(self, quantization_config, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n \n-        if isinstance(self.quantization_config.quant_type, str):\n-            is_int_4 = \"int4\" in self.quantization_config.quant_type\n-        else:\n-            config_name = self.quantization_config.quant_type.__class__.__name__\n-            is_int_4 = fuzzy_match_size(config_name) == \"4\"\n-\n-        # TODO: better way to get the serialized key names? Hard to read from torchao codebase\n-        if is_int_4:\n-            self.weight_ao_keys = [\"qdata\", \"scale\", \"zero_point\"]\n+        self.quantized_param_size = None\n+        quant_type = self.quantization_config.quant_type\n+        if isinstance(quant_type, str):\n+            map_to_param_size = {\n+                \"int4_weight_only\": 0.5,\n+                \"int8_weight_only\": 1,\n+                \"int8_dynamic_activation_int8_weight\": 1,\n+            }\n+            if quant_type in map_to_param_size:\n+                self.quantized_param_size = map_to_param_size[quant_type]\n         else:\n-            self.weight_ao_keys = [\"qdata\", \"scale\"]\n-        # Instead of serializing the simple torch.Tensor like usual, torchao adds a `:_data` suffix so we need this\n-        self.full_ao_keys = self.weight_ao_keys + [\"_data\"]\n+            size_digit = fuzzy_match_size(quant_type.__class__.__name__)\n+            self.quantized_param_size = 0.5 if size_digit == \"4\" else 1\n \n     def validate_environment(self, *args, **kwargs):\n         if not is_torchao_available():\n@@ -152,40 +152,12 @@ def get_state_dict_and_metadata(self, model):\n                 f\"In order to use safetensors with torchao, please use torchao version >= 0.15.0. Current version: {TORCHAO_VERSION}\"\n             )\n \n-    def adjust_target_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        from accelerate.utils import CustomDtype\n-\n-        # Import AOBaseConfig directly since we know we have the right version\n-        if self.quantization_config._get_ao_version() > version.Version(\"0.9.0\"):\n-            from torchao.core.config import AOBaseConfig\n-\n-            quant_type = self.quantization_config.quant_type\n-            if isinstance(quant_type, AOBaseConfig):\n-                # Extract size digit using fuzzy match on the class name\n-                config_name = quant_type.__class__.__name__\n-                size_digit = fuzzy_match_size(config_name)\n-\n-                # Map the extracted digit to appropriate dtype\n-                if size_digit == \"4\":\n-                    return CustomDtype.INT4\n-                else:\n-                    # Default to int8\n-                    return torch.int8\n-\n-            # Original mapping for non-AOBaseConfig types\n-            map_to_target_dtype = {\n-                \"int4_weight_only\": CustomDtype.INT4,\n-                \"int8_weight_only\": torch.int8,\n-                \"int8_dynamic_activation_int8_weight\": torch.int8,\n-                \"autoquant\": None,\n-            }\n-            return map_to_target_dtype[self.quantization_config.quant_type]\n-        else:\n-            raise ValueError(\n-                \"You are using `device_map='auto'` on a torchao quantized model. To automatically compute\"\n-                \" the appropriate device map, you should upgrade your `accelerate` library with \"\n-                \"`pip install --upgrade accelerate`\"\n-            )\n+    def param_element_size(self, model: \"PreTrainedModel\", param_name: str, param: \"torch.Tensor\") -> float:\n+        \"Return the element size (in bytes) for `param_name`.\"\n+        if self.param_needs_quantization(model, param_name) and self.quantized_param_size is not None:\n+            return self.quantized_param_size\n+\n+        return super().param_element_size(model, param_name, param)\n \n     def adjust_max_memory(self, max_memory: dict[str, int | str]) -> dict[str, int | str]:\n         # need more space for the quantization parameters (e.g. scale). Tested with int4 wo and group size = 128\n@@ -209,13 +181,11 @@ def _process_model_before_weight_loading(self, model: \"PreTrainedModel\", checkpo\n             self.set_metadata(checkpoint_files)\n \n     def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n-        if self.pre_quantized:\n-            return False\n         if self.quantization_config.quant_type == \"autoquant\":\n             return False\n \n         # check if the param_name is not in self.modules_to_not_convert\n-        if any(key + \".\" in param_name or key == param_name for key in self.modules_to_not_convert):\n+        if not should_convert_module(param_name, self.modules_to_not_convert):\n             return False\n \n         # we only quantize the weight of nn.Linear and nn.Embedding\n@@ -267,45 +237,6 @@ def is_serializable(self) -> bool:\n             )\n         return _is_torchao_serializable\n \n-    def get_accelerator_warm_up_factor(self):\n-        \"\"\"\n-        This factor is used in caching_allocator_warmup to determine how many bytes to pre-allocate for accelerator warmup.\n-        - A factor of 2 means we pre-allocate the full memory footprint of the model.\n-        - A factor of 4 means we pre-allocate half of that, and so on\n-\n-        However, when using TorchAO, calculating memory usage with param.numel() * param.element_size() doesn't give the correct size for quantized weights (like int4 or int8)\n-        That's because TorchAO internally represents quantized tensors using subtensors and metadata, and the reported element_size() still corresponds to the dtype\n-        not the actual bit-width of the quantized data.\n-\n-        To correct for this:\n-        - Use a division factor of 8 for int4 weights\n-        - Use a division factor of 4 for int8 weights\n-        \"\"\"\n-        if self.quantization_config._get_ao_version() > version.Version(\"0.9.0\"):\n-            from torchao.core.config import AOBaseConfig\n-\n-            quant_type = self.quantization_config.quant_type\n-            # For autoquant case, it will be treated in the string implementation below in map_to_target_dtype\n-            if isinstance(quant_type, AOBaseConfig):\n-                # Extract size digit using fuzzy match on the class name\n-                config_name = quant_type.__class__.__name__\n-                size_digit = fuzzy_match_size(config_name)\n-\n-                if size_digit == \"4\":\n-                    return 8\n-                else:\n-                    return 4\n-\n-        # Original mapping for non-AOBaseConfig types\n-        map_to_target_dtype = {\n-            \"int4_weight_only\": 8,\n-            \"int8_weight_only\": 4,\n-            \"int8_dynamic_activation_int8_weight\": 4,\n-            \"autoquant\": 4,\n-        }\n-\n-        return map_to_target_dtype[self.quantization_config.quant_type]\n-\n     @property\n     def is_trainable(self) -> bool:\n         supported_quant_types_for_training = ["
        },
        {
            "sha": "f85dadeb07759ce4270603d70869a2c573ddb934",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5",
            "patch": "@@ -154,6 +154,52 @@ def test_quantization_num_parameters(self):\n \n         self.assertEqual(num_params_4bit, num_params_fp16)\n \n+    def test_compute_module_sizes(self):\n+        r\"\"\"\n+        Test if we compute the right module sizes needed to generate the device map.\n+        Also test if we get the right values for `total_byte_count` in `caching_allocator_warmup`.\n+        \"\"\"\n+        from transformers.integrations.accelerate import compute_module_sizes\n+        from transformers.modeling_utils import expand_device_map, get_total_byte_count\n+        from transformers.quantizers import AutoHfQuantizer\n+\n+        # we need to preprocess the model like that because device_map calculation happens before we load the weights inside the model.\n+        # For normal wieghts, it's fine but for quantized weights, the tensors dtype might change during loading.\n+        with torch.device(\"meta\"):\n+            model = AutoModelForCausalLM.from_config(self.model_fp16.config, dtype=torch.float16)\n+            model_size, _ = compute_module_sizes(model, only_modules=False)\n+\n+            expected_keys = [name for name, _ in model.named_parameters()] + [\n+                name for name, _ in model.named_buffers()\n+            ]\n+            expanded_device_map = expand_device_map({\"\": torch_device}, expected_keys)\n+            total_byte_count = list(get_total_byte_count(model, expanded_device_map).values())[0]\n+\n+            # testing prequantized = False should be enough, the shape should be the same whether it is pre-quantized or not\n+            hf_quantizer = AutoHfQuantizer.from_config(BitsAndBytesConfig(load_in_4bit=True), pre_quantized=False)\n+            hf_quantizer.preprocess_model(model=model, config=model.config)\n+            quantized_model_size, _ = compute_module_sizes(model, hf_quantizer, only_modules=False)\n+\n+            expected_keys = [name for name, _ in model.named_parameters()] + [\n+                name for name, _ in model.named_buffers()\n+            ]\n+            expanded_device_map = expand_device_map({\"\": torch_device}, expected_keys)\n+            quantized_total_byte_count = list(get_total_byte_count(model, expanded_device_map, hf_quantizer).values())[\n+                0\n+            ]\n+\n+        for name, module in model.named_modules():\n+            if isinstance(module, bnb.nn.Linear4bit):\n+                # from 16 bits to 4 bits\n+                assert int(model_size[f\"{name}.weight\"] // 4) == int(quantized_model_size[f\"{name}.weight\"])\n+\n+        # check that we get the same value, as we use `compute_module_sizes` in `get_total_byte_count`\n+        assert total_byte_count == model_size[\"\"]\n+        assert quantized_total_byte_count == quantized_model_size[\"\"]\n+\n+        # we should at least have 2 times memory reduction in total\n+        assert model_size[\"\"] > quantized_model_size[\"\"] * 2\n+\n     def test_quantization_config_json_serialization(self):\n         r\"\"\"\n         A simple test to check if the quantization config is correctly serialized and deserialized"
        },
        {
            "sha": "0b7d3f54141e95824aeac25c3ece00c0ff77efce",
            "filename": "tests/quantization/bnb/test_mixed_int8.py",
            "status": "modified",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py?ref=dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5",
            "patch": "@@ -423,6 +423,52 @@ def test_int8_from_pretrained(self):\n \n         self.assertIn(self.tokenizer.decode(output_sequences[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n \n+    def test_compute_module_sizes(self):\n+        r\"\"\"\n+        Test if we compute the right module sizes needed to generate the device map.\n+        Also test if we get the right values for `total_byte_count` in `caching_allocator_warmup`.\n+        \"\"\"\n+        from transformers.integrations.accelerate import compute_module_sizes\n+        from transformers.modeling_utils import expand_device_map, get_total_byte_count\n+        from transformers.quantizers import AutoHfQuantizer\n+\n+        # we need to preprocess the model like that because device_map calculation happens before we load the weights inside the model.\n+        # For normal wieghts, it's fine but for quantized weights, the tensors dtype might change during loading.\n+        with torch.device(\"meta\"):\n+            model = AutoModelForCausalLM.from_config(self.model_fp16.config, dtype=torch.float16)\n+            model_size, _ = compute_module_sizes(model, only_modules=False)\n+\n+            expected_keys = [name for name, _ in model.named_parameters()] + [\n+                name for name, _ in model.named_buffers()\n+            ]\n+            expanded_device_map = expand_device_map({\"\": torch_device}, expected_keys)\n+            total_byte_count = list(get_total_byte_count(model, expanded_device_map).values())[0]\n+\n+            # testing prequantized = False should be enough, the shape should be the same whether it is pre-quantized or not\n+            hf_quantizer = AutoHfQuantizer.from_config(BitsAndBytesConfig(load_in_8bit=True), pre_quantized=False)\n+            hf_quantizer.preprocess_model(model=model, config=model.config)\n+            quantized_model_size, _ = compute_module_sizes(model, hf_quantizer, only_modules=False)\n+\n+            expected_keys = [name for name, _ in model.named_parameters()] + [\n+                name for name, _ in model.named_buffers()\n+            ]\n+            expanded_device_map = expand_device_map({\"\": torch_device}, expected_keys)\n+            quantized_total_byte_count = list(get_total_byte_count(model, expanded_device_map, hf_quantizer).values())[\n+                0\n+            ]\n+\n+        for name, module in model.named_modules():\n+            if isinstance(module, bnb.nn.Linear8bitLt):\n+                # from 16 bits to 8 bits\n+                assert int(model_size[f\"{name}.weight\"] // 2) == int(quantized_model_size[f\"{name}.weight\"])\n+\n+        # check that we get the same value, as we use `compute_module_sizes` in `get_total_byte_count`\n+        assert total_byte_count == model_size[\"\"]\n+        assert quantized_total_byte_count == quantized_model_size[\"\"]\n+\n+        # we should at least have 1.5 times memory reduction in total\n+        assert model_size[\"\"] > quantized_model_size[\"\"] * 1.5\n+\n \n @require_bitsandbytes\n @require_accelerate"
        },
        {
            "sha": "e82113df501a58a17bd4311e0b881b095c7616f3",
            "filename": "tests/quantization/finegrained_fp8/test_fp8.py",
            "status": "modified",
            "additions": 60,
            "deletions": 4,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py?ref=dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5",
            "patch": "@@ -259,16 +259,21 @@ def test_block_size(self):\n         self.assertEqual(quantized_model.config.quantization_config.weight_block_size, (32, 32))\n \n     @require_torch_multi_accelerator\n-    def test_quantized_model_multi_accelerator(self):\n+    def test_quantized_model_multi_accelerators(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly with multiple accelerators\n         set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 GPUs; or set ZE_AFFINITY_MASK=0,1 if you\n         have more than 2 XPUs.\n         \"\"\"\n         input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(self.device_map)\n         quantization_config = FineGrainedFP8Config()\n+        # need to empty cache or set max_memory, otherwise we will use the reserved memory that was not allocated when computing max-memory\n+        # this will lead to put the entire model to device 0.\n         quantized_model = AutoModelForCausalLM.from_pretrained(\n-            self.model_name, device_map=\"auto\", quantization_config=quantization_config\n+            self.model_name,\n+            device_map=\"auto\",\n+            quantization_config=quantization_config,\n+            max_memory={0: \"1GB\", 1: \"10GB\"},\n         )\n         self.assertTrue(set(quantized_model.hf_device_map.values()) == {0, 1})\n \n@@ -282,8 +287,11 @@ def test_save_pretrained_multi_accelerators(self):\n         \"\"\"\n         with tempfile.TemporaryDirectory() as tmpdirname:\n             self.quantized_model.save_pretrained(tmpdirname)\n-\n-            model = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=\"auto\")\n+            # need to empty cache or set max_memory, otherwise we will use the reserved memory that was not allocated when computing max-memory\n+            # this will lead to put the entire model to device 0.\n+            model = AutoModelForCausalLM.from_pretrained(\n+                tmpdirname, device_map=\"auto\", max_memory={0: \"1GB\", 1: \"10GB\"}\n+            )\n             self.assertTrue(set(model.hf_device_map.values()) == {0, 1})\n \n             input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(self.device_map)\n@@ -315,6 +323,54 @@ def test_save_pretrained_offload(self):\n             output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n             self.assertIn(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n \n+    def test_compute_module_sizes(self):\n+        r\"\"\"\n+        Test if we compute the right module sizes needed to generate the device map.\n+        Also test if we get the right values for `total_byte_count` in `caching_allocator_warmup`.\n+        \"\"\"\n+        from transformers.integrations import FP8Linear\n+        from transformers.integrations.accelerate import compute_module_sizes\n+        from transformers.modeling_utils import expand_device_map, get_total_byte_count\n+        from transformers.quantizers import AutoHfQuantizer\n+\n+        # we need to preprocess the model like that because device_map calculation happens before we load the weights inside the model.\n+        # For normal wieghts, it's fine but for quantized weights, the tensors dtype might change during loading.\n+        with torch.device(\"meta\"):\n+            config = AutoConfig.from_pretrained(self.model_name)\n+            model = AutoModelForCausalLM.from_config(config, dtype=torch.bfloat16)\n+            model_size, _ = compute_module_sizes(model, only_modules=False)\n+\n+            expected_keys = [name for name, _ in model.named_parameters()] + [\n+                name for name, _ in model.named_buffers()\n+            ]\n+            expanded_device_map = expand_device_map({\"\": torch_device}, expected_keys)\n+            total_byte_count = list(get_total_byte_count(model, expanded_device_map).values())[0]\n+\n+            # testing prequantized = False should be enough, the shape should be the same whether it is pre-quantized or not\n+            hf_quantizer = AutoHfQuantizer.from_config(FineGrainedFP8Config(), pre_quantized=False)\n+            hf_quantizer.preprocess_model(model=model, config=model.config)\n+            quantized_model_size, _ = compute_module_sizes(model, hf_quantizer, only_modules=False)\n+\n+            expected_keys = [name for name, _ in model.named_parameters()] + [\n+                name for name, _ in model.named_buffers()\n+            ]\n+            expanded_device_map = expand_device_map({\"\": torch_device}, expected_keys)\n+            quantized_total_byte_count = list(get_total_byte_count(model, expanded_device_map, hf_quantizer).values())[\n+                0\n+            ]\n+\n+        for name, module in model.named_modules():\n+            if isinstance(module, FP8Linear):\n+                # from 16 bits to 8 bits\n+                assert int(model_size[f\"{name}.weight\"] // 2) == int(quantized_model_size[f\"{name}.weight\"])\n+\n+        # check that we get the same value, as we use `compute_module_sizes` in `get_total_byte_count`\n+        assert total_byte_count == model_size[\"\"]\n+        assert quantized_total_byte_count == quantized_model_size[\"\"]\n+\n+        # we should at least have 1.5 times memory reduction in total\n+        assert model_size[\"\"] > quantized_model_size[\"\"] * 1.5\n+\n \n @require_torch_accelerator\n @unittest.skipIf("
        },
        {
            "sha": "fc9464532227cdeca3242168be8fddd24c19b207",
            "filename": "tests/quantization/mxfp4/test_mxfp4.py",
            "status": "modified",
            "additions": 52,
            "deletions": 33,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/tests%2Fquantization%2Fmxfp4%2Ftest_mxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/tests%2Fquantization%2Fmxfp4%2Ftest_mxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fmxfp4%2Ftest_mxfp4.py?ref=dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5",
            "patch": "@@ -26,6 +26,7 @@\n     require_torch_large_accelerator,\n     require_triton,\n     slow,\n+    torch_device,\n )\n from transformers.utils import (\n     is_torch_available,\n@@ -225,39 +226,6 @@ def test_quantizer_validation_missing_triton_pre_quantized_no_dequantize(self):\n             quantizer.validate_environment()\n             self.assertTrue(quantizer.quantization_config.dequantize)\n \n-    def test_get_param_name_dequantize(self):\n-        \"\"\"Test parameter name updating when dequantizing\"\"\"\n-        from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n-\n-        config = Mxfp4Config(dequantize=True)\n-        quantizer = Mxfp4HfQuantizer(config)\n-\n-        # Should remove _blocks suffix\n-        param_name = \"model.layers.0.mlp.experts.gate_up_proj_blocks\"\n-        updated_name = quantizer.get_param_name(param_name)\n-        self.assertEqual(updated_name, \"model.layers.0.mlp.experts.gate_up_proj\")\n-\n-        # Should remove _scales suffix\n-        param_name = \"model.layers.0.mlp.experts.down_proj_scales\"\n-        updated_name = quantizer.get_param_name(param_name)\n-        self.assertEqual(updated_name, \"model.layers.0.mlp.experts.down_proj\")\n-\n-        # Should not change other names\n-        param_name = \"model.embed_tokens.weight\"\n-        updated_name = quantizer.get_param_name(param_name)\n-        self.assertEqual(updated_name, \"model.embed_tokens.weight\")\n-\n-    def test_get_param_name_no_dequantize(self):\n-        \"\"\"Test parameter name updating when not dequantizing\"\"\"\n-        from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n-\n-        config = Mxfp4Config(dequantize=False)\n-        quantizer = Mxfp4HfQuantizer(config)\n-\n-        param_name = \"model.layers.0.mlp.experts.gate_up_proj_blocks\"\n-        updated_name = quantizer.get_param_name(param_name)\n-        self.assertEqual(updated_name, param_name)\n-\n     def test_is_trainable(self):\n         \"\"\"Test trainability\"\"\"\n         from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n@@ -486,3 +454,54 @@ def test_save_mxfp4_non_quantized(self):\n                 device_map=\"auto\",\n             )\n             self.check_inference_correctness_quantized(loaded_model, tokenizer)\n+\n+    def test_compute_module_sizes(self):\n+        r\"\"\"\n+        Test if we compute the right module sizes needed to generate the device map.\n+        Also test if we get the right values for `total_byte_count` in `caching_allocator_warmup`.\n+        \"\"\"\n+        from transformers import AutoConfig, AutoModelForCausalLM\n+        from transformers.integrations import Mxfp4GptOssExperts\n+        from transformers.integrations.accelerate import compute_module_sizes\n+        from transformers.modeling_utils import expand_device_map, get_total_byte_count\n+        from transformers.quantizers import AutoHfQuantizer\n+\n+        # we need to preprocess the model like that because device_map calculation happens before we load the weights inside the model.\n+        # For normal wieghts, it's fine but for quantized weights, the tensors dtype might change during loading.\n+        with torch.device(\"meta\"):\n+            config = AutoConfig.from_pretrained(self.model_name)\n+            model = AutoModelForCausalLM.from_config(config, dtype=torch.bfloat16)\n+            model_size, _ = compute_module_sizes(model, only_modules=False)\n+\n+            expected_keys = [name for name, _ in model.named_parameters()] + [\n+                name for name, _ in model.named_buffers()\n+            ]\n+            expanded_device_map = expand_device_map({\"\": torch_device}, expected_keys)\n+            total_byte_count = list(get_total_byte_count(model, expanded_device_map).values())[0]\n+\n+            # testing prequantized = False should be enough, the shape should be the same whether it is pre-quantized or not\n+            hf_quantizer = AutoHfQuantizer.from_config(Mxfp4Config(), pre_quantized=False)\n+            hf_quantizer.preprocess_model(model=model, config=model.config)\n+            quantized_model_size, _ = compute_module_sizes(model, hf_quantizer, only_modules=False)\n+\n+            expected_keys = [name for name, _ in model.named_parameters()] + [\n+                name for name, _ in model.named_buffers()\n+            ]\n+            expanded_device_map = expand_device_map({\"\": torch_device}, expected_keys)\n+            quantized_total_byte_count = list(get_total_byte_count(model, expanded_device_map, hf_quantizer).values())[\n+                0\n+            ]\n+        for name, module in model.named_modules():\n+            if isinstance(module, Mxfp4GptOssExperts):\n+                # from 16 bits to 4 bits\n+                assert int(model_size[f\"{name}.gate_up_proj\"] // 4) == int(\n+                    quantized_model_size[f\"{name}.gate_up_proj\"]\n+                )\n+                assert int(model_size[f\"{name}.down_proj\"] // 4) == int(quantized_model_size[f\"{name}.down_proj\"])\n+\n+        # check that we get the same value, as we use `compute_module_sizes` in `get_total_byte_count`\n+        assert total_byte_count == model_size[\"\"]\n+        assert quantized_total_byte_count == quantized_model_size[\"\"]\n+\n+        # we should at least have 3 times memory reduction in total for this model\n+        assert model_size[\"\"] > quantized_model_size[\"\"] * 3"
        },
        {
            "sha": "84198ff19ead6505ca3b344aae6efceb5b7004d7",
            "filename": "tests/quantization/quanto_integration/test_quanto.py",
            "status": "modified",
            "additions": 48,
            "deletions": 0,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py?ref=dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5",
            "patch": "@@ -238,6 +238,54 @@ def test_compare_with_quanto(self):\n         self.check_same_model(model, self.quantized_model)\n         self.check_inference_correctness(model, device=torch_device)\n \n+    def test_compute_module_sizes(self):\n+        r\"\"\"\n+        Test if we compute the right module sizes needed to generate the device map.\n+        Also test if we get the right values for `total_byte_count` in `caching_allocator_warmup`.\n+        Note that `compute_module_sizes` is being used in `get_total_byte_count`\n+        \"\"\"\n+        from transformers.integrations.accelerate import compute_module_sizes\n+        from transformers.modeling_utils import expand_device_map, get_total_byte_count\n+        from transformers.quantizers import AutoHfQuantizer\n+\n+        # we need to preprocess the model like that because device_map calculation happens before we load the weights inside the model.\n+        # For normal wieghts, it's fine but for quantized weights, the tensors dtype might change during loading.\n+        with torch.device(\"meta\"):\n+            config = AutoConfig.from_pretrained(self.model_name)\n+            model = AutoModelForCausalLM.from_config(config, dtype=torch.bfloat16)\n+            model_size, _ = compute_module_sizes(model, only_modules=False)\n+\n+            expected_keys = [name for name, _ in model.named_parameters()] + [\n+                name for name, _ in model.named_buffers()\n+            ]\n+            expanded_device_map = expand_device_map({\"\": torch_device}, expected_keys)\n+            total_byte_count = list(get_total_byte_count(model, expanded_device_map).values())[0]\n+\n+            # testing prequantized = False should be enough, the shape should be the same whether it is pre-quantized or not\n+            hf_quantizer = AutoHfQuantizer.from_config(QuantoConfig(weights=\"int4\"), pre_quantized=False)\n+            hf_quantizer.preprocess_model(model=model, config=model.config)\n+            quantized_model_size, _ = compute_module_sizes(model, hf_quantizer, only_modules=False)\n+\n+            expected_keys = [name for name, _ in model.named_parameters()] + [\n+                name for name, _ in model.named_buffers()\n+            ]\n+            expanded_device_map = expand_device_map({\"\": torch_device}, expected_keys)\n+            quantized_total_byte_count = list(get_total_byte_count(model, expanded_device_map, hf_quantizer).values())[\n+                0\n+            ]\n+\n+        for name, module in model.named_modules():\n+            if isinstance(module, torch.nn.Linear) and \"lm_head\" not in name:\n+                # from 16 bits to 4 bits\n+                assert int(model_size[f\"{name}.weight\"] // 4) == int(quantized_model_size[f\"{name}.weight\"])\n+\n+        # check that we get the same value, as we use `compute_module_sizes` in `get_total_byte_count`\n+        assert total_byte_count == model_size[\"\"]\n+        assert quantized_total_byte_count == quantized_model_size[\"\"]\n+\n+        # we should at least have 1.5 times memory reduction in total\n+        assert model_size[\"\"] > quantized_model_size[\"\"] * 1.5\n+\n \n class QuantoQuantizationQBitsTensorTest(QuantoQuantizationTest):\n     EXPECTED_OUTPUTS = \"Hello my name is joe and i am a little girl\\n\\n\""
        },
        {
            "sha": "49858ba4ab5866ebb21498f50dac91f6680b2517",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 51,
            "deletions": 0,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=dd8057afa6b2dcc8f3d7c657f612bb0e494c43c5",
            "patch": "@@ -540,6 +540,57 @@ def test_fqn_to_config_non_weight_param(self):\n         )\n         self.assertTrue(isinstance(quantized_model.model.layers[1].self_attn.q_proj.weight, AffineQuantizedTensor))\n \n+    def test_compute_module_sizes(self):\n+        r\"\"\"\n+        Test if we compute the right module sizes needed to generate the device map.\n+        Also test if we get the right values for `total_byte_count` in `caching_allocator_warmup`.\n+        \"\"\"\n+        from transformers import AutoConfig\n+        from transformers.integrations.accelerate import compute_module_sizes\n+        from transformers.modeling_utils import expand_device_map, get_total_byte_count\n+        from transformers.quantizers import AutoHfQuantizer\n+\n+        # we need to preprocess the model like that because device_map calculation happens before we load the weights inside the model.\n+        # For normal wieghts, it's fine but for quantized weights, the tensors dtype might change during loading.\n+        with torch.device(\"meta\"):\n+            config = AutoConfig.from_pretrained(self.model_name)\n+            model = AutoModelForCausalLM.from_config(config, dtype=torch.bfloat16)\n+            model_size, _ = compute_module_sizes(model, only_modules=False)\n+\n+            expected_keys = [name for name, _ in model.named_parameters()] + [\n+                name for name, _ in model.named_buffers()\n+            ]\n+            expanded_device_map = expand_device_map({\"\": torch_device}, expected_keys)\n+            total_byte_count = list(get_total_byte_count(model, expanded_device_map).values())[0]\n+\n+            # testing prequantized = False should be enough, the shape should be the same whether it is pre-quantized or not\n+            hf_quantizer = AutoHfQuantizer.from_config(\n+                TorchAoConfig(quant_type=Int4WeightOnlyConfig(**self.quant_scheme_kwargs)), pre_quantized=False\n+            )\n+            hf_quantizer.preprocess_model(model=model, config=model.config)\n+            quantized_model_size, _ = compute_module_sizes(model, hf_quantizer, only_modules=False)\n+\n+            expected_keys = [name for name, _ in model.named_parameters()] + [\n+                name for name, _ in model.named_buffers()\n+            ]\n+            expanded_device_map = expand_device_map({\"\": torch_device}, expected_keys)\n+            quantized_total_byte_count = list(get_total_byte_count(model, expanded_device_map, hf_quantizer).values())[\n+                0\n+            ]\n+\n+        for name, module in model.named_modules():\n+            # modules are not replaced when using torchao\n+            if isinstance(module, torch.nn.Linear) and \"lm_head\" not in name:\n+                # from 16 bits to 4 bits\n+                assert int(model_size[f\"{name}.weight\"] // 4) == int(quantized_model_size[f\"{name}.weight\"])\n+\n+        # check that we get the same value, as we use `compute_module_sizes` in `get_total_byte_count`\n+        assert total_byte_count == model_size[\"\"]\n+        assert quantized_total_byte_count == quantized_model_size[\"\"]\n+\n+        # we should at least have 1.5 times memory reduction in total\n+        assert model_size[\"\"] > quantized_model_size[\"\"] * 2\n+\n \n @require_torch_accelerator\n class TorchAoAcceleratorTest(TorchAoTest):"
        }
    ],
    "stats": {
        "total": 665,
        "additions": 399,
        "deletions": 266
    }
}