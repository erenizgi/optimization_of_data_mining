{
    "author": "martin0258",
    "message": "Update perf_infer_gpu_one.md: fix a typo (#35441)",
    "sha": "90f256c90c927a804847bb825b145b282d81b495",
    "files": [
        {
            "sha": "1b7b2a39b67e053151b1d70adfa489f0af58b121",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/90f256c90c927a804847bb825b145b282d81b495/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/90f256c90c927a804847bb825b145b282d81b495/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=90f256c90c927a804847bb825b145b282d81b495",
            "patch": "@@ -462,7 +462,7 @@ generated_ids = model.generate(**inputs)\n outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n ```\n \n-To load a model in 4-bit for inference with multiple GPUs, you can control how much GPU RAM you want to allocate to each GPU. For example, to distribute 1GB of memory to the first GPU and 2GB of memory to the second GPU:\n+To load a model in 8-bit for inference with multiple GPUs, you can control how much GPU RAM you want to allocate to each GPU. For example, to distribute 1GB of memory to the first GPU and 2GB of memory to the second GPU:\n \n ```py\n max_memory_mapping = {0: \"1GB\", 1: \"2GB\"}"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}