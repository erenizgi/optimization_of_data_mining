{
    "author": "vijayabhaskar-ev",
    "message": "Add dinov3 autobackbone (#41276)\n\n* feat: Add DINOv3 support to AutoBackbone [DRAFT]\n\n- Implement DINOv3ViTConfig, DINOv3ViTModel, and DINOv3ViTBackbone\n- Add DINOv3 to MODEL_FOR_BACKBONE_MAPPING_NAMES\n- Support get_intermediate_layers for Facebook compatibility\n- Enable multi-scale feature extraction for detection/segmentation\n\nNote: Tests and documentation coming in follow-up commits\nAddresses #40323\n\n* Updated import structure of get_aligned_output_features_output_indices\n\n* Added test for DINOv3ViTBackbone\n\n* Add DINOv3ViTBackbone to model documentation\n\n* Refactored the code to adhere to the Transformers principles\n\n* Generated modeling_dinov3_vit.py\n\n* DINOv3ViT backbone: keep hidden_states with return_dict=False, add @check_model_inputs and polish docs\n\n- Add @check_model_inputs to DINOv3ViTBackbone.forward to normalize flags and enable output recording.\n- Preserve hidden_states when return_dict=False by appending them to the tuple output when requested.\n- Clean up config docstring formatting (consistent indentation and use list[...] types).\n\n* Restructure DINOv3 backbone and update its tests\n\n* Resolved merge conflicts\n\n* Resolved failing testcase\n\n* Fix DINOv3 backbone to use self.norm for feature maps\n\n---------\n\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>",
    "sha": "496c283615dc1aaf0863089b16d55abbf17ac681",
    "files": [
        {
            "sha": "0eadc7c4cb4dd884649cc978608b8e595f19f149",
            "filename": "docs/source/en/model_doc/dinov3.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/496c283615dc1aaf0863089b16d55abbf17ac681/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/496c283615dc1aaf0863089b16d55abbf17ac681/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov3.md?ref=496c283615dc1aaf0863089b16d55abbf17ac681",
            "patch": "@@ -169,6 +169,9 @@ print(\"Pooled output shape:\", pooled_output.shape)\n [[autodoc]] DINOv3ViTModel\n     - forward\n \n+## DINOv3ViTBackbone    \n+[[autodoc]] DINOv3ViTBackbone\n+\n ## DINOv3ConvNextModel\n \n [[autodoc]] DINOv3ConvNextModel"
        },
        {
            "sha": "dc01afa05a85b42d313f962e9e4d29733ca4c715",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/496c283615dc1aaf0863089b16d55abbf17ac681/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/496c283615dc1aaf0863089b16d55abbf17ac681/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=496c283615dc1aaf0863089b16d55abbf17ac681",
            "patch": "@@ -1700,6 +1700,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"dinov2\", \"Dinov2Backbone\"),\n         (\"dinov2_with_registers\", \"Dinov2WithRegistersBackbone\"),\n         (\"dinov3_convnext\", \"DINOv3ConvNextBackbone\"),\n+        (\"dinov3_vit\", \"DINOv3ViTBackbone\"),\n         (\"focalnet\", \"FocalNetBackbone\"),\n         (\"hgnet_v2\", \"HGNetV2Backbone\"),\n         (\"hiera\", \"HieraBackbone\"),"
        },
        {
            "sha": "e189f599b2ed0281cf8adc5b7ea6f57257c434d6",
            "filename": "src/transformers/models/dinov3_vit/configuration_dinov3_vit.py",
            "status": "modified",
            "additions": 28,
            "deletions": 1,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/496c283615dc1aaf0863089b16d55abbf17ac681/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fconfiguration_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/496c283615dc1aaf0863089b16d55abbf17ac681/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fconfiguration_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fconfiguration_dinov3_vit.py?ref=496c283615dc1aaf0863089b16d55abbf17ac681",
            "patch": "@@ -18,12 +18,13 @@\n \n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n+from ...utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n \n \n logger = logging.get_logger(__name__)\n \n \n-class DINOv3ViTConfig(PreTrainedConfig):\n+class DINOv3ViTConfig(BackboneConfigMixin, PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DINOv3Model`]. It is used to instantiate an\n     DINOv3 model according to the specified arguments, defining the model architecture. Instantiating a configuration\n@@ -86,6 +87,16 @@ class DINOv3ViTConfig(PreTrainedConfig):\n         pos_embed_rescale (`float`, *optional*, defaults to 2.0):\n             Amount to randomly rescale position embedding coordinates in log-uniform value in [1/rescale, rescale],\n             applied only in training mode if not `None`.\n+        out_features (`list[str]`, *optional*):\n+            If used as backbone, list of features to output. Can be any of `\"stem\"`, `\"stage1\"`, `\"stage2\"`, etc.\n+            (depending on how many stages the model has). Will default to the last stage if unset.\n+        out_indices (`list[int]`, *optional*):\n+            If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc.\n+            (depending on how many stages the model has). Will default to the last stage if unset.\n+        apply_layernorm (`bool`, *optional*, defaults to `True`):\n+            Whether to apply layer normalization to the feature maps when used as backbone.\n+        reshape_hidden_states (`bool`, *optional*, defaults to `True`):\n+            Whether to reshape the hidden states to spatial dimensions when used as backbone.\n \n     Example:\n \n@@ -131,6 +142,10 @@ def __init__(\n         pos_embed_shift: Optional[float] = None,\n         pos_embed_jitter: Optional[float] = None,\n         pos_embed_rescale: Optional[float] = 2.0,\n+        out_features: Optional[list[str]] = None,\n+        out_indices: Optional[list[int]] = None,\n+        apply_layernorm: bool = True,\n+        reshape_hidden_states: bool = True,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -161,6 +176,18 @@ def __init__(\n         self.pos_embed_shift = pos_embed_shift\n         self.pos_embed_jitter = pos_embed_jitter\n         self.pos_embed_rescale = pos_embed_rescale\n+        # Initialize backbone-specific configuration\n+        self.apply_layernorm = apply_layernorm\n+        self.reshape_hidden_states = reshape_hidden_states\n+\n+        # Initialize backbone stage names\n+        stage_names = [\"stem\"] + [f\"stage{i}\" for i in range(1, num_hidden_layers + 1)]\n+        self.stage_names = stage_names\n+\n+        # Initialize backbone features/indices\n+        self._out_features, self._out_indices = get_aligned_output_features_output_indices(\n+            out_features=out_features, out_indices=out_indices, stage_names=stage_names\n+        )\n \n \n __all__ = [\"DINOv3ViTConfig\"]"
        },
        {
            "sha": "c1b7868f097997a40f71d6a2c59016e6398f0d7b",
            "filename": "src/transformers/models/dinov3_vit/modeling_dinov3_vit.py",
            "status": "modified",
            "additions": 77,
            "deletions": 7,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/496c283615dc1aaf0863089b16d55abbf17ac681/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/496c283615dc1aaf0863089b16d55abbf17ac681/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py?ref=496c283615dc1aaf0863089b16d55abbf17ac681",
            "patch": "@@ -29,11 +29,12 @@\n \n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import BaseModelOutputWithPooling\n+from ...modeling_outputs import BackboneOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import compile_compatible_method_lru_cache\n-from ...utils import TransformersKwargs, auto_docstring\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.backbone_utils import BackboneMixin\n from ...utils.generic import check_model_inputs\n from .configuration_dinov3_vit import DINOv3ViTConfig\n \n@@ -522,10 +523,79 @@ def forward(\n         sequence_output = self.norm(hidden_states)\n         pooled_output = sequence_output[:, 0, :]\n \n-        return BaseModelOutputWithPooling(\n-            last_hidden_state=sequence_output,\n-            pooler_output=pooled_output,\n-        )\n+        return BaseModelOutputWithPooling(last_hidden_state=sequence_output, pooler_output=pooled_output)\n+\n+\n+@auto_docstring\n+class DINOv3ViTBackbone(DINOv3ViTPreTrainedModel, BackboneMixin):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        super()._init_backbone(config)\n+\n+        self.embeddings = DINOv3ViTEmbeddings(config)\n+        self.rope_embeddings = DINOv3ViTRopePositionEmbedding(config)\n+        self.layer = nn.ModuleList([DINOv3ViTLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.gradient_checkpointing = False\n+\n+        self.num_features = [config.hidden_size for _ in range(config.num_hidden_layers + 1)]\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embeddings.patch_embeddings\n+\n+    @check_model_inputs()\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        pixel_values: torch.Tensor,\n+        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BackboneOutput:\n+        pixel_values = pixel_values.to(self.embeddings.patch_embeddings.weight.dtype)\n+        hidden_states = self.embeddings(pixel_values)\n+        position_embeddings = self.rope_embeddings(pixel_values)\n+\n+        stage_hidden_states: list[torch.Tensor] = [hidden_states]\n+\n+        for layer_module in self.layer:\n+            hidden_states = layer_module(hidden_states, position_embeddings=position_embeddings)\n+            stage_hidden_states.append(hidden_states)\n+\n+        batch_size, _, image_height, image_width = pixel_values.shape\n+        patch_size = self.config.patch_size\n+        num_patches_height = image_height // patch_size\n+        num_patches_width = image_width // patch_size\n+\n+        num_prefix = 1 + getattr(self.config, \"num_register_tokens\", 0)\n+\n+        feature_maps = []\n+        sequence_output = None\n+        last_stage_idx = len(self.stage_names) - 1\n+        for idx, (stage_name, hidden_state) in enumerate(zip(self.stage_names, stage_hidden_states)):\n+            if idx == last_stage_idx:\n+                hidden_state = self.norm(hidden_state)\n+                sequence_output = hidden_state\n+            elif self.config.apply_layernorm:\n+                hidden_state = self.norm(hidden_state)\n+\n+            if stage_name in self.out_features:\n+                patch_tokens = hidden_state[:, num_prefix:, :]\n+                if self.config.reshape_hidden_states:\n+                    fmap = (\n+                        patch_tokens.reshape(batch_size, num_patches_height, num_patches_width, patch_tokens.shape[-1])\n+                        .permute(0, 3, 1, 2)\n+                        .contiguous()\n+                    )\n+                else:\n+                    fmap = patch_tokens\n+\n+                feature_maps.append(fmap)\n+\n+        output = BackboneOutput(feature_maps=tuple(feature_maps))\n+        output.last_hidden_state = sequence_output\n+\n+        return output\n \n \n-__all__ = [\"DINOv3ViTModel\", \"DINOv3ViTPreTrainedModel\"]\n+__all__ = [\"DINOv3ViTModel\", \"DINOv3ViTPreTrainedModel\", \"DINOv3ViTBackbone\"]"
        },
        {
            "sha": "6c4a4b13fcc544b194ac87bbcc6a7a816b03a8a4",
            "filename": "src/transformers/models/dinov3_vit/modular_dinov3_vit.py",
            "status": "modified",
            "additions": 77,
            "deletions": 7,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/496c283615dc1aaf0863089b16d55abbf17ac681/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodular_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/496c283615dc1aaf0863089b16d55abbf17ac681/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodular_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodular_dinov3_vit.py?ref=496c283615dc1aaf0863089b16d55abbf17ac681",
            "patch": "@@ -33,11 +33,12 @@\n from transformers.models.pixtral.modeling_pixtral import PixtralAttention, rotate_half\n \n from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import BaseModelOutputWithPooling\n+from ...modeling_outputs import BackboneOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...pytorch_utils import compile_compatible_method_lru_cache\n-from ...utils import TransformersKwargs, auto_docstring, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.backbone_utils import BackboneMixin\n from ...utils.generic import check_model_inputs\n from .configuration_dinov3_vit import DINOv3ViTConfig\n \n@@ -417,10 +418,79 @@ def forward(\n         sequence_output = self.norm(hidden_states)\n         pooled_output = sequence_output[:, 0, :]\n \n-        return BaseModelOutputWithPooling(\n-            last_hidden_state=sequence_output,\n-            pooler_output=pooled_output,\n-        )\n+        return BaseModelOutputWithPooling(last_hidden_state=sequence_output, pooler_output=pooled_output)\n+\n+\n+@auto_docstring\n+class DINOv3ViTBackbone(DINOv3ViTPreTrainedModel, BackboneMixin):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        super()._init_backbone(config)\n+\n+        self.embeddings = DINOv3ViTEmbeddings(config)\n+        self.rope_embeddings = DINOv3ViTRopePositionEmbedding(config)\n+        self.layer = nn.ModuleList([DINOv3ViTLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.gradient_checkpointing = False\n+\n+        self.num_features = [config.hidden_size for _ in range(config.num_hidden_layers + 1)]\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embeddings.patch_embeddings\n+\n+    @check_model_inputs()\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        pixel_values: torch.Tensor,\n+        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BackboneOutput:\n+        pixel_values = pixel_values.to(self.embeddings.patch_embeddings.weight.dtype)\n+        hidden_states = self.embeddings(pixel_values)\n+        position_embeddings = self.rope_embeddings(pixel_values)\n+\n+        stage_hidden_states: list[torch.Tensor] = [hidden_states]\n+\n+        for layer_module in self.layer:\n+            hidden_states = layer_module(hidden_states, position_embeddings=position_embeddings)\n+            stage_hidden_states.append(hidden_states)\n+\n+        batch_size, _, image_height, image_width = pixel_values.shape\n+        patch_size = self.config.patch_size\n+        num_patches_height = image_height // patch_size\n+        num_patches_width = image_width // patch_size\n+\n+        num_prefix = 1 + getattr(self.config, \"num_register_tokens\", 0)\n+\n+        feature_maps = []\n+        sequence_output = None\n+        last_stage_idx = len(self.stage_names) - 1\n+        for idx, (stage_name, hidden_state) in enumerate(zip(self.stage_names, stage_hidden_states)):\n+            if idx == last_stage_idx:\n+                hidden_state = self.norm(hidden_state)\n+                sequence_output = hidden_state\n+            elif self.config.apply_layernorm:\n+                hidden_state = self.norm(hidden_state)\n+\n+            if stage_name in self.out_features:\n+                patch_tokens = hidden_state[:, num_prefix:, :]\n+                if self.config.reshape_hidden_states:\n+                    fmap = (\n+                        patch_tokens.reshape(batch_size, num_patches_height, num_patches_width, patch_tokens.shape[-1])\n+                        .permute(0, 3, 1, 2)\n+                        .contiguous()\n+                    )\n+                else:\n+                    fmap = patch_tokens\n+\n+                feature_maps.append(fmap)\n+\n+        output = BackboneOutput(feature_maps=tuple(feature_maps))\n+        output.last_hidden_state = sequence_output\n+\n+        return output\n \n \n-__all__ = [\"DINOv3ViTModel\", \"DINOv3ViTPreTrainedModel\"]\n+__all__ = [\"DINOv3ViTModel\", \"DINOv3ViTPreTrainedModel\", \"DINOv3ViTBackbone\"]"
        },
        {
            "sha": "c5997e97e8315e7c2d4d506a67c0f19352279328",
            "filename": "tests/models/dinov3_vit/test_modeling_dinov3_vit.py",
            "status": "modified",
            "additions": 52,
            "deletions": 2,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/496c283615dc1aaf0863089b16d55abbf17ac681/tests%2Fmodels%2Fdinov3_vit%2Ftest_modeling_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/496c283615dc1aaf0863089b16d55abbf17ac681/tests%2Fmodels%2Fdinov3_vit%2Ftest_modeling_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov3_vit%2Ftest_modeling_dinov3_vit.py?ref=496c283615dc1aaf0863089b16d55abbf17ac681",
            "patch": "@@ -29,7 +29,7 @@\n     import torch\n     from torch import nn\n \n-    from transformers import DINOv3ViTModel\n+    from transformers import DINOv3ViTBackbone, DINOv3ViTModel\n \n \n if is_vision_available():\n@@ -112,8 +112,53 @@ def get_config(self):\n             is_decoder=False,\n             initializer_range=self.initializer_range,\n             num_register_tokens=self.num_register_tokens,\n+            stage_names=[\"embeddings\"] + [f\"stage{i}\" for i in range(1, self.num_hidden_layers + 1)],\n+            out_indices=[0, 1],\n+            reshape_hidden_states=True,\n         )\n \n+    def create_and_check_backbone(self, config, pixel_values, labels):\n+        config.out_features = [\"stage1\", \"stage2\"]\n+        config.reshape_hidden_states = True\n+\n+        model = DINOv3ViTBackbone(config)\n+        model.to(torch_device)\n+        model.eval()\n+\n+        with torch.no_grad():\n+            outputs = model(pixel_values)\n+\n+        self.parent.assertEqual(len(outputs.feature_maps), 2)\n+        for fm in outputs.feature_maps:\n+            b, c, h, w = fm.shape\n+            self.parent.assertEqual(b, self.batch_size)\n+            self.parent.assertEqual(c, self.hidden_size)\n+            self.parent.assertGreater(h, 0)\n+            self.parent.assertGreater(w, 0)\n+\n+    def test_output_hidden_states(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                outputs = model(**inputs_dict, output_hidden_states=True)\n+\n+            self.assertIsNotNone(outputs.hidden_states)\n+            expected_num_hidden_states = config.num_hidden_layers + 1\n+            self.assertEqual(len(outputs.hidden_states), expected_num_hidden_states)\n+\n+            for hidden_state in outputs.hidden_states:\n+                expected_shape = (\n+                    self.model_tester.batch_size,\n+                    self.model_tester.seq_length,\n+                    self.model_tester.hidden_size,\n+                )\n+                self.assertEqual(hidden_state.shape, expected_shape)\n+\n     def create_and_check_model(self, config, pixel_values, labels):\n         model = DINOv3ViTModel(config=config)\n         model.to(torch_device)\n@@ -142,7 +187,7 @@ class Dinov3ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     attention_mask and seq_length.\n     \"\"\"\n \n-    all_model_classes = (DINOv3ViTModel,) if is_torch_available() else ()\n+    all_model_classes = (DINOv3ViTModel, DINOv3ViTBackbone) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"image-feature-extraction\": DINOv3ViTModel,\n@@ -153,11 +198,16 @@ class Dinov3ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n \n     test_resize_embeddings = False\n     test_torch_exportable = True\n+    test_attention_outputs = False\n \n     def setUp(self):\n         self.model_tester = DINOv3ViTModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=DINOv3ViTConfig, has_text_modality=False, hidden_size=37)\n \n+    def test_backbone(self):\n+        config, pixel_values, labels = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_backbone(config, pixel_values, labels)\n+\n     def test_config(self):\n         self.config_tester.run_common_tests()\n "
        }
    ],
    "stats": {
        "total": 255,
        "additions": 238,
        "deletions": 17
    }
}