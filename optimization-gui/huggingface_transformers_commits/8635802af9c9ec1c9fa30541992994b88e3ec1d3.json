{
    "author": "g-prz",
    "message": "Move weight initilization deformabledetr (#33339)\n\n* fix(copy): fixup copy\r\n\r\n* fix(deformable_detr): move weight initialization to the right place\r\n\r\n* fix(grounding_dino): move weight initialization to the right place\r\n\r\n* fix(rt_detr): move weight initialization to the right place\r\n\r\n* [run-slow] deformable_detr, grounding_dino, rt_detr",
    "sha": "8635802af9c9ec1c9fa30541992994b88e3ec1d3",
    "files": [
        {
            "sha": "1084e7136a428f4df8371a943943b9540a10e937",
            "filename": "src/transformers/models/deformable_detr/modeling_deformable_detr.py",
            "status": "modified",
            "additions": 21,
            "deletions": 24,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/8635802af9c9ec1c9fa30541992994b88e3ec1d3/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8635802af9c9ec1c9fa30541992994b88e3ec1d3/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py?ref=8635802af9c9ec1c9fa30541992994b88e3ec1d3",
            "patch": "@@ -660,29 +660,6 @@ def __init__(self, config: DeformableDetrConfig, num_heads: int, n_points: int):\n \n         self.disable_custom_kernels = config.disable_custom_kernels\n \n-        self._reset_parameters()\n-\n-    def _reset_parameters(self):\n-        nn.init.constant_(self.sampling_offsets.weight.data, 0.0)\n-        default_dtype = torch.get_default_dtype()\n-        thetas = torch.arange(self.n_heads, dtype=torch.int64).to(default_dtype) * (2.0 * math.pi / self.n_heads)\n-        grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n-        grid_init = (\n-            (grid_init / grid_init.abs().max(-1, keepdim=True)[0])\n-            .view(self.n_heads, 1, 1, 2)\n-            .repeat(1, self.n_levels, self.n_points, 1)\n-        )\n-        for i in range(self.n_points):\n-            grid_init[:, :, i, :] *= i + 1\n-        with torch.no_grad():\n-            self.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n-        nn.init.constant_(self.attention_weights.weight.data, 0.0)\n-        nn.init.constant_(self.attention_weights.bias.data, 0.0)\n-        nn.init.xavier_uniform_(self.value_proj.weight.data)\n-        nn.init.constant_(self.value_proj.bias.data, 0.0)\n-        nn.init.xavier_uniform_(self.output_proj.weight.data)\n-        nn.init.constant_(self.output_proj.bias.data, 0.0)\n-\n     def with_pos_embed(self, tensor: torch.Tensor, position_embeddings: Optional[Tensor]):\n         return tensor if position_embeddings is None else tensor + position_embeddings\n \n@@ -1088,7 +1065,27 @@ def _init_weights(self, module):\n             nn.init.uniform_(module.row_embeddings.weight)\n             nn.init.uniform_(module.column_embeddings.weight)\n         elif isinstance(module, DeformableDetrMultiscaleDeformableAttention):\n-            module._reset_parameters()\n+            nn.init.constant_(module.sampling_offsets.weight.data, 0.0)\n+            default_dtype = torch.get_default_dtype()\n+            thetas = torch.arange(module.n_heads, dtype=torch.int64).to(default_dtype) * (\n+                2.0 * math.pi / module.n_heads\n+            )\n+            grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n+            grid_init = (\n+                (grid_init / grid_init.abs().max(-1, keepdim=True)[0])\n+                .view(module.n_heads, 1, 1, 2)\n+                .repeat(1, module.n_levels, module.n_points, 1)\n+            )\n+            for i in range(module.n_points):\n+                grid_init[:, :, i, :] *= i + 1\n+            with torch.no_grad():\n+                module.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n+            nn.init.constant_(module.attention_weights.weight.data, 0.0)\n+            nn.init.constant_(module.attention_weights.bias.data, 0.0)\n+            nn.init.xavier_uniform_(module.value_proj.weight.data)\n+            nn.init.constant_(module.value_proj.bias.data, 0.0)\n+            nn.init.xavier_uniform_(module.output_proj.weight.data)\n+            nn.init.constant_(module.output_proj.bias.data, 0.0)\n         elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n             # Slightly different from the TF version which uses truncated_normal for initialization\n             # cf https://github.com/pytorch/pytorch/pull/5617"
        },
        {
            "sha": "08e4b27af64d7dc416d66f0793208ba090365aef",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 21,
            "deletions": 24,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/8635802af9c9ec1c9fa30541992994b88e3ec1d3/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8635802af9c9ec1c9fa30541992994b88e3ec1d3/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=8635802af9c9ec1c9fa30541992994b88e3ec1d3",
            "patch": "@@ -664,29 +664,6 @@ def __init__(self, config: GroundingDinoConfig, num_heads: int, n_points: int):\n \n         self.disable_custom_kernels = config.disable_custom_kernels\n \n-        self._reset_parameters()\n-\n-    def _reset_parameters(self):\n-        nn.init.constant_(self.sampling_offsets.weight.data, 0.0)\n-        default_dtype = torch.get_default_dtype()\n-        thetas = torch.arange(self.n_heads, dtype=torch.int64).to(default_dtype) * (2.0 * math.pi / self.n_heads)\n-        grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n-        grid_init = (\n-            (grid_init / grid_init.abs().max(-1, keepdim=True)[0])\n-            .view(self.n_heads, 1, 1, 2)\n-            .repeat(1, self.n_levels, self.n_points, 1)\n-        )\n-        for i in range(self.n_points):\n-            grid_init[:, :, i, :] *= i + 1\n-        with torch.no_grad():\n-            self.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n-        nn.init.constant_(self.attention_weights.weight.data, 0.0)\n-        nn.init.constant_(self.attention_weights.bias.data, 0.0)\n-        nn.init.xavier_uniform_(self.value_proj.weight.data)\n-        nn.init.constant_(self.value_proj.bias.data, 0.0)\n-        nn.init.xavier_uniform_(self.output_proj.weight.data)\n-        nn.init.constant_(self.output_proj.bias.data, 0.0)\n-\n     def with_pos_embed(self, tensor: torch.Tensor, position_embeddings: Optional[Tensor]):\n         return tensor if position_embeddings is None else tensor + position_embeddings\n \n@@ -1509,7 +1486,27 @@ def _init_weights(self, module):\n             nn.init.uniform_(module.row_embeddings.weight)\n             nn.init.uniform_(module.column_embeddings.weight)\n         elif isinstance(module, GroundingDinoMultiscaleDeformableAttention):\n-            module._reset_parameters()\n+            nn.init.constant_(module.sampling_offsets.weight.data, 0.0)\n+            default_dtype = torch.get_default_dtype()\n+            thetas = torch.arange(module.n_heads, dtype=torch.int64).to(default_dtype) * (\n+                2.0 * math.pi / module.n_heads\n+            )\n+            grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n+            grid_init = (\n+                (grid_init / grid_init.abs().max(-1, keepdim=True)[0])\n+                .view(module.n_heads, 1, 1, 2)\n+                .repeat(1, module.n_levels, module.n_points, 1)\n+            )\n+            for i in range(module.n_points):\n+                grid_init[:, :, i, :] *= i + 1\n+            with torch.no_grad():\n+                module.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n+            nn.init.constant_(module.attention_weights.weight.data, 0.0)\n+            nn.init.constant_(module.attention_weights.bias.data, 0.0)\n+            nn.init.xavier_uniform_(module.value_proj.weight.data)\n+            nn.init.constant_(module.value_proj.bias.data, 0.0)\n+            nn.init.xavier_uniform_(module.output_proj.weight.data)\n+            nn.init.constant_(module.output_proj.bias.data, 0.0)\n         elif isinstance(module, GroundingDinoBiMultiHeadAttention):\n             nn.init.xavier_uniform_(module.vision_proj.weight)\n             module.vision_proj.bias.data.fill_(0)"
        },
        {
            "sha": "35af2ec8ecfb481dba09c5395c4fc2e1919c131c",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr.py",
            "status": "modified",
            "additions": 23,
            "deletions": 23,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/8635802af9c9ec1c9fa30541992994b88e3ec1d3/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8635802af9c9ec1c9fa30541992994b88e3ec1d3/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py?ref=8635802af9c9ec1c9fa30541992994b88e3ec1d3",
            "patch": "@@ -816,29 +816,6 @@ def __init__(self, config: RTDetrConfig, num_heads: int, n_points: int):\n \n         self.disable_custom_kernels = config.disable_custom_kernels\n \n-        self._reset_parameters()\n-\n-    def _reset_parameters(self):\n-        nn.init.constant_(self.sampling_offsets.weight.data, 0.0)\n-        default_dtype = torch.get_default_dtype()\n-        thetas = torch.arange(self.n_heads, dtype=torch.int64).to(default_dtype) * (2.0 * math.pi / self.n_heads)\n-        grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n-        grid_init = (\n-            (grid_init / grid_init.abs().max(-1, keepdim=True)[0])\n-            .view(self.n_heads, 1, 1, 2)\n-            .repeat(1, self.n_levels, self.n_points, 1)\n-        )\n-        for i in range(self.n_points):\n-            grid_init[:, :, i, :] *= i + 1\n-        with torch.no_grad():\n-            self.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n-        nn.init.constant_(self.attention_weights.weight.data, 0.0)\n-        nn.init.constant_(self.attention_weights.bias.data, 0.0)\n-        nn.init.xavier_uniform_(self.value_proj.weight.data)\n-        nn.init.constant_(self.value_proj.bias.data, 0.0)\n-        nn.init.xavier_uniform_(self.output_proj.weight.data)\n-        nn.init.constant_(self.output_proj.bias.data, 0.0)\n-\n     def with_pos_embed(self, tensor: torch.Tensor, position_embeddings: Optional[Tensor]):\n         return tensor if position_embeddings is None else tensor + position_embeddings\n \n@@ -1176,6 +1153,29 @@ def _init_weights(self, module):\n                     nn.init.constant_(layer.layers[-1].weight, 0)\n                     nn.init.constant_(layer.layers[-1].bias, 0)\n \n+        if isinstance(module, RTDetrMultiscaleDeformableAttention):\n+            nn.init.constant_(module.sampling_offsets.weight.data, 0.0)\n+            default_dtype = torch.get_default_dtype()\n+            thetas = torch.arange(module.n_heads, dtype=torch.int64).to(default_dtype) * (\n+                2.0 * math.pi / module.n_heads\n+            )\n+            grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n+            grid_init = (\n+                (grid_init / grid_init.abs().max(-1, keepdim=True)[0])\n+                .view(module.n_heads, 1, 1, 2)\n+                .repeat(1, module.n_levels, module.n_points, 1)\n+            )\n+            for i in range(module.n_points):\n+                grid_init[:, :, i, :] *= i + 1\n+            with torch.no_grad():\n+                module.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n+            nn.init.constant_(module.attention_weights.weight.data, 0.0)\n+            nn.init.constant_(module.attention_weights.bias.data, 0.0)\n+            nn.init.xavier_uniform_(module.value_proj.weight.data)\n+            nn.init.constant_(module.value_proj.bias.data, 0.0)\n+            nn.init.xavier_uniform_(module.output_proj.weight.data)\n+            nn.init.constant_(module.output_proj.bias.data, 0.0)\n+\n         if isinstance(module, RTDetrModel):\n             prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n             bias = float(-math.log((1 - prior_prob) / prior_prob))"
        }
    ],
    "stats": {
        "total": 136,
        "additions": 65,
        "deletions": 71
    }
}