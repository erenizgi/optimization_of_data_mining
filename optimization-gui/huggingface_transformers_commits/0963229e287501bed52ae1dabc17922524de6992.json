{
    "author": "SunMarc",
    "message": "Enable finetuning with torchao quantized model  (#33361)\n\nenable training",
    "sha": "0963229e287501bed52ae1dabc17922524de6992",
    "files": [
        {
            "sha": "02ea8294a2d54aa65d4e2158ade64b2967ba1738",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/0963229e287501bed52ae1dabc17922524de6992/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0963229e287501bed52ae1dabc17922524de6992/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=0963229e287501bed52ae1dabc17922524de6992",
            "patch": "@@ -166,7 +166,8 @@ def is_serializable(self):\n \n     @property\n     def is_trainable(self):\n-        # torchao does not have official support for QAT (Quantization Aware Training)\n-        # but torchao support nf4/PEFT, but it is not integrated yet\n-        # TODO: if this is supported in the future, do a version check here.\n-        return False\n+        supported_quant_types_for_training = [\n+            \"int8_weight_only\",\n+            \"int8_dynamic_activation_int8_weight\",\n+        ]\n+        return self.quantization_config.quant_type in supported_quant_types_for_training"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 5,
        "deletions": 4
    }
}