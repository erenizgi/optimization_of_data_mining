{
    "author": "keetrap",
    "message": "Add Fast Grounding-Dino Processor (#37108)\n\n* Add Fast Grounding-Dino Processor\n\n* Added modular file\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "6fd87d1172c1ab48e810448230aa536b0d9f3322",
    "files": [
        {
            "sha": "022224351951c69d37c5999ec5944e9af59076b9",
            "filename": "docs/source/en/model_doc/grounding-dino.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6fd87d1172c1ab48e810448230aa536b0d9f3322/docs%2Fsource%2Fen%2Fmodel_doc%2Fgrounding-dino.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6fd87d1172c1ab48e810448230aa536b0d9f3322/docs%2Fsource%2Fen%2Fmodel_doc%2Fgrounding-dino.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgrounding-dino.md?ref=6fd87d1172c1ab48e810448230aa536b0d9f3322",
            "patch": "@@ -102,6 +102,11 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n \n [[autodoc]] GroundingDinoImageProcessor\n     - preprocess\n+\n+## GroundingDinoImageProcessorFast\n+\n+[[autodoc]] GroundingDinoImageProcessorFast\n+    - preprocess\n     - post_process_object_detection\n \n ## GroundingDinoProcessor"
        },
        {
            "sha": "d04c6b6ee46b757a8672e3cdb5d05b5b433ef22d",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6fd87d1172c1ab48e810448230aa536b0d9f3322/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6fd87d1172c1ab48e810448230aa536b0d9f3322/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=6fd87d1172c1ab48e810448230aa536b0d9f3322",
            "patch": "@@ -91,7 +91,7 @@\n             (\"git\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"glpn\", (\"GLPNImageProcessor\",)),\n             (\"got_ocr2\", (\"GotOcr2ImageProcessor\", \"GotOcr2ImageProcessorFast\")),\n-            (\"grounding-dino\", (\"GroundingDinoImageProcessor\",)),\n+            (\"grounding-dino\", (\"GroundingDinoImageProcessor\", \"GroundingDinoImageProcessorFast\")),\n             (\"groupvit\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"hiera\", (\"BitImageProcessor\", \"BitImageProcessorFast\")),\n             (\"idefics\", (\"IdeficsImageProcessor\",)),"
        },
        {
            "sha": "68f678c9cf9c9b239a79e1ded06958c43ecbaebe",
            "filename": "src/transformers/models/grounding_dino/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6fd87d1172c1ab48e810448230aa536b0d9f3322/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6fd87d1172c1ab48e810448230aa536b0d9f3322/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2F__init__.py?ref=6fd87d1172c1ab48e810448230aa536b0d9f3322",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_grounding_dino import *\n     from .image_processing_grounding_dino import *\n+    from .image_processing_grounding_dino_fast import *\n     from .modeling_grounding_dino import *\n     from .processing_grounding_dino import *\n else:"
        },
        {
            "sha": "3d6389047c87eee463c320a7a1cf07591893e871",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino_fast.py",
            "status": "added",
            "additions": 825,
            "deletions": 0,
            "changes": 825,
            "blob_url": "https://github.com/huggingface/transformers/blob/6fd87d1172c1ab48e810448230aa536b0d9f3322/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6fd87d1172c1ab48e810448230aa536b0d9f3322/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py?ref=6fd87d1172c1ab48e810448230aa536b0d9f3322",
            "patch": "@@ -0,0 +1,825 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/grounding_dino/modular_grounding_dino.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_grounding_dino.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+import pathlib\n+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union\n+\n+from ...image_processing_utils import BatchFeature, get_size_dict\n+from ...image_processing_utils_fast import (\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    SizeDict,\n+    get_image_size_for_max_height_width,\n+    get_max_height_width,\n+    safe_squeeze,\n+)\n+from ...image_transforms import center_to_corners_format, corners_to_center_format\n+from ...image_utils import (\n+    IMAGENET_DEFAULT_MEAN,\n+    IMAGENET_DEFAULT_STD,\n+    AnnotationFormat,\n+    AnnotationType,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    get_image_size,\n+    validate_annotations,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    add_start_docstrings,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    logging,\n+)\n+from ...utils.import_utils import requires\n+from .image_processing_grounding_dino import get_size_with_aspect_ratio\n+\n+\n+if TYPE_CHECKING:\n+    from .modeling_grounding_dino import GroundingDinoObjectDetectionOutput\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+if is_torchvision_v2_available():\n+    from torchvision.io import read_image\n+    from torchvision.transforms.v2 import functional as F\n+elif is_torchvision_available():\n+    from torchvision.io import read_image\n+    from torchvision.transforms import functional as F\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class GroundingDinoFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    format: Optional[Union[str, AnnotationFormat]]\n+    do_convert_annotations: Optional[bool]\n+    do_pad: Optional[bool]\n+    pad_size: Optional[Dict[str, int]]\n+    return_segmentation_masks: Optional[bool]\n+\n+\n+SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n+\n+\n+# inspired by https://github.com/facebookresearch/grounding_dino/blob/master/datasets/coco.py#L33\n+def convert_coco_poly_to_mask(segmentations, height: int, width: int, device: torch.device) -> torch.Tensor:\n+    \"\"\"\n+    Convert a COCO polygon annotation to a mask.\n+\n+    Args:\n+        segmentations (`List[List[float]]`):\n+            List of polygons, each polygon represented by a list of x-y coordinates.\n+        height (`int`):\n+            Height of the mask.\n+        width (`int`):\n+            Width of the mask.\n+    \"\"\"\n+    try:\n+        from pycocotools import mask as coco_mask\n+    except ImportError:\n+        raise ImportError(\"Pycocotools is not installed in your environment.\")\n+\n+    masks = []\n+    for polygons in segmentations:\n+        rles = coco_mask.frPyObjects(polygons, height, width)\n+        mask = coco_mask.decode(rles)\n+        if len(mask.shape) < 3:\n+            mask = mask[..., None]\n+        mask = torch.as_tensor(mask, dtype=torch.uint8, device=device)\n+        mask = torch.any(mask, axis=2)\n+        masks.append(mask)\n+    if masks:\n+        masks = torch.stack(masks, axis=0)\n+    else:\n+        masks = torch.zeros((0, height, width), dtype=torch.uint8, device=device)\n+\n+    return masks\n+\n+\n+# inspired by https://github.com/facebookresearch/grounding_dino/blob/master/datasets/coco.py#L50\n+def prepare_coco_detection_annotation(\n+    image,\n+    target,\n+    return_segmentation_masks: bool = False,\n+    input_data_format: Optional[Union[ChannelDimension, str]] = None,\n+):\n+    \"\"\"\n+    Convert the target in COCO format into the format expected by GROUNDING_DINO.\n+    \"\"\"\n+    image_height, image_width = image.size()[-2:]\n+\n+    image_id = target[\"image_id\"]\n+    image_id = torch.as_tensor([image_id], dtype=torch.int64, device=image.device)\n+\n+    # Get all COCO annotations for the given image.\n+    annotations = target[\"annotations\"]\n+    classes = []\n+    area = []\n+    boxes = []\n+    keypoints = []\n+    for obj in annotations:\n+        if \"iscrowd\" not in obj or obj[\"iscrowd\"] == 0:\n+            classes.append(obj[\"category_id\"])\n+            area.append(obj[\"area\"])\n+            boxes.append(obj[\"bbox\"])\n+            if \"keypoints\" in obj:\n+                keypoints.append(obj[\"keypoints\"])\n+\n+    classes = torch.as_tensor(classes, dtype=torch.int64, device=image.device)\n+    area = torch.as_tensor(area, dtype=torch.float32, device=image.device)\n+    iscrowd = torch.zeros_like(classes, dtype=torch.int64, device=image.device)\n+    # guard against no boxes via resizing\n+    boxes = torch.as_tensor(boxes, dtype=torch.float32, device=image.device).reshape(-1, 4)\n+    boxes[:, 2:] += boxes[:, :2]\n+    boxes[:, 0::2] = boxes[:, 0::2].clip(min=0, max=image_width)\n+    boxes[:, 1::2] = boxes[:, 1::2].clip(min=0, max=image_height)\n+\n+    keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n+\n+    new_target = {\n+        \"image_id\": image_id,\n+        \"class_labels\": classes[keep],\n+        \"boxes\": boxes[keep],\n+        \"area\": area[keep],\n+        \"iscrowd\": iscrowd[keep],\n+        \"orig_size\": torch.as_tensor([int(image_height), int(image_width)], dtype=torch.int64, device=image.device),\n+    }\n+\n+    if keypoints:\n+        keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=image.device)\n+        # Apply the keep mask here to filter the relevant annotations\n+        keypoints = keypoints[keep]\n+        num_keypoints = keypoints.shape[0]\n+        keypoints = keypoints.reshape((-1, 3)) if num_keypoints else keypoints\n+        new_target[\"keypoints\"] = keypoints\n+\n+    if return_segmentation_masks:\n+        segmentation_masks = [obj[\"segmentation\"] for obj in annotations]\n+        masks = convert_coco_poly_to_mask(segmentation_masks, image_height, image_width, device=image.device)\n+        new_target[\"masks\"] = masks[keep]\n+\n+    return new_target\n+\n+\n+def masks_to_boxes(masks: torch.Tensor) -> torch.Tensor:\n+    \"\"\"\n+    Compute the bounding boxes around the provided panoptic segmentation masks.\n+\n+    Args:\n+        masks: masks in format `[number_masks, height, width]` where N is the number of masks\n+\n+    Returns:\n+        boxes: bounding boxes in format `[number_masks, 4]` in xyxy format\n+    \"\"\"\n+    if masks.numel() == 0:\n+        return torch.zeros((0, 4), device=masks.device)\n+\n+    h, w = masks.shape[-2:]\n+    y = torch.arange(0, h, dtype=torch.float32, device=masks.device)\n+    x = torch.arange(0, w, dtype=torch.float32, device=masks.device)\n+    # see https://github.com/pytorch/pytorch/issues/50276\n+    y, x = torch.meshgrid(y, x, indexing=\"ij\")\n+\n+    x_mask = masks * torch.unsqueeze(x, 0)\n+    x_max = x_mask.view(x_mask.shape[0], -1).max(-1)[0]\n+    x_min = (\n+        torch.where(masks, x.unsqueeze(0), torch.tensor(1e8, device=masks.device)).view(masks.shape[0], -1).min(-1)[0]\n+    )\n+\n+    y_mask = masks * torch.unsqueeze(y, 0)\n+    y_max = y_mask.view(y_mask.shape[0], -1).max(-1)[0]\n+    y_min = (\n+        torch.where(masks, y.unsqueeze(0), torch.tensor(1e8, device=masks.device)).view(masks.shape[0], -1).min(-1)[0]\n+    )\n+\n+    return torch.stack([x_min, y_min, x_max, y_max], 1)\n+\n+\n+# 2 functions below adapted from https://github.com/cocodataset/panopticapi/blob/master/panopticapi/utils.py\n+# Copyright (c) 2018, Alexander Kirillov\n+# All rights reserved.\n+def rgb_to_id(color):\n+    \"\"\"\n+    Converts RGB color to unique ID.\n+    \"\"\"\n+    if isinstance(color, torch.Tensor) and len(color.shape) == 3:\n+        if color.dtype == torch.uint8:\n+            color = color.to(torch.int32)\n+        return color[:, :, 0] + 256 * color[:, :, 1] + 256 * 256 * color[:, :, 2]\n+    return int(color[0] + 256 * color[1] + 256 * 256 * color[2])\n+\n+\n+def prepare_coco_panoptic_annotation(\n+    image: torch.Tensor,\n+    target: Dict,\n+    masks_path: Union[str, pathlib.Path],\n+    return_masks: bool = True,\n+    input_data_format: Union[ChannelDimension, str] = None,\n+) -> Dict:\n+    \"\"\"\n+    Prepare a coco panoptic annotation for GROUNDING_DINO.\n+    \"\"\"\n+    image_height, image_width = get_image_size(image, channel_dim=input_data_format)\n+    annotation_path = pathlib.Path(masks_path) / target[\"file_name\"]\n+\n+    new_target = {}\n+    new_target[\"image_id\"] = torch.as_tensor(\n+        [target[\"image_id\"] if \"image_id\" in target else target[\"id\"]], dtype=torch.int64, device=image.device\n+    )\n+    new_target[\"size\"] = torch.as_tensor([image_height, image_width], dtype=torch.int64, device=image.device)\n+    new_target[\"orig_size\"] = torch.as_tensor([image_height, image_width], dtype=torch.int64, device=image.device)\n+\n+    if \"segments_info\" in target:\n+        masks = read_image(annotation_path).permute(1, 2, 0).to(dtype=torch.int32, device=image.device)\n+        masks = rgb_to_id(masks)\n+\n+        ids = torch.as_tensor([segment_info[\"id\"] for segment_info in target[\"segments_info\"]], device=image.device)\n+        masks = masks == ids[:, None, None]\n+        masks = masks.to(torch.bool)\n+        if return_masks:\n+            new_target[\"masks\"] = masks\n+        new_target[\"boxes\"] = masks_to_boxes(masks)\n+        new_target[\"class_labels\"] = torch.as_tensor(\n+            [segment_info[\"category_id\"] for segment_info in target[\"segments_info\"]],\n+            dtype=torch.int64,\n+            device=image.device,\n+        )\n+        new_target[\"iscrowd\"] = torch.as_tensor(\n+            [segment_info[\"iscrowd\"] for segment_info in target[\"segments_info\"]],\n+            dtype=torch.int64,\n+            device=image.device,\n+        )\n+        new_target[\"area\"] = torch.as_tensor(\n+            [segment_info[\"area\"] for segment_info in target[\"segments_info\"]],\n+            dtype=torch.float32,\n+            device=image.device,\n+        )\n+\n+    return new_target\n+\n+\n+def _scale_boxes(boxes, target_sizes):\n+    \"\"\"\n+    Scale batch of bounding boxes to the target sizes.\n+\n+    Args:\n+        boxes (`torch.Tensor` of shape `(batch_size, num_boxes, 4)`):\n+            Bounding boxes to scale. Each box is expected to be in (x1, y1, x2, y2) format.\n+        target_sizes (`List[Tuple[int, int]]` or `torch.Tensor` of shape `(batch_size, 2)`):\n+            Target sizes to scale the boxes to. Each target size is expected to be in (height, width) format.\n+\n+    Returns:\n+        `torch.Tensor` of shape `(batch_size, num_boxes, 4)`: Scaled bounding boxes.\n+    \"\"\"\n+\n+    if isinstance(target_sizes, (list, tuple)):\n+        image_height = torch.tensor([i[0] for i in target_sizes])\n+        image_width = torch.tensor([i[1] for i in target_sizes])\n+    elif isinstance(target_sizes, torch.Tensor):\n+        image_height, image_width = target_sizes.unbind(1)\n+    else:\n+        raise ValueError(\"`target_sizes` must be a list, tuple or torch.Tensor\")\n+\n+    scale_factor = torch.stack([image_width, image_height, image_width, image_height], dim=1)\n+    scale_factor = scale_factor.unsqueeze(1).to(boxes.device)\n+    boxes = boxes * scale_factor\n+    return boxes\n+\n+\n+@add_start_docstrings(\n+    \"Constructs a fast GroundingDino image processor.\",\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    \"\"\"\n+        format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n+            Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n+        do_convert_annotations (`bool`, *optional*, defaults to `True`):\n+            Controls whether to convert the annotations to the format expected by the GROUNDING_DINO model. Converts the\n+            bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n+            Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n+        do_pad (`bool`, *optional*, defaults to `True`):\n+            Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n+            method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n+            If `pad_size` is provided, the image will be padded to the specified dimensions.\n+            Otherwise, the image will be padded to the maximum height and width of the batch.\n+        pad_size (`Dict[str, int]`, *optional*):\n+            The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+            provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+            height and width in the batch.\n+        return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n+            Whether to return segmentation masks.\n+    \"\"\",\n+)\n+@requires(backends=(\"torchvision\", \"torch\"))\n+class GroundingDinoImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_DEFAULT_MEAN\n+    image_std = IMAGENET_DEFAULT_STD\n+    format = AnnotationFormat.COCO_DETECTION\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_pad = True\n+    size = {\"shortest_edge\": 800, \"longest_edge\": 1333}\n+    default_to_square = False\n+    model_input_names = [\"pixel_values\", \"pixel_mask\"]\n+    valid_kwargs = GroundingDinoFastImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[GroundingDinoFastImageProcessorKwargs]) -> None:\n+        if \"pad_and_return_pixel_mask\" in kwargs:\n+            kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n+\n+        size = kwargs.pop(\"size\", None)\n+        if \"max_size\" in kwargs:\n+            logger.warning_once(\n+                \"The `max_size` parameter is deprecated and will be removed in v4.26. \"\n+                \"Please specify in `size['longest_edge'] instead`.\",\n+            )\n+            max_size = kwargs.pop(\"max_size\")\n+        else:\n+            max_size = None if size is None else 1333\n+\n+        size = size if size is not None else {\"shortest_edge\": 800, \"longest_edge\": 1333}\n+        self.size = get_size_dict(size, max_size=max_size, default_to_square=False)\n+\n+        # Backwards compatibility\n+        do_convert_annotations = kwargs.get(\"do_convert_annotations\", None)\n+        do_normalize = kwargs.get(\"do_normalize\", None)\n+        if do_convert_annotations is None and getattr(self, \"do_convert_annotations\", None) is None:\n+            self.do_convert_annotations = do_normalize if do_normalize is not None else self.do_normalize\n+\n+        super().__init__(**kwargs)\n+\n+    @classmethod\n+    def from_dict(cls, image_processor_dict: Dict[str, Any], **kwargs):\n+        \"\"\"\n+        Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n+        created using from_dict and kwargs e.g. `GroundingDinoImageProcessorFast.from_pretrained(checkpoint, size=600,\n+        max_size=800)`\n+        \"\"\"\n+        image_processor_dict = image_processor_dict.copy()\n+        if \"max_size\" in kwargs:\n+            image_processor_dict[\"max_size\"] = kwargs.pop(\"max_size\")\n+        if \"pad_and_return_pixel_mask\" in kwargs:\n+            image_processor_dict[\"pad_and_return_pixel_mask\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n+        return super().from_dict(image_processor_dict, **kwargs)\n+\n+    def prepare_annotation(\n+        self,\n+        image: torch.Tensor,\n+        target: Dict,\n+        format: Optional[AnnotationFormat] = None,\n+        return_segmentation_masks: Optional[bool] = None,\n+        masks_path: Optional[Union[str, pathlib.Path]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> Dict:\n+        \"\"\"\n+        Prepare an annotation for feeding into GROUNDING_DINO model.\n+        \"\"\"\n+        format = format if format is not None else self.format\n+\n+        if format == AnnotationFormat.COCO_DETECTION:\n+            return_segmentation_masks = False if return_segmentation_masks is None else return_segmentation_masks\n+            target = prepare_coco_detection_annotation(\n+                image, target, return_segmentation_masks, input_data_format=input_data_format\n+            )\n+        elif format == AnnotationFormat.COCO_PANOPTIC:\n+            return_segmentation_masks = True if return_segmentation_masks is None else return_segmentation_masks\n+            target = prepare_coco_panoptic_annotation(\n+                image,\n+                target,\n+                masks_path=masks_path,\n+                return_masks=return_segmentation_masks,\n+                input_data_format=input_data_format,\n+            )\n+        else:\n+            raise ValueError(f\"Format {format} is not supported.\")\n+        return target\n+\n+    def resize(\n+        self,\n+        image: torch.Tensor,\n+        size: SizeDict,\n+        interpolation: \"F.InterpolationMode\" = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Resize the image to the given size. Size can be `min_size` (scalar) or `(height, width)` tuple. If size is an\n+        int, smaller edge of the image will be matched to this number.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`SizeDict`):\n+                Size of the image's `(height, width)` dimensions after resizing. Available options are:\n+                    - `{\"height\": int, \"width\": int}`: The image will be resized to the exact size `(height, width)`.\n+                        Do NOT keep the aspect ratio.\n+                    - `{\"shortest_edge\": int, \"longest_edge\": int}`: The image will be resized to a maximum size respecting\n+                        the aspect ratio and keeping the shortest edge less or equal to `shortest_edge` and the longest edge\n+                        less or equal to `longest_edge`.\n+                    - `{\"max_height\": int, \"max_width\": int}`: The image will be resized to the maximum size respecting the\n+                        aspect ratio and keeping the height less or equal to `max_height` and the width less or equal to\n+                        `max_width`.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                Resampling filter to use if resizing the image.\n+        \"\"\"\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.BILINEAR\n+        if size.shortest_edge and size.longest_edge:\n+            # Resize the image so that the shortest edge or the longest edge is of the given size\n+            # while maintaining the aspect ratio of the original image.\n+            new_size = get_size_with_aspect_ratio(\n+                image.size()[-2:],\n+                size[\"shortest_edge\"],\n+                size[\"longest_edge\"],\n+            )\n+        elif size.max_height and size.max_width:\n+            new_size = get_image_size_for_max_height_width(image.size()[-2:], size[\"max_height\"], size[\"max_width\"])\n+        elif size.height and size.width:\n+            new_size = (size[\"height\"], size[\"width\"])\n+        else:\n+            raise ValueError(\n+                \"Size must contain 'height' and 'width' keys or 'shortest_edge' and 'longest_edge' keys. Got\"\n+                f\" {size.keys()}.\"\n+            )\n+\n+        image = F.resize(\n+            image,\n+            size=new_size,\n+            interpolation=interpolation,\n+            **kwargs,\n+        )\n+        return image\n+\n+    def resize_annotation(\n+        self,\n+        annotation: Dict[str, Any],\n+        orig_size: Tuple[int, int],\n+        target_size: Tuple[int, int],\n+        threshold: float = 0.5,\n+        interpolation: \"F.InterpolationMode\" = None,\n+    ):\n+        \"\"\"\n+        Resizes an annotation to a target size.\n+\n+        Args:\n+            annotation (`Dict[str, Any]`):\n+                The annotation dictionary.\n+            orig_size (`Tuple[int, int]`):\n+                The original size of the input image.\n+            target_size (`Tuple[int, int]`):\n+                The target size of the image, as returned by the preprocessing `resize` step.\n+            threshold (`float`, *optional*, defaults to 0.5):\n+                The threshold used to binarize the segmentation masks.\n+            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST`):\n+                The resampling filter to use when resizing the masks.\n+        \"\"\"\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST\n+        ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n+\n+        new_annotation = {}\n+        new_annotation[\"size\"] = target_size\n+\n+        for key, value in annotation.items():\n+            if key == \"boxes\":\n+                boxes = value\n+                scaled_boxes = boxes * torch.as_tensor(\n+                    [ratio_width, ratio_height, ratio_width, ratio_height], dtype=torch.float32, device=boxes.device\n+                )\n+                new_annotation[\"boxes\"] = scaled_boxes\n+            elif key == \"area\":\n+                area = value\n+                scaled_area = area * (ratio_width * ratio_height)\n+                new_annotation[\"area\"] = scaled_area\n+            elif key == \"masks\":\n+                masks = value[:, None]\n+                masks = [F.resize(mask, target_size, interpolation=interpolation) for mask in masks]\n+                masks = torch.stack(masks).to(torch.float32)\n+                masks = masks[:, 0] > threshold\n+                new_annotation[\"masks\"] = masks\n+            elif key == \"size\":\n+                new_annotation[\"size\"] = target_size\n+            else:\n+                new_annotation[key] = value\n+\n+        return new_annotation\n+\n+    def normalize_annotation(self, annotation: Dict, image_size: Tuple[int, int]) -> Dict:\n+        image_height, image_width = image_size\n+        norm_annotation = {}\n+        for key, value in annotation.items():\n+            if key == \"boxes\":\n+                boxes = value\n+                boxes = corners_to_center_format(boxes)\n+                boxes /= torch.as_tensor(\n+                    [image_width, image_height, image_width, image_height], dtype=torch.float32, device=boxes.device\n+                )\n+                norm_annotation[key] = boxes\n+            else:\n+                norm_annotation[key] = value\n+        return norm_annotation\n+\n+    def _update_annotation_for_padded_image(\n+        self,\n+        annotation: Dict,\n+        input_image_size: Tuple[int, int],\n+        output_image_size: Tuple[int, int],\n+        padding,\n+        update_bboxes,\n+    ) -> Dict:\n+        \"\"\"\n+        Update the annotation for a padded image.\n+        \"\"\"\n+        new_annotation = {}\n+        new_annotation[\"size\"] = output_image_size\n+        ratio_height, ratio_width = (input / output for output, input in zip(output_image_size, input_image_size))\n+\n+        for key, value in annotation.items():\n+            if key == \"masks\":\n+                masks = value\n+                masks = F.pad(\n+                    masks,\n+                    padding,\n+                    fill=0,\n+                )\n+                masks = safe_squeeze(masks, 1)\n+                new_annotation[\"masks\"] = masks\n+            elif key == \"boxes\" and update_bboxes:\n+                boxes = value\n+                boxes *= torch.as_tensor([ratio_width, ratio_height, ratio_width, ratio_height], device=boxes.device)\n+                new_annotation[\"boxes\"] = boxes\n+            elif key == \"size\":\n+                new_annotation[\"size\"] = output_image_size\n+            else:\n+                new_annotation[key] = value\n+        return new_annotation\n+\n+    def pad(\n+        self,\n+        image: torch.Tensor,\n+        padded_size: Tuple[int, int],\n+        annotation: Optional[Dict[str, Any]] = None,\n+        update_bboxes: bool = True,\n+        fill: int = 0,\n+    ):\n+        original_size = image.size()[-2:]\n+        padding_bottom = padded_size[0] - original_size[0]\n+        padding_right = padded_size[1] - original_size[1]\n+        if padding_bottom < 0 or padding_right < 0:\n+            raise ValueError(\n+                f\"Padding dimensions are negative. Please make sure that the padded size is larger than the \"\n+                f\"original size. Got padded size: {padded_size}, original size: {original_size}.\"\n+            )\n+        if original_size != padded_size:\n+            padding = [0, 0, padding_right, padding_bottom]\n+            image = F.pad(image, padding, fill=fill)\n+            if annotation is not None:\n+                annotation = self._update_annotation_for_padded_image(\n+                    annotation, original_size, padded_size, padding, update_bboxes\n+                )\n+\n+        # Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n+        pixel_mask = torch.zeros(padded_size, dtype=torch.int64, device=image.device)\n+        pixel_mask[: original_size[0], : original_size[1]] = 1\n+\n+        return image, pixel_mask, annotation\n+\n+    @add_start_docstrings(\n+        BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+        \"\"\"\n+        annotations (`AnnotationType` or `List[AnnotationType]`, *optional*):\n+            List of annotations associated with the image or batch of images. If annotation is for object\n+            detection, the annotations should be a dictionary with the following keys:\n+            - \"image_id\" (`int`): The image id.\n+            - \"annotations\" (`List[Dict]`): List of annotations for an image. Each annotation should be a\n+                dictionary. An image can have no annotations, in which case the list should be empty.\n+            If annotation is for segmentation, the annotations should be a dictionary with the following keys:\n+            - \"image_id\" (`int`): The image id.\n+            - \"segments_info\" (`List[Dict]`): List of segments for an image. Each segment should be a dictionary.\n+                An image can have no segments, in which case the list should be empty.\n+            - \"file_name\" (`str`): The file name of the image.\n+        format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n+            Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n+        do_convert_annotations (`bool`, *optional*, defaults to `True`):\n+            Controls whether to convert the annotations to the format expected by the GROUNDING_DINO model. Converts the\n+            bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n+            Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n+        do_pad (`bool`, *optional*, defaults to `True`):\n+            Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n+            method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n+            If `pad_size` is provided, the image will be padded to the specified dimensions.\n+            Otherwise, the image will be padded to the maximum height and width of the batch.\n+        pad_size (`Dict[str, int]`, *optional*):\n+            The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+            provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+            height and width in the batch.\n+        return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n+            Whether to return segmentation masks.\n+        masks_path (`str` or `pathlib.Path`, *optional*):\n+            Path to the directory containing the segmentation masks.\n+        \"\"\",\n+    )\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        annotations: Optional[Union[AnnotationType, List[AnnotationType]]] = None,\n+        masks_path: Optional[Union[str, pathlib.Path]] = None,\n+        **kwargs: Unpack[GroundingDinoFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        if \"pad_and_return_pixel_mask\" in kwargs:\n+            kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n+            logger.warning_once(\n+                \"The `pad_and_return_pixel_mask` argument is deprecated and will be removed in a future version, \"\n+                \"use `do_pad` instead.\"\n+            )\n+\n+        if \"max_size\" in kwargs:\n+            logger.warning_once(\n+                \"The `max_size` argument is deprecated and will be removed in a future version, use\"\n+                \" `size['longest_edge']` instead.\"\n+            )\n+            kwargs[\"size\"] = kwargs.pop(\"max_size\")\n+\n+        return super().preprocess(images, annotations=annotations, masks_path=masks_path, **kwargs)\n+\n+    def _preprocess(\n+        self,\n+        images: List[\"torch.Tensor\"],\n+        annotations: Optional[Union[AnnotationType, List[AnnotationType]]],\n+        return_segmentation_masks: bool,\n+        masks_path: Optional[Union[str, pathlib.Path]],\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_center_crop: bool,\n+        crop_size: SizeDict,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        do_convert_annotations: bool,\n+        image_mean: Optional[Union[float, List[float]]],\n+        image_std: Optional[Union[float, List[float]]],\n+        do_pad: bool,\n+        pad_size: Optional[Dict[str, int]],\n+        format: Optional[Union[str, AnnotationFormat]],\n+        return_tensors: Optional[Union[str, TensorType]],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess an image or a batch of images so that it can be used by the model.\n+        \"\"\"\n+        if annotations is not None and isinstance(annotations, dict):\n+            annotations = [annotations]\n+\n+        if annotations is not None and len(images) != len(annotations):\n+            raise ValueError(\n+                f\"The number of images ({len(images)}) and annotations ({len(annotations)}) do not match.\"\n+            )\n+\n+        format = AnnotationFormat(format)\n+        if annotations is not None:\n+            validate_annotations(format, SUPPORTED_ANNOTATION_FORMATS, annotations)\n+\n+        if (\n+            masks_path is not None\n+            and format == AnnotationFormat.COCO_PANOPTIC\n+            and not isinstance(masks_path, (pathlib.Path, str))\n+        ):\n+            raise ValueError(\n+                \"The path to the directory containing the mask PNG files should be provided as a\"\n+                f\" `pathlib.Path` or string object, but is {type(masks_path)} instead.\"\n+            )\n+\n+        data = {}\n+\n+        processed_images = []\n+        processed_annotations = []\n+        pixel_masks = []  # Initialize pixel_masks here\n+        for image, annotation in zip(images, annotations if annotations is not None else [None] * len(images)):\n+            # prepare (COCO annotations as a list of Dict -> GROUNDING_DINO target as a single Dict per image)\n+            if annotations is not None:\n+                annotation = self.prepare_annotation(\n+                    image,\n+                    annotation,\n+                    format,\n+                    return_segmentation_masks=return_segmentation_masks,\n+                    masks_path=masks_path,\n+                    input_data_format=ChannelDimension.FIRST,\n+                )\n+\n+            if do_resize:\n+                resized_image = self.resize(image, size=size, interpolation=interpolation)\n+                if annotations is not None:\n+                    annotation = self.resize_annotation(\n+                        annotation,\n+                        orig_size=image.size()[-2:],\n+                        target_size=resized_image.size()[-2:],\n+                    )\n+                image = resized_image\n+            # Fused rescale and normalize\n+            image = self.rescale_and_normalize(image, do_rescale, rescale_factor, do_normalize, image_mean, image_std)\n+            if do_convert_annotations and annotations is not None:\n+                annotation = self.normalize_annotation(annotation, get_image_size(image, ChannelDimension.FIRST))\n+\n+            processed_images.append(image)\n+            processed_annotations.append(annotation)\n+        images = processed_images\n+        annotations = processed_annotations if annotations is not None else None\n+\n+        if do_pad:\n+            # depends on all resized image shapes so we need another loop\n+            if pad_size is not None:\n+                padded_size = (pad_size[\"height\"], pad_size[\"width\"])\n+            else:\n+                padded_size = get_max_height_width(images)\n+\n+            padded_images = []\n+            padded_annotations = []\n+            for image, annotation in zip(images, annotations if annotations is not None else [None] * len(images)):\n+                # Pads images and returns their mask: {'pixel_values': ..., 'pixel_mask': ...}\n+                if padded_size == image.size()[-2:]:\n+                    padded_images.append(image)\n+                    pixel_masks.append(torch.ones(padded_size, dtype=torch.int64, device=image.device))\n+                    padded_annotations.append(annotation)\n+                    continue\n+                image, pixel_mask, annotation = self.pad(\n+                    image, padded_size, annotation=annotation, update_bboxes=do_convert_annotations\n+                )\n+                padded_images.append(image)\n+                padded_annotations.append(annotation)\n+                pixel_masks.append(pixel_mask)\n+            images = padded_images\n+            annotations = padded_annotations if annotations is not None else None\n+            data.update({\"pixel_mask\": torch.stack(pixel_masks, dim=0)})\n+\n+        data.update({\"pixel_values\": torch.stack(images, dim=0)})\n+        encoded_inputs = BatchFeature(data, tensor_type=return_tensors)\n+        if annotations is not None:\n+            encoded_inputs[\"labels\"] = [\n+                BatchFeature(annotation, tensor_type=return_tensors) for annotation in annotations\n+            ]\n+        return encoded_inputs\n+\n+    def post_process_object_detection(\n+        self,\n+        outputs: \"GroundingDinoObjectDetectionOutput\",\n+        threshold: float = 0.1,\n+        target_sizes: Optional[Union[TensorType, List[Tuple]]] = None,\n+    ):\n+        \"\"\"\n+        Converts the raw output of [`GroundingDinoForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n+        bottom_right_x, bottom_right_y) format.\n+\n+        Args:\n+            outputs ([`GroundingDinoObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            threshold (`float`, *optional*, defaults to 0.1):\n+                Score threshold to keep object detection predictions.\n+            target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n+                `(height, width)` of each image in the batch. If unset, predictions will not be resized.\n+\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the following keys:\n+            - \"scores\": The confidence scores for each predicted box on the image.\n+            - \"labels\": Indexes of the classes predicted by the model on the image.\n+            - \"boxes\": Image bounding boxes in (top_left_x, top_left_y, bottom_right_x, bottom_right_y) format.\n+        \"\"\"\n+        batch_logits, batch_boxes = outputs.logits, outputs.pred_boxes\n+        batch_size = len(batch_logits)\n+\n+        if target_sizes is not None and len(target_sizes) != batch_size:\n+            raise ValueError(\"Make sure that you pass in as many target sizes as images\")\n+\n+        # batch_logits of shape (batch_size, num_queries, num_classes)\n+        batch_class_logits = torch.max(batch_logits, dim=-1)\n+        batch_scores = torch.sigmoid(batch_class_logits.values)\n+        batch_labels = batch_class_logits.indices\n+\n+        # Convert to [x0, y0, x1, y1] format\n+        batch_boxes = center_to_corners_format(batch_boxes)\n+\n+        # Convert from relative [0, 1] to absolute [0, height] coordinates\n+        if target_sizes is not None:\n+            batch_boxes = _scale_boxes(batch_boxes, target_sizes)\n+\n+        results = []\n+        for scores, labels, boxes in zip(batch_scores, batch_labels, batch_boxes):\n+            keep = scores > threshold\n+            scores = scores[keep]\n+            labels = labels[keep]\n+            boxes = boxes[keep]\n+            results.append({\"scores\": scores, \"labels\": labels, \"boxes\": boxes})\n+\n+        return results\n+\n+\n+__all__ = [\"GroundingDinoImageProcessorFast\"]"
        },
        {
            "sha": "2aacd3532390e1d633e039871767ba9719473653",
            "filename": "src/transformers/models/grounding_dino/modular_grounding_dino.py",
            "status": "added",
            "additions": 127,
            "deletions": 0,
            "changes": 127,
            "blob_url": "https://github.com/huggingface/transformers/blob/6fd87d1172c1ab48e810448230aa536b0d9f3322/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodular_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6fd87d1172c1ab48e810448230aa536b0d9f3322/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodular_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodular_grounding_dino.py?ref=6fd87d1172c1ab48e810448230aa536b0d9f3322",
            "patch": "@@ -0,0 +1,127 @@\n+from typing import TYPE_CHECKING, List, Optional, Tuple, Union\n+\n+from transformers.models.detr.image_processing_detr_fast import DetrImageProcessorFast\n+\n+from ...image_transforms import center_to_corners_format\n+from ...utils import (\n+    TensorType,\n+    is_torch_available,\n+    logging,\n+)\n+\n+\n+if TYPE_CHECKING:\n+    from .modeling_grounding_dino import GroundingDinoObjectDetectionOutput\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def _scale_boxes(boxes, target_sizes):\n+    \"\"\"\n+    Scale batch of bounding boxes to the target sizes.\n+\n+    Args:\n+        boxes (`torch.Tensor` of shape `(batch_size, num_boxes, 4)`):\n+            Bounding boxes to scale. Each box is expected to be in (x1, y1, x2, y2) format.\n+        target_sizes (`List[Tuple[int, int]]` or `torch.Tensor` of shape `(batch_size, 2)`):\n+            Target sizes to scale the boxes to. Each target size is expected to be in (height, width) format.\n+\n+    Returns:\n+        `torch.Tensor` of shape `(batch_size, num_boxes, 4)`: Scaled bounding boxes.\n+    \"\"\"\n+\n+    if isinstance(target_sizes, (list, tuple)):\n+        image_height = torch.tensor([i[0] for i in target_sizes])\n+        image_width = torch.tensor([i[1] for i in target_sizes])\n+    elif isinstance(target_sizes, torch.Tensor):\n+        image_height, image_width = target_sizes.unbind(1)\n+    else:\n+        raise ValueError(\"`target_sizes` must be a list, tuple or torch.Tensor\")\n+\n+    scale_factor = torch.stack([image_width, image_height, image_width, image_height], dim=1)\n+    scale_factor = scale_factor.unsqueeze(1).to(boxes.device)\n+    boxes = boxes * scale_factor\n+    return boxes\n+\n+\n+class GroundingDinoImageProcessorFast(DetrImageProcessorFast):\n+    def post_process_object_detection(\n+        self,\n+        outputs: \"GroundingDinoObjectDetectionOutput\",\n+        threshold: float = 0.1,\n+        target_sizes: Optional[Union[TensorType, List[Tuple]]] = None,\n+    ):\n+        \"\"\"\n+        Converts the raw output of [`GroundingDinoForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n+        bottom_right_x, bottom_right_y) format.\n+\n+        Args:\n+            outputs ([`GroundingDinoObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            threshold (`float`, *optional*, defaults to 0.1):\n+                Score threshold to keep object detection predictions.\n+            target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n+                `(height, width)` of each image in the batch. If unset, predictions will not be resized.\n+\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the following keys:\n+            - \"scores\": The confidence scores for each predicted box on the image.\n+            - \"labels\": Indexes of the classes predicted by the model on the image.\n+            - \"boxes\": Image bounding boxes in (top_left_x, top_left_y, bottom_right_x, bottom_right_y) format.\n+        \"\"\"\n+        batch_logits, batch_boxes = outputs.logits, outputs.pred_boxes\n+        batch_size = len(batch_logits)\n+\n+        if target_sizes is not None and len(target_sizes) != batch_size:\n+            raise ValueError(\"Make sure that you pass in as many target sizes as images\")\n+\n+        # batch_logits of shape (batch_size, num_queries, num_classes)\n+        batch_class_logits = torch.max(batch_logits, dim=-1)\n+        batch_scores = torch.sigmoid(batch_class_logits.values)\n+        batch_labels = batch_class_logits.indices\n+\n+        # Convert to [x0, y0, x1, y1] format\n+        batch_boxes = center_to_corners_format(batch_boxes)\n+\n+        # Convert from relative [0, 1] to absolute [0, height] coordinates\n+        if target_sizes is not None:\n+            batch_boxes = _scale_boxes(batch_boxes, target_sizes)\n+\n+        results = []\n+        for scores, labels, boxes in zip(batch_scores, batch_labels, batch_boxes):\n+            keep = scores > threshold\n+            scores = scores[keep]\n+            labels = labels[keep]\n+            boxes = boxes[keep]\n+            results.append({\"scores\": scores, \"labels\": labels, \"boxes\": boxes})\n+\n+        return results\n+\n+    def post_process():\n+        raise NotImplementedError(\"Post-processing is not implemented for Grounding-Dino yet.\")\n+\n+    def post_process_segmentation():\n+        raise NotImplementedError(\"Segmentation post-processing is not implemented for Grounding-Dino yet.\")\n+\n+    def post_process_instance():\n+        raise NotImplementedError(\"Instance post-processing is not implemented for Grounding-Dino yet.\")\n+\n+    def post_process_panoptic():\n+        raise NotImplementedError(\"Panoptic post-processing is not implemented for Grounding-Dino yet.\")\n+\n+    def post_process_instance_segmentation():\n+        raise NotImplementedError(\"Segmentation post-processing is not implemented for Grounding-Dino yet.\")\n+\n+    def post_process_semantic_segmentation():\n+        raise NotImplementedError(\"Semantic segmentation post-processing is not implemented for Grounding-Dino yet.\")\n+\n+    def post_process_panoptic_segmentation():\n+        raise NotImplementedError(\"Panoptic segmentation post-processing is not implemented for Grounding-Dino yet.\")\n+\n+\n+__all__ = [\"GroundingDinoImageProcessorFast\"]"
        },
        {
            "sha": "2c4ecb297e62db69e5db3ead2cbda5bcf89757ca",
            "filename": "tests/models/grounding_dino/test_image_processing_grounding_dino.py",
            "status": "modified",
            "additions": 17,
            "deletions": 12,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/6fd87d1172c1ab48e810448230aa536b0d9f3322/tests%2Fmodels%2Fgrounding_dino%2Ftest_image_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6fd87d1172c1ab48e810448230aa536b0d9f3322/tests%2Fmodels%2Fgrounding_dino%2Ftest_image_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_image_processing_grounding_dino.py?ref=6fd87d1172c1ab48e810448230aa536b0d9f3322",
            "patch": "@@ -20,7 +20,7 @@\n import numpy as np\n \n from transformers.testing_utils import require_torch, require_vision, slow\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import AnnotationFormatTestMixin, ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -35,6 +35,9 @@\n \n     from transformers import GroundingDinoImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import GroundingDinoImageProcessorFast\n+\n \n class GroundingDinoImageProcessingTester:\n     def __init__(\n@@ -147,6 +150,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class GroundingDinoImageProcessingTest(AnnotationFormatTestMixin, ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = GroundingDinoImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = GroundingDinoImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -182,20 +186,21 @@ def test_image_processor_from_dict_with_kwargs(self):\n             self.assertEqual(image_processor.do_pad, False)\n \n     def test_post_process_object_detection(self):\n-        image_processor = self.image_processing_class(**self.image_processor_dict)\n-        outputs = self.image_processor_tester.get_fake_grounding_dino_output()\n-        results = image_processor.post_process_object_detection(outputs, threshold=0.0)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            outputs = self.image_processor_tester.get_fake_grounding_dino_output()\n+            results = image_processor.post_process_object_detection(outputs, threshold=0.0)\n \n-        self.assertEqual(len(results), self.image_processor_tester.batch_size)\n-        self.assertEqual(list(results[0].keys()), [\"scores\", \"labels\", \"boxes\"])\n-        self.assertEqual(results[0][\"boxes\"].shape, (self.image_processor_tester.num_queries, 4))\n-        self.assertEqual(results[0][\"scores\"].shape, (self.image_processor_tester.num_queries,))\n+            self.assertEqual(len(results), self.image_processor_tester.batch_size)\n+            self.assertEqual(list(results[0].keys()), [\"scores\", \"labels\", \"boxes\"])\n+            self.assertEqual(results[0][\"boxes\"].shape, (self.image_processor_tester.num_queries, 4))\n+            self.assertEqual(results[0][\"scores\"].shape, (self.image_processor_tester.num_queries,))\n \n-        expected_scores = torch.tensor([0.7050, 0.7222, 0.7222, 0.6829, 0.7220])\n-        torch.testing.assert_close(results[0][\"scores\"], expected_scores, rtol=1e-4, atol=1e-4)\n+            expected_scores = torch.tensor([0.7050, 0.7222, 0.7222, 0.6829, 0.7220])\n+            torch.testing.assert_close(results[0][\"scores\"], expected_scores, rtol=1e-4, atol=1e-4)\n \n-        expected_box_slice = torch.tensor([0.6908, 0.4354, 1.0737, 1.3947])\n-        torch.testing.assert_close(results[0][\"boxes\"][0], expected_box_slice, rtol=1e-4, atol=1e-4)\n+            expected_box_slice = torch.tensor([0.6908, 0.4354, 1.0737, 1.3947])\n+            torch.testing.assert_close(results[0][\"boxes\"][0], expected_box_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     # Copied from tests.models.deformable_detr.test_image_processing_deformable_detr.DeformableDetrImageProcessingTest.test_call_pytorch_with_coco_detection_annotations with DeformableDetr->GroundingDino"
        }
    ],
    "stats": {
        "total": 989,
        "additions": 976,
        "deletions": 13
    }
}