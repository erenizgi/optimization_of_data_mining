{
    "author": "ivarflakstad",
    "message": "More fault tolerant notification service (#37924)\n\n* Let notification service succeed even when artifacts and reported jobs on github have mismatch\n\n* Use default trace msg if no trace msg available\n\n* Add pop_default helper fn\n\n* style",
    "sha": "afbc293e2b8523bfc92232d8a8d50e29bdce9f41",
    "files": [
        {
            "sha": "ea622d60911f09e77dbbf0624ea096d56077b8ac",
            "filename": "utils/notification_service.py",
            "status": "modified",
            "additions": 44,
            "deletions": 29,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/afbc293e2b8523bfc92232d8a8d50e29bdce9f41/utils%2Fnotification_service.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/afbc293e2b8523bfc92232d8a8d50e29bdce9f41/utils%2Fnotification_service.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnotification_service.py?ref=afbc293e2b8523bfc92232d8a8d50e29bdce9f41",
            "patch": "@@ -22,7 +22,7 @@\n import re\n import sys\n import time\n-from typing import Dict, List, Optional, Union\n+from typing import Any, Dict, List, Optional, Union\n \n import requests\n from get_ci_error_statistics import get_jobs\n@@ -920,6 +920,13 @@ def prepare_reports(title, header, reports, to_truncate=True):\n     return report\n \n \n+def pop_default(l: list[Any], i: int, default: Any) -> Any:\n+    try:\n+        return l.pop(i)\n+    except IndexError:\n+        return default\n+\n+\n if __name__ == \"__main__\":\n     SLACK_REPORT_CHANNEL_ID = os.environ[\"SLACK_REPORT_CHANNEL\"]\n \n@@ -1070,12 +1077,19 @@ def prepare_reports(title, header, reports, to_truncate=True):\n     unclassified_model_failures = []\n \n     for model in model_results.keys():\n-        for artifact_path in available_artifacts[f\"{report_name_prefix}_{model}_test_reports\"].paths:\n-            artifact = retrieve_artifact(artifact_path[\"path\"], artifact_path[\"gpu\"])\n+        for artifact_path_dict in available_artifacts[f\"{report_name_prefix}_{model}_test_reports\"].paths:\n+            path = artifact_path_dict[\"path\"]\n+            artifact_gpu = artifact_path_dict[\"gpu\"]\n+\n+            if path not in artifact_name_to_job_map:\n+                # Mismatch between available artifacts and reported jobs on github. It happens.\n+                continue\n+\n+            artifact = retrieve_artifact(path, artifact_gpu)\n             if \"stats\" in artifact:\n                 # Link to the GitHub Action job\n-                job = artifact_name_to_job_map[artifact_path[\"path\"]]\n-                model_results[model][\"job_link\"][artifact_path[\"gpu\"]] = job[\"html_url\"]\n+                job = artifact_name_to_job_map[path]\n+                model_results[model][\"job_link\"][artifact_gpu] = job[\"html_url\"]\n                 failed, success, time_spent = handle_test_results(artifact[\"stats\"])\n                 model_results[model][\"success\"] += success\n                 model_results[model][\"time_spent\"] += time_spent[1:-1] + \", \"\n@@ -1092,39 +1106,38 @@ def prepare_reports(title, header, reports, to_truncate=True):\n                         line = line[len(\"FAILED \") :]\n                         line = line.split()[0].replace(\"\\n\", \"\")\n \n-                        if artifact_path[\"gpu\"] not in model_results[model][\"failures\"]:\n-                            model_results[model][\"failures\"][artifact_path[\"gpu\"]] = []\n+                        if artifact_gpu not in model_results[model][\"failures\"]:\n+                            model_results[model][\"failures\"][artifact_gpu] = []\n \n-                        model_results[model][\"failures\"][artifact_path[\"gpu\"]].append(\n-                            {\"line\": line, \"trace\": stacktraces.pop(0)}\n-                        )\n+                        trace = pop_default(stacktraces, 0, \"Cannot retrieve error message.\")\n+                        model_results[model][\"failures\"][artifact_gpu].append({\"line\": line, \"trace\": trace})\n \n                         if re.search(\"test_modeling_tf_\", line):\n-                            model_results[model][\"failed\"][\"TensorFlow\"][artifact_path[\"gpu\"]] += 1\n+                            model_results[model][\"failed\"][\"TensorFlow\"][artifact_gpu] += 1\n \n                         elif re.search(\"test_modeling_flax_\", line):\n-                            model_results[model][\"failed\"][\"Flax\"][artifact_path[\"gpu\"]] += 1\n+                            model_results[model][\"failed\"][\"Flax\"][artifact_gpu] += 1\n \n                         elif re.search(\"test_modeling\", line):\n-                            model_results[model][\"failed\"][\"PyTorch\"][artifact_path[\"gpu\"]] += 1\n+                            model_results[model][\"failed\"][\"PyTorch\"][artifact_gpu] += 1\n \n                         elif re.search(\"test_tokenization\", line):\n-                            model_results[model][\"failed\"][\"Tokenizers\"][artifact_path[\"gpu\"]] += 1\n+                            model_results[model][\"failed\"][\"Tokenizers\"][artifact_gpu] += 1\n \n                         elif re.search(\"test_pipelines\", line):\n-                            model_results[model][\"failed\"][\"Pipelines\"][artifact_path[\"gpu\"]] += 1\n+                            model_results[model][\"failed\"][\"Pipelines\"][artifact_gpu] += 1\n \n                         elif re.search(\"test_trainer\", line):\n-                            model_results[model][\"failed\"][\"Trainer\"][artifact_path[\"gpu\"]] += 1\n+                            model_results[model][\"failed\"][\"Trainer\"][artifact_gpu] += 1\n \n                         elif re.search(\"onnx\", line):\n-                            model_results[model][\"failed\"][\"ONNX\"][artifact_path[\"gpu\"]] += 1\n+                            model_results[model][\"failed\"][\"ONNX\"][artifact_gpu] += 1\n \n                         elif re.search(\"auto\", line):\n-                            model_results[model][\"failed\"][\"Auto\"][artifact_path[\"gpu\"]] += 1\n+                            model_results[model][\"failed\"][\"Auto\"][artifact_gpu] += 1\n \n                         else:\n-                            model_results[model][\"failed\"][\"Unclassified\"][artifact_path[\"gpu\"]] += 1\n+                            model_results[model][\"failed\"][\"Unclassified\"][artifact_gpu] += 1\n                             unclassified_model_failures.append(line)\n \n     # Additional runs\n@@ -1179,16 +1192,19 @@ def prepare_reports(title, header, reports, to_truncate=True):\n             additional_results[key][\"error\"] = True\n             continue\n \n-        for artifact_path in available_artifacts[additional_files[key]].paths:\n+        for artifact_path_dict in available_artifacts[additional_files[key]].paths:\n+            path = artifact_path_dict[\"path\"]\n+            artifact_gpu = artifact_path_dict[\"gpu\"]\n+\n             # Link to the GitHub Action job\n-            job = artifact_name_to_job_map[artifact_path[\"path\"]]\n-            additional_results[key][\"job_link\"][artifact_path[\"gpu\"]] = job[\"html_url\"]\n+            job = artifact_name_to_job_map[path]\n+            additional_results[key][\"job_link\"][artifact_gpu] = job[\"html_url\"]\n \n-            artifact = retrieve_artifact(artifact_path[\"path\"], artifact_path[\"gpu\"])\n+            artifact = retrieve_artifact(path, artifact_gpu)\n             stacktraces = handle_stacktraces(artifact[\"failures_line\"])\n \n             failed, success, time_spent = handle_test_results(artifact[\"stats\"])\n-            additional_results[key][\"failed\"][artifact_path[\"gpu\"] or \"unclassified\"] += failed\n+            additional_results[key][\"failed\"][artifact_gpu or \"unclassified\"] += failed\n             additional_results[key][\"success\"] += success\n             additional_results[key][\"time_spent\"] += time_spent[1:-1] + \", \"\n \n@@ -1206,12 +1222,11 @@ def prepare_reports(title, header, reports, to_truncate=True):\n                         line = line[len(\"FAILED \") :]\n                         line = line.split()[0].replace(\"\\n\", \"\")\n \n-                        if artifact_path[\"gpu\"] not in additional_results[key][\"failures\"]:\n-                            additional_results[key][\"failures\"][artifact_path[\"gpu\"]] = []\n+                        if artifact_gpu not in additional_results[key][\"failures\"]:\n+                            additional_results[key][\"failures\"][artifact_gpu] = []\n \n-                        additional_results[key][\"failures\"][artifact_path[\"gpu\"]].append(\n-                            {\"line\": line, \"trace\": stacktraces.pop(0)}\n-                        )\n+                        trace = pop_default(stacktraces, 0, \"Cannot retrieve error message.\")\n+                        additional_results[key][\"failures\"][artifact_gpu].append({\"line\": line, \"trace\": trace})\n \n     # Let's only check the warning for the model testing job. Currently, the job `run_extract_warnings` is only run\n     # when `inputs.job` (in the workflow file) is `run_models_gpu`. The reason is: otherwise we need to save several"
        }
    ],
    "stats": {
        "total": 73,
        "additions": 44,
        "deletions": 29
    }
}