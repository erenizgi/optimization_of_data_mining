{
    "author": "faaany",
    "message": "[tests] make cuda-only tests device-agnostic   (#35222)\n\nfix cuda-only tests",
    "sha": "c7e48053aab09ad11efa2ad12513e9ab56f29563",
    "files": [
        {
            "sha": "b219d5c74edff0053c7440081199d6899b4a7bd9",
            "filename": "tests/models/rag/test_modeling_rag.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7e48053aab09ad11efa2ad12513e9ab56f29563/tests%2Fmodels%2Frag%2Ftest_modeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7e48053aab09ad11efa2ad12513e9ab56f29563/tests%2Fmodels%2Frag%2Ftest_modeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frag%2Ftest_modeling_rag.py?ref=c7e48053aab09ad11efa2ad12513e9ab56f29563",
            "patch": "@@ -33,7 +33,7 @@\n     require_sentencepiece,\n     require_tokenizers,\n     require_torch,\n-    require_torch_non_multi_gpu,\n+    require_torch_non_multi_accelerator,\n     slow,\n     torch_device,\n )\n@@ -678,7 +678,7 @@ def config_and_inputs(self):\n @require_retrieval\n @require_sentencepiece\n @require_tokenizers\n-@require_torch_non_multi_gpu\n+@require_torch_non_multi_accelerator\n class RagModelIntegrationTests(unittest.TestCase):\n     def tearDown(self):\n         super().tearDown()\n@@ -1002,7 +1002,7 @@ def test_rag_token_generate_batch(self):\n             torch_device\n         )\n \n-        if torch_device == \"cuda\":\n+        if torch_device != \"cpu\":\n             rag_token.half()\n \n         input_dict = tokenizer("
        }
    ],
    "stats": {
        "total": 6,
        "additions": 3,
        "deletions": 3
    }
}