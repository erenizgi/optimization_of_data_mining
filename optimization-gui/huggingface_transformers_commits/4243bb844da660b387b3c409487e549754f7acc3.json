{
    "author": "kaixuanliu",
    "message": "fix bug using FSDP V1 will lead to model device not properly set (#39177)\n\n* fix bug using FSDP V1 will lead to model device not properly set\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* update the code\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n---------\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>",
    "sha": "4243bb844da660b387b3c409487e549754f7acc3",
    "files": [
        {
            "sha": "ddc447b6a473164d30860da13869ae2a9ebc9ed8",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/4243bb844da660b387b3c409487e549754f7acc3/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4243bb844da660b387b3c409487e549754f7acc3/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=4243bb844da660b387b3c409487e549754f7acc3",
            "patch": "@@ -2294,9 +2294,7 @@ def _inner_training_loop(\n             else:\n                 debug_overflow = DebugUnderflowOverflow(self.model)  # noqa\n \n-        delay_optimizer_creation = (\n-            is_sagemaker_mp_enabled() or self.is_fsdp_xla_enabled or self.is_fsdp_enabled or self.is_tp_enabled\n-        )\n+        delay_optimizer_creation = is_sagemaker_mp_enabled() or self.is_fsdp_xla_enabled or self.is_fsdp_enabled\n \n         # Can't delay optimizer creation when using FSDP2: https://github.com/huggingface/accelerate/blob/3f636d626063ffcf9a337c7d3624d61b7d187d59/src/accelerate/accelerator.py#L1404\n         is_fsdp2 = self.is_fsdp_enabled and (getattr(self.accelerator.state.fsdp_plugin, \"fsdp_version\", 1) == 2)\n@@ -2356,8 +2354,9 @@ def _inner_training_loop(\n                 if self.use_apex:\n                     model = self.accelerator.prepare(self.model)\n                 else:\n-                    if delay_optimizer_creation:\n-                        model = self.accelerator.prepare(self.model)\n+                    # We should avoid accelerate preparing the model in TP case since we dont need it as it is handled by transformers from_pretrained and also it goes into DDP based preparation.\n+                    if self.is_tp_enabled:\n+                        self.optimizer = self.accelerator.prepare(self.optimizer)\n                     else:\n                         model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\n             else:"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 4,
        "deletions": 5
    }
}