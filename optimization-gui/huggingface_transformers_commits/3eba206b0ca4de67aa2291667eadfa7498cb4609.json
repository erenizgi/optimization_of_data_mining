{
    "author": "Cyrilvallez",
    "message": "Remove outdated methods in modeling_utils.py (#42302)\n\n* cleanup\n\n* more\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* CI",
    "sha": "3eba206b0ca4de67aa2291667eadfa7498cb4609",
    "files": [
        {
            "sha": "f8050fd15c0f41d0cd636f7e1d828bd169a8d180",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 22,
            "deletions": 237,
            "changes": 259,
            "blob_url": "https://github.com/huggingface/transformers/blob/3eba206b0ca4de67aa2291667eadfa7498cb4609/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3eba206b0ca4de67aa2291667eadfa7498cb4609/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=3eba206b0ca4de67aa2291667eadfa7498cb4609",
            "patch": "@@ -32,7 +32,7 @@\n from functools import partial, wraps\n from itertools import cycle\n from threading import Thread\n-from typing import Any, Optional, TypeVar, Union, get_type_hints\n+from typing import Optional, TypeVar, Union, get_type_hints\n from zipfile import is_zipfile\n \n import torch\n@@ -106,7 +106,6 @@\n     download_url,\n     has_file,\n     is_accelerate_available,\n-    is_bitsandbytes_available,\n     is_flash_attn_2_available,\n     is_flash_attn_3_available,\n     is_kernels_available,\n@@ -116,13 +115,11 @@\n     is_torch_greater_or_equal,\n     is_torch_mlu_available,\n     is_torch_npu_available,\n-    is_torch_xla_available,\n     logging,\n )\n from .utils.generic import _CAN_RECORD_REGISTRY, GeneralInterface, OutputRecorder\n from .utils.hub import DownloadKwargs, create_and_tag_model_card, get_checkpoint_shard_files\n from .utils.import_utils import (\n-    ENV_VARS_TRUE_VALUES,\n     is_huggingface_hub_greater_or_equal,\n     is_sagemaker_mp_enabled,\n     is_tracing,\n@@ -299,70 +296,6 @@ def get_torch_context_manager_or_global_device():\n     return device_in_context\n \n \n-def get_parameter_device(parameter: Union[nn.Module, \"ModuleUtilsMixin\"]):\n-    try:\n-        return next(parameter.parameters()).device\n-    except StopIteration:\n-        # For nn.DataParallel compatibility in PyTorch 1.5\n-\n-        def find_tensor_attributes(module: nn.Module) -> list[tuple[str, Tensor]]:\n-            tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n-            return tuples\n-\n-        gen = parameter._named_members(get_members_fn=find_tensor_attributes)\n-        first_tuple = next(gen)\n-        return first_tuple[1].device\n-\n-\n-def get_parameter_dtype(parameter: Union[nn.Module, \"ModuleUtilsMixin\"]):\n-    \"\"\"\n-    Returns the first found floating dtype in parameters if there is one, otherwise returns the last dtype it found.\n-    \"\"\"\n-    last_dtype = None\n-    for t in parameter.parameters():\n-        last_dtype = t.dtype\n-        if t.is_floating_point():\n-            # Adding fix for https://github.com/pytorch/xla/issues/4152\n-            # Fixes issue where the model code passes a value that is out of range for XLA_USE_BF16=1\n-            # and XLA_DOWNCAST_BF16=1 so the conversion would cast it to -inf\n-            # NOTE: `is_torch_xla_available()` is checked last as it induces a graph break in torch dynamo\n-            if XLA_USE_BF16 in ENV_VARS_TRUE_VALUES and is_torch_xla_available():\n-                return torch.bfloat16\n-            if XLA_DOWNCAST_BF16 in ENV_VARS_TRUE_VALUES and is_torch_xla_available():\n-                if t.dtype == torch.float:\n-                    return torch.bfloat16\n-                if t.dtype == torch.double:\n-                    return torch.float32\n-            return t.dtype\n-\n-    if last_dtype is not None:\n-        # if no floating dtype was found return whatever the first dtype is\n-        return last_dtype\n-\n-    # For nn.DataParallel compatibility in PyTorch > 1.5\n-    def find_tensor_attributes(module: nn.Module) -> list[tuple[str, Tensor]]:\n-        tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n-        return tuples\n-\n-    gen = parameter._named_members(get_members_fn=find_tensor_attributes)\n-    last_tuple = None\n-    for gen_tuple in gen:\n-        last_tuple = gen_tuple\n-        if gen_tuple[1].is_floating_point():\n-            return gen_tuple[1].dtype\n-\n-    if last_tuple is not None:\n-        # fallback to the last dtype\n-        return last_tuple[1].dtype\n-\n-    # fallback to buffer dtype\n-    for t in parameter.buffers():\n-        last_dtype = t.dtype\n-        if t.is_floating_point():\n-            return t.dtype\n-    return last_dtype\n-\n-\n def get_state_dict_dtype(state_dict):\n     \"\"\"\n     Returns the first found floating dtype in `state_dict` if there is one, otherwise returns the first dtype.\n@@ -398,11 +331,8 @@ def get_state_dict_dtype(state_dict):\n \n \n def load_state_dict(\n-    checkpoint_file: Union[str, os.PathLike],\n-    is_quantized: bool = False,\n-    map_location: Optional[Union[str, torch.device]] = \"cpu\",\n-    weights_only: bool = True,\n-):\n+    checkpoint_file: Union[str, os.PathLike], map_location: Union[str, torch.device] = \"cpu\", weights_only: bool = True\n+) -> dict[str, torch.Tensor]:\n     \"\"\"\n     Reads a `safetensor` or a `.bin` checkpoint file. We load the checkpoint on \"cpu\" by default.\n     \"\"\"\n@@ -420,51 +350,18 @@ def load_state_dict(\n                         raise ValueError(f\"Cannot load safetensors of unknown dtype {k_dtype}\")\n                     state_dict[k] = torch.empty(size=_slice.get_shape(), dtype=dtype, device=\"meta\")\n                 else:\n-                    state_dict[k] = f.get_tensor(k)\n+                    state_dict[k] = f.get_tensor(k).to(map_location)\n             return state_dict\n \n     # Fallback to torch.load (if weights_only was explicitly False, do not check safety as this is known to be unsafe)\n     if weights_only:\n         check_torch_load_is_safe()\n-    try:\n-        if map_location is None:\n-            if (\n-                (\n-                    is_deepspeed_zero3_enabled()\n-                    and torch.distributed.is_initialized()\n-                    and torch.distributed.get_rank() > 0\n-                )\n-                or (is_fsdp_enabled() and not is_local_dist_rank_0())\n-            ) and not is_quantized:\n-                map_location = \"meta\"\n-            else:\n-                map_location = \"cpu\"\n-        extra_args = {}\n-        # mmap can only be used with files serialized with zipfile-based format.\n-        if isinstance(checkpoint_file, str) and map_location != \"meta\" and is_zipfile(checkpoint_file):\n-            extra_args = {\"mmap\": True}\n-        return torch.load(\n-            checkpoint_file,\n-            map_location=map_location,\n-            weights_only=weights_only,\n-            **extra_args,\n-        )\n-    except Exception as e:\n-        try:\n-            with open(checkpoint_file) as f:\n-                if f.read(7) == \"version\":\n-                    raise OSError(\n-                        \"You seem to have cloned a repository without having git-lfs installed. Please install \"\n-                        \"git-lfs and run `git lfs install` followed by `git lfs pull` in the folder \"\n-                        \"you cloned.\"\n-                    )\n-                else:\n-                    raise ValueError(\n-                        f\"Unable to locate the file {checkpoint_file} which is necessary to load this pretrained \"\n-                        \"model. Make sure you have saved the model properly.\"\n-                    ) from e\n-        except (UnicodeDecodeError, ValueError):\n-            raise OSError(f\"Unable to load weights from pytorch checkpoint file '{checkpoint_file}'.\")\n+    extra_args = {}\n+    # mmap can only be used with files serialized with zipfile-based format.\n+    if isinstance(checkpoint_file, str) and map_location != \"meta\" and is_zipfile(checkpoint_file):\n+        extra_args = {\"mmap\": True}\n+\n+    return torch.load(checkpoint_file, map_location=map_location, weights_only=weights_only, **extra_args)\n \n \n def _end_ptr(tensor: torch.Tensor) -> int:\n@@ -918,67 +815,20 @@ class ModuleUtilsMixin:\n     A few utilities for `torch.nn.Modules`, to be used as a mixin.\n     \"\"\"\n \n-    @staticmethod\n-    def _hook_rss_memory_pre_forward(module, *args, **kwargs):\n-        try:\n-            import psutil\n-        except ImportError:\n-            raise ImportError(\"You need to install psutil (pip install psutil) to use memory tracing.\")\n-\n-        process = psutil.Process(os.getpid())\n-        mem = process.memory_info()\n-        module.mem_rss_pre_forward = mem.rss\n-        return None\n-\n-    @staticmethod\n-    def _hook_rss_memory_post_forward(module, *args, **kwargs):\n-        try:\n-            import psutil\n-        except ImportError:\n-            raise ImportError(\"You need to install psutil (pip install psutil) to use memory tracing.\")\n-\n-        process = psutil.Process(os.getpid())\n-        mem = process.memory_info()\n-        module.mem_rss_post_forward = mem.rss\n-        mem_rss_diff = module.mem_rss_post_forward - module.mem_rss_pre_forward\n-        module.mem_rss_diff = mem_rss_diff + (module.mem_rss_diff if hasattr(module, \"mem_rss_diff\") else 0)\n-        return None\n-\n-    def add_memory_hooks(self):\n-        \"\"\"\n-        Add a memory hook before and after each sub-module forward pass to record increase in memory consumption.\n-\n-        Increase in memory consumption is stored in a `mem_rss_diff` attribute for each module and can be reset to zero\n-        with `model.reset_memory_hooks_state()`.\n-        \"\"\"\n-        for module in self.modules():\n-            module.register_forward_pre_hook(self._hook_rss_memory_pre_forward)\n-            module.register_forward_hook(self._hook_rss_memory_post_forward)\n-        self.reset_memory_hooks_state()\n-\n-    def reset_memory_hooks_state(self):\n-        \"\"\"\n-        Reset the `mem_rss_diff` attribute of each module (see [`~modeling_utils.ModuleUtilsMixin.add_memory_hooks`]).\n-        \"\"\"\n-        for module in self.modules():\n-            module.mem_rss_diff = 0\n-            module.mem_rss_post_forward = 0\n-            module.mem_rss_pre_forward = 0\n-\n     @property\n     def device(self) -> torch.device:\n         \"\"\"\n         `torch.device`: The device on which the module is (assuming that all the module parameters are on the same\n         device).\n         \"\"\"\n-        return get_parameter_device(self)\n+        return next(param.device for param in self.parameters())\n \n     @property\n     def dtype(self) -> torch.dtype:\n         \"\"\"\n         `torch.dtype`: The dtype of the module (assuming that all the module parameters have the same dtype).\n         \"\"\"\n-        return get_parameter_dtype(self)\n+        return next(param.dtype for param in self.parameters() if param.is_floating_point())\n \n     def invert_attention_mask(self, encoder_attention_mask: Tensor) -> Tensor:\n         \"\"\"\n@@ -1104,25 +954,15 @@ def num_parameters(self, only_trainable: bool = False, exclude_embeddings: bool\n             embedding_param_names = [\n                 f\"{name}.weight\" for name, module_type in self.named_modules() if isinstance(module_type, nn.Embedding)\n             ]\n-            total_parameters = [\n-                parameter for name, parameter in self.named_parameters() if name not in embedding_param_names\n-            ]\n-        else:\n-            total_parameters = list(self.parameters())\n \n-        total_numel = []\n         is_loaded_in_4bit = getattr(self, \"is_loaded_in_4bit\", False)\n-\n         if is_loaded_in_4bit:\n-            if is_bitsandbytes_available():\n-                import bitsandbytes as bnb\n-            else:\n-                raise ValueError(\n-                    \"bitsandbytes is not installed but it seems that the model has been loaded in 4bit precision, something went wrong\"\n-                    \" make sure to install bitsandbytes with `pip install bitsandbytes`. You also need a GPU. \"\n-                )\n+            import bitsandbytes as bnb\n \n-        for param in total_parameters:\n+        total_params = 0\n+        for name, param in self.named_parameters():\n+            if exclude_embeddings and name in embedding_param_names:\n+                continue\n             if param.requires_grad or not only_trainable:\n                 # For 4bit models, we need to multiply the number of parameters by 2 as half of the parameters are\n                 # used for the 4bit quantization (uint8 tensors are stored)\n@@ -1133,58 +973,11 @@ def num_parameters(self, only_trainable: bool = False, exclude_embeddings: bool\n                         num_bytes = param.quant_storage.itemsize\n                     else:\n                         num_bytes = 1\n-                    total_numel.append(param.numel() * 2 * num_bytes)\n+                    total_params += param.numel() * 2 * num_bytes\n                 else:\n-                    total_numel.append(param.numel())\n-\n-        return sum(total_numel)\n-\n-    def estimate_tokens(self, input_dict: dict[str, Union[torch.Tensor, Any]]) -> int:\n-        \"\"\"\n-        Helper function to estimate the total number of tokens from the model inputs.\n-\n-        Args:\n-            inputs (`dict`): The model inputs.\n+                    total_params += param.numel()\n \n-        Returns:\n-            `int`: The total number of tokens.\n-        \"\"\"\n-        if not hasattr(self, \"warnings_issued\"):\n-            self.warnings_issued = {}\n-        if self.main_input_name in input_dict:\n-            return input_dict[self.main_input_name].numel()\n-        elif \"estimate_tokens\" not in self.warnings_issued:\n-            logger.warning(\n-                \"Could not estimate the number of tokens of the input, floating-point operations will not be computed\"\n-            )\n-            self.warnings_issued[\"estimate_tokens\"] = True\n-        return 0\n-\n-    def floating_point_ops(\n-        self, input_dict: dict[str, Union[torch.Tensor, Any]], exclude_embeddings: bool = True\n-    ) -> int:\n-        \"\"\"\n-        Get number of (optionally, non-embeddings) floating-point operations for the forward and backward passes of a\n-        batch with this transformer model. Default approximation neglects the quadratic dependency on the number of\n-        tokens (valid if `12 * d_model << sequence_length`) as laid out in [this\n-        paper](https://huggingface.co/papers/2001.08361) section 2.1. Should be overridden for transformers with parameter\n-        re-use e.g. Albert or Universal Transformers, or if doing long-range modeling with very high sequence lengths.\n-\n-        Args:\n-            batch_size (`int`):\n-                The batch size for the forward pass.\n-\n-            sequence_length (`int`):\n-                The number of tokens in each line of the batch.\n-\n-            exclude_embeddings (`bool`, *optional*, defaults to `True`):\n-                Whether or not to count embedding and softmax operations.\n-\n-        Returns:\n-            `int`: The number of floating-point operations.\n-        \"\"\"\n-\n-        return 6 * self.estimate_tokens(input_dict) * self.num_parameters(exclude_embeddings=exclude_embeddings)\n+        return total_params\n \n \n class EmbeddingAccessMixin:\n@@ -3218,7 +3011,7 @@ def save_pretrained(\n         model_to_save = unwrap_model(self)\n         # save the string version of dtype to the config, e.g. convert torch.float32 => \"float32\"\n         # we currently don't use this setting automatically, but may start to use with v5\n-        dtype = get_parameter_dtype(model_to_save)\n+        dtype = model_to_save.dtype\n         model_to_save.config.dtype = str(dtype).split(\".\")[1]\n \n         # Attach architecture to the config\n@@ -4237,17 +4030,9 @@ def _load_pretrained_model(\n \n         if is_deepspeed_zero3_enabled() and not is_quantized:\n             if state_dict is None:\n-                if checkpoint_files is None:\n-                    raise ValueError(\n-                        \"DeepSpeed ZeRO-3 initialization requires a state_dict or checkpoint files to load from.\"\n-                    )\n                 merged_state_dict = {}\n                 for ckpt_file in checkpoint_files:\n-                    merged_state_dict.update(\n-                        load_state_dict(\n-                            ckpt_file, is_quantized=is_quantized, map_location=\"cpu\", weights_only=weights_only\n-                        )\n-                    )\n+                    merged_state_dict.update(load_state_dict(ckpt_file, map_location=\"cpu\", weights_only=weights_only))\n                 state_dict = merged_state_dict\n             error_msgs += _load_state_dict_into_zero3_model(model, state_dict)\n             # This is not true but for now we assume only best-case scenario with deepspeed, i.e. perfectly matching checkpoints"
        },
        {
            "sha": "fe3afdc7bbd2110ad9432fdd0d901c243901c16b",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3eba206b0ca4de67aa2291667eadfa7498cb4609/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3eba206b0ca4de67aa2291667eadfa7498cb4609/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=3eba206b0ca4de67aa2291667eadfa7498cb4609",
            "patch": "@@ -34,7 +34,7 @@\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import CausalLMOutputWithPast, MaskedLMOutput\n-from ...modeling_utils import PreTrainedModel, get_parameter_device\n+from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     auto_docstring,\n     is_accelerate_available,\n@@ -338,7 +338,7 @@ def device(self) -> torch.device:\n \n         # if has _hf_hook, has been offloaded so the device has to be found in the hook\n         if not hasattr(self, \"_hf_hook\"):\n-            return get_parameter_device(self)\n+            return super().device\n         for module in self.modules():\n             if (\n                 hasattr(module, \"_hf_hook\")\n@@ -347,7 +347,7 @@ def device(self) -> torch.device:\n             ):\n                 return torch.device(module._hf_hook.execution_device)\n \n-        return get_parameter_device(self)\n+        return super().device\n \n \n # GPT2-like autoregressive model\n@@ -1318,7 +1318,7 @@ def device(self) -> torch.device:\n         # for bark_model, device must be verified on its sub-models\n         # if has _hf_hook, has been offloaded so the device has to be found in the hook\n         if not hasattr(self.semantic, \"_hf_hook\"):\n-            return get_parameter_device(self)\n+            return super().device\n         for module in self.semantic.modules():\n             if (\n                 hasattr(module, \"_hf_hook\")"
        },
        {
            "sha": "d0e7a966f0af5820dc86c7f290885bf802ca03ff",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3eba206b0ca4de67aa2291667eadfa7498cb4609/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3eba206b0ca4de67aa2291667eadfa7498cb4609/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=3eba206b0ca4de67aa2291667eadfa7498cb4609",
            "patch": "@@ -4639,10 +4639,11 @@ def floating_point_ops(self, inputs: dict[str, Union[torch.Tensor, Any]]):\n         Returns:\n             `int`: The number of floating-point operations.\n         \"\"\"\n-        if hasattr(self.model, \"floating_point_ops\"):\n-            return self.model.floating_point_ops(inputs)\n-        else:\n-            return 0\n+        if (main_input := getattr(self.model, \"main_input_name\", \"input_ids\")) in inputs and hasattr(\n+            self.model, \"num_parameters\"\n+        ):\n+            return 6 * inputs[main_input].numel() * self.model.num_parameters(exclude_embeddings=True)\n+        return 0\n \n     def init_hf_repo(self, token: Optional[str] = None):\n         \"\"\""
        }
    ],
    "stats": {
        "total": 276,
        "additions": 31,
        "deletions": 245
    }
}