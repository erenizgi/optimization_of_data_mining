{
    "author": "SunMarc",
    "message": "[v5] Bump min version of bitsandbytes to 0.46.1  (#41283)\n\n* bump bitsandbytes to 0.46.1\n\n* huge cleanup\n\n* style\n\n* fix\n\n* req\n\n* fix\n\n* importerror\n\n* fix",
    "sha": "13791d8f482aad9ac31c29866eafc760ccbfa0f3",
    "files": [
        {
            "sha": "018ea74904609cc2b53ec1f5df20c2fa35698e37",
            "filename": "src/transformers/integrations/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/13791d8f482aad9ac31c29866eafc760ccbfa0f3/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13791d8f482aad9ac31c29866eafc760ccbfa0f3/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2F__init__.py?ref=13791d8f482aad9ac31c29866eafc760ccbfa0f3",
            "patch": "@@ -34,10 +34,7 @@\n     \"bitsandbytes\": [\n         \"dequantize_and_replace\",\n         \"get_keys_to_not_convert\",\n-        \"replace_8bit_linear\",\n         \"replace_with_bnb_linear\",\n-        \"set_module_8bit_tensor_to_device\",\n-        \"set_module_quantized_tensor_to_device\",\n         \"validate_bnb_backend_availability\",\n     ],\n     \"deepspeed\": [\n@@ -184,10 +181,7 @@\n     from .bitsandbytes import (\n         dequantize_and_replace,\n         get_keys_to_not_convert,\n-        replace_8bit_linear,\n         replace_with_bnb_linear,\n-        set_module_8bit_tensor_to_device,\n-        set_module_quantized_tensor_to_device,\n         validate_bnb_backend_availability,\n     )\n     from .deepspeed import ("
        },
        {
            "sha": "be117ff3013efce1c5791f7fb2db5a98727c6c3d",
            "filename": "src/transformers/integrations/bitsandbytes.py",
            "status": "modified",
            "additions": 8,
            "deletions": 188,
            "changes": 196,
            "blob_url": "https://github.com/huggingface/transformers/blob/13791d8f482aad9ac31c29866eafc760ccbfa0f3/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13791d8f482aad9ac31c29866eafc760ccbfa0f3/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py?ref=13791d8f482aad9ac31c29866eafc760ccbfa0f3",
            "patch": "@@ -1,23 +1,20 @@\n-import importlib.metadata\n import inspect\n-import warnings\n from copy import deepcopy\n from inspect import signature\n \n-from packaging import version\n-\n from ..utils import (\n     get_available_devices,\n     is_accelerate_available,\n     is_bitsandbytes_available,\n-    is_bitsandbytes_multi_backend_available,\n     is_torch_available,\n     logging,\n )\n \n \n if is_bitsandbytes_available():\n     import bitsandbytes as bnb\n+\n+if is_torch_available():\n     import torch\n     import torch.nn as nn\n \n@@ -32,121 +29,6 @@\n logger = logging.get_logger(__name__)\n \n \n-def set_module_quantized_tensor_to_device(module, tensor_name, device, value=None, quantized_stats=None):\n-    \"\"\"\n-    A helper function to set a given tensor (parameter of buffer) of a module on a specific device (note that doing\n-    `param.to(device)` creates a new tensor not linked to the parameter, which is why we need this function). The\n-    function is adapted from `set_module_tensor_to_device` function from accelerate that is adapted to support the\n-    class `Int8Params` from `bitsandbytes`.\n-\n-    Args:\n-        module (`torch.nn.Module`):\n-            The module in which the tensor we want to move lives.\n-        tensor_name (`str`):\n-            The full name of the parameter/buffer.\n-        device (`int`, `str` or `torch.device`):\n-            The device on which to set the tensor.\n-        value (`torch.Tensor`, *optional*):\n-            The value of the tensor (useful when going from the meta device to any other device).\n-        quantized_stats (`dict[str, Any]`, *optional*):\n-            Dict with items for either 4-bit or 8-bit serialization\n-    \"\"\"\n-    # Recurse if needed\n-    if \".\" in tensor_name:\n-        splits = tensor_name.split(\".\")\n-        for split in splits[:-1]:\n-            new_module = getattr(module, split)\n-            if new_module is None:\n-                raise ValueError(f\"{module} has no attribute {split}.\")\n-            module = new_module\n-        tensor_name = splits[-1]\n-\n-    if tensor_name not in module._parameters and tensor_name not in module._buffers:\n-        raise ValueError(f\"{module} does not have a parameter or a buffer named {tensor_name}.\")\n-    is_buffer = tensor_name in module._buffers\n-    old_value = getattr(module, tensor_name)\n-\n-    if old_value.device == torch.device(\"meta\") and device not in [\"meta\", torch.device(\"meta\")] and value is None:\n-        raise ValueError(f\"{tensor_name} is on the meta device, we need a `value` to put in on {device}.\")\n-\n-    prequantized_loading = quantized_stats is not None\n-    if is_buffer or not is_bitsandbytes_available():\n-        is_8bit = False\n-        is_4bit = False\n-    else:\n-        is_4bit = hasattr(bnb.nn, \"Params4bit\") and isinstance(module._parameters[tensor_name], bnb.nn.Params4bit)\n-        is_8bit = isinstance(module._parameters[tensor_name], bnb.nn.Int8Params)\n-\n-    if is_8bit or is_4bit:\n-        param = module._parameters[tensor_name]\n-        if param.device.type != \"cuda\":\n-            if value is None:\n-                new_value = old_value.to(device)\n-            elif isinstance(value, torch.Tensor):\n-                new_value = value.to(\"cpu\")\n-            else:\n-                new_value = torch.tensor(value, device=\"cpu\")\n-\n-            # Support models using `Conv1D` in place of `nn.Linear` (e.g. openai-community/gpt2) by transposing the weight matrix prior to quantization.\n-            # Since weights are saved in the correct \"orientation\", we skip transposing when loading.\n-            if issubclass(module.source_cls, Conv1D) and not prequantized_loading:\n-                new_value = new_value.T\n-\n-            kwargs = old_value.__dict__\n-\n-            if prequantized_loading != (new_value.dtype in (torch.int8, torch.uint8)):\n-                raise ValueError(\n-                    f\"Value dtype `{new_value.dtype}` is not compatible with parameter quantization status.\"\n-                )\n-\n-            if is_8bit:\n-                is_8bit_serializable = version.parse(importlib.metadata.version(\"bitsandbytes\")) > version.parse(\n-                    \"0.37.2\"\n-                )\n-                if new_value.dtype in (torch.int8, torch.uint8) and not is_8bit_serializable:\n-                    raise ValueError(\n-                        \"Detected int8 weights but the version of bitsandbytes is not compatible with int8 serialization. \"\n-                        \"Make sure to download the latest `bitsandbytes` version. `pip install --upgrade bitsandbytes`.\"\n-                    )\n-                new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(device)\n-                if prequantized_loading:\n-                    setattr(new_value, \"SCB\", quantized_stats[\"SCB\"].to(device))\n-            elif is_4bit:\n-                if prequantized_loading:\n-                    is_4bit_serializable = version.parse(importlib.metadata.version(\"bitsandbytes\")) >= version.parse(\n-                        \"0.41.3\"\n-                    )\n-                    if new_value.dtype in (torch.int8, torch.uint8) and not is_4bit_serializable:\n-                        raise ValueError(\n-                            \"Detected 4-bit weights but the version of bitsandbytes is not compatible with 4-bit serialization. \"\n-                            \"Make sure to download the latest `bitsandbytes` version. `pip install --upgrade bitsandbytes`.\"\n-                        )\n-                    new_value = bnb.nn.Params4bit.from_prequantized(\n-                        data=new_value,\n-                        quantized_stats=quantized_stats,\n-                        requires_grad=False,\n-                        device=device,\n-                        **kwargs,\n-                    )\n-                else:\n-                    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(device)\n-            module._parameters[tensor_name] = new_value\n-\n-    else:\n-        if value is None:\n-            new_value = old_value.to(device)\n-        elif isinstance(value, torch.Tensor):\n-            new_value = value.to(device)\n-        else:\n-            new_value = torch.tensor(value, device=device)\n-\n-        if is_buffer:\n-            module._buffers[tensor_name] = new_value\n-        else:\n-            new_value = nn.Parameter(new_value, requires_grad=old_value.requires_grad)\n-            module._parameters[tensor_name] = new_value\n-\n-\n def _replace_with_bnb_linear(\n     model,\n     modules_to_not_convert=None,\n@@ -266,28 +148,9 @@ def replace_with_bnb_linear(model, modules_to_not_convert=None, current_key_name\n             \" Please double check your model architecture, or submit an issue on github if you think this is\"\n             \" a bug.\"\n         )\n-\n     return model\n \n \n-# For backward compatibility\n-def replace_8bit_linear(*args, **kwargs):\n-    warnings.warn(\n-        \"`replace_8bit_linear` will be deprecated in a future version, please use `replace_with_bnb_linear` instead\",\n-        FutureWarning,\n-    )\n-    return replace_with_bnb_linear(*args, **kwargs)\n-\n-\n-# For backward compatibility\n-def set_module_8bit_tensor_to_device(*args, **kwargs):\n-    warnings.warn(\n-        \"`set_module_8bit_tensor_to_device` will be deprecated in a future version, please use `set_module_quantized_tensor_to_device` instead\",\n-        FutureWarning,\n-    )\n-    return set_module_quantized_tensor_to_device(*args, **kwargs)\n-\n-\n def get_keys_to_not_convert(model):\n     r\"\"\"\n     An utility function to get the key of the module to keep in full precision if any For example for CausalLM modules\n@@ -305,11 +168,7 @@ def get_keys_to_not_convert(model):\n     tied_model.tie_weights()\n \n     tied_params = find_tied_parameters(tied_model)\n-    # For compatibility with Accelerate < 0.18\n-    if isinstance(tied_params, dict):\n-        tied_keys = sum(list(tied_params.values()), []) + list(tied_params.keys())\n-    else:\n-        tied_keys = sum(tied_params, [])\n+    tied_keys = sum(tied_params, [])\n     has_tied_params = len(tied_keys) > 0\n \n     # If there is not tied weights, we want to keep the lm_headï¼ˆoutput_embedding) in full precision\n@@ -481,7 +340,10 @@ def dequantize_and_replace(\n     return model\n \n \n-def _validate_bnb_multi_backend_availability(raise_exception):\n+def validate_bnb_backend_availability(raise_exception=False):\n+    \"\"\"\n+    Validates if the available devices are supported by bitsandbytes, optionally raising an exception if not.\n+    \"\"\"\n     import bitsandbytes as bnb\n \n     bnb_supported_devices = getattr(bnb, \"supported_torch_devices\", set())\n@@ -493,50 +355,8 @@ def _validate_bnb_multi_backend_availability(raise_exception):\n                 f\"None of the available devices `available_devices = {available_devices or None}` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {bnb_supported_devices}`. \"\n                 \"Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation\"\n             )\n-\n-            logger.error(err_msg)\n             raise RuntimeError(err_msg)\n \n-        logger.warning(\"No supported devices found for bitsandbytes multi-backend.\")\n+        logger.warning(\"No supported devices found for bitsandbytes\")\n         return False\n-\n-    logger.debug(\"Multi-backend validation successful.\")\n     return True\n-\n-\n-def _validate_bnb_cuda_backend_availability(raise_exception):\n-    if not is_torch_available():\n-        return False\n-\n-    import torch\n-\n-    if not torch.cuda.is_available():\n-        log_msg = (\n-            \"CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. \"\n-            \"Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\"\n-        )\n-        if raise_exception:\n-            logger.error(log_msg)\n-            raise RuntimeError(log_msg)\n-\n-        logger.warning(log_msg)\n-        return False\n-\n-    logger.debug(\"CUDA backend validation successful.\")\n-    return True\n-\n-\n-def validate_bnb_backend_availability(raise_exception=False):\n-    \"\"\"\n-    Validates if the available devices are supported by bitsandbytes, optionally raising an exception if not.\n-    \"\"\"\n-    if not is_bitsandbytes_available():\n-        if importlib.util.find_spec(\"bitsandbytes\") and version.parse(\n-            importlib.metadata.version(\"bitsandbytes\")\n-        ) < version.parse(\"0.43.1\"):\n-            return _validate_bnb_cuda_backend_availability(raise_exception)\n-        return False\n-\n-    if is_bitsandbytes_multi_backend_available():\n-        return _validate_bnb_multi_backend_availability(raise_exception)\n-    return _validate_bnb_cuda_backend_availability(raise_exception)"
        },
        {
            "sha": "1aba123f4b540386b83df484a00aa095446a65a5",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/13791d8f482aad9ac31c29866eafc760ccbfa0f3/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13791d8f482aad9ac31c29866eafc760ccbfa0f3/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=13791d8f482aad9ac31c29866eafc760ccbfa0f3",
            "patch": "@@ -4116,11 +4116,6 @@ def cuda(self, *args, **kwargs):\n                     \"Calling `cuda()` is not supported for `8-bit` quantized models. \"\n                     \" Please use the model as it is, since the model has already been set to the correct devices.\"\n                 )\n-            elif version.parse(importlib.metadata.version(\"bitsandbytes\")) < version.parse(\"0.43.2\"):\n-                raise ValueError(\n-                    \"Calling `cuda()` is not supported for `4-bit` quantized models with the installed version of bitsandbytes. \"\n-                    f\"The current device is `{self.device}`. If you intended to move the model, please install bitsandbytes >= 0.43.2.\"\n-                )\n         return super().cuda(*args, **kwargs)\n \n     @wraps(torch.nn.Module.to)\n@@ -4177,11 +4172,6 @@ def to(self, *args, **kwargs):\n                     \"`.to` is not supported for `8-bit` bitsandbytes models. Please use the model as it is, since the\"\n                     \" model has already been set to the correct devices and casted to the correct `dtype`.\"\n                 )\n-            elif version.parse(importlib.metadata.version(\"bitsandbytes\")) < version.parse(\"0.43.2\"):\n-                raise ValueError(\n-                    \"Calling `to()` is not supported for `4-bit` quantized models with the installed version of bitsandbytes. \"\n-                    f\"The current device is `{self.device}`. If you intended to move the model, please install bitsandbytes >= 0.43.2.\"\n-                )\n         elif getattr(self, \"quantization_method\", None) == QuantizationMethod.GPTQ:\n             if dtype_present_in_args:\n                 raise ValueError("
        },
        {
            "sha": "5e344a37af9395796c48dfc7cb4800c6326812bd",
            "filename": "src/transformers/quantizers/quantizer_bnb_4bit.py",
            "status": "modified",
            "additions": 7,
            "deletions": 33,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/13791d8f482aad9ac31c29866eafc760ccbfa0f3/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13791d8f482aad9ac31c29866eafc760ccbfa0f3/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py?ref=13791d8f482aad9ac31c29866eafc760ccbfa0f3",
            "patch": "@@ -11,13 +11,10 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import importlib\n from collections import defaultdict\n from functools import cached_property\n from typing import TYPE_CHECKING, Optional, Union\n \n-from packaging import version\n-\n from .base import HfQuantizer\n from .quantizers_utils import get_module_from_name\n \n@@ -27,6 +24,7 @@\n \n from ..utils import (\n     ACCELERATE_MIN_VERSION,\n+    BITSANDBYTES_MIN_VERSION,\n     is_accelerate_available,\n     is_bitsandbytes_available,\n     is_torch_available,\n@@ -47,7 +45,7 @@\n \n class Bnb4BitHfQuantizer(HfQuantizer):\n     \"\"\"\n-    4-bit quantization from bitsandbytes.py quantization method:\n+    4-bit quantization from bitsandbytes quantization method:\n         before loading: converts transformer layers into Linear4bit during loading: load 16bit weight and pass to the\n         layer object after: quantizes individual weights in Linear4bit into 4bit at the first .cuda() call\n         saving:\n@@ -80,30 +78,15 @@ def __init__(self, quantization_config, **kwargs):\n     def validate_environment(self, *args, **kwargs):\n         if not is_accelerate_available():\n             raise ImportError(\n-                f\"Using `bitsandbytes` 4-bit quantization requires Accelerate: `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`\"\n-            )\n-        if not is_bitsandbytes_available(check_library_only=True):\n-            raise ImportError(\n-                \"Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\"\n+                f\"Using `bitsandbytes` 4-bit quantization requires accelerate: `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`\"\n             )\n-        if not is_torch_available():\n+        if not is_bitsandbytes_available():\n             raise ImportError(\n-                \"The bitsandbytes library requires PyTorch but it was not found in your environment. \"\n-                \"You can install it with `pip install torch`.\"\n+                f\"Using `bitsandbytes` 4-bit quantization requires bitsandbytes: `pip install -U bitsandbytes>={BITSANDBYTES_MIN_VERSION}`\"\n             )\n-        # `bitsandbytes` versions older than 0.43.1 eagerly require CUDA at import time,\n-        # so those versions of the library are practically only available when CUDA is too.\n-        if version.parse(importlib.metadata.version(\"bitsandbytes\")) < version.parse(\"0.43.1\"):\n-            if not torch.cuda.is_available():\n-                raise ImportError(\n-                    \"The installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. \"\n-                    \"You may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1.\"\n-                )\n \n         from ..integrations import validate_bnb_backend_availability\n-        from ..utils import is_bitsandbytes_multi_backend_available\n \n-        bnb_multibackend_is_enabled = is_bitsandbytes_multi_backend_available()\n         validate_bnb_backend_availability(raise_exception=True)\n \n         device_map = kwargs.get(\"device_map\")\n@@ -115,7 +98,7 @@ def validate_environment(self, *args, **kwargs):\n             device_map_without_lm_head = {\n                 key: device_map[key] for key in device_map if key not in self.modules_to_not_convert\n             }\n-            if set(device_map.values()) == {\"cpu\"} and bnb_multibackend_is_enabled:\n+            if set(device_map.values()) == {\"cpu\"}:\n                 pass\n             elif \"cpu\" in device_map_without_lm_head.values() or \"disk\" in device_map_without_lm_head.values():\n                 raise ValueError(\n@@ -289,15 +272,6 @@ def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs\n         return model\n \n     def is_serializable(self, safe_serialization=None):\n-        _is_4bit_serializable = version.parse(importlib.metadata.version(\"bitsandbytes\")) >= version.parse(\"0.41.3\")\n-\n-        if not _is_4bit_serializable:\n-            logger.warning(\n-                \"You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. \"\n-                \"If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\"\n-            )\n-            return False\n-\n         return True\n \n     @cached_property\n@@ -307,7 +281,7 @@ def is_bnb_supports_quant_storage_module(self) -> bool:\n         the `module` parameter in `Params4bit.from_prequantized`\n         :return:\n         \"\"\"\n-        return version.parse(importlib.metadata.version(\"bitsandbytes\")) >= version.parse(\"0.43.3\")\n+        return True\n \n     @property\n     def is_trainable(self) -> bool:"
        },
        {
            "sha": "ac00dcf52dcc0dbf062b8db125e507848c4c19a5",
            "filename": "src/transformers/quantizers/quantizer_bnb_8bit.py",
            "status": "modified",
            "additions": 6,
            "deletions": 41,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/13791d8f482aad9ac31c29866eafc760ccbfa0f3/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13791d8f482aad9ac31c29866eafc760ccbfa0f3/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py?ref=13791d8f482aad9ac31c29866eafc760ccbfa0f3",
            "patch": "@@ -11,11 +11,8 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import importlib\n from typing import TYPE_CHECKING, Optional, Union\n \n-from packaging import version\n-\n from .base import HfQuantizer\n \n \n@@ -24,6 +21,7 @@\n \n from ..utils import (\n     ACCELERATE_MIN_VERSION,\n+    BITSANDBYTES_MIN_VERSION,\n     is_accelerate_available,\n     is_bitsandbytes_available,\n     is_torch_available,\n@@ -67,30 +65,15 @@ def __init__(self, quantization_config, **kwargs):\n     def validate_environment(self, *args, **kwargs):\n         if not is_accelerate_available():\n             raise ImportError(\n-                f\"Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`\"\n-            )\n-        if not is_bitsandbytes_available(check_library_only=True):\n-            raise ImportError(\n-                \"Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\"\n+                f\"Using `bitsandbytes` 8-bit quantization requires accelerate: `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`\"\n             )\n-        if not is_torch_available():\n+        if not is_bitsandbytes_available():\n             raise ImportError(\n-                \"The bitsandbytes library requires PyTorch but it was not found in your environment. \"\n-                \"You can install it with `pip install torch`.\"\n+                f\"Using `bitsandbytes` 8-bit quantization requires bitsandbytes: `pip install -U bitsandbytes>={BITSANDBYTES_MIN_VERSION}`\"\n             )\n-        # `bitsandbytes` versions older than 0.43.1 eagerly require CUDA at import time,\n-        # so those versions of the library are practically only available when CUDA is too.\n-        if version.parse(importlib.metadata.version(\"bitsandbytes\")) < version.parse(\"0.43.1\"):\n-            if not torch.cuda.is_available():\n-                raise ImportError(\n-                    \"The installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. \"\n-                    \"You may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1.\"\n-                )\n \n         from ..integrations import validate_bnb_backend_availability\n-        from ..utils import is_bitsandbytes_multi_backend_available\n \n-        bnb_multibackend_is_enabled = is_bitsandbytes_multi_backend_available()\n         validate_bnb_backend_availability(raise_exception=True)\n \n         device_map = kwargs.get(\"device_map\")\n@@ -102,7 +85,7 @@ def validate_environment(self, *args, **kwargs):\n             device_map_without_lm_head = {\n                 key: device_map[key] for key in device_map if key not in self.modules_to_not_convert\n             }\n-            if set(device_map.values()) == {\"cpu\"} and bnb_multibackend_is_enabled:\n+            if set(device_map.values()) == {\"cpu\"}:\n                 pass\n             elif \"cpu\" in device_map_without_lm_head.values() or \"disk\" in device_map_without_lm_head.values():\n                 raise ValueError(\n@@ -114,12 +97,6 @@ def validate_environment(self, *args, **kwargs):\n                     \"for more details. \"\n                 )\n \n-        if version.parse(importlib.metadata.version(\"bitsandbytes\")) < version.parse(\"0.37.2\"):\n-            raise ValueError(\n-                \"You have a version of `bitsandbytes` that is not compatible with 8bit inference and training\"\n-                \" make sure you have the latest version of `bitsandbytes` installed\"\n-            )\n-\n     def adjust_max_memory(self, max_memory: dict[str, Union[int, str]]) -> dict[str, Union[int, str]]:\n         # need more space for buffers that are created during quantization\n         max_memory = {key: val * 0.90 for key, val in max_memory.items()}\n@@ -248,23 +225,11 @@ def _process_model_before_weight_loading(\n         model.config.quantization_config = self.quantization_config\n \n     def is_serializable(self, safe_serialization=None):\n-        _bnb_supports_8bit_serialization = version.parse(importlib.metadata.version(\"bitsandbytes\")) > version.parse(\n-            \"0.37.2\"\n-        )\n-\n-        if not _bnb_supports_8bit_serialization:\n-            logger.warning(\n-                \"You are calling `save_pretrained` to a 8-bit converted model, but your `bitsandbytes` version doesn't support it. \"\n-                \"If you want to save 8-bit models, make sure to have `bitsandbytes>0.37.2` installed. You will most likely face errors or\"\n-                \" unexpected behaviours.\"\n-            )\n-            return False\n-\n         return True\n \n     @property\n     def is_trainable(self) -> bool:\n-        return version.parse(importlib.metadata.version(\"bitsandbytes\")) >= version.parse(\"0.37.0\")\n+        return True\n \n     def _dequantize(self, model):\n         from ..integrations import dequantize_and_replace"
        },
        {
            "sha": "0a5aee795ee030fa192a84606654c49a45ba9579",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 19,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/13791d8f482aad9ac31c29866eafc760ccbfa0f3/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13791d8f482aad9ac31c29866eafc760ccbfa0f3/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=13791d8f482aad9ac31c29866eafc760ccbfa0f3",
            "patch": "@@ -77,7 +77,6 @@\n     is_auto_round_available,\n     is_av_available,\n     is_bitsandbytes_available,\n-    is_bitsandbytes_multi_backend_available,\n     is_bs4_available,\n     is_compressed_tensors_available,\n     is_cv2_available,\n@@ -1072,15 +1071,6 @@ def require_torch_large_accelerator(test_case, memory: float = 20):\n     )(test_case)\n \n \n-def require_torch_gpu_if_bnb_not_multi_backend_enabled(test_case):\n-    \"\"\"\n-    Decorator marking a test that requires a GPU if bitsandbytes multi-backend feature is not enabled.\n-    \"\"\"\n-    if is_bitsandbytes_available() and is_bitsandbytes_multi_backend_available():\n-        return test_case\n-    return require_torch_gpu(test_case)\n-\n-\n def require_torch_accelerator(test_case):\n     \"\"\"Decorator marking a test that requires an accessible accelerator and PyTorch.\"\"\"\n     return unittest.skipUnless(torch_device is not None and torch_device != \"cpu\", \"test requires accelerator\")(\n@@ -1290,15 +1280,7 @@ def require_bitsandbytes(test_case):\n     \"\"\"\n     Decorator marking a test that requires the bitsandbytes library. Will be skipped when the library or its hard dependency torch is not installed.\n     \"\"\"\n-    if is_bitsandbytes_available() and is_torch_available():\n-        try:\n-            import pytest\n-\n-            return pytest.mark.bitsandbytes(test_case)\n-        except ImportError:\n-            return test_case\n-    else:\n-        return unittest.skip(reason=\"test requires bitsandbytes and torch\")(test_case)\n+    return unittest.skipUnless(is_bitsandbytes_available(), \"test requires bitsandbytes\")(test_case)\n \n \n def require_optimum(test_case):"
        },
        {
            "sha": "a59d04da31a55c7049032f9268ff77c6e12325c9",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 46,
            "deletions": 59,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/13791d8f482aad9ac31c29866eafc760ccbfa0f3/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13791d8f482aad9ac31c29866eafc760ccbfa0f3/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=13791d8f482aad9ac31c29866eafc760ccbfa0f3",
            "patch": "@@ -1454,70 +1454,57 @@ def optimizer_hook(param):\n             OptimizerNames.RMSPROP_8BIT,\n             OptimizerNames.RMSPROP_32BIT,\n         ]:\n-            try:\n-                from bitsandbytes.optim import AdamW, Lion, RMSprop\n-\n-                is_paged = False\n-                optim_bits = 32\n-                optimizer_cls = None\n-                additional_optim_kwargs = adam_kwargs\n-                if \"paged\" in args.optim:\n-                    is_paged = True\n-                if \"8bit\" in args.optim:\n-                    optim_bits = 8\n-                if \"adam\" in args.optim:\n-                    optimizer_cls = AdamW\n-                elif \"lion\" in args.optim:\n-                    optimizer_cls = Lion\n-                    additional_optim_kwargs = {\"betas\": (args.adam_beta1, args.adam_beta2)}\n-                elif \"rmsprop\" in args.optim:\n-                    optimizer_cls = RMSprop\n-                    # Above we pass all `adam_kwargs` to the optimizer, here\n-                    # we only pass `optim_args` which can be passed by the user.\n-                    additional_optim_kwargs = optim_args\n-                elif \"ademamix\" in args.optim:\n-                    if is_bitsandbytes_available() and version.parse(\n-                        importlib.metadata.version(\"bitsandbytes\")\n-                    ) < version.parse(\"0.44.0\"):\n-                        raise ValueError(\n-                            \"The AdEMAMix optimizer is not supported by your current version of `bitsandbytes`. \"\n-                            \"Please install `bitsandbytes` >= 0.44.0.\"\n-                        )\n-\n-                    from bitsandbytes.optim import AdEMAMix\n+            if not is_bitsandbytes_available():\n+                raise ImportError(\n+                    \"You need to install `bitsandbytes` in order to use bitsandbytes optimizers: `pip install -U bitsandbytes`\"\n+                )\n \n-                    optimizer_cls = AdEMAMix\n-                    additional_optim_kwargs = {\n-                        \"betas\": (\n-                            float(optim_args.get(\"beta1\", args.adam_beta1)),\n-                            float(optim_args.get(\"beta2\", args.adam_beta2)),\n-                            float(optim_args.get(\"beta3\", 0.9999)),\n-                        ),\n-                        \"alpha\": float(optim_args.get(\"alpha\", 5.0)),\n-                        \"eps\": float(optim_args.get(\"eps\", args.adam_epsilon)),\n-                    }\n+            from bitsandbytes.optim import AdamW, Lion, RMSprop\n+\n+            is_paged = False\n+            optim_bits = 32\n+            optimizer_cls = None\n+            additional_optim_kwargs = adam_kwargs\n+            if \"paged\" in args.optim:\n+                is_paged = True\n+            if \"8bit\" in args.optim:\n+                optim_bits = 8\n+            if \"adam\" in args.optim:\n+                optimizer_cls = AdamW\n+            elif \"lion\" in args.optim:\n+                optimizer_cls = Lion\n+                additional_optim_kwargs = {\"betas\": (args.adam_beta1, args.adam_beta2)}\n+            elif \"rmsprop\" in args.optim:\n+                optimizer_cls = RMSprop\n+                # Above we pass all `adam_kwargs` to the optimizer, here\n+                # we only pass `optim_args` which can be passed by the user.\n+                additional_optim_kwargs = optim_args\n+            elif \"ademamix\" in args.optim:\n+                from bitsandbytes.optim import AdEMAMix\n+\n+                optimizer_cls = AdEMAMix\n+                additional_optim_kwargs = {\n+                    \"betas\": (\n+                        float(optim_args.get(\"beta1\", args.adam_beta1)),\n+                        float(optim_args.get(\"beta2\", args.adam_beta2)),\n+                        float(optim_args.get(\"beta3\", 0.9999)),\n+                    ),\n+                    \"alpha\": float(optim_args.get(\"alpha\", 5.0)),\n+                    \"eps\": float(optim_args.get(\"eps\", args.adam_epsilon)),\n+                }\n \n-                    if \"t_alpha\" in optim_args:\n-                        additional_optim_kwargs[\"t_alpha\"] = int(optim_args[\"t_alpha\"])\n+                if \"t_alpha\" in optim_args:\n+                    additional_optim_kwargs[\"t_alpha\"] = int(optim_args[\"t_alpha\"])\n \n-                    if \"t_beta3\" in optim_args:\n-                        additional_optim_kwargs[\"t_beta3\"] = int(optim_args[\"t_beta3\"])\n+                if \"t_beta3\" in optim_args:\n+                    additional_optim_kwargs[\"t_beta3\"] = int(optim_args[\"t_beta3\"])\n \n-                bnb_kwargs = {\"optim_bits\": optim_bits}\n-                if \"rmsprop\" not in args.optim:\n-                    bnb_kwargs[\"is_paged\"] = is_paged\n+            bnb_kwargs = {\"optim_bits\": optim_bits}\n+            if \"rmsprop\" not in args.optim:\n+                bnb_kwargs[\"is_paged\"] = is_paged\n \n-                optimizer_kwargs.update(additional_optim_kwargs)\n-                optimizer_kwargs.update(bnb_kwargs)\n-            except ImportError:\n-                raise ValueError(\"Trainer tried to instantiate bnb optimizer but `bitsandbytes` is not installed!\")\n-            if is_bitsandbytes_available() and version.parse(\n-                importlib.metadata.version(\"bitsandbytes\")\n-            ) < version.parse(\"0.41.1\"):\n-                logger.warning(\n-                    \"You are using 8-bit optimizers with a version of `bitsandbytes` < 0.41.1. \"\n-                    \"It is recommended to update your version as a major bug has been fixed in 8-bit optimizers.\"\n-                )\n+            optimizer_kwargs.update(additional_optim_kwargs)\n+            optimizer_kwargs.update(bnb_kwargs)\n         elif args.optim == OptimizerNames.ADAMW_ANYPRECISION:\n             try:\n                 from torchdistx.optimizers import AnyPrecisionAdamW"
        },
        {
            "sha": "82a9e3a85bd1830b93916d13d1cebbb93a932494",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/13791d8f482aad9ac31c29866eafc760ccbfa0f3/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13791d8f482aad9ac31c29866eafc760ccbfa0f3/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=13791d8f482aad9ac31c29866eafc760ccbfa0f3",
            "patch": "@@ -105,6 +105,7 @@\n )\n from .import_utils import (\n     ACCELERATE_MIN_VERSION,\n+    BITSANDBYTES_MIN_VERSION,\n     ENV_VARS_TRUE_AND_AUTO_VALUES,\n     ENV_VARS_TRUE_VALUES,\n     GGUF_MIN_VERSION,\n@@ -125,7 +126,6 @@\n     is_auto_round_available,\n     is_av_available,\n     is_bitsandbytes_available,\n-    is_bitsandbytes_multi_backend_available,\n     is_bs4_available,\n     is_ccl_available,\n     is_coloredlogs_available,"
        },
        {
            "sha": "71707cf5659909f7e28f939e91df6c48e64aba43",
            "filename": "src/transformers/utils/bitsandbytes.py",
            "status": "removed",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Futils%2Fbitsandbytes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Futils%2Fbitsandbytes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fbitsandbytes.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -1,28 +0,0 @@\n-# Copyright 2023 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import warnings\n-\n-\n-warnings.warn(\n-    \"transformers.utils.bitsandbytes module is deprecated and will be removed in a future version. Please import bitsandbytes modules directly from transformers.integrations\",\n-    FutureWarning,\n-)\n-\n-from ..integrations import (  # noqa\n-    get_keys_to_not_convert,\n-    replace_8bit_linear,\n-    replace_with_bnb_linear,\n-    set_module_8bit_tensor_to_device,\n-    set_module_quantized_tensor_to_device,\n-)"
        },
        {
            "sha": "118b5c024cc07b5cdbd37b158c6cca4e55f5e542",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 20,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/13791d8f482aad9ac31c29866eafc760ccbfa0f3/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13791d8f482aad9ac31c29866eafc760ccbfa0f3/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=13791d8f482aad9ac31c29866eafc760ccbfa0f3",
            "patch": "@@ -72,6 +72,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n USE_TORCH_XLA = os.environ.get(\"USE_TORCH_XLA\", \"1\").upper()\n \n ACCELERATE_MIN_VERSION = \"1.1.0\"\n+BITSANDBYTES_MIN_VERSION = \"0.46.1\"\n SCHEDULEFREE_MIN_VERSION = \"1.2.6\"\n FSDP_MIN_VERSION = \"1.12.0\"\n GGUF_MIN_VERSION = \"0.10.0\"\n@@ -814,27 +815,9 @@ def get_major_and_minor_from_version(full_version):\n \n \n @lru_cache\n-def is_bitsandbytes_available(check_library_only: bool = False) -> bool:\n+def is_bitsandbytes_available(min_version: str = BITSANDBYTES_MIN_VERSION) -> bool:\n     is_available, bitsandbytes_version = _is_package_available(\"bitsandbytes\", return_version=True)\n-    if check_library_only or not is_available:\n-        return is_available\n-\n-    # `bitsandbytes` versions older than 0.43.1 eagerly require CUDA at import time,\n-    # so those versions of the library are practically only available when CUDA is too.\n-    if version.parse(bitsandbytes_version) < version.parse(\"0.43.1\"):\n-        return is_torch_cuda_available() and is_available\n-    # Newer versions of `bitsandbytes` can be imported on systems without CUDA.\n-    return is_torch_available() and is_available\n-\n-\n-@lru_cache\n-def is_bitsandbytes_multi_backend_available() -> bool:\n-    if not is_bitsandbytes_available():\n-        return False\n-\n-    import bitsandbytes as bnb\n-\n-    return \"multi_backend\" in getattr(bnb, \"features\", set())\n+    return is_available and version.parse(bitsandbytes_version) >= version.parse(min_version)\n \n \n @lru_cache"
        },
        {
            "sha": "b7ca32d6e0d2e9b6d782fb4ebf8ef246b08b08c4",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/13791d8f482aad9ac31c29866eafc760ccbfa0f3/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13791d8f482aad9ac31c29866eafc760ccbfa0f3/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=13791d8f482aad9ac31c29866eafc760ccbfa0f3",
            "patch": "@@ -565,13 +565,6 @@ def post_init(self):\n         if not isinstance(self.bnb_4bit_use_double_quant, bool):\n             raise TypeError(\"bnb_4bit_use_double_quant must be a boolean\")\n \n-        if self.load_in_4bit and not version.parse(importlib.metadata.version(\"bitsandbytes\")) >= version.parse(\n-            \"0.39.0\"\n-        ):\n-            raise ValueError(\n-                \"4 bit quantization requires bitsandbytes>=0.39.0 - please upgrade your bitsandbytes version\"\n-            )\n-\n     def is_quantizable(self):\n         r\"\"\"\n         Returns `True` if the model is quantizable, `False` otherwise."
        },
        {
            "sha": "bd61ca63f889eb3f972475e46ffbbb96be65ef73",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/13791d8f482aad9ac31c29866eafc760ccbfa0f3/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13791d8f482aad9ac31c29866eafc760ccbfa0f3/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=13791d8f482aad9ac31c29866eafc760ccbfa0f3",
            "patch": "@@ -12,12 +12,10 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import gc\n-import importlib.metadata\n import tempfile\n import unittest\n \n import pytest\n-from packaging import version\n \n from transformers import (\n     AutoConfig,\n@@ -40,7 +38,6 @@\n     require_accelerate,\n     require_bitsandbytes,\n     require_torch,\n-    require_torch_gpu_if_bnb_not_multi_backend_enabled,\n     require_torch_multi_accelerator,\n     slow,\n     torch_device,\n@@ -92,7 +89,6 @@ def forward(self, input, *args, **kwargs):\n @require_bitsandbytes\n @require_accelerate\n @require_torch\n-@require_torch_gpu_if_bnb_not_multi_backend_enabled\n @slow\n class Base4bitTest(unittest.TestCase):\n     # We keep the constants inside the init function and model loading inside setUp function\n@@ -298,9 +294,6 @@ def test_to_device_dequantized(self):\n         model_4bit.to(dtype=torch.float16)\n \n     def test_device_assignment(self):\n-        if version.parse(importlib.metadata.version(\"bitsandbytes\")) < version.parse(\"0.43.2\"):\n-            self.skipTest(reason=\"This test requires bitsandbytes >= 0.43.2\")\n-\n         mem_before = self.model_4bit.get_memory_footprint()\n \n         # Move to CPU\n@@ -322,20 +315,6 @@ def test_device_and_dtype_assignment(self):\n         to prevent invalid conversions.\n         \"\"\"\n \n-        # Moving with `to` or `cuda` is not supported with versions < 0.43.2.\n-        if version.parse(importlib.metadata.version(\"bitsandbytes\")) < version.parse(\"0.43.2\"):\n-            with self.assertRaises(ValueError):\n-                # Tries with `str`\n-                self.model_4bit.to(\"cpu\")\n-\n-            with self.assertRaises(ValueError):\n-                # Tries with a `device`\n-                self.model_4bit.to(torch.device(\"cuda:0\"))\n-\n-            with self.assertRaises(ValueError):\n-                # Tries with `cuda`\n-                self.model_4bit.cuda()\n-\n         with self.assertRaises(ValueError):\n             # Tries with a `dtype`\n             self.model_4bit.to(torch.float16)\n@@ -389,7 +368,6 @@ def test_bnb_4bit_wrong_config(self):\n @require_bitsandbytes\n @require_accelerate\n @require_torch\n-@require_torch_gpu_if_bnb_not_multi_backend_enabled\n @slow\n @apply_skip_if_not_implemented\n class Bnb4BitT5Test(unittest.TestCase):\n@@ -615,9 +593,6 @@ def setUp(self):\n         super().setUp()\n \n     def test_training(self):\n-        if version.parse(importlib.metadata.version(\"bitsandbytes\")) < version.parse(\"0.37.0\"):\n-            self.skipTest(reason=\"This test requires bitsandbytes >= 0.37.0\")\n-\n         # Step 1: freeze all parameters\n         model = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True)\n \n@@ -672,7 +647,6 @@ class Bnb4BitLlamaTest(Bnb4BitTest):\n @require_bitsandbytes\n @require_accelerate\n @require_torch\n-@require_torch_gpu_if_bnb_not_multi_backend_enabled\n @slow\n @apply_skip_if_not_implemented\n class BaseSerializationTest(unittest.TestCase):\n@@ -820,7 +794,6 @@ class LlamaSerializationTest(BaseSerializationTest):\n \n @require_bitsandbytes\n @require_accelerate\n-@require_torch_gpu_if_bnb_not_multi_backend_enabled\n @slow\n @apply_skip_if_not_implemented\n class Bnb4BitTestBasicConfigTest(unittest.TestCase):\n@@ -836,7 +809,6 @@ def test_set_load_in_8_bit(self):\n \n @require_bitsandbytes\n @require_accelerate\n-@require_torch_gpu_if_bnb_not_multi_backend_enabled\n @slow\n @apply_skip_if_not_implemented\n class Bnb4bitCompile(unittest.TestCase):"
        },
        {
            "sha": "e54926867ae281587538f865177ee5e6f3704d01",
            "filename": "tests/quantization/bnb/test_mixed_int8.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/13791d8f482aad9ac31c29866eafc760ccbfa0f3/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13791d8f482aad9ac31c29866eafc760ccbfa0f3/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py?ref=13791d8f482aad9ac31c29866eafc760ccbfa0f3",
            "patch": "@@ -12,12 +12,10 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import gc\n-import importlib.metadata\n import tempfile\n import unittest\n \n import pytest\n-from packaging import version\n \n from transformers import (\n     AutoConfig,\n@@ -41,7 +39,6 @@\n     require_accelerate,\n     require_bitsandbytes,\n     require_torch,\n-    require_torch_gpu_if_bnb_not_multi_backend_enabled,\n     require_torch_multi_accelerator,\n     slow,\n     torch_device,\n@@ -93,7 +90,6 @@ def forward(self, input, *args, **kwargs):\n @require_bitsandbytes\n @require_accelerate\n @require_torch\n-@require_torch_gpu_if_bnb_not_multi_backend_enabled\n @slow\n class BaseMixedInt8Test(unittest.TestCase):\n     # We keep the constants inside the init function and model loading inside setUp function\n@@ -472,7 +468,6 @@ def test_int8_from_pretrained(self):\n @require_bitsandbytes\n @require_accelerate\n @require_torch\n-@require_torch_gpu_if_bnb_not_multi_backend_enabled\n @slow\n class MixedInt8T5Test(unittest.TestCase):\n     @classmethod\n@@ -871,9 +866,6 @@ def setUp(self):\n         super().setUp()\n \n     def test_training(self):\n-        if version.parse(importlib.metadata.version(\"bitsandbytes\")) < version.parse(\"0.37.0\"):\n-            self.skipTest(reason=\"This test requires bitsandbytes>=0.37.0\")\n-\n         # Step 1: freeze all parameters\n         model = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True)\n         model.train()\n@@ -983,7 +975,6 @@ def test_int8_from_pretrained(self):\n @require_bitsandbytes\n @require_accelerate\n @require_torch\n-@require_torch_gpu_if_bnb_not_multi_backend_enabled\n @slow\n @apply_skip_if_not_implemented\n class Bnb8bitCompile(unittest.TestCase):"
        },
        {
            "sha": "a09543955d567dcf391d613d49b1ed5b77200ffb",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 45,
            "deletions": 36,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/13791d8f482aad9ac31c29866eafc760ccbfa0f3/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13791d8f482aad9ac31c29866eafc760ccbfa0f3/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=13791d8f482aad9ac31c29866eafc760ccbfa0f3",
            "patch": "@@ -14,7 +14,6 @@\n \n import dataclasses\n import gc\n-import importlib\n import json\n import math\n import os\n@@ -33,7 +32,6 @@\n import numpy as np\n import pytest\n from huggingface_hub import ModelCard, create_branch, list_repo_commits, list_repo_files\n-from packaging import version\n from parameterized import parameterized\n \n from transformers import (\n@@ -5856,35 +5854,34 @@ def hp_name(trial):\n             )\n         )\n \n-        if version.parse(importlib.metadata.version(\"bitsandbytes\")) >= version.parse(\"0.44.0\"):\n-            optim_test_params.append(\n-                (\n-                    OptimizerNames.ADEMAMIX,\n-                    bnb.optim.AdEMAMix,\n-                    default_ademamix_kwargs,\n-                )\n+        optim_test_params.append(\n+            (\n+                OptimizerNames.ADEMAMIX,\n+                bnb.optim.AdEMAMix,\n+                default_ademamix_kwargs,\n             )\n-            optim_test_params.append(\n-                (\n-                    OptimizerNames.ADEMAMIX_8BIT,\n-                    bnb.optim.AdEMAMix,\n-                    default_ademamix_kwargs,\n-                )\n+        )\n+        optim_test_params.append(\n+            (\n+                OptimizerNames.ADEMAMIX_8BIT,\n+                bnb.optim.AdEMAMix,\n+                default_ademamix_kwargs,\n             )\n-            optim_test_params.append(\n-                (\n-                    OptimizerNames.PAGED_ADEMAMIX_8BIT,\n-                    bnb.optim.AdEMAMix,\n-                    default_ademamix_kwargs,\n-                )\n+        )\n+        optim_test_params.append(\n+            (\n+                OptimizerNames.PAGED_ADEMAMIX_8BIT,\n+                bnb.optim.AdEMAMix,\n+                default_ademamix_kwargs,\n             )\n-            optim_test_params.append(\n-                (\n-                    OptimizerNames.PAGED_ADEMAMIX,\n-                    bnb.optim.AdEMAMix,\n-                    default_ademamix_kwargs,\n-                )\n+        )\n+        optim_test_params.append(\n+            (\n+                OptimizerNames.PAGED_ADEMAMIX,\n+                bnb.optim.AdEMAMix,\n+                default_ademamix_kwargs,\n             )\n+        )\n \n     if is_torchdistx_available():\n         import torchdistx\n@@ -5965,6 +5962,7 @@ def test_fused_adam_no_apex(self):\n                 with self.assertRaises(ValueError):\n                     Trainer.get_optimizer_cls_and_kwargs(args)\n \n+    @require_bitsandbytes\n     def test_bnb_adam8bit(self):\n         # Pretend that Bits and Bytes is installed and mock bnb.optim.Adam8bit exists.\n         # Trainer.get_optimizer_cls_and_kwargs does not use Adam8bit. It only has to return the\n@@ -5984,6 +5982,7 @@ def test_bnb_adam8bit(self):\n                     default_adam_kwargs,\n                 )\n \n+    @require_bitsandbytes\n     def test_bnb_paged_adam8bit_alias(self):\n         mock = Mock()\n         modules = {\n@@ -5999,6 +5998,7 @@ def test_bnb_paged_adam8bit_alias(self):\n                     default_adam_kwargs,\n                 )\n \n+    @require_bitsandbytes\n     def test_bnb_paged_adam(self):\n         mock = Mock()\n         modules = {\n@@ -6014,6 +6014,7 @@ def test_bnb_paged_adam(self):\n                     default_adam_kwargs,\n                 )\n \n+    @require_bitsandbytes\n     def test_bnb_paged_adam8bit(self):\n         mock = Mock()\n         modules = {\n@@ -6029,6 +6030,7 @@ def test_bnb_paged_adam8bit(self):\n                     default_adam_kwargs,\n                 )\n \n+    @require_bitsandbytes\n     def test_bnb_ademamix(self):\n         mock = Mock()\n         modules = {\n@@ -6044,6 +6046,7 @@ def test_bnb_ademamix(self):\n                     default_ademamix_kwargs,\n                 )\n \n+    @require_bitsandbytes\n     def test_bnb_ademamix8bit(self):\n         mock = Mock()\n         modules = {\n@@ -6059,6 +6062,7 @@ def test_bnb_ademamix8bit(self):\n                     default_ademamix_kwargs,\n                 )\n \n+    @require_bitsandbytes\n     def test_bnb_paged_ademamix(self):\n         mock = Mock()\n         modules = {\n@@ -6074,6 +6078,7 @@ def test_bnb_paged_ademamix(self):\n                     default_ademamix_kwargs,\n                 )\n \n+    @require_bitsandbytes\n     def test_bnb_paged_ademamix8bit(self):\n         mock = Mock()\n         modules = {\n@@ -6089,6 +6094,7 @@ def test_bnb_paged_ademamix8bit(self):\n                     default_ademamix_kwargs,\n                 )\n \n+    @require_bitsandbytes\n     def test_bnb_lion(self):\n         mock = Mock()\n         modules = {\n@@ -6104,6 +6110,7 @@ def test_bnb_lion(self):\n                     default_lion_kwargs,\n                 )\n \n+    @require_bitsandbytes\n     def test_bnb_lion8bit(self):\n         mock = Mock()\n         modules = {\n@@ -6119,6 +6126,7 @@ def test_bnb_lion8bit(self):\n                     default_lion_kwargs,\n                 )\n \n+    @require_bitsandbytes\n     def test_bnb_paged_lion8bit(self):\n         mock = Mock()\n         modules = {\n@@ -6134,6 +6142,7 @@ def test_bnb_paged_lion8bit(self):\n                     default_lion_kwargs,\n                 )\n \n+    @require_bitsandbytes\n     def test_bnb_paged_lion(self):\n         mock = Mock()\n         modules = {\n@@ -6156,7 +6165,7 @@ def test_bnb_adam8bit_no_bnb(self):\n             # Pretend that bnb does not exist, even if installed. By setting bnb to None, importing\n             # bnb will fail even if `bitsandbytes` is installed.\n             with patch.dict(\"sys.modules\", {\"bitsandbytes.optim\": None}):\n-                with self.assertRaises(ValueError):\n+                with self.assertRaises(ImportError):\n                     Trainer.get_optimizer_cls_and_kwargs(args)\n \n     def test_bnb_paged_adam_no_bnb(self):\n@@ -6166,7 +6175,7 @@ def test_bnb_paged_adam_no_bnb(self):\n             # Pretend that bnb does not exist, even if installed. By setting bnb to None, importing\n             # bnb will fail even if `bitsandbytes` is installed.\n             with patch.dict(\"sys.modules\", {\"bitsandbytes.optim\": None}):\n-                with self.assertRaises(ValueError):\n+                with self.assertRaises(ImportError):\n                     Trainer.get_optimizer_cls_and_kwargs(args)\n \n     def test_bnb_paged_adam8bit_no_bnb(self):\n@@ -6176,7 +6185,7 @@ def test_bnb_paged_adam8bit_no_bnb(self):\n             # Pretend that bnb does not exist, even if installed. By setting bnb to None, importing\n             # bnb will fail even if `bitsandbytes` is installed.\n             with patch.dict(\"sys.modules\", {\"bitsandbytes.optim\": None}):\n-                with self.assertRaises(ValueError):\n+                with self.assertRaises(ImportError):\n                     Trainer.get_optimizer_cls_and_kwargs(args)\n \n     def test_bnb_ademamix_no_bnb(self):\n@@ -6186,7 +6195,7 @@ def test_bnb_ademamix_no_bnb(self):\n             # Pretend that bnb does not exist, even if installed. By setting bnb to None, importing\n             # bnb will fail even if `bitsandbytes` is installed.\n             with patch.dict(\"sys.modules\", {\"bitsandbytes.optim\": None}):\n-                with self.assertRaises(ValueError):\n+                with self.assertRaises(ImportError):\n                     Trainer.get_optimizer_cls_and_kwargs(args)\n \n     def test_bnb_ademamix8bit_no_bnb(self):\n@@ -6196,7 +6205,7 @@ def test_bnb_ademamix8bit_no_bnb(self):\n             # Pretend that bnb does not exist, even if installed. By setting bnb to None, importing\n             # bnb will fail even if `bitsandbytes` is installed.\n             with patch.dict(\"sys.modules\", {\"bitsandbytes.optim\": None}):\n-                with self.assertRaises(ValueError):\n+                with self.assertRaises(ImportError):\n                     Trainer.get_optimizer_cls_and_kwargs(args)\n \n     def test_bnb_paged_ademamix_no_bnb(self):\n@@ -6206,7 +6215,7 @@ def test_bnb_paged_ademamix_no_bnb(self):\n             # Pretend that bnb does not exist, even if installed. By setting bnb to None, importing\n             # bnb will fail even if `bitsandbytes` is installed.\n             with patch.dict(\"sys.modules\", {\"bitsandbytes.optim\": None}):\n-                with self.assertRaises(ValueError):\n+                with self.assertRaises(ImportError):\n                     Trainer.get_optimizer_cls_and_kwargs(args)\n \n     def test_bnb_paged_ademamix8bit_no_bnb(self):\n@@ -6216,7 +6225,7 @@ def test_bnb_paged_ademamix8bit_no_bnb(self):\n             # Pretend that bnb does not exist, even if installed. By setting bnb to None, importing\n             # bnb will fail even if `bitsandbytes` is installed.\n             with patch.dict(\"sys.modules\", {\"bitsandbytes.optim\": None}):\n-                with self.assertRaises(ValueError):\n+                with self.assertRaises(ImportError):\n                     Trainer.get_optimizer_cls_and_kwargs(args)\n \n     def test_bnb_paged_lion_no_bnb(self):\n@@ -6226,7 +6235,7 @@ def test_bnb_paged_lion_no_bnb(self):\n             # Pretend that bnb does not exist, even if installed. By setting bnb to None, importing\n             # bnb will fail even if `bitsandbytes` is installed.\n             with patch.dict(\"sys.modules\", {\"bitsandbytes.optim\": None}):\n-                with self.assertRaises(ValueError):\n+                with self.assertRaises(ImportError):\n                     Trainer.get_optimizer_cls_and_kwargs(args)\n \n     def test_bnb_paged_lion8bit_no_bnb(self):\n@@ -6236,7 +6245,7 @@ def test_bnb_paged_lion8bit_no_bnb(self):\n             # Pretend that bnb does not exist, even if installed. By setting bnb to None, importing\n             # bnb will fail even if `bitsandbytes` is installed.\n             with patch.dict(\"sys.modules\", {\"bitsandbytes.optim\": None}):\n-                with self.assertRaises(ValueError):\n+                with self.assertRaises(ImportError):\n                     Trainer.get_optimizer_cls_and_kwargs(args)\n \n     def test_anyprecision_adamw(self):"
        },
        {
            "sha": "5bcf676b481d032e929259afbababa0e3a040a6b",
            "filename": "utils/not_doctested.txt",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/13791d8f482aad9ac31c29866eafc760ccbfa0f3/utils%2Fnot_doctested.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/13791d8f482aad9ac31c29866eafc760ccbfa0f3/utils%2Fnot_doctested.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnot_doctested.txt?ref=13791d8f482aad9ac31c29866eafc760ccbfa0f3",
            "patch": "@@ -785,7 +785,6 @@ src/transformers/trainer_utils.py\n src/transformers/training_args.py\n src/transformers/training_args_seq2seq.py\n src/transformers/utils/backbone_utils.py\n-src/transformers/utils/bitsandbytes.py\n src/transformers/utils/constants.py\n src/transformers/utils/doc.py\n src/transformers/utils/dummy_detectron2_objects.py"
        }
    ],
    "stats": {
        "total": 603,
        "additions": 117,
        "deletions": 486
    }
}