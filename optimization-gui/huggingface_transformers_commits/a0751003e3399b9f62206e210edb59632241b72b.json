{
    "author": "LysandreJik",
    "message": "cheap cli",
    "sha": "a0751003e3399b9f62206e210edb59632241b72b",
    "files": [
        {
            "sha": "8a0a5446c301ffbd6f7f674c41f9ac717f9d1c7b",
            "filename": "src/transformers/cli/add_new_model_like.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a0751003e3399b9f62206e210edb59632241b72b/src%2Ftransformers%2Fcli%2Fadd_new_model_like.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a0751003e3399b9f62206e210edb59632241b72b/src%2Ftransformers%2Fcli%2Fadd_new_model_like.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fadd_new_model_like.py?ref=a0751003e3399b9f62206e210edb59632241b72b",
            "patch": "@@ -23,12 +23,6 @@\n \n import typer\n \n-from ..models.auto.configuration_auto import CONFIG_MAPPING_NAMES, MODEL_NAMES_MAPPING\n-from ..models.auto.feature_extraction_auto import FEATURE_EXTRACTOR_MAPPING_NAMES\n-from ..models.auto.image_processing_auto import IMAGE_PROCESSOR_MAPPING_NAMES\n-from ..models.auto.processing_auto import PROCESSOR_MAPPING_NAMES\n-from ..models.auto.tokenization_auto import TOKENIZER_MAPPING_NAMES\n-from ..models.auto.video_processing_auto import VIDEO_PROCESSOR_MAPPING_NAMES\n from ..utils import is_libcst_available\n from .add_fast_image_processor import add_fast_image_processor\n \n@@ -128,6 +122,13 @@ class ModelInfos:\n     \"\"\"\n \n     def __init__(self, lowercase_name: str):\n+        from ..models.auto.configuration_auto import CONFIG_MAPPING_NAMES, MODEL_NAMES_MAPPING\n+        from ..models.auto.feature_extraction_auto import FEATURE_EXTRACTOR_MAPPING_NAMES\n+        from ..models.auto.image_processing_auto import IMAGE_PROCESSOR_MAPPING_NAMES\n+        from ..models.auto.processing_auto import PROCESSOR_MAPPING_NAMES\n+        from ..models.auto.tokenization_auto import TOKENIZER_MAPPING_NAMES\n+        from ..models.auto.video_processing_auto import VIDEO_PROCESSOR_MAPPING_NAMES\n+\n         # Just to make sure it's indeed lowercase\n         self.lowercase_name = lowercase_name.lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n         if self.lowercase_name not in CONFIG_MAPPING_NAMES:\n@@ -676,6 +677,8 @@ def get_user_input():\n     \"\"\"\n     Ask the user for the necessary inputs to add the new model.\n     \"\"\"\n+    from transformers.models.auto.configuration_auto import MODEL_NAMES_MAPPING\n+\n     model_types = list(MODEL_NAMES_MAPPING.keys())\n \n     # Get old model type"
        },
        {
            "sha": "db292ddc98cae6b453dec261bd7099645254b8c9",
            "filename": "src/transformers/cli/serve.py",
            "status": "modified",
            "additions": 44,
            "deletions": 28,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/a0751003e3399b9f62206e210edb59632241b72b/src%2Ftransformers%2Fcli%2Fserve.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a0751003e3399b9f62206e210edb59632241b72b/src%2Ftransformers%2Fcli%2Fserve.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fserve.py?ref=a0751003e3399b9f62206e210edb59632241b72b",
            "patch": "@@ -14,9 +14,7 @@\n import asyncio\n import base64\n import copy\n-import datetime\n import enum\n-import functools\n import gc\n import io\n import json\n@@ -29,7 +27,7 @@\n from contextlib import asynccontextmanager\n from io import BytesIO\n from threading import Thread\n-from typing import Annotated, Optional, TypedDict, Union\n+from typing import TYPE_CHECKING, Annotated, Optional, TypedDict, Union\n \n import typer\n from huggingface_hub import model_info\n@@ -38,10 +36,7 @@\n from tokenizers.decoders import DecodeStream\n \n import transformers\n-from transformers.models.auto.modeling_auto import (\n-    MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,\n-    MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES,\n-)\n+from transformers import BitsAndBytesConfig, GenerationConfig\n from transformers.utils.import_utils import (\n     is_fastapi_available,\n     is_librosa_available,\n@@ -52,26 +47,20 @@\n )\n \n from .. import (\n-    AutoConfig,\n     LogitsProcessorList,\n-    PreTrainedTokenizerFast,\n-    ProcessorMixin,\n     TextIteratorStreamer,\n )\n-from ..utils import is_torch_available, logging\n-\n+from ..generation.continuous_batching import ContinuousBatchingManager, RequestStatus\n+from ..utils import logging\n \n-if is_torch_available():\n-    import torch\n \n+if TYPE_CHECKING:\n     from transformers import (\n-        AutoProcessor,\n-        BitsAndBytesConfig,\n-        GenerationConfig,\n         PreTrainedModel,\n+        PreTrainedTokenizerFast,\n+        ProcessorMixin,\n     )\n \n-    from ..generation.continuous_batching import ContinuousBatchingManager, RequestStatus\n \n if is_librosa_available():\n     import librosa\n@@ -215,6 +204,25 @@ class TransformersTranscriptionCreateParams(TranscriptionCreateParamsBase, total\n X_REQUEST_ID = \"x-request-id\"\n \n \n+def set_torch_seed(_seed):\n+    import torch\n+\n+    torch.manual_seed(_seed)\n+\n+\n+def reset_torch_cache():\n+    import torch\n+\n+    if torch.cuda.is_available():\n+        torch.cuda.empty_cache()\n+\n+\n+def torch_ones_like(_input_tensor):\n+    import torch\n+\n+    return torch.ones_like(_input_tensor)\n+\n+\n class Modality(enum.Enum):\n     LLM = \"LLM\"\n     VLM = \"VLM\"\n@@ -276,7 +284,7 @@ def create_generation_config_from_req(\n     if req.get(\"top_p\") is not None:\n         generation_config.top_p = float(req[\"top_p\"])\n     if req.get(\"seed\") is not None:\n-        torch.manual_seed(req[\"seed\"])\n+        set_torch_seed(req[\"seed\"])\n \n     return generation_config\n \n@@ -330,8 +338,7 @@ def delete_model(self):\n             gc.collect()\n \n             # Clear CUDA cache if available\n-            if torch.cuda.is_available():\n-                torch.cuda.empty_cache()\n+            reset_torch_cache()\n \n             # XXX: in case we manually delete the model, like on server shutdown\n             self._timer.cancel()\n@@ -433,7 +440,7 @@ def __init__(\n \n         # Seed\n         if default_seed is not None:\n-            torch.manual_seed(default_seed)\n+            set_torch_seed(default_seed)\n \n         # Set up logging\n         transformers_logger = logging.get_logger(\"transformers\")\n@@ -901,6 +908,11 @@ def cancellation_wrapper_buffer(_request_id):\n \n     @staticmethod\n     def get_model_modality(model: \"PreTrainedModel\") -> Modality:\n+        from transformers.models.auto.modeling_auto import (\n+            MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,\n+            MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES,\n+        )\n+\n         model_classname = model.__class__.__name__\n         if model_classname in MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES.values():\n             modality = Modality.VLM\n@@ -1241,7 +1253,7 @@ def generate_response(self, req: dict) -> Generator[str, None, None]:\n         inputs = inputs.to(model.device)\n         request_id = req.get(\"previous_response_id\", \"req_0\")\n \n-        # Temporary hack for GPTOSS 1: don't filter special tokens\n+        # Temporary hack for GPT-OSS 1: don't filter special tokens\n         skip_special_tokens = True\n         if \"gptoss\" in model.config.architectures[0].lower():\n             skip_special_tokens = False\n@@ -1261,15 +1273,15 @@ def generate_response(self, req: dict) -> Generator[str, None, None]:\n \n         generation_kwargs = {\n             \"inputs\": inputs,\n-            \"attention_mask\": torch.ones_like(inputs),\n+            \"attention_mask\": torch_ones_like(inputs),\n             \"streamer\": generation_streamer,\n             \"generation_config\": generation_config,\n             \"return_dict_in_generate\": True,\n             \"past_key_values\": last_kv_cache,\n         }\n \n         def stream_response(streamer, _request_id):\n-            # Temporary hack for GPTOS 2: filter out the CoT tokens. Full solution here implies defining new output\n+            # Temporary hack for GPT-OSS 2: filter out the CoT tokens. Full solution here implies defining new output\n             # classes and piping the reasoning trace into a new field\n             filter_cot = False\n             cot_trace_end = None\n@@ -1560,7 +1572,7 @@ def generate_response_non_streaming(self, req: dict) -> dict:\n \n         generate_output = model.generate(\n             inputs=inputs,\n-            attention_mask=torch.ones_like(inputs),\n+            attention_mask=torch_ones_like(inputs),\n             generation_config=generation_config,\n             return_dict_in_generate=True,\n             past_key_values=last_kv_cache,\n@@ -1729,6 +1741,10 @@ def _load_model_and_data_processor(self, model_id_and_revision: str):\n             `tuple[PreTrainedModel, Union[ProcessorMixin, PreTrainedTokenizerFast]]`: The loaded model and\n             data processor (tokenizer, audio processor, etc.).\n         \"\"\"\n+        import torch\n+\n+        from transformers import AutoConfig, AutoProcessor\n+\n         logger.info(f\"Loading {model_id_and_revision}\")\n \n         if \"@\" in model_id_and_revision:\n@@ -1771,7 +1787,7 @@ def _load_model_and_data_processor(self, model_id_and_revision: str):\n \n     def load_model_and_processor(\n         self, model_id_and_revision: str\n-    ) -> tuple[\"PreTrainedModel\", PreTrainedTokenizerFast]:\n+    ) -> tuple[\"PreTrainedModel\", \"PreTrainedTokenizerFast\"]:\n         \"\"\"\n         Loads the text model and processor from the given model ID and revision into the ServeCommand instance.\n \n@@ -1796,7 +1812,7 @@ def load_model_and_processor(\n \n         return model, processor\n \n-    def load_audio_model_and_processor(self, model_id_and_revision: str) -> tuple[\"PreTrainedModel\", ProcessorMixin]:\n+    def load_audio_model_and_processor(self, model_id_and_revision: str) -> tuple[\"PreTrainedModel\", \"ProcessorMixin\"]:\n         \"\"\"\n         Loads the audio model and processor from the given model ID and revision into the ServeCommand instance.\n "
        },
        {
            "sha": "011daa4d536fec7bc1a4e83e242ec8558667571f",
            "filename": "src/transformers/cli/transformers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a0751003e3399b9f62206e210edb59632241b72b/src%2Ftransformers%2Fcli%2Ftransformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a0751003e3399b9f62206e210edb59632241b72b/src%2Ftransformers%2Fcli%2Ftransformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Ftransformers.py?ref=a0751003e3399b9f62206e210edb59632241b72b",
            "patch": "@@ -19,7 +19,6 @@\n from transformers.cli.add_new_model_like import add_new_model_like\n from transformers.cli.chat import Chat, ChatCommand\n from transformers.cli.download import download\n-from transformers.cli.run import run\n from transformers.cli.serve import Serve\n from transformers.cli.system import env, version\n \n@@ -31,7 +30,6 @@\n app.command(name=\"chat\", cls=ChatCommand)(Chat)\n app.command()(download)\n app.command()(env)\n-app.command()(run)\n app.command(name=\"serve\")(Serve)\n app.command()(version)\n "
        }
    ],
    "stats": {
        "total": 89,
        "additions": 53,
        "deletions": 36
    }
}