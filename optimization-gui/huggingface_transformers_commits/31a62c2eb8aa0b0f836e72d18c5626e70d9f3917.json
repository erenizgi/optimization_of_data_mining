{
    "author": "Logeswaran7",
    "message": "Updated Model-card for donut (#37290)\n\n* Updated documentation for Donut model\n\n* Update docs/source/en/model_doc/donut.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/donut.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/donut.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/donut.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Updated code suggestions\n\n* Update docs/source/en/model_doc/donut.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Updated code suggestion to Align with the AutoModel example\n\n* Update docs/source/en/model_doc/donut.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Updated notes section included code examples\n\n* close hfoption block and indent\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "31a62c2eb8aa0b0f836e72d18c5626e70d9f3917",
    "files": [
        {
            "sha": "1bc1a3bcfd0b871e90f96f1e5aba664dbe0af6cd",
            "filename": "docs/source/en/model_doc/donut.md",
            "status": "modified",
            "additions": 166,
            "deletions": 155,
            "changes": 321,
            "blob_url": "https://github.com/huggingface/transformers/blob/31a62c2eb8aa0b0f836e72d18c5626e70d9f3917/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/31a62c2eb8aa0b0f836e72d18c5626e70d9f3917/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md?ref=31a62c2eb8aa0b0f836e72d18c5626e70d9f3917",
            "patch": "@@ -13,180 +13,191 @@ rendered properly in your Markdown viewer.\n \n specific language governing permissions and limitations under the License. -->\n \n-# Donut\n-\n-## Overview\n-\n-The Donut model was proposed in [OCR-free Document Understanding Transformer](https://arxiv.org/abs/2111.15664) by\n-Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park.\n-Donut consists of an image Transformer encoder and an autoregressive text Transformer decoder to perform document understanding\n-tasks such as document image classification, form understanding and visual question answering.\n-\n-The abstract from the paper is the following:\n-\n-*Understanding document images (e.g., invoices) is a core but challenging task since it requires complex functions such as reading text and a holistic understanding of the document. Current Visual Document Understanding (VDU) methods outsource the task of reading text to off-the-shelf Optical Character Recognition (OCR) engines and focus on the understanding task with the OCR outputs. Although such OCR-based approaches have shown promising performance, they suffer from 1) high computational costs for using OCR; 2) inflexibility of OCR models on languages or types of document; 3) OCR error propagation to the subsequent process. To address these issues, in this paper, we introduce a novel OCR-free VDU model named Donut, which stands for Document understanding transformer. As the first step in OCR-free VDU research, we propose a simple architecture (i.e., Transformer) with a pre-training objective (i.e., cross-entropy loss). Donut is conceptually simple yet effective. Through extensive experiments and analyses, we show a simple OCR-free VDU model, Donut, achieves state-of-the-art performances on various VDU tasks in terms of both speed and accuracy. In addition, we offer a synthetic data generator that helps the model pre-training to be flexible in various languages and domains.*\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/donut_architecture.jpg\"\n-alt=\"drawing\" width=\"600\"/>\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n \n-<small> Donut high-level overview. Taken from the <a href=\"https://arxiv.org/abs/2111.15664\">original paper</a>. </small>\n-\n-This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found\n-[here](https://github.com/clovaai/donut).\n+# Donut\n \n-## Usage tips\n+[Donut (Document Understanding Transformer)](https://huggingface.co/papers2111.15664) is a visual document understanding model that doesn't require an Optical Character Recognition (OCR) engine. Unlike traditional approaches that extract text using OCR before processing, Donut employs an end-to-end Transformer-based architecture to directly analyze document images. This eliminates OCR-related inefficiencies making it more accurate and adaptable to diverse languages and formats. \n \n-- The quickest way to get started with Donut is by checking the [tutorial\n-  notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Donut), which show how to use the model\n-  at inference time as well as fine-tuning on custom data.\n-- Donut is always used within the [VisionEncoderDecoder](vision-encoder-decoder) framework.\n+Donut features vision encoder ([Swin](./swin)) and a text decoder ([BART](./bart)). Swin converts document images into embeddings and BART processes them into meaningful text sequences.\n \n-## Inference examples\n+You can find all the original Donut checkpoints under the [Naver Clova Information Extraction](https://huggingface.co/naver-clova-ix) organization.\n \n-Donut's [`VisionEncoderDecoder`] model accepts images as input and makes use of\n-[`~generation.GenerationMixin.generate`] to autoregressively generate text given the input image.\n+> [!TIP]\n+> Click on the Donut models in the right sidebar for more examples of how to apply Donut to different language and vision tasks.\n \n-The [`DonutImageProcessor`] class is responsible for preprocessing the input image and\n-[`XLMRobertaTokenizer`/`XLMRobertaTokenizerFast`] decodes the generated target tokens to the target string. The\n-[`DonutProcessor`] wraps [`DonutImageProcessor`] and [`XLMRobertaTokenizer`/`XLMRobertaTokenizerFast`]\n-into a single instance to both extract the input features and decode the predicted token ids.\n+The examples below demonstrate how to perform document understanding tasks using Donut with [`Pipeline`] and [`AutoModel`]\n \n-- Step-by-step Document Image Classification\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n \n ```py\n->>> import re\n-\n->>> from transformers import DonutProcessor, VisionEncoderDecoderModel\n->>> from datasets import load_dataset\n->>> import torch\n-\n->>> processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n->>> model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n-\n->>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n->>> model.to(device)  # doctest: +IGNORE_RESULT\n-\n->>> # load document image\n->>> dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n->>> image = dataset[1][\"image\"]\n-\n->>> # prepare decoder inputs\n->>> task_prompt = \"<s_rvlcdip>\"\n->>> decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n-\n->>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n-\n->>> outputs = model.generate(\n-...     pixel_values.to(device),\n-...     decoder_input_ids=decoder_input_ids.to(device),\n-...     max_length=model.decoder.config.max_position_embeddings,\n-...     pad_token_id=processor.tokenizer.pad_token_id,\n-...     eos_token_id=processor.tokenizer.eos_token_id,\n-...     use_cache=True,\n-...     bad_words_ids=[[processor.tokenizer.unk_token_id]],\n-...     return_dict_in_generate=True,\n-... )\n-\n->>> sequence = processor.batch_decode(outputs.sequences)[0]\n->>> sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n->>> sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n->>> print(processor.token2json(sequence))\n-{'class': 'advertisement'}\n+# pip install datasets\n+import torch\n+from transformers import pipeline\n+from PIL import Image\n+\n+pipeline = pipeline(\n+    task=\"document-question-answering\",\n+    model=\"naver-clova-ix/donut-base-finetuned-docvqa\",\n+    device=0,\n+    torch_dtype=torch.float16\n+)\n+dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n+image = dataset[0][\"image\"]\n+\n+pipeline(image=image, question=\"What time is the coffee break?\")\n ```\n \n-- Step-by-step Document Parsing\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n \n ```py\n->>> import re\n-\n->>> from transformers import DonutProcessor, VisionEncoderDecoderModel\n->>> from datasets import load_dataset\n->>> import torch\n-\n->>> processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n->>> model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n-\n->>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n->>> model.to(device)  # doctest: +IGNORE_RESULT\n-\n->>> # load document image\n->>> dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n->>> image = dataset[2][\"image\"]\n-\n->>> # prepare decoder inputs\n->>> task_prompt = \"<s_cord-v2>\"\n->>> decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n-\n->>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n-\n->>> outputs = model.generate(\n-...     pixel_values.to(device),\n-...     decoder_input_ids=decoder_input_ids.to(device),\n-...     max_length=model.decoder.config.max_position_embeddings,\n-...     pad_token_id=processor.tokenizer.pad_token_id,\n-...     eos_token_id=processor.tokenizer.eos_token_id,\n-...     use_cache=True,\n-...     bad_words_ids=[[processor.tokenizer.unk_token_id]],\n-...     return_dict_in_generate=True,\n-... )\n-\n->>> sequence = processor.batch_decode(outputs.sequences)[0]\n->>> sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n->>> sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n->>> print(processor.token2json(sequence))\n-{'menu': {'nm': 'CINNAMON SUGAR', 'unitprice': '17,000', 'cnt': '1 x', 'price': '17,000'}, 'sub_total': {'subtotal_price': '17,000'}, 'total': {'total_price': '17,000', 'cashprice': '20,000', 'changeprice': '3,000'}}\n+# pip install datasets\n+import torch\n+from datasets import load_dataset\n+from transformers import AutoProcessor, AutoModelForVision2Seq\n+\n+processor = AutoProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n+model = AutoModelForVision2Seq.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n+\n+dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n+image = dataset[0][\"image\"]\n+question = \"What time is the coffee break?\"\n+task_prompt = f\"<s_docvqa><s_question>{question}</s_question><s_answer>\"\n+inputs = processor(image, task_prompt, return_tensors=\"pt\")\n+\n+outputs = model.generate(\n+    input_ids=inputs.input_ids,\n+    pixel_values=inputs.pixel_values,\n+    max_length=512\n+)\n+answer = processor.decode(outputs[0], skip_special_tokens=True)\n+print(answer)\n ```\n \n-- Step-by-step Document Visual Question Answering (DocVQA)\n+</hfoption>\n+</hfoptions>\n \n-```py\n->>> import re\n-\n->>> from transformers import DonutProcessor, VisionEncoderDecoderModel\n->>> from datasets import load_dataset\n->>> import torch\n-\n->>> processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n->>> model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n-\n->>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n->>> model.to(device)  # doctest: +IGNORE_RESULT\n-\n->>> # load document image from the DocVQA dataset\n->>> dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n->>> image = dataset[0][\"image\"]\n-\n->>> # prepare decoder inputs\n->>> task_prompt = \"<s_docvqa><s_question>{user_input}</s_question><s_answer>\"\n->>> question = \"When is the coffee break?\"\n->>> prompt = task_prompt.replace(\"{user_input}\", question)\n->>> decoder_input_ids = processor.tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n-\n->>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n-\n->>> outputs = model.generate(\n-...     pixel_values.to(device),\n-...     decoder_input_ids=decoder_input_ids.to(device),\n-...     max_length=model.decoder.config.max_position_embeddings,\n-...     pad_token_id=processor.tokenizer.pad_token_id,\n-...     eos_token_id=processor.tokenizer.eos_token_id,\n-...     use_cache=True,\n-...     bad_words_ids=[[processor.tokenizer.unk_token_id]],\n-...     return_dict_in_generate=True,\n-... )\n-\n->>> sequence = processor.batch_decode(outputs.sequences)[0]\n->>> sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n->>> sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n->>> print(processor.token2json(sequence))\n-{'question': 'When is the coffee break?', 'answer': '11-14 to 11:39 a.m.'}\n-```\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n \n-See the [model hub](https://huggingface.co/models?filter=donut) to look for Donut checkpoints.\n+The example below uses [torchao](../quantization/torchao) to only quantize the weights to int4.\n \n-## Training\n+```py\n+# pip install datasets torchao\n+import torch\n+from datasets import load_dataset\n+from transformers import TorchAoConfig, AutoProcessor, AutoModelForVision2Seq\n+\n+quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n+processor = AutoProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n+model = AutoModelForVision2Seq.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\", quantization_config=quantization_config)\n+\n+dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n+image = dataset[0][\"image\"]\n+question = \"What time is the coffee break?\"\n+task_prompt = f\"<s_docvqa><s_question>{question}</s_question><s_answer>\"\n+inputs = processor(image, task_prompt, return_tensors=\"pt\")\n+\n+outputs = model.generate(\n+    input_ids=inputs.input_ids,\n+    pixel_values=inputs.pixel_values,\n+    max_length=512\n+)\n+answer = processor.decode(outputs[0], skip_special_tokens=True)\n+print(answer)\n+```\n \n-We refer to the [tutorial notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Donut).\n+## Notes\n+\n+- Use Donut for document image classification as shown below.\n+\n+    ```py\n+    >>> import re\n+    >>> from transformers import DonutProcessor, VisionEncoderDecoderModel\n+    >>> from datasets import load_dataset\n+    >>> import torch\n+\n+    >>> processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n+    >>> model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n+\n+    >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+    >>> model.to(device)  # doctest: +IGNORE_RESULT\n+\n+    >>> # load document image\n+    >>> dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n+    >>> image = dataset[1][\"image\"]\n+\n+    >>> # prepare decoder inputs\n+    >>> task_prompt = \"<s_rvlcdip>\"\n+    >>> decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n+\n+    >>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n+\n+    >>> outputs = model.generate(\n+    ...     pixel_values.to(device),\n+    ...     decoder_input_ids=decoder_input_ids.to(device),\n+    ...     max_length=model.decoder.config.max_position_embeddings,\n+    ...     pad_token_id=processor.tokenizer.pad_token_id,\n+    ...     eos_token_id=processor.tokenizer.eos_token_id,\n+    ...     use_cache=True,\n+    ...     bad_words_ids=[[processor.tokenizer.unk_token_id]],\n+    ...     return_dict_in_generate=True,\n+    ... )\n+\n+    >>> sequence = processor.batch_decode(outputs.sequences)[0]\n+    >>> sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n+    >>> sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n+    >>> print(processor.token2json(sequence))\n+    {'class': 'advertisement'}\n+    ```\n+\n+- Use Donut for document parsing as shown below.\n+\n+    ```py\n+    >>> import re\n+    >>> from transformers import DonutProcessor, VisionEncoderDecoderModel\n+    >>> from datasets import load_dataset\n+    >>> import torch\n+\n+    >>> processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n+    >>> model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n+\n+    >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+    >>> model.to(device)  # doctest: +IGNORE_RESULT\n+\n+    >>> # load document image\n+    >>> dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n+    >>> image = dataset[2][\"image\"]\n+\n+    >>> # prepare decoder inputs\n+    >>> task_prompt = \"<s_cord-v2>\"\n+    >>> decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n+\n+    >>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n+\n+    >>> outputs = model.generate(\n+    ...     pixel_values.to(device),\n+    ...     decoder_input_ids=decoder_input_ids.to(device),\n+    ...     max_length=model.decoder.config.max_position_embeddings,\n+    ...     pad_token_id=processor.tokenizer.pad_token_id,\n+    ...     eos_token_id=processor.tokenizer.eos_token_id,\n+    ...     use_cache=True,\n+    ...     bad_words_ids=[[processor.tokenizer.unk_token_id]],\n+    ...     return_dict_in_generate=True,\n+    ... )\n+\n+    >>> sequence = processor.batch_decode(outputs.sequences)[0]\n+    >>> sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n+    >>> sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n+    >>> print(processor.token2json(sequence))\n+    {'menu': {'nm': 'CINNAMON SUGAR', 'unitprice': '17,000', 'cnt': '1 x', 'price': '17,000'}, 'sub_total': {'subtotal_price': '17,000'}, 'total': \n+    {'total_price': '17,000', 'cashprice': '20,000', 'changeprice': '3,000'}}\n+    ```\n \n ## DonutSwinConfig\n "
        }
    ],
    "stats": {
        "total": 321,
        "additions": 166,
        "deletions": 155
    }
}