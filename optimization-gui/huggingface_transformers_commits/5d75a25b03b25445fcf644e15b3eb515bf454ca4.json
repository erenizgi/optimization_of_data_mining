{
    "author": "zucchini-nlp",
    "message": "Qwen2-VL: fix rope delta calculation (#36013)\n\n* fix rope delats calculation\r\n\r\n* add test\r\n\r\n* style",
    "sha": "5d75a25b03b25445fcf644e15b3eb515bf454ca4",
    "files": [
        {
            "sha": "78186b062b44e2e2f7fbea2ebee0d3bf02409112",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d75a25b03b25445fcf644e15b3eb515bf454ca4/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d75a25b03b25445fcf644e15b3eb515bf454ca4/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=5d75a25b03b25445fcf644e15b3eb515bf454ca4",
            "patch": "@@ -1776,7 +1776,11 @@ def forward(\n         # if we get 4D attention mask we cannot calculate rope deltas anymore. TODO @raushan fixme\n         if position_ids is None and (attention_mask is None or attention_mask.ndim == 2):\n             # calculate RoPE index once per generation in the pre-fill stage only\n-            if (cache_position is not None and cache_position[0] == 0) or self.rope_deltas is None:\n+            if (\n+                (cache_position is not None and cache_position[0] == 0)\n+                or self.rope_deltas is None\n+                or (past_key_values is None or past_key_values.get_seq_length() == 0)\n+            ):\n                 position_ids, rope_deltas = self.get_rope_index(\n                     input_ids,\n                     image_grid_thw,"
        },
        {
            "sha": "15abcb53deebbb884f606768afe8f8fd3facf5d6",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d75a25b03b25445fcf644e15b3eb515bf454ca4/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d75a25b03b25445fcf644e15b3eb515bf454ca4/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=5d75a25b03b25445fcf644e15b3eb515bf454ca4",
            "patch": "@@ -675,7 +675,11 @@ def forward(\n         # if we get 4D attention mask we cannot calculate rope deltas anymore. TODO @raushan fixme\n         if position_ids is None and (attention_mask is None or attention_mask.ndim == 2):\n             # calculate RoPE index once per generation in the pre-fill stage only\n-            if (cache_position is not None and cache_position[0] == 0) or self.rope_deltas is None:\n+            if (\n+                (cache_position is not None and cache_position[0] == 0)\n+                or self.rope_deltas is None\n+                or (past_key_values is None or past_key_values.get_seq_length() == 0)\n+            ):\n                 position_ids, rope_deltas = self.get_rope_index(\n                     input_ids,\n                     image_grid_thw,"
        },
        {
            "sha": "512cc602c5d803e4fd37a8459adbf5072078bc9e",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d75a25b03b25445fcf644e15b3eb515bf454ca4/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d75a25b03b25445fcf644e15b3eb515bf454ca4/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=5d75a25b03b25445fcf644e15b3eb515bf454ca4",
            "patch": "@@ -1648,7 +1648,11 @@ def forward(\n         # if we get 4D attention mask we cannot calculate rope deltas anymore. TODO @raushan fixme\n         if position_ids is None and (attention_mask is None or attention_mask.ndim == 2):\n             # calculate RoPE index once per generation in the pre-fill stage only\n-            if (cache_position is not None and cache_position[0] == 0) or self.rope_deltas is None:\n+            if (\n+                (cache_position is not None and cache_position[0] == 0)\n+                or self.rope_deltas is None\n+                or (past_key_values is None or past_key_values.get_seq_length() == 0)\n+            ):\n                 position_ids, rope_deltas = self.get_rope_index(\n                     input_ids, image_grid_thw, video_grid_thw, attention_mask\n                 )"
        },
        {
            "sha": "6ef958e2aaed4ebea652510eb0e5d7112e203f21",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 23,
            "deletions": 0,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d75a25b03b25445fcf644e15b3eb515bf454ca4/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d75a25b03b25445fcf644e15b3eb515bf454ca4/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=5d75a25b03b25445fcf644e15b3eb515bf454ca4",
            "patch": "@@ -284,6 +284,29 @@ def test_mismatching_num_image_tokens(self):\n             image_grid_thw = torch.cat([image_grid_thw, image_grid_thw], dim=0)\n             _ = model(input_ids=input_ids, pixel_values=pixel_values, image_grid_thw=image_grid_thw)\n \n+    def test_forward_with_rope_deltas_cached(self):\n+        \"\"\"\n+        Tests that Qwen2-VL computes new rope deltas every forward pass with new set of inputs.\n+        Rope deltas are cached when we generate and re-used for decoding phase, byt are not reset\n+        automatically after generation ends. See https://github.com/huggingface/transformers/pull/36013 for more\n+        \"\"\"\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_generative_model_classes:\n+            model = model_class(config).to(torch_device)\n+\n+            # Generate and make sure rope_deltas are not `None`\n+            self.assertTrue(model.rope_deltas is None)\n+            generation_output = model.generate(\n+                **input_dict, max_new_tokens=4, return_dict_in_generate=True, output_logits=True\n+            )\n+            self.assertTrue(model.rope_deltas is not None)\n+\n+            # Now if we try to do forward pass, we should get new rope logits, because cache is not passed\n+            forward_output = model(**input_dict)\n+            torch.testing.assert_close(\n+                generation_output.logits[0], forward_output.logits[:, -1, :], rtol=1e-4, atol=1e-4\n+            )\n+\n     @unittest.skip(reason=\"Feedforward chunking is not yet supported\")\n     def test_feed_forward_chunking(self):\n         pass"
        }
    ],
    "stats": {
        "total": 41,
        "additions": 38,
        "deletions": 3
    }
}