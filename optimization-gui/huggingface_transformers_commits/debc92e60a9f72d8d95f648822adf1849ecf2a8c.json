{
    "author": "zucchini-nlp",
    "message": "Skip broken tests (#40157)\n\nskip these tests",
    "sha": "debc92e60a9f72d8d95f648822adf1849ecf2a8c",
    "files": [
        {
            "sha": "26c07e651ab7deb6f66c36007d796fab5e8cfa49",
            "filename": "tests/models/gemma3n/test_modeling_gemma3n.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/debc92e60a9f72d8d95f648822adf1849ecf2a8c/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/debc92e60a9f72d8d95f648822adf1849ecf2a8c/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py?ref=debc92e60a9f72d8d95f648822adf1849ecf2a8c",
            "patch": "@@ -436,6 +436,14 @@ def test_dola_decoding_sample(self):\n     def test_generate_with_quant_cache(self):\n         pass\n \n+    @unittest.skip(\"Gemma3n applies key/query norm which doesn't work with packing\")\n+    def test_eager_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma3n applies key/query norm which doesn't work with packing\")\n+    def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n \n class Gemma3nVision2TextModelTester:\n     text_config = {\"activation_sparsity_pattern\": None}\n@@ -646,6 +654,14 @@ def test_initialization(self):\n     def test_flex_attention_with_grads(self):\n         pass\n \n+    @unittest.skip(\"Gemma3n applies key/query norm which doesn't work with packing\")\n+    def test_eager_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma3n applies key/query norm which doesn't work with packing\")\n+    def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n     def test_automodelforcausallm(self):\n         \"\"\"\n         Regression test for #36741 -- make sure `AutoModelForCausalLM` works with a Gemma3n config, i.e. that"
        },
        {
            "sha": "daec885c12ea7848b2904656c6366b68e446d888",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/debc92e60a9f72d8d95f648822adf1849ecf2a8c/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/debc92e60a9f72d8d95f648822adf1849ecf2a8c/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=debc92e60a9f72d8d95f648822adf1849ecf2a8c",
            "patch": "@@ -256,6 +256,14 @@ def create_and_check_mllama_model_fp16_forward(self, config, input_ids, pixel_va\n             )[\"logits\"]\n         self.parent.assertFalse(torch.isnan(logits).any().item())\n \n+    @unittest.skip(\"Mllama applies key/query norm which doesn't work with packing\")\n+    def test_eager_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\"Mllama applies key/query norm which doesn't work with packing\")\n+    def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n \n @require_torch\n class MllamaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n@@ -386,6 +394,14 @@ def test_disk_offload_bin(self):\n     def test_disk_offload_safetensors(self):\n         pass\n \n+    @unittest.skip(\"Mllama applies key/query norm which doesn't work with packing\")\n+    def test_eager_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\"Mllama applies key/query norm which doesn't work with packing\")\n+    def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n     @pytest.mark.generate\n     # overridden because mllama is not an encoder-decoder model, but has encoder-decoder-like cache\n     def test_past_key_values_format(self):"
        }
    ],
    "stats": {
        "total": 32,
        "additions": 32,
        "deletions": 0
    }
}