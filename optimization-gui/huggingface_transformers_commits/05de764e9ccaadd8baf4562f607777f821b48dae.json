{
    "author": "ydshieh",
    "message": "Aurevoir PyTorch 1 (#35358)\n\n* fix\r\n\r\n* fix\r\n\r\n* fix\r\n\r\n---------\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "05de764e9ccaadd8baf4562f607777f821b48dae",
    "files": [
        {
            "sha": "46d811d4a43394a86f256669b69d19cf5d109fab",
            "filename": ".github/workflows/self-nightly-past-ci-caller.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 33,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/.github%2Fworkflows%2Fself-nightly-past-ci-caller.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/.github%2Fworkflows%2Fself-nightly-past-ci-caller.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-nightly-past-ci-caller.yml?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -21,39 +21,6 @@ jobs:\n           echo \"$(python3 -c 'print(int(${{ github.run_number }}) % 10)')\"\r\n           echo \"run_number=$(python3 -c 'print(int(${{ github.run_number }}) % 10)')\" >> $GITHUB_OUTPUT\r\n \r\n-  run_past_ci_pytorch_1-13:\r\n-    name: PyTorch 1.13\r\n-    needs: get_number\r\n-    if: needs.get_number.outputs.run_number == 0 && (cancelled() != true) && ((github.event_name == 'schedule') || ((github.event_name == 'push') && startsWith(github.ref_name, 'run_past_ci')))\r\n-    uses: ./.github/workflows/self-past-caller.yml\r\n-    with:\r\n-      framework: pytorch\r\n-      version: \"1.13\"\r\n-      sha: ${{ github.sha }}\r\n-    secrets: inherit\r\n-\r\n-  run_past_ci_pytorch_1-12:\r\n-    name: PyTorch 1.12\r\n-    needs: get_number\r\n-    if: needs.get_number.outputs.run_number == 1 && (cancelled() != true) && ((github.event_name == 'schedule') || ((github.event_name == 'push') && startsWith(github.ref_name, 'run_past_ci')))\r\n-    uses: ./.github/workflows/self-past-caller.yml\r\n-    with:\r\n-      framework: pytorch\r\n-      version: \"1.12\"\r\n-      sha: ${{ github.sha }}\r\n-    secrets: inherit\r\n-\r\n-  run_past_ci_pytorch_1-11:\r\n-    name: PyTorch 1.11\r\n-    needs: get_number\r\n-    if: needs.get_number.outputs.run_number == 2 && (cancelled() != true) && ((github.event_name == 'schedule') || ((github.event_name == 'push') && startsWith(github.ref_name, 'run_past_ci')))\r\n-    uses: ./.github/workflows/self-past-caller.yml\r\n-    with:\r\n-      framework: pytorch\r\n-      version: \"1.11\"\r\n-      sha: ${{ github.sha }}\r\n-    secrets: inherit\r\n-\r\n   run_past_ci_tensorflow_2-11:\r\n     name: TensorFlow 2.11\r\n     needs: get_number\r"
        },
        {
            "sha": "42403f84b885daf03378fcc951722259c507b8d6",
            "filename": "README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/README.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/README.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/README.md?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -249,7 +249,7 @@ The model itself is a regular [Pytorch `nn.Module`](https://pytorch.org/docs/sta\n \n ### With pip\n \n-This repository is tested on Python 3.9+, Flax 0.4.1+, PyTorch 1.11+, and TensorFlow 2.6+.\n+This repository is tested on Python 3.9+, Flax 0.4.1+, PyTorch 2.0+, and TensorFlow 2.6+.\n \n You should install ðŸ¤— Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, check out the [user guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).\n "
        },
        {
            "sha": "c7249ac23d2e7f346abb1b31d5d9b1d421f0d5cc",
            "filename": "i18n/README_ar.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_ar.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_ar.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/i18n%2FREADME_ar.md?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -245,7 +245,7 @@ limitations under the License.\n \n ### Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… pip\n \n-ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ø¹Ù„Ù‰ Python 3.9+ØŒ Flax 0.4.1+ØŒ PyTorch 1.11+ØŒ Ùˆ TensorFlow 2.6+.\n+ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ø¹Ù„Ù‰ Python 3.9+ØŒ Flax 0.4.1+ØŒ PyTorch 2.0+ØŒ Ùˆ TensorFlow 2.6+.\n \n ÙŠØ¬Ø¨ ØªØ«Ø¨ÙŠØª ðŸ¤— Transformers ÙÙŠ [Ø¨ÙŠØ¦Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©](https://docs.python.org/3/library/venv.html). Ø¥Ø°Ø§ ÙƒÙ†Øª ØºÙŠØ± Ù…Ø¹ØªØ§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ¦Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© PythonØŒ ÙØ±Ø§Ø¬Ø¹ [Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).\n "
        },
        {
            "sha": "78447af41a7a82913b7055d045d41b4d9abedd95",
            "filename": "i18n/README_de.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_de.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_de.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/i18n%2FREADME_de.md?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -246,7 +246,7 @@ Das Modell selbst ist ein regulÃ¤res [PyTorch `nn.Module`](https://pytorch.org/d\n \n ### Mit pip\n \n-Dieses Repository wurde mit Python 3.9+, Flax 0.4.1+, PyTorch 1.11+ und TensorFlow 2.6+ getestet.\n+Dieses Repository wurde mit Python 3.9+, Flax 0.4.1+, PyTorch 2.0+ und TensorFlow 2.6+ getestet.\n \n Sie sollten ðŸ¤— Transformers in einer [virtuellen Umgebung](https://docs.python.org/3/library/venv.html) installieren. Wenn Sie mit virtuellen Python-Umgebungen nicht vertraut sind, schauen Sie sich den [Benutzerleitfaden](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/) an.\n "
        },
        {
            "sha": "57eb8117fc0d5dd6ac15790a5749b5e094851591",
            "filename": "i18n/README_es.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_es.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_es.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/i18n%2FREADME_es.md?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -222,7 +222,7 @@ El modelo en si es un [Pytorch `nn.Module`](https://pytorch.org/docs/stable/nn.h\n \n ### Con pip\n \n-Este repositorio estÃ¡ probado en Python 3.9+, Flax 0.4.1+, PyTorch 1.11+ y TensorFlow 2.6+.\n+Este repositorio estÃ¡ probado en Python 3.9+, Flax 0.4.1+, PyTorch 2.0+ y TensorFlow 2.6+.\n \n DeberÃ­as instalar ðŸ¤— Transformers en un [entorno virtual](https://docs.python.org/3/library/venv.html). Si no estas familiarizado con los entornos virtuales de Python, consulta la [guÃ­a de usuario](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).\n "
        },
        {
            "sha": "02714d52bff39b43b7dfe5e9319cbac295031a30",
            "filename": "i18n/README_fr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_fr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_fr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/i18n%2FREADME_fr.md?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -243,7 +243,7 @@ Le modÃ¨le lui-mÃªme est un module [`nn.Module` PyTorch](https://pytorch.org/doc\n \n ### Avec pip\n \n-Ce rÃ©fÃ©rentiel est testÃ© sur Python 3.9+, Flax 0.4.1+, PyTorch 1.11+ et TensorFlow 2.6+.\n+Ce rÃ©fÃ©rentiel est testÃ© sur Python 3.9+, Flax 0.4.1+, PyTorch 2.0+ et TensorFlow 2.6+.\n \n Vous devriez installer ðŸ¤— Transformers dans un [environnement virtuel](https://docs.python.org/3/library/venv.html). Si vous n'Ãªtes pas familier avec les environnements virtuels Python, consultez le [guide utilisateur](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).\n "
        },
        {
            "sha": "1541e4df66fcbdedaef91a45b2cfbcdc59a9d4cb",
            "filename": "i18n/README_hd.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_hd.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_hd.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/i18n%2FREADME_hd.md?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -198,7 +198,7 @@ checkpoint: à¤œà¤¾à¤à¤š à¤¬à¤¿à¤‚à¤¦à¥\n \n ### à¤ªà¤¿à¤ª à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¨à¤¾\n \n-à¤‡à¤¸ à¤°à¤¿à¤ªà¥‰à¤œà¤¿à¤Ÿà¤°à¥€ à¤•à¤¾ à¤ªà¤°à¥€à¤•à¥à¤·à¤£ Python 3.9+, Flax 0.4.1+, PyTorch 1.11+ à¤”à¤° TensorFlow 2.6+ à¤•à¥‡ à¤¤à¤¹à¤¤ à¤•à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾ à¤¹à¥ˆà¥¤\n+à¤‡à¤¸ à¤°à¤¿à¤ªà¥‰à¤œà¤¿à¤Ÿà¤°à¥€ à¤•à¤¾ à¤ªà¤°à¥€à¤•à¥à¤·à¤£ Python 3.9+, Flax 0.4.1+, PyTorch 2.0+ à¤”à¤° TensorFlow 2.6+ à¤•à¥‡ à¤¤à¤¹à¤¤ à¤•à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾ à¤¹à¥ˆà¥¤\n \n à¤†à¤ª [à¤µà¤°à¥à¤šà¥à¤…à¤² à¤à¤¨à¤µà¤¾à¤¯à¤°à¤¨à¤®à¥‡à¤‚à¤Ÿ](https://docs.python.org/3/library/venv.html) à¤®à¥‡à¤‚ ðŸ¤— à¤Ÿà¥à¤°à¤¾à¤‚à¤¸à¤«à¥‰à¤°à¥à¤®à¤° à¤‡à¤‚à¤¸à¥à¤Ÿà¥‰à¤² à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤ à¤¯à¤¦à¤¿ à¤†à¤ª à¤…à¤­à¥€ à¤¤à¤• à¤ªà¤¾à¤¯à¤¥à¤¨ à¤•à¥‡ à¤µà¤°à¥à¤šà¥à¤…à¤² à¤à¤¨à¤µà¤¾à¤¯à¤°à¤¨à¤®à¥‡à¤‚à¤Ÿ à¤¸à¥‡ à¤ªà¤°à¤¿à¤šà¤¿à¤¤ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆà¤‚, à¤¤à¥‹ à¤•à¥ƒà¤ªà¤¯à¤¾ à¤‡à¤¸à¥‡ [à¤‰à¤ªà¤¯à¥‹à¤—à¤•à¤°à¥à¤¤à¤¾ à¤¨à¤¿à¤°à¥à¤¦à¥‡à¤¶](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/) à¤ªà¤¢à¤¼à¥‡à¤‚à¥¤\n "
        },
        {
            "sha": "fc3d4ae945cefd4f318558cf71b0b43025beaf35",
            "filename": "i18n/README_ja.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_ja.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_ja.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/i18n%2FREADME_ja.md?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -256,7 +256,7 @@ Hugging Faceãƒãƒ¼ãƒ ã«ã‚ˆã£ã¦ä½œã‚‰ã‚ŒãŸ **[ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒžãƒ¼ã‚’\n \n ### pipã«ã¦\n \n-ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€Python 3.9+, Flax 0.4.1+, PyTorch 1.11+, TensorFlow 2.6+ ã§ãƒ†ã‚¹ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚\n+ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€Python 3.9+, Flax 0.4.1+, PyTorch 2.0+, TensorFlow 2.6+ ã§ãƒ†ã‚¹ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚\n \n ðŸ¤—Transformersã¯[ä»®æƒ³ç’°å¢ƒ](https://docs.python.org/3/library/venv.html)ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚Pythonã®ä»®æƒ³ç’°å¢ƒã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€[ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¬ã‚¤ãƒ‰](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n "
        },
        {
            "sha": "6d6559398e4d1763c620aaf9803ec3c6b5d04b25",
            "filename": "i18n/README_ko.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_ko.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_ko.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/i18n%2FREADME_ko.md?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -242,7 +242,7 @@ Transformersì— ë‹¬ë¦° 100,000ê°œì˜ ë³„ì„ ì¶•í•˜í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì»¤\n \n ### pipë¡œ ì„¤ì¹˜í•˜ê¸°\n \n-ì´ ì €ìž¥ì†ŒëŠ” Python 3.9+, Flax 0.4.1+, PyTorch 1.11+, TensorFlow 2.6+ì—ì„œ í…ŒìŠ¤íŠ¸ ë˜ì—ˆìŠµë‹ˆë‹¤.\n+ì´ ì €ìž¥ì†ŒëŠ” Python 3.9+, Flax 0.4.1+, PyTorch 2.0+, TensorFlow 2.6+ì—ì„œ í…ŒìŠ¤íŠ¸ ë˜ì—ˆìŠµë‹ˆë‹¤.\n \n [ê°€ìƒ í™˜ê²½](https://docs.python.org/3/library/venv.html)ì— ðŸ¤— Transformersë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”. Python ê°€ìƒ í™˜ê²½ì— ìµìˆ™í•˜ì§€ ì•Šë‹¤ë©´, [ì‚¬ìš©ìž ê°€ì´ë“œ](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)ë¥¼ í™•ì¸í•˜ì„¸ìš”.\n "
        },
        {
            "sha": "f865f1b6ed9ca5880eb8bd6f20894bc169fe89dc",
            "filename": "i18n/README_pt-br.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_pt-br.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_pt-br.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/i18n%2FREADME_pt-br.md?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -253,7 +253,7 @@ O modelo em si Ã© um [Pytorch `nn.Module`](https://pytorch.org/docs/stable/nn.ht\n \n ### Com pip\n \n-Este repositÃ³rio Ã© testado no Python 3.9+, Flax 0.4.1+, PyTorch 1.11+ e TensorFlow 2.6+.\n+Este repositÃ³rio Ã© testado no Python 3.9+, Flax 0.4.1+, PyTorch 2.0+ e TensorFlow 2.6+.\n \n VocÃª deve instalar o ðŸ¤— Transformers em um [ambiente virtual](https://docs.python.org/3/library/venv.html). Se vocÃª nÃ£o estÃ¡ familiarizado com ambientes virtuais em Python, confira o [guia do usuÃ¡rio](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).\n "
        },
        {
            "sha": "c153474f33900053dcc8628d90b100982fefbc6a",
            "filename": "i18n/README_ru.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_ru.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_ru.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/i18n%2FREADME_ru.md?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -244,7 +244,7 @@ Hugging Face Hub. ÐœÑ‹ Ñ…Ð¾Ñ‚Ð¸Ð¼, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Transformers Ð¿Ð¾Ð·Ð²Ð¾Ð»Ð¸Ð» Ñ€Ð°\n \n ### Ð¡ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ pip\n \n-Ð”Ð°Ð½Ð½Ñ‹Ð¹ Ñ€ÐµÐ¿Ð¾Ð·Ð¸Ñ‚Ð¾Ñ€Ð¸Ð¹ Ð¿Ñ€Ð¾Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ð½Ð° Python 3.9+, Flax 0.4.1+, PyTorch 1.11+ Ð¸ TensorFlow 2.6+.\n+Ð”Ð°Ð½Ð½Ñ‹Ð¹ Ñ€ÐµÐ¿Ð¾Ð·Ð¸Ñ‚Ð¾Ñ€Ð¸Ð¹ Ð¿Ñ€Ð¾Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ð½Ð° Python 3.9+, Flax 0.4.1+, PyTorch 2.0+ Ð¸ TensorFlow 2.6+.\n \n Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°Ñ‚ÑŒ ðŸ¤— Transformers ÑÐ»ÐµÐ´ÑƒÐµÑ‚ Ð² [Ð²Ð¸Ñ€Ñ‚ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð¹ ÑÑ€ÐµÐ´Ðµ](https://docs.python.org/3/library/venv.html). Ð•ÑÐ»Ð¸ Ð²Ñ‹ Ð½Ðµ Ð·Ð½Ð°ÐºÐ¾Ð¼Ñ‹ Ñ Ð²Ð¸Ñ€Ñ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ ÑÑ€ÐµÐ´Ð°Ð¼Ð¸ Python, Ð¾Ð·Ð½Ð°ÐºÐ¾Ð¼ÑŒÑ‚ÐµÑÑŒ Ñ [Ñ€ÑƒÐºÐ¾Ð²Ð¾Ð´ÑÑ‚Ð²Ð¾Ð¼ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).\n "
        },
        {
            "sha": "791ed6414f73d2005386fdef788c41becc0d51bd",
            "filename": "i18n/README_te.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_te.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_te.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/i18n%2FREADME_te.md?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -246,7 +246,7 @@ limitations under the License.\n \n ### à°ªà°¿à°ªà± à°¤à±‹\n \n-à°ˆ à°°à°¿à°ªà±‹à°œà°¿à°Ÿà°°à±€ à°ªà±ˆà°¥à°¾à°¨à± 3.9+, à°«à±à°²à°¾à°•à±à°¸à± 0.4.1+, PyTorch 1.11+ à°®à°°à°¿à°¯à± TensorFlow 2.6+à°²à±‹ à°ªà°°à±€à°•à±à°·à°¿à°‚à°šà°¬à°¡à°¿à°‚à°¦à°¿.\n+à°ˆ à°°à°¿à°ªà±‹à°œà°¿à°Ÿà°°à±€ à°ªà±ˆà°¥à°¾à°¨à± 3.9+, à°«à±à°²à°¾à°•à±à°¸à± 0.4.1+, PyTorch 2.0+ à°®à°°à°¿à°¯à± TensorFlow 2.6+à°²à±‹ à°ªà°°à±€à°•à±à°·à°¿à°‚à°šà°¬à°¡à°¿à°‚à°¦à°¿.\n \n à°®à±€à°°à± [à°µà°°à±à°šà±à°µà°²à± à°µà°¾à°¤à°¾à°µà°°à°£à°‚](https://docs.python.org/3/library/venv.html)à°²à±‹ ðŸ¤— à°Ÿà±à°°à°¾à°¨à±à°¸à±â€Œà°«à°¾à°°à±à°®à°°à±â€Œà°²à°¨à± à°‡à°¨à±â€Œà°¸à±à°Ÿà°¾à°²à± à°šà±‡à°¯à°¾à°²à°¿. à°®à±€à°•à± à°ªà±ˆà°¥à°¾à°¨à± à°µà°°à±à°šà±à°µà°²à± à°ªà°°à°¿à°¸à°°à°¾à°² à°—à±à°°à°¿à°‚à°šà°¿ à°¤à±†à°²à°¿à°¯à°•à±à°‚à°Ÿà±‡, [à°¯à±‚à°œà°°à± à°—à±ˆà°¡à±](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/) à°šà±‚à°¡à°‚à°¡à°¿.\n "
        },
        {
            "sha": "2d4d7745f68eaf436c8117c3df89d40ee8b083a8",
            "filename": "i18n/README_ur.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_ur.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_ur.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/i18n%2FREADME_ur.md?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -259,7 +259,7 @@ limitations under the License.\n \n #### &#8207; pip Ú©Û’ Ø³Ø§ØªÚ¾\n \n-ÛŒÛ Ø±ÛŒÙ¾ÙˆØ²Ù¹Ø±ÛŒ Python 3.9+ØŒ Flax 0.4.1+ØŒ PyTorch 1.11+ØŒ Ø§ÙˆØ± TensorFlow 2.6+ Ù¾Ø± Ù¹ÛŒØ³Ù¹ Ú©ÛŒ Ú¯Ø¦ÛŒ ÛÛ’Û”\n+ÛŒÛ Ø±ÛŒÙ¾ÙˆØ²Ù¹Ø±ÛŒ Python 3.9+ØŒ Flax 0.4.1+ØŒ PyTorch 2.0+ØŒ Ø§ÙˆØ± TensorFlow 2.6+ Ù¾Ø± Ù¹ÛŒØ³Ù¹ Ú©ÛŒ Ú¯Ø¦ÛŒ ÛÛ’Û”\n \n Ø¢Ù¾ Ú©Ùˆ ðŸ¤— Transformers Ú©Ùˆ Ø§ÛŒÚ© [ÙˆØ±Ú†ÙˆØ¦Ù„ Ù…Ø§Ø­ÙˆÙ„](https://docs.python.org/3/library/venv.html) Ù…ÛŒÚº Ø§Ù†Ø³Ù¹Ø§Ù„ Ú©Ø±Ù†Ø§ Ú†Ø§ÛÛŒÛ’Û” Ø§Ú¯Ø± Ø¢Ù¾ Python ÙˆØ±Ú†ÙˆØ¦Ù„ Ù…Ø§Ø­ÙˆÙ„ Ø³Û’ ÙˆØ§Ù‚Ù Ù†ÛÛŒÚº ÛÛŒÚºØŒ ØªÙˆ [ÛŒÙˆØ²Ø± Ú¯Ø§Ø¦ÛŒÚˆ](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/) Ø¯ÛŒÚ©Ú¾ÛŒÚºÛ”\n "
        },
        {
            "sha": "4f7f67bfce90ffc6ead5da5fb2480a1d0cc73ec0",
            "filename": "i18n/README_vi.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_vi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_vi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/i18n%2FREADME_vi.md?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -245,7 +245,7 @@ ChÃ­nh mÃ´ hÃ¬nh lÃ  má»™t [Pytorch `nn.Module`](https://pytorch.org/docs/stable\n \n ### Sá»­ dá»¥ng pip\n \n-ThÆ° viá»‡n nÃ y Ä‘Æ°á»£c kiá»ƒm tra trÃªn Python 3.9+, Flax 0.4.1+, PyTorch 1.11+ vÃ  TensorFlow 2.6+.\n+ThÆ° viá»‡n nÃ y Ä‘Æ°á»£c kiá»ƒm tra trÃªn Python 3.9+, Flax 0.4.1+, PyTorch 2.0+ vÃ  TensorFlow 2.6+.\n \n Báº¡n nÃªn cÃ i Ä‘áº·t ðŸ¤— Transformers trong má»™t [mÃ´i trÆ°á»ng áº£o Python](https://docs.python.org/3/library/venv.html). Náº¿u báº¡n chÆ°a quen vá»›i mÃ´i trÆ°á»ng áº£o Python, hÃ£y xem [hÆ°á»›ng dáº«n sá»­ dá»¥ng](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).\n "
        },
        {
            "sha": "b4d121df0d32009df809714dd040244427866ec5",
            "filename": "i18n/README_zh-hans.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_zh-hans.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_zh-hans.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/i18n%2FREADME_zh-hans.md?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -198,7 +198,7 @@ checkpoint: æ£€æŸ¥ç‚¹\n \n ### ä½¿ç”¨ pip\n \n-è¿™ä¸ªä»“åº“å·²åœ¨ Python 3.9+ã€Flax 0.4.1+ã€PyTorch 1.11+ å’Œ TensorFlow 2.6+ ä¸‹ç»è¿‡æµ‹è¯•ã€‚\n+è¿™ä¸ªä»“åº“å·²åœ¨ Python 3.9+ã€Flax 0.4.1+ã€PyTorch 2.0+ å’Œ TensorFlow 2.6+ ä¸‹ç»è¿‡æµ‹è¯•ã€‚\n \n ä½ å¯ä»¥åœ¨[è™šæ‹ŸçŽ¯å¢ƒ](https://docs.python.org/3/library/venv.html)ä¸­å®‰è£… ðŸ¤— Transformersã€‚å¦‚æžœä½ è¿˜ä¸ç†Ÿæ‚‰ Python çš„è™šæ‹ŸçŽ¯å¢ƒï¼Œè¯·é˜…æ­¤[ç”¨æˆ·è¯´æ˜Ž](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)ã€‚\n "
        },
        {
            "sha": "dcafd4958ed1d1dfd55e8948ae92149cedfafc99",
            "filename": "i18n/README_zh-hant.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_zh-hant.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/i18n%2FREADME_zh-hant.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/i18n%2FREADME_zh-hant.md?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -210,7 +210,7 @@ Tokenizer ç‚ºæ‰€æœ‰çš„é è¨“ç·´æ¨¡åž‹æä¾›äº†é è™•ç†ï¼Œä¸¦å¯ä»¥ç›´æŽ¥è½‰æ›\n \n ### ä½¿ç”¨ pip\n \n-é€™å€‹ Repository å·²åœ¨ Python 3.9+ã€Flax 0.4.1+ã€PyTorch 1.11+ å’Œ TensorFlow 2.6+ ä¸‹ç¶“éŽæ¸¬è©¦ã€‚\n+é€™å€‹ Repository å·²åœ¨ Python 3.9+ã€Flax 0.4.1+ã€PyTorch 2.0+ å’Œ TensorFlow 2.6+ ä¸‹ç¶“éŽæ¸¬è©¦ã€‚\n \n ä½ å¯ä»¥åœ¨[è™›æ“¬ç’°å¢ƒ](https://docs.python.org/3/library/venv.html)ä¸­å®‰è£ ðŸ¤— Transformersã€‚å¦‚æžœä½ é‚„ä¸ç†Ÿæ‚‰ Python çš„è™›æ“¬ç’°å¢ƒï¼Œè«‹é–±æ­¤[ä½¿ç”¨è€…æŒ‡å¼•](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)ã€‚\n "
        },
        {
            "sha": "c3431ad5b2e0ac6e0969e24b3a00922edb382116",
            "filename": "src/transformers/convert_pytorch_checkpoint_to_tf2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Fconvert_pytorch_checkpoint_to_tf2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Fconvert_pytorch_checkpoint_to_tf2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_pytorch_checkpoint_to_tf2.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -106,7 +106,6 @@\n         XLMWithLMHeadModel,\n         XLNetLMHeadModel,\n     )\n-    from .pytorch_utils import is_torch_greater_or_equal_than_1_13\n \n \n logging.set_verbosity_info()\n@@ -279,7 +278,7 @@ def convert_pt_checkpoint_to_tf(\n     if compare_with_pt_model:\n         tfo = tf_model(tf_model.dummy_inputs, training=False)  # build the network\n \n-        weights_only_kwarg = {\"weights_only\": True} if is_torch_greater_or_equal_than_1_13 else {}\n+        weights_only_kwarg = {\"weights_only\": True}\n         state_dict = torch.load(\n             pytorch_checkpoint_path,\n             map_location=\"cpu\","
        },
        {
            "sha": "8fbba8a1651364eef58cbed7fa6c037c6eee86e2",
            "filename": "src/transformers/modeling_flax_pytorch_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Fmodeling_flax_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Fmodeling_flax_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flax_pytorch_utils.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -63,8 +63,6 @@ def load_pytorch_checkpoint_in_flax_state_dict(\n         else:\n             try:\n                 import torch  # noqa: F401\n-\n-                from .pytorch_utils import is_torch_greater_or_equal_than_1_13  # noqa: F401\n             except (ImportError, ModuleNotFoundError):\n                 logger.error(\n                     \"Loading a PyTorch model in Flax, requires both PyTorch and Flax to be installed. Please see\"\n@@ -73,7 +71,7 @@ def load_pytorch_checkpoint_in_flax_state_dict(\n                 )\n                 raise\n \n-            weights_only_kwarg = {\"weights_only\": True} if is_torch_greater_or_equal_than_1_13 else {}\n+            weights_only_kwarg = {\"weights_only\": True}\n             pt_state_dict = torch.load(pt_path, map_location=\"cpu\", **weights_only_kwarg)\n             logger.info(f\"PyTorch checkpoint contains {sum(t.numel() for t in pt_state_dict.values()):,} parameters.\")\n \n@@ -246,13 +244,11 @@ def convert_pytorch_state_dict_to_flax(pt_state_dict, flax_model):\n def convert_pytorch_sharded_state_dict_to_flax(shard_filenames, flax_model):\n     import torch\n \n-    from .pytorch_utils import is_torch_greater_or_equal_than_1_13\n-\n     # Load the index\n     flax_state_dict = {}\n     for shard_file in shard_filenames:\n         # load using msgpack utils\n-        weights_only_kwarg = {\"weights_only\": True} if is_torch_greater_or_equal_than_1_13 else {}\n+        weights_only_kwarg = {\"weights_only\": True}\n         pt_state_dict = torch.load(shard_file, **weights_only_kwarg)\n         weight_dtypes = {k: v.dtype for k, v in pt_state_dict.items()}\n         pt_state_dict = {"
        },
        {
            "sha": "8ec24d6e1872ef1ab8878e5cf6e8df2919b76cf7",
            "filename": "src/transformers/modeling_tf_pytorch_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Fmodeling_tf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Fmodeling_tf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_tf_pytorch_utils.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -180,8 +180,6 @@ def load_pytorch_checkpoint_in_tf2_model(\n         import tensorflow as tf  # noqa: F401\n         import torch  # noqa: F401\n         from safetensors.torch import load_file as safe_load_file  # noqa: F401\n-\n-        from .pytorch_utils import is_torch_greater_or_equal_than_1_13  # noqa: F401\n     except ImportError:\n         logger.error(\n             \"Loading a PyTorch model in TensorFlow, requires both PyTorch and TensorFlow to be installed. Please see \"\n@@ -201,7 +199,7 @@ def load_pytorch_checkpoint_in_tf2_model(\n         if pt_path.endswith(\".safetensors\"):\n             state_dict = safe_load_file(pt_path)\n         else:\n-            weights_only_kwarg = {\"weights_only\": True} if is_torch_greater_or_equal_than_1_13 else {}\n+            weights_only_kwarg = {\"weights_only\": True}\n             state_dict = torch.load(pt_path, map_location=\"cpu\", **weights_only_kwarg)\n \n         pt_state_dict.update(state_dict)"
        },
        {
            "sha": "a6d4a1cc5b54ed4acd65510f28f4d41a594fb565",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -54,7 +54,6 @@\n     apply_chunking_to_forward,\n     find_pruneable_heads_and_indices,\n     id_tensor_storage,\n-    is_torch_greater_or_equal_than_1_13,\n     prune_conv1d_layer,\n     prune_layer,\n     prune_linear_layer,\n@@ -476,7 +475,7 @@ def load_sharded_checkpoint(model, folder, strict=True, prefer_safe=True):\n             error_message += f\"\\nMissing key(s): {str_unexpected_keys}.\"\n         raise RuntimeError(error_message)\n \n-    weights_only_kwarg = {\"weights_only\": True} if is_torch_greater_or_equal_than_1_13 else {}\n+    weights_only_kwarg = {\"weights_only\": True}\n     loader = safe_load_file if load_safe else partial(torch.load, map_location=\"cpu\", **weights_only_kwarg)\n \n     for shard_file in shard_files:\n@@ -532,7 +531,7 @@ def load_state_dict(\n             and is_zipfile(checkpoint_file)\n         ):\n             extra_args = {\"mmap\": True}\n-        weights_only_kwarg = {\"weights_only\": weights_only} if is_torch_greater_or_equal_than_1_13 else {}\n+        weights_only_kwarg = {\"weights_only\": weights_only}\n         return torch.load(\n             checkpoint_file,\n             map_location=map_location,"
        },
        {
            "sha": "e0e4ff424cb47d1dde348c15285397fd2cacbc2a",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -38,7 +38,6 @@\n )\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import is_torch_greater_or_equal_than_2_0\n from ...utils import (\n     add_code_sample_docstrings,\n     add_start_docstrings,\n@@ -815,14 +814,6 @@ def _init_weights(self, module: nn.Module):\n     # Adapted from transformers.modeling_utils.PreTrainedModel._check_and_enable_sdpa\n     @classmethod\n     def _check_and_enable_sdpa(cls, config, hard_check_only: bool = False) -> \"PretrainedConfig\":\n-        # NOTE: Falcon supported SDPA from PyTorch 2.0. We keep it like that for backward compatibility (automatically use SDPA for torch>=2.0).\n-        if hard_check_only:\n-            if not is_torch_greater_or_equal_than_2_0:\n-                raise ImportError(\"PyTorch SDPA requirements in Transformers are not met. Please install torch>=2.0.\")\n-\n-        if not is_torch_greater_or_equal_than_2_0:\n-            return config\n-\n         _is_bettertransformer = getattr(cls, \"use_bettertransformer\", False)\n         if _is_bettertransformer:\n             return config"
        },
        {
            "sha": "ef23b5d208fd79bfecf6a7cd19a0f4769f2beb6e",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -36,7 +36,6 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import is_torch_greater_or_equal_than_1_13\n from ...utils import (\n     add_code_sample_docstrings,\n     add_start_docstrings,\n@@ -56,9 +55,6 @@\n # This makes `_prepare_4d_causal_attention_mask` a leaf function in the FX graph.\n # It means that the function will not be traced through and simply appear as a node in the graph.\n if is_torch_fx_available():\n-    if not is_torch_greater_or_equal_than_1_13:\n-        import torch.fx\n-\n     _prepare_4d_causal_attention_mask = torch.fx.wrap(_prepare_4d_causal_attention_mask)\n \n "
        },
        {
            "sha": "8f6b092da6e6adbafcb24b3c361d8f5f9c289e2d",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -33,7 +33,6 @@\n )\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import is_torch_greater_or_equal_than_1_13\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n@@ -51,9 +50,6 @@\n # This makes `_prepare_4d_causal_attention_mask` a leaf function in the FX graph.\n # It means that the function will not be traced through and simply appear as a node in the graph.\n if is_torch_fx_available():\n-    if not is_torch_greater_or_equal_than_1_13:\n-        import torch.fx\n-\n     _prepare_4d_causal_attention_mask = torch.fx.wrap(_prepare_4d_causal_attention_mask)\n \n "
        },
        {
            "sha": "dcdd85460b39bd6824a97dd16bfe049467f6e6b8",
            "filename": "src/transformers/models/superpoint/modeling_superpoint.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fmodeling_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fmodeling_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fmodeling_superpoint.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -25,7 +25,6 @@\n )\n from transformers.models.superpoint.configuration_superpoint import SuperPointConfig\n \n-from ...pytorch_utils import is_torch_greater_or_equal_than_1_13\n from ...utils import (\n     ModelOutput,\n     add_start_docstrings,\n@@ -314,7 +313,7 @@ def _sample_descriptors(keypoints, descriptors, scale: int = 8) -> torch.Tensor:\n         divisor = divisor.to(keypoints)\n         keypoints /= divisor\n         keypoints = keypoints * 2 - 1  # normalize to (-1, 1)\n-        kwargs = {\"align_corners\": True} if is_torch_greater_or_equal_than_1_13 else {}\n+        kwargs = {\"align_corners\": True}\n         # [batch_size, num_channels, num_keypoints, 2] -> [batch_size, num_channels, num_keypoints, 2]\n         keypoints = keypoints.view(batch_size, 1, -1, 2)\n         descriptors = nn.functional.grid_sample(descriptors, keypoints, mode=\"bilinear\", **kwargs)"
        },
        {
            "sha": "2ea0d38a23f933d887112bdd0da4c0dfa51bc214",
            "filename": "src/transformers/models/tapas/modeling_tapas.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -31,7 +31,6 @@\n from ...pytorch_utils import (\n     apply_chunking_to_forward,\n     find_pruneable_heads_and_indices,\n-    is_torch_greater_or_equal_than_1_12,\n     prune_linear_layer,\n )\n from ...utils import (\n@@ -46,12 +45,6 @@\n \n logger = logging.get_logger(__name__)\n \n-if not is_torch_greater_or_equal_than_1_12:\n-    logger.warning(\n-        f\"You are using torch=={torch.__version__}, but torch>=1.12.0 is required to use \"\n-        \"TapasModel. Please upgrade torch.\"\n-    )\n-\n _CONFIG_FOR_DOC = \"TapasConfig\"\n _CHECKPOINT_FOR_DOC = \"google/tapas-base\"\n "
        },
        {
            "sha": "5168904a3579d99a2a2283e8448f82114b470f27",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -38,7 +38,6 @@\n     XVectorOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import is_torch_greater_or_equal_than_1_13\n from ...utils import (\n     ModelOutput,\n     add_code_sample_docstrings,\n@@ -1590,7 +1589,7 @@ def load_adapter(self, target_lang: str, force_load=True, **kwargs):\n                     cache_dir=cache_dir,\n                 )\n \n-                weights_only_kwarg = {\"weights_only\": True} if is_torch_greater_or_equal_than_1_13 else {}\n+                weights_only_kwarg = {\"weights_only\": True}\n                 state_dict = torch.load(\n                     weight_path,\n                     map_location=\"cpu\","
        },
        {
            "sha": "95c8748375ce0a5823c6260d558fce2a17736b96",
            "filename": "src/transformers/pytorch_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Fpytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Fpytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpytorch_utils.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -34,9 +34,6 @@\n is_torch_greater_or_equal_than_2_3 = parsed_torch_version_base >= version.parse(\"2.3\")\n is_torch_greater_or_equal_than_2_2 = parsed_torch_version_base >= version.parse(\"2.2\")\n is_torch_greater_or_equal_than_2_1 = parsed_torch_version_base >= version.parse(\"2.1\")\n-is_torch_greater_or_equal_than_2_0 = parsed_torch_version_base >= version.parse(\"2.0\")\n-is_torch_greater_or_equal_than_1_13 = parsed_torch_version_base >= version.parse(\"1.13\")\n-is_torch_greater_or_equal_than_1_12 = parsed_torch_version_base >= version.parse(\"1.12\")\n \n # Cache this result has it's a C FFI call which can be pretty time-consuming\n _torch_distributed_available = torch.distributed.is_available()"
        },
        {
            "sha": "c878d2b345cc3144cd9de767da725132b12a3a7e",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -75,7 +75,6 @@\n from .processing_utils import ProcessorMixin\n from .pytorch_utils import (\n     ALL_LAYERNORM_LAYERS,\n-    is_torch_greater_or_equal_than_1_13,\n     is_torch_greater_or_equal_than_2_3,\n )\n from .tokenization_utils_base import PreTrainedTokenizerBase\n@@ -2778,7 +2777,7 @@ def _load_from_checkpoint(self, resume_from_checkpoint, model=None):\n                 )\n \n         if os.path.isfile(weights_file) or os.path.isfile(safe_weights_file) or is_fsdp_ckpt:\n-            weights_only_kwarg = {\"weights_only\": True} if is_torch_greater_or_equal_than_1_13 else {}\n+            weights_only_kwarg = {\"weights_only\": True}\n             # If the model is on the GPU, it still works!\n             if is_sagemaker_mp_enabled():\n                 if os.path.isfile(os.path.join(resume_from_checkpoint, \"user_content.pt\")):\n@@ -2899,7 +2898,7 @@ def _load_best_model(self):\n             or os.path.exists(best_safe_adapter_model_path)\n         ):\n             has_been_loaded = True\n-            weights_only_kwarg = {\"weights_only\": True} if is_torch_greater_or_equal_than_1_13 else {}\n+            weights_only_kwarg = {\"weights_only\": True}\n             if is_sagemaker_mp_enabled():\n                 if os.path.isfile(os.path.join(self.state.best_model_checkpoint, \"user_content.pt\")):\n                     # If the 'user_content.pt' file exists, load with the new smp api."
        },
        {
            "sha": "da95329e1845676ddde9086fa9a8c0cf67233c3a",
            "filename": "src/transformers/trainer_pt_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_pt_utils.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -56,12 +56,7 @@\n     import torch_xla.core.xla_model as xm\n \n if is_torch_available():\n-    from .pytorch_utils import is_torch_greater_or_equal_than_2_0\n-\n-    if is_torch_greater_or_equal_than_2_0:\n-        from torch.optim.lr_scheduler import LRScheduler\n-    else:\n-        from torch.optim.lr_scheduler import _LRScheduler as LRScheduler\n+    from torch.optim.lr_scheduler import LRScheduler\n \n \n logger = logging.get_logger(__name__)"
        },
        {
            "sha": "6950e8e66d3ac1e01241e11f89549aba89071b61",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 2,
            "deletions": 16,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -71,8 +71,6 @@\n     import torch\n     import torch.distributed as dist\n \n-    from .pytorch_utils import is_torch_greater_or_equal_than_2_0\n-\n if is_accelerate_available():\n     from accelerate.state import AcceleratorState, PartialState\n     from accelerate.utils import DistributedType\n@@ -1157,7 +1155,7 @@ class TrainingArguments:\n         },\n     )\n     dataloader_prefetch_factor: Optional[int] = field(\n-        default=None if not is_torch_available() or is_torch_greater_or_equal_than_2_0 else 2,\n+        default=None,\n         metadata={\n             \"help\": (\n                 \"Number of batches loaded in advance by each worker. \"\n@@ -1702,14 +1700,6 @@ def __post_init__(self):\n                         raise ValueError(\n                             \"Your setup doesn't support bf16/gpu. You need torch>=1.10, using Ampere GPU with cuda>=11.0\"\n                         )\n-                    elif not is_torch_xpu_available():\n-                        # xpu\n-                        from .pytorch_utils import is_torch_greater_or_equal_than_1_12\n-\n-                        if not is_torch_greater_or_equal_than_1_12:\n-                            raise ValueError(\n-                                \"Your setup doesn't support bf16/xpu. You need torch>=1.12, using Intel XPU/GPU with IPEX installed\"\n-                            )\n \n         if self.fp16 and self.bf16:\n             raise ValueError(\"At most one of fp16 and bf16 can be True, but not both\")\n@@ -2056,11 +2046,7 @@ def __post_init__(self):\n         if self.use_cpu:\n             self.dataloader_pin_memory = False\n \n-        if (\n-            (not is_torch_available() or is_torch_greater_or_equal_than_2_0)\n-            and self.dataloader_num_workers == 0\n-            and self.dataloader_prefetch_factor is not None\n-        ):\n+        if self.dataloader_num_workers == 0 and self.dataloader_prefetch_factor is not None:\n             raise ValueError(\n                 \"--dataloader_prefetch_factor can only be set when data is loaded in a different process, i.e.\"\n                 \" when --dataloader_num_workers > 1.\""
        },
        {
            "sha": "45fa3d9ca68c51609bbfca08b6840b2c4a6c728e",
            "filename": "src/transformers/utils/fx.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Futils%2Ffx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/src%2Ftransformers%2Futils%2Ffx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Ffx.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -60,7 +60,6 @@\n     MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING_NAMES,\n     MODEL_MAPPING_NAMES,\n )\n-from ..pytorch_utils import is_torch_greater_or_equal_than_2_0\n from .import_utils import (\n     ENV_VARS_TRUE_VALUES,\n     TORCH_FX_REQUIRED_VERSION,\n@@ -635,10 +634,9 @@ def to_concrete(t):\n     operator.getitem: operator_getitem,\n }\n \n-if is_torch_greater_or_equal_than_2_0:\n-    _MANUAL_META_OVERRIDES[torch.nn.functional.scaled_dot_product_attention] = (\n-        torch_nn_functional_scaled_dot_product_attention\n-    )\n+_MANUAL_META_OVERRIDES[torch.nn.functional.scaled_dot_product_attention] = (\n+    torch_nn_functional_scaled_dot_product_attention\n+)\n \n \n class HFProxy(Proxy):"
        },
        {
            "sha": "b6f1da56c6782eff3a8bd68571f89e9ef3c84a93",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -45,8 +45,7 @@\n \n if is_torch_available():\n     import torch\n-else:\n-    is_torch_greater_or_equal_than_2_0 = False\n+\n \n if is_vision_available():\n     from PIL import Image"
        },
        {
            "sha": "f02e8f167636eb36c54e34cb7e80f41f21376a56",
            "filename": "tests/models/falcon_mamba/test_modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -43,9 +43,6 @@\n         FalconMambaModel,\n     )\n     from transformers.cache_utils import MambaCache\n-    from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_0\n-else:\n-    is_torch_greater_or_equal_than_2_0 = False\n \n \n # Copied from transformers.tests.models.mamba.MambaModelTester with Mamba->FalconMamba,mamba->falcon_mamba\n@@ -246,9 +243,6 @@ def prepare_config_and_inputs_for_common(self):\n         return config, inputs_dict\n \n \n-@unittest.skipIf(\n-    not is_torch_greater_or_equal_than_2_0, reason=\"See https://github.com/huggingface/transformers/pull/24204\"\n-)\n @require_torch\n # Copied from transformers.tests.models.mamba.MambaModelTest with Mamba->Falcon,mamba->falcon_mamba,FalconMambaCache->MambaCache\n class FalconMambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):"
        },
        {
            "sha": "281594492500b089509c4840fcb4318fcb802a99",
            "filename": "tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -37,9 +37,6 @@\n         GPTBigCodeModel,\n     )\n     from transformers.models.gpt_bigcode.modeling_gpt_bigcode import GPTBigCodeAttention\n-    from transformers.pytorch_utils import is_torch_greater_or_equal_than_1_12\n-else:\n-    is_torch_greater_or_equal_than_1_12 = False\n \n \n class GPTBigCodeModelTester:\n@@ -504,10 +501,6 @@ class GPTBigCodeMHAModelTest(GPTBigCodeModelTest):\n     multi_query = False\n \n \n-@unittest.skipIf(\n-    not is_torch_greater_or_equal_than_1_12,\n-    reason=\"`GPTBigCode` checkpoints use `PytorchGELUTanh` which requires `torch>=1.12.0`.\",\n-)\n @slow\n @require_torch\n class GPTBigCodeModelLanguageGenerationTest(unittest.TestCase):"
        },
        {
            "sha": "50840bbcfaa6dc31895329a2419fe40e9f1106fc",
            "filename": "tests/models/gptj/test_modeling_gptj.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -41,9 +41,6 @@\n         GPTJForSequenceClassification,\n         GPTJModel,\n     )\n-    from transformers.pytorch_utils import is_torch_greater_or_equal_than_1_12\n-else:\n-    is_torch_greater_or_equal_than_1_12 = False\n \n \n class GPTJModelTester:\n@@ -363,15 +360,9 @@ class GPTJModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n     test_model_parallel = False\n     test_head_masking = False\n \n-    @unittest.skipIf(\n-        not is_torch_greater_or_equal_than_1_12, reason=\"PR #22069 made changes that require torch v1.12+.\"\n-    )\n     def test_torch_fx(self):\n         super().test_torch_fx()\n \n-    @unittest.skipIf(\n-        not is_torch_greater_or_equal_than_1_12, reason=\"PR #22069 made changes that require torch v1.12+.\"\n-    )\n     def test_torch_fx_output_loss(self):\n         super().test_torch_fx_output_loss()\n "
        },
        {
            "sha": "94229b13d2cbfeba19cd5cdbafc12713faa0fbbd",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -44,9 +44,6 @@\n \n     from transformers import IdeficsForVisionText2Text, IdeficsModel, IdeficsProcessor\n     from transformers.models.idefics.configuration_idefics import IdeficsPerceiverConfig, IdeficsVisionConfig\n-    from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_0\n-else:\n-    is_torch_greater_or_equal_than_2_0 = False\n \n if is_vision_available():\n     from PIL import Image\n@@ -327,7 +324,6 @@ def test_eager_matches_sdpa_generate(self):\n         self.skipTest(reason=\"Idefics has a hard requirement on SDPA, skipping this test\")\n \n \n-@unittest.skipIf(not is_torch_greater_or_equal_than_2_0, reason=\"pytorch 2.0 or higher is required\")\n @require_torch\n class IdeficsModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (IdeficsModel, IdeficsForVisionText2Text) if is_torch_available() else ()\n@@ -594,7 +590,6 @@ def test_sdpa_can_dispatch_non_composite_models(self):\n         pass\n \n \n-@unittest.skipIf(not is_torch_greater_or_equal_than_2_0, reason=\"pytorch 2.0 or higher is required\")\n @require_torch\n class IdeficsForVisionText2TextTest(IdeficsModelTest, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (IdeficsForVisionText2Text,) if is_torch_available() else ()\n@@ -818,7 +813,6 @@ def test_sdpa_can_dispatch_non_composite_models(self):\n         pass\n \n \n-@unittest.skipIf(not is_torch_greater_or_equal_than_2_0, reason=\"pytorch 2.0 or higher is required\")\n @require_torch\n @require_vision\n class IdeficsModelIntegrationTest(TestCasePlus):"
        },
        {
            "sha": "974628c8b4324f3daccb0851ae96139eddba5b0b",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -48,8 +48,6 @@\n \n if is_torch_available():\n     import torch\n-else:\n-    is_torch_greater_or_equal_than_2_0 = False\n \n if is_vision_available():\n     from PIL import Image"
        },
        {
            "sha": "c25fa1180649fa41714ffb57d6157559793f607f",
            "filename": "tests/models/idefics3/test_modeling_idefics3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -40,8 +40,6 @@\n         Idefics3ForConditionalGeneration,\n         Idefics3Model,\n     )\n-else:\n-    is_torch_greater_or_equal_than_2_0 = False\n \n if is_vision_available():\n     from PIL import Image"
        },
        {
            "sha": "b4a959a00d2a0c0097d4df6afae90922efe5a769",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -43,8 +43,7 @@\n \n if is_torch_available():\n     import torch\n-else:\n-    is_torch_greater_or_equal_than_2_0 = False\n+\n \n if is_vision_available():\n     from PIL import Image"
        },
        {
            "sha": "14b0fb8cc07db731b145eb6f15ec0e2031694c43",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -48,8 +48,7 @@\n     import torch\n \n     from transformers.models.llava_next.modeling_llava_next import image_size_to_num_patches\n-else:\n-    is_torch_greater_or_equal_than_2_0 = False\n+\n \n if is_vision_available():\n     from PIL import Image"
        },
        {
            "sha": "c431f91bf5102f53abec1a6346d5a47b1491cc16",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -48,8 +48,6 @@\n if is_torch_available():\n     import torch\n \n-else:\n-    is_torch_greater_or_equal_than_2_0 = False\n \n if is_vision_available():\n     from PIL import Image"
        },
        {
            "sha": "6965d2033ec73099d44354769e14b1d72b47f346",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -48,8 +48,6 @@\n if is_torch_available():\n     import torch\n \n-else:\n-    is_torch_greater_or_equal_than_2_0 = False\n \n if is_vision_available():\n     from PIL import Image"
        },
        {
            "sha": "455022140f7c5b79c3a6c412f9d91ad072a36c18",
            "filename": "tests/models/mamba/test_modeling_mamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -38,9 +38,6 @@\n         MambaModel,\n     )\n     from transformers.models.mamba.modeling_mamba import MambaCache\n-    from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_0\n-else:\n-    is_torch_greater_or_equal_than_2_0 = False\n \n \n class MambaModelTester:\n@@ -239,9 +236,6 @@ def prepare_config_and_inputs_for_common(self):\n         return config, inputs_dict\n \n \n-@unittest.skipIf(\n-    not is_torch_greater_or_equal_than_2_0, reason=\"See https://github.com/huggingface/transformers/pull/24204\"\n-)\n @require_torch\n class MambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (MambaModel, MambaForCausalLM) if is_torch_available() else ()"
        },
        {
            "sha": "17cbdc1e8d51dd33ad8ceff13a0189ed714c02c7",
            "filename": "tests/models/mamba2/test_modeling_mamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -37,9 +37,6 @@\n         Mamba2Model,\n     )\n     from transformers.models.mamba2.modeling_mamba2 import Mamba2Cache, Mamba2Mixer\n-    from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_0\n-else:\n-    is_torch_greater_or_equal_than_2_0 = False\n \n \n class Mamba2ModelTester:\n@@ -214,9 +211,6 @@ def create_and_check_mamba2_slow_vs_fast_forward(self, config, input_ids, *args,\n         self.parent.assertTrue(torch.allclose(outputs_fast, outputs_slow, atol=1e-3, rtol=1e-3))\n \n \n-@unittest.skipIf(\n-    not is_torch_greater_or_equal_than_2_0, reason=\"See https://github.com/huggingface/transformers/pull/24204\"\n-)\n @require_torch\n class Mamba2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (Mamba2Model, Mamba2ForCausalLM) if is_torch_available() else ()"
        },
        {
            "sha": "f973e1211dc081f899ab012bea162a06a2b6c0ca",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -40,8 +40,7 @@\n \n if is_torch_available():\n     import torch\n-else:\n-    is_torch_greater_or_equal_than_2_0 = False\n+\n \n if is_vision_available():\n     from PIL import Image"
        },
        {
            "sha": "3e5667caf45e3e6e41bb38aaf098309d2e242c24",
            "filename": "tests/models/pixtral/test_modeling_pixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -33,8 +33,7 @@\n \n if is_torch_available():\n     import torch\n-else:\n-    is_torch_greater_or_equal_than_2_0 = False\n+\n \n if is_vision_available():\n     pass"
        },
        {
            "sha": "8974d6923b391c731106761d46ebaafa6a736c39",
            "filename": "tests/models/qwen2_audio/test_modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -41,8 +41,6 @@\n \n if is_torch_available():\n     import torch\n-else:\n-    is_torch_greater_or_equal_than_2_0 = False\n \n \n class Qwen2AudioModelTester:"
        },
        {
            "sha": "2c27e1a03a647cabc56f1d07c73aa2be686d6c27",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -47,8 +47,6 @@\n if is_torch_available():\n     import torch\n \n-else:\n-    is_torch_greater_or_equal_than_2_0 = False\n \n if is_vision_available():\n     from PIL import Image"
        },
        {
            "sha": "0bc5c2de070135a38b2e15257f7ac6597d88110d",
            "filename": "tests/models/rwkv/test_modeling_rwkv.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -33,9 +33,6 @@\n         RwkvForCausalLM,\n         RwkvModel,\n     )\n-    from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_0\n-else:\n-    is_torch_greater_or_equal_than_2_0 = False\n \n \n class RwkvModelTester:\n@@ -231,9 +228,6 @@ def prepare_config_and_inputs_for_common(self):\n         return config, inputs_dict\n \n \n-@unittest.skipIf(\n-    not is_torch_greater_or_equal_than_2_0, reason=\"See https://github.com/huggingface/transformers/pull/24204\"\n-)\n @require_torch\n class RwkvModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (RwkvModel, RwkvForCausalLM) if is_torch_available() else ()\n@@ -440,9 +434,6 @@ def test_left_padding_compatibility(self):\n         pass\n \n \n-@unittest.skipIf(\n-    not is_torch_greater_or_equal_than_2_0, reason=\"See https://github.com/huggingface/transformers/pull/24204\"\n-)\n @slow\n class RWKVIntegrationTests(unittest.TestCase):\n     def setUp(self):"
        },
        {
            "sha": "05618f4a4efd8cae15bf0b611e31d0aba1df1153",
            "filename": "tests/models/tapas/test_modeling_tapas.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Ftapas%2Ftest_modeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Ftapas%2Ftest_modeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftapas%2Ftest_modeling_tapas.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -60,9 +60,6 @@\n         reduce_mean,\n         reduce_sum,\n     )\n-    from transformers.pytorch_utils import is_torch_greater_or_equal_than_1_12\n-else:\n-    is_torch_greater_or_equal_than_1_12 = False\n \n \n class TapasModelTester:\n@@ -411,7 +408,6 @@ def prepare_config_and_inputs_for_common(self):\n         return config, inputs_dict\n \n \n-@unittest.skipIf(not is_torch_greater_or_equal_than_1_12, reason=\"Tapas is only available in torch v1.12+\")\n @require_torch\n class TapasModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (\n@@ -578,7 +574,6 @@ def prepare_tapas_batch_inputs_for_training():\n     return table, queries, answer_coordinates, answer_text, float_answer\n \n \n-@unittest.skipIf(not is_torch_greater_or_equal_than_1_12, reason=\"Tapas is only available in torch v1.12+\")\n @require_torch\n class TapasModelIntegrationTest(unittest.TestCase):\n     @cached_property\n@@ -930,10 +925,6 @@ def test_inference_classification_head(self):\n         self.assertTrue(torch.allclose(outputs.logits, expected_tensor, atol=0.05))\n \n \n-# Below: tests for Tapas utilities which are defined in modeling_tapas.py.\n-# These are based on segmented_tensor_test.py of the original implementation.\n-# URL: https://github.com/google-research/tapas/blob/master/tapas/models/segmented_tensor_test.py\n-@unittest.skipIf(not is_torch_greater_or_equal_than_1_12, reason=\"Tapas is only available in torch v1.12+\")\n @require_torch\n class TapasUtilitiesTest(unittest.TestCase):\n     def _prepare_tables(self):"
        },
        {
            "sha": "9a3a2578fd16b38aa7ad2064864815e6f67b2e32",
            "filename": "tests/models/tapas/test_tokenization_tapas.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Ftapas%2Ftest_tokenization_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Ftapas%2Ftest_tokenization_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftapas%2Ftest_tokenization_tapas.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -23,7 +23,7 @@\n import pandas as pd\n from parameterized import parameterized\n \n-from transformers import AddedToken, is_torch_available\n+from transformers import AddedToken\n from transformers.models.tapas.tokenization_tapas import (\n     VOCAB_FILES_NAMES,\n     BasicTokenizer,\n@@ -45,12 +45,6 @@\n from ...test_tokenization_common import TokenizerTesterMixin, filter_non_english, merge_model_tokenizer_mappings\n \n \n-if is_torch_available():\n-    from transformers.pytorch_utils import is_torch_greater_or_equal_than_1_12\n-else:\n-    is_torch_greater_or_equal_than_1_12 = False\n-\n-\n @require_tokenizers\n @require_pandas\n class TapasTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n@@ -1048,7 +1042,6 @@ def test_token_type_ids(self):\n                 # Do the same test as modeling common.\n                 self.assertIn(0, output[\"token_type_ids\"][0])\n \n-    @unittest.skipIf(not is_torch_greater_or_equal_than_1_12, reason=\"Tapas is only available in torch v1.12+\")\n     @require_torch\n     @slow\n     def test_torch_encode_plus_sent_to_model(self):"
        },
        {
            "sha": "8286b3c94fb9da3ba0dda4f57bf280d4eec5e568",
            "filename": "tests/models/vipllava/test_modeling_vipllava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -41,8 +41,6 @@\n \n if is_torch_available():\n     import torch\n-else:\n-    is_torch_greater_or_equal_than_2_0 = False\n \n if is_vision_available():\n     from PIL import Image"
        },
        {
            "sha": "e2141dc7cc2f66810d7ec8456a6618c025d221c2",
            "filename": "tests/pipelines/test_pipelines_table_question_answering.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fpipelines%2Ftest_pipelines_table_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05de764e9ccaadd8baf4562f607777f821b48dae/tests%2Fpipelines%2Ftest_pipelines_table_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_table_question_answering.py?ref=05de764e9ccaadd8baf4562f607777f821b48dae",
            "patch": "@@ -20,7 +20,6 @@\n     AutoTokenizer,\n     TableQuestionAnsweringPipeline,\n     TFAutoModelForTableQuestionAnswering,\n-    is_torch_available,\n     pipeline,\n )\n from transformers.testing_utils import (\n@@ -33,12 +32,6 @@\n )\n \n \n-if is_torch_available():\n-    from transformers.pytorch_utils import is_torch_greater_or_equal_than_1_12\n-else:\n-    is_torch_greater_or_equal_than_1_12 = False\n-\n-\n @is_pipeline_test\n class TQAPipelineTests(unittest.TestCase):\n     # Putting it there for consistency, but TQA do not have fast tokenizer\n@@ -150,7 +143,6 @@ def test_small_model_tf(self):\n                 },\n             )\n \n-    @unittest.skipIf(not is_torch_greater_or_equal_than_1_12, reason=\"Tapas is only available in torch v1.12+\")\n     @require_torch\n     def test_small_model_pt(self, torch_dtype=\"float32\"):\n         model_id = \"lysandre/tiny-tapas-random-wtq\"\n@@ -253,12 +245,10 @@ def test_small_model_pt(self, torch_dtype=\"float32\"):\n                 },\n             )\n \n-    @unittest.skipIf(not is_torch_greater_or_equal_than_1_12, reason=\"Tapas is only available in torch v1.12+\")\n     @require_torch\n     def test_small_model_pt_fp16(self):\n         self.test_small_model_pt(torch_dtype=\"float16\")\n \n-    @unittest.skipIf(not is_torch_greater_or_equal_than_1_12, reason=\"Tapas is only available in torch v1.12+\")\n     @require_torch\n     def test_slow_tokenizer_sqa_pt(self, torch_dtype=\"float32\"):\n         model_id = \"lysandre/tiny-tapas-random-sqa\"\n@@ -378,7 +368,6 @@ def test_slow_tokenizer_sqa_pt(self, torch_dtype=\"float32\"):\n                 },\n             )\n \n-    @unittest.skipIf(not is_torch_greater_or_equal_than_1_12, reason=\"Tapas is only available in torch v1.12+\")\n     @require_torch\n     def test_slow_tokenizer_sqa_pt_fp16(self):\n         self.test_slow_tokenizer_sqa_pt(torch_dtype=\"float16\")\n@@ -505,7 +494,6 @@ def test_slow_tokenizer_sqa_tf(self):\n                 },\n             )\n \n-    @unittest.skipIf(not is_torch_greater_or_equal_than_1_12, reason=\"Tapas is only available in torch v1.12+\")\n     @slow\n     @require_torch\n     def test_integration_wtq_pt(self, torch_dtype=\"float32\"):\n@@ -551,7 +539,6 @@ def test_integration_wtq_pt(self, torch_dtype=\"float32\"):\n         ]\n         self.assertListEqual(results, expected_results)\n \n-    @unittest.skipIf(not is_torch_greater_or_equal_than_1_12, reason=\"Tapas is only available in torch v1.12+\")\n     @slow\n     @require_torch\n     def test_integration_wtq_pt_fp16(self):\n@@ -606,7 +593,6 @@ def test_integration_wtq_tf(self):\n         ]\n         self.assertListEqual(results, expected_results)\n \n-    @unittest.skipIf(not is_torch_greater_or_equal_than_1_12, reason=\"Tapas is only available in torch v1.12+\")\n     @slow\n     @require_torch\n     def test_integration_sqa_pt(self, torch_dtype=\"float32\"):\n@@ -632,7 +618,6 @@ def test_integration_sqa_pt(self, torch_dtype=\"float32\"):\n         ]\n         self.assertListEqual(results, expected_results)\n \n-    @unittest.skipIf(not is_torch_greater_or_equal_than_1_12, reason=\"Tapas is only available in torch v1.12+\")\n     @slow\n     @require_torch\n     def test_integration_sqa_pt_fp16(self):"
        }
    ],
    "stats": {
        "total": 265,
        "additions": 37,
        "deletions": 228
    }
}