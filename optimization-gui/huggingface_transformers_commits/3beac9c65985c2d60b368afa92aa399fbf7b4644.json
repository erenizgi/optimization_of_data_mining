{
    "author": "Cyrilvallez",
    "message": "Fix quite a lot of FA tests (#40548)\n\n* fix_rope_change\n\n* fix\n\n* do it dynamically\n\n* style\n\n* simplify a lot\n\n* better fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* style\n\n* fix",
    "sha": "3beac9c65985c2d60b368afa92aa399fbf7b4644",
    "files": [
        {
            "sha": "fd31ff7a3c40fa9c1e1a24af8eb7f8fc8e6e0e04",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -387,7 +387,6 @@ def __init__(self, config):\n         self.drop = nn.Dropout(config.dropout)\n \n         self.layers = nn.ModuleList([BarkBlock(config, is_causal=True, layer_idx=i) for i in range(config.num_layers)])\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n         self.layernorm_final = nn.LayerNorm(config.hidden_size, bias=config.bias)\n \n@@ -520,7 +519,7 @@ def forward(\n         if attention_mask is not None:\n             if batch_size <= 0:\n                 raise ValueError(\"batch_size has to be defined and > 0\")\n-            if self._use_flash_attention_2:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             else:\n                 attention_mask = attention_mask.view(batch_size, -1)\n@@ -943,7 +942,6 @@ def __init__(self, config):\n         self.layers = nn.ModuleList(\n             [BarkBlock(config, is_causal=False, layer_idx=i) for i in range(config.num_layers)]\n         )\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n         self.layernorm_final = nn.LayerNorm(config.hidden_size)\n \n@@ -1140,7 +1138,7 @@ def forward(\n         if attention_mask is not None:\n             if batch_size <= 0:\n                 raise ValueError(\"batch_size has to be defined and > 0\")\n-            if self._use_flash_attention_2:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             else:\n                 # [bsz, to_seq_length] -> [bsz, 1, 1, to_seq_length]"
        },
        {
            "sha": "e20c006f89f181a5d285a41bc7b9068b357844ee",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -587,9 +587,6 @@ def __init__(self, config: CLIPTextConfig):\n         # For `pooled_output` computation\n         self.eos_token_id = config.eos_token_id\n \n-        # For attention mask, it differs between `flash_attention_2` and other attention implementations\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -619,7 +616,7 @@ def forward(\n         )\n \n         # expand attention_mask\n-        if attention_mask is not None and not self._use_flash_attention_2:\n+        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n             # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n             attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n \n@@ -1039,6 +1036,7 @@ def forward(\n class CLIPTextModelWithProjection(CLIPPreTrainedModel):\n     config: CLIPTextConfig\n \n+    _supports_flash_attn = False\n     _no_split_modules = [\"CLIPTextEmbeddings\", \"CLIPEncoderLayer\"]\n \n     def __init__(self, config: CLIPTextConfig):"
        },
        {
            "sha": "bc116b231af102fead8ed826e42c30c900c59524",
            "filename": "src/transformers/models/distilbert/modeling_distilbert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -596,8 +596,6 @@ def __init__(self, config: PretrainedConfig):\n \n         self.embeddings = Embeddings(config)  # Embeddings\n         self.transformer = Transformer(config)  # Encoder\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n-        self._use_sdpa = config._attn_implementation == \"sdpa\"\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -712,13 +710,13 @@ def forward(\n \n         embeddings = self.embeddings(input_ids, inputs_embeds)  # (bs, seq_length, dim)\n \n-        if self._use_flash_attention_2:\n+        if self.config._attn_implementation == \"flash_attention_2\":\n             attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n         else:\n             if attention_mask is None:\n                 attention_mask = torch.ones(input_shape, device=device)  # (bs, seq_length)\n \n-            if self._use_sdpa and head_mask_is_none and not output_attentions:\n+            if self.config._attn_implementation == \"sdpa\" and head_mask_is_none and not output_attentions:\n                 attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     attention_mask, embeddings.dtype, tgt_len=input_shape[1]\n                 )"
        },
        {
            "sha": "3e979040388dfa2453c4158497e29049af302b88",
            "filename": "src/transformers/models/eomt/modeling_eomt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -994,7 +994,6 @@ class EomtPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = False\n     _no_split_modules = [\"EomtLayer\"]\n     _supports_sdpa = True\n-    _supports_flash_attn = True\n     _can_record_outputs = {\n         \"hidden_states\": EomtLayer,\n         \"attentions\": EomtAttention,"
        },
        {
            "sha": "17fb96ac60aae7d57b510d7e19830d8d1cc0ed9f",
            "filename": "src/transformers/models/eomt/modular_eomt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -395,7 +395,6 @@ class EomtPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = False\n     _no_split_modules = [\"EomtLayer\"]\n     _supports_sdpa = True\n-    _supports_flash_attn = True\n     _can_record_outputs = {\n         \"hidden_states\": EomtLayer,\n         \"attentions\": EomtAttention,"
        },
        {
            "sha": "2489efaf2e95d77e7160b59f1e564dd57f127b20",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -197,7 +197,6 @@ def __init__(self, config: FalconConfig, layer_idx=None):\n         self.max_position_embeddings = config.max_position_embeddings\n         self.rope_theta = config.rope_theta\n         self.is_causal = True\n-        self._use_sdpa = config._attn_implementation == \"sdpa\"\n         self.layer_idx = layer_idx\n         if layer_idx is None:\n             logger.warning_once(\n@@ -320,7 +319,11 @@ def forward(\n             key_layer, value_layer = layer_past.update(key_layer, value_layer, self.layer_idx, cache_kwargs)\n \n         kv_length = key_layer.shape[-2]\n-        if self._use_sdpa and query_layer.device.type == \"cuda\" and attention_mask is not None:\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and query_layer.device.type == \"cuda\"\n+            and attention_mask is not None\n+        ):\n             # For torch<=2.1.2, SDPA with memory-efficient backend is bugged with non-contiguous inputs with custom attn_mask,\n             # Reference: https://github.com/pytorch/pytorch/issues/112577.\n             query_layer = query_layer.contiguous()\n@@ -331,7 +334,7 @@ def forward(\n             attention_mask = attention_mask[:, :, :, : key_layer.shape[-2]]\n \n         if alibi is None:\n-            if self._use_sdpa and not output_attentions:\n+            if self.config._attn_implementation == \"sdpa\" and not output_attentions:\n                 # We dispatch to SDPA's Flash Attention or Efficient kernels via this if statement instead of an\n                 # inline conditional assignment to support both torch.compile's `dynamic=True` and `fullgraph=True`\n                 # The query_length > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not\n@@ -363,7 +366,7 @@ def forward(\n             return attn_output, attention_scores\n \n         else:\n-            if self._use_sdpa and not output_attentions and head_mask is None:\n+            if self.config._attn_implementation == \"sdpa\" and not output_attentions and head_mask is None:\n                 # We dispatch to SDPA's Flash Attention or Efficient kernels via this if statement instead of an\n                 # inline conditional assignment to support both torch.compile's `dynamic=True` and `fullgraph=True`\n                 is_causal = self.is_causal and attention_mask is None and query_length > 1\n@@ -692,8 +695,6 @@ def __init__(self, config: FalconConfig):\n \n         # Transformer blocks\n         self.h = nn.ModuleList([FalconDecoderLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n-        self._use_sdpa = config._attn_implementation == \"sdpa\"\n \n         # Final Layer Norm\n         self.ln_f = LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)"
        },
        {
            "sha": "eea96798e4cd798c283fe709c1fc0afb984b91ae",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -419,9 +419,6 @@ def __init__(self, config):\n \n         self.gradient_checkpointing = False\n \n-        self._use_sdpa = config._attn_implementation == \"sdpa\"\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -518,7 +515,7 @@ def forward(\n             past_key_values=past_key_values,\n         )\n \n-        if self._use_flash_attention_2:\n+        if self.config._attn_implementation == \"flash_attention_2\":\n             encoder_attention_mask = (\n                 encoder_attention_mask.bool()\n                 if (encoder_attention_mask is not None and 0 in encoder_attention_mask)"
        },
        {
            "sha": "88c66089aead3a993adf1e52f056ef7cc601a6ff",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -564,8 +564,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n-\n     @add_start_docstrings(PARALLELIZE_DOCSTRING)\n     def parallelize(self, device_map=None):\n         warnings.warn("
        },
        {
            "sha": "ec3d8b64cb925169309f9fce8dfab65386d72241",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -512,7 +512,6 @@ def __init__(self, config: Idefics2VisionConfig):\n         self.embeddings = Idefics2VisionEmbeddings(config)\n         self.encoder = Idefics2Encoder(config)\n         self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n     def get_input_embeddings(self):\n         return self.embeddings\n@@ -559,7 +558,7 @@ def forward(\n         # avoiding passing the attention_mask, which is equivalent to attending to the full sequence\n         if not torch.any(~patch_attention_mask):\n             patch_attention_mask = None\n-        elif not self._use_flash_attention_2:\n+        elif self.config._attn_implementation != \"flash_attention_2\":\n             patch_attention_mask = _prepare_4d_attention_mask(patch_attention_mask, hidden_states.dtype)\n \n         encoder_outputs = self.encoder(\n@@ -808,8 +807,6 @@ def __init__(self, config) -> None:\n         self.layers = nn.ModuleList([Idefics2PerceiverLayer(config, idx) for idx in range(self.depth)])\n         self.norm = Idefics2RMSNorm(self.hidden_size, eps=self.rms_norm_eps)\n \n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -829,7 +826,7 @@ def forward(\n         attention_mask = torch.cat([attention_mask, latent_attention_mask], dim=-1)\n         attention_mask = (\n             _prepare_4d_attention_mask(attention_mask, latents.dtype, tgt_len=self.n_latents)\n-            if not self._use_flash_attention_2\n+            if self.config._attn_implementation != \"flash_attention_2\"\n             else attention_mask\n         )\n \n@@ -887,8 +884,6 @@ def __init__(self, config: Idefics2Config):\n         self.image_seq_len = config.perceiver_config.resampler_n_latents\n         self.image_token_id = self.config.image_token_id\n \n-        self._use_flash_attention_2 = config.text_config._attn_implementation == \"flash_attention_2\"\n-\n         self.post_init()\n \n     def enable_input_require_grads(self):"
        },
        {
            "sha": "1d0d00bb6a11712865528425ba739df1f46b3cb3",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -522,7 +522,6 @@ def __init__(self, config: Idefics3VisionConfig):\n         self.encoder = Idefics3Encoder(config)\n         self.patch_size = config.patch_size\n         self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n     # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2VisionTransformer.get_input_embeddings\n     def get_input_embeddings(self):\n@@ -564,7 +563,7 @@ def forward(\n         # The call to `_upad_input` in `_flash_attention_forward` is expensive\n         # So when the `patch_attention_mask` is full of 1s (i.e. attending to the whole sequence),\n         # avoiding passing the attention_mask, which is equivalent to attending to the full sequence\n-        if not self._use_flash_attention_2:\n+        if self.config._attn_implementation != \"flash_attention_2\":\n             patch_attention_mask = _prepare_4d_attention_mask(patch_attention_mask, hidden_states.dtype)\n         elif not torch.any(~patch_attention_mask):\n             patch_attention_mask = None\n@@ -610,8 +609,6 @@ def __init__(self, config: Idefics3Config):\n         )\n         self.image_token_id = self.config.image_token_id\n \n-        self._use_flash_attention_2 = config.text_config._attn_implementation == \"flash_attention_2\"\n-\n         self.post_init()\n \n     # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2Model.enable_input_require_grads"
        },
        {
            "sha": "8fd8254f3dfe9e1d05ba686cbae330cbc08310fb",
            "filename": "src/transformers/models/metaclip_2/modeling_metaclip_2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -491,9 +491,6 @@ def __init__(self, config: MetaClip2TextConfig):\n         # For `pooled_output` computation\n         self.eos_token_id = config.eos_token_id\n \n-        # For attention mask, it differs between `flash_attention_2` and other attention implementations\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -516,7 +513,7 @@ def forward(\n         )\n \n         # expand attention_mask\n-        if attention_mask is not None and not self._use_flash_attention_2:\n+        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n             # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n             attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n \n@@ -624,6 +621,7 @@ class MetaClip2TextModelOutput(ModelOutput):\n class MetaClip2TextModelWithProjection(MetaClip2PreTrainedModel):\n     config: MetaClip2TextConfig\n \n+    _supports_flash_attn = False\n     _no_split_modules = [\"MetaClip2TextEmbeddings\", \"MetaClip2EncoderLayer\"]\n \n     def __init__(self, config: MetaClip2TextConfig):"
        },
        {
            "sha": "f0eff11e4a9a7adf3bead09267d597e126ba0222",
            "filename": "src/transformers/models/metaclip_2/modular_metaclip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -146,7 +146,7 @@ def forward(\n         )\n \n         # expand attention_mask\n-        if attention_mask is not None and not self._use_flash_attention_2:\n+        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n             # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n             attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n "
        },
        {
            "sha": "7f0d21d2806bf50518e47b758cd2600363b364a2",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -453,7 +453,6 @@ def __init__(self, config: Siglip2VisionConfig):\n         self.use_head = True if not hasattr(config, \"vision_use_head\") else config.vision_use_head\n         if self.use_head:\n             self.head = Siglip2MultiheadAttentionPoolingHead(config)\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n     @can_return_tuple\n     @auto_docstring\n@@ -476,7 +475,7 @@ def forward(\n \n         hidden_states = self.embeddings(pixel_values, spatial_shapes)\n \n-        if attention_mask is not None and not self._use_flash_attention_2:\n+        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n             # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n             encoder_attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n         else:"
        },
        {
            "sha": "5a13b8f69efc43271642ea02236ae0822f1adcd5",
            "filename": "src/transformers/models/siglip2/modular_siglip2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -234,7 +234,6 @@ def forward(self, pixel_values: torch.FloatTensor, spatial_shapes: torch.LongTen\n class Siglip2VisionTransformer(SiglipVisionTransformer):\n     def __init__(self, config: Siglip2VisionConfig):\n         super().__init__(config)\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n     # Update: add `spatial_shapes` and `attention_mask`\n     def forward(\n@@ -256,7 +255,7 @@ def forward(\n \n         hidden_states = self.embeddings(pixel_values, spatial_shapes)\n \n-        if attention_mask is not None and not self._use_flash_attention_2:\n+        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n             # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n             encoder_attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n         else:"
        },
        {
            "sha": "d6d1239ebcba7ea15a4018184137d974ee16e571",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -411,7 +411,6 @@ def __init__(self, config: SmolVLMVisionConfig):\n         self.encoder = SmolVLMEncoder(config)\n         self.patch_size = config.patch_size\n         self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n     def get_input_embeddings(self):\n         return self.embeddings\n@@ -451,7 +450,7 @@ def forward(\n         # The call to `_upad_input` in `_flash_attention_forward` is expensive\n         # So when the `patch_attention_mask` is full of 1s (i.e. attending to the whole sequence),\n         # avoiding passing the attention_mask, which is equivalent to attending to the full sequence\n-        if not self._use_flash_attention_2:\n+        if self.config._attn_implementation != \"flash_attention_2\":\n             patch_attention_mask = _prepare_4d_attention_mask(patch_attention_mask, hidden_states.dtype)\n         elif not torch.any(~patch_attention_mask):\n             patch_attention_mask = None\n@@ -569,8 +568,6 @@ def __init__(self, config: SmolVLMConfig):\n         )\n         self.image_token_id = self.config.image_token_id\n \n-        self._use_flash_attention_2 = config.text_config._attn_implementation == \"flash_attention_2\"\n-\n         self.post_init()\n \n     def enable_input_require_grads(self):"
        },
        {
            "sha": "9167e198f2a04b75b6f2bb67cdc92932c23a4b0d",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -756,8 +756,6 @@ def __init__(self, config: WhisperConfig):\n         self.layers = nn.ModuleList(\n             [WhisperDecoderLayer(config, layer_idx) for layer_idx in range(config.decoder_layers)]\n         )\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n-        self._use_sdpa = config._attn_implementation == \"sdpa\"\n \n         self.layer_norm = nn.LayerNorm(config.d_model)\n "
        },
        {
            "sha": "541f13077d2a6acb06876ddceedb2e00c48a0607",
            "filename": "tests/models/aimv2/test_modeling_aimv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 91,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -21,13 +21,10 @@\n import numpy as np\n import requests\n from parameterized import parameterized\n-from pytest import mark\n \n from transformers import Aimv2Config, Aimv2TextConfig, Aimv2VisionConfig\n from transformers.testing_utils import (\n-    require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n     require_vision,\n     slow,\n     torch_device,\n@@ -471,94 +468,6 @@ def test_load_vision_text_config(self):\n             text_config = Aimv2TextConfig.from_pretrained(tmp_dir_name)\n             self.assertDictEqual(config.text_config.to_dict(), text_config.to_dict())\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_inference_equivalence(self):\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_fa = model_class.from_pretrained(\n-                    tmpdirname, dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n-                )\n-                model_fa.to(torch_device)\n-\n-                model = model_class.from_pretrained(tmpdirname, dtype=torch.bfloat16)\n-                model.to(torch_device)\n-\n-                dummy_pixel_values = inputs_dict[\"pixel_values\"].to(torch.bfloat16)\n-                dummy_input_ids = inputs_dict[\"input_ids\"]\n-\n-                outputs = model(pixel_values=dummy_pixel_values, input_ids=dummy_input_ids, output_hidden_states=True)\n-                outputs_fa = model_fa(\n-                    pixel_values=dummy_pixel_values, input_ids=dummy_input_ids, output_hidden_states=True\n-                )\n-\n-                self.assertTrue(\n-                    torch.allclose(outputs.logits_per_image, outputs_fa.logits_per_image, atol=4e-2, rtol=4e-2),\n-                    f\"Image logits max diff: {torch.max(torch.abs(outputs.logits_per_image - outputs_fa.logits_per_image))}\",\n-                )\n-                self.assertTrue(\n-                    torch.allclose(outputs.logits_per_text, outputs_fa.logits_per_text, atol=4e-2, rtol=4e-2),\n-                    f\"Text logits max diff: {torch.max(torch.abs(outputs.logits_per_text - outputs_fa.logits_per_text))}\",\n-                )\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_fa = model_class.from_pretrained(\n-                    tmpdirname, dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n-                )\n-                model_fa.to(torch_device)\n-\n-                model = model_class.from_pretrained(tmpdirname, dtype=torch.bfloat16, attn_implementation=\"eager\")\n-                model.to(torch_device)\n-\n-                dummy_pixel_values = inputs_dict[\"pixel_values\"].to(torch.bfloat16)\n-                dummy_input_ids = inputs_dict[\"input_ids\"]\n-                dummy_pixel_mask = inputs_dict[\"attention_mask\"]\n-\n-                # right padding\n-                dummy_pixel_mask[:] = 1\n-                dummy_pixel_mask[:, -1:] = 0\n-\n-                outputs = model(pixel_values=dummy_pixel_values, input_ids=dummy_input_ids, output_hidden_states=True)\n-                outputs_fa = model_fa(\n-                    pixel_values=dummy_pixel_values, input_ids=dummy_input_ids, output_hidden_states=True\n-                )\n-\n-                logits_per_image_eager = outputs.logits_per_image[:, :-1]\n-                logits_per_text_eager = outputs.logits_per_text[:, :-1]\n-\n-                logits_per_image_sdpa = outputs_fa.logits_per_image[:, :-1]\n-                logits_per_text_sdpa = outputs_fa.logits_per_text[:, :-1]\n-\n-                self.assertTrue(\n-                    torch.allclose(logits_per_image_eager, logits_per_image_sdpa, atol=4e-2, rtol=4e-2),\n-                    f\"Image logits max diff: {torch.max(torch.abs(logits_per_image_eager - logits_per_image_sdpa))}\",\n-                )\n-                self.assertTrue(\n-                    torch.allclose(logits_per_text_eager, logits_per_text_sdpa, atol=4e-2, rtol=4e-2),\n-                    f\"Text logits max diff: {torch.max(torch.abs(logits_per_text_eager - logits_per_text_sdpa))}\",\n-                )\n-\n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n     def test_eager_matches_sdpa_inference(\n         self,"
        },
        {
            "sha": "9ad0556e1521cce14013890a3c6a604bff3ea598",
            "filename": "tests/models/bark/test_modeling_bark.py",
            "status": "modified",
            "additions": 0,
            "deletions": 119,
            "changes": 119,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -18,8 +18,6 @@\n import tempfile\n import unittest\n \n-import pytest\n-\n from transformers import (\n     BarkCausalModel,\n     BarkCoarseConfig,\n@@ -35,11 +33,9 @@\n )\n from transformers.testing_utils import (\n     backend_torch_accelerator_module,\n-    require_flash_attn,\n     require_torch,\n     require_torch_accelerator,\n     require_torch_fp16,\n-    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -928,121 +924,6 @@ def test_resize_embeddings_untied(self):\n             # Check that the model can still do a forward pass successfully (every parameter should be resized)\n             model(**self._prepare_for_class(inputs_dict, model_class))\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_inference_equivalence(self):\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn:\n-                self.skipTest(reason=\"Model does not support flash_attention_2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_fa = model_class.from_pretrained(\n-                    tmpdirname, dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n-                )\n-                model_fa.to(torch_device)\n-\n-                model = model_class.from_pretrained(tmpdirname, dtype=torch.bfloat16)\n-                model.to(torch_device)\n-\n-                dummy_input = inputs_dict[\"input_ids\"][:1]\n-                if dummy_input.dtype in [torch.float32, torch.float16]:\n-                    dummy_input = dummy_input.to(torch.bfloat16)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n-\n-                if dummy_attention_mask is not None:\n-                    dummy_attention_mask = dummy_attention_mask[:1]\n-                    dummy_attention_mask[:, 1:] = 1\n-                    dummy_attention_mask[:, :1] = 0\n-\n-                outputs = model(inputs_dict[\"codebook_idx\"], dummy_input, output_hidden_states=True)\n-                outputs_fa = model_fa(inputs_dict[\"codebook_idx\"], dummy_input, output_hidden_states=True)\n-\n-                logits = outputs.hidden_states[-1]\n-                logits_fa = outputs_fa.hidden_states[-1]\n-\n-                assert torch.allclose(logits_fa, logits, atol=4e-2, rtol=4e-2)\n-\n-                other_inputs = {\"output_hidden_states\": True}\n-                if dummy_attention_mask is not None:\n-                    other_inputs[\"attention_mask\"] = dummy_attention_mask\n-\n-                outputs = model(inputs_dict[\"codebook_idx\"], dummy_input, **other_inputs)\n-                outputs_fa = model_fa(inputs_dict[\"codebook_idx\"], dummy_input, **other_inputs)\n-\n-                logits = outputs.hidden_states[-1]\n-                logits_fa = outputs_fa.hidden_states[-1]\n-\n-                assert torch.allclose(logits_fa[1:], logits[1:], atol=4e-2, rtol=4e-2)\n-\n-                # check with inference + dropout\n-                model.train()\n-                _ = model_fa(inputs_dict[\"codebook_idx\"], dummy_input, **other_inputs)\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn:\n-                self.skipTest(reason=\"Model does not support flash_attention_2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_fa = model_class.from_pretrained(\n-                    tmpdirname, dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n-                )\n-                model_fa.to(torch_device)\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    dtype=torch.bfloat16,\n-                )\n-                model.to(torch_device)\n-\n-                dummy_input = inputs_dict[\"input_ids\"][:1]\n-                if dummy_input.dtype in [torch.float32, torch.float16]:\n-                    dummy_input = dummy_input.to(torch.bfloat16)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n-\n-                if dummy_attention_mask is not None:\n-                    dummy_attention_mask = dummy_attention_mask[:1]\n-                    dummy_attention_mask[:, :-1] = 1\n-                    dummy_attention_mask[:, -1:] = 0\n-\n-                outputs = model(inputs_dict[\"codebook_idx\"], dummy_input, output_hidden_states=True)\n-                outputs_fa = model_fa(inputs_dict[\"codebook_idx\"], dummy_input, output_hidden_states=True)\n-\n-                logits = outputs.hidden_states[-1]\n-                logits_fa = outputs_fa.hidden_states[-1]\n-\n-                assert torch.allclose(logits_fa, logits, atol=4e-2, rtol=4e-2)\n-\n-                other_inputs = {\n-                    \"output_hidden_states\": True,\n-                }\n-                if dummy_attention_mask is not None:\n-                    other_inputs[\"attention_mask\"] = dummy_attention_mask\n-\n-                outputs = model(inputs_dict[\"codebook_idx\"], dummy_input, **other_inputs)\n-                outputs_fa = model_fa(inputs_dict[\"codebook_idx\"], dummy_input, **other_inputs)\n-\n-                logits = outputs.hidden_states[-1]\n-                logits_fa = outputs_fa.hidden_states[-1]\n-\n-                assert torch.allclose(logits_fa[:-1], logits[:-1], atol=4e-2, rtol=4e-2)\n-\n \n @require_torch\n class BarkModelIntegrationTests(unittest.TestCase):"
        },
        {
            "sha": "75d885ba4d51c1138538cb8ea017b7b4438ef4c5",
            "filename": "tests/models/bitnet/test_modeling_bitnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fbitnet%2Ftest_modeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fbitnet%2Ftest_modeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbitnet%2Ftest_modeling_bitnet.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -16,14 +16,10 @@\n import gc\n import unittest\n \n-import pytest\n-\n from transformers import AutoTokenizer, BitNetConfig, is_torch_available\n from transformers.testing_utils import (\n     backend_empty_cache,\n-    require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -179,20 +175,6 @@ def test_model_various_embeddings(self):\n             config_and_inputs[0].position_embedding_type = type\n             self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_torch_fx_output_loss(self):\n-        super().test_torch_fx_output_loss()\n-\n-    # Ignore copy\n-    def test_past_key_values_format(self):\n-        super().test_past_key_values_format()\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        self.skipTest(reason=\"BitNet flash attention does not support right padding\")\n-\n \n @require_torch\n class BitNetIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "b352b816046887f2a995434d17298f78d30113f4",
            "filename": "tests/models/clip/test_modeling_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 91,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -22,13 +22,10 @@\n import pytest\n import requests\n from parameterized import parameterized\n-from pytest import mark\n \n from transformers import CLIPConfig, CLIPTextConfig, CLIPVisionConfig\n from transformers.testing_utils import (\n-    require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n     require_vision,\n     slow,\n     torch_device,\n@@ -701,94 +698,6 @@ def test_sdpa_can_dispatch_on_flash(self):\n     def test_sdpa_can_compile_dynamic(self):\n         self.skipTest(reason=\"CLIP model can't be compiled dynamic, error in clip_loss`\")\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_inference_equivalence(self):\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_fa = model_class.from_pretrained(\n-                    tmpdirname, dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n-                )\n-                model_fa.to(torch_device)\n-\n-                model = model_class.from_pretrained(tmpdirname, dtype=torch.bfloat16)\n-                model.to(torch_device)\n-\n-                dummy_pixel_values = inputs_dict[\"pixel_values\"].to(torch.bfloat16)\n-                dummy_input_ids = inputs_dict[\"input_ids\"]\n-\n-                outputs = model(pixel_values=dummy_pixel_values, input_ids=dummy_input_ids, output_hidden_states=True)\n-                outputs_fa = model_fa(\n-                    pixel_values=dummy_pixel_values, input_ids=dummy_input_ids, output_hidden_states=True\n-                )\n-\n-                self.assertTrue(\n-                    torch.allclose(outputs.logits_per_image, outputs_fa.logits_per_image, atol=4e-2, rtol=4e-2),\n-                    f\"Image logits max diff: {torch.max(torch.abs(outputs.logits_per_image - outputs_fa.logits_per_image))}\",\n-                )\n-                self.assertTrue(\n-                    torch.allclose(outputs.logits_per_text, outputs_fa.logits_per_text, atol=4e-2, rtol=4e-2),\n-                    f\"Text logits max diff: {torch.max(torch.abs(outputs.logits_per_text - outputs_fa.logits_per_text))}\",\n-                )\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_fa = model_class.from_pretrained(\n-                    tmpdirname, dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n-                )\n-                model_fa.to(torch_device)\n-\n-                model = model_class.from_pretrained(tmpdirname, dtype=torch.bfloat16, attn_implementation=\"eager\")\n-                model.to(torch_device)\n-\n-                dummy_pixel_values = inputs_dict[\"pixel_values\"].to(torch.bfloat16)\n-                dummy_input_ids = inputs_dict[\"input_ids\"]\n-                dummy_pixel_mask = inputs_dict[\"attention_mask\"]\n-\n-                # right padding\n-                dummy_pixel_mask[:] = 1\n-                dummy_pixel_mask[:, -1:] = 0\n-\n-                outputs = model(pixel_values=dummy_pixel_values, input_ids=dummy_input_ids, output_hidden_states=True)\n-                outputs_fa = model_fa(\n-                    pixel_values=dummy_pixel_values, input_ids=dummy_input_ids, output_hidden_states=True\n-                )\n-\n-                logits_per_image_eager = outputs.logits_per_image[:, :-1]\n-                logits_per_text_eager = outputs.logits_per_text[:, :-1]\n-\n-                logits_per_image_sdpa = outputs_fa.logits_per_image[:, :-1]\n-                logits_per_text_sdpa = outputs_fa.logits_per_text[:, :-1]\n-\n-                self.assertTrue(\n-                    torch.allclose(logits_per_image_eager, logits_per_image_sdpa, atol=4e-2, rtol=4e-2),\n-                    f\"Image logits max diff: {torch.max(torch.abs(logits_per_image_eager - logits_per_image_sdpa))}\",\n-                )\n-                self.assertTrue(\n-                    torch.allclose(logits_per_text_eager, logits_per_text_sdpa, atol=4e-2, rtol=4e-2),\n-                    f\"Text logits max diff: {torch.max(torch.abs(logits_per_text_eager - logits_per_text_sdpa))}\",\n-                )\n-\n \n class CLIPForImageClassificationModelTester(CLIPModelTester):\n     def __init__(self, parent):"
        },
        {
            "sha": "78707e6518ff441f9ab83c5fb6a3599f19acdaf5",
            "filename": "tests/models/dots1/test_modeling_dots1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fdots1%2Ftest_modeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fdots1%2Ftest_modeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdots1%2Ftest_modeling_dots1.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -16,16 +16,12 @@\n import gc\n import unittest\n \n-import pytest\n-\n from transformers import AutoTokenizer, Dots1Config, is_torch_available\n from transformers.testing_utils import (\n     backend_empty_cache,\n     cleanup,\n-    require_flash_attn,\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -87,13 +83,6 @@ class Dots1ModelTest(CausalLMModelTest, unittest.TestCase):\n     test_pruning = False\n     model_tester_class = Dots1ModelTester\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        self.skipTest(reason=\"dots.llm1 flash attention does not support right padding\")\n-\n \n @require_torch_accelerator\n class Dots1IntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "06e4c0031f78710c8c1379b880947795a45ad424",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -30,7 +30,6 @@\n     require_read_token,\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -95,13 +94,6 @@ def is_pipeline_test_to_skip(\n     ):\n         return True\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        self.skipTest(reason=\"Gemma flash attention does not support right padding\")\n-\n \n @slow\n @require_torch_accelerator"
        },
        {
            "sha": "d6594e989f3dae2ae981d6b0978d49fd1e3c727f",
            "filename": "tests/models/gemma3n/test_modeling_gemma3n.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -39,7 +39,6 @@\n )\n from transformers.testing_utils import (\n     cleanup,\n-    require_flash_attn,\n     require_read_token,\n     require_torch,\n     require_torch_gpu,\n@@ -377,13 +376,6 @@ def _check_hidden_states_for_generate(\n                 [expected_shape] * len(iter_hidden_states),\n             )\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        self.skipTest(reason=\"Gemma3n flash attention does not support right padding\")\n-\n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n     def test_eager_matches_sdpa_inference(\n         self,"
        },
        {
            "sha": "12a20267514d6b1681a6a1585128cbd3cea666a4",
            "filename": "tests/models/internvl/test_modeling_internvl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -231,10 +231,6 @@ def test_sdpa_can_compile_dynamic(self):\n     def test_flash_attn_2_fp32_ln(self):\n         pass\n \n-    @unittest.skip(\"Qwen2 flash attention does not support right padding\")\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        pass\n-\n \n @slow\n @require_torch_accelerator"
        },
        {
            "sha": "8b55233066b05f0a3bbe26029af8b65b2383424c",
            "filename": "tests/models/jamba/test_modeling_jamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -560,17 +560,6 @@ def test_flash_attn_2_fp32_ln(self):\n                 # with attention mask\n                 _ = model(dummy_input, attention_mask=dummy_attention_mask)\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        r\"\"\"\n-        Overriding the test_flash_attn_2_inference_padding_right test as the Jamba model, like Mixtral, doesn't support\n-        right padding + use cache with FA2\n-        \"\"\"\n-        self.skipTest(reason=\"Jamba flash attention does not support right padding\")\n-\n \n @require_torch\n class JambaModelIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "5d000f1634d6a37d7b5af772bacee67d52a5a623",
            "filename": "tests/models/kosmos2_5/test_modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -529,26 +529,9 @@ def test_model_from_pretrained(self):\n \n     @unittest.skip(reason=\"Does not work on the tiny model as we keep hitting edge cases.\")\n     def test_model_parallelism(self):\n-        super().test_model_parallelism()\n-\n-    # TODO: ydshieh\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    @unittest.skip(reason=\"kosmos-2.5 flash attention does not support right padding\")\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        pass\n-\n-    # TODO: ydshieh\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    @unittest.skip(reason=\"kosmos-2.5 test : the dummy inputs should be tweaked: dummy_input = inputs_dict\")\n-    def test_flash_attn_2_inference_equivalence(self):\n         pass\n \n     # TODO: ydshieh\n-\n     @require_torch_gpu\n     @slow\n     @unittest.skip(reason=\"_update_causal_mask is not implemented yet which fails this test\")"
        },
        {
            "sha": "f8ad7701eab302d5ec227ff9d1ba57c302ae9312",
            "filename": "tests/models/metaclip_2/test_modeling_metaclip_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 91,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fmetaclip_2%2Ftest_modeling_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fmetaclip_2%2Ftest_modeling_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmetaclip_2%2Ftest_modeling_metaclip_2.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -21,13 +21,10 @@\n import numpy as np\n import requests\n from parameterized import parameterized\n-from pytest import mark\n \n from transformers import MetaClip2Config, MetaClip2TextConfig, MetaClip2VisionConfig\n from transformers.testing_utils import (\n-    require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n     require_vision,\n     slow,\n     torch_device,\n@@ -712,94 +709,6 @@ def test_sdpa_can_dispatch_on_flash(self):\n     def test_sdpa_can_compile_dynamic(self):\n         self.skipTest(reason=\"MetaClip2 model can't be compiled dynamic, error in metaclip_2_loss`\")\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_inference_equivalence(self):\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_fa = model_class.from_pretrained(\n-                    tmpdirname, dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n-                )\n-                model_fa.to(torch_device)\n-\n-                model = model_class.from_pretrained(tmpdirname, dtype=torch.bfloat16)\n-                model.to(torch_device)\n-\n-                dummy_pixel_values = inputs_dict[\"pixel_values\"].to(torch.bfloat16)\n-                dummy_input_ids = inputs_dict[\"input_ids\"]\n-\n-                outputs = model(pixel_values=dummy_pixel_values, input_ids=dummy_input_ids, output_hidden_states=True)\n-                outputs_fa = model_fa(\n-                    pixel_values=dummy_pixel_values, input_ids=dummy_input_ids, output_hidden_states=True\n-                )\n-\n-                self.assertTrue(\n-                    torch.allclose(outputs.logits_per_image, outputs_fa.logits_per_image, atol=4e-2, rtol=4e-2),\n-                    f\"Image logits max diff: {torch.max(torch.abs(outputs.logits_per_image - outputs_fa.logits_per_image))}\",\n-                )\n-                self.assertTrue(\n-                    torch.allclose(outputs.logits_per_text, outputs_fa.logits_per_text, atol=4e-2, rtol=4e-2),\n-                    f\"Text logits max diff: {torch.max(torch.abs(outputs.logits_per_text - outputs_fa.logits_per_text))}\",\n-                )\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_fa = model_class.from_pretrained(\n-                    tmpdirname, dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n-                )\n-                model_fa.to(torch_device)\n-\n-                model = model_class.from_pretrained(tmpdirname, dtype=torch.bfloat16, attn_implementation=\"eager\")\n-                model.to(torch_device)\n-\n-                dummy_pixel_values = inputs_dict[\"pixel_values\"].to(torch.bfloat16)\n-                dummy_input_ids = inputs_dict[\"input_ids\"]\n-                dummy_pixel_mask = inputs_dict[\"attention_mask\"]\n-\n-                # right padding\n-                dummy_pixel_mask[:] = 1\n-                dummy_pixel_mask[:, -1:] = 0\n-\n-                outputs = model(pixel_values=dummy_pixel_values, input_ids=dummy_input_ids, output_hidden_states=True)\n-                outputs_fa = model_fa(\n-                    pixel_values=dummy_pixel_values, input_ids=dummy_input_ids, output_hidden_states=True\n-                )\n-\n-                logits_per_image_eager = outputs.logits_per_image[:, :-1]\n-                logits_per_text_eager = outputs.logits_per_text[:, :-1]\n-\n-                logits_per_image_sdpa = outputs_fa.logits_per_image[:, :-1]\n-                logits_per_text_sdpa = outputs_fa.logits_per_text[:, :-1]\n-\n-                self.assertTrue(\n-                    torch.allclose(logits_per_image_eager, logits_per_image_sdpa, atol=4e-2, rtol=4e-2),\n-                    f\"Image logits max diff: {torch.max(torch.abs(logits_per_image_eager - logits_per_image_sdpa))}\",\n-                )\n-                self.assertTrue(\n-                    torch.allclose(logits_per_text_eager, logits_per_text_sdpa, atol=4e-2, rtol=4e-2),\n-                    f\"Text logits max diff: {torch.max(torch.abs(logits_per_text_eager - logits_per_text_sdpa))}\",\n-                )\n-\n \n class MetaClip2ForImageClassificationModelTester(MetaClip2ModelTester):\n     def __init__(self, parent):"
        },
        {
            "sha": "4985d9ac1a2f03fa8a1d87f1f1dc14fec7c3d118",
            "filename": "tests/models/minimax/test_modeling_minimax.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -21,10 +21,8 @@\n from transformers.cache_utils import Cache\n from transformers.testing_utils import (\n     Expectations,\n-    require_flash_attn,\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -101,13 +99,6 @@ def is_pipeline_test_to_skip(\n     ):\n         return True\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        self.skipTest(reason=\"MiniMax flash attention does not support right padding\")\n-\n     def test_load_balancing_loss(self):\n         r\"\"\"\n         Let's make sure we can actually compute the loss and do a backward on it."
        },
        {
            "sha": "0248aa2240f820834421c53165e641c16c15a84e",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -33,7 +33,6 @@\n     require_read_token,\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -104,13 +103,6 @@ def is_pipeline_test_to_skip(\n     ):\n         return True\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        self.skipTest(reason=\"Mistral flash attention does not support right padding\")\n-\n \n @require_torch_accelerator\n @require_read_token"
        },
        {
            "sha": "33c5dc191c8ba38a8590e3089f376ecc9d7ba76a",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 171,
            "changes": 171,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -937,93 +937,6 @@ def test_greedy_generate_stereo_outputs(self):\n         super().test_greedy_generate_dict_outputs()\n         self.model_tester.audio_channels = original_audio_channels\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    # Adapted from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_inference_equivalence\n-    def test_flash_attn_2_inference_equivalence(self):\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_fa = model_class.from_pretrained(\n-                    tmpdirname,\n-                    dtype=torch.bfloat16,\n-                    attn_implementation={\"decoder\": \"flash_attention_2\", \"audio_encoder\": None, \"text_encoder\": None},\n-                )\n-                model_fa.to(torch_device)\n-\n-                model = model_class.from_pretrained(tmpdirname, dtype=torch.bfloat16)\n-                model.to(torch_device)\n-\n-                # Ignore copy\n-                dummy_input = inputs_dict[model.main_input_name]\n-                if dummy_input.dtype in [torch.float32, torch.float16]:\n-                    dummy_input = dummy_input.to(torch.bfloat16)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n-\n-                if dummy_attention_mask is not None:\n-                    # Ignore copy\n-                    dummy_attention_mask[:, 1:] = 1\n-                    dummy_attention_mask[:, :1] = 0\n-\n-                # Ignore copy\n-                decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", dummy_input)\n-                # Ignore copy\n-                outputs = model(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n-                # Ignore copy\n-                outputs_fa = model_fa(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n-\n-                logits = (\n-                    outputs.hidden_states[-1]\n-                    if not model.config.is_encoder_decoder\n-                    else outputs.decoder_hidden_states[-1]\n-                )\n-                logits_fa = (\n-                    outputs_fa.hidden_states[-1]\n-                    if not model.config.is_encoder_decoder\n-                    else outputs_fa.decoder_hidden_states[-1]\n-                )\n-\n-                assert torch.allclose(logits_fa, logits, atol=4e-2, rtol=4e-2)\n-                # Ignore copy\n-                other_inputs = {\n-                    \"decoder_input_ids\": decoder_input_ids,\n-                    \"decoder_attention_mask\": dummy_attention_mask,\n-                    \"output_hidden_states\": True,\n-                }\n-                # Ignore copy\n-                if dummy_attention_mask is not None:\n-                    other_inputs[\"attention_mask\"] = dummy_attention_mask\n-                # Ignore copy\n-                outputs = model(dummy_input, **other_inputs)\n-                # Ignore copy\n-                outputs_fa = model_fa(dummy_input, **other_inputs)\n-\n-                logits = (\n-                    outputs.hidden_states[-1]\n-                    if not model.config.is_encoder_decoder\n-                    else outputs.decoder_hidden_states[-1]\n-                )\n-                logits_fa = (\n-                    outputs_fa.hidden_states[-1]\n-                    if not model.config.is_encoder_decoder\n-                    else outputs_fa.decoder_hidden_states[-1]\n-                )\n-\n-                assert torch.allclose(logits_fa[1:], logits[1:], atol=4e-2, rtol=4e-2)\n-\n-                # check with inference + dropout\n-                model.train()\n-                _ = model_fa(dummy_input, **other_inputs)\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @mark.flash_attn_test\n@@ -1084,90 +997,6 @@ def test_sdpa_can_dispatch_on_flash(self):\n                 with sdpa_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n                     _ = model(**inputs_dict)\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    # Adapted from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_inference_equivalence_right_padding\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_fa = model_class.from_pretrained(\n-                    tmpdirname,\n-                    dtype=torch.bfloat16,\n-                    attn_implementation={\"decoder\": \"flash_attention_2\", \"audio_encoder\": None, \"text_encoder\": None},\n-                )\n-                model_fa.to(torch_device)\n-\n-                model = model_class.from_pretrained(tmpdirname, dtype=torch.bfloat16)\n-                model.to(torch_device)\n-\n-                # Ignore copy\n-                dummy_input = inputs_dict[model.main_input_name]\n-                if dummy_input.dtype in [torch.float32, torch.float16]:\n-                    dummy_input = dummy_input.to(torch.bfloat16)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n-\n-                if dummy_attention_mask is not None:\n-                    # Ignore copy\n-                    dummy_attention_mask[:, :-1] = 1\n-                    dummy_attention_mask[:, -1:] = 0\n-\n-                # Ignore copy\n-                decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", dummy_input)\n-                # Ignore copy\n-                outputs = model(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n-                # Ignore copy\n-                outputs_fa = model_fa(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n-\n-                logits = (\n-                    outputs.hidden_states[-1]\n-                    if not model.config.is_encoder_decoder\n-                    else outputs.decoder_hidden_states[-1]\n-                )\n-                logits_fa = (\n-                    outputs_fa.hidden_states[-1]\n-                    if not model.config.is_encoder_decoder\n-                    else outputs_fa.decoder_hidden_states[-1]\n-                )\n-\n-                assert torch.allclose(logits_fa, logits, atol=4e-2, rtol=4e-2)\n-\n-                # Ignore copy\n-                other_inputs = {\n-                    \"decoder_input_ids\": decoder_input_ids,\n-                    \"decoder_attention_mask\": dummy_attention_mask,\n-                    \"output_hidden_states\": True,\n-                }\n-                # Ignore copy\n-                if dummy_attention_mask is not None:\n-                    other_inputs[\"attention_mask\"] = dummy_attention_mask\n-                # Ignore copy\n-                outputs = model(dummy_input, **other_inputs)\n-                # Ignore copy\n-                outputs_fa = model_fa(dummy_input, **other_inputs)\n-\n-                logits = (\n-                    outputs.hidden_states[-1]\n-                    if not model.config.is_encoder_decoder\n-                    else outputs.decoder_hidden_states[-1]\n-                )\n-                logits_fa = (\n-                    outputs_fa.hidden_states[-1]\n-                    if not model.config.is_encoder_decoder\n-                    else outputs_fa.decoder_hidden_states[-1]\n-                )\n-\n-                assert torch.allclose(logits_fa[:-1], logits[:-1], atol=4e-2, rtol=4e-2)\n-\n     def test_sdpa_can_dispatch_composite_models(self):\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")"
        },
        {
            "sha": "8152db392ff445ccd84b64040a9252a1c05c3d7a",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 0,
            "deletions": 171,
            "changes": 171,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -938,93 +938,6 @@ def test_greedy_generate_stereo_outputs(self):\n         super().test_greedy_generate_dict_outputs()\n         self.model_tester.audio_channels = original_audio_channels\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    # Adapted from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_inference_equivalence\n-    def test_flash_attn_2_inference_equivalence(self):\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_fa = model_class.from_pretrained(\n-                    tmpdirname,\n-                    dtype=torch.bfloat16,\n-                    attn_implementation={\"decoder\": \"flash_attention_2\", \"audio_encoder\": None, \"text_encoder\": None},\n-                )\n-                model_fa.to(torch_device)\n-\n-                model = model_class.from_pretrained(tmpdirname, dtype=torch.bfloat16)\n-                model.to(torch_device)\n-\n-                # Ignore copy\n-                dummy_input = inputs_dict[model.main_input_name]\n-                if dummy_input.dtype in [torch.float32, torch.float16]:\n-                    dummy_input = dummy_input.to(torch.bfloat16)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n-\n-                if dummy_attention_mask is not None:\n-                    # Ignore copy\n-                    dummy_attention_mask[:, 1:] = 1\n-                    dummy_attention_mask[:, :1] = 0\n-\n-                # Ignore copy\n-                decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", dummy_input)\n-                # Ignore copy\n-                outputs = model(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n-                # Ignore copy\n-                outputs_fa = model_fa(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n-\n-                logits = (\n-                    outputs.hidden_states[-1]\n-                    if not model.config.is_encoder_decoder\n-                    else outputs.decoder_hidden_states[-1]\n-                )\n-                logits_fa = (\n-                    outputs_fa.hidden_states[-1]\n-                    if not model.config.is_encoder_decoder\n-                    else outputs_fa.decoder_hidden_states[-1]\n-                )\n-\n-                assert torch.allclose(logits_fa, logits, atol=4e-2, rtol=4e-2)\n-                # Ignore copy\n-                other_inputs = {\n-                    \"decoder_input_ids\": decoder_input_ids,\n-                    \"decoder_attention_mask\": dummy_attention_mask,\n-                    \"output_hidden_states\": True,\n-                }\n-                # Ignore copy\n-                if dummy_attention_mask is not None:\n-                    other_inputs[\"attention_mask\"] = dummy_attention_mask\n-                # Ignore copy\n-                outputs = model(dummy_input, **other_inputs)\n-                # Ignore copy\n-                outputs_fa = model_fa(dummy_input, **other_inputs)\n-\n-                logits = (\n-                    outputs.hidden_states[-1]\n-                    if not model.config.is_encoder_decoder\n-                    else outputs.decoder_hidden_states[-1]\n-                )\n-                logits_fa = (\n-                    outputs_fa.hidden_states[-1]\n-                    if not model.config.is_encoder_decoder\n-                    else outputs_fa.decoder_hidden_states[-1]\n-                )\n-\n-                assert torch.allclose(logits_fa[1:], logits[1:], atol=4e-2, rtol=4e-2)\n-\n-                # check with inference + dropout\n-                model.train()\n-                _ = model_fa(dummy_input, **other_inputs)\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @mark.flash_attn_test\n@@ -1085,90 +998,6 @@ def test_sdpa_can_dispatch_on_flash(self):\n                 with sdpa_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n                     _ = model(**inputs_dict)\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    # Adapted from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_inference_equivalence_right_padding\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_fa = model_class.from_pretrained(\n-                    tmpdirname,\n-                    dtype=torch.bfloat16,\n-                    attn_implementation={\"decoder\": \"flash_attention_2\", \"audio_encoder\": None, \"text_encoder\": None},\n-                )\n-                model_fa.to(torch_device)\n-\n-                model = model_class.from_pretrained(tmpdirname, dtype=torch.bfloat16)\n-                model.to(torch_device)\n-\n-                # Ignore copy\n-                dummy_input = inputs_dict[model.main_input_name]\n-                if dummy_input.dtype in [torch.float32, torch.float16]:\n-                    dummy_input = dummy_input.to(torch.bfloat16)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n-\n-                if dummy_attention_mask is not None:\n-                    # Ignore copy\n-                    dummy_attention_mask[:, :-1] = 1\n-                    dummy_attention_mask[:, -1:] = 0\n-\n-                # Ignore copy\n-                decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", dummy_input)\n-                # Ignore copy\n-                outputs = model(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n-                # Ignore copy\n-                outputs_fa = model_fa(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n-\n-                logits = (\n-                    outputs.hidden_states[-1]\n-                    if not model.config.is_encoder_decoder\n-                    else outputs.decoder_hidden_states[-1]\n-                )\n-                logits_fa = (\n-                    outputs_fa.hidden_states[-1]\n-                    if not model.config.is_encoder_decoder\n-                    else outputs_fa.decoder_hidden_states[-1]\n-                )\n-\n-                assert torch.allclose(logits_fa, logits, atol=4e-2, rtol=4e-2)\n-\n-                # Ignore copy\n-                other_inputs = {\n-                    \"decoder_input_ids\": decoder_input_ids,\n-                    \"decoder_attention_mask\": dummy_attention_mask,\n-                    \"output_hidden_states\": True,\n-                }\n-                # Ignore copy\n-                if dummy_attention_mask is not None:\n-                    other_inputs[\"attention_mask\"] = dummy_attention_mask\n-                # Ignore copy\n-                outputs = model(dummy_input, **other_inputs)\n-                # Ignore copy\n-                outputs_fa = model_fa(dummy_input, **other_inputs)\n-\n-                logits = (\n-                    outputs.hidden_states[-1]\n-                    if not model.config.is_encoder_decoder\n-                    else outputs.decoder_hidden_states[-1]\n-                )\n-                logits_fa = (\n-                    outputs_fa.hidden_states[-1]\n-                    if not model.config.is_encoder_decoder\n-                    else outputs_fa.decoder_hidden_states[-1]\n-                )\n-\n-                assert torch.allclose(logits_fa[:-1], logits[:-1], atol=4e-2, rtol=4e-2)\n-\n     def test_sdpa_can_dispatch_composite_models(self):\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")"
        },
        {
            "sha": "3c2cfee9987bf76e5dfd975e96986c4ce406dc89",
            "filename": "tests/models/phi4_multimodal/test_modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -213,10 +213,6 @@ def setUp(self):\n     def test_initialization(self):\n         pass\n \n-    @unittest.skip(reason=\"Right padding not supported\")\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        pass\n-\n     @unittest.skip(reason=\"Depending on input modalities, some params may not have gradients\")\n     def test_training_gradient_checkpointing(self):\n         pass"
        },
        {
            "sha": "d5c207fb9b8cd0d957443cc631c4596cba5068b4",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -27,7 +27,6 @@\n     require_bitsandbytes,\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -99,13 +98,6 @@ def is_pipeline_test_to_skip(\n     ):\n         return True\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        self.skipTest(reason=\"Qwen2 flash attention does not support right padding\")\n-\n \n @require_torch\n class Qwen2IntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "1bc5749a1a256c63ea872670b0a52e794cd64504",
            "filename": "tests/models/qwen2_audio/test_modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -156,10 +156,6 @@ def test_sdpa_can_compile_dynamic(self):\n     def test_sdpa_can_dispatch_on_flash(self):\n         pass\n \n-    @unittest.skip(reason=\"Qwen2 Audio does not support right padding.\")\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        pass\n-\n     def test_sdpa_can_dispatch_composite_models(self):\n         # overwrite because Qwen2 is audio+text model (not vision+text)\n         if not self.has_attentions:"
        },
        {
            "sha": "169d7ee784a09a53960a7d1978a02015bb478ca5",
            "filename": "tests/models/qwen3/test_modeling_qwen3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -26,7 +26,6 @@\n     require_bitsandbytes,\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -97,13 +96,6 @@ def is_pipeline_test_to_skip(\n     ):\n         return True\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        self.skipTest(reason=\"Qwen3 flash attention does not support right padding\")\n-\n \n @require_torch\n class Qwen3IntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "7fd07e45e222e623ffaaebdc3975e1b215d90548",
            "filename": "tests/models/qwen3_moe/test_modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -23,7 +23,6 @@\n     require_bitsandbytes,\n     require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n     require_torch_large_accelerator,\n     require_torch_multi_accelerator,\n     slow,\n@@ -98,13 +97,6 @@ def is_pipeline_test_to_skip(\n     ):\n         return True\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        self.skipTest(reason=\"Qwen3Moe flash attention does not support right padding\")\n-\n     # Ignore copy\n     def test_load_balancing_loss(self):\n         r\"\"\""
        },
        {
            "sha": "a4c829493b1757e381a57984a9a2e3ea26d7d8d5",
            "filename": "tests/models/siglip/test_modeling_siglip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 86,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -21,13 +21,10 @@\n import numpy as np\n import requests\n from parameterized import parameterized\n-from pytest import mark\n \n from transformers import SiglipConfig, SiglipTextConfig, SiglipVisionConfig\n from transformers.testing_utils import (\n-    require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n     require_vision,\n     slow,\n     torch_device,\n@@ -599,89 +596,6 @@ def test_model_from_pretrained(self):\n         model = SiglipModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_inference_equivalence(self):\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_fa = model_class.from_pretrained(\n-                    tmpdirname, dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n-                )\n-                model_fa.to(torch_device)\n-\n-                model = model_class.from_pretrained(tmpdirname, dtype=torch.bfloat16)\n-                model.to(torch_device)\n-\n-                dummy_pixel_values = inputs_dict[\"pixel_values\"].to(torch.bfloat16)\n-                dummy_input_ids = inputs_dict[\"input_ids\"]\n-\n-                outputs = model(pixel_values=dummy_pixel_values, input_ids=dummy_input_ids, output_hidden_states=True)\n-                outputs_fa = model_fa(\n-                    pixel_values=dummy_pixel_values, input_ids=dummy_input_ids, output_hidden_states=True\n-                )\n-\n-                self.assertTrue(\n-                    torch.allclose(outputs.logits_per_image, outputs_fa.logits_per_image, atol=4e-2, rtol=4e-2),\n-                    f\"Image logits max diff: {torch.max(torch.abs(outputs.logits_per_image - outputs_fa.logits_per_image))}\",\n-                )\n-                self.assertTrue(\n-                    torch.allclose(outputs.logits_per_text, outputs_fa.logits_per_text, atol=4e-2, rtol=4e-2),\n-                    f\"Text logits max diff: {torch.max(torch.abs(outputs.logits_per_text - outputs_fa.logits_per_text))}\",\n-                )\n-\n-                # Test with attention mask\n-                dummy_attention_mask = inputs_dict[\"attention_mask\"]\n-\n-                if dummy_attention_mask is not None:\n-                    dummy_attention_mask[:, 1:] = 1\n-                    dummy_attention_mask[:, :1] = 0\n-\n-                outputs = model(\n-                    pixel_values=dummy_pixel_values,\n-                    input_ids=dummy_input_ids,\n-                    attention_mask=dummy_attention_mask,\n-                    output_hidden_states=True,\n-                )\n-                outputs_fa = model_fa(\n-                    pixel_values=dummy_pixel_values,\n-                    input_ids=dummy_input_ids,\n-                    attention_mask=dummy_attention_mask,\n-                    output_hidden_states=True,\n-                )\n-\n-                self.assertTrue(\n-                    torch.allclose(outputs.logits_per_image, outputs_fa.logits_per_image, atol=4e-2, rtol=4e-2),\n-                    f\"Logits max diff: {torch.max(torch.abs(outputs.logits_per_image - outputs_fa.logits_per_image))}\",\n-                )\n-                self.assertTrue(\n-                    torch.allclose(outputs.logits_per_text, outputs_fa.logits_per_text, atol=4e-2, rtol=4e-2),\n-                    f\"Logits max diff: {torch.max(torch.abs(outputs.logits_per_text - outputs_fa.logits_per_text))}\",\n-                )\n-\n-                # check with inference + dropout\n-                model.train()\n-                _ = model_fa(\n-                    pixel_values=dummy_pixel_values,\n-                    input_ids=dummy_input_ids,\n-                    attention_mask=dummy_attention_mask,\n-                    output_hidden_states=True,\n-                )\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        self.skipTest(\"SigLIP does not support right padding\")\n-\n \n class SiglipForImageClassificationModelTester(SiglipModelTester):\n     def __init__(self, parent):"
        },
        {
            "sha": "74350dfc45f9b27c44ecdc19e8b08af86c7c64d3",
            "filename": "tests/models/starcoder2/test_modeling_starcoder2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -24,7 +24,6 @@\n     require_flash_attn,\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -74,13 +73,6 @@ class Starcoder2ModelTest(CausalLMModelTest, unittest.TestCase):\n         else {}\n     )\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        self.skipTest(reason=\"Starcoder2 flash attention does not support right padding\")\n-\n \n @slow\n @require_torch_accelerator"
        },
        {
            "sha": "16e4c177ddaae996342165617870cd533662e2db",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 104,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -31,12 +31,10 @@\n from transformers.testing_utils import (\n     Expectations,\n     is_flaky,\n-    require_flash_attn,\n     require_read_token,\n     require_torch,\n     require_torch_accelerator,\n     require_torch_fp16,\n-    require_torch_gpu,\n     require_torch_multi_accelerator,\n     require_torchaudio,\n     slow,\n@@ -848,108 +846,6 @@ def test_resize_embeddings_untied(self):\n     def test_generate_without_input_ids(self):\n         pass\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_inference_equivalence(self):\n-        import torch\n-\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn:\n-                self.skipTest(reason=\"Model does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_fa = model_class.from_pretrained(\n-                    tmpdirname, dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n-                )\n-                model_fa.to(torch_device)\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    dtype=torch.bfloat16,\n-                )\n-                model.to(torch_device)\n-\n-                dummy_input = inputs_dict[model.main_input_name][:1]\n-                if dummy_input.dtype in [torch.float32, torch.float16]:\n-                    dummy_input = dummy_input.to(torch.bfloat16)\n-\n-                decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", dummy_input)[:1]\n-\n-                outputs = model(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n-                outputs_fa = model_fa(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n-\n-                logits = outputs.decoder_hidden_states[-1]\n-                logits_fa = outputs_fa.decoder_hidden_states[-1]\n-\n-                # whisper FA2 needs very high tolerance\n-                torch.testing.assert_close(logits_fa, logits, rtol=4e-1, atol=4e-1)\n-\n-                # check with inference + dropout\n-                model.train()\n-                _ = model_fa(dummy_input, decoder_input_ids=decoder_input_ids)\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        import torch\n-\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn:\n-                self.skipTest(reason=\"Model does not support flash_attention_2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_fa = model_class.from_pretrained(\n-                    tmpdirname, dtype=torch.float16, attn_implementation=\"flash_attention_2\"\n-                )\n-                model_fa.to(torch_device)\n-\n-                model = model_class.from_pretrained(tmpdirname, dtype=torch.float16)\n-                model.to(torch_device)\n-\n-                dummy_input = inputs_dict[model.main_input_name][:1]\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-                decoder_input_ids = torch.tensor([[0, 1, 2, 3, 4, 5]], device=dummy_input.device, dtype=torch.long)\n-                decoder_attention_mask = torch.tensor(\n-                    [[0, 0, 0, 1, 1, 1]], device=dummy_input.device, dtype=torch.long\n-                )\n-\n-                outputs = model(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n-                outputs_fa = model_fa(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n-\n-                logits = outputs.decoder_hidden_states[-1]\n-                logits_fa = outputs_fa.decoder_hidden_states[-1]\n-\n-                # whisper FA2 needs very high tolerance\n-                torch.testing.assert_close(logits_fa, logits, rtol=4e-1, atol=4e-1)\n-\n-                other_inputs = {\n-                    \"decoder_input_ids\": decoder_input_ids,\n-                    \"decoder_attention_mask\": decoder_attention_mask,\n-                    \"output_hidden_states\": True,\n-                }\n-\n-                outputs = model(dummy_input, **other_inputs)\n-                outputs_fa = model_fa(dummy_input, **other_inputs)\n-\n-                logits = outputs.decoder_hidden_states[-1]\n-                logits_fa = outputs_fa.decoder_hidden_states[-1]\n-\n-                # whisper FA2 needs very high tolerance\n-                torch.testing.assert_close(logits_fa[:, -2:], logits[:, -2:], rtol=4e-1, atol=4e-1)\n-\n     def _create_and_check_torchscript(self, config, inputs_dict):\n         if not self.test_torchscript:\n             self.skipTest(reason=\"test_torchscript is set to False\")"
        },
        {
            "sha": "7708edef27e95aa1fb7cce22596d158e1d79d5a3",
            "filename": "tests/models/xcodec/test_modeling_xcodec.py",
            "status": "modified",
            "additions": 0,
            "deletions": 40,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fxcodec%2Ftest_modeling_xcodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fxcodec%2Ftest_modeling_xcodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxcodec%2Ftest_modeling_xcodec.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -24,17 +24,13 @@\n import numpy as np\n from datasets import Audio, load_dataset\n from parameterized import parameterized\n-from pytest import mark\n \n from tests.test_configuration_common import ConfigTester\n from tests.test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor\n from transformers import AutoFeatureExtractor, XcodecConfig\n from transformers.testing_utils import (\n-    is_flaky,\n     is_torch_available,\n-    require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -366,42 +362,6 @@ def test_initialization(self):\n                             msg=f\"Parameter {name} of {model_class.__name__} seems not properly initialized\",\n                         )\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    @is_flaky()\n-    def test_flash_attn_2_inference_equivalence(self):\n-        for model_class in self.all_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_fa = model_class.from_pretrained(\n-                    tmpdirname, dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n-                )\n-                model_fa.to(torch_device)\n-\n-                model = model_class.from_pretrained(tmpdirname, dtype=torch.bfloat16)\n-                model.to(torch_device)\n-\n-                dummy_input = inputs_dict[model.main_input_name][:1]\n-                if dummy_input.dtype in [torch.float32, torch.float16]:\n-                    dummy_input = dummy_input.to(torch.bfloat16)\n-\n-                outputs = model(dummy_input)\n-                outputs_fa = model_fa(dummy_input)\n-\n-                logits = outputs[1]\n-                logits_fa = outputs_fa[1]\n-\n-                assert torch.allclose(logits_fa, logits, atol=4e-2, rtol=4e-2)\n-\n-    @unittest.skip(reason=\"The XcodecModel does not support right padding\")\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        pass\n-\n     @unittest.skip(reason=\"The XcodecModel does not have support dynamic compile yet\")\n     def test_sdpa_can_compile_dynamic(self):\n         pass"
        },
        {
            "sha": "b601b280558b92d4b27b91e0c82f576693e1507a",
            "filename": "tests/models/zamba/test_modeling_zamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -563,17 +563,6 @@ def test_flash_attn_2_fp32_ln(self):\n                 # with attention mask\n                 _ = model(dummy_input, attention_mask=dummy_attention_mask)\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        r\"\"\"\n-        Overriding the test_flash_attn_2_inference_padding_right test as the Zamba model, like Mixtral, doesn't support\n-        right padding + use cache with FA2\n-        \"\"\"\n-        self.skipTest(reason=\"Zamba flash attention does not support right padding\")\n-\n \n @require_torch\n class ZambaModelIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "c6921297d6e70229e3837053cf2b2c14986503d4",
            "filename": "tests/models/zamba2/test_modeling_zamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -582,17 +582,6 @@ def test_flash_attn_2_fp32_ln(self):\n                 # with attention mask\n                 _ = model(dummy_input, attention_mask=dummy_attention_mask)\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_inference_equivalence_right_padding(self):\n-        r\"\"\"\n-        Overriding the test_flash_attn_2_inference_padding_right test as the Zamba2 model, like Mixtral, doesn't support\n-        right padding + use cache with FA2\n-        \"\"\"\n-        self.skipTest(reason=\"Zamba2 flash attention does not support right padding\")\n-\n     @unittest.skip(reason=\"Zamba2 has its own special cache type\")\n     @parameterized.expand([(1, False), (1, True), (4, False)])\n     def test_new_cache_format(self, num_beams, do_sample):"
        },
        {
            "sha": "0609c85ac3c36b99cdd84b329290fc52d29da775",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 88,
            "deletions": 70,
            "changes": 158,
            "blob_url": "https://github.com/huggingface/transformers/blob/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3beac9c65985c2d60b368afa92aa399fbf7b4644/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=3beac9c65985c2d60b368afa92aa399fbf7b4644",
            "patch": "@@ -3484,12 +3484,13 @@ def flash_attn_inference_equivalence(self, attn_implementation: str, padding_sid\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n \n+        # This flag is used to know if the test was skipped for all `self.all_model_classes` or not\n+        _has_run_at_least_one_model = False\n+\n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn:\n-                self.skipTest(f\"{model_class.__name__} does not support {attn_implementation}\")\n             # Custom kernel which needs the mask interface to be properly usable on these models\n             if not model_class._supports_attention_backend and not attn_implementation.startswith(\"flash_attention\"):\n-                self.skipTest(f\"{model_class.__name__} does not support {attn_implementation}\")\n+                continue\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -3500,103 +3501,117 @@ def flash_attn_inference_equivalence(self, attn_implementation: str, padding_sid\n             if getattr(config, \"sliding_window\", None):\n                 config.sliding_window = 2\n \n-            # TODO it is unclear why saving and reloading with dtype works while\n-            # casting with `.to(dtype=..., device=...)` does not.\n-            # Discovered on tests with `Bart` models.\n             model = model_class(config)\n+            if not all(\n+                submodel._supports_flash_attn for submodel in model.modules() if isinstance(submodel, PreTrainedModel)\n+            ):\n+                continue\n+\n+            # If we end up here, at least one model class was not skipped\n+            _has_run_at_least_one_model = True\n             with tempfile.TemporaryDirectory() as tmpdirname:\n+                # Save the model so we can reload with correct attention\n                 model.save_pretrained(tmpdirname)\n-                model = model_class.from_pretrained(tmpdirname, dtype=torch.bfloat16)\n-                model.to(torch_device)\n-\n-                # Some models have support for FA but not SDPA - making sure we have a valid attention\n-                initial_attention_implementation = \"sdpa\"\n-                if model.config._attn_implementation != \"sdpa\":\n-                    initial_attention_implementation = \"eager\"\n \n-                dummy_input = inputs_dict[model.main_input_name][:1]\n-                if dummy_input.dtype in [torch.float32, torch.float16]:\n-                    dummy_input = dummy_input.to(torch.bfloat16)\n+                # Create first inputs without attention mask\n+                main_input = inputs_dict[model.main_input_name]\n+                # Only keep first batch sequence\n+                if isinstance(main_input, torch.Tensor):\n+                    main_input = main_input[:1]\n+                    # Fix the dtype\n+                    if torch.is_floating_point(main_input):\n+                        main_input = main_input.to(torch.bfloat16)\n+                first_inputs = {model.main_input_name: main_input, \"output_hidden_states\": True}\n+                # Some models have main input name which is different from input_ids, but require input_ids... e.g. BarkFine\n+                if model.main_input_name != \"input_ids\" and \"input_ids\" in inputs_dict:\n+                    first_inputs[\"input_ids\"] = inputs_dict[\"input_ids\"][:1]\n+                # If we have some pixel values, use them as well\n+                if model.main_input_name != \"pixel_values\" and \"pixel_values\" in inputs_dict:\n+                    first_inputs[\"pixel_values\"] = inputs_dict[\"pixel_values\"][:1].to(torch.bfloat16)\n+                if model.config.is_encoder_decoder:\n+                    decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", first_inputs.get(\"input_ids\"))\n+                    if decoder_input_ids is not None:\n+                        first_inputs[\"decoder_input_ids\"] = decoder_input_ids[:1]\n \n+                # Create attention mask with padding\n                 dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n-\n                 if dummy_attention_mask is not None:\n                     dummy_attention_mask = dummy_attention_mask[:1]\n                     if padding_side == \"left\":\n                         dummy_attention_mask[:, 1:] = 1\n-                        dummy_attention_mask[:, :1] = 0\n+                        dummy_attention_mask[:, 0] = 0\n                     else:\n                         dummy_attention_mask[:, :-1] = 1\n-                        dummy_attention_mask[:, -1:] = 0\n-                if model.config.is_encoder_decoder:\n-                    decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", dummy_input)[:1]\n+                        dummy_attention_mask[:, -1] = 0\n \n-                    outputs = model(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n-                    model.set_attn_implementation(attn_implementation)\n-                    outputs_fa = model(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n-                else:\n-                    outputs = model(dummy_input, output_hidden_states=True)\n-                    model.set_attn_implementation(attn_implementation)\n-                    outputs_fa = model(dummy_input, output_hidden_states=True)\n+                # Create second inputs with attention mask and padding\n+                second_inputs = copy.deepcopy(first_inputs)\n+                if dummy_attention_mask is not None:\n+                    second_inputs[\"attention_mask\"] = dummy_attention_mask\n+                    if model.config.is_encoder_decoder:\n+                        second_inputs[\"decoder_attention_mask\"] = dummy_attention_mask\n \n-                model.set_attn_implementation(initial_attention_implementation)\n-                logits = (\n+                model = model_class.from_pretrained(\n+                    tmpdirname, dtype=torch.bfloat16, attn_implementation=\"eager\", device_map=torch_device\n+                )\n+\n+                # First run without attention mask\n+                outputs = model(**first_inputs)\n+                logits_1_eager = (\n                     outputs.hidden_states[-1]\n+                    if \"hidden_states\" in outputs\n+                    else outputs.logits_per_image\n                     if not model.config.is_encoder_decoder\n                     else outputs.decoder_hidden_states[-1]\n                 )\n-                logits_fa = (\n-                    outputs_fa.hidden_states[-1]\n+                # Second run with attention mask and padding\n+                outputs = model(**second_inputs)\n+                logits_2_eager = (\n+                    outputs.hidden_states[-1]\n+                    if \"hidden_states\" in outputs\n+                    else outputs.logits_per_image\n                     if not model.config.is_encoder_decoder\n-                    else outputs_fa.decoder_hidden_states[-1]\n+                    else outputs.decoder_hidden_states[-1]\n                 )\n \n-                assert torch.allclose(logits_fa, logits, atol=4e-2, rtol=4e-2)\n-\n-                if model.config.is_encoder_decoder:\n-                    other_inputs = {\n-                        \"decoder_input_ids\": decoder_input_ids,\n-                        \"decoder_attention_mask\": dummy_attention_mask,\n-                        \"output_hidden_states\": True,\n-                    }\n-                    if dummy_attention_mask is not None:\n-                        other_inputs[\"attention_mask\"] = dummy_attention_mask\n-\n-                    outputs = model(dummy_input, **other_inputs)\n-                    model.set_attn_implementation(attn_implementation)\n-                    outputs_fa = model(dummy_input, **other_inputs)\n-                else:\n-                    other_inputs = {\n-                        \"output_hidden_states\": True,\n-                    }\n-                    if dummy_attention_mask is not None:\n-                        other_inputs[\"attention_mask\"] = dummy_attention_mask\n-\n-                    outputs = model(dummy_input, **other_inputs)\n-                    model.set_attn_implementation(attn_implementation)\n-                    outputs_fa = model(dummy_input, **other_inputs)\n-\n-                model.set_attn_implementation(initial_attention_implementation)\n-                logits = (\n+                # Switch to FA\n+                del model\n+                model = model_class.from_pretrained(\n+                    tmpdirname, dtype=torch.bfloat16, attn_implementation=attn_implementation, device_map=torch_device\n+                )\n+                outputs = model(**first_inputs)\n+                logits_1_fa = (\n                     outputs.hidden_states[-1]\n+                    if \"hidden_states\" in outputs\n+                    else outputs.logits_per_image\n                     if not model.config.is_encoder_decoder\n                     else outputs.decoder_hidden_states[-1]\n                 )\n-                logits_fa = (\n-                    outputs_fa.hidden_states[-1]\n+                # Second run with attention mask and padding\n+                outputs = model(**second_inputs)\n+                logits_2_fa = (\n+                    outputs.hidden_states[-1]\n+                    if \"hidden_states\" in outputs\n+                    else outputs.logits_per_image\n                     if not model.config.is_encoder_decoder\n-                    else outputs_fa.decoder_hidden_states[-1]\n+                    else outputs.decoder_hidden_states[-1]\n                 )\n \n+                # Check the results\n+                torch.testing.assert_close(logits_1_eager, logits_1_fa, atol=4e-2, rtol=4e-2)\n                 if padding_side == \"left\":\n-                    assert torch.allclose(logits_fa[1:], logits[1:], atol=4e-2, rtol=4e-2)\n-\n-                    # check with inference + dropout\n+                    torch.testing.assert_close(logits_2_eager[1:], logits_2_fa[1:], atol=4e-2, rtol=4e-2)\n+                    # Check it can run in training mode\n                     model.train()\n-                    model.set_attn_implementation(attn_implementation)\n-                    _ = model(dummy_input, **other_inputs)\n+                    _ = model(**second_inputs)\n                 else:\n-                    assert torch.allclose(logits_fa[:-1], logits[:-1], atol=4e-2, rtol=4e-2)\n+                    torch.testing.assert_close(logits_2_eager[:-1], logits_2_fa[:-1], atol=4e-2, rtol=4e-2)\n+\n+        # In this case, the test should appear as skipped, not successful\n+        if not _has_run_at_least_one_model:\n+            self.skipTest(\n+                f\"Model architecture does not support {attn_implementation}, or setting its attention dynamically\"\n+            )\n \n     @require_kernels\n     @require_torch_gpu\n@@ -4806,7 +4821,10 @@ def update_config_headdim(config, requested_dim):\n \n             # 3d rope also depends on the head dim\n             # (we assume easy shapes here where we get to the requested head dim at least)\n-            if hasattr(config, \"rope_scaling\") and len(config.rope_scaling.get(\"mrope_section\", None)) > 0:\n+            if (\n+                getattr(config, \"rope_scaling\", None) is not None\n+                and len(config.rope_scaling.get(\"mrope_section\", [])) > 0\n+            ):\n                 scaling_factor = max(requested_dim // (sum(config.rope_scaling[\"mrope_section\"]) * 2), 1)\n                 config.rope_scaling[\"mrope_section\"] = [\n                     section * scaling_factor for section in config.rope_scaling[\"mrope_section\"]"
        }
    ],
    "stats": {
        "total": 1353,
        "additions": 111,
        "deletions": 1242
    }
}