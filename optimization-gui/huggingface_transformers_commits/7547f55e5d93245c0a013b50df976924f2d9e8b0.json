{
    "author": "winglian",
    "message": "fix iterator overflow when gradient accumulation is 1 (#35960)",
    "sha": "7547f55e5d93245c0a013b50df976924f2d9e8b0",
    "files": [
        {
            "sha": "fe15d57bb116dbd1a869e4cd5b1a6f38c09d7088",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7547f55e5d93245c0a013b50df976924f2d9e8b0/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7547f55e5d93245c0a013b50df976924f2d9e8b0/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=7547f55e5d93245c0a013b50df976924f2d9e8b0",
            "patch": "@@ -2434,6 +2434,8 @@ def _inner_training_loop(\n                 remainder = args.gradient_accumulation_steps\n             update_step = -1\n             total_updates = steps_in_epoch // args.gradient_accumulation_steps + 1\n+            if args.gradient_accumulation_steps == 1:\n+                total_updates -= 1\n             for _ in range(total_updates):\n                 update_step += 1\n                 num_batches = args.gradient_accumulation_steps if update_step != (total_updates - 1) else remainder"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 2,
        "deletions": 0
    }
}