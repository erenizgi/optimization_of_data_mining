{
    "author": "MekkCyber",
    "message": "[fp8] fix scales param name (#42434)\n\n* fix\n\n* up\n\n* up",
    "sha": "1fe7cfab8e5c2dc5d5ca3b1f62ec5c44ae6b41ce",
    "files": [
        {
            "sha": "42854324bc0d628cd6c8071e188ac3d2b8b09ae1",
            "filename": "src/transformers/integrations/finegrained_fp8.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fe7cfab8e5c2dc5d5ca3b1f62ec5c44ae6b41ce/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fe7cfab8e5c2dc5d5ca3b1f62ec5c44ae6b41ce/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py?ref=1fe7cfab8e5c2dc5d5ca3b1f62ec5c44ae6b41ce",
            "patch": "@@ -409,14 +409,14 @@ def __init__(self, config, block_size, device):\n             # gate_up tiles: ceil(Wg_out/bo) x ceil(Wg_in/bi)\n             gu_scale_o = _ceil_div(Wg_out, bo)\n             gu_scale_i = _ceil_div(Wg_in, bi)\n-            self.gate_up_proj_scales_inv = nn.Parameter(\n+            self.gate_up_proj_scale_inv = nn.Parameter(\n                 torch.zeros(self.num_experts, gu_scale_o, gu_scale_i, dtype=torch.float32, device=device)\n             )\n \n             # down tiles: ceil(Wd_out/bo) x ceil(Wd_in/bi)\n             dp_scale_o = _ceil_div(Wd_out, bo)\n             dp_scale_i = _ceil_div(Wd_in, bi)\n-            self.down_proj_scales_inv = nn.Parameter(\n+            self.down_proj_scale_inv = nn.Parameter(\n                 torch.zeros(self.num_experts, dp_scale_o, dp_scale_i, dtype=torch.float32, device=device)\n             )\n         else:\n@@ -452,11 +452,11 @@ def forward(\n             _, token_idx = torch.where(expert_mask[expert_idx])\n             current_state = hidden_states.index_select(0, token_idx)\n             gate, up = self.linear(\n-                current_state, self.gate_up_proj[expert_idx], self.gate_up_proj_scales_inv[expert_idx]\n+                current_state, self.gate_up_proj[expert_idx], self.gate_up_proj_scale_inv[expert_idx]\n             ).chunk(2, dim=-1)\n             current_hidden_states = self.act_fn(gate) * up\n             current_hidden_states = self.linear(\n-                current_hidden_states, self.down_proj[expert_idx], self.down_proj_scales_inv[expert_idx]\n+                current_hidden_states, self.down_proj[expert_idx], self.down_proj_scale_inv[expert_idx]\n             )\n \n             routing_weights = top_k_weights[token_idx, expert_idx].unsqueeze(-1)\n@@ -643,7 +643,7 @@ def convert(self, input_dict: torch.Tensor, **kwargs) -> dict[str, torch.Tensor]\n         if target_keys.endswith(\"weight\"):\n             scale_key = target_keys.rsplit(\".\", 1)[0] + \".weight_scale_inv\"\n         else:\n-            scale_key = target_keys + \"_scales_inv\"\n+            scale_key = target_keys + \"_scale_inv\"\n \n         # Return both quantized weights and per-tile inverse scales (keeps leading dims, e.g., num_experts)\n         return {"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 5,
        "deletions": 5
    }
}