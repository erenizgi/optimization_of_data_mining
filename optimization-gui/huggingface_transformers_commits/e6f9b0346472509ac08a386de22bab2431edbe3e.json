{
    "author": "ArthurZucker",
    "message": "[`Compile`] Only test compiling model forward pass (#35658)\n\n* rename test to only compile forward!\n\n* style emu",
    "sha": "e6f9b0346472509ac08a386de22bab2431edbe3e",
    "files": [
        {
            "sha": "95dcaea95e5cccc8d17717bca371d1efa8a59d3f",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 21,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/e6f9b0346472509ac08a386de22bab2431edbe3e/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e6f9b0346472509ac08a386de22bab2431edbe3e/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=e6f9b0346472509ac08a386de22bab2431edbe3e",
            "patch": "@@ -2042,16 +2042,10 @@ def test_generate_with_quant_cache(self):\n             with self.assertRaises(ValueError):\n                 model.generate(**generation_kwargs, **inputs_dict)\n \n-    @parameterized.expand(\n-        [\n-            (\"forward_only\", False),  # TODO (@joao): a few models failing. After fixed, this should not be \"@slow\"\n-            (\"end_to_end\", True),  # TODO (@joao): end-to-end compilation is broken with torch 2.5+, explore and fix\n-        ]\n-    )\n     @pytest.mark.generate\n     @require_torch_gpu\n     @slow\n-    def test_generate_compile(self, _, end_to_end):\n+    def test_generate_compile_model_forward(self):\n         \"\"\"\n         Tests that `.generate` is compatible with torch.compile without graph breaks, keeping the same results. Tests\n         end-to-end compilation and forward pass compilation only.\n@@ -2061,14 +2055,7 @@ def test_generate_compile(self, _, end_to_end):\n             if not model_class._supports_static_cache:\n                 self.skipTest(\"This model doesn't support static cache\")\n \n-            # TODO (joao) -- fix and enable me :)\n-            if end_to_end and any(model_name in model_class.__name__.lower() for model_name in [\"whisper\"]):\n-                self.skipTest(\"whisper model end-to-end generate compile not yet supported\")\n-\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            # TODO (joao) -- fix and enable me :)\n-            if end_to_end and config.is_encoder_decoder:\n-                self.skipTest(\"Encoder-decoder model end-to-end generate compile not yet supported\")\n \n             model = model_class(config).to(torch_device)\n             model.eval()  # otherwise `self.training` is `True` -- this flag is used at attn mask creation time\n@@ -2084,10 +2071,8 @@ def test_generate_compile(self, _, end_to_end):\n                 \"max_new_tokens\": 10,\n                 \"return_dict_in_generate\": True,\n                 \"output_scores\": True,\n+                \"cache_implementation\": \"static\",\n             }\n-            # end-to-end works best with dynamic cache, forward compilation works best with static cache\n-            if not end_to_end:\n-                generation_kwargs[\"cache_implementation\"] = \"static\"\n \n             # get eager + dynamic cache results for future comparison\n             dynamic_outputs = []\n@@ -2098,10 +2083,8 @@ def test_generate_compile(self, _, end_to_end):\n             generation_config = copy.deepcopy(model.generation_config)\n             generation_config.update(**generation_kwargs)\n             torch.compiler.reset()\n-            if end_to_end:\n-                model.generate = torch.compile(model.generate, fullgraph=True, mode=\"reduce-overhead\")\n-            else:\n-                model.forward = torch.compile(model.forward, fullgraph=True, mode=\"reduce-overhead\")\n+\n+            model.forward = torch.compile(model.forward, fullgraph=True, mode=\"reduce-overhead\")\n \n             compiled_outputs = []\n             for model_inputs in input_ids_sets:"
        },
        {
            "sha": "01d4ef720e57d80163973257aea99239f28cc30f",
            "filename": "tests/models/chameleon/test_modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e6f9b0346472509ac08a386de22bab2431edbe3e/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e6f9b0346472509ac08a386de22bab2431edbe3e/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py?ref=e6f9b0346472509ac08a386de22bab2431edbe3e",
            "patch": "@@ -333,7 +333,7 @@ def test_batching_equivalence(self):\n \n     # TODO (joao, raushan): fix me -- the problem is in `cache_position[0] == 0`, i.e. dynamic control flow\n     @unittest.skip(\"Chameleon is not compatible with end-to-end generation compilation\")\n-    def test_generate_compile_fullgraph(self):\n+    def test_generate_compile_model_forward(self):\n         pass\n \n "
        },
        {
            "sha": "dee93109da24466e361bc4d2013f5e9175c498a6",
            "filename": "tests/models/dbrx/test_modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e6f9b0346472509ac08a386de22bab2431edbe3e/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e6f9b0346472509ac08a386de22bab2431edbe3e/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py?ref=e6f9b0346472509ac08a386de22bab2431edbe3e",
            "patch": "@@ -369,7 +369,7 @@ def test_disk_offload_bin(self):\n         pass\n \n     @unittest.skip(\"Dbrx does not support `torch.compile` with `fullgraph=True`.\")\n-    def test_generate_compile_fullgraph(self):\n+    def test_generate_compile_model_forward(self):\n         pass\n \n "
        },
        {
            "sha": "007207c0694290d29c416a754668b3efa119a44e",
            "filename": "tests/models/emu3/test_modeling_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e6f9b0346472509ac08a386de22bab2431edbe3e/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e6f9b0346472509ac08a386de22bab2431edbe3e/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py?ref=e6f9b0346472509ac08a386de22bab2431edbe3e",
            "patch": "@@ -176,10 +176,6 @@ def test_model_rope_scaling(self, scaling_type):\n     def test_custom_4d_attention_mask(self):\n         pass\n \n-    @unittest.skip(\"Fails with unknown error only on end-to-end compile\")  # TODO raushan fixme\n-    def test_generate_compile_1_end_to_end(self):\n-        pass\n-\n \n class Emu3Vision2TextModelTester:\n     def __init__(\n@@ -398,10 +394,6 @@ def test_custom_4d_attention_mask(self):\n     def test_initialization(self):\n         pass\n \n-    @unittest.skip(\"End-to-end compilation is not supported due to dynamic control in `prepare_inputs_for_generation`\")\n-    def test_generate_compile_1_end_to_end(self):\n-        pass\n-\n \n @require_torch\n class Emu3IntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "a8f1304b6fc743961bca505bed4a457b42365275",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e6f9b0346472509ac08a386de22bab2431edbe3e/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e6f9b0346472509ac08a386de22bab2431edbe3e/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=e6f9b0346472509ac08a386de22bab2431edbe3e",
            "patch": "@@ -781,7 +781,7 @@ def test_custom_4d_attention_mask(self):\n         pass\n \n     @unittest.skip(reason=\"IDEFICS cannot compile due to dynamic control flow when checking inputs\")\n-    def test_generate_compile_fullgraph(self):\n+    def test_generate_compile_model_forward(self):\n         pass\n \n     @unittest.skip(reason=\"We only test the model that takes in multiple images\")"
        },
        {
            "sha": "587d4606493b9b25beb09d721aabb92a9d054ef5",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e6f9b0346472509ac08a386de22bab2431edbe3e/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e6f9b0346472509ac08a386de22bab2431edbe3e/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=e6f9b0346472509ac08a386de22bab2431edbe3e",
            "patch": "@@ -348,7 +348,7 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n \n     # TODO (joao, raushan): fix me -- the problem is in `cache_position[0] == 0`, i.e. dynamic control flow\n     @unittest.skip(\"PaliGemma is not compatible with end-to-end generation compilation\")\n-    def test_generate_compile_fullgraph(self):\n+    def test_generate_compile_model_forward(self):\n         pass\n \n "
        },
        {
            "sha": "fc9adcebf5f7ad813a845c8f37ac271113fcd0f5",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e6f9b0346472509ac08a386de22bab2431edbe3e/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e6f9b0346472509ac08a386de22bab2431edbe3e/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=e6f9b0346472509ac08a386de22bab2431edbe3e",
            "patch": "@@ -333,7 +333,7 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass\n \n     @unittest.skip(reason=\"Can't compile fullgraph due to dynamic control flow in `prepare_inputs_for_generate`\")\n-    def test_generate_compile_fullgraph(self):\n+    def test_generate_compile_model_forward(self):\n         pass\n \n "
        }
    ],
    "stats": {
        "total": 43,
        "additions": 9,
        "deletions": 34
    }
}