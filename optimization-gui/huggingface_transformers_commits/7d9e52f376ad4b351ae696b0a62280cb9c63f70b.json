{
    "author": "LysandreJik",
    "message": "Fix continuous batching in `transformers serve` (#39149)\n\n* Fix CB\n\n* Nit\n\n* Update src/transformers/commands/serving.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Add todos\n\n---------\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "7d9e52f376ad4b351ae696b0a62280cb9c63f70b",
    "files": [
        {
            "sha": "5ab209966937d9c35c0f58082ce135355fd01c1f",
            "filename": "src/transformers/commands/serving.py",
            "status": "modified",
            "additions": 38,
            "deletions": 28,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/7d9e52f376ad4b351ae696b0a62280cb9c63f70b/src%2Ftransformers%2Fcommands%2Fserving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7d9e52f376ad4b351ae696b0a62280cb9c63f70b/src%2Ftransformers%2Fcommands%2Fserving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fserving.py?ref=7d9e52f376ad4b351ae696b0a62280cb9c63f70b",
            "patch": "@@ -29,7 +29,7 @@\n \n from transformers.utils.import_utils import is_fastapi_available, is_pydantic_available, is_uvicorn_available\n \n-from .. import PreTrainedTokenizerFast, TextIteratorStreamer\n+from .. import LogitsProcessorList, PreTrainedTokenizerFast, TextIteratorStreamer\n from ..generation.continuous_batching import ContinuousBatchingManager, RequestStatus\n from ..utils import is_torch_available, logging\n from . import BaseTransformersCLICommand\n@@ -110,7 +110,7 @@ def serve_command_factory(args: Namespace):\n     return ServeCommand(args)\n \n \n-def create_generation_config_from_req(req: \"ChatCompletionInput\") -> \"GenerationConfig\":\n+def create_generation_config_from_req(req: \"ChatCompletionInput\", **kwargs) -> \"GenerationConfig\":\n     \"\"\"\n     Creates a generation config from the parameters of the request. Note that we can pass a `GenerationConfig`\n     (serialized into a `dict`) in `extra_body`, for full `generate` parameterization.\n@@ -124,12 +124,12 @@ def create_generation_config_from_req(req: \"ChatCompletionInput\") -> \"Generation\n     if req.extra_body is not None and \"generation_config\" in req.extra_body:\n         for key in req.extra_body[\"generation_config\"].keys():\n             if key in ChatCompletionInput.base_field_names.keys():\n-                return {\"error\": \"Duplicated key in the root request and in the passed generation config.\"}\n+                raise ValueError(\"error: Duplicated key in the root request and in the passed generation config.\")\n \n     if req.extra_body is not None and \"generation_config\" in req.extra_body:\n-        generation_config = GenerationConfig(**(req.extra_body[\"generation_config\"]))\n+        generation_config = GenerationConfig(**(req.extra_body[\"generation_config\"]), **kwargs)\n     else:\n-        generation_config = GenerationConfig()\n+        generation_config = GenerationConfig(**kwargs)\n \n     if req.frequency_penalty is not None:\n         generation_config.repetition_penalty = float(req.frequency_penalty)\n@@ -223,6 +223,8 @@ class ServeArguments:\n \n class ServeCommand(BaseTransformersCLICommand):\n     loaded_model: Optional[str] = None\n+    running_continuous_batching_manager: Optional[ContinuousBatchingManager] = None\n+\n     model: PreTrainedModel\n     tokenizer: PreTrainedTokenizerFast\n \n@@ -367,46 +369,54 @@ def get_all_models():\n         uvicorn.run(app, host=self.args.host, port=self.args.port, log_level=self.args.log_level)\n \n     def continuous_batching(self, app):\n-        generation_config = GenerationConfig(\n-            eos_token_id=self.tokenizer.eos_token_id,\n-            pad_token_id=self.tokenizer.pad_token_id,\n-            use_cache=False,\n-            num_blocks=1,\n-            block_size=1024,\n-            do_sample=False,\n-            max_batch_tokens=10,\n-            scheduler=\"fifo\",\n-        )\n-\n-        manager: ContinuousBatchingManager = self.model.init_continuous_batching(\n-            generation_config=generation_config, streaming=True\n-        )\n-        manager.start()\n-\n         @app.post(\"/v1/chat/completions\")\n         def _serve(req: \"ChatCompletionInput\"):\n             if not req.stream:\n                 return {\"error\": \"Only streaming mode is supported.\"}\n \n             update_model = self.canonicalized_model_name(req.model) != self.loaded_model\n-\n             if update_model:\n                 self.model, self.tokenizer = self.load_model_and_tokenizer(req.model, self.args)\n \n-            chat = req.messages\n-            inputs = self.tokenizer.apply_chat_template(chat, return_tensors=\"pt\", add_generation_prompt=True).to(\n-                self.model.device\n+            generation_config = create_generation_config_from_req(\n+                req,\n+                eos_token_id=self.tokenizer.eos_token_id,\n+                pad_token_id=self.tokenizer.pad_token_id,\n+                use_cache=False,\n+                num_blocks=1,\n+                block_size=1024,\n+                do_sample=False,\n+                max_batch_tokens=10,\n+                scheduler=\"fifo\",\n             )\n \n-            generation_config = create_generation_config_from_req(req)\n+            if self.running_continuous_batching_manager is None or update_model:\n+                self.running_continuous_batching_manager = self.model.init_continuous_batching(\n+                    generation_config=generation_config, streaming=True\n+                )\n+\n+                # TODO (Joao, Lysandre): the logits processors should be fixed in continuous batching\n+                # and correctly applied in non-cb\n+                self.running_continuous_batching_manager.logit_processor = LogitsProcessorList()\n+                self.running_continuous_batching_manager.start()\n+\n+            # TODO (Joao, Lysandre): this should also work with tool support\n+            inputs = self.tokenizer.apply_chat_template(\n+                req.messages, return_tensors=\"pt\", add_generation_prompt=True\n+            ).to(self.model.device)\n \n             def stream_response(_inputs):\n                 try:\n                     max_new_tokens = req.max_tokens or generation_config.max_new_tokens or 1024\n-                    request_id = manager.add_request(_inputs, request_id=req.request_id, max_new_tokens=max_new_tokens)\n+                    request_id = self.running_continuous_batching_manager.add_request(\n+                        _inputs, request_id=req.request_id, max_new_tokens=max_new_tokens\n+                    )\n                     queue_is_flushed = False\n \n-                    for result in manager:\n+                    for result in self.running_continuous_batching_manager:\n+                        if result.request_id != request_id:\n+                            continue\n+\n                         if req.request_id is not None and not queue_is_flushed:\n                             if result.status == RequestStatus.FINISHED:\n                                 continue"
        }
    ],
    "stats": {
        "total": 66,
        "additions": 38,
        "deletions": 28
    }
}