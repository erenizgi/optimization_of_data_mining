{
    "author": "yonigozlan",
    "message": "Uniformize LlavaNextVideoProcessor kwargs (#35613)\n\n* Uniformize processor kwargs and add tests\r\n\r\n* add videos_kwargs tests\r\n\r\n* fix copies\r\n\r\n* fix llava_next_video chat template tests\r\n\r\n* remove unnecessary default kwargs",
    "sha": "9b479a245b793cac2a8b2e87c6d8e81bb24e20c4",
    "files": [
        {
            "sha": "6ec40209df8b58fc404e8e02e0c42432323c2769",
            "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
            "status": "modified",
            "additions": 30,
            "deletions": 34,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b479a245b793cac2a8b2e87c6d8e81bb24e20c4/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b479a245b793cac2a8b2e87c6d8e81bb24e20c4/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py?ref=9b479a245b793cac2a8b2e87c6d8e81bb24e20c4",
            "patch": "@@ -16,24 +16,33 @@\n Processor class for LLaVa-NeXT-Video.\n \"\"\"\n \n-from typing import TYPE_CHECKING, List, Optional, Union\n+from typing import List, Union\n \n import numpy as np\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_processing_utils import select_best_resolution\n from ...image_utils import ImageInput, VideoInput, get_image_size, to_numpy_array\n-from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n-from ...utils import TensorType, logging\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack, _validate_images_text_input_order\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import logging\n \n \n-if TYPE_CHECKING:\n-    pass\n-\n logger = logging.get_logger(__name__)\n \n \n+class LlavaNextVideoProcessorKwargs(ProcessingKwargs, total=False):\n+    # see processing_utils.ProcessingKwargs documentation for usage.\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": False,\n+        },\n+        \"common_kwargs\": {\n+            \"return_tensors\": \"pt\",\n+        },\n+    }\n+\n+\n class LlavaNextVideoProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs a LLaVa-NeXT-Video processor which wraps a LLaVa-NeXT image processor, LLaVa-NeXT-Video video processor and\n@@ -102,13 +111,11 @@ def __init__(\n \n     def __call__(\n         self,\n-        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]],\n         images: ImageInput = None,\n+        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        audio=None,\n         videos: VideoInput = None,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length: int = None,\n-        return_tensors: Optional[Union[str, TensorType]] = TensorType.PYTORCH,\n+        **kwargs: Unpack[LlavaNextVideoProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n@@ -130,19 +137,6 @@ def __call__(\n             videos (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`):\n                 The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n                 tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n-            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n-                Select a strategy to pad the returned sequences (according to the model's padding side and padding\n-                index) among:\n-                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n-                  sequence if provided).\n-                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n-                  acceptable input length for the model if that argument is not provided.\n-                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n-                  lengths).\n-            max_length (`int`, *optional*):\n-                Maximum length of the returned list and optionally padding length (see above).\n-            truncation (`bool`, *optional*):\n-                Activates truncation to cut input sequences longer than `max_length` to `max_length`.\n             return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                 If set, will return tensors of a particular framework. Acceptable values are:\n \n@@ -160,13 +154,21 @@ def __call__(\n               `None`).\n             - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n         \"\"\"\n+        # check if images and text inputs are reversed for BC\n+        images, text = _validate_images_text_input_order(images, text)\n+\n+        output_kwargs = self._merge_kwargs(\n+            LlavaNextVideoProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n         if images is not None:\n-            image_inputs = self.image_processor(images, return_tensors=return_tensors)\n+            image_inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n         else:\n             image_inputs = {}\n \n         if videos is not None:\n-            videos_inputs = self.video_processor(videos, return_tensors=return_tensors)\n+            videos_inputs = self.video_processor(videos, **output_kwargs[\"videos_kwargs\"])\n         else:\n             videos_inputs = {}\n \n@@ -212,13 +214,7 @@ def __call__(\n                 prompt_strings.append(sample)\n             text = prompt_strings\n \n-        text_inputs = self.tokenizer(\n-            text,\n-            return_tensors=return_tensors,\n-            padding=padding,\n-            truncation=truncation,\n-            max_length=max_length,\n-        )\n+        text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n         return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs})\n \n     # Copied from transformers.models.llava_next.processing_llava_next.LlavaNextProcessor._get_number_of_features"
        },
        {
            "sha": "aa97799da6450ddb99f94291705ba1959bfbbbe0",
            "filename": "src/transformers/models/llava_onevision/processing_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b479a245b793cac2a8b2e87c6d8e81bb24e20c4/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b479a245b793cac2a8b2e87c6d8e81bb24e20c4/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py?ref=9b479a245b793cac2a8b2e87c6d8e81bb24e20c4",
            "patch": "@@ -41,7 +41,7 @@ class LlavaOnevisionProcessorKwargs(ProcessingKwargs, total=False):\n             \"padding\": False,\n         },\n         \"image_kwargs\": {},\n-        \"video_kwargs\": {},\n+        \"videos_kwargs\": {},\n     }\n \n "
        },
        {
            "sha": "0ce0d3521d3a8fd19327c8db1715af540a9c1166",
            "filename": "src/transformers/pipelines/image_text_to_text.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b479a245b793cac2a8b2e87c6d8e81bb24e20c4/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b479a245b793cac2a8b2e87c6d8e81bb24e20c4/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py?ref=9b479a245b793cac2a8b2e87c6d8e81bb24e20c4",
            "patch": "@@ -345,9 +345,9 @@ def preprocess(self, inputs=None, timeout=None, continue_final_message=None, pro\n         # if batched text inputs, we set padding to True unless specified otherwise\n         if isinstance(text, (list, tuple)) and len(text) > 1:\n             processing_kwargs.setdefault(\"padding\", True)\n-        model_inputs = self.processor(\n-            images=images, text=text, return_tensors=self.framework, legacy=False, **processing_kwargs\n-        ).to(dtype=self.torch_dtype)\n+        model_inputs = self.processor(images=images, text=text, return_tensors=self.framework, **processing_kwargs).to(\n+            dtype=self.torch_dtype\n+        )\n \n         model_inputs[\"text\"] = inputs_text\n "
        },
        {
            "sha": "764c944bac8983cbba4eb65de26d463983e54871",
            "filename": "tests/models/llava_next_video/test_processor_llava_next_video.py",
            "status": "added",
            "additions": 166,
            "deletions": 0,
            "changes": 166,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b479a245b793cac2a8b2e87c6d8e81bb24e20c4/tests%2Fmodels%2Fllava_next_video%2Ftest_processor_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b479a245b793cac2a8b2e87c6d8e81bb24e20c4/tests%2Fmodels%2Fllava_next_video%2Ftest_processor_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_processor_llava_next_video.py?ref=9b479a245b793cac2a8b2e87c6d8e81bb24e20c4",
            "patch": "@@ -0,0 +1,166 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import json\n+import shutil\n+import tempfile\n+import unittest\n+\n+from transformers import AutoProcessor, LlamaTokenizerFast, LlavaNextVideoProcessor\n+from transformers.testing_utils import require_av, require_torch, require_vision\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_vision_available():\n+    from transformers import LlavaNextImageProcessor, LlavaNextVideoImageProcessor\n+\n+if is_torch_available:\n+    import torch\n+\n+\n+@require_vision\n+class LlavaNextVideoProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = LlavaNextVideoProcessor\n+\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+        image_processor = LlavaNextImageProcessor()\n+        video_processor = LlavaNextVideoImageProcessor()\n+        tokenizer = LlamaTokenizerFast.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n+        processor_kwargs = self.prepare_processor_dict()\n+\n+        processor = LlavaNextVideoProcessor(\n+            video_processor=video_processor, image_processor=image_processor, tokenizer=tokenizer, **processor_kwargs\n+        )\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    def get_video_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).video_processor\n+\n+    def prepare_processor_dict(self):\n+        return {\n+            \"chat_template\": \"{% for message in messages %}{{'<|im_start|>' + message['role'] + ' '}}{# Render all images first #}{% for content in message['content'] | selectattr('type', 'equalto', 'image') %}{{ '<image>' }}{% endfor %}{# Render all video then #}{% for content in message['content'] | selectattr('type', 'equalto', 'video') %}{{ '<video>' }}{% endfor %}{# Render all text next #}{% if message['role'] != 'assistant' %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{{ '\\n' + content['text'] }}{% endfor %}{% else %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{% generation %}{{ '\\n' + content['text'] }}{% endgeneration %}{% endfor %}{% endif %}{{'<|im_end|>'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\",\n+            \"num_additional_image_tokens\": 6,\n+            \"patch_size\": 4,\n+            \"vision_feature_select_strategy\": \"default\",\n+        }\n+\n+    def test_processor_to_json_string(self):\n+        processor = self.get_processor()\n+        obj = json.loads(processor.to_json_string())\n+        for key, value in self.prepare_processor_dict().items():\n+            # chat_tempalate are tested as a separate test because they are saved in separate files\n+            if key != \"chat_template\":\n+                self.assertEqual(obj[key], value)\n+                self.assertEqual(getattr(processor, key, None), value)\n+\n+    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_chat_template_is_saved\n+    def test_chat_template_is_saved(self):\n+        processor_loaded = self.processor_class.from_pretrained(self.tmpdirname)\n+        processor_dict_loaded = json.loads(processor_loaded.to_json_string())\n+        # chat templates aren't serialized to json in processors\n+        self.assertFalse(\"chat_template\" in processor_dict_loaded.keys())\n+\n+        # they have to be saved as separate file and loaded back from that file\n+        # so we check if the same template is loaded\n+        processor_dict = self.prepare_processor_dict()\n+        self.assertTrue(processor_loaded.chat_template == processor_dict.get(\"chat_template\", None))\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n+\n+    def test_chat_template(self):\n+        processor = AutoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n+        expected_prompt = \"USER: <image>\\nWhat is shown in this image? ASSISTANT:\"\n+\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                ],\n+            },\n+        ]\n+\n+        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n+        self.assertEqual(expected_prompt, formatted_prompt)\n+\n+    @require_av\n+    def test_chat_template_dict(self):\n+        processor = AutoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"video\"},\n+                    {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n+                ],\n+            },\n+        ]\n+\n+        formatted_prompt_tokenized = processor.apply_chat_template(\n+            messages, add_generation_prompt=True, tokenize=True, return_tensors=None\n+        )\n+        expected_output = [[1, 3148, 1001, 29901, 29871, 32000, 13, 5618, 338, 4318, 297, 445, 4863, 29973, 319, 1799, 9047, 13566, 29901]]  # fmt: skip\n+        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n+\n+        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n+        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n+\n+        # add image URL for return dict\n+        messages[0][\"content\"][0] = {\n+            \"type\": \"video\",\n+            \"url\": \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\",\n+        }\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages, add_generation_prompt=True, tokenize=True, return_dict=True\n+        )\n+        self.assertListEqual(list(out_dict_with_video.keys()), [\"input_ids\", \"attention_mask\", \"pixel_values_videos\"])\n+\n+    @require_torch\n+    @require_av\n+    def test_chat_template_dict_torch(self):\n+        processor = AutoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"video\",\n+                        \"url\": \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n+                ],\n+            },\n+        ]\n+\n+        out_dict_tensors = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+        )\n+        self.assertListEqual(list(out_dict_tensors.keys()), [\"input_ids\", \"attention_mask\", \"pixel_values_videos\"])\n+        self.assertTrue(isinstance(out_dict_tensors[\"input_ids\"], torch.Tensor))"
        },
        {
            "sha": "3552439aeaa2b13bfd5206909e1f7c8eb7626487",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 202,
            "deletions": 7,
            "changes": 209,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b479a245b793cac2a8b2e87c6d8e81bb24e20c4/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b479a245b793cac2a8b2e87c6d8e81bb24e20c4/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=9b479a245b793cac2a8b2e87c6d8e81bb24e20c4",
            "patch": "@@ -126,11 +126,12 @@ def prepare_image_inputs(self, batch_size: Optional[int] = None):\n         return prepare_image_inputs() * batch_size\n \n     @require_vision\n-    def prepare_video_inputs(self):\n+    def prepare_video_inputs(self, batch_size: Optional[int] = None):\n         \"\"\"This function prepares a list of numpy videos.\"\"\"\n         video_input = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)] * 8\n-        image_inputs = [video_input] * 3  # batch-size=3\n-        return image_inputs\n+        if batch_size is None:\n+            return video_input\n+        return [video_input] * batch_size\n \n     def test_processor_to_json_string(self):\n         processor = self.get_processor()\n@@ -491,6 +492,192 @@ def test_structured_kwargs_audio_nested(self):\n         elif \"labels\" in inputs:\n             self.assertEqual(len(inputs[\"labels\"][0]), 76)\n \n+    def test_tokenizer_defaults_preserved_by_kwargs_video(self):\n+        if \"video_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"video_processor attribute not present in {self.processor_class}\")\n+        processor_components = self.prepare_components()\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+        processor_kwargs = self.prepare_processor_dict()\n+\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = self.prepare_text_inputs()\n+        video_input = self.prepare_video_inputs()\n+        inputs = processor(text=input_str, videos=video_input, return_tensors=\"pt\")\n+        self.assertEqual(inputs[self.text_input_name].shape[-1], 117)\n+\n+    def test_video_processor_defaults_preserved_by_video_kwargs(self):\n+        \"\"\"\n+        We use do_rescale=True, rescale_factor=-1 to ensure that image_processor kwargs are preserved in the processor.\n+        We then check that the mean of the pixel_values is less than or equal to 0 after processing.\n+        Since the original pixel_values are in [0, 255], this is a good indicator that the rescale_factor is indeed applied.\n+        \"\"\"\n+        if \"video_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"video_processor attribute not present in {self.processor_class}\")\n+        processor_components = self.prepare_components()\n+        processor_components[\"video_processor\"] = self.get_component(\n+            \"video_processor\", do_rescale=True, rescale_factor=-1\n+        )\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+        processor_kwargs = self.prepare_processor_dict()\n+\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = self.prepare_text_inputs()\n+        video_input = self.prepare_video_inputs()\n+\n+        inputs = processor(text=input_str, videos=video_input, return_tensors=\"pt\")\n+        self.assertLessEqual(inputs[self.videos_input_name][0][0][0].mean(), 0)\n+\n+    def test_kwargs_overrides_default_tokenizer_kwargs_video(self):\n+        if \"video_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"video_processor attribute not present in {self.processor_class}\")\n+        processor_components = self.prepare_components()\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", padding=\"longest\")\n+        processor_kwargs = self.prepare_processor_dict()\n+\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = self.prepare_text_inputs()\n+        video_input = self.prepare_video_inputs()\n+        inputs = processor(\n+            text=input_str, videos=video_input, return_tensors=\"pt\", max_length=112, padding=\"max_length\"\n+        )\n+        self.assertEqual(inputs[self.text_input_name].shape[-1], 112)\n+\n+    def test_kwargs_overrides_default_video_processor_kwargs(self):\n+        if \"video_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"video_processor attribute not present in {self.processor_class}\")\n+        processor_components = self.prepare_components()\n+        processor_components[\"video_processor\"] = self.get_component(\n+            \"video_processor\", do_rescale=True, rescale_factor=1\n+        )\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+        processor_kwargs = self.prepare_processor_dict()\n+\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = self.prepare_text_inputs()\n+        video_input = self.prepare_video_inputs()\n+\n+        inputs = processor(text=input_str, videos=video_input, do_rescale=True, rescale_factor=-1, return_tensors=\"pt\")\n+        self.assertLessEqual(inputs[self.videos_input_name][0][0][0].mean(), 0)\n+\n+    def test_unstructured_kwargs_video(self):\n+        if \"video_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"video_processor attribute not present in {self.processor_class}\")\n+        processor_components = self.prepare_components()\n+        processor_kwargs = self.prepare_processor_dict()\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = self.prepare_text_inputs()\n+        video_input = self.prepare_video_inputs()\n+        inputs = processor(\n+            text=input_str,\n+            videos=video_input,\n+            return_tensors=\"pt\",\n+            do_rescale=True,\n+            rescale_factor=-1,\n+            padding=\"max_length\",\n+            max_length=76,\n+        )\n+\n+        self.assertLessEqual(inputs[self.videos_input_name][0][0][0].mean(), 0)\n+        self.assertEqual(inputs[self.text_input_name].shape[-1], 76)\n+\n+    def test_unstructured_kwargs_batched_video(self):\n+        if \"video_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"video_processor attribute not present in {self.processor_class}\")\n+        processor_components = self.prepare_components()\n+        processor_kwargs = self.prepare_processor_dict()\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = self.prepare_text_inputs(batch_size=2)\n+        video_input = self.prepare_video_inputs(batch_size=2)\n+        inputs = processor(\n+            text=input_str,\n+            videos=video_input,\n+            return_tensors=\"pt\",\n+            do_rescale=True,\n+            rescale_factor=-1,\n+            padding=\"longest\",\n+            max_length=76,\n+        )\n+\n+        self.assertLessEqual(inputs[self.videos_input_name][0][0][0].mean(), 0)\n+        self.assertTrue(\n+            len(inputs[self.text_input_name][0]) == len(inputs[self.text_input_name][1])\n+            and len(inputs[self.text_input_name][1]) < 76\n+        )\n+\n+    def test_doubly_passed_kwargs_video(self):\n+        if \"video_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"video_processor attribute not present in {self.processor_class}\")\n+        processor_components = self.prepare_components()\n+        processor_kwargs = self.prepare_processor_dict()\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = [self.prepare_text_inputs()]\n+        video_input = self.prepare_video_inputs()\n+        with self.assertRaises(ValueError):\n+            _ = processor(\n+                text=input_str,\n+                videos=video_input,\n+                videos_kwargs={\"do_rescale\": True, \"rescale_factor\": -1},\n+                do_rescale=True,\n+                return_tensors=\"pt\",\n+            )\n+\n+    def test_structured_kwargs_nested_video(self):\n+        if \"video_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"video_processor attribute not present in {self.processor_class}\")\n+        processor_components = self.prepare_components()\n+        processor_kwargs = self.prepare_processor_dict()\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = self.prepare_text_inputs()\n+        video_input = self.prepare_video_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"videos_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, videos=video_input, **all_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        self.assertLessEqual(inputs[self.videos_input_name][0][0][0].mean(), 0)\n+        self.assertEqual(inputs[self.text_input_name].shape[-1], 76)\n+\n+    def test_structured_kwargs_nested_from_dict_video(self):\n+        if \"video_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"video_processor attribute not present in {self.processor_class}\")\n+        processor_components = self.prepare_components()\n+        processor_kwargs = self.prepare_processor_dict()\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = self.prepare_text_inputs()\n+        video_input = self.prepare_video_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"videos_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, videos=video_input, **all_kwargs)\n+        self.assertLessEqual(inputs[self.videos_input_name][0][0][0].mean(), 0)\n+        self.assertEqual(inputs[self.text_input_name].shape[-1], 76)\n+\n     # TODO: the same test, but for audio + text processors that have strong overlap in kwargs\n     # TODO (molbap) use the same structure of attribute kwargs for other tests to avoid duplication\n     def test_overlapping_text_kwargs_handling(self):\n@@ -584,7 +771,9 @@ def test_chat_template_single(self):\n         formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n         self.assertEqual(len(formatted_prompt), 1)\n \n-        formatted_prompt_tokenized = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)\n+        formatted_prompt_tokenized = processor.apply_chat_template(\n+            messages, add_generation_prompt=True, tokenize=True, return_tensors=None\n+        )\n         add_special_tokens = True\n         if processor.tokenizer.bos_token is not None and formatted_prompt[0].startswith(processor.tokenizer.bos_token):\n             add_special_tokens = False\n@@ -636,7 +825,7 @@ def test_chat_template_batched(self):\n         self.assertEqual(len(formatted_prompt), 2)\n \n         formatted_prompt_tokenized = processor.apply_chat_template(\n-            batched_messages, add_generation_prompt=True, tokenize=True, padding=True\n+            batched_messages, add_generation_prompt=True, tokenize=True, padding=True, return_tensors=None\n         )\n         add_special_tokens = True\n         if processor.tokenizer.bos_token is not None and formatted_prompt[0].startswith(processor.tokenizer.bos_token):\n@@ -650,7 +839,11 @@ def test_chat_template_batched(self):\n         self.assertListEqual(expected_output, formatted_prompt_tokenized)\n \n         out_dict = processor.apply_chat_template(\n-            batched_messages, add_generation_prompt=True, tokenize=True, return_dict=True, padding=True\n+            batched_messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            padding=True,\n         )\n         self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n \n@@ -775,7 +968,9 @@ def test_chat_template_video(self):\n         formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n         self.assertEqual(len(formatted_prompt), 1)\n \n-        formatted_prompt_tokenized = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)\n+        formatted_prompt_tokenized = processor.apply_chat_template(\n+            messages, add_generation_prompt=True, tokenize=True, return_tensors=None\n+        )\n         add_special_tokens = True\n         if processor.tokenizer.bos_token is not None and formatted_prompt[0].startswith(processor.tokenizer.bos_token):\n             add_special_tokens = False"
        }
    ],
    "stats": {
        "total": 447,
        "additions": 402,
        "deletions": 45
    }
}