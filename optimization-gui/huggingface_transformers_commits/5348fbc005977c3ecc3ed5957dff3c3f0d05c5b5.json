{
    "author": "Cyrilvallez",
    "message": "[modular] Follow global indexing and attribute setting, and their dependencies (#39180)\n\n* export global indexing statements\n\n* add example\n\n* style\n\n* examples",
    "sha": "5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5",
    "files": [
        {
            "sha": "1e462c958295f65d054feb1ee2133c476c91530f",
            "filename": "examples/modular-transformers/modeling_global_indexing.py",
            "status": "added",
            "additions": 169,
            "deletions": 0,
            "changes": 169,
            "blob_url": "https://github.com/huggingface/transformers/blob/5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5/examples%2Fmodular-transformers%2Fmodeling_global_indexing.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5/examples%2Fmodular-transformers%2Fmodeling_global_indexing.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_global_indexing.py?ref=5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5",
            "patch": "@@ -0,0 +1,169 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from examples/modular-transformers/modular_global_indexing.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_global_indexing.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+from typing import Callable, Optional\n+\n+import torch\n+from torch import nn\n+\n+from transformers.modeling_utils import AttentionInterface\n+\n+from ...cache_utils import Cache\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs\n+from .configuration_global_indexing import GlobalIndexingConfig\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+def custom_flex(x, **kwargs):\n+    \"\"\"Dummy function.\"\"\"\n+    return x\n+\n+\n+ALL_ATTENTION_FUNCTIONS = AttentionInterface()\n+# This indexing statement and associated function should be exported correctly!\n+ALL_ATTENTION_FUNCTIONS[\"flex_attention\"] = custom_flex\n+\n+\n+class GlobalIndexingAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: GlobalIndexingConfig, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights"
        },
        {
            "sha": "64264ca30bd978a4aab2c6a7a76dfeee62e3e0d5",
            "filename": "examples/modular-transformers/modeling_multimodal2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py?ref=5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5",
            "patch": "@@ -289,7 +289,6 @@ def __init__(self, config):\n         self.layers = nn.ModuleList([Multimodal2VisionEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n-    @can_return_tuple\n     def forward(\n         self,\n         inputs_embeds,\n@@ -455,7 +454,6 @@ def __init__(self, config):\n         self.encoder = Multimodal2VisionEncoder(config)\n         self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n \n-    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "58bce5227532a69febd8ca7d17b5cfd6518b8f59",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 19,
            "deletions": 59,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5",
            "patch": "@@ -12,13 +12,13 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...masking_utils import create_causal_mask\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, SequenceClassifierOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, can_return_tuple, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.generic import check_model_inputs\n from .configuration_my_new_model2 import MyNewModel2Config\n \n \n@@ -149,7 +149,7 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: float,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     key_states = repeat_kv(key, module.num_key_value_groups)\n     value_states = repeat_kv(value, module.num_key_value_groups)\n@@ -200,8 +200,8 @@ def forward(\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -254,22 +254,19 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor]:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n-\n         # Self Attention\n-        hidden_states, self_attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_value=past_key_value,\n-            output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -282,12 +279,7 @@ def forward(\n         hidden_states = self.post_attention_layernorm(hidden_states)\n         hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n-\n-        outputs = (hidden_states,)\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n @auto_docstring\n@@ -304,6 +296,10 @@ class MyNewModel2PreTrainedModel(PreTrainedModel):\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n     _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": MyNewModel2DecoderLayer,\n+        \"attentions\": MyNewModel2Attention,\n+    }\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -343,7 +339,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n-    @can_return_tuple\n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n@@ -353,26 +349,12 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPast:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training and use_cache:\n-            logger.warning_once(\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n-            )\n-            use_cache = False\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n@@ -394,6 +376,7 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         # embed positions\n@@ -408,42 +391,21 @@ def forward(\n         normalizer = torch.tensor(self.config.hidden_size**0.5, dtype=hidden_states.dtype)\n         hidden_states = hidden_states * normalizer\n \n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n-            layer_outputs = decoder_layer(\n+            hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n                 past_key_value=past_key_values,\n-                output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n-\n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n-\n         hidden_states = self.norm(hidden_states)\n-\n-        # add hidden states from the last decoder layer\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n         )\n \n \n@@ -488,8 +450,7 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -505,8 +466,7 @@ def forward(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n         hidden_states = transformer_outputs.last_hidden_state\n         logits = self.score(hidden_states)"
        },
        {
            "sha": "15865a2c167719e46c86c5d315fce178436a956e",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5",
            "patch": "@@ -118,6 +118,8 @@ def _init_weights(self, module):\n )\n class NewTaskModelModel(NewTaskModelPreTrainedModel):\n     _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n+    # we are filtering the logits/labels so we shouldn't divide the loss based on num_items_in_batch\n+    accepts_loss_kwargs = False\n \n     def __init__(self, config: NewTaskModelConfig):\n         super().__init__(config)\n@@ -313,9 +315,11 @@ def forward(\n                 special_image_mask = inputs_embeds == self.get_input_embeddings()(\n                     torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n+                special_image_mask = special_image_mask.all(-1)\n             else:\n-                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-                special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n \n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n                 image_tokens_in_text = (special_image_mask).sum(dim=1).sum(dim=0)[0]"
        },
        {
            "sha": "6bc5faf78e0b857eade89725c540e041d728f5c5",
            "filename": "examples/modular-transformers/modeling_super.py",
            "status": "modified",
            "additions": 14,
            "deletions": 18,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_super.py?ref=5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5",
            "patch": "@@ -14,12 +14,12 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...integrations import use_kernel_forward_from_hub\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, can_return_tuple\n+from ...utils import TransformersKwargs, auto_docstring\n+from ...utils.generic import check_model_inputs\n from .configuration_super import SuperConfig\n \n \n@@ -148,7 +148,7 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: float,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     key_states = repeat_kv(key, module.num_key_value_groups)\n     value_states = repeat_kv(value, module.num_key_value_groups)\n@@ -199,8 +199,8 @@ def forward(\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -253,22 +253,19 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor]:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n-\n         # Self Attention\n-        hidden_states, self_attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_value=past_key_value,\n-            output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -281,12 +278,7 @@ def forward(\n         hidden_states = self.post_attention_layernorm(hidden_states)\n         hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n-\n-        outputs = (hidden_states,)\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n @auto_docstring\n@@ -303,6 +295,10 @@ class SuperPreTrainedModel(PreTrainedModel):\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n     _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": SuperDecoderLayer,\n+        \"attentions\": SuperAttention,\n+    }\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -342,7 +338,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n-    @can_return_tuple\n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "6b443d3411f31be391ad998059b0b5068c28173b",
            "filename": "examples/modular-transformers/modeling_switch_function.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5/examples%2Fmodular-transformers%2Fmodeling_switch_function.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5/examples%2Fmodular-transformers%2Fmodeling_switch_function.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_switch_function.py?ref=5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5",
            "patch": "@@ -11,9 +11,9 @@\n from torch import nn\n \n from ...cache_utils import Cache\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs\n from .configuration_switch_function import SwitchFunctionConfig\n \n \n@@ -72,7 +72,7 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: float,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     key_states = repeat_kv(key, module.num_key_value_groups)\n     value_states = repeat_kv(value, module.num_key_value_groups)\n@@ -123,8 +123,8 @@ def forward(\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "1bd94682c2c02e91cfe62448db630d650a125f91",
            "filename": "examples/modular-transformers/modular_global_indexing.py",
            "status": "added",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5/examples%2Fmodular-transformers%2Fmodular_global_indexing.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5/examples%2Fmodular-transformers%2Fmodular_global_indexing.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_global_indexing.py?ref=5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5",
            "patch": "@@ -0,0 +1,16 @@\n+from transformers.modeling_utils import AttentionInterface\n+from transformers.models.llama.modeling_llama import LlamaAttention\n+\n+\n+def custom_flex(x, **kwargs):\n+    \"\"\"Dummy function.\"\"\"\n+    return x\n+\n+\n+ALL_ATTENTION_FUNCTIONS = AttentionInterface()\n+# This indexing statement and associated function should be exported correctly!\n+ALL_ATTENTION_FUNCTIONS[\"flex_attention\"] = custom_flex\n+\n+\n+class GlobalIndexingAttention(LlamaAttention):\n+    pass"
        },
        {
            "sha": "187fb60afb503bb1b39d864d898c831dc798c5ce",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=5348fbc005977c3ecc3ed5957dff3c3f0d05c5b5",
            "patch": "@@ -673,11 +673,24 @@ def visit_SimpleStatementLine(self, node):\n         simple_top_level_assign_structure = m.SimpleStatementLine(\n             body=[m.Assign(targets=[m.AssignTarget(target=m.Name())])]\n         )\n+        simple_top_level_variable_indexing = m.SimpleStatementLine(\n+            body=[m.Assign(targets=[m.AssignTarget(target=m.Subscript(value=m.Name()) | m.Attribute(value=m.Name()))])]\n+        )\n+\n         if m.matches(parent_node, m.Module()):\n             if m.matches(node, simple_top_level_assign_structure):\n                 left_hand_side = node.body[0].targets[0].target.value\n                 self.current_assignment = left_hand_side\n                 self.assignments[left_hand_side] = node\n+            # This corresponds to a global variable being indexed or having an attribute look-up\n+            elif m.matches(node, simple_top_level_variable_indexing):\n+                indexed_variable = node.body[0].targets[0].target.value.value\n+                # We should follow any dependencies relative to the variable being indexed\n+                self.current_assignment = indexed_variable\n+                # The indexing node should be directly added as a dependency of the indexed variable (register the node with a \"fake\" name)\n+                node_name = self.python_module.code_for_node(node)\n+                self.assignments[node_name] = node\n+                self.object_dependency_mapping[indexed_variable].add(node_name)\n             elif m.matches(node, m.SimpleStatementLine(body=[m.Import() | m.ImportFrom()])):\n                 self.imports.append(node)\n \n@@ -1315,6 +1328,10 @@ def visit_SimpleStatementLine(self, node):\n         simple_top_level_assign_structure = m.SimpleStatementLine(\n             body=[m.Assign(targets=[m.AssignTarget(target=m.Name())])]\n         )\n+        simple_top_level_variable_indexing = m.SimpleStatementLine(\n+            body=[m.Assign(targets=[m.AssignTarget(target=m.Subscript(value=m.Name()) | m.Attribute(value=m.Name()))])]\n+        )\n+\n         if m.matches(parent_node, m.Module()):\n             if m.matches(node, m.SimpleStatementLine(body=[m.Import()])):\n                 self.imports.append(node)\n@@ -1334,6 +1351,15 @@ def visit_SimpleStatementLine(self, node):\n                 else:\n                     self.current_assignment = assigned_variable\n                     self.assignments[assigned_variable] = node\n+            # This corresponds to a global variable being indexed or having an attribute look-up\n+            elif m.matches(node, simple_top_level_variable_indexing):\n+                indexed_variable = node.body[0].targets[0].target.value.value\n+                # We should follow any dependencies relative to the variable being indexed\n+                self.current_assignment = indexed_variable\n+                # The indexing node should be directly added as a dependency of the indexed variable (register the node with a \"fake\" name)\n+                node_name = self.python_module.code_for_node(node)\n+                self.assignments[node_name] = node\n+                self.object_dependency_mapping[indexed_variable].add(node_name)\n \n     def leave_Module(self, node):\n         \"\"\"When we leave the modular file, we do the following in order:"
        }
    ],
    "stats": {
        "total": 339,
        "additions": 254,
        "deletions": 85
    }
}