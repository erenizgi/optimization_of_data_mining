{
    "author": "jbn",
    "message": "Make siglip examples clearer and error free (#33667)\n\nUpdate siglip.md\r\n\r\nThis was already partially fixed relative to the deployed docs. But the partial fix made it inconsistent. Additionally, giving the full text (\"This is a photo of...\") is likely not the desired output.",
    "sha": "55b7a0404ea6bfc5f9197b1042a57304af05eb92",
    "files": [
        {
            "sha": "88e38cbb590edcbbf245d4fcd7ec6c49151a2744",
            "filename": "docs/source/en/model_doc/siglip.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b7a0404ea6bfc5f9197b1042a57304af05eb92/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b7a0404ea6bfc5f9197b1042a57304af05eb92/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip.md?ref=55b7a0404ea6bfc5f9197b1042a57304af05eb92",
            "patch": "@@ -85,7 +85,7 @@ If you want to do the pre- and postprocessing yourself, here's how to do that:\n \n >>> candidate_labels = [\"2 cats\", \"2 dogs\"]\n # follows the pipeline prompt template to get same results\n->>> candidate_labels = [f'This is a photo of {label}.' for label in candidate_labels]\n+>>> texts = [f'This is a photo of {label}.' for label in candidate_labels]\n >>> # important: we pass `padding=max_length` since the model was trained with this\n >>> inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\")\n \n@@ -94,7 +94,7 @@ If you want to do the pre- and postprocessing yourself, here's how to do that:\n \n >>> logits_per_image = outputs.logits_per_image\n >>> probs = torch.sigmoid(logits_per_image) # these are the probabilities\n->>> print(f\"{probs[0][0]:.1%} that image 0 is '{texts[0]}'\")\n+>>> print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n 31.9% that image 0 is 'a photo of 2 cats'\n ```\n \n@@ -140,9 +140,9 @@ To load and run a model using Flash Attention 2, refer to the snippet below:\n \n >>> candidate_labels = [\"2 cats\", \"2 dogs\"]\n # follows the pipeline prompt template to get same results\n->>> candidate_labels = [f'This is a photo of {label}.' for label in candidate_labels]\n+>>> texts = [f'This is a photo of {label}.' for label in candidate_labels]\n # important: we pass `padding=max_length` since the model was trained with this\n->>> inputs = processor(text=candidate_labels, images=image, padding=\"max_length\", return_tensors=\"pt\")\n+>>> inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\")\n >>> inputs.to(device)\n \n >>> with torch.no_grad():\n@@ -240,4 +240,4 @@ Below is an expected speedup diagram that compares inference time between the na\n ## SiglipForImageClassification\n \n [[autodoc]] SiglipForImageClassification\n-    - forward\n\\ No newline at end of file\n+    - forward"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 5,
        "deletions": 5
    }
}