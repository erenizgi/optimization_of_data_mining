{
    "author": "ydshieh",
    "message": "Update after #41007 (#41014)\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "8076e755e532dcf4083fc7a869be8d3418b9078c",
    "files": [
        {
            "sha": "ba6b1d50be71d6d21fb1672904dd08d263a02fa0",
            "filename": "tests/models/phimoe/test_modeling_phimoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 18,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/8076e755e532dcf4083fc7a869be8d3418b9078c/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8076e755e532dcf4083fc7a869be8d3418b9078c/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py?ref=8076e755e532dcf4083fc7a869be8d3418b9078c",
            "patch": "@@ -14,7 +14,6 @@\n \n \"\"\"Testing suite for the PyTorch PhiMoE model.\"\"\"\n \n-import copy\n import unittest\n \n from parameterized import parameterized\n@@ -59,6 +58,7 @@ def forward(\n                 past_key_values=self.cache,\n             ).logits\n \n+        @torch.no_grad()\n         @staticmethod\n         def generate(model: PhimoeForCausalLM, prompt_tokens: torch.LongTensor, max_seq_len: int) -> list[int]:\n             model = PhimoeMiniWithStaticCache(model, 1, max_seq_len + prompt_tokens.shape[-1])\n@@ -194,19 +194,6 @@ def test_phimoe_instruct_generation(self):\n \n     def test_phimoe_instruct_with_static_cache(self):\n         model = self.get_model()\n-        # Can't run with the real checkpoint, even if offloaded. Let's just use a tiny dummy one\n-        config = copy.deepcopy(model.config)\n-        config.num_hidden_layers = 2\n-        # make `head_dim = 128`\n-        config.hidden_size = 512\n-        config.num_attention_heads = 4\n-        config.num_key_value_heads = 1\n-        config.intermediate_size = 512\n-        config.max_position_embeddinqgs = 64\n-        config.num_local_experts = 4\n-        torch.manual_seed(42)\n-        model = PhimoeForCausalLM(config).to(torch_device)\n-        model.eval()\n         tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-MoE-instruct\")\n \n         messages = [\n@@ -221,12 +208,9 @@ def test_phimoe_instruct_with_static_cache(self):\n         )\n \n         response_tokens = PhimoeMiniWithStaticCache.generate(model, inputs, max_seq_len=30)\n-\n         output_text = tokenizer.batch_decode(torch.tensor([response_tokens], dtype=torch.long, device=torch_device))\n \n-        # This is dummy outputs. We actually check if it could run with static cache, not the output quality.\n         EXPECTED_OUTPUT = [\n-            \"<|system|> You are a helpful digital assistant. Please provide safe, ethical and accurate information to the user.<|end|><|user|> Can you provide ways to eat combinations of bananas and dragonfruits?<|end|><|assistant|> awards\"\n+            \"<|system|> You are a helpful digital assistant. Please provide safe, ethical and accurate information to the user.<|end|><|user|> Can you provide ways to eat combinations of bananas and dragonfruits?<|end|><|assistant|> C\"\n         ]\n-\n         self.assertListEqual(output_text, EXPECTED_OUTPUT)"
        }
    ],
    "stats": {
        "total": 20,
        "additions": 2,
        "deletions": 18
    }
}