{
    "author": "unknown",
    "message": "Don't import torch.distributed when it's not available (#35777)\n\nThis is a continuation of 217c47e31bc0cd442443e5b4a62c8bc2785d53ee but\r\nfor another module. This issue was spotted in nixpkgs (again) when\r\nbuilding lm-eval package that used a different path in transformers\r\nlibrary to reach the same failure.\r\n\r\nRelated: #35133",
    "sha": "ec97417827471351c86cf655ee94d38ddf5bfa3f",
    "files": [
        {
            "sha": "332231c04f433f4ebd1364347c5cbce62dd498e6",
            "filename": "src/transformers/integrations/fsdp.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ec97417827471351c86cf655ee94d38ddf5bfa3f/src%2Ftransformers%2Fintegrations%2Ffsdp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ec97417827471351c86cf655ee94d38ddf5bfa3f/src%2Ftransformers%2Fintegrations%2Ffsdp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffsdp.py?ref=ec97417827471351c86cf655ee94d38ddf5bfa3f",
            "patch": "@@ -26,6 +26,11 @@ def is_fsdp_managed_module(module: nn.Module) -> bool:\n     if not is_torch_available():\n         return False\n \n+    import torch\n+\n+    if not torch.distributed.is_available():\n+        return False\n+\n     import torch.distributed.fsdp\n \n     return isinstance(module, torch.distributed.fsdp.FullyShardedDataParallel) or getattr("
        }
    ],
    "stats": {
        "total": 5,
        "additions": 5,
        "deletions": 0
    }
}