{
    "author": "laurentd-lunit",
    "message": "[feat] LlavaNext add feature size check to avoid CUDA Runtime Error (#33608)\n\n* [feat] add feature size check to avoid CUDA Runtime Error\r\n\r\n* [minor] add error handling to all llava models\r\n\r\n* [minor] avoid nested if else\r\n\r\n* [minor] add error message to Qwen2-vl and chameleon\r\n\r\n* [fix] token dimension for check\r\n\r\n* [minor] add feature dim check for videos too\r\n\r\n* [fix] dimension check\r\n\r\n* [fix] test reference values\r\n\r\n---------\r\n\r\nCo-authored-by: Raushan Turganbay <raushan@huggingface.co>",
    "sha": "0f49deacbff3e57cde45222842c0db6375e4fa43",
    "files": [
        {
            "sha": "20dbfc317e133d4168d2b45f665b0f8e1f7ce347",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f49deacbff3e57cde45222842c0db6375e4fa43/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f49deacbff3e57cde45222842c0db6375e4fa43/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=0f49deacbff3e57cde45222842c0db6375e4fa43",
            "patch": "@@ -1287,6 +1287,12 @@ def forward(\n \n         if pixel_values is not None:\n             image_tokens = self.get_image_tokens(pixel_values)\n+            n_image_tokens_in_text = (input_ids == self.vocabulary_mapping.image_token_id).sum().item()\n+            n_image_features = image_tokens.shape[0]\n+            if n_image_tokens_in_text != n_image_features:\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens_in_text}, features {n_image_features}\"\n+                )\n             special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n             image_tokens = image_tokens.to(input_ids.device, input_ids.dtype)\n             input_ids = input_ids.masked_scatter(special_image_mask, image_tokens)"
        },
        {
            "sha": "411b96f5c57a50a089487e8903ea58b1e4bc77c5",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f49deacbff3e57cde45222842c0db6375e4fa43/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f49deacbff3e57cde45222842c0db6375e4fa43/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=0f49deacbff3e57cde45222842c0db6375e4fa43",
            "patch": "@@ -518,6 +518,12 @@ def forward(\n \n             # TODO: @raushan retain only the new behavior after v4.47\n             else:\n+                n_image_tokens = (input_ids == self.config.image_token_index).sum(dim=-1)[0].item()\n+                n_image_features = image_features.shape[1]\n+                if n_image_tokens != n_image_features:\n+                    raise ValueError(\n+                        f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                    )\n                 special_image_mask = (\n                     (input_ids == self.config.image_token_index)\n                     .unsqueeze(-1)"
        },
        {
            "sha": "75dfcf5393ea15dd9f42872753bb08f3dbf6e0ad",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f49deacbff3e57cde45222842c0db6375e4fa43/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f49deacbff3e57cde45222842c0db6375e4fa43/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=0f49deacbff3e57cde45222842c0db6375e4fa43",
            "patch": "@@ -895,6 +895,12 @@ def forward(\n \n             # TODO: @raushan retain only the new behavior after v4.47\n             else:\n+                n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n+                n_image_features = image_features.shape[0]\n+                if n_image_tokens != n_image_features:\n+                    raise ValueError(\n+                        f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                    )\n                 special_image_mask = (\n                     (input_ids == self.config.image_token_index)\n                     .unsqueeze(-1)"
        },
        {
            "sha": "30257b84397814bc80d3410bc3800918a4f52102",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f49deacbff3e57cde45222842c0db6375e4fa43/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f49deacbff3e57cde45222842c0db6375e4fa43/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=0f49deacbff3e57cde45222842c0db6375e4fa43",
            "patch": "@@ -967,6 +967,12 @@ def forward(\n         # TODO: @raushan retain only the new behavior after v4.47\n         else:\n             if image_features is not None:\n+                n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n+                n_image_features = image_features.shape[0]\n+                if n_image_tokens != n_image_features:\n+                    raise ValueError(\n+                        f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                    )\n                 special_image_mask = (\n                     (input_ids == self.config.image_token_index)\n                     .unsqueeze(-1)\n@@ -976,6 +982,12 @@ def forward(\n                 image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n                 inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n             if video_features is not None:\n+                n_video_tokens = (input_ids == self.config.video_token_index).sum().item()\n+                n_video_features = video_features.shape[0]\n+                if n_video_tokens != n_video_features:\n+                    raise ValueError(\n+                        f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n+                    )\n                 special_image_mask = (\n                     (input_ids == self.config.video_token_index)\n                     .unsqueeze(-1)"
        },
        {
            "sha": "e7de66de444af79c236f79f405bb4554e72dd128",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f49deacbff3e57cde45222842c0db6375e4fa43/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f49deacbff3e57cde45222842c0db6375e4fa43/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=0f49deacbff3e57cde45222842c0db6375e4fa43",
            "patch": "@@ -482,6 +482,12 @@ def forward(\n         # TODO: @raushan retain only the new behavior after v4.47\n         else:\n             if image_features is not None:\n+                n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n+                n_image_features = image_features.shape[0]\n+                if n_image_tokens != n_image_features:\n+                    raise ValueError(\n+                        f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                    )\n                 special_image_mask = (\n                     (input_ids == self.config.image_token_index)\n                     .unsqueeze(-1)\n@@ -491,6 +497,12 @@ def forward(\n                 image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n                 inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n             if video_features is not None:\n+                n_video_tokens = (input_ids == self.config.video_token_index).sum().item()\n+                n_video_features = video_features.shape[0]\n+                if n_video_tokens != n_video_features:\n+                    raise ValueError(\n+                        f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n+                    )\n                 special_image_mask = (\n                     (input_ids == self.config.video_token_index)\n                     .unsqueeze(-1)"
        },
        {
            "sha": "3eefb517b16d9fc242419eee6607f38818628eda",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 12,
            "deletions": 2,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f49deacbff3e57cde45222842c0db6375e4fa43/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f49deacbff3e57cde45222842c0db6375e4fa43/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=0f49deacbff3e57cde45222842c0db6375e4fa43",
            "patch": "@@ -619,7 +619,12 @@ def forward(\n                 image_newline=self.image_newline,\n                 vision_aspect_ratio=vision_aspect_ratio,\n             )\n-\n+            n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n+            n_image_features = image_features.shape[0]\n+            if n_image_tokens != n_image_features:\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n             special_image_mask = (\n                 (input_ids == self.config.image_token_index)\n                 .unsqueeze(-1)\n@@ -647,7 +652,12 @@ def forward(\n             image_newline = self.image_newline[None, None, :].repeat(batch_size, 1, 1).to(video_features.device)\n             video_features = torch.cat((video_features, image_newline), dim=1)\n             video_features = video_features.flatten(0, 1)\n-\n+            n_video_tokens = (input_ids == self.config.video_token_index).sum().item()\n+            n_video_features = video_features.shape[0]\n+            if n_video_tokens != n_video_features:\n+                raise ValueError(\n+                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n+                )\n             special_video_mask = (\n                 (input_ids == self.config.video_token_index)\n                 .unsqueeze(-1)"
        },
        {
            "sha": "e014a6da6bb3bcc528ba76c6ff8b272cca21505e",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f49deacbff3e57cde45222842c0db6375e4fa43/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f49deacbff3e57cde45222842c0db6375e4fa43/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=0f49deacbff3e57cde45222842c0db6375e4fa43",
            "patch": "@@ -1710,6 +1710,12 @@ def forward(\n             if pixel_values is not None:\n                 pixel_values = pixel_values.type(self.visual.get_dtype())\n                 image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum().item()\n+                n_image_features = image_embeds.shape[0]\n+                if n_image_tokens != n_image_features:\n+                    raise ValueError(\n+                        f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                    )\n                 image_mask = (\n                     (input_ids == self.config.image_token_id)\n                     .unsqueeze(-1)\n@@ -1722,6 +1728,12 @@ def forward(\n             if pixel_values_videos is not None:\n                 pixel_values_videos = pixel_values_videos.type(self.visual.get_dtype())\n                 video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)\n+                n_video_tokens = (input_ids == self.config.video_token_id).sum().item()\n+                n_video_features = video_embeds.shape[0]\n+                if n_video_tokens != n_video_features:\n+                    raise ValueError(\n+                        f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n+                    )\n                 video_mask = (\n                     (input_ids == self.config.video_token_id)\n                     .unsqueeze(-1)"
        },
        {
            "sha": "20fa0166b80c9c43531973ba7035d3bdc141acfc",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f49deacbff3e57cde45222842c0db6375e4fa43/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f49deacbff3e57cde45222842c0db6375e4fa43/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=0f49deacbff3e57cde45222842c0db6375e4fa43",
            "patch": "@@ -618,6 +618,12 @@ def forward(\n             # TODO: @raushan retain only the new behavior after v4.47\n             else:\n                 if image_outputs is not None:\n+                    n_image_tokens = (input_ids == self.config.image_token_index).sum(dim=-1)[0].item()\n+                    n_image_features = image_features.shape[1]\n+                    if n_image_tokens != n_image_features:\n+                        raise ValueError(\n+                            f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                        )\n                     special_image_mask = (\n                         (input_ids == self.config.image_token_index)\n                         .unsqueeze(-1)\n@@ -626,8 +632,13 @@ def forward(\n                     )\n                     image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n                     inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n-\n                 if video_outputs is not None:\n+                    n_video_tokens = (input_ids == self.config.video_token_index).sum(dim=-1)[0].item()\n+                    n_video_features = video_features.shape[1]\n+                    if n_video_tokens != n_video_features:\n+                        raise ValueError(\n+                            f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n+                        )\n                     special_image_mask = (\n                         (input_ids == self.config.video_token_index)\n                         .unsqueeze(-1)"
        },
        {
            "sha": "76348228476757f3cdf878cdcbbdb111b19ba72f",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f49deacbff3e57cde45222842c0db6375e4fa43/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f49deacbff3e57cde45222842c0db6375e4fa43/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=0f49deacbff3e57cde45222842c0db6375e4fa43",
            "patch": "@@ -511,6 +511,12 @@ def forward(\n \n             # TODO: @raushan retain only the new behavior after v4.47\n             else:\n+                n_image_tokens = (input_ids == self.config.image_token_index).sum(dim=-1)[0].item()\n+                n_image_features = image_features.shape[1]\n+                if n_image_tokens != n_image_features:\n+                    raise ValueError(\n+                        f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                    )\n                 special_image_mask = (\n                     (input_ids == self.config.image_token_index)\n                     .unsqueeze(-1)"
        },
        {
            "sha": "07415900bb93dbb12a975b13b7565659f31dd13d",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f49deacbff3e57cde45222842c0db6375e4fa43/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f49deacbff3e57cde45222842c0db6375e4fa43/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=0f49deacbff3e57cde45222842c0db6375e4fa43",
            "patch": "@@ -118,8 +118,8 @@ def __init__(\n         self.batch_size = 3\n         self.num_channels = 3\n         self.image_size = 336\n-        self.encoder_seq_length = 231\n-        self.num_image_tokens = 224\n+        self.encoder_seq_length = 232\n+        self.num_image_tokens = 225\n         self.seq_length = seq_length + self.num_image_tokens\n \n     def get_config(self):"
        },
        {
            "sha": "862e144ecdd7d89a13fa612b53e5a7108732e4b0",
            "filename": "tests/models/vipllava/test_modeling_vipllava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f49deacbff3e57cde45222842c0db6375e4fa43/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f49deacbff3e57cde45222842c0db6375e4fa43/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py?ref=0f49deacbff3e57cde45222842c0db6375e4fa43",
            "patch": "@@ -111,8 +111,8 @@ def __init__(\n         self.batch_size = 3\n         self.num_channels = 3\n         self.image_size = 336\n-        self.encoder_seq_length = 231\n-        self.num_image_tokens = 224\n+        self.encoder_seq_length = 232\n+        self.num_image_tokens = 225\n         self.seq_length = seq_length + self.num_image_tokens\n \n     def get_config(self):"
        }
    ],
    "stats": {
        "total": 95,
        "additions": 88,
        "deletions": 7
    }
}