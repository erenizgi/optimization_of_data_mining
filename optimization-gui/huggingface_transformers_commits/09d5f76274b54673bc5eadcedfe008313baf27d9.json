{
    "author": "zucchini-nlp",
    "message": "Clean-up composite configs (#34603)\n\n* remove manual assignment tie-word-embeddings\r\n\r\n* remove another unused attribute\r\n\r\n* fix tests\r\n\r\n* fix tests\r\n\r\n* remove unnecessary overwrites\r\n\r\n* fix\r\n\r\n* decoder=True\r\n\r\n* clean pix2struct\r\n\r\n* run-all\r\n\r\n* forgot `_tied_weights_keys` when adding Emu3\r\n\r\n* also Aria + fix-copies\r\n\r\n* and clean aria",
    "sha": "09d5f76274b54673bc5eadcedfe008313baf27d9",
    "files": [
        {
            "sha": "da0b354fe76efaddb8b9fc7ed362cfdd37fdcf13",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -249,9 +249,6 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n \n-    def tie_weights(self):\n-        return self.language_model.tie_weights()\n-\n     def _update_causal_mask(\n         self,\n         attention_mask,"
        },
        {
            "sha": "3e8d73f94b13ecca80abdd5e06efe39b9f21a0cd",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -1862,7 +1862,7 @@ def tie_weights(self):\n         If the `torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning the\n         weights instead.\n         \"\"\"\n-        if getattr(self.config, \"tie_word_embeddings\", True):\n+        if getattr(self.config.get_text_config(decoder=True), \"tie_word_embeddings\", True):\n             output_embeddings = self.get_output_embeddings()\n             if output_embeddings is not None:\n                 self._tie_or_clone_weights(output_embeddings, self.get_input_embeddings())\n@@ -2104,7 +2104,10 @@ def _resize_token_embeddings(self, new_num_tokens, pad_to_multiple_of=None, mean\n                 new_num_tokens = new_embeddings.weight.shape[0]\n \n         # if word embeddings are not tied, make sure that lm head is resized as well\n-        if self.get_output_embeddings() is not None and not self.config.tie_word_embeddings:\n+        if (\n+            self.get_output_embeddings() is not None\n+            and not self.config.get_text_config(decoder=True).tie_word_embeddings\n+        ):\n             old_lm_head = self.get_output_embeddings()\n             if isinstance(old_lm_head, torch.nn.Embedding):\n                 new_lm_head = self._get_resized_embeddings(old_lm_head, new_num_tokens, mean_resizing=mean_resizing)\n@@ -4604,7 +4607,10 @@ def _load_pretrained_model(\n                     _loaded_keys = loaded_keys\n                 not_initialized_submodules = set_initialized_submodules(model, _loaded_keys)\n                 # If we're about to tie the output embeds to the input embeds we don't need to init them\n-                if hasattr(model.config, \"tie_word_embeddings\") and model.config.tie_word_embeddings:\n+                if (\n+                    hasattr(model.config.get_text_config(decoder=True), \"tie_word_embeddings\")\n+                    and model.config.get_text_config(decoder=True).tie_word_embeddings\n+                ):\n                     output_embeddings = model.get_output_embeddings()\n                     if output_embeddings is not None:\n                         # Still need to initialize if there is a bias term since biases are not tied."
        },
        {
            "sha": "0b330b4aeeda2c7da5210ed6c2daff9391c57333",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -1360,6 +1360,7 @@ class AriaForConditionalGeneration(AriaPreTrainedModel, GenerationMixin):\n     config_class = AriaConfig\n     _supports_flash_attn_2 = False\n     _supports_sdpa = False\n+    _tied_weights_keys = [\"language_model.lm_head.weight\"]\n \n     def __init__(self, config: AriaConfig):\n         super().__init__(config)\n@@ -1406,9 +1407,6 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n \n-    def tie_weights(self):\n-        return self.language_model.tie_weights()\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,"
        },
        {
            "sha": "295e2dcb7465b1ad256edeab85d427fd30bafbb4",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -1337,6 +1337,7 @@ class AriaForConditionalGeneration(AriaPreTrainedModel, GenerationMixin):\n     config_class = AriaConfig\n     _supports_flash_attn_2 = False\n     _supports_sdpa = False\n+    _tied_weights_keys = [\"language_model.lm_head.weight\"]\n \n     def __init__(self, config: AriaConfig):\n         super().__init__(config)\n@@ -1383,9 +1384,6 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n \n-    def tie_weights(self):\n-        return self.language_model.tie_weights()\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,"
        },
        {
            "sha": "3ea3b76878da7c1ccf5934c13f4bc3932fad7358",
            "filename": "src/transformers/models/blip_2/configuration_blip_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -302,9 +302,6 @@ def __init__(\n         text_model_type = text_config[\"model_type\"] if \"model_type\" in text_config else \"opt\"\n         self.text_config = CONFIG_MAPPING[text_model_type](**text_config)\n \n-        self.tie_word_embeddings = self.text_config.tie_word_embeddings\n-        self.is_encoder_decoder = self.text_config.is_encoder_decoder\n-\n         self.num_query_tokens = num_query_tokens\n         self.image_text_hidden_size = image_text_hidden_size\n         self.image_token_index = image_token_index"
        },
        {
            "sha": "01c8f4dcbc9a5b84185b3d54b45886a2811aa65a",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -1433,7 +1433,7 @@ class CLIPTextModelWithProjection(CLIPPreTrainedModel):\n     def __init__(self, config: CLIPTextConfig):\n         super().__init__(config)\n \n-        text_model = CLIPTextModel._from_config(config, attn_implementation=config._attn_implementation)\n+        text_model = CLIPTextModel._from_config(config)\n         self.text_model = text_model.text_model\n \n         self.text_projection = nn.Linear(config.hidden_size, config.projection_dim, bias=False)\n@@ -1514,7 +1514,7 @@ class CLIPVisionModelWithProjection(CLIPPreTrainedModel):\n     def __init__(self, config: CLIPVisionConfig):\n         super().__init__(config)\n \n-        vision_model = CLIPVisionModel._from_config(config, attn_implementation=config._attn_implementation)\n+        vision_model = CLIPVisionModel._from_config(config)\n         self.vision_model = vision_model.vision_model\n \n         self.visual_projection = nn.Linear(config.hidden_size, config.projection_dim, bias=False)"
        },
        {
            "sha": "722d9078d28a09a19dab33dfd2d8f4a825a1a68a",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -1791,6 +1791,8 @@ def forward(\n \n \n class Emu3ForConditionalGeneration(Emu3PreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"text_model.lm_head.weight\"]\n+\n     def __init__(self, config):\n         super().__init__(config)\n         self.text_model = Emu3ForCausalLM._from_config(config.text_config)"
        },
        {
            "sha": "da6016dc266bf920a56ee159ac970ec8cd03f1f0",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -1103,6 +1103,8 @@ def forward(**super_kwargs):\n \n \n class Emu3ForConditionalGeneration(Emu3PreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"text_model.lm_head.weight\"]\n+\n     def __init__(self, config):\n         super().__init__(config)\n         self.text_model = Emu3ForCausalLM._from_config(config.text_config)"
        },
        {
            "sha": "79f82b5ac4bd99d22be3a724ce93107ce0bf286a",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -151,9 +151,9 @@ def __init__(self, config: FuyuConfig):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n         self.vocab_size = config.text_config.vocab_size\n-        self.language_model = AutoModelForCausalLM.from_config(\n-            config.text_config, attn_implementation=config._attn_implementation\n-        )\n+        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n+        if self.language_model._tied_weights_keys is not None:\n+            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n \n         self.vision_embed_tokens = nn.Linear(\n             config.patch_size * config.patch_size * config.num_channels, config.hidden_size\n@@ -181,9 +181,6 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n \n-    def tie_weights(self):\n-        return self.language_model.tie_weights()\n-\n     def gather_continuous_embeddings(\n         self,\n         word_embeddings: torch.Tensor,"
        },
        {
            "sha": "283409327bf9142599d0848143cf1cbb905a282c",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -2104,9 +2104,7 @@ def __init__(self, config: GroundingDinoConfig):\n             )\n \n         # Create text backbone\n-        self.text_backbone = AutoModel.from_config(\n-            config.text_config, add_pooling_layer=False, attn_implementation=config._attn_implementation\n-        )\n+        self.text_backbone = AutoModel.from_config(config.text_config, add_pooling_layer=False)\n         self.text_projection = nn.Linear(config.text_config.hidden_size, config.d_model)\n \n         if config.embedding_init_target or not config.two_stage:"
        },
        {
            "sha": "4e819811a9849d498dad5c142ba012bd4dad392a",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 33,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -1285,13 +1285,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.text_model.set_input_embeddings(value)\n \n-    def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of=None) -> nn.Embedding:\n-        model_embeds = self.text_model.resize_token_embeddings(\n-            new_num_tokens=new_num_tokens, pad_to_multiple_of=pad_to_multiple_of\n-        )\n-        self.config.text_config.vocab_size = model_embeds.num_embeddings\n-        return model_embeds\n-\n     def inputs_merger(\n         self,\n         input_ids: torch.LongTensor,\n@@ -1515,32 +1508,6 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n-    def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of=None) -> nn.Embedding:\n-        # model_embeds = self.model.resize_token_embeddings(new_num_tokens=new_num_tokens, pad_to_multiple_of=pad_to_multiple_of)\n-        model_embeds = self._resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n-        if new_num_tokens is None and pad_to_multiple_of is None:\n-            return model_embeds\n-\n-        # Update base model and current model config\n-        # Ignore copy\n-        self.config.text_config.vocab_size = model_embeds.weight.shape[0]\n-        self.vocab_size = self.config.text_config.vocab_size\n-\n-        # Tie weights again if needed\n-        self.tie_weights()\n-\n-        return model_embeds\n-\n-    def tie_weights(self):\n-        \"\"\"\n-        Overwrite `transformers.modeling_utils.PreTrainedModel.tie_weights` to handle the case of DecoupledLinear and DecoupledEmbedding.\n-        \"\"\"\n-        output_embeddings = self.get_output_embeddings()\n-        input_embeddings = self.get_input_embeddings()\n-\n-        if getattr(self.config, \"tie_word_embeddings\", True):\n-            output_embeddings.weight = input_embeddings.weight\n-\n     @add_start_docstrings_to_model_forward(IDEFICS2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Idefics2CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "31cf1a2e8f11737217e781373a4b87ce180dae0d",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -1094,17 +1094,6 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n-    # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2ForConditionalGeneration.tie_weights\n-    def tie_weights(self):\n-        \"\"\"\n-        Overwrite `transformers.modeling_utils.PreTrainedModel.tie_weights` to handle the case of DecoupledLinear and DecoupledEmbedding.\n-        \"\"\"\n-        output_embeddings = self.get_output_embeddings()\n-        input_embeddings = self.get_input_embeddings()\n-\n-        if getattr(self.config, \"tie_word_embeddings\", True):\n-            output_embeddings.weight = input_embeddings.weight\n-\n     @add_start_docstrings_to_model_forward(IDEFICS3_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Idefics3CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "328d64761a566704a0c598d76a540c426a6084fe",
            "filename": "src/transformers/models/instructblip/configuration_instructblip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Finstructblip%2Fconfiguration_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Finstructblip%2Fconfiguration_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fconfiguration_instructblip.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -302,9 +302,6 @@ def __init__(\n         text_model_type = text_config[\"model_type\"] if \"model_type\" in text_config else \"opt\"\n         self.text_config = CONFIG_MAPPING[text_model_type](**text_config)\n \n-        self.tie_word_embeddings = self.text_config.tie_word_embeddings\n-        self.is_encoder_decoder = self.text_config.is_encoder_decoder\n-\n         self.num_query_tokens = num_query_tokens\n         self.image_token_index = image_token_index\n         self.qformer_config.encoder_hidden_size = self.vision_config.hidden_size"
        },
        {
            "sha": "6776c1b62b88523e0482aa010b64764197693d61",
            "filename": "src/transformers/models/instructblipvideo/configuration_instructblipvideo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -308,9 +308,6 @@ def __init__(\n         text_model_type = text_config[\"model_type\"] if \"model_type\" in text_config else \"opt\"\n         self.text_config = CONFIG_MAPPING[text_model_type](**text_config)\n \n-        self.tie_word_embeddings = self.text_config.tie_word_embeddings\n-        self.is_encoder_decoder = self.text_config.is_encoder_decoder\n-\n         self.num_query_tokens = num_query_tokens\n         self.video_token_index = video_token_index\n         self.qformer_config.encoder_hidden_size = self.vision_config.hidden_size"
        },
        {
            "sha": "1376e85c6f95253c751463530aa43ccce12451fe",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -137,9 +137,6 @@ def __init__(\n         text_model_type = text_config[\"model_type\"] if \"model_type\" in text_config else \"opt\"\n         self.text_config = CONFIG_MAPPING[text_model_type](**text_config)\n \n-        self.tie_word_embeddings = self.text_config.tie_word_embeddings\n-        self.is_encoder_decoder = self.text_config.is_encoder_decoder\n-\n         self.num_query_tokens = num_query_tokens\n         self.video_token_index = video_token_index\n         self.qformer_config.encoder_hidden_size = self.vision_config.hidden_size"
        },
        {
            "sha": "93d7465291cb19b947171154b1a946b3d372ca33",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 10,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -242,6 +242,10 @@ def __init__(self, config: LlavaConfig):\n         self.multi_modal_projector = LlavaMultiModalProjector(config)\n         self.vocab_size = config.text_config.vocab_size\n         self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n+\n+        if self.language_model._tied_weights_keys is not None:\n+            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n+\n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n \n         self.post_init()\n@@ -264,16 +268,6 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n \n-    def tie_weights(self):\n-        return self.language_model.tie_weights()\n-\n-    def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of=None) -> nn.Embedding:\n-        model_embeds = self.language_model.resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n-        # update vocab size\n-        self.config.text_config.vocab_size = model_embeds.num_embeddings\n-        self.vocab_size = model_embeds.num_embeddings\n-        return model_embeds\n-\n     def get_image_features(\n         self, pixel_values: torch.FloatTensor, vision_feature_layer: int, vision_feature_select_strategy: str\n     ):"
        },
        {
            "sha": "51df47233b26c0a0c95815d3f94d0fb46c932112",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 3,
            "deletions": 12,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -357,6 +357,9 @@ def __init__(self, config: LlavaNextConfig):\n \n         self.vocab_size = config.text_config.vocab_size\n         self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n+        if self.language_model._tied_weights_keys is not None:\n+            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n+\n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n         self._padding_side = \"left\"  # set it to left by default, user can use setter to change padding_sides\n         self.post_init()\n@@ -395,18 +398,6 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n \n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.tie_weights\n-    def tie_weights(self):\n-        return self.language_model.tie_weights()\n-\n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.resize_token_embeddings\n-    def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of=None) -> nn.Embedding:\n-        model_embeds = self.language_model.resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n-        # update vocab size\n-        self.config.text_config.vocab_size = model_embeds.num_embeddings\n-        self.vocab_size = model_embeds.num_embeddings\n-        return model_embeds\n-\n     def _merge_input_ids_with_image_features(\n         self,\n         image_features,"
        },
        {
            "sha": "257c81aa8fe4dffa04c56f740bedd83492cad299",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -397,6 +397,9 @@ def __init__(\n \n         self.vocab_size = config.text_config.vocab_size\n         self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n+        if self.language_model._tied_weights_keys is not None:\n+            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n+\n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n         self._padding_side = \"left\"  # set it to left by default, user can use setter to change padding_sides\n         self.vision_resampler = LlavaNextVideoPooler(config)\n@@ -430,16 +433,6 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n \n-    def tie_weights(self):\n-        return self.language_model.tie_weights()\n-\n-    def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of=None) -> nn.Embedding:\n-        model_embeds = self.language_model.resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n-        # update vocab size\n-        self.config.text_config.vocab_size = model_embeds.num_embeddings\n-        self.vocab_size = model_embeds.num_embeddings\n-        return model_embeds\n-\n     def _merge_input_ids_with_image_features(\n         self,\n         image_features,"
        },
        {
            "sha": "5c5471479e86bf026c53828fafd45ddee42e0358",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -374,6 +374,9 @@ def __init__(self, config: LlavaOnevisionConfig):\n \n         self.vocab_size = config.text_config.vocab_size\n         self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n+        if self.language_model._tied_weights_keys is not None:\n+            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n+\n         self.post_init()\n \n     # Copied from transformers.models.llava_next.modeling_llava_next.LlavaNextForConditionalGeneration.get_input_embeddings\n@@ -400,10 +403,6 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n \n-    # Copied from transformers.models.llava_next.modeling_llava_next.LlavaNextForConditionalGeneration.tie_weights\n-    def tie_weights(self):\n-        return self.language_model.tie_weights()\n-\n     def pack_image_features(self, image_features, image_sizes, image_newline=None, vision_aspect_ratio=\"anyres_max_9\"):\n         \"\"\"\n         Reshape, unpad and then pack each image_feature into a single image_features tensor containing all visual vectors."
        },
        {
            "sha": "b40c366a6d75ef4a02121dcd1c9cf04ae7e4dece",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -1986,6 +1986,9 @@ def __init__(self, config: MllamaConfig):\n \n         self.vision_model = MllamaVisionModel._from_config(config.vision_config)\n         self.language_model = MllamaForCausalLM._from_config(config.text_config)\n+        if self.language_model._tied_weights_keys is not None:\n+            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n+\n         self.multi_modal_projector = nn.Linear(\n             config.vision_config.vision_output_dim,\n             config.text_config.hidden_size,\n@@ -2011,9 +2014,6 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n \n-    def tie_weights(self):\n-        return self.language_model.tie_weights()\n-\n     @add_start_docstrings_to_model_forward(MLLAMA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=\"MllamaConfig\")\n     def forward("
        },
        {
            "sha": "a1c15b7a0b377568fd2c64e9ccef4a28aa320acd",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -1914,12 +1914,9 @@ def __init__(self, config: MoshiConfig):\n         self.embed_tokens = nn.ModuleList(\n             [nn.Embedding(config.audio_vocab_size + 1, config.hidden_size) for _ in range(2 * config.num_codebooks)]\n         )\n-        self.audio_encoder = AutoModel.from_config(\n-            config.audio_encoder_config, attn_implementation=config._attn_implementation\n-        )\n+        self.audio_encoder = AutoModel.from_config(config.audio_encoder_config)\n         self.decoder = MoshiForCausalLM(config)\n \n-        config.depth_decoder_config._attn_implementation_internal = config._attn_implementation\n         self.depth_decoder = MoshiDepthDecoder(config.depth_decoder_config)\n \n         self.num_codebooks = config.num_codebooks"
        },
        {
            "sha": "36a9e59118b67817c11cd68b5aced2cc201cd4ff",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -335,10 +335,6 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n \n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.tie_weights with Llava->PaliGemma\n-    def tie_weights(self):\n-        return self.language_model.tie_weights()\n-\n     def _update_causal_mask(\n         self,\n         attention_mask,"
        },
        {
            "sha": "db2f0ff7e3ca3187ee8ad00897c7aa5124011ad8",
            "filename": "src/transformers/models/pix2struct/configuration_pix2struct.py",
            "status": "modified",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fconfiguration_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fconfiguration_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fconfiguration_pix2struct.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -14,9 +14,6 @@\n # limitations under the License.\n \"\"\"Pix2Struct model configuration\"\"\"\n \n-import os\n-from typing import Union\n-\n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n \n@@ -147,26 +144,6 @@ def __init__(\n             **kwargs,\n         )\n \n-    @classmethod\n-    def from_pretrained(\n-        cls, pretrainehidden_size_name_or_path: Union[str, os.PathLike], **kwargs\n-    ) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrainehidden_size_name_or_path, **kwargs)\n-\n-        # get the text config dict if we are loading from Pix2StructConfig\n-        if config_dict.get(\"model_type\") == \"pix2struct\":\n-            config_dict = config_dict[\"text_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class Pix2StructVisionConfig(PretrainedConfig):\n     r\"\"\"\n@@ -266,26 +243,6 @@ def __init__(\n         self.relative_attention_max_distance = relative_attention_max_distance\n         self.d_kv = d_kv\n \n-    @classmethod\n-    def from_pretrained(\n-        cls, pretrainehidden_size_name_or_path: Union[str, os.PathLike], **kwargs\n-    ) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrainehidden_size_name_or_path, **kwargs)\n-\n-        # get the vision config dict if we are loading from Pix2StructConfig\n-        if config_dict.get(\"model_type\") == \"pix2struct\":\n-            config_dict = config_dict[\"vision_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class Pix2StructConfig(PretrainedConfig):\n     r\"\"\""
        },
        {
            "sha": "77ce68659a50f7eaf6d5210c76a2979abc51e72f",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -1733,14 +1733,6 @@ def get_output_embeddings(self) -> nn.Module:\n     def set_output_embeddings(self, new_embeddings):\n         self.decoder.set_output_embeddings(new_embeddings)\n \n-    def resize_token_embeddings(self, new_num_tokens: Optional[int] = None) -> nn.Embedding:\n-        model_embeds = self.decoder.resize_token_embeddings(new_num_tokens)\n-\n-        # update vocab size\n-        self.config.text_config.vocab_size = new_num_tokens\n-\n-        return model_embeds\n-\n     def get_decoder(self):\n         return self.decoder\n "
        },
        {
            "sha": "a844a67861d5407095561a51cf67a4e95e20af31",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 3,
            "deletions": 12,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -856,6 +856,9 @@ def __init__(self, config: Qwen2AudioConfig):\n         self.multi_modal_projector = Qwen2AudioMultiModalProjector(config)\n         self.vocab_size = config.text_config.vocab_size\n         self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n+        if self.language_model._tied_weights_keys is not None:\n+            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n+\n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n         self._padding_side = \"left\"  # set it to left by default, user can use setter to change padding_sides\n         self.post_init()\n@@ -894,18 +897,6 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n \n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.tie_weights\n-    def tie_weights(self):\n-        return self.language_model.tie_weights()\n-\n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.resize_token_embeddings\n-    def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of=None) -> nn.Embedding:\n-        model_embeds = self.language_model.resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n-        # update vocab size\n-        self.config.text_config.vocab_size = model_embeds.num_embeddings\n-        self.vocab_size = model_embeds.num_embeddings\n-        return model_embeds\n-\n     def _merge_input_ids_with_audio_features(\n         self, audio_features, num_audio_tokens, inputs_embeds, input_ids, attention_mask, labels\n     ):"
        },
        {
            "sha": "9fa099d19230d5e2768b3d0d3b60b1d4104f9f5d",
            "filename": "src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -261,6 +261,9 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.decoder\n \n+    def get_input_embeddings(self):\n+        return self.decoder.get_input_embeddings()\n+\n     def get_output_embeddings(self):\n         return self.decoder.get_output_embeddings()\n "
        },
        {
            "sha": "293fb10ae2779526e8f56c29ffe0d3b307dbc444",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 11,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -245,6 +245,9 @@ def __init__(self, config: VideoLlavaConfig):\n         self.multi_modal_projector = VideoLlavaMultiModalProjector(config)\n         self.vocab_size = config.text_config.vocab_size\n         self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n+        if self.language_model._tied_weights_keys is not None:\n+            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n+\n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n         self.post_init()\n \n@@ -266,17 +269,6 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n \n-    def tie_weights(self):\n-        return self.language_model.tie_weights()\n-\n-    def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of=None) -> nn.Embedding:\n-        model_embeds = self.language_model.resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n-        # update vocab size\n-        self.config.text_config.vocab_size = model_embeds.num_embeddings\n-        self.config.vocab_size = model_embeds.num_embeddings\n-        self.vocab_size = model_embeds.num_embeddings\n-        return model_embeds\n-\n     def _merge_input_ids_with_visual_features(\n         self, visual_features, inputs_embeds, input_ids, attention_mask, labels, num_frames=1\n     ):"
        },
        {
            "sha": "0daaa8327b631b7396a8a2fac5fd8224ebf3ac3c",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 10,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -242,6 +242,10 @@ def __init__(self, config: VipLlavaConfig):\n         self.multi_modal_projector = VipLlavaMultiModalProjector(config)\n         self.vocab_size = config.text_config.vocab_size\n         self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n+\n+        if self.language_model._tied_weights_keys is not None:\n+            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n+\n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n \n         self.post_init()\n@@ -264,16 +268,6 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n \n-    def tie_weights(self):\n-        return self.language_model.tie_weights()\n-\n-    def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of=None) -> nn.Embedding:\n-        model_embeds = self.language_model.resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n-        # update vocab size\n-        self.config.text_config.vocab_size = model_embeds.num_embeddings\n-        self.vocab_size = model_embeds.num_embeddings\n-        return model_embeds\n-\n     # Ignore copy\n     def get_image_features(self, pixel_values: torch.FloatTensor, vision_feature_layers: List[int]):\n         \"\"\""
        },
        {
            "sha": "55c759f8e9ae2ec320a3454634d51db7c17f6ac9",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -237,6 +237,9 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.decoder\n \n+    def get_input_embeddings(self):\n+        return self.decoder.get_input_embeddings()\n+\n     def get_output_embeddings(self):\n         return self.decoder.get_output_embeddings()\n \n@@ -659,12 +662,6 @@ def forward(\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n \n-    def resize_token_embeddings(self, *args, **kwargs):\n-        raise NotImplementedError(\n-            \"Resizing the embedding layers via the VisionEncoderDecoderModel directly is not supported.Please use the\"\n-            \" respective methods of the wrapped decoder object (model.decoder.resize_token_embeddings(...))\"\n-        )\n-\n     def _reorder_cache(self, past_key_values, beam_idx):\n         # apply decoder cache reordering here\n         return self.decoder._reorder_cache(past_key_values, beam_idx)"
        },
        {
            "sha": "5556f14a0b93b13e0790d1d4ae3db0a4d4b5ed68",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -1123,6 +1123,13 @@ def is_pipeline_test_to_skip(\n \n     def setUp(self):\n         self.model_tester = Blip2ModelTester(self)\n+        common_properties = [\"image_token_index\", \"num_query_tokens\", \"image_text_hidden_size\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=Blip2Config, has_text_modality=False, common_properties=common_properties\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n \n     def test_for_conditional_generation(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()"
        },
        {
            "sha": "335b690d879f1871639e505a51b5f9b7d08f41eb",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -158,7 +158,10 @@ class InstructBlipVisionModelTest(ModelTesterMixin, unittest.TestCase):\n     def setUp(self):\n         self.model_tester = InstructBlipVisionModelTester(self)\n         self.config_tester = ConfigTester(\n-            self, config_class=InstructBlipVisionConfig, has_text_modality=False, hidden_size=37\n+            self,\n+            config_class=InstructBlipConfig,\n+            has_text_modality=False,\n+            common_properties=[\"num_query_tokens\", \"image_token_index\"],\n         )\n \n     def test_config(self):"
        },
        {
            "sha": "6b59a9878aa4f4535e8cb4e5e718cb7de251d625",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -163,8 +163,9 @@ class InstructBlipVideoVisionModelTest(ModelTesterMixin, unittest.TestCase):\n \n     def setUp(self):\n         self.model_tester = InstructBlipVideoVisionModelTester(self)\n+        common_properties = [\"num_query_tokens\", \"video_token_index\"]\n         self.config_tester = ConfigTester(\n-            self, config_class=InstructBlipVideoVisionConfig, has_text_modality=False, hidden_size=37\n+            self, config_class=InstructBlipVideoConfig, has_text_modality=False, common_properties=common_properties\n         )\n \n     def test_config(self):"
        },
        {
            "sha": "965d7593693397f7846332da929c46fbc3c81268",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/09d5f76274b54673bc5eadcedfe008313baf27d9/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09d5f76274b54673bc5eadcedfe008313baf27d9/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=09d5f76274b54673bc5eadcedfe008313baf27d9",
            "patch": "@@ -2283,7 +2283,7 @@ def test_load_save_without_tied_weights(self):\n \n     def test_tied_weights_keys(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.tie_word_embeddings = True\n+        config.get_text_config().tie_word_embeddings = True\n         for model_class in self.all_model_classes:\n             model_tied = model_class(config)\n "
        }
    ],
    "stats": {
        "total": 287,
        "additions": 68,
        "deletions": 219
    }
}