{
    "author": "itazap",
    "message": "mllama outputs refactor (#39643)\n\n* mllama outputs refactor\n\n* forgot kwargs\n\n* fix output\n\n* add can_record_outputs\n\n* correct @check_model_inputs placement\n\n* ruff and copies\n\n* rebase\n\n* feedback\n\n* only return hidden_states\n\n---------\n\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-161-153.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-162-14.ec2.internal>",
    "sha": "da823fc04e2e249c7a7c73bda1ec219c07359d60",
    "files": [
        {
            "sha": "5a0bcb55d42a96faa63103053d6fd130d4dd1128",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 56,
            "deletions": 269,
            "changes": 325,
            "blob_url": "https://github.com/huggingface/transformers/blob/da823fc04e2e249c7a7c73bda1ec219c07359d60/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da823fc04e2e249c7a7c73bda1ec219c07359d60/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=da823fc04e2e249c7a7c73bda1ec219c07359d60",
            "patch": "@@ -27,11 +27,13 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_mllama import MllamaConfig, MllamaTextConfig, MllamaVisionConfig\n \n \n@@ -40,7 +42,6 @@\n \n     from ...integrations.flex_attention import make_flex_block_causal_mask\n \n-\n logger = logging.get_logger(__name__)\n \n \n@@ -235,7 +236,6 @@ def forward(\n         self,\n         hidden_state: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         query = self.q_proj(hidden_state)\n@@ -252,13 +252,7 @@ def forward(\n         attention_interface: Callable = eager_attention_forward\n \n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -274,9 +268,6 @@ def forward(\n         attn_output = attn_output.reshape(batch_size, q_seq_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n \n-        if not output_attentions:\n-            attn_weights = None\n-\n         return attn_output, attn_weights\n \n \n@@ -303,7 +294,6 @@ def forward(\n         self,\n         hidden_state: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n     ):\n         # Self Attention\n         residual = hidden_state\n@@ -321,12 +311,7 @@ def forward(\n             hidden_state = self.gate_ffn.tanh() * hidden_state\n         hidden_state = residual + hidden_state\n \n-        outputs = (hidden_state,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_state\n \n \n class MllamaVisionEncoder(nn.Module):\n@@ -349,10 +334,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, BaseModelOutput]:\n+    ) -> BaseModelOutput:\n         r\"\"\"\n         Args:\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n@@ -366,54 +348,15 @@ def forward(\n                 - 0 for tokens that are **masked**.\n \n                 [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n \n+        \"\"\"\n         for encoder_layer in self.layers:\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_state=hidden_states,\n-                    attention_mask=attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-            hidden_states = layer_outputs[0]\n+            hidden_states = encoder_layer(\n+                hidden_state=hidden_states,\n+                attention_mask=attention_mask,\n+            )\n \n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n-        return BaseModelOutput(\n-            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n-        )\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n \n \n # Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->MllamaText\n@@ -470,7 +413,6 @@ def forward(\n         cross_attention_states: Optional[torch.Tensor] = None,\n         past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n@@ -507,13 +449,7 @@ def forward(\n         attention_interface: Callable = eager_attention_forward\n \n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -529,9 +465,6 @@ def forward(\n         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n \n-        if not output_attentions:\n-            attn_weights = None\n-\n         return attn_output, attn_weights\n \n \n@@ -595,7 +528,6 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n         position_embeddings: torch.Tensor,\n-        output_attentions: bool = False,\n         use_cache: bool = False,\n         past_key_value=None,\n         cache_position=None,\n@@ -622,13 +554,7 @@ def forward(\n         attention_interface: Callable = eager_attention_forward\n \n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -644,9 +570,6 @@ def forward(\n         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n \n-        if not output_attentions:\n-            attn_weights = None\n-\n         return attn_output, attn_weights\n \n \n@@ -669,7 +592,7 @@ def forward(self, x):\n \n \n # Modified from transformers.models.llama.modeling_llama.LlamaDecoderLayer\n-class MllamaSelfAttentionDecoderLayer(nn.Module):\n+class MllamaSelfAttentionDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: MllamaTextConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -691,7 +614,6 @@ def forward(\n         full_text_row_masked_out_mask: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -703,9 +625,7 @@ def forward(\n             attention_mask (`torch.FloatTensor`, *optional*):\n                 attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n                 query_sequence_length, key_sequence_length)` if default attention is used.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n+\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n@@ -729,7 +649,6 @@ def forward(\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_value=past_key_value,\n-            output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -743,15 +662,10 @@ def forward(\n         hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n-class MllamaCrossAttentionDecoderLayer(torch.nn.Module):\n+class MllamaCrossAttentionDecoderLayer(GradientCheckpointingLayer):\n     \"\"\"Cross-attention transformer block with tanh-gated attention and feedforward.\"\"\"\n \n     def __init__(self, config: MllamaTextConfig, layer_idx: int) -> None:\n@@ -775,7 +689,6 @@ def forward(\n         full_text_row_masked_out_mask: tuple[torch.Tensor, torch.Tensor],\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[torch.Tensor] = None,\n@@ -789,7 +702,6 @@ def forward(\n             attention_mask=cross_attention_mask,\n             cross_attention_states=cross_attention_states,\n             past_key_value=past_key_value,\n-            output_attentions=output_attentions,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -802,12 +714,7 @@ def forward(\n             hidden_states = full_text_row_masked_out_mask[:, 0] * hidden_states  # type: ignore\n         hidden_states = residual + self.cross_attn_mlp_gate.tanh() * hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class MllamaRotaryEmbedding(nn.Module):\n@@ -849,12 +756,19 @@ class MllamaPreTrainedModel(PreTrainedModel):\n         \"MllamaCrossAttentionDecoderLayer\",\n         \"MllamaSelfAttentionDecoderLayer\",\n     ]\n-\n     _can_compile_fullgraph = False  # static cache cannot have different shapes for each layer\n     _supports_sdpa = True\n     _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": [MllamaSelfAttentionDecoderLayer, MllamaCrossAttentionDecoderLayer],\n+        \"attentions\": [\n+            OutputRecorder(MllamaTextSelfAttention, index=1, layer_name=\"self_attn\"),\n+            OutputRecorder(MllamaTextSelfAttention, index=1, layer_name=\"cross_attn\"),\n+            OutputRecorder(MllamaTextCrossAttention, index=1, layer_name=\"cross_attn\"),\n+        ],\n+    }\n \n     def _init_weights(self, module):\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n@@ -1071,16 +985,11 @@ def apply_class_embedding(self, hidden_state: torch.Tensor) -> torch.Tensor:\n         hidden_state = torch.cat([class_embedding, hidden_state], dim=1)\n         return hidden_state\n \n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n-        self,\n-        pixel_values: torch.Tensor,\n-        aspect_ratio_ids: torch.Tensor,\n-        aspect_ratio_mask: torch.Tensor,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[BaseModelOutput, tuple[torch.Tensor, ...]]:\n+        self, pixel_values: torch.Tensor, aspect_ratio_ids: torch.Tensor, aspect_ratio_mask: torch.Tensor, **kwargs\n+    ) -> BaseModelOutput:\n         r\"\"\"\n         aspect_ratio_ids (`torch.Tensor` of shape `(batch_size, max_num_images)`, *optional*):\n             Aspect ratio ids used to select the appropriate precomputed tile embeddings based on the aspect ratio of each input image.\n@@ -1121,12 +1030,6 @@ def forward(\n         torch.Size([1, 1, 4, 1025, 7680])\n         ```\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         batch_size, num_concurrent_media, num_tiles, num_channels, height, width = pixel_values.shape\n \n         pixel_values = pixel_values.reshape(batch_size * num_concurrent_media * num_tiles, num_channels, height, width)\n@@ -1176,10 +1079,8 @@ def forward(\n         output = self.transformer(\n             hidden_state,\n             attention_mask=attention_mask,\n-            output_hidden_states=True,\n-            output_attentions=output_attentions,\n         )\n-        hidden_state = output[0]\n+        hidden_state = output.last_hidden_state\n \n         hidden_state = self.layernorm_post(hidden_state)\n \n@@ -1194,10 +1095,8 @@ def forward(\n         global_output = self.global_transformer(\n             hidden_state,\n             attention_mask=attention_mask,\n-            output_hidden_states=output_hidden_states,\n-            output_attentions=output_attentions,\n         )\n-        hidden_state = global_output[0]\n+        hidden_state = global_output.last_hidden_state\n \n         # Remove padding form hidden state\n         hidden_state = hidden_state.reshape(\n@@ -1207,7 +1106,7 @@ def forward(\n         hidden_state = hidden_state.reshape(batch_size, num_concurrent_media, num_tiles, num_patches, dim)\n \n         # Collect intermediate layer outputs from encoder output\n-        all_intermediate_hidden_states = [output[1][i] for i in self.intermediate_layers_indices]\n+        all_intermediate_hidden_states = [output.last_hidden_state for _ in self.intermediate_layers_indices]\n         intermediate_hidden_states = torch.stack(all_intermediate_hidden_states, dim=-1)\n \n         # Remove padding from intermediate hidden states\n@@ -1222,26 +1121,7 @@ def forward(\n         # Concatenate final hidden state and intermediate hidden states\n         hidden_state = torch.cat([hidden_state, intermediate_hidden_states], dim=-1)\n \n-        if output_hidden_states:\n-            hidden_states = tuple(all_intermediate_hidden_states) + tuple(global_output[1])\n-        else:\n-            hidden_states = None\n-\n-        if output_attentions:\n-            # global transformer in contrast to `self.transformer` doesn't always return hidden states so we might go index out-of-range\n-            global_attn = tuple(global_output[2]) if output_hidden_states else tuple(global_output[1])\n-            attentions = tuple(output[2]) + global_attn\n-        else:\n-            attentions = None\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_state, hidden_states, attentions] if v is not None)\n-\n-        return BaseModelOutput(\n-            last_hidden_state=hidden_state,\n-            hidden_states=hidden_states,\n-            attentions=attentions,\n-        )\n+        return BaseModelOutput(last_hidden_state=hidden_state)\n \n \n @auto_docstring(\n@@ -1273,6 +1153,8 @@ def __init__(self, config: MllamaTextConfig):\n         self.gradient_checkpointing = False\n         self.post_init()\n \n+    @check_model_inputs\n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1285,12 +1167,9 @@ def forward(\n         past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         r\"\"\"\n         cross_attention_states (`torch.FloatTensor`, *optional*):\n             Output of the vision model, used for cross-attention. This tensor contains the processed image features that\n@@ -1330,22 +1209,11 @@ def forward(\n         torch.Size([1, 13, 4096])\n         ```\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training and use_cache:\n-            logger.warning_once(\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n-            )\n-            use_cache = False\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n@@ -1362,21 +1230,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n-        )\n+        causal_mask = self._update_causal_mask(attention_mask, inputs_embeds, cache_position, past_key_values)\n \n         # create position embeddings to be shared across the decoder layers\n         position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n         # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-\n         for idx, decoder_layer in enumerate(self.layers):\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n             # For text-only path we should skip cross attention layers.\n             # Let's check if the layer is cross attention layer and if we have cross attention states\n             # or cached cross attention states.\n@@ -1388,57 +1248,25 @@ def forward(\n             if is_cross_attention_layer and cross_attention_states is None and is_cross_attention_cache_empty:\n                 continue\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    cross_attention_states,\n-                    cross_attention_mask,\n-                    causal_mask,\n-                    full_text_row_masked_out_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    cross_attention_states=cross_attention_states,\n-                    cross_attention_mask=cross_attention_mask,\n-                    attention_mask=causal_mask,\n-                    full_text_row_masked_out_mask=full_text_row_masked_out_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **kwargs,\n-                )\n-\n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                cross_attention_states=cross_attention_states,\n+                cross_attention_mask=cross_attention_mask,\n+                attention_mask=causal_mask,\n+                full_text_row_masked_out_mask=full_text_row_masked_out_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n \n         hidden_states = self.norm(hidden_states)\n \n-        # add hidden states from the last decoder layer\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(\n-                v for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns] if v is not None\n-            )\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n         )\n \n \n@@ -1468,6 +1296,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1481,9 +1310,6 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1532,12 +1358,6 @@ def forward(\n         I love the idea of snowflakes gently falling, each one\n         ```\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs = self.model(\n             input_ids=input_ids,\n@@ -1549,25 +1369,18 @@ def forward(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :]).float()\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n@@ -1614,6 +1427,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.language_model\n \n+    @check_model_inputs\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1629,12 +1443,9 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[tuple, CausalLMOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         r\"\"\"\n         aspect_ratio_mask (`torch.Tensor` of shape `(batch_size, max_num_images, max_num_tiles)`, *optional*):\n             Mask to avoid performing attention on padding tiles. Mask values selected in `[0, 1]`:\n@@ -1664,12 +1475,6 @@ def forward(\n             Output of the vision model, used for cross-attention. This tensor contains the processed image features that\n             the language model will attend to.\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n@@ -1684,11 +1489,8 @@ def forward(\n                 pixel_values=pixel_values,\n                 aspect_ratio_ids=aspect_ratio_ids,\n                 aspect_ratio_mask=aspect_ratio_mask,\n-                output_hidden_states=output_hidden_states,\n-                output_attentions=output_attentions,\n-                return_dict=return_dict,\n             )\n-            cross_attention_states = vision_outputs[0]\n+            cross_attention_states = vision_outputs.last_hidden_state\n             cross_attention_states = self.multi_modal_projector(cross_attention_states).reshape(\n                 -1, cross_attention_states.shape[-2], self.hidden_size\n             )\n@@ -1716,9 +1518,6 @@ def forward(\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n             inputs_embeds=inputs_embeds,\n-            output_hidden_states=output_hidden_states,\n-            output_attentions=output_attentions,\n-            return_dict=True,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -1788,9 +1587,6 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1855,12 +1651,6 @@ def forward(\n         [', it would be:.\\\\nA stop sign in Chinatown.\\\\n']\n         ```\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.model(\n             input_ids=input_ids,\n             pixel_values=pixel_values,\n@@ -1873,14 +1663,11 @@ def forward(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=True,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])"
        }
    ],
    "stats": {
        "total": 325,
        "additions": 56,
        "deletions": 269
    }
}