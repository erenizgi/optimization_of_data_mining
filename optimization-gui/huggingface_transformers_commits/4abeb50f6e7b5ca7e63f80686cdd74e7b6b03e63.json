{
    "author": "VladOS95-cyber",
    "message": "Add D-FINE Model into Transformers (#36261)\n\n* copy the last changes from broken PR\n\n* small format\n\n* some fixes and refactoring after review\n\n* format\n\n* add config attr for loss\n\n* some fixes and refactoring\n\n* fix copies\n\n* fix style\n\n* add test for d-fine resnet\n\n* fix decoder layer prop\n\n* fix dummies\n\n* format init\n\n* remove extra print\n\n* refactor modeling, move resnet into separate folder\n\n* fix resnet config\n\n* change resnet on hgnet_v2, add clamp into decoder\n\n* fix init\n\n* fix config doc\n\n* fix init\n\n* fix dummies\n\n* fix config docs\n\n* fix hgnet_v2 config typo\n\n* format modular\n\n* add image classification for hgnet, some refactoring\n\n* format tests\n\n* fix dummies\n\n* fix init\n\n* fix style\n\n* fix init for hgnet v2\n\n* fix index.md, add init rnage for hgnet\n\n* fix conversion\n\n* add missing attr to encoder\n\n* add loss for d-fine, add additional output for rt-detr decoder\n\n* tests and docs fixes\n\n* fix rt_detr v2 conversion\n\n* some fixes for loos and decoder output\n\n* some fixes for loss\n\n* small fix for converted modeling\n\n* add n model config, some todo comments for modular\n\n* convert script adjustments and fixes, small refact\n\n* remove extra output for rt_detr\n\n* make some outputs optionsl, fix conversion\n\n* some posr merge fixes\n\n* small fix\n\n* last field fix\n\n* fix not split for hgnet_v2\n\n* disable parallelism test for hgnet_v2 image classification\n\n* skip multi gpu for d-fine\n\n* adjust after merge init\n\n* remove extra comment\n\n* fix repo name references\n\n* small fixes for tests\n\n* Fix checkpoint path\n\n* Fix consistency\n\n* Fixing docs\n\n---------\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
    "sha": "4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63",
    "files": [
        {
            "sha": "bf503f96be0058945396a84974bab1f60316cc25",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63",
            "patch": "@@ -499,6 +499,8 @@\n         title: Helium\n       - local: model_doc/herbert\n         title: HerBERT\n+      - local: model_doc/hgnet_v2\n+        title: HGNet-V2\n       - local: model_doc/ibert\n         title: I-BERT\n       - local: model_doc/jamba\n@@ -691,6 +693,8 @@\n         title: ConvNeXTV2\n       - local: model_doc/cvt\n         title: CvT\n+      - local: model_doc/d_fine\n+        title: D-FINE\n       - local: model_doc/dab-detr\n         title: DAB-DETR\n       - local: model_doc/deformable_detr"
        },
        {
            "sha": "0d4689f049914f8f5909762157c2a016b04f9d5b",
            "filename": "docs/source/en/model_doc/d_fine.md",
            "status": "added",
            "additions": 76,
            "deletions": 0,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/docs%2Fsource%2Fen%2Fmodel_doc%2Fd_fine.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/docs%2Fsource%2Fen%2Fmodel_doc%2Fd_fine.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fd_fine.md?ref=4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63",
            "patch": "@@ -0,0 +1,76 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# D-FINE\n+\n+## Overview\n+\n+The D-FINE model was proposed in [D-FINE: Redefine Regression Task in DETRs as Fine-grained Distribution Refinement](https://arxiv.org/abs/2410.13842) by\n+Yansong Peng, Hebei Li, Peixi Wu, Yueyi Zhang, Xiaoyan Sun, Feng Wu\n+\n+The abstract from the paper is the following:\n+\n+*We introduce D-FINE, a powerful real-time object detector that achieves outstanding localization precision by redefining the bounding box regression task in DETR models. D-FINE comprises two key components: Fine-grained Distribution Refinement (FDR) and Global Optimal Localization Self-Distillation (GO-LSD). \n+FDR transforms the regression process from predicting fixed coordinates to iteratively refining probability distributions, providing a fine-grained intermediate representation that significantly enhances localization accuracy. GO-LSD is a bidirectional optimization strategy that transfers localization knowledge from refined distributions to shallower layers through self-distillation, while also simplifying the residual prediction tasks for deeper layers. Additionally, D-FINE incorporates lightweight optimizations in computationally intensive modules and operations, achieving a better balance between speed and accuracy. Specifically, D-FINE-L / X achieves 54.0% / 55.8% AP on the COCO dataset at 124 / 78 FPS on an NVIDIA T4 GPU. When pretrained on Objects365, D-FINE-L / X attains 57.1% / 59.3% AP, surpassing all existing real-time detectors. Furthermore, our method significantly enhances the performance of a wide range of DETR models by up to 5.3% AP with negligible extra parameters and training costs. Our code and pretrained models: this https URL.*\n+\n+This model was contributed by [VladOS95-cyber](https://github.com/VladOS95-cyber). \n+The original code can be found [here](https://github.com/Peterande/D-FINE).\n+\n+## Usage tips \n+\n+```python\n+>>> import torch\n+>>> from transformers.image_utils import load_image\n+>>> from transformers import DFineForObjectDetection, AutoImageProcessor\n+\n+>>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n+>>> image = load_image(url)\n+\n+>>> image_processor = AutoImageProcessor.from_pretrained(\"ustc-community/dfine_x_coco\")\n+>>> model = DFineForObjectDetection.from_pretrained(\"ustc-community/dfine_x_coco\")\n+\n+>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs)\n+\n+>>> results = image_processor.post_process_object_detection(outputs, target_sizes=[(image.height, image.width)], threshold=0.5)\n+\n+>>> for result in results:\n+...     for score, label_id, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n+...         score, label = score.item(), label_id.item()\n+...         box = [round(i, 2) for i in box.tolist()]\n+...         print(f\"{model.config.id2label[label]}: {score:.2f} {box}\")\n+cat: 0.96 [344.49, 23.4, 639.84, 374.27]\n+cat: 0.96 [11.71, 53.52, 316.64, 472.33]\n+remote: 0.95 [40.46, 73.7, 175.62, 117.57]\n+sofa: 0.92 [0.59, 1.88, 640.25, 474.74]\n+remote: 0.89 [333.48, 77.04, 370.77, 187.3]\n+```\n+\n+## DFineConfig\n+\n+[[autodoc]] DFineConfig\n+\n+## DFineModel\n+\n+[[autodoc]] DFineModel\n+    - forward\n+\n+## DFineForObjectDetection\n+\n+[[autodoc]] DFineForObjectDetection\n+    - forward"
        },
        {
            "sha": "7c868608f4c1c7070d2b9913330005fb9eb1ce0e",
            "filename": "docs/source/en/model_doc/hgnet_v2.md",
            "status": "added",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/docs%2Fsource%2Fen%2Fmodel_doc%2Fhgnet_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/docs%2Fsource%2Fen%2Fmodel_doc%2Fhgnet_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhgnet_v2.md?ref=4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63",
            "patch": "@@ -0,0 +1,46 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# HGNet-V2\n+\n+## Overview\n+\n+A HGNet-V2 (High Performance GPU Net) image classification model.\n+HGNet arhtictecture was proposed in [HGNET: A Hierarchical Feature Guided Network for Occupancy Flow Field Prediction](https://arxiv.org/abs/2407.01097) by\n+Zhan Chen, Chen Tang, Lu Xiong\n+\n+The abstract from the HGNET paper is the following:\n+\n+*Predicting the motion of multiple traffic participants has always been one of the most challenging tasks in autonomous driving. The recently proposed occupancy flow field prediction method has shown to be a more effective and scalable representation compared to general trajectory prediction methods. However, in complex multi-agent traffic scenarios, it remains difficult to model the interactions among various factors and the dependencies among prediction outputs at different time steps. In view of this, we propose a transformer-based hierarchical feature guided network (HGNET), which can efficiently extract features of agents and map information from visual and vectorized inputs, modeling multimodal interaction relationships. Second, we design the Feature-Guided Attention (FGAT) module to leverage the potential guiding effects between different prediction targets, thereby improving prediction accuracy. Additionally, to enhance the temporal consistency and causal relationships of the predictions, we propose a Time Series Memory framework to learn the conditional distribution models of the prediction outputs at future time steps from multivariate time series. The results demonstrate that our model exhibits competitive performance, which ranks 3rd in the 2024 Waymo Occupancy and Flow Prediction Challenge.*\n+\n+This model was contributed by [VladOS95-cyber](https://github.com/VladOS95-cyber). \n+The original code can be found [here](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/backbones/hgnet_v2.py).\n+\n+## HGNetV2Config\n+\n+[[autodoc]] HGNetV2Config\n+\n+\n+## HGNetV2Backbone\n+\n+[[autodoc]] HGNetV2Backbone\n+    - forward\n+\n+\n+## HGNetV2ForImageClassification\n+\n+[[autodoc]] HGNetV2ForImageClassification\n+    - forward\n\\ No newline at end of file"
        },
        {
            "sha": "722c5949c19377de60b96b29df7f42f5c38b7a27",
            "filename": "src/transformers/loss/loss_d_fine.py",
            "status": "added",
            "additions": 385,
            "deletions": 0,
            "changes": 385,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Floss%2Floss_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Floss%2Floss_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_d_fine.py?ref=4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63",
            "patch": "@@ -0,0 +1,385 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+\n+from ..utils import is_vision_available\n+from .loss_for_object_detection import (\n+    box_iou,\n+)\n+from .loss_rt_detr import RTDetrHungarianMatcher, RTDetrLoss\n+\n+\n+if is_vision_available():\n+    from transformers.image_transforms import center_to_corners_format\n+\n+\n+@torch.jit.unused\n+def _set_aux_loss(outputs_class, outputs_coord):\n+    # this is a workaround to make torchscript happy, as torchscript\n+    # doesn't support dictionary with non-homogeneous values, such\n+    # as a dict having both a Tensor and a list.\n+    return [{\"logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class, outputs_coord)]\n+\n+\n+@torch.jit.unused\n+def _set_aux_loss2(\n+    outputs_class, outputs_coord, outputs_corners, outputs_ref, teacher_corners=None, teacher_logits=None\n+):\n+    # this is a workaround to make torchscript happy, as torchscript\n+    # doesn't support dictionary with non-homogeneous values, such\n+    # as a dict having both a Tensor and a list.\n+    return [\n+        {\n+            \"logits\": a,\n+            \"pred_boxes\": b,\n+            \"pred_corners\": c,\n+            \"ref_points\": d,\n+            \"teacher_corners\": teacher_corners,\n+            \"teacher_logits\": teacher_logits,\n+        }\n+        for a, b, c, d in zip(outputs_class, outputs_coord, outputs_corners, outputs_ref)\n+    ]\n+\n+\n+def weighting_function(max_num_bins: int, up: torch.Tensor, reg_scale: int) -> torch.Tensor:\n+    \"\"\"\n+    Generates the non-uniform Weighting Function W(n) for bounding box regression.\n+\n+    Args:\n+        max_num_bins (int): Max number of the discrete bins.\n+        up (Tensor): Controls upper bounds of the sequence,\n+                     where maximum offset is Â±up * H / W.\n+        reg_scale (float): Controls the curvature of the Weighting Function.\n+                           Larger values result in flatter weights near the central axis W(max_num_bins/2)=0\n+                           and steeper weights at both ends.\n+    Returns:\n+        Tensor: Sequence of Weighting Function.\n+    \"\"\"\n+    upper_bound1 = abs(up[0]) * abs(reg_scale)\n+    upper_bound2 = abs(up[0]) * abs(reg_scale) * 2\n+    step = (upper_bound1 + 1) ** (2 / (max_num_bins - 2))\n+    left_values = [-((step) ** i) + 1 for i in range(max_num_bins // 2 - 1, 0, -1)]\n+    right_values = [(step) ** i - 1 for i in range(1, max_num_bins // 2)]\n+    values = [-upper_bound2] + left_values + [torch.zeros_like(up[0][None])] + right_values + [upper_bound2]\n+    values = [v if v.dim() > 0 else v.unsqueeze(0) for v in values]\n+    values = torch.cat(values, 0)\n+    return values\n+\n+\n+def translate_gt(gt: torch.Tensor, max_num_bins: int, reg_scale: int, up: torch.Tensor):\n+    \"\"\"\n+    Decodes bounding box ground truth (GT) values into distribution-based GT representations.\n+\n+    This function maps continuous GT values into discrete distribution bins, which can be used\n+    for regression tasks in object detection models. It calculates the indices of the closest\n+    bins to each GT value and assigns interpolation weights to these bins based on their proximity\n+    to the GT value.\n+\n+    Args:\n+        gt (Tensor): Ground truth bounding box values, shape (N, ).\n+        max_num_bins (int): Maximum number of discrete bins for the distribution.\n+        reg_scale (float): Controls the curvature of the Weighting Function.\n+        up (Tensor): Controls the upper bounds of the Weighting Function.\n+\n+    Returns:\n+        Tuple[Tensor, Tensor, Tensor]:\n+            - indices (Tensor): Index of the left bin closest to each GT value, shape (N, ).\n+            - weight_right (Tensor): Weight assigned to the right bin, shape (N, ).\n+            - weight_left (Tensor): Weight assigned to the left bin, shape (N, ).\n+    \"\"\"\n+    gt = gt.reshape(-1)\n+    function_values = weighting_function(max_num_bins, up, reg_scale)\n+\n+    # Find the closest left-side indices for each value\n+    diffs = function_values.unsqueeze(0) - gt.unsqueeze(1)\n+    mask = diffs <= 0\n+    closest_left_indices = torch.sum(mask, dim=1) - 1\n+\n+    # Calculate the weights for the interpolation\n+    indices = closest_left_indices.float()\n+\n+    weight_right = torch.zeros_like(indices)\n+    weight_left = torch.zeros_like(indices)\n+\n+    valid_idx_mask = (indices >= 0) & (indices < max_num_bins)\n+    valid_indices = indices[valid_idx_mask].long()\n+\n+    # Obtain distances\n+    left_values = function_values[valid_indices]\n+    right_values = function_values[valid_indices + 1]\n+\n+    left_diffs = torch.abs(gt[valid_idx_mask] - left_values)\n+    right_diffs = torch.abs(right_values - gt[valid_idx_mask])\n+\n+    # Valid weights\n+    weight_right[valid_idx_mask] = left_diffs / (left_diffs + right_diffs)\n+    weight_left[valid_idx_mask] = 1.0 - weight_right[valid_idx_mask]\n+\n+    # Invalid weights (out of range)\n+    invalid_idx_mask_neg = indices < 0\n+    weight_right[invalid_idx_mask_neg] = 0.0\n+    weight_left[invalid_idx_mask_neg] = 1.0\n+    indices[invalid_idx_mask_neg] = 0.0\n+\n+    invalid_idx_mask_pos = indices >= max_num_bins\n+    weight_right[invalid_idx_mask_pos] = 1.0\n+    weight_left[invalid_idx_mask_pos] = 0.0\n+    indices[invalid_idx_mask_pos] = max_num_bins - 0.1\n+\n+    return indices, weight_right, weight_left\n+\n+\n+def bbox2distance(points, bbox, max_num_bins, reg_scale, up, eps=0.1):\n+    \"\"\"\n+    Converts bounding box coordinates to distances from a reference point.\n+\n+    Args:\n+        points (Tensor): (n, 4) [x, y, w, h], where (x, y) is the center.\n+        bbox (Tensor): (n, 4) bounding boxes in \"xyxy\" format.\n+        max_num_bins (float): Maximum bin value.\n+        reg_scale (float): Controling curvarture of W(n).\n+        up (Tensor): Controling upper bounds of W(n).\n+        eps (float): Small value to ensure target < max_num_bins.\n+\n+    Returns:\n+        Tensor: Decoded distances.\n+    \"\"\"\n+\n+    reg_scale = abs(reg_scale)\n+    left = (points[:, 0] - bbox[:, 0]) / (points[..., 2] / reg_scale + 1e-16) - 0.5 * reg_scale\n+    top = (points[:, 1] - bbox[:, 1]) / (points[..., 3] / reg_scale + 1e-16) - 0.5 * reg_scale\n+    right = (bbox[:, 2] - points[:, 0]) / (points[..., 2] / reg_scale + 1e-16) - 0.5 * reg_scale\n+    bottom = (bbox[:, 3] - points[:, 1]) / (points[..., 3] / reg_scale + 1e-16) - 0.5 * reg_scale\n+    four_lens = torch.stack([left, top, right, bottom], -1)\n+    four_lens, weight_right, weight_left = translate_gt(four_lens, max_num_bins, reg_scale, up)\n+    if max_num_bins is not None:\n+        four_lens = four_lens.clamp(min=0, max=max_num_bins - eps)\n+    return four_lens.reshape(-1).detach(), weight_right.detach(), weight_left.detach()\n+\n+\n+class DFineLoss(RTDetrLoss):\n+    \"\"\"\n+    This class computes the losses for D-FINE. The process happens in two steps: 1) we compute hungarian assignment\n+    between ground truth boxes and the outputs of the model 2) we supervise each pair of matched ground-truth /\n+    prediction (supervise class and box).\n+\n+    Args:\n+        matcher (`DetrHungarianMatcher`):\n+            Module able to compute a matching between targets and proposals.\n+        weight_dict (`Dict`):\n+            Dictionary relating each loss with its weights. These losses are configured in DFineConf as\n+            `weight_loss_vfl`, `weight_loss_bbox`, `weight_loss_giou`, `weight_loss_fgl`, `weight_loss_ddf`\n+        losses (`List[str]`):\n+            List of all the losses to be applied. See `get_loss` for a list of all available losses.\n+        alpha (`float`):\n+            Parameter alpha used to compute the focal loss.\n+        gamma (`float`):\n+            Parameter gamma used to compute the focal loss.\n+        eos_coef (`float`):\n+            Relative classification weight applied to the no-object category.\n+        num_classes (`int`):\n+            Number of object categories, omitting the special no-object category.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+\n+        self.matcher = RTDetrHungarianMatcher(config)\n+        self.max_num_bins = config.max_num_bins\n+        self.weight_dict = {\n+            \"loss_vfl\": config.weight_loss_vfl,\n+            \"loss_bbox\": config.weight_loss_bbox,\n+            \"loss_giou\": config.weight_loss_giou,\n+            \"loss_fgl\": config.weight_loss_fgl,\n+            \"loss_ddf\": config.weight_loss_ddf,\n+        }\n+        self.losses = [\"vfl\", \"boxes\", \"local\"]\n+        self.reg_scale = config.reg_scale\n+        self.up = nn.Parameter(torch.tensor([config.up]), requires_grad=False)\n+\n+    def unimodal_distribution_focal_loss(\n+        self, pred, label, weight_right, weight_left, weight=None, reduction=\"sum\", avg_factor=None\n+    ):\n+        dis_left = label.long()\n+        dis_right = dis_left + 1\n+\n+        loss = F.cross_entropy(pred, dis_left, reduction=\"none\") * weight_left.reshape(-1) + F.cross_entropy(\n+            pred, dis_right, reduction=\"none\"\n+        ) * weight_right.reshape(-1)\n+\n+        if weight is not None:\n+            weight = weight.float()\n+            loss = loss * weight\n+\n+        if avg_factor is not None:\n+            loss = loss.sum() / avg_factor\n+        elif reduction == \"mean\":\n+            loss = loss.mean()\n+        elif reduction == \"sum\":\n+            loss = loss.sum()\n+\n+        return loss\n+\n+    def loss_local(self, outputs, targets, indices, num_boxes, T=5):\n+        \"\"\"Compute Fine-Grained Localization (FGL) Loss\n+        and Decoupled Distillation Focal (DDF) Loss.\"\"\"\n+\n+        losses = {}\n+        if \"pred_corners\" in outputs:\n+            idx = self._get_source_permutation_idx(indices)\n+            target_boxes = torch.cat([t[\"boxes\"][i] for t, (_, i) in zip(targets, indices)], dim=0)\n+\n+            pred_corners = outputs[\"pred_corners\"][idx].reshape(-1, (self.max_num_bins + 1))\n+            ref_points = outputs[\"ref_points\"][idx].detach()\n+            with torch.no_grad():\n+                self.fgl_targets = bbox2distance(\n+                    ref_points,\n+                    center_to_corners_format(target_boxes),\n+                    self.max_num_bins,\n+                    self.reg_scale,\n+                    self.up,\n+                )\n+\n+            target_corners, weight_right, weight_left = self.fgl_targets\n+\n+            ious = torch.diag(\n+                box_iou(center_to_corners_format(outputs[\"pred_boxes\"][idx]), center_to_corners_format(target_boxes))[\n+                    0\n+                ]\n+            )\n+            weight_targets = ious.unsqueeze(-1).repeat(1, 1, 4).reshape(-1).detach()\n+\n+            losses[\"loss_fgl\"] = self.unimodal_distribution_focal_loss(\n+                pred_corners,\n+                target_corners,\n+                weight_right,\n+                weight_left,\n+                weight_targets,\n+                avg_factor=num_boxes,\n+            )\n+\n+            pred_corners = outputs[\"pred_corners\"].reshape(-1, (self.max_num_bins + 1))\n+            target_corners = outputs[\"teacher_corners\"].reshape(-1, (self.max_num_bins + 1))\n+            if torch.equal(pred_corners, target_corners):\n+                losses[\"loss_ddf\"] = pred_corners.sum() * 0\n+            else:\n+                weight_targets_local = outputs[\"teacher_logits\"].sigmoid().max(dim=-1)[0]\n+                mask = torch.zeros_like(weight_targets_local, dtype=torch.bool)\n+                mask[idx] = True\n+                mask = mask.unsqueeze(-1).repeat(1, 1, 4).reshape(-1)\n+\n+                weight_targets_local[idx] = ious.reshape_as(weight_targets_local[idx]).to(weight_targets_local.dtype)\n+                weight_targets_local = weight_targets_local.unsqueeze(-1).repeat(1, 1, 4).reshape(-1).detach()\n+\n+                loss_match_local = (\n+                    weight_targets_local\n+                    * (T**2)\n+                    * (\n+                        nn.KLDivLoss(reduction=\"none\")(\n+                            F.log_softmax(pred_corners / T, dim=1),\n+                            F.softmax(target_corners.detach() / T, dim=1),\n+                        )\n+                    ).sum(-1)\n+                )\n+\n+                batch_scale = 1 / outputs[\"pred_boxes\"].shape[0]  # it should be refined\n+                self.num_pos, self.num_neg = (\n+                    (mask.sum() * batch_scale) ** 0.5,\n+                    ((~mask).sum() * batch_scale) ** 0.5,\n+                )\n+                loss_match_local1 = loss_match_local[mask].mean() if mask.any() else 0\n+                loss_match_local2 = loss_match_local[~mask].mean() if (~mask).any() else 0\n+                losses[\"loss_ddf\"] = (loss_match_local1 * self.num_pos + loss_match_local2 * self.num_neg) / (\n+                    self.num_pos + self.num_neg\n+                )\n+\n+        return losses\n+\n+    def get_loss(self, loss, outputs, targets, indices, num_boxes):\n+        loss_map = {\n+            \"cardinality\": self.loss_cardinality,\n+            \"local\": self.loss_local,\n+            \"boxes\": self.loss_boxes,\n+            \"focal\": self.loss_labels_focal,\n+            \"vfl\": self.loss_labels_vfl,\n+        }\n+        if loss not in loss_map:\n+            raise ValueError(f\"Loss {loss} not supported\")\n+        return loss_map[loss](outputs, targets, indices, num_boxes)\n+\n+\n+def DFineForObjectDetectionLoss(\n+    logits,\n+    labels,\n+    device,\n+    pred_boxes,\n+    config,\n+    outputs_class=None,\n+    outputs_coord=None,\n+    enc_topk_logits=None,\n+    enc_topk_bboxes=None,\n+    denoising_meta_values=None,\n+    predicted_corners=None,\n+    initial_reference_points=None,\n+    **kwargs,\n+):\n+    criterion = DFineLoss(config)\n+    criterion.to(device)\n+    # Second: compute the losses, based on outputs and labels\n+    outputs_loss = {}\n+    outputs_loss[\"logits\"] = logits\n+    outputs_loss[\"pred_boxes\"] = pred_boxes.clamp(min=0, max=1)\n+    auxiliary_outputs = None\n+    if config.auxiliary_loss:\n+        if denoising_meta_values is not None:\n+            dn_out_coord, outputs_coord = torch.split(\n+                outputs_coord.clamp(min=0, max=1), denoising_meta_values[\"dn_num_split\"], dim=2\n+            )\n+            dn_out_class, outputs_class = torch.split(outputs_class, denoising_meta_values[\"dn_num_split\"], dim=2)\n+            dn_out_corners, out_corners = torch.split(predicted_corners, denoising_meta_values[\"dn_num_split\"], dim=2)\n+            dn_out_refs, out_refs = torch.split(initial_reference_points, denoising_meta_values[\"dn_num_split\"], dim=2)\n+\n+            auxiliary_outputs = _set_aux_loss2(\n+                outputs_class[:, :-1].transpose(0, 1),\n+                outputs_coord[:, :-1].transpose(0, 1),\n+                out_corners[:, :-1].transpose(0, 1),\n+                out_refs[:, :-1].transpose(0, 1),\n+                out_corners[:, -1],\n+                outputs_class[:, -1],\n+            )\n+\n+            outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n+            outputs_loss[\"auxiliary_outputs\"].extend(\n+                _set_aux_loss([enc_topk_logits], [enc_topk_bboxes.clamp(min=0, max=1)])\n+            )\n+\n+            dn_auxiliary_outputs = _set_aux_loss2(\n+                dn_out_class.transpose(0, 1),\n+                dn_out_coord.transpose(0, 1),\n+                dn_out_corners.transpose(0, 1),\n+                dn_out_refs.transpose(0, 1),\n+                dn_out_corners[:, -1],\n+                dn_out_class[:, -1],\n+            )\n+            outputs_loss[\"dn_auxiliary_outputs\"] = dn_auxiliary_outputs\n+            outputs_loss[\"denoising_meta_values\"] = denoising_meta_values\n+\n+    loss_dict = criterion(outputs_loss, labels)\n+\n+    loss = sum(loss_dict.values())\n+    return loss, loss_dict, auxiliary_outputs"
        },
        {
            "sha": "12da2f3d7a087ee22b9ded1d6bf17c976767e5ea",
            "filename": "src/transformers/loss/loss_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Floss%2Floss_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Floss%2Floss_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_utils.py?ref=4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63",
            "patch": "@@ -18,6 +18,7 @@\n import torch.nn as nn\n from torch.nn import BCEWithLogitsLoss, MSELoss\n \n+from .loss_d_fine import DFineForObjectDetectionLoss\n from .loss_deformable_detr import DeformableDetrForObjectDetectionLoss, DeformableDetrForSegmentationLoss\n from .loss_for_object_detection import ForObjectDetectionLoss, ForSegmentationLoss\n from .loss_grounding_dino import GroundingDinoForObjectDetectionLoss\n@@ -156,4 +157,5 @@ def ForTokenClassification(logits: torch.Tensor, labels, config, **kwargs):\n     \"ConditionalDetrForSegmentation\": DeformableDetrForSegmentationLoss,\n     \"RTDetrForObjectDetection\": RTDetrForObjectDetectionLoss,\n     \"RTDetrV2ForObjectDetection\": RTDetrForObjectDetectionLoss,\n+    \"DFineForObjectDetection\": DFineForObjectDetectionLoss,\n }"
        },
        {
            "sha": "d3513db4310136279c94a4d24aa8ab216ef613a4",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63",
            "patch": "@@ -70,6 +70,7 @@\n     from .cpmant import *\n     from .ctrl import *\n     from .cvt import *\n+    from .d_fine import *\n     from .dab_detr import *\n     from .dac import *\n     from .data2vec import *\n@@ -133,6 +134,7 @@\n     from .groupvit import *\n     from .helium import *\n     from .herbert import *\n+    from .hgnet_v2 import *\n     from .hiera import *\n     from .hubert import *\n     from .ibert import *"
        },
        {
            "sha": "f4dd37d64c7de26cf508a940f442c1e6277f5560",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63",
            "patch": "@@ -82,6 +82,7 @@\n         (\"cpmant\", \"CpmAntConfig\"),\n         (\"ctrl\", \"CTRLConfig\"),\n         (\"cvt\", \"CvtConfig\"),\n+        (\"d_fine\", \"DFineConfig\"),\n         (\"dab-detr\", \"DabDetrConfig\"),\n         (\"dac\", \"DacConfig\"),\n         (\"data2vec-audio\", \"Data2VecAudioConfig\"),\n@@ -151,6 +152,7 @@\n         (\"grounding-dino\", \"GroundingDinoConfig\"),\n         (\"groupvit\", \"GroupViTConfig\"),\n         (\"helium\", \"HeliumConfig\"),\n+        (\"hgnet_v2\", \"HGNetV2Config\"),\n         (\"hiera\", \"HieraConfig\"),\n         (\"hubert\", \"HubertConfig\"),\n         (\"ibert\", \"IBertConfig\"),\n@@ -436,6 +438,7 @@\n         (\"cpmant\", \"CPM-Ant\"),\n         (\"ctrl\", \"CTRL\"),\n         (\"cvt\", \"CvT\"),\n+        (\"d_fine\", \"D-FINE\"),\n         (\"dab-detr\", \"DAB-DETR\"),\n         (\"dac\", \"DAC\"),\n         (\"data2vec-audio\", \"Data2VecAudio\"),\n@@ -513,6 +516,7 @@\n         (\"groupvit\", \"GroupViT\"),\n         (\"helium\", \"Helium\"),\n         (\"herbert\", \"HerBERT\"),\n+        (\"hgnet_v2\", \"HGNet-V2\"),\n         (\"hiera\", \"Hiera\"),\n         (\"hubert\", \"Hubert\"),\n         (\"ibert\", \"I-BERT\"),"
        },
        {
            "sha": "1dd4a25102198c3e7491f95c0f232ba5a72d9fb8",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63",
            "patch": "@@ -80,6 +80,7 @@\n         (\"cpmant\", \"CpmAntModel\"),\n         (\"ctrl\", \"CTRLModel\"),\n         (\"cvt\", \"CvtModel\"),\n+        (\"d_fine\", \"DFineModel\"),\n         (\"dab-detr\", \"DabDetrModel\"),\n         (\"dac\", \"DacModel\"),\n         (\"data2vec-audio\", \"Data2VecAudioModel\"),\n@@ -142,6 +143,7 @@\n         (\"grounding-dino\", \"GroundingDinoModel\"),\n         (\"groupvit\", \"GroupViTModel\"),\n         (\"helium\", \"HeliumModel\"),\n+        (\"hgnet_v2\", \"HGNetV2Backbone\"),\n         (\"hiera\", \"HieraModel\"),\n         (\"hubert\", \"HubertModel\"),\n         (\"ibert\", \"IBertModel\"),\n@@ -729,6 +731,7 @@\n         ),\n         (\"efficientnet\", \"EfficientNetForImageClassification\"),\n         (\"focalnet\", \"FocalNetForImageClassification\"),\n+        (\"hgnet_v2\", \"HGNetV2ForImageClassification\"),\n         (\"hiera\", \"HieraForImageClassification\"),\n         (\"ijepa\", \"IJepaForImageClassification\"),\n         (\"imagegpt\", \"ImageGPTForImageClassification\"),\n@@ -945,6 +948,7 @@\n     [\n         # Model for Object Detection mapping\n         (\"conditional_detr\", \"ConditionalDetrForObjectDetection\"),\n+        (\"d_fine\", \"DFineForObjectDetection\"),\n         (\"dab-detr\", \"DabDetrForObjectDetection\"),\n         (\"deformable_detr\", \"DeformableDetrForObjectDetection\"),\n         (\"deta\", \"DetaForObjectDetection\"),\n@@ -1476,6 +1480,7 @@\n         (\"dinov2\", \"Dinov2Backbone\"),\n         (\"dinov2_with_registers\", \"Dinov2WithRegistersBackbone\"),\n         (\"focalnet\", \"FocalNetBackbone\"),\n+        (\"hgnet_v2\", \"HGNetV2Backbone\"),\n         (\"hiera\", \"HieraBackbone\"),\n         (\"maskformer-swin\", \"MaskFormerSwinBackbone\"),\n         (\"nat\", \"NatBackbone\"),"
        },
        {
            "sha": "879b53709bc673bcf28553a51175f06fa1e362c0",
            "filename": "src/transformers/models/d_fine/__init__.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Fd_fine%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Fd_fine%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2F__init__.py?ref=4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63",
            "patch": "@@ -0,0 +1,29 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_d_fine import *\n+    from .modeling_d_fine import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "4775203f19cb6035397c5cb1477788fff006a21b",
            "filename": "src/transformers/models/d_fine/configuration_d_fine.py",
            "status": "added",
            "additions": 425,
            "deletions": 0,
            "changes": 425,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py?ref=4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63",
            "patch": "@@ -0,0 +1,425 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/d_fine/modular_d_fine.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_d_fine.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 Baidu Inc and The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+from ...utils.backbone_utils import verify_backbone_config_arguments\n+from ..auto import CONFIG_MAPPING\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+# TODO: Attribute map assignment logic should be fixed in modular\n+# as well as super() call parsing becuase otherwise we cannot re-write args after initialization\n+class DFineConfig(PretrainedConfig):\n+    \"\"\"\n+    This is the configuration class to store the configuration of a [`DFineModel`]. It is used to instantiate a D-FINE\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of D-FINE-X-COCO \"[ustc-community/dfine_x_coco\"](https://huggingface.co/ustc-community/dfine_x_coco\").\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        initializer_range (`float`, *optional*, defaults to 0.01):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        initializer_bias_prior_prob (`float`, *optional*):\n+            The prior probability used by the bias initializer to initialize biases for `enc_score_head` and `class_embed`.\n+            If `None`, `prior_prob` computed as `prior_prob = 1 / (num_labels + 1)` while initializing model weights.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the layer normalization layers.\n+        batch_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the batch normalization layers.\n+        backbone_config (`Dict`, *optional*, defaults to `RTDetrResNetConfig()`):\n+            The configuration of the backbone model.\n+        backbone (`str`, *optional*):\n+            Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this\n+            will load the corresponding pretrained weights from the timm or transformers library. If `use_pretrained_backbone`\n+            is `False`, this loads the backbone's config and uses that to initialize the backbone with random weights.\n+        use_pretrained_backbone (`bool`, *optional*, defaults to `False`):\n+            Whether to use pretrained weights for the backbone.\n+        use_timm_backbone (`bool`, *optional*, defaults to `False`):\n+            Whether to load `backbone` from the timm library. If `False`, the backbone is loaded from the transformers\n+            library.\n+        freeze_backbone_batch_norms (`bool`, *optional*, defaults to `True`):\n+            Whether to freeze the batch normalization layers in the backbone.\n+        backbone_kwargs (`dict`, *optional*):\n+            Keyword arguments to be passed to AutoBackbone when loading from a checkpoint\n+            e.g. `{'out_indices': (0, 1, 2, 3)}`. Cannot be specified if `backbone_config` is set.\n+        encoder_hidden_dim (`int`, *optional*, defaults to 256):\n+            Dimension of the layers in hybrid encoder.\n+        encoder_in_channels (`list`, *optional*, defaults to `[512, 1024, 2048]`):\n+            Multi level features input for encoder.\n+        feat_strides (`List[int]`, *optional*, defaults to `[8, 16, 32]`):\n+            Strides used in each feature map.\n+        encoder_layers (`int`, *optional*, defaults to 1):\n+            Total of layers to be used by the encoder.\n+        encoder_ffn_dim (`int`, *optional*, defaults to 1024):\n+            Dimension of the \"intermediate\" (often named feed-forward) layer in decoder.\n+        encoder_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        dropout (`float`, *optional*, defaults to 0.0):\n+            The ratio for all dropout layers.\n+        activation_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for activations inside the fully connected layer.\n+        encode_proj_layers (`List[int]`, *optional*, defaults to `[2]`):\n+            Indexes of the projected layers to be used in the encoder.\n+        positional_encoding_temperature (`int`, *optional*, defaults to 10000):\n+            The temperature parameter used to create the positional encodings.\n+        encoder_activation_function (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n+        activation_function (`str`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the general layer. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n+        eval_size (`Tuple[int, int]`, *optional*):\n+            Height and width used to computes the effective height and width of the position embeddings after taking\n+            into account the stride.\n+        normalize_before (`bool`, *optional*, defaults to `False`):\n+            Determine whether to apply layer normalization in the transformer encoder layer before self-attention and\n+            feed-forward modules.\n+        hidden_expansion (`float`, *optional*, defaults to 1.0):\n+            Expansion ratio to enlarge the dimension size of RepVGGBlock and CSPRepLayer.\n+        d_model (`int`, *optional*, defaults to 256):\n+            Dimension of the layers exclude hybrid encoder.\n+        num_queries (`int`, *optional*, defaults to 300):\n+            Number of object queries.\n+        decoder_in_channels (`list`, *optional*, defaults to `[256, 256, 256]`):\n+            Multi level features dimension for decoder\n+        decoder_ffn_dim (`int`, *optional*, defaults to 1024):\n+            Dimension of the \"intermediate\" (often named feed-forward) layer in decoder.\n+        num_feature_levels (`int`, *optional*, defaults to 3):\n+            The number of input feature levels.\n+        decoder_n_points (`int`, *optional*, defaults to 4):\n+            The number of sampled keys in each feature level for each attention head in the decoder.\n+        decoder_layers (`int`, *optional*, defaults to 6):\n+            Number of decoder layers.\n+        decoder_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        decoder_activation_function (`str`, *optional*, defaults to `\"relu\"`):\n+            The non-linear activation function (function or string) in the decoder. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        num_denoising (`int`, *optional*, defaults to 100):\n+            The total number of denoising tasks or queries to be used for contrastive denoising.\n+        label_noise_ratio (`float`, *optional*, defaults to 0.5):\n+            The fraction of denoising labels to which random noise should be added.\n+        box_noise_scale (`float`, *optional*, defaults to 1.0):\n+            Scale or magnitude of noise to be added to the bounding boxes.\n+        learn_initial_query (`bool`, *optional*, defaults to `False`):\n+            Indicates whether the initial query embeddings for the decoder should be learned during training\n+        anchor_image_size (`Tuple[int, int]`, *optional*):\n+            Height and width of the input image used during evaluation to generate the bounding box anchors. If None, automatic generate anchor is applied.\n+        with_box_refine (`bool`, *optional*, defaults to `True`):\n+            Whether to apply iterative bounding box refinement, where each decoder layer refines the bounding boxes\n+            based on the predictions from the previous layer.\n+        is_encoder_decoder (`bool`, *optional*, defaults to `True`):\n+            Whether the architecture has an encoder decoder structure.\n+        matcher_alpha (`float`, *optional*, defaults to 0.25):\n+            Parameter alpha used by the Hungarian Matcher.\n+        matcher_gamma (`float`, *optional*, defaults to 2.0):\n+            Parameter gamma used by the Hungarian Matcher.\n+        matcher_class_cost (`float`, *optional*, defaults to 2.0):\n+            The relative weight of the class loss used by the Hungarian Matcher.\n+        matcher_bbox_cost (`float`, *optional*, defaults to 5.0):\n+            The relative weight of the bounding box loss used by the Hungarian Matcher.\n+        matcher_giou_cost (`float`, *optional*, defaults to 2.0):\n+            The relative weight of the giou loss of used by the Hungarian Matcher.\n+        use_focal_loss (`bool`, *optional*, defaults to `True`):\n+            Parameter informing if focal focal should be used.\n+        auxiliary_loss (`bool`, *optional*, defaults to `True`):\n+            Whether auxiliary decoding losses (loss at each decoder layer) are to be used.\n+        focal_loss_alpha (`float`, *optional*, defaults to 0.75):\n+            Parameter alpha used to compute the focal loss.\n+        focal_loss_gamma (`float`, *optional*, defaults to 2.0):\n+            Parameter gamma used to compute the focal loss.\n+        weight_loss_vfl (`float`, *optional*, defaults to 1.0):\n+            Relative weight of the varifocal loss in the object detection loss.\n+        weight_loss_bbox (`float`, *optional*, defaults to 5.0):\n+            Relative weight of the L1 bounding box loss in the object detection loss.\n+        weight_loss_giou (`float`, *optional*, defaults to 2.0):\n+            Relative weight of the generalized IoU loss in the object detection loss.\n+        weight_loss_fgl (`float`, *optional*, defaults to 0.15):\n+            Relative weight of the fine-grained localization loss in the object detection loss.\n+        weight_loss_ddf (`float`, *optional*, defaults to 1.5):\n+            Relative weight of the decoupled distillation focal loss in the object detection loss.\n+        eos_coefficient (`float`, *optional*, defaults to 0.0001):\n+            Relative classification weight of the 'no-object' class in the object detection loss.\n+        eval_idx (`int`, *optional*, defaults to -1):\n+            Index of the decoder layer to use for evaluation. If negative, counts from the end\n+            (e.g., -1 means use the last layer). This allows for early prediction in the decoder\n+            stack while still training later layers.\n+        layer_scale (`float`, *optional*, defaults to `1.0`):\n+            Scaling factor for the hidden dimension in later decoder layers. Used to adjust the\n+            model capacity after the evaluation layer.\n+        max_num_bins (`int`, *optional*, defaults to 32):\n+            Maximum number of bins for the distribution-guided bounding box refinement.\n+            Higher values allow for more fine-grained localization but increase computation.\n+        reg_scale (`float`, *optional*, defaults to 4.0):\n+            Scale factor for the regression distribution. Controls the range and granularity\n+            of the bounding box refinement process.\n+        depth_mult (`float`, *optional*, defaults to 1.0):\n+            Multiplier for the number of blocks in RepNCSPELAN4 layers. Used to scale the model's\n+            depth while maintaining its architecture.\n+        top_prob_values (`int`, *optional*, defaults to 4):\n+            Number of top probability values to consider from each corner's distribution.\n+        lqe_hidden_dim (`int`, *optional*, defaults to 64):\n+            Hidden dimension size for the Location Quality Estimator (LQE) network.\n+        lqe_layers (`int`, *optional*, defaults to 2):\n+            Number of layers in the Location Quality Estimator MLP.\n+        decoder_offset_scale (`float`, *optional*, defaults to 0.5):\n+            Offset scale used in deformable attention.\n+        decoder_method (`str`, *optional*, defaults to `\"default\"`):\n+            The method to use for the decoder: `\"default\"` or `\"discrete\"`.\n+        up (`float`, *optional*, defaults to 0.5):\n+            Controls the upper bounds of the Weighting Function.\n+    \"\"\"\n+\n+    model_type = \"d_fine\"\n+    layer_types = [\"basic\", \"bottleneck\"]\n+    attribute_map = {\n+        \"hidden_size\": \"d_model\",\n+        \"num_attention_heads\": \"encoder_attention_heads\",\n+    }\n+\n+    def __init__(\n+        self,\n+        initializer_range=0.01,\n+        initializer_bias_prior_prob=None,\n+        layer_norm_eps=1e-5,\n+        batch_norm_eps=1e-5,\n+        # backbone\n+        backbone_config=None,\n+        backbone=None,\n+        use_pretrained_backbone=False,\n+        use_timm_backbone=False,\n+        freeze_backbone_batch_norms=True,\n+        backbone_kwargs=None,\n+        # encoder HybridEncoder\n+        encoder_hidden_dim=256,\n+        encoder_in_channels=[512, 1024, 2048],\n+        feat_strides=[8, 16, 32],\n+        encoder_layers=1,\n+        encoder_ffn_dim=1024,\n+        encoder_attention_heads=8,\n+        dropout=0.0,\n+        activation_dropout=0.0,\n+        encode_proj_layers=[2],\n+        positional_encoding_temperature=10000,\n+        encoder_activation_function=\"gelu\",\n+        activation_function=\"silu\",\n+        eval_size=None,\n+        normalize_before=False,\n+        hidden_expansion=1.0,\n+        # decoder DFineTransformer\n+        d_model=256,\n+        num_queries=300,\n+        decoder_in_channels=[256, 256, 256],\n+        decoder_ffn_dim=1024,\n+        num_feature_levels=3,\n+        decoder_n_points=4,\n+        decoder_layers=6,\n+        decoder_attention_heads=8,\n+        decoder_activation_function=\"relu\",\n+        attention_dropout=0.0,\n+        num_denoising=100,\n+        label_noise_ratio=0.5,\n+        box_noise_scale=1.0,\n+        learn_initial_query=False,\n+        anchor_image_size=None,\n+        with_box_refine=True,\n+        is_encoder_decoder=True,\n+        # Loss\n+        matcher_alpha=0.25,\n+        matcher_gamma=2.0,\n+        matcher_class_cost=2.0,\n+        matcher_bbox_cost=5.0,\n+        matcher_giou_cost=2.0,\n+        use_focal_loss=True,\n+        auxiliary_loss=True,\n+        focal_loss_alpha=0.75,\n+        focal_loss_gamma=2.0,\n+        weight_loss_vfl=1.0,\n+        weight_loss_bbox=5.0,\n+        weight_loss_giou=2.0,\n+        weight_loss_fgl=0.15,\n+        weight_loss_ddf=1.5,\n+        eos_coefficient=1e-4,\n+        eval_idx=-1,\n+        layer_scale=1,\n+        max_num_bins=32,\n+        reg_scale=4.0,\n+        depth_mult=1.0,\n+        top_prob_values=4,\n+        lqe_hidden_dim=64,\n+        lqe_layers=2,\n+        decoder_offset_scale=0.5,\n+        decoder_method=\"default\",\n+        up=0.5,\n+        **kwargs,\n+    ):\n+        self.initializer_range = initializer_range\n+        self.initializer_bias_prior_prob = initializer_bias_prior_prob\n+        self.layer_norm_eps = layer_norm_eps\n+        self.batch_norm_eps = batch_norm_eps\n+        # backbone\n+        if backbone_config is None and backbone is None:\n+            logger.info(\n+                \"`backbone_config` and `backbone` are `None`. Initializing the config with the default `HGNet-V2` backbone.\"\n+            )\n+            backbone_model_type = \"hgnet_v2\"\n+            config_class = CONFIG_MAPPING[backbone_model_type]\n+            # this will map it to RTDetrResNetConfig\n+            # note: we can instead create HGNetV2Config\n+            # and we would need to create HGNetV2Backbone\n+            backbone_config = config_class(\n+                num_channels=3,\n+                embedding_size=64,\n+                hidden_sizes=[256, 512, 1024, 2048],\n+                depths=[3, 4, 6, 3],\n+                layer_type=\"bottleneck\",\n+                hidden_act=\"relu\",\n+                downsample_in_first_stage=False,\n+                downsample_in_bottleneck=False,\n+                out_features=None,\n+                out_indices=[2, 3, 4],\n+            )\n+        elif isinstance(backbone_config, dict):\n+            backbone_model_type = backbone_config.pop(\"model_type\")\n+            config_class = CONFIG_MAPPING[backbone_model_type]\n+            backbone_config = config_class.from_dict(backbone_config)\n+\n+        verify_backbone_config_arguments(\n+            use_timm_backbone=use_timm_backbone,\n+            use_pretrained_backbone=use_pretrained_backbone,\n+            backbone=backbone,\n+            backbone_config=backbone_config,\n+            backbone_kwargs=backbone_kwargs,\n+        )\n+\n+        self.backbone_config = backbone_config\n+        self.backbone = backbone\n+        self.use_pretrained_backbone = use_pretrained_backbone\n+        self.use_timm_backbone = use_timm_backbone\n+        self.freeze_backbone_batch_norms = freeze_backbone_batch_norms\n+        self.backbone_kwargs = backbone_kwargs\n+        # encoder\n+        self.encoder_hidden_dim = encoder_hidden_dim\n+        self.encoder_in_channels = encoder_in_channels\n+        self.feat_strides = feat_strides\n+        self.encoder_attention_heads = encoder_attention_heads\n+        self.encoder_ffn_dim = encoder_ffn_dim\n+        self.dropout = dropout\n+        self.activation_dropout = activation_dropout\n+        self.encode_proj_layers = encode_proj_layers\n+        self.encoder_layers = encoder_layers\n+        self.positional_encoding_temperature = positional_encoding_temperature\n+        self.eval_size = eval_size\n+        self.normalize_before = normalize_before\n+        self.encoder_activation_function = encoder_activation_function\n+        self.activation_function = activation_function\n+        self.hidden_expansion = hidden_expansion\n+        # decoder\n+        self.d_model = d_model\n+        self.num_queries = num_queries\n+        self.decoder_ffn_dim = decoder_ffn_dim\n+        self.decoder_in_channels = decoder_in_channels\n+        self.num_feature_levels = num_feature_levels\n+        self.decoder_n_points = decoder_n_points\n+        self.decoder_layers = decoder_layers\n+        self.decoder_attention_heads = decoder_attention_heads\n+        self.decoder_activation_function = decoder_activation_function\n+        self.attention_dropout = attention_dropout\n+        self.num_denoising = num_denoising\n+        self.label_noise_ratio = label_noise_ratio\n+        self.box_noise_scale = box_noise_scale\n+        self.learn_initial_query = learn_initial_query\n+        self.anchor_image_size = anchor_image_size\n+        self.auxiliary_loss = auxiliary_loss\n+        self.with_box_refine = with_box_refine\n+        # Loss\n+        self.matcher_alpha = matcher_alpha\n+        self.matcher_gamma = matcher_gamma\n+        self.matcher_class_cost = matcher_class_cost\n+        self.matcher_bbox_cost = matcher_bbox_cost\n+        self.matcher_giou_cost = matcher_giou_cost\n+        self.use_focal_loss = use_focal_loss\n+        self.focal_loss_alpha = focal_loss_alpha\n+        self.focal_loss_gamma = focal_loss_gamma\n+        self.weight_loss_vfl = weight_loss_vfl\n+        self.weight_loss_bbox = weight_loss_bbox\n+        self.weight_loss_giou = weight_loss_giou\n+        self.weight_loss_fgl = weight_loss_fgl\n+        self.weight_loss_ddf = weight_loss_ddf\n+        self.eos_coefficient = eos_coefficient\n+        # add the new attributes with the given values or defaults\n+        self.eval_idx = eval_idx\n+        self.layer_scale = layer_scale\n+        self.max_num_bins = max_num_bins\n+        self.reg_scale = reg_scale\n+        self.depth_mult = depth_mult\n+        self.decoder_offset_scale = decoder_offset_scale\n+        self.decoder_method = decoder_method\n+        self.top_prob_values = top_prob_values\n+        self.lqe_hidden_dim = lqe_hidden_dim\n+        self.lqe_layers = lqe_layers\n+        self.up = up\n+\n+        if isinstance(self.decoder_n_points, list):\n+            if len(self.decoder_n_points) != self.num_feature_levels:\n+                raise ValueError(\n+                    f\"Length of decoder_n_points list ({len(self.decoder_n_points)}) must match num_feature_levels ({self.num_feature_levels}).\"\n+                )\n+\n+        head_dim = self.d_model // self.decoder_attention_heads\n+        if head_dim * self.decoder_attention_heads != self.d_model:\n+            raise ValueError(\n+                f\"Embedded dimension {self.d_model} must be divisible by decoder_attention_heads {self.decoder_attention_heads}\"\n+            )\n+        super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n+\n+    @property\n+    def num_attention_heads(self) -> int:\n+        return self.encoder_attention_heads\n+\n+    @property\n+    def hidden_size(self) -> int:\n+        return self.d_model\n+\n+    @classmethod\n+    def from_backbone_configs(cls, backbone_config: PretrainedConfig, **kwargs):\n+        \"\"\"Instantiate a [`DFineConfig`] (or a derived class) from a pre-trained backbone model configuration and DETR model\n+        configuration.\n+\n+            Args:\n+                backbone_config ([`PretrainedConfig`]):\n+                    The backbone configuration.\n+\n+            Returns:\n+                [`DFineConfig`]: An instance of a configuration object\n+        \"\"\"\n+        return cls(\n+            backbone_config=backbone_config,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"DFineConfig\"]"
        },
        {
            "sha": "fa217f6b0eedadc5c3ffeb31e66a2d2ac5c06ea3",
            "filename": "src/transformers/models/d_fine/convert_d_fine_original_pytorch_checkpoint_to_hf.py",
            "status": "added",
            "additions": 688,
            "deletions": 0,
            "changes": 688,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconvert_d_fine_original_pytorch_checkpoint_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconvert_d_fine_original_pytorch_checkpoint_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconvert_d_fine_original_pytorch_checkpoint_to_hf.py?ref=4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63",
            "patch": "@@ -0,0 +1,688 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import argparse\n+import json\n+import re\n+from pathlib import Path\n+\n+import requests\n+import torch\n+from huggingface_hub import hf_hub_download\n+from PIL import Image\n+from torchvision import transforms\n+\n+from transformers import DFineConfig, DFineForObjectDetection, RTDetrImageProcessor\n+from transformers.utils import logging\n+\n+\n+logging.set_verbosity_info()\n+logger = logging.get_logger(__name__)\n+\n+\n+def get_d_fine_config(model_name: str) -> DFineConfig:\n+    config = DFineConfig()\n+\n+    config.num_labels = 80\n+    repo_id = \"huggingface/label-files\"\n+    filename = \"object365-id2label.json\" if \"obj365\" in model_name else \"coco-detection-mmdet-id2label.json\"\n+    id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type=\"dataset\"), \"r\"))\n+    id2label = {int(k): v for k, v in id2label.items()}\n+    config.id2label = id2label\n+    config.label2id = {v: k for k, v in id2label.items()}\n+\n+    config.backbone_config.hidden_sizes = [64, 128, 256, 512]\n+    config.backbone_config.layer_type = \"basic\"\n+    config.backbone_config.embedding_size = 32\n+    config.hidden_expansion = 1.0\n+    config.decoder_layers = 6\n+\n+    if model_name in [\"dfine_x_coco\", \"dfine_x_obj2coco\", \"dfine_x_obj365\"]:\n+        config.backbone_config.hidden_sizes = [256, 512, 1024, 2048]\n+        config.backbone_config.stage_in_channels = [64, 128, 512, 1024]\n+        config.backbone_config.stage_mid_channels = [64, 128, 256, 512]\n+        config.backbone_config.stage_out_channels = [128, 512, 1024, 2048]\n+        config.backbone_config.stage_num_blocks = [1, 2, 5, 2]\n+        config.backbone_config.stage_downsample = [False, True, True, True]\n+        config.backbone_config.stage_light_block = [False, False, True, True]\n+        config.backbone_config.stage_kernel_size = [3, 3, 5, 5]\n+        config.backbone_config.stage_numb_of_layers = [6, 6, 6, 6]\n+        config.backbone_config.stem_channels = [3, 32, 64]\n+        config.encoder_in_channels = [512, 1024, 2048]\n+        config.encoder_hidden_dim = 384\n+        config.encoder_ffn_dim = 2048\n+        config.decoder_n_points = [3, 6, 3]\n+        config.decoder_in_channels = [384, 384, 384]\n+        if model_name == \"dfine_x_obj365\":\n+            config.num_labels = 366\n+    elif model_name in [\"dfine_m_coco\", \"dfine_m_obj2coco\", \"dfine_m_obj365\"]:\n+        config.backbone_config.hidden_sizes = [192, 384, 768, 1536]\n+        config.backbone_config.stem_channels = [3, 24, 32]\n+        config.backbone_config.stage_in_channels = [32, 96, 384, 768]\n+        config.backbone_config.stage_mid_channels = [32, 64, 128, 256]\n+        config.backbone_config.stage_out_channels = [96, 384, 768, 1536]\n+        config.backbone_config.stage_num_blocks = [1, 1, 3, 1]\n+        config.backbone_config.stage_downsample = [False, True, True, True]\n+        config.backbone_config.stage_light_block = [False, False, True, True]\n+        config.backbone_config.stage_kernel_size = [3, 3, 5, 5]\n+        config.backbone_config.stage_numb_of_layers = [4, 4, 4, 4]\n+        config.decoder_layers = 4\n+        config.decoder_n_points = [3, 6, 3]\n+        config.encoder_in_channels = [384, 768, 1536]\n+        config.backbone_config.use_learnable_affine_block = True\n+        config.depth_mult = 0.67\n+        if model_name == \"dfine_m_obj365\":\n+            config.num_labels = 366\n+    elif model_name in [\"dfine_l_coco\", \"dfine_l_obj2coco_e25\", \"dfine_l_obj365\"]:\n+        config.backbone_config.hidden_sizes = [256, 512, 1024, 2048]\n+        config.backbone_config.stem_channels = [3, 32, 48]\n+        config.backbone_config.stage_in_channels = [48, 128, 512, 1024]\n+        config.backbone_config.stage_mid_channels = [48, 96, 192, 384]\n+        config.backbone_config.stage_out_channels = [128, 512, 1024, 2048]\n+        config.backbone_config.stage_num_blocks = [1, 1, 3, 1]\n+        config.backbone_config.stage_downsample = [False, True, True, True]\n+        config.backbone_config.stage_light_block = [False, False, True, True]\n+        config.backbone_config.stage_kernel_size = [3, 3, 5, 5]\n+        config.backbone_config.stage_numb_of_layers = [6, 6, 6, 6]\n+        config.encoder_ffn_dim = 1024\n+        config.encoder_in_channels = [512, 1024, 2048]\n+        config.decoder_n_points = [3, 6, 3]\n+        if model_name == \"dfine_l_obj365\":\n+            config.num_labels = 366\n+    elif model_name in [\"dfine_n_coco\", \"dfine_n_obj2coco_e25\", \"dfine_n_obj365\"]:\n+        config.backbone_config.hidden_sizes = [128, 256, 512, 1024]\n+        config.backbone_config.stem_channels = [3, 16, 16]\n+        config.backbone_config.stage_in_channels = [16, 64, 256, 512]\n+        config.backbone_config.stage_mid_channels = [16, 32, 64, 128]\n+        config.backbone_config.stage_out_channels = [64, 256, 512, 1024]\n+        config.backbone_config.stage_num_blocks = [1, 1, 2, 1]\n+        config.backbone_config.stage_downsample = [False, True, True, True]\n+        config.backbone_config.stage_light_block = [False, False, True, True]\n+        config.backbone_config.stage_kernel_size = [3, 3, 5, 5]\n+        config.backbone_config.stage_numb_of_layers = [3, 3, 3, 3]\n+        config.backbone_config.out_indices = [3, 4]\n+        config.backbone_config.use_learnable_affine_block = True\n+        config.num_feature_levels = 2\n+        config.encoder_ffn_dim = 512\n+        config.encode_proj_layers = [1]\n+        config.d_model = 128\n+        config.encoder_hidden_dim = 128\n+        config.decoder_ffn_dim = 512\n+        config.encoder_in_channels = [512, 1024]\n+        config.decoder_n_points = [6, 6]\n+        config.decoder_in_channels = [128, 128]\n+        config.feat_strides = [16, 32]\n+        config.depth_mult = 0.5\n+        config.decoder_layers = 3\n+        config.hidden_expansion = 0.34\n+        if model_name == \"dfine_n_obj365\":\n+            config.num_labels = 366\n+    else:\n+        config.backbone_config.hidden_sizes = [128, 256, 512, 1024]\n+        config.backbone_config.stem_channels = [3, 16, 16]\n+        config.backbone_config.stage_in_channels = [16, 64, 256, 512]\n+        config.backbone_config.stage_mid_channels = [16, 32, 64, 128]\n+        config.backbone_config.stage_out_channels = [64, 256, 512, 1024]\n+        config.backbone_config.stage_num_blocks = [1, 1, 2, 1]\n+        config.backbone_config.stage_downsample = [False, True, True, True]\n+        config.backbone_config.stage_light_block = [False, False, True, True]\n+        config.backbone_config.stage_kernel_size = [3, 3, 5, 5]\n+        config.backbone_config.stage_numb_of_layers = [3, 3, 3, 3]\n+        config.decoder_layers = 3\n+        config.hidden_expansion = 0.5\n+        config.depth_mult = 0.34\n+        config.decoder_n_points = [3, 6, 3]\n+        config.encoder_in_channels = [256, 512, 1024]\n+        config.backbone_config.use_learnable_affine_block = True\n+        if model_name == \"dfine_s_obj365\":\n+            config.num_labels = 366\n+\n+    return config\n+\n+\n+def load_original_state_dict(repo_id, model_name):\n+    directory_path = hf_hub_download(repo_id=repo_id, filename=f\"{model_name}.pth\")\n+\n+    original_state_dict = {}\n+    model = torch.load(directory_path, map_location=\"cpu\")[\"model\"]\n+    for key in model.keys():\n+        original_state_dict[key] = model[key]\n+\n+    return original_state_dict\n+\n+\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    # Decoder base mappings\n+    r\"decoder.valid_mask\": r\"model.decoder.valid_mask\",\n+    r\"decoder.anchors\": r\"model.decoder.anchors\",\n+    r\"decoder.up\": r\"model.decoder.up\",\n+    r\"decoder.reg_scale\": r\"model.decoder.reg_scale\",\n+    # Backbone stem mappings - including stem2a and stem2b\n+    r\"backbone.stem.stem1.conv.weight\": r\"model.backbone.model.embedder.stem1.convolution.weight\",\n+    r\"backbone.stem.stem2a.conv.weight\": r\"model.backbone.model.embedder.stem2a.convolution.weight\",\n+    r\"backbone.stem.stem2b.conv.weight\": r\"model.backbone.model.embedder.stem2b.convolution.weight\",\n+    r\"backbone.stem.stem3.conv.weight\": r\"model.backbone.model.embedder.stem3.convolution.weight\",\n+    r\"backbone.stem.stem4.conv.weight\": r\"model.backbone.model.embedder.stem4.convolution.weight\",\n+    # Stem normalization\n+    r\"backbone.stem.stem1.bn.(weight|bias|running_mean|running_var)\": r\"model.backbone.model.embedder.stem1.normalization.\\1\",\n+    r\"backbone.stem.stem2a.bn.(weight|bias|running_mean|running_var)\": r\"model.backbone.model.embedder.stem2a.normalization.\\1\",\n+    r\"backbone.stem.stem2b.bn.(weight|bias|running_mean|running_var)\": r\"model.backbone.model.embedder.stem2b.normalization.\\1\",\n+    r\"backbone.stem.stem3.bn.(weight|bias|running_mean|running_var)\": r\"model.backbone.model.embedder.stem3.normalization.\\1\",\n+    r\"backbone.stem.stem4.bn.(weight|bias|running_mean|running_var)\": r\"model.backbone.model.embedder.stem4.normalization.\\1\",\n+    # Stem lab parameters - fixed with .lab in the path\n+    r\"backbone.stem.stem1.lab.(scale|bias)\": r\"model.backbone.model.embedder.stem1.lab.\\1\",\n+    r\"backbone.stem.stem2a.lab.(scale|bias)\": r\"model.backbone.model.embedder.stem2a.lab.\\1\",\n+    r\"backbone.stem.stem2b.lab.(scale|bias)\": r\"model.backbone.model.embedder.stem2b.lab.\\1\",\n+    r\"backbone.stem.stem3.lab.(scale|bias)\": r\"model.backbone.model.embedder.stem3.lab.\\1\",\n+    r\"backbone.stem.stem4.lab.(scale|bias)\": r\"model.backbone.model.embedder.stem4.lab.\\1\",\n+    # Backbone stages mappings\n+    r\"backbone.stages.(\\d+).blocks.(\\d+).layers.(\\d+).conv.weight\": r\"model.backbone.model.encoder.stages.\\1.blocks.\\2.layers.\\3.convolution.weight\",\n+    r\"backbone.stages.(\\d+).blocks.(\\d+).layers.(\\d+).bn.(weight|bias|running_mean|running_var)\": r\"model.backbone.model.encoder.stages.\\1.blocks.\\2.layers.\\3.normalization.\\4\",\n+    r\"backbone.stages.(\\d+).blocks.(\\d+).layers.(\\d+).conv1.conv.weight\": r\"model.backbone.model.encoder.stages.\\1.blocks.\\2.layers.\\3.conv1.convolution.weight\",\n+    r\"backbone.stages.(\\d+).blocks.(\\d+).layers.(\\d+).conv2.conv.weight\": r\"model.backbone.model.encoder.stages.\\1.blocks.\\2.layers.\\3.conv2.convolution.weight\",\n+    r\"backbone.stages.(\\d+).blocks.(\\d+).layers.(\\d+).conv1.bn.(weight|bias|running_mean|running_var)\": r\"model.backbone.model.encoder.stages.\\1.blocks.\\2.layers.\\3.conv1.normalization.\\4\",\n+    r\"backbone.stages.(\\d+).blocks.(\\d+).layers.(\\d+).conv2.bn.(weight|bias|running_mean|running_var)\": r\"model.backbone.model.encoder.stages.\\1.blocks.\\2.layers.\\3.conv2.normalization.\\4\",\n+    # Backbone stages aggregation\n+    r\"backbone.stages.(\\d+).blocks.(\\d+).aggregation.0.conv.weight\": r\"model.backbone.model.encoder.stages.\\1.blocks.\\2.aggregation.0.convolution.weight\",\n+    r\"backbone.stages.(\\d+).blocks.(\\d+).aggregation.1.conv.weight\": r\"model.backbone.model.encoder.stages.\\1.blocks.\\2.aggregation.1.convolution.weight\",\n+    r\"backbone.stages.(\\d+).blocks.(\\d+).aggregation.0.bn.(weight|bias|running_mean|running_var)\": r\"model.backbone.model.encoder.stages.\\1.blocks.\\2.aggregation.0.normalization.\\3\",\n+    r\"backbone.stages.(\\d+).blocks.(\\d+).aggregation.1.bn.(weight|bias|running_mean|running_var)\": r\"model.backbone.model.encoder.stages.\\1.blocks.\\2.aggregation.1.normalization.\\3\",\n+    # Backbone stages lab parameters for aggregation\n+    r\"backbone.stages.(\\d+).blocks.(\\d+).aggregation.0.lab.(scale|bias)\": r\"model.backbone.model.encoder.stages.\\1.blocks.\\2.aggregation.0.lab.\\3\",\n+    r\"backbone.stages.(\\d+).blocks.(\\d+).aggregation.1.lab.(scale|bias)\": r\"model.backbone.model.encoder.stages.\\1.blocks.\\2.aggregation.1.lab.\\3\",\n+    r\"backbone.stages.(\\d+).blocks.(\\d+).layers.(\\d+).lab.(scale|bias)\": r\"model.backbone.model.encoder.stages.\\1.blocks.\\2.layers.\\3.lab.\\4\",\n+    # Conv1/Conv2 layers with lab\n+    r\"backbone.stages.(\\d+).blocks.(\\d+).layers.(\\d+).conv1.lab.(scale|bias)\": r\"model.backbone.model.encoder.stages.\\1.blocks.\\2.layers.\\3.conv1.lab.\\4\",\n+    r\"backbone.stages.(\\d+).blocks.(\\d+).layers.(\\d+).conv2.lab.(scale|bias)\": r\"model.backbone.model.encoder.stages.\\1.blocks.\\2.layers.\\3.conv2.lab.\\4\",\n+    # Downsample with lab\n+    r\"backbone.stages.(\\d+).downsample.lab.(scale|bias)\": r\"model.backbone.model.encoder.stages.\\1.downsample.lab.\\2\",\n+    # Backbone downsample\n+    r\"backbone.stages.(\\d+).downsample.conv.weight\": r\"model.backbone.model.encoder.stages.\\1.downsample.convolution.weight\",\n+    r\"backbone.stages.(\\d+).downsample.bn.(weight|bias|running_mean|running_var)\": r\"model.backbone.model.encoder.stages.\\1.downsample.normalization.\\2\",\n+    # Encoder mappings\n+    r\"encoder.encoder.(\\d+).layers.0.self_attn.out_proj.(weight|bias)\": r\"model.encoder.encoder.\\1.layers.0.self_attn.out_proj.\\2\",\n+    r\"encoder.encoder.(\\d+).layers.0.linear1.(weight|bias)\": r\"model.encoder.encoder.\\1.layers.0.fc1.\\2\",\n+    r\"encoder.encoder.(\\d+).layers.0.linear2.(weight|bias)\": r\"model.encoder.encoder.\\1.layers.0.fc2.\\2\",\n+    r\"encoder.encoder.(\\d+).layers.0.norm1.(weight|bias)\": r\"model.encoder.encoder.\\1.layers.0.self_attn_layer_norm.\\2\",\n+    r\"encoder.encoder.(\\d+).layers.0.norm2.(weight|bias)\": r\"model.encoder.encoder.\\1.layers.0.final_layer_norm.\\2\",\n+    # Encoder projections and convolutions\n+    r\"encoder.input_proj.(\\d+).conv.weight\": r\"model.encoder_input_proj.\\1.0.weight\",\n+    r\"encoder.input_proj.(\\d+).norm.(weight|bias|running_mean|running_var)\": r\"model.encoder_input_proj.\\1.1.\\2\",\n+    r\"encoder.lateral_convs.(\\d+).conv.weight\": r\"model.encoder.lateral_convs.\\1.conv.weight\",\n+    r\"encoder.lateral_convs.(\\d+).norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.lateral_convs.\\1.norm.\\2\",\n+    # FPN blocks - complete structure\n+    # Basic convolutions\n+    r\"encoder.fpn_blocks.(\\d+).cv1.conv.weight\": r\"model.encoder.fpn_blocks.\\1.conv1.conv.weight\",\n+    r\"encoder.fpn_blocks.(\\d+).cv1.norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.fpn_blocks.\\1.conv1.norm.\\2\",\n+    # CSP Rep1 path\n+    r\"encoder.fpn_blocks.(\\d+).cv2.0.conv1.conv.weight\": r\"model.encoder.fpn_blocks.\\1.csp_rep1.conv1.conv.weight\",\n+    r\"encoder.fpn_blocks.(\\d+).cv2.0.conv1.norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.fpn_blocks.\\1.csp_rep1.conv1.norm.\\2\",\n+    r\"encoder.fpn_blocks.(\\d+).cv2.0.conv2.conv.weight\": r\"model.encoder.fpn_blocks.\\1.csp_rep1.conv2.conv.weight\",\n+    r\"encoder.fpn_blocks.(\\d+).cv2.0.conv2.norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.fpn_blocks.\\1.csp_rep1.conv2.norm.\\2\",\n+    r\"encoder.fpn_blocks.(\\d+).cv2.1.conv.weight\": r\"model.encoder.fpn_blocks.\\1.conv2.conv.weight\",\n+    r\"encoder.fpn_blocks.(\\d+).cv2.1.norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.fpn_blocks.\\1.conv2.norm.\\2\",\n+    # CSP Rep2 path\n+    r\"encoder.fpn_blocks.(\\d+).cv3.0.conv1.conv.weight\": r\"model.encoder.fpn_blocks.\\1.csp_rep2.conv1.conv.weight\",\n+    r\"encoder.fpn_blocks.(\\d+).cv3.0.conv1.norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.fpn_blocks.\\1.csp_rep2.conv1.norm.\\2\",\n+    r\"encoder.fpn_blocks.(\\d+).cv3.0.conv2.conv.weight\": r\"model.encoder.fpn_blocks.\\1.csp_rep2.conv2.conv.weight\",\n+    r\"encoder.fpn_blocks.(\\d+).cv3.0.conv2.norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.fpn_blocks.\\1.csp_rep2.conv2.norm.\\2\",\n+    r\"encoder.fpn_blocks.(\\d+).cv3.1.conv.weight\": r\"model.encoder.fpn_blocks.\\1.conv3.conv.weight\",\n+    r\"encoder.fpn_blocks.(\\d+).cv3.1.norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.fpn_blocks.\\1.conv3.norm.\\2\",\n+    # Final conv\n+    r\"encoder.fpn_blocks.(\\d+).cv4.conv.weight\": r\"model.encoder.fpn_blocks.\\1.conv4.conv.weight\",\n+    r\"encoder.fpn_blocks.(\\d+).cv4.norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.fpn_blocks.\\1.conv4.norm.\\2\",\n+    # Bottlenecks for CSP Rep1\n+    r\"encoder.fpn_blocks.(\\d+).cv2.0.bottlenecks.(\\d+).conv1.conv.weight\": r\"model.encoder.fpn_blocks.\\1.csp_rep1.bottlenecks.\\2.conv1.conv.weight\",\n+    r\"encoder.fpn_blocks.(\\d+).cv2.0.bottlenecks.(\\d+).conv1.norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.fpn_blocks.\\1.csp_rep1.bottlenecks.\\2.conv1.norm.\\3\",\n+    r\"encoder.fpn_blocks.(\\d+).cv2.0.bottlenecks.(\\d+).conv2.conv.weight\": r\"model.encoder.fpn_blocks.\\1.csp_rep1.bottlenecks.\\2.conv2.conv.weight\",\n+    r\"encoder.fpn_blocks.(\\d+).cv2.0.bottlenecks.(\\d+).conv2.norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.fpn_blocks.\\1.csp_rep1.bottlenecks.\\2.conv2.norm.\\3\",\n+    # Bottlenecks for CSP Rep2\n+    r\"encoder.fpn_blocks.(\\d+).cv3.0.bottlenecks.(\\d+).conv1.conv.weight\": r\"model.encoder.fpn_blocks.\\1.csp_rep2.bottlenecks.\\2.conv1.conv.weight\",\n+    r\"encoder.fpn_blocks.(\\d+).cv3.0.bottlenecks.(\\d+).conv1.norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.fpn_blocks.\\1.csp_rep2.bottlenecks.\\2.conv1.norm.\\3\",\n+    r\"encoder.fpn_blocks.(\\d+).cv3.0.bottlenecks.(\\d+).conv2.conv.weight\": r\"model.encoder.fpn_blocks.\\1.csp_rep2.bottlenecks.\\2.conv2.conv.weight\",\n+    r\"encoder.fpn_blocks.(\\d+).cv3.0.bottlenecks.(\\d+).conv2.norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.fpn_blocks.\\1.csp_rep2.bottlenecks.\\2.conv2.norm.\\3\",\n+    # PAN blocks - complete structure\n+    # Basic convolutions\n+    r\"encoder.pan_blocks.(\\d+).cv1.conv.weight\": r\"model.encoder.pan_blocks.\\1.conv1.conv.weight\",\n+    r\"encoder.pan_blocks.(\\d+).cv1.norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.pan_blocks.\\1.conv1.norm.\\2\",\n+    # CSP Rep1 path\n+    r\"encoder.pan_blocks.(\\d+).cv2.0.conv1.conv.weight\": r\"model.encoder.pan_blocks.\\1.csp_rep1.conv1.conv.weight\",\n+    r\"encoder.pan_blocks.(\\d+).cv2.0.conv1.norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.pan_blocks.\\1.csp_rep1.conv1.norm.\\2\",\n+    r\"encoder.pan_blocks.(\\d+).cv2.0.conv2.conv.weight\": r\"model.encoder.pan_blocks.\\1.csp_rep1.conv2.conv.weight\",\n+    r\"encoder.pan_blocks.(\\d+).cv2.0.conv2.norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.pan_blocks.\\1.csp_rep1.conv2.norm.\\2\",\n+    r\"encoder.pan_blocks.(\\d+).cv2.1.conv.weight\": r\"model.encoder.pan_blocks.\\1.conv2.conv.weight\",\n+    r\"encoder.pan_blocks.(\\d+).cv2.1.norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.pan_blocks.\\1.conv2.norm.\\2\",\n+    # CSP Rep2 path\n+    r\"encoder.pan_blocks.(\\d+).cv3.0.conv1.conv.weight\": r\"model.encoder.pan_blocks.\\1.csp_rep2.conv1.conv.weight\",\n+    r\"encoder.pan_blocks.(\\d+).cv3.0.conv1.norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.pan_blocks.\\1.csp_rep2.conv1.norm.\\2\",\n+    r\"encoder.pan_blocks.(\\d+).cv3.0.conv2.conv.weight\": r\"model.encoder.pan_blocks.\\1.csp_rep2.conv2.conv.weight\",\n+    r\"encoder.pan_blocks.(\\d+).cv3.0.conv2.norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.pan_blocks.\\1.csp_rep2.conv2.norm.\\2\",\n+    r\"encoder.pan_blocks.(\\d+).cv3.1.conv.weight\": r\"model.encoder.pan_blocks.\\1.conv3.conv.weight\",\n+    r\"encoder.pan_blocks.(\\d+).cv3.1.norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.pan_blocks.\\1.conv3.norm.\\2\",\n+    # Final conv\n+    r\"encoder.pan_blocks.(\\d+).cv4.conv.weight\": r\"model.encoder.pan_blocks.\\1.conv4.conv.weight\",\n+    r\"encoder.pan_blocks.(\\d+).cv4.norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.pan_blocks.\\1.conv4.norm.\\2\",\n+    # Bottlenecks for CSP Rep1\n+    r\"encoder.pan_blocks.(\\d+).cv2.0.bottlenecks.(\\d+).conv1.conv.weight\": r\"model.encoder.pan_blocks.\\1.csp_rep1.bottlenecks.\\2.conv1.conv.weight\",\n+    r\"encoder.pan_blocks.(\\d+).cv2.0.bottlenecks.(\\d+).conv1.norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.pan_blocks.\\1.csp_rep1.bottlenecks.\\2.conv1.norm.\\3\",\n+    r\"encoder.pan_blocks.(\\d+).cv2.0.bottlenecks.(\\d+).conv2.conv.weight\": r\"model.encoder.pan_blocks.\\1.csp_rep1.bottlenecks.\\2.conv2.conv.weight\",\n+    r\"encoder.pan_blocks.(\\d+).cv2.0.bottlenecks.(\\d+).conv2.norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.pan_blocks.\\1.csp_rep1.bottlenecks.\\2.conv2.norm.\\3\",\n+    # Bottlenecks for CSP Rep2\n+    r\"encoder.pan_blocks.(\\d+).cv3.0.bottlenecks.(\\d+).conv1.conv.weight\": r\"model.encoder.pan_blocks.\\1.csp_rep2.bottlenecks.\\2.conv1.conv.weight\",\n+    r\"encoder.pan_blocks.(\\d+).cv3.0.bottlenecks.(\\d+).conv1.norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.pan_blocks.\\1.csp_rep2.bottlenecks.\\2.conv1.norm.\\3\",\n+    r\"encoder.pan_blocks.(\\d+).cv3.0.bottlenecks.(\\d+).conv2.conv.weight\": r\"model.encoder.pan_blocks.\\1.csp_rep2.bottlenecks.\\2.conv2.conv.weight\",\n+    r\"encoder.pan_blocks.(\\d+).cv3.0.bottlenecks.(\\d+).conv2.norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.pan_blocks.\\1.csp_rep2.bottlenecks.\\2.conv2.norm.\\3\",\n+    # Downsample convolutions\n+    r\"encoder.downsample_convs.(\\d+).0.cv(\\d+).conv.weight\": r\"model.encoder.downsample_convs.\\1.conv\\2.conv.weight\",\n+    r\"encoder.downsample_convs.(\\d+).0.cv(\\d+).norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.downsample_convs.\\1.conv\\2.norm.\\3\",\n+    # Decoder layers\n+    r\"decoder.decoder.layers.(\\d+).self_attn.out_proj.(weight|bias)\": r\"model.decoder.layers.\\1.self_attn.out_proj.\\2\",\n+    r\"decoder.decoder.layers.(\\d+).cross_attn.sampling_offsets.(weight|bias)\": r\"model.decoder.layers.\\1.encoder_attn.sampling_offsets.\\2\",\n+    r\"decoder.decoder.layers.(\\d+).cross_attn.attention_weights.(weight|bias)\": r\"model.decoder.layers.\\1.encoder_attn.attention_weights.\\2\",\n+    r\"decoder.decoder.layers.(\\d+).cross_attn.value_proj.(weight|bias)\": r\"model.decoder.layers.\\1.encoder_attn.value_proj.\\2\",\n+    r\"decoder.decoder.layers.(\\d+).cross_attn.output_proj.(weight|bias)\": r\"model.decoder.layers.\\1.encoder_attn.output_proj.\\2\",\n+    r\"decoder.decoder.layers.(\\d+).cross_attn.num_points_scale\": r\"model.decoder.layers.\\1.encoder_attn.num_points_scale\",\n+    r\"decoder.decoder.layers.(\\d+).gateway.gate.(weight|bias)\": r\"model.decoder.layers.\\1.gateway.gate.\\2\",\n+    r\"decoder.decoder.layers.(\\d+).gateway.norm.(weight|bias)\": r\"model.decoder.layers.\\1.gateway.norm.\\2\",\n+    r\"decoder.decoder.layers.(\\d+).norm1.(weight|bias)\": r\"model.decoder.layers.\\1.self_attn_layer_norm.\\2\",\n+    r\"decoder.decoder.layers.(\\d+).norm2.(weight|bias)\": r\"model.decoder.layers.\\1.encoder_attn_layer_norm.\\2\",\n+    r\"decoder.decoder.layers.(\\d+).norm3.(weight|bias)\": r\"model.decoder.layers.\\1.final_layer_norm.\\2\",\n+    r\"decoder.decoder.layers.(\\d+).linear1.(weight|bias)\": r\"model.decoder.layers.\\1.fc1.\\2\",\n+    r\"decoder.decoder.layers.(\\d+).linear2.(weight|bias)\": r\"model.decoder.layers.\\1.fc2.\\2\",\n+    # LQE layers\n+    r\"decoder.decoder.lqe_layers.(\\d+).reg_conf.layers.(\\d+).(weight|bias)\": r\"model.decoder.lqe_layers.\\1.reg_conf.layers.\\2.\\3\",\n+    # Decoder heads and projections\n+    r\"decoder.dec_score_head.(\\d+).(weight|bias)\": r\"model.decoder.class_embed.\\1.\\2\",\n+    r\"decoder.dec_bbox_head.(\\d+).layers.(\\d+).(weight|bias)\": r\"model.decoder.bbox_embed.\\1.layers.\\2.\\3\",\n+    r\"decoder.pre_bbox_head.layers.(\\d+).(weight|bias)\": r\"model.decoder.pre_bbox_head.layers.\\1.\\2\",\n+    r\"decoder.input_proj.(\\d+).conv.weight\": r\"model.decoder_input_proj.\\1.0.weight\",\n+    r\"decoder.input_proj.(\\d+).norm.(weight|bias|running_mean|running_var)\": r\"model.decoder_input_proj.\\1.1.\\2\",\n+    # Other decoder components\n+    r\"decoder.denoising_class_embed.weight\": r\"model.denoising_class_embed.weight\",\n+    r\"decoder.query_pos_head.layers.(\\d+).(weight|bias)\": r\"model.decoder.query_pos_head.layers.\\1.\\2\",\n+    r\"decoder.enc_output.proj.(weight|bias)\": r\"model.enc_output.0.\\1\",\n+    r\"decoder.enc_output.norm.(weight|bias)\": r\"model.enc_output.1.\\1\",\n+    r\"decoder.enc_score_head.(weight|bias)\": r\"model.enc_score_head.\\1\",\n+    r\"decoder.enc_bbox_head.layers.(\\d+).(weight|bias)\": r\"model.enc_bbox_head.layers.\\1.\\2\",\n+}\n+\n+\n+def convert_old_keys_to_new_keys(state_dict_keys: dict = None):\n+    # Use the mapping to rename keys\n+    for original_key, converted_key in ORIGINAL_TO_CONVERTED_KEY_MAPPING.items():\n+        for key in list(state_dict_keys.keys()):\n+            new_key = re.sub(original_key, converted_key, key)\n+            if new_key != key:\n+                state_dict_keys[new_key] = state_dict_keys.pop(key)\n+\n+    return state_dict_keys\n+\n+\n+def read_in_q_k_v(state_dict, config, model_name):\n+    prefix = \"\"\n+    encoder_hidden_dim = config.encoder_hidden_dim\n+\n+    # first: transformer encoder\n+    for i in range(config.encoder_layers):\n+        # read in weights + bias of input projection layer (in PyTorch's MultiHeadAttention, this is a single matrix + bias)\n+        in_proj_weight = state_dict.pop(f\"{prefix}encoder.encoder.{i}.layers.0.self_attn.in_proj_weight\")\n+        in_proj_bias = state_dict.pop(f\"{prefix}encoder.encoder.{i}.layers.0.self_attn.in_proj_bias\")\n+        # next, add query, keys and values (in that order) to the state dict\n+        state_dict[f\"model.encoder.encoder.{i}.layers.0.self_attn.q_proj.weight\"] = in_proj_weight[\n+            :encoder_hidden_dim, :\n+        ]\n+        state_dict[f\"model.encoder.encoder.{i}.layers.0.self_attn.q_proj.bias\"] = in_proj_bias[:encoder_hidden_dim]\n+        state_dict[f\"model.encoder.encoder.{i}.layers.0.self_attn.k_proj.weight\"] = in_proj_weight[\n+            encoder_hidden_dim : 2 * encoder_hidden_dim, :\n+        ]\n+        state_dict[f\"model.encoder.encoder.{i}.layers.0.self_attn.k_proj.bias\"] = in_proj_bias[\n+            encoder_hidden_dim : 2 * encoder_hidden_dim\n+        ]\n+        state_dict[f\"model.encoder.encoder.{i}.layers.0.self_attn.v_proj.weight\"] = in_proj_weight[\n+            -encoder_hidden_dim:, :\n+        ]\n+        state_dict[f\"model.encoder.encoder.{i}.layers.0.self_attn.v_proj.bias\"] = in_proj_bias[-encoder_hidden_dim:]\n+    # next: transformer decoder (which is a bit more complex because it also includes cross-attention)\n+    for i in range(config.decoder_layers):\n+        # read in weights + bias of input projection layer of self-attention\n+        in_proj_weight = state_dict.pop(f\"{prefix}decoder.decoder.layers.{i}.self_attn.in_proj_weight\", None)\n+        in_proj_bias = state_dict.pop(f\"{prefix}decoder.decoder.layers.{i}.self_attn.in_proj_bias\", None)\n+        # next, add query, keys and values (in that order) to the state dict\n+        if model_name in [\"dfine_n_coco\", \"dfine_n_obj2coco_e25\", \"dfine_n_obj365\"]:\n+            state_dict[f\"model.decoder.layers.{i}.self_attn.q_proj.weight\"] = in_proj_weight[:128, :]\n+            state_dict[f\"model.decoder.layers.{i}.self_attn.q_proj.bias\"] = in_proj_bias[:128]\n+            state_dict[f\"model.decoder.layers.{i}.self_attn.k_proj.weight\"] = in_proj_weight[256:384, :]\n+            state_dict[f\"model.decoder.layers.{i}.self_attn.k_proj.bias\"] = in_proj_bias[256:384]\n+            state_dict[f\"model.decoder.layers.{i}.self_attn.v_proj.weight\"] = in_proj_weight[-128:, :]\n+            state_dict[f\"model.decoder.layers.{i}.self_attn.v_proj.bias\"] = in_proj_bias[-128:]\n+        else:\n+            state_dict[f\"model.decoder.layers.{i}.self_attn.q_proj.weight\"] = in_proj_weight[:256, :]\n+            state_dict[f\"model.decoder.layers.{i}.self_attn.q_proj.bias\"] = in_proj_bias[:256]\n+            state_dict[f\"model.decoder.layers.{i}.self_attn.k_proj.weight\"] = in_proj_weight[256:512, :]\n+            state_dict[f\"model.decoder.layers.{i}.self_attn.k_proj.bias\"] = in_proj_bias[256:512]\n+            state_dict[f\"model.decoder.layers.{i}.self_attn.v_proj.weight\"] = in_proj_weight[-256:, :]\n+            state_dict[f\"model.decoder.layers.{i}.self_attn.v_proj.bias\"] = in_proj_bias[-256:]\n+\n+\n+# We will verify our results on an image of cute cats\n+def prepare_img():\n+    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    im = Image.open(requests.get(url, stream=True).raw)\n+\n+    return im\n+\n+\n+@torch.no_grad()\n+def convert_d_fine_checkpoint(model_name, pytorch_dump_folder_path, push_to_hub, repo_id):\n+    \"\"\"\n+    Copy/paste/tweak model's weights to our D-FINE structure.\n+    \"\"\"\n+\n+    # load default config\n+    config = get_d_fine_config(model_name)\n+    state_dict = load_original_state_dict(repo_id, model_name)\n+    state_dict.pop(\"decoder.valid_mask\", None)\n+    state_dict.pop(\"decoder.anchors\", None)\n+    model = DFineForObjectDetection(config)\n+    logger.info(f\"Converting model {model_name}...\")\n+\n+    state_dict = convert_old_keys_to_new_keys(state_dict)\n+    state_dict.pop(\"decoder.model.decoder.up\", None)\n+    state_dict.pop(\"decoder.model.decoder.reg_scale\", None)\n+\n+    # query, key and value matrices need special treatment\n+    read_in_q_k_v(state_dict, config, model_name)\n+    # important: we need to prepend a prefix to each of the base model keys as the head models use different attributes for them\n+    for key in state_dict.copy().keys():\n+        if key.endswith(\"num_batches_tracked\"):\n+            del state_dict[key]\n+        # for two_stage\n+        if \"bbox_embed\" in key or (\"class_embed\" in key and \"denoising_\" not in key):\n+            state_dict[key.split(\"model.decoder.\")[-1]] = state_dict[key]\n+\n+    # finally, create HuggingFace model and load state dict\n+    model.load_state_dict(state_dict)\n+    model.eval()\n+\n+    # load image processor\n+    image_processor = RTDetrImageProcessor()\n+\n+    # prepare image\n+    img = prepare_img()\n+\n+    # preprocess image\n+    transformations = transforms.Compose(\n+        [\n+            transforms.Resize([640, 640], interpolation=transforms.InterpolationMode.BILINEAR),\n+            transforms.ToTensor(),\n+        ]\n+    )\n+    original_pixel_values = transformations(img).unsqueeze(0)  # insert batch dimension\n+\n+    encoding = image_processor(images=img, return_tensors=\"pt\")\n+    pixel_values = encoding[\"pixel_values\"]\n+\n+    assert torch.allclose(original_pixel_values, pixel_values)\n+\n+    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n+    model.to(device)\n+    pixel_values = pixel_values.to(device)\n+\n+    outputs = model(pixel_values)\n+\n+    if model_name == \"dfine_x_coco\":\n+        expected_slice_logits = torch.tensor(\n+            [\n+                [-4.844723, -4.7293096, -4.5971327],\n+                [-4.554266, -4.61723, -4.627926],\n+                [-4.3934402, -4.6064143, -4.139952],\n+            ]\n+        )\n+        expected_slice_boxes = torch.tensor(\n+            [\n+                [0.2565248, 0.5477609, 0.47644863],\n+                [0.7690029, 0.41423926, 0.46148556],\n+                [0.1688096, 0.19923759, 0.21118002],\n+            ]\n+        )\n+    elif model_name == \"dfine_x_obj2coco\":\n+        expected_slice_logits = torch.tensor(\n+            [\n+                [-4.230433, -6.6295037, -4.8339615],\n+                [-4.085411, -6.3280816, -4.695468],\n+                [-3.8968022, -6.336813, -4.67051],\n+            ]\n+        )\n+        expected_slice_boxes = torch.tensor(\n+            [\n+                [0.25707328, 0.54842496, 0.47624254],\n+                [0.76967394, 0.41272867, 0.45970756],\n+                [0.16882066, 0.19918433, 0.2112098],\n+            ]\n+        )\n+    elif model_name == \"dfine_x_obj365\":\n+        expected_slice_logits = torch.tensor(\n+            [\n+                [-6.3844957, -3.7549126, -4.6873264],\n+                [-5.8433194, -3.4490552, -3.3228905],\n+                [-6.5314736, -3.7856622, -4.895984],\n+            ]\n+        )\n+        expected_slice_boxes = torch.tensor(\n+            [\n+                [0.7703046, 0.41329497, 0.45932162],\n+                [0.16898105, 0.19876392, 0.21050783],\n+                [0.25134972, 0.5517619, 0.4864124],\n+            ]\n+        )\n+    elif model_name == \"dfine_m_coco\":\n+        expected_slice_logits = torch.tensor(\n+            [\n+                [-4.5187078, -4.71708, -4.117749],\n+                [-4.513984, -4.937715, -3.829125],\n+                [-4.830042, -6.931682, -3.1740026],\n+            ]\n+        )\n+        expected_slice_boxes = torch.tensor(\n+            [\n+                [0.25851426, 0.5489963, 0.4757598],\n+                [0.769683, 0.41411665, 0.45988125],\n+                [0.16866133, 0.19921188, 0.21207744],\n+            ]\n+        )\n+    elif model_name == \"dfine_m_obj2coco\":\n+        expected_slice_logits = torch.tensor(\n+            [\n+                [-4.520666, -7.6678333, -5.739887],\n+                [-4.5053635, -7.510611, -5.452532],\n+                [-4.70348, -5.6098466, -5.0199957],\n+            ]\n+        )\n+        expected_slice_boxes = torch.tensor(\n+            [\n+                [0.2567608, 0.5485795, 0.4767465],\n+                [0.77035284, 0.41236404, 0.4580645],\n+                [0.5498525, 0.27548885, 0.05886984],\n+            ]\n+        )\n+    elif model_name == \"dfine_m_obj365\":\n+        expected_slice_logits = torch.tensor(\n+            [\n+                [-5.770525, -3.1610885, -5.2807794],\n+                [-5.7809954, -3.768266, -5.1146393],\n+                [-6.180705, -3.7357295, -3.1651964],\n+            ]\n+        )\n+        expected_slice_boxes = torch.tensor(\n+            [\n+                [0.2529114, 0.5526663, 0.48270613],\n+                [0.7712474, 0.41294736, 0.457174],\n+                [0.5497157, 0.27588123, 0.05813372],\n+            ]\n+        )\n+    elif model_name == \"dfine_l_coco\":\n+        expected_slice_logits = torch.tensor(\n+            [\n+                [-4.068779, -5.169955, -4.339212],\n+                [-3.9461594, -5.0279613, -4.0161457],\n+                [-4.218292, -6.196324, -5.175245],\n+            ]\n+        )\n+        expected_slice_boxes = torch.tensor(\n+            [\n+                [0.2564867, 0.5489948, 0.4748876],\n+                [0.7693534, 0.4138953, 0.4598034],\n+                [0.16875696, 0.19875404, 0.21196914],\n+            ]\n+        )\n+    elif model_name == \"dfine_l_obj365\":\n+        expected_slice_logits = torch.tensor(\n+            [\n+                [-5.7953215, -3.4901116, -5.4394145],\n+                [-5.7032104, -3.671125, -5.76121],\n+                [-6.09466, -3.1512096, -4.285499],\n+            ]\n+        )\n+        expected_slice_boxes = torch.tensor(\n+            [\n+                [0.7693825, 0.41265628, 0.4606362],\n+                [0.25306237, 0.55187637, 0.4832178],\n+                [0.16892478, 0.19880727, 0.21115331],\n+            ]\n+        )\n+    elif model_name == \"dfine_l_obj2coco_e25\":\n+        expected_slice_logits = torch.tensor(\n+            [\n+                [-3.6098495, -6.633563, -5.1227236],\n+                [-3.682696, -6.9178205, -5.414557],\n+                [-4.491674, -6.0823426, -4.5718226],\n+            ]\n+        )\n+        expected_slice_boxes = torch.tensor(\n+            [\n+                [0.7697078, 0.41368833, 0.45879585],\n+                [0.2573691, 0.54856044, 0.47715297],\n+                [0.16895264, 0.19871138, 0.2115552],\n+            ]\n+        )\n+    elif model_name == \"dfine_n_coco\":\n+        expected_slice_logits = torch.tensor(\n+            [\n+                [-3.7827945, -5.0889463, -4.8341026],\n+                [-5.3046904, -6.2801714, -2.9276395],\n+                [-4.497901, -5.2670407, -6.2380104],\n+            ]\n+        )\n+        expected_slice_boxes = torch.tensor(\n+            [\n+                [0.73334837, 0.4270624, 0.39424777],\n+                [0.1680235, 0.1988639, 0.21031213],\n+                [0.25370035, 0.5534435, 0.48496848],\n+            ]\n+        )\n+    elif model_name == \"dfine_s_coco\":\n+        expected_slice_logits = torch.tensor(\n+            [\n+                [-3.8097816, -4.7724586, -5.994499],\n+                [-5.2974715, -9.499067, -6.1653666],\n+                [-5.3502765, -3.9530406, -6.3630295],\n+            ]\n+        )\n+        expected_slice_boxes = torch.tensor(\n+            [\n+                [0.7677696, 0.41479152, 0.46441072],\n+                [0.16912134, 0.19869131, 0.2123824],\n+                [0.2581653, 0.54818195, 0.47512347],\n+            ]\n+        )\n+    elif model_name == \"dfine_s_obj2coco\":\n+        expected_slice_logits = torch.tensor(\n+            [\n+                [-6.0208125, -7.532673, -5.0572147],\n+                [-3.3595953, -9.057545, -6.376975],\n+                [-4.3203554, -9.546032, -6.075504],\n+            ]\n+        )\n+        expected_slice_boxes = torch.tensor(\n+            [\n+                [0.16901012, 0.19883151, 0.21121952],\n+                [0.76784194, 0.41266578, 0.46402973],\n+                [00.2563128, 0.54797643, 0.47937632],\n+            ]\n+        )\n+    elif model_name == \"dfine_s_obj365\":\n+        expected_slice_logits = torch.tensor(\n+            [\n+                [-6.3807316, -4.320986, -6.4775343],\n+                [-6.5818424, -3.5009093, -5.75824],\n+                [-5.748005, -4.3228016, -4.003726],\n+            ]\n+        )\n+        expected_slice_boxes = torch.tensor(\n+            [\n+                [0.2532072, 0.5491191, 0.48222217],\n+                [0.76586807, 0.41175705, 0.46789962],\n+                [0.169111, 0.19844547, 0.21069047],\n+            ]\n+        )\n+    else:\n+        raise ValueError(f\"Unknown d_fine_name: {model_name}\")\n+\n+    assert torch.allclose(outputs.logits[0, :3, :3], expected_slice_logits.to(outputs.logits.device), atol=1e-3)\n+    assert torch.allclose(outputs.pred_boxes[0, :3, :3], expected_slice_boxes.to(outputs.pred_boxes.device), atol=1e-4)\n+\n+    if pytorch_dump_folder_path is not None:\n+        Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n+        print(f\"Saving model {model_name} to {pytorch_dump_folder_path}\")\n+        model.save_pretrained(pytorch_dump_folder_path)\n+        print(f\"Saving image processor to {pytorch_dump_folder_path}\")\n+        image_processor.save_pretrained(pytorch_dump_folder_path)\n+\n+    if push_to_hub:\n+        # Upload model, image processor and config to the hub\n+        logger.info(\"Uploading PyTorch model and image processor to the hub...\")\n+        config.push_to_hub(\n+            repo_id=repo_id,\n+            commit_message=\"Add config from convert_d_fine_original_pytorch_checkpoint_to_hf.py\",\n+        )\n+        model.push_to_hub(\n+            repo_id=repo_id,\n+            commit_message=\"Add model from convert_d_fine_original_pytorch_checkpoint_to_hf.py\",\n+        )\n+        image_processor.push_to_hub(\n+            repo_id=repo_id,\n+            commit_message=\"Add image processor from convert_d_fine_original_pytorch_checkpoint_to_hf.py\",\n+        )\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--model_name\",\n+        default=\"dfine_s_coco\",\n+        type=str,\n+        help=\"model_name of the checkpoint you'd like to convert.\",\n+    )\n+    parser.add_argument(\n+        \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the output PyTorch model directory.\"\n+    )\n+    parser.add_argument(\"--push_to_hub\", action=\"store_true\", help=\"Whether to push the model to the hub or not.\")\n+    parser.add_argument(\n+        \"--repo_id\",\n+        type=str,\n+        help=\"repo_id where the model will be pushed to.\",\n+    )\n+    args = parser.parse_args()\n+    convert_d_fine_checkpoint(args.model_name, args.pytorch_dump_folder_path, args.push_to_hub, args.repo_id)"
        },
        {
            "sha": "bd981b17c657c6b0db829327b2bdc98e6ee45640",
            "filename": "src/transformers/models/d_fine/modeling_d_fine.py",
            "status": "added",
            "additions": 2295,
            "deletions": 0,
            "changes": 2295,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py?ref=4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63"
        },
        {
            "sha": "adbfa68477950ec0ae41a7b4461ea37e2acc07de",
            "filename": "src/transformers/models/d_fine/modular_d_fine.py",
            "status": "added",
            "additions": 1207,
            "deletions": 0,
            "changes": 1207,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py?ref=4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63",
            "patch": "@@ -0,0 +1,1207 @@\n+# coding=utf-8\n+# Copyright 2025 Baidu Inc and The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import math\n+from typing import Any, Optional\n+\n+import torch\n+import torch.nn.functional as F\n+import torch.nn.init as init\n+from torch import nn\n+\n+from ...activations import ACT2CLS\n+from ...configuration_utils import PretrainedConfig\n+from ...image_transforms import corners_to_center_format\n+from ...utils import is_torchdynamo_compiling, logging\n+from ...utils.backbone_utils import verify_backbone_config_arguments\n+from ..auto import CONFIG_MAPPING\n+from ..rt_detr.modeling_rt_detr import (\n+    RTDetrConvNormLayer,\n+    RTDetrDecoder,\n+    RTDetrDecoderLayer,\n+    RTDetrDecoderOutput,\n+    RTDetrEncoder,\n+    RTDetrForObjectDetection,\n+    RTDetrHybridEncoder,\n+    RTDetrMLPPredictionHead,\n+    RTDetrModel,\n+    RTDetrPreTrainedModel,\n+    RTDetrRepVggBlock,\n+    inverse_sigmoid,\n+)\n+from ..rt_detr_v2.modeling_rt_detr_v2 import multi_scale_deformable_attention_v2\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+# TODO: Attribute map assignment logic should be fixed in modular\n+# as well as super() call parsing becuase otherwise we cannot re-write args after initialization\n+class DFineConfig(PretrainedConfig):\n+    \"\"\"\n+    This is the configuration class to store the configuration of a [`DFineModel`]. It is used to instantiate a D-FINE\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of D-FINE-X-COCO \"[ustc-community/dfine_x_coco\"](https://huggingface.co/ustc-community/dfine_x_coco\").\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        initializer_range (`float`, *optional*, defaults to 0.01):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        initializer_bias_prior_prob (`float`, *optional*):\n+            The prior probability used by the bias initializer to initialize biases for `enc_score_head` and `class_embed`.\n+            If `None`, `prior_prob` computed as `prior_prob = 1 / (num_labels + 1)` while initializing model weights.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the layer normalization layers.\n+        batch_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the batch normalization layers.\n+        backbone_config (`Dict`, *optional*, defaults to `RTDetrResNetConfig()`):\n+            The configuration of the backbone model.\n+        backbone (`str`, *optional*):\n+            Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this\n+            will load the corresponding pretrained weights from the timm or transformers library. If `use_pretrained_backbone`\n+            is `False`, this loads the backbone's config and uses that to initialize the backbone with random weights.\n+        use_pretrained_backbone (`bool`, *optional*, defaults to `False`):\n+            Whether to use pretrained weights for the backbone.\n+        use_timm_backbone (`bool`, *optional*, defaults to `False`):\n+            Whether to load `backbone` from the timm library. If `False`, the backbone is loaded from the transformers\n+            library.\n+        freeze_backbone_batch_norms (`bool`, *optional*, defaults to `True`):\n+            Whether to freeze the batch normalization layers in the backbone.\n+        backbone_kwargs (`dict`, *optional*):\n+            Keyword arguments to be passed to AutoBackbone when loading from a checkpoint\n+            e.g. `{'out_indices': (0, 1, 2, 3)}`. Cannot be specified if `backbone_config` is set.\n+        encoder_hidden_dim (`int`, *optional*, defaults to 256):\n+            Dimension of the layers in hybrid encoder.\n+        encoder_in_channels (`list`, *optional*, defaults to `[512, 1024, 2048]`):\n+            Multi level features input for encoder.\n+        feat_strides (`List[int]`, *optional*, defaults to `[8, 16, 32]`):\n+            Strides used in each feature map.\n+        encoder_layers (`int`, *optional*, defaults to 1):\n+            Total of layers to be used by the encoder.\n+        encoder_ffn_dim (`int`, *optional*, defaults to 1024):\n+            Dimension of the \"intermediate\" (often named feed-forward) layer in decoder.\n+        encoder_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        dropout (`float`, *optional*, defaults to 0.0):\n+            The ratio for all dropout layers.\n+        activation_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for activations inside the fully connected layer.\n+        encode_proj_layers (`List[int]`, *optional*, defaults to `[2]`):\n+            Indexes of the projected layers to be used in the encoder.\n+        positional_encoding_temperature (`int`, *optional*, defaults to 10000):\n+            The temperature parameter used to create the positional encodings.\n+        encoder_activation_function (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n+        activation_function (`str`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the general layer. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n+        eval_size (`Tuple[int, int]`, *optional*):\n+            Height and width used to computes the effective height and width of the position embeddings after taking\n+            into account the stride.\n+        normalize_before (`bool`, *optional*, defaults to `False`):\n+            Determine whether to apply layer normalization in the transformer encoder layer before self-attention and\n+            feed-forward modules.\n+        hidden_expansion (`float`, *optional*, defaults to 1.0):\n+            Expansion ratio to enlarge the dimension size of RepVGGBlock and CSPRepLayer.\n+        d_model (`int`, *optional*, defaults to 256):\n+            Dimension of the layers exclude hybrid encoder.\n+        num_queries (`int`, *optional*, defaults to 300):\n+            Number of object queries.\n+        decoder_in_channels (`list`, *optional*, defaults to `[256, 256, 256]`):\n+            Multi level features dimension for decoder\n+        decoder_ffn_dim (`int`, *optional*, defaults to 1024):\n+            Dimension of the \"intermediate\" (often named feed-forward) layer in decoder.\n+        num_feature_levels (`int`, *optional*, defaults to 3):\n+            The number of input feature levels.\n+        decoder_n_points (`int`, *optional*, defaults to 4):\n+            The number of sampled keys in each feature level for each attention head in the decoder.\n+        decoder_layers (`int`, *optional*, defaults to 6):\n+            Number of decoder layers.\n+        decoder_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        decoder_activation_function (`str`, *optional*, defaults to `\"relu\"`):\n+            The non-linear activation function (function or string) in the decoder. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        num_denoising (`int`, *optional*, defaults to 100):\n+            The total number of denoising tasks or queries to be used for contrastive denoising.\n+        label_noise_ratio (`float`, *optional*, defaults to 0.5):\n+            The fraction of denoising labels to which random noise should be added.\n+        box_noise_scale (`float`, *optional*, defaults to 1.0):\n+            Scale or magnitude of noise to be added to the bounding boxes.\n+        learn_initial_query (`bool`, *optional*, defaults to `False`):\n+            Indicates whether the initial query embeddings for the decoder should be learned during training\n+        anchor_image_size (`Tuple[int, int]`, *optional*):\n+            Height and width of the input image used during evaluation to generate the bounding box anchors. If None, automatic generate anchor is applied.\n+        with_box_refine (`bool`, *optional*, defaults to `True`):\n+            Whether to apply iterative bounding box refinement, where each decoder layer refines the bounding boxes\n+            based on the predictions from the previous layer.\n+        is_encoder_decoder (`bool`, *optional*, defaults to `True`):\n+            Whether the architecture has an encoder decoder structure.\n+        matcher_alpha (`float`, *optional*, defaults to 0.25):\n+            Parameter alpha used by the Hungarian Matcher.\n+        matcher_gamma (`float`, *optional*, defaults to 2.0):\n+            Parameter gamma used by the Hungarian Matcher.\n+        matcher_class_cost (`float`, *optional*, defaults to 2.0):\n+            The relative weight of the class loss used by the Hungarian Matcher.\n+        matcher_bbox_cost (`float`, *optional*, defaults to 5.0):\n+            The relative weight of the bounding box loss used by the Hungarian Matcher.\n+        matcher_giou_cost (`float`, *optional*, defaults to 2.0):\n+            The relative weight of the giou loss of used by the Hungarian Matcher.\n+        use_focal_loss (`bool`, *optional*, defaults to `True`):\n+            Parameter informing if focal focal should be used.\n+        auxiliary_loss (`bool`, *optional*, defaults to `True`):\n+            Whether auxiliary decoding losses (loss at each decoder layer) are to be used.\n+        focal_loss_alpha (`float`, *optional*, defaults to 0.75):\n+            Parameter alpha used to compute the focal loss.\n+        focal_loss_gamma (`float`, *optional*, defaults to 2.0):\n+            Parameter gamma used to compute the focal loss.\n+        weight_loss_vfl (`float`, *optional*, defaults to 1.0):\n+            Relative weight of the varifocal loss in the object detection loss.\n+        weight_loss_bbox (`float`, *optional*, defaults to 5.0):\n+            Relative weight of the L1 bounding box loss in the object detection loss.\n+        weight_loss_giou (`float`, *optional*, defaults to 2.0):\n+            Relative weight of the generalized IoU loss in the object detection loss.\n+        weight_loss_fgl (`float`, *optional*, defaults to 0.15):\n+            Relative weight of the fine-grained localization loss in the object detection loss.\n+        weight_loss_ddf (`float`, *optional*, defaults to 1.5):\n+            Relative weight of the decoupled distillation focal loss in the object detection loss.\n+        eos_coefficient (`float`, *optional*, defaults to 0.0001):\n+            Relative classification weight of the 'no-object' class in the object detection loss.\n+        eval_idx (`int`, *optional*, defaults to -1):\n+            Index of the decoder layer to use for evaluation. If negative, counts from the end\n+            (e.g., -1 means use the last layer). This allows for early prediction in the decoder\n+            stack while still training later layers.\n+        layer_scale (`float`, *optional*, defaults to `1.0`):\n+            Scaling factor for the hidden dimension in later decoder layers. Used to adjust the\n+            model capacity after the evaluation layer.\n+        max_num_bins (`int`, *optional*, defaults to 32):\n+            Maximum number of bins for the distribution-guided bounding box refinement.\n+            Higher values allow for more fine-grained localization but increase computation.\n+        reg_scale (`float`, *optional*, defaults to 4.0):\n+            Scale factor for the regression distribution. Controls the range and granularity\n+            of the bounding box refinement process.\n+        depth_mult (`float`, *optional*, defaults to 1.0):\n+            Multiplier for the number of blocks in RepNCSPELAN4 layers. Used to scale the model's\n+            depth while maintaining its architecture.\n+        top_prob_values (`int`, *optional*, defaults to 4):\n+            Number of top probability values to consider from each corner's distribution.\n+        lqe_hidden_dim (`int`, *optional*, defaults to 64):\n+            Hidden dimension size for the Location Quality Estimator (LQE) network.\n+        lqe_layers (`int`, *optional*, defaults to 2):\n+            Number of layers in the Location Quality Estimator MLP.\n+        decoder_offset_scale (`float`, *optional*, defaults to 0.5):\n+            Offset scale used in deformable attention.\n+        decoder_method (`str`, *optional*, defaults to `\"default\"`):\n+            The method to use for the decoder: `\"default\"` or `\"discrete\"`.\n+        up (`float`, *optional*, defaults to 0.5):\n+            Controls the upper bounds of the Weighting Function.\n+    \"\"\"\n+\n+    model_type = \"d_fine\"\n+    layer_types = [\"basic\", \"bottleneck\"]\n+    attribute_map = {\n+        \"hidden_size\": \"d_model\",\n+        \"num_attention_heads\": \"encoder_attention_heads\",\n+    }\n+\n+    def __init__(\n+        self,\n+        initializer_range=0.01,\n+        initializer_bias_prior_prob=None,\n+        layer_norm_eps=1e-5,\n+        batch_norm_eps=1e-5,\n+        # backbone\n+        backbone_config=None,\n+        backbone=None,\n+        use_pretrained_backbone=False,\n+        use_timm_backbone=False,\n+        freeze_backbone_batch_norms=True,\n+        backbone_kwargs=None,\n+        # encoder HybridEncoder\n+        encoder_hidden_dim=256,\n+        encoder_in_channels=[512, 1024, 2048],\n+        feat_strides=[8, 16, 32],\n+        encoder_layers=1,\n+        encoder_ffn_dim=1024,\n+        encoder_attention_heads=8,\n+        dropout=0.0,\n+        activation_dropout=0.0,\n+        encode_proj_layers=[2],\n+        positional_encoding_temperature=10000,\n+        encoder_activation_function=\"gelu\",\n+        activation_function=\"silu\",\n+        eval_size=None,\n+        normalize_before=False,\n+        hidden_expansion=1.0,\n+        # decoder DFineTransformer\n+        d_model=256,\n+        num_queries=300,\n+        decoder_in_channels=[256, 256, 256],\n+        decoder_ffn_dim=1024,\n+        num_feature_levels=3,\n+        decoder_n_points=4,\n+        decoder_layers=6,\n+        decoder_attention_heads=8,\n+        decoder_activation_function=\"relu\",\n+        attention_dropout=0.0,\n+        num_denoising=100,\n+        label_noise_ratio=0.5,\n+        box_noise_scale=1.0,\n+        learn_initial_query=False,\n+        anchor_image_size=None,\n+        with_box_refine=True,\n+        is_encoder_decoder=True,\n+        # Loss\n+        matcher_alpha=0.25,\n+        matcher_gamma=2.0,\n+        matcher_class_cost=2.0,\n+        matcher_bbox_cost=5.0,\n+        matcher_giou_cost=2.0,\n+        use_focal_loss=True,\n+        auxiliary_loss=True,\n+        focal_loss_alpha=0.75,\n+        focal_loss_gamma=2.0,\n+        weight_loss_vfl=1.0,\n+        weight_loss_bbox=5.0,\n+        weight_loss_giou=2.0,\n+        weight_loss_fgl=0.15,\n+        weight_loss_ddf=1.5,\n+        eos_coefficient=1e-4,\n+        eval_idx=-1,\n+        layer_scale=1,\n+        max_num_bins=32,\n+        reg_scale=4.0,\n+        depth_mult=1.0,\n+        top_prob_values=4,\n+        lqe_hidden_dim=64,\n+        lqe_layers=2,\n+        decoder_offset_scale=0.5,\n+        decoder_method=\"default\",\n+        up=0.5,\n+        **kwargs,\n+    ):\n+        self.initializer_range = initializer_range\n+        self.initializer_bias_prior_prob = initializer_bias_prior_prob\n+        self.layer_norm_eps = layer_norm_eps\n+        self.batch_norm_eps = batch_norm_eps\n+        # backbone\n+        if backbone_config is None and backbone is None:\n+            logger.info(\n+                \"`backbone_config` and `backbone` are `None`. Initializing the config with the default `HGNet-V2` backbone.\"\n+            )\n+            backbone_model_type = \"hgnet_v2\"\n+            config_class = CONFIG_MAPPING[backbone_model_type]\n+            # this will map it to RTDetrResNetConfig\n+            # note: we can instead create HGNetV2Config\n+            # and we would need to create HGNetV2Backbone\n+            backbone_config = config_class(\n+                num_channels=3,\n+                embedding_size=64,\n+                hidden_sizes=[256, 512, 1024, 2048],\n+                depths=[3, 4, 6, 3],\n+                layer_type=\"bottleneck\",\n+                hidden_act=\"relu\",\n+                downsample_in_first_stage=False,\n+                downsample_in_bottleneck=False,\n+                out_features=None,\n+                out_indices=[2, 3, 4],\n+            )\n+        elif isinstance(backbone_config, dict):\n+            backbone_model_type = backbone_config.pop(\"model_type\")\n+            config_class = CONFIG_MAPPING[backbone_model_type]\n+            backbone_config = config_class.from_dict(backbone_config)\n+\n+        verify_backbone_config_arguments(\n+            use_timm_backbone=use_timm_backbone,\n+            use_pretrained_backbone=use_pretrained_backbone,\n+            backbone=backbone,\n+            backbone_config=backbone_config,\n+            backbone_kwargs=backbone_kwargs,\n+        )\n+\n+        self.backbone_config = backbone_config\n+        self.backbone = backbone\n+        self.use_pretrained_backbone = use_pretrained_backbone\n+        self.use_timm_backbone = use_timm_backbone\n+        self.freeze_backbone_batch_norms = freeze_backbone_batch_norms\n+        self.backbone_kwargs = backbone_kwargs\n+        # encoder\n+        self.encoder_hidden_dim = encoder_hidden_dim\n+        self.encoder_in_channels = encoder_in_channels\n+        self.feat_strides = feat_strides\n+        self.encoder_attention_heads = encoder_attention_heads\n+        self.encoder_ffn_dim = encoder_ffn_dim\n+        self.dropout = dropout\n+        self.activation_dropout = activation_dropout\n+        self.encode_proj_layers = encode_proj_layers\n+        self.encoder_layers = encoder_layers\n+        self.positional_encoding_temperature = positional_encoding_temperature\n+        self.eval_size = eval_size\n+        self.normalize_before = normalize_before\n+        self.encoder_activation_function = encoder_activation_function\n+        self.activation_function = activation_function\n+        self.hidden_expansion = hidden_expansion\n+        # decoder\n+        self.d_model = d_model\n+        self.num_queries = num_queries\n+        self.decoder_ffn_dim = decoder_ffn_dim\n+        self.decoder_in_channels = decoder_in_channels\n+        self.num_feature_levels = num_feature_levels\n+        self.decoder_n_points = decoder_n_points\n+        self.decoder_layers = decoder_layers\n+        self.decoder_attention_heads = decoder_attention_heads\n+        self.decoder_activation_function = decoder_activation_function\n+        self.attention_dropout = attention_dropout\n+        self.num_denoising = num_denoising\n+        self.label_noise_ratio = label_noise_ratio\n+        self.box_noise_scale = box_noise_scale\n+        self.learn_initial_query = learn_initial_query\n+        self.anchor_image_size = anchor_image_size\n+        self.auxiliary_loss = auxiliary_loss\n+        self.with_box_refine = with_box_refine\n+        # Loss\n+        self.matcher_alpha = matcher_alpha\n+        self.matcher_gamma = matcher_gamma\n+        self.matcher_class_cost = matcher_class_cost\n+        self.matcher_bbox_cost = matcher_bbox_cost\n+        self.matcher_giou_cost = matcher_giou_cost\n+        self.use_focal_loss = use_focal_loss\n+        self.focal_loss_alpha = focal_loss_alpha\n+        self.focal_loss_gamma = focal_loss_gamma\n+        self.weight_loss_vfl = weight_loss_vfl\n+        self.weight_loss_bbox = weight_loss_bbox\n+        self.weight_loss_giou = weight_loss_giou\n+        self.weight_loss_fgl = weight_loss_fgl\n+        self.weight_loss_ddf = weight_loss_ddf\n+        self.eos_coefficient = eos_coefficient\n+        # add the new attributes with the given values or defaults\n+        self.eval_idx = eval_idx\n+        self.layer_scale = layer_scale\n+        self.max_num_bins = max_num_bins\n+        self.reg_scale = reg_scale\n+        self.depth_mult = depth_mult\n+        self.decoder_offset_scale = decoder_offset_scale\n+        self.decoder_method = decoder_method\n+        self.top_prob_values = top_prob_values\n+        self.lqe_hidden_dim = lqe_hidden_dim\n+        self.lqe_layers = lqe_layers\n+        self.up = up\n+\n+        if isinstance(self.decoder_n_points, list):\n+            if len(self.decoder_n_points) != self.num_feature_levels:\n+                raise ValueError(\n+                    f\"Length of decoder_n_points list ({len(self.decoder_n_points)}) must match num_feature_levels ({self.num_feature_levels}).\"\n+                )\n+\n+        head_dim = self.d_model // self.decoder_attention_heads\n+        if head_dim * self.decoder_attention_heads != self.d_model:\n+            raise ValueError(\n+                f\"Embedded dimension {self.d_model} must be divisible by decoder_attention_heads {self.decoder_attention_heads}\"\n+            )\n+        super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n+\n+    @property\n+    def num_attention_heads(self) -> int:\n+        return self.encoder_attention_heads\n+\n+    @property\n+    def hidden_size(self) -> int:\n+        return self.d_model\n+\n+    @classmethod\n+    def from_backbone_configs(cls, backbone_config: PretrainedConfig, **kwargs):\n+        \"\"\"Instantiate a [`DFineConfig`] (or a derived class) from a pre-trained backbone model configuration and DETR model\n+        configuration.\n+\n+            Args:\n+                backbone_config ([`PretrainedConfig`]):\n+                    The backbone configuration.\n+\n+            Returns:\n+                [`DFineConfig`]: An instance of a configuration object\n+        \"\"\"\n+        return cls(\n+            backbone_config=backbone_config,\n+            **kwargs,\n+        )\n+\n+\n+class DFineMultiscaleDeformableAttention(nn.Module):\n+    def __init__(self, config: DFineConfig):\n+        \"\"\"\n+        D-Fine version of multiscale deformable attention\n+        \"\"\"\n+        super().__init__()\n+        self.d_model = config.d_model\n+        self.n_heads = config.decoder_attention_heads\n+        self.n_levels = config.num_feature_levels\n+        self.offset_scale = config.decoder_offset_scale\n+        self.decoder_method = config.decoder_method\n+        self.n_points = config.decoder_n_points\n+\n+        if isinstance(self.n_points, list):\n+            num_points_list = self.n_points\n+        else:\n+            num_points_list = [self.n_points for _ in range(self.n_levels)]\n+\n+        self.num_points_list = num_points_list\n+        num_points_scale = [1 / n for n in self.num_points_list for _ in range(n)]\n+        self.register_buffer(\"num_points_scale\", torch.tensor(num_points_scale, dtype=torch.float32))\n+\n+        self.total_points = self.n_heads * sum(self.num_points_list)\n+\n+        self.sampling_offsets = nn.Linear(self.d_model, self.total_points * 2)\n+        self.attention_weights = nn.Linear(self.d_model, self.total_points)\n+\n+        self.ms_deformable_attn_core = multi_scale_deformable_attention_v2\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        reference_points=None,\n+        encoder_hidden_states=None,\n+        spatial_shapes=None,\n+        spatial_shapes_list=None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        batch_size, num_queries, _ = hidden_states.shape\n+        batch_size, sequence_length, _ = encoder_hidden_states.shape\n+\n+        if not is_torchdynamo_compiling() and (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() != sequence_length:\n+            raise ValueError(\n+                \"Make sure to align the spatial shapes with the sequence length of the encoder hidden states\"\n+            )\n+\n+        # Reshape for multi-head attention\n+        value = encoder_hidden_states.reshape(batch_size, sequence_length, self.n_heads, self.d_model // self.n_heads)\n+        if attention_mask is not None:\n+            value = value.masked_fill(~attention_mask[..., None], float(0))\n+\n+        sampling_offsets: torch.Tensor = self.sampling_offsets(hidden_states)\n+        sampling_offsets = sampling_offsets.reshape(\n+            batch_size, num_queries, self.n_heads, sum(self.num_points_list), 2\n+        )\n+\n+        attention_weights = self.attention_weights(hidden_states).reshape(\n+            batch_size, num_queries, self.n_heads, sum(self.num_points_list)\n+        )\n+        attention_weights = F.softmax(attention_weights, dim=-1)\n+\n+        if reference_points.shape[-1] == 2:\n+            offset_normalizer = torch.tensor(spatial_shapes)\n+            offset_normalizer = offset_normalizer.flip([1]).reshape(1, 1, 1, self.n_levels, 1, 2)\n+            sampling_locations = (\n+                reference_points.reshape(batch_size, sequence_length, 1, self.n_levels, 1, 2)\n+                + sampling_offsets / offset_normalizer\n+            )\n+        elif reference_points.shape[-1] == 4:\n+            # reference_points [8, 480, None, 1,  4]\n+            # sampling_offsets [8, 480, 8,    12, 2]\n+            num_points_scale = self.num_points_scale.to(dtype=hidden_states.dtype).unsqueeze(-1)\n+            offset = sampling_offsets * num_points_scale * reference_points[:, :, None, :, 2:] * self.offset_scale\n+            sampling_locations = reference_points[:, :, None, :, :2] + offset\n+        else:\n+            raise ValueError(\n+                \"Last dim of reference_points must be 2 or 4, but get {} instead.\".format(reference_points.shape[-1])\n+            )\n+\n+        output = self.ms_deformable_attn_core(\n+            value,\n+            spatial_shapes_list,\n+            sampling_locations,\n+            attention_weights,\n+            self.num_points_list,\n+            self.decoder_method,\n+        )\n+\n+        return output, attention_weights\n+\n+\n+class DFineGate(nn.Module):\n+    def __init__(self, d_model: int):\n+        super().__init__()\n+        self.gate = nn.Linear(2 * d_model, 2 * d_model)\n+        self.norm = nn.LayerNorm(d_model)\n+\n+    def forward(self, second_residual: torch.Tensor, hidden_states: torch.Tensor) -> torch.Tensor:\n+        gate_input = torch.cat([second_residual, hidden_states], dim=-1)\n+        gates = torch.sigmoid(self.gate(gate_input))\n+        gate1, gate2 = gates.chunk(2, dim=-1)\n+        hidden_states = self.norm(gate1 * second_residual + gate2 * hidden_states)\n+        return hidden_states\n+\n+\n+class DFineDecoderLayer(RTDetrDecoderLayer):\n+    def __init__(self, config: DFineConfig):\n+        super().__init__(config)\n+\n+        # override the encoder attention module with d-fine version\n+        self.encoder_attn = DFineMultiscaleDeformableAttention(config=config)\n+        # gate\n+        self.gateway = DFineGate(config.d_model)\n+\n+        del self.encoder_attn_layer_norm\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[torch.Tensor] = None,\n+        reference_points=None,\n+        spatial_shapes=None,\n+        spatial_shapes_list=None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> tuple[torch.Tensor, Any, Any]:\n+        # Self Attention\n+        hidden_states_2, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=encoder_attention_mask,\n+            position_embeddings=position_embeddings,\n+            output_attentions=output_attentions,\n+        )\n+\n+        hidden_states_2 = nn.functional.dropout(hidden_states_2, p=self.dropout, training=self.training)\n+        hidden_states = hidden_states + hidden_states_2\n+        hidden_states = self.self_attn_layer_norm(hidden_states)\n+        residual = hidden_states\n+\n+        # Cross-Attention\n+        cross_attn_weights = None\n+        hidden_states = hidden_states if position_embeddings is None else hidden_states + position_embeddings\n+        hidden_states_2, cross_attn_weights = self.encoder_attn(\n+            hidden_states=hidden_states,\n+            encoder_hidden_states=encoder_hidden_states,\n+            reference_points=reference_points,\n+            spatial_shapes=spatial_shapes,\n+            spatial_shapes_list=spatial_shapes_list,\n+        )\n+\n+        hidden_states_2 = nn.functional.dropout(hidden_states_2, p=self.dropout, training=self.training)\n+        hidden_states = self.gateway(residual, hidden_states_2)\n+\n+        # Fully Connected\n+        hidden_states_2 = self.activation_fn(self.fc1(hidden_states))\n+        hidden_states_2 = nn.functional.dropout(hidden_states_2, p=self.activation_dropout, training=self.training)\n+        hidden_states_2 = self.fc2(hidden_states_2)\n+        hidden_states_2 = nn.functional.dropout(hidden_states_2, p=self.dropout, training=self.training)\n+        hidden_states = hidden_states + hidden_states_2\n+        hidden_states = self.final_layer_norm(hidden_states.clamp(min=-65504, max=65504))\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights, cross_attn_weights)\n+\n+        return outputs\n+\n+\n+class DFinePreTrainedModel(RTDetrPreTrainedModel):\n+    def _init_weights(self, module):\n+        # initialize linear layer bias value according to a given probability value.\n+        if isinstance(module, (DFineForObjectDetection, DFineDecoder)):\n+            if module.class_embed is not None:\n+                for layer in module.class_embed:\n+                    prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n+                    bias = float(-math.log((1 - prior_prob) / prior_prob))\n+                    nn.init.xavier_uniform_(layer.weight)\n+                    nn.init.constant_(layer.bias, bias)\n+\n+            if module.bbox_embed is not None:\n+                for layer in module.bbox_embed:\n+                    nn.init.constant_(layer.layers[-1].weight, 0)\n+                    nn.init.constant_(layer.layers[-1].bias, 0)\n+\n+        if isinstance(module, DFineMultiscaleDeformableAttention):\n+            nn.init.constant_(module.sampling_offsets.weight.data, 0.0)\n+            default_dtype = torch.get_default_dtype()\n+            thetas = torch.arange(module.n_heads, dtype=torch.int64).to(default_dtype) * (\n+                2.0 * math.pi / module.n_heads\n+            )\n+            grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n+            grid_init = grid_init / grid_init.abs().max(-1, keepdim=True).values\n+            grid_init = grid_init.reshape(module.n_heads, 1, 2).tile([1, sum(module.num_points_list), 1])\n+            scaling = torch.concat([torch.arange(1, n + 1) for n in module.num_points_list]).reshape(1, -1, 1)\n+            grid_init *= scaling\n+            with torch.no_grad():\n+                module.sampling_offsets.bias.data[...] = grid_init.flatten()\n+\n+            nn.init.constant_(module.attention_weights.weight.data, 0.0)\n+            nn.init.constant_(module.attention_weights.bias.data, 0.0)\n+\n+        if isinstance(module, DFineModel):\n+            prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n+            bias = float(-math.log((1 - prior_prob) / prior_prob))\n+            nn.init.xavier_uniform_(module.enc_score_head.weight)\n+            nn.init.constant_(module.enc_score_head.bias, bias)\n+\n+        if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+\n+        if isinstance(module, DFineGate):\n+            bias = float(-math.log((1 - 0.5) / 0.5))\n+            init.constant_(module.gate.bias, bias)\n+            init.constant_(module.gate.weight, 0)\n+\n+        if isinstance(module, DFineLQE):\n+            init.constant_(module.reg_conf.layers[-1].bias, 0)\n+            init.constant_(module.reg_conf.layers[-1].weight, 0)\n+\n+        if hasattr(module, \"weight_embedding\") and self.config.learn_initial_query:\n+            nn.init.xavier_uniform_(module.weight_embedding.weight)\n+        if hasattr(module, \"denoising_class_embed\") and self.config.num_denoising > 0:\n+            nn.init.xavier_uniform_(module.denoising_class_embed.weight)\n+\n+\n+class DFineIntegral(nn.Module):\n+    \"\"\"\n+    A static layer that calculates integral results from a distribution.\n+\n+    This layer computes the target location using the formula: `sum{Pr(n) * W(n)}`,\n+    where Pr(n) is the softmax probability vector representing the discrete\n+    distribution, and W(n) is the non-uniform Weighting Function.\n+\n+    Args:\n+        max_num_bins (int): Max number of the discrete bins. Default is 32.\n+                       It can be adjusted based on the dataset or task requirements.\n+    \"\"\"\n+\n+    def __init__(self, config: DFineConfig):\n+        super().__init__()\n+        self.max_num_bins = config.max_num_bins\n+\n+    def forward(self, pred_corners: torch.Tensor, project: torch.Tensor) -> torch.Tensor:\n+        batch_size, num_queries, _ = pred_corners.shape\n+        pred_corners = F.softmax(pred_corners.reshape(-1, self.max_num_bins + 1), dim=1)\n+        pred_corners = F.linear(pred_corners, project.to(pred_corners.device)).reshape(-1, 4)\n+        pred_corners = pred_corners.reshape(batch_size, num_queries, -1)\n+        return pred_corners\n+\n+\n+class DFineDecoderOutput(RTDetrDecoderOutput):\n+    pass\n+\n+\n+class DFineDecoder(RTDetrDecoder):\n+    \"\"\"\n+    D-FINE Decoder implementing Fine-grained Distribution Refinement (FDR).\n+\n+    This decoder refines object detection predictions through iterative updates across multiple layers,\n+    utilizing attention mechanisms, location quality estimators, and distribution refinement techniques\n+    to improve bounding box accuracy and robustness.\n+    \"\"\"\n+\n+    def __init__(self, config: DFineConfig):\n+        self.eval_idx = config.eval_idx if config.eval_idx >= 0 else config.decoder_layers + config.eval_idx\n+        super().__init__(config=config)\n+        self.reg_scale = nn.Parameter(torch.tensor([config.reg_scale]), requires_grad=False)\n+        self.max_num_bins = config.max_num_bins\n+        self.d_model = config.d_model\n+        self.layer_scale = config.layer_scale\n+        self.pre_bbox_head = DFineMLP(config.hidden_size, config.hidden_size, 4, 3)\n+        self.integral = DFineIntegral(config)\n+        self.num_head = config.decoder_attention_heads\n+        self.up = nn.Parameter(torch.tensor([config.up]), requires_grad=False)\n+        self.lqe_layers = nn.ModuleList([DFineLQE(config) for _ in range(config.decoder_layers)])\n+        self.layers = nn.ModuleList(\n+            [DFineDecoderLayer(config) for _ in range(config.decoder_layers)]\n+            + [DFineDecoderLayer(config) for _ in range(config.decoder_layers - self.eval_idx - 1)]\n+        )\n+\n+    def forward(\n+        self,\n+        encoder_hidden_states: torch.Tensor,\n+        reference_points: torch.Tensor,\n+        inputs_embeds: torch.Tensor,\n+        spatial_shapes,\n+        level_start_index=None,\n+        spatial_shapes_list=None,\n+        output_hidden_states=None,\n+        encoder_attention_mask=None,\n+        memory_mask=None,\n+        output_attentions=None,\n+        return_dict=None,\n+    ) -> DFineDecoderOutput:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if inputs_embeds is not None:\n+            hidden_states = inputs_embeds\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n+        intermediate = ()\n+        intermediate_reference_points = ()\n+        intermediate_logits = ()\n+        intermediate_predicted_corners = ()\n+        initial_reference_points = ()\n+\n+        output_detach = pred_corners_undetach = 0\n+\n+        project = weighting_function(self.max_num_bins, self.up, self.reg_scale)\n+        ref_points_detach = F.sigmoid(reference_points)\n+\n+        for i, decoder_layer in enumerate(self.layers):\n+            ref_points_input = ref_points_detach.unsqueeze(2)\n+            query_pos_embed = self.query_pos_head(ref_points_detach).clamp(min=-10, max=10)\n+\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            output = decoder_layer(\n+                hidden_states=hidden_states,\n+                position_embeddings=query_pos_embed,\n+                reference_points=ref_points_input,\n+                spatial_shapes=spatial_shapes,\n+                spatial_shapes_list=spatial_shapes_list,\n+                encoder_hidden_states=encoder_hidden_states,\n+                encoder_attention_mask=encoder_attention_mask,\n+                output_attentions=output_attentions,\n+            )\n+\n+            hidden_states = output[0]\n+\n+            if i == 0:\n+                # Initial bounding box predictions with inverse sigmoid refinement\n+                new_reference_points = F.sigmoid(self.pre_bbox_head(output[0]) + inverse_sigmoid(ref_points_detach))\n+                ref_points_initial = new_reference_points.detach()\n+\n+            # Refine bounding box corners using FDR, integrating previous layer's corrections\n+            if self.bbox_embed is not None:\n+                pred_corners = self.bbox_embed[i](hidden_states + output_detach) + pred_corners_undetach\n+                inter_ref_bbox = distance2bbox(\n+                    ref_points_initial, self.integral(pred_corners, project), self.reg_scale\n+                )\n+                pred_corners_undetach = pred_corners\n+                ref_points_detach = inter_ref_bbox.detach()\n+\n+            output_detach = hidden_states.detach()\n+\n+            intermediate += (hidden_states,)\n+\n+            if self.class_embed is not None and (self.training or i == self.eval_idx):\n+                scores = self.class_embed[i](hidden_states)\n+                # Lqe does not affect the performance here.\n+                scores = self.lqe_layers[i](scores, pred_corners)\n+                intermediate_logits += (scores,)\n+                intermediate_reference_points += (inter_ref_bbox,)\n+                initial_reference_points += (ref_points_initial,)\n+                intermediate_predicted_corners += (pred_corners,)\n+\n+            if output_attentions:\n+                all_self_attns += (output[1],)\n+\n+                if encoder_hidden_states is not None:\n+                    all_cross_attentions += (output[2],)\n+\n+        # Keep batch_size as first dimension\n+        intermediate = torch.stack(intermediate)\n+        if self.class_embed is not None and self.bbox_embed is not None:\n+            intermediate_logits = torch.stack(intermediate_logits, dim=1)\n+            intermediate_predicted_corners = torch.stack(intermediate_predicted_corners, dim=1)\n+            initial_reference_points = torch.stack(initial_reference_points, dim=1)\n+            intermediate_reference_points = torch.stack(intermediate_reference_points, dim=1)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        if not return_dict:\n+            return tuple(\n+                v\n+                for v in [\n+                    hidden_states,\n+                    intermediate,\n+                    intermediate_logits,\n+                    intermediate_reference_points,\n+                    intermediate_predicted_corners,\n+                    initial_reference_points,\n+                    all_hidden_states,\n+                    all_self_attns,\n+                    all_cross_attentions,\n+                ]\n+                if v is not None\n+            )\n+\n+        return DFineDecoderOutput(\n+            last_hidden_state=hidden_states,\n+            intermediate_hidden_states=intermediate,\n+            intermediate_logits=intermediate_logits,\n+            intermediate_reference_points=intermediate_reference_points,\n+            intermediate_predicted_corners=intermediate_predicted_corners,\n+            initial_reference_points=initial_reference_points,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+            cross_attentions=all_cross_attentions,\n+        )\n+\n+\n+class DFineModel(RTDetrModel):\n+    def __init__(self, config: DFineConfig):\n+        super().__init__(config)\n+        del self.decoder_input_proj\n+        self.encoder = DFineHybridEncoder(config=config)\n+        num_backbone_outs = len(config.decoder_in_channels)\n+        decoder_input_proj = []\n+        in_channels = config.decoder_in_channels[-1]\n+        for _ in range(num_backbone_outs):\n+            if config.hidden_size == config.decoder_in_channels[-1]:\n+                decoder_input_proj.append(nn.Identity())\n+            else:\n+                conv = nn.Conv2d(in_channels, config.d_model, kernel_size=1, bias=False)\n+                batchnorm = nn.BatchNorm2d(config.d_model, config.batch_norm_eps)\n+                decoder_input_proj.append(nn.Sequential(conv, batchnorm))\n+        for _ in range(config.num_feature_levels - num_backbone_outs):\n+            if config.hidden_size == config.decoder_in_channels[-1]:\n+                decoder_input_proj.append(nn.Identity())\n+            else:\n+                conv = nn.Conv2d(in_channels, config.d_model, kernel_size=3, stride=2, padding=1, bias=False)\n+                batchnorm = nn.BatchNorm2d(config.d_model, config.batch_norm_eps)\n+                decoder_input_proj.append(nn.Sequential(conv, batchnorm))\n+        self.decoder_input_proj = nn.ModuleList(decoder_input_proj)\n+        self.decoder = DFineDecoder(config)\n+\n+\n+class DFineForObjectDetection(RTDetrForObjectDetection, DFinePreTrainedModel):\n+    def __init__(self, config: DFineConfig):\n+        DFinePreTrainedModel.__init__(config)\n+\n+        # D-FINE encoder-decoder model\n+        self.eval_idx = config.eval_idx if config.eval_idx >= 0 else config.decoder_layers + config.eval_idx\n+        self.model = DFineModel(config)\n+        scaled_dim = round(config.layer_scale * config.hidden_size)\n+        num_pred = config.decoder_layers\n+        self.class_embed = nn.ModuleList([nn.Linear(config.d_model, config.num_labels) for _ in range(num_pred)])\n+        self.bbox_embed = nn.ModuleList(\n+            [\n+                DFineMLP(config.hidden_size, config.hidden_size, 4 * (config.max_num_bins + 1), 3)\n+                for _ in range(self.eval_idx + 1)\n+            ]\n+            + [\n+                DFineMLP(scaled_dim, scaled_dim, 4 * (config.max_num_bins + 1), 3)\n+                for _ in range(config.decoder_layers - self.eval_idx - 1)\n+            ]\n+        )\n+\n+        # here self.model.decoder.bbox_embed is null, but not self.bbox_embed\n+        self.model.decoder.class_embed = self.class_embed\n+        self.model.decoder.bbox_embed = self.bbox_embed\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def forward(**super_kwargs):\n+        \"\"\"\n+        ```python\n+        >>> import torch\n+        >>> from transformers.image_utils import load_image\n+        >>> from transformers import AutoImageProcessor, DFineForObjectDetection\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = load_image(url)\n+\n+        >>> image_processor = AutoImageProcessor.from_pretrained(\"ustc-community/dfine_x_coco\")\n+        >>> model = DFineForObjectDetection.from_pretrained(\"ustc-community/dfine_x_coco\")\n+\n+        >>> # prepare image for the model\n+        >>> inputs = image_processor(images=image, return_tensors=\"pt\")\n+\n+        >>> # forward pass\n+        >>> outputs = model(**inputs)\n+\n+        >>> logits = outputs.logits\n+        >>> list(logits.shape)\n+        [1, 300, 80]\n+\n+        >>> boxes = outputs.pred_boxes\n+        >>> list(boxes.shape)\n+        [1, 300, 4]\n+\n+        >>> # convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n+        >>> target_sizes = torch.tensor([image.size[::-1]])\n+        >>> results = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)\n+        >>> result = results[0]  # first image in batch\n+\n+        >>> for score, label, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n+        ...     box = [round(i, 2) for i in box.tolist()]\n+        ...     print(\n+        ...         f\"Detected {model.config.id2label[label.item()]} with confidence \"\n+        ...         f\"{round(score.item(), 3)} at location {box}\"\n+        ...     )\n+        Detected cat with confidence 0.958 at location [344.49, 23.4, 639.84, 374.27]\n+        Detected cat with confidence 0.956 at location [11.71, 53.52, 316.64, 472.33]\n+        Detected remote with confidence 0.947 at location [40.46, 73.7, 175.62, 117.57]\n+        Detected sofa with confidence 0.918 at location [0.59, 1.88, 640.25, 474.74]\n+        ```\n+        \"\"\"\n+        super().forward(**super_kwargs)\n+\n+\n+def weighting_function(max_num_bins: int, up: torch.Tensor, reg_scale: int) -> torch.Tensor:\n+    \"\"\"\n+    Generates the non-uniform Weighting Function W(n) for bounding box regression.\n+\n+    Args:\n+        max_num_bins (int): Max number of the discrete bins.\n+        up (Tensor): Controls upper bounds of the sequence,\n+                     where maximum offset is Â±up * H / W.\n+        reg_scale (float): Controls the curvature of the Weighting Function.\n+                           Larger values result in flatter weights near the central axis W(max_num_bins/2)=0\n+                           and steeper weights at both ends.\n+    Returns:\n+        Tensor: Sequence of Weighting Function.\n+    \"\"\"\n+    upper_bound1 = abs(up[0]) * abs(reg_scale)\n+    upper_bound2 = abs(up[0]) * abs(reg_scale) * 2\n+    step = (upper_bound1 + 1) ** (2 / (max_num_bins - 2))\n+    left_values = [-((step) ** i) + 1 for i in range(max_num_bins // 2 - 1, 0, -1)]\n+    right_values = [(step) ** i - 1 for i in range(1, max_num_bins // 2)]\n+    values = [-upper_bound2] + left_values + [torch.zeros_like(up[0][None])] + right_values + [upper_bound2]\n+    values = torch.cat(values, 0)\n+    return values\n+\n+\n+class DFineMLPPredictionHead(RTDetrMLPPredictionHead):\n+    pass\n+\n+\n+def distance2bbox(points, distance: torch.Tensor, reg_scale: float) -> torch.Tensor:\n+    \"\"\"\n+    Decodes edge-distances into bounding box coordinates.\n+\n+    Args:\n+        points (`torch.Tensor`):\n+            (batch_size, num_boxes, 4) or (num_boxes, 4) format, representing [x_center, y_center, width, height]\n+        distance (`torch.Tensor`):\n+            (batch_size, num_boxes, 4) or (num_boxes, 4), representing distances from the point to the left, top, right, and bottom boundaries.\n+        reg_scale (`float`):\n+            Controls the curvature of the Weighting Function.\n+    Returns:\n+        `torch.Tensor`: Bounding boxes in (batch_size, num_boxes, 4) or (num_boxes, 4) format, representing [x_center, y_center, width, height]\n+    \"\"\"\n+    reg_scale = abs(reg_scale)\n+    top_left_x = points[..., 0] - (0.5 * reg_scale + distance[..., 0]) * (points[..., 2] / reg_scale)\n+    top_left_y = points[..., 1] - (0.5 * reg_scale + distance[..., 1]) * (points[..., 3] / reg_scale)\n+    bottom_right_x = points[..., 0] + (0.5 * reg_scale + distance[..., 2]) * (points[..., 2] / reg_scale)\n+    bottom_right_y = points[..., 1] + (0.5 * reg_scale + distance[..., 3]) * (points[..., 3] / reg_scale)\n+\n+    bboxes = torch.stack([top_left_x, top_left_y, bottom_right_x, bottom_right_y], -1)\n+\n+    return corners_to_center_format(bboxes)\n+\n+\n+class DFineMLP(nn.Module):\n+    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, num_layers: int, act: str = \"relu\"):\n+        super().__init__()\n+        self.num_layers = num_layers\n+        hidden_dims = [hidden_dim] * (num_layers - 1)\n+        input_dims = [input_dim] + hidden_dims\n+        output_dims = hidden_dims + [output_dim]\n+        self.layers = nn.ModuleList(nn.Linear(in_dim, out_dim) for in_dim, out_dim in zip(input_dims, output_dims))\n+        self.act = ACT2CLS[act]()\n+\n+    def forward(self, stat_features: torch.Tensor) -> torch.Tensor:\n+        for i, layer in enumerate(self.layers):\n+            stat_features = self.act(layer(stat_features)) if i < self.num_layers - 1 else layer(stat_features)\n+        return stat_features\n+\n+\n+class DFineLQE(nn.Module):\n+    def __init__(self, config: DFineConfig):\n+        super().__init__()\n+        self.top_prob_values = config.top_prob_values\n+        self.max_num_bins = config.max_num_bins\n+        self.reg_conf = DFineMLP(4 * (self.top_prob_values + 1), config.lqe_hidden_dim, 1, config.lqe_layers)\n+\n+    def forward(self, scores: torch.Tensor, pred_corners: torch.Tensor) -> torch.Tensor:\n+        batch_size, length, _ = pred_corners.size()\n+        prob = F.softmax(pred_corners.reshape(batch_size, length, 4, self.max_num_bins + 1), dim=-1)\n+        prob_topk, _ = prob.topk(self.top_prob_values, dim=-1)\n+        stat = torch.cat([prob_topk, prob_topk.mean(dim=-1, keepdim=True)], dim=-1)\n+        quality_score = self.reg_conf(stat.reshape(batch_size, length, -1))\n+        scores = scores + quality_score\n+        return scores\n+\n+\n+class DFineConvNormLayer(RTDetrConvNormLayer):\n+    def __init__(\n+        self,\n+        config: DFineConfig,\n+        in_channels: int,\n+        out_channels: int,\n+        kernel_size: int,\n+        stride: int,\n+        groups: int = 1,\n+        padding: int = None,\n+        activation: str = None,\n+    ):\n+        super().__init__(config, in_channels, out_channels, kernel_size, stride, padding=None, activation=activation)\n+        self.conv = nn.Conv2d(\n+            in_channels,\n+            out_channels,\n+            kernel_size,\n+            stride,\n+            groups=groups,\n+            padding=(kernel_size - 1) // 2 if padding is None else padding,\n+            bias=False,\n+        )\n+\n+\n+class DFineRepVggBlock(RTDetrRepVggBlock):\n+    def __init__(self, config: DFineConfig, in_channels: int, out_channels: int):\n+        super().__init__(config)\n+        hidden_channels = in_channels\n+        self.conv1 = DFineConvNormLayer(config, hidden_channels, out_channels, 3, 1, padding=1)\n+        self.conv2 = DFineConvNormLayer(config, hidden_channels, out_channels, 1, 1, padding=0)\n+\n+\n+class DFineCSPRepLayer(nn.Module):\n+    \"\"\"\n+    Cross Stage Partial (CSP) network layer with RepVGG blocks.\n+    \"\"\"\n+\n+    def __init__(\n+        self, config: DFineConfig, in_channels: int, out_channels: int, num_blocks: int, expansion: float = 1.0\n+    ):\n+        super().__init__()\n+        in_channels = in_channels\n+        out_channels = out_channels\n+        activation = config.activation_function\n+\n+        hidden_channels = int(out_channels * expansion)\n+        self.conv1 = DFineConvNormLayer(config, in_channels, hidden_channels, 1, 1, activation=activation)\n+        self.conv2 = DFineConvNormLayer(config, in_channels, hidden_channels, 1, 1, activation=activation)\n+        self.bottlenecks = nn.ModuleList(\n+            [DFineRepVggBlock(config, hidden_channels, hidden_channels) for _ in range(num_blocks)]\n+        )\n+        if hidden_channels != out_channels:\n+            self.conv3 = DFineConvNormLayer(config, hidden_channels, out_channels, 1, 1, activation=activation)\n+        else:\n+            self.conv3 = nn.Identity()\n+\n+    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n+        hidden_state_1 = self.conv1(hidden_state)\n+        for bottleneck in self.bottlenecks:\n+            hidden_state_1 = bottleneck(hidden_state_1)\n+        hidden_state_2 = self.conv2(hidden_state)\n+        hidden_state_3 = self.conv3(hidden_state_1 + hidden_state_2)\n+        return hidden_state_3\n+\n+\n+class DFineRepNCSPELAN4(nn.Module):\n+    def __init__(self, config: DFineConfig, act: str = \"silu\", numb_blocks: int = 3):\n+        super().__init__()\n+        conv1_dim = config.encoder_hidden_dim * 2\n+        conv2_dim = config.encoder_hidden_dim\n+        conv3_dim = config.encoder_hidden_dim * 2\n+        conv4_dim = round(config.hidden_expansion * config.encoder_hidden_dim // 2)\n+        self.conv_dim = conv3_dim // 2\n+        self.conv1 = DFineConvNormLayer(config, conv1_dim, conv3_dim, 1, 1, activation=act)\n+        self.csp_rep1 = DFineCSPRepLayer(config, conv3_dim // 2, conv4_dim, num_blocks=numb_blocks)\n+        self.conv2 = DFineConvNormLayer(config, conv4_dim, conv4_dim, 3, 1, activation=act)\n+        self.csp_rep2 = DFineCSPRepLayer(config, conv4_dim, conv4_dim, num_blocks=numb_blocks)\n+        self.conv3 = DFineConvNormLayer(config, conv4_dim, conv4_dim, 3, 1, activation=act)\n+        self.conv4 = DFineConvNormLayer(config, conv3_dim + (2 * conv4_dim), conv2_dim, 1, 1, activation=act)\n+\n+    def forward(self, input_features: torch.Tensor) -> torch.Tensor:\n+        # Split initial features into two branches after first convolution\n+        split_features = list(self.conv1(input_features).split((self.conv_dim, self.conv_dim), 1))\n+\n+        # Process branches sequentially\n+        branch1 = self.csp_rep1(split_features[-1])\n+        branch1 = self.conv2(branch1)\n+        branch2 = self.csp_rep2(branch1)\n+        branch2 = self.conv3(branch2)\n+\n+        split_features.extend([branch1, branch2])\n+        merged_features = torch.cat(split_features, 1)\n+        merged_features = self.conv4(merged_features)\n+        return merged_features\n+\n+\n+class DFineSCDown(nn.Module):\n+    def __init__(self, config: DFineConfig, kernel_size: int, stride: int):\n+        super().__init__()\n+        self.conv1 = DFineConvNormLayer(config, config.encoder_hidden_dim, config.encoder_hidden_dim, 1, 1)\n+        self.conv2 = DFineConvNormLayer(\n+            config,\n+            config.encoder_hidden_dim,\n+            config.encoder_hidden_dim,\n+            kernel_size,\n+            stride,\n+            config.encoder_hidden_dim,\n+        )\n+\n+    def forward(self, input_features: torch.Tensor) -> torch.Tensor:\n+        input_features = self.conv1(input_features)\n+        input_features = self.conv2(input_features)\n+        return input_features\n+\n+\n+class DFineEncoder(RTDetrEncoder):\n+    pass\n+\n+\n+class DFineHybridEncoder(RTDetrHybridEncoder):\n+    def __init__(self, config: DFineConfig):\n+        nn.Module.__init__(self)\n+        self.config = config\n+        self.in_channels = config.encoder_in_channels\n+        self.num_fpn_stages = len(self.in_channels) - 1\n+        self.feat_strides = config.feat_strides\n+        self.encoder_hidden_dim = config.encoder_hidden_dim\n+        self.encode_proj_layers = config.encode_proj_layers\n+        self.positional_encoding_temperature = config.positional_encoding_temperature\n+        self.eval_size = config.eval_size\n+        self.out_channels = [self.encoder_hidden_dim for _ in self.in_channels]\n+        self.out_strides = self.feat_strides\n+\n+        # encoder transformer\n+        self.encoder = nn.ModuleList([DFineEncoder(config) for _ in range(len(self.encode_proj_layers))])\n+        # top-down fpn\n+        self.lateral_convs = nn.ModuleList()\n+        self.fpn_blocks = nn.ModuleList()\n+        for _ in range(len(self.in_channels) - 1, 0, -1):\n+            lateral_layer = DFineConvNormLayer(config, self.encoder_hidden_dim, self.encoder_hidden_dim, 1, 1)\n+            self.lateral_convs.append(lateral_layer)\n+            num_blocks = round(3 * config.depth_mult)\n+            fpn_layer = DFineRepNCSPELAN4(config, numb_blocks=num_blocks)\n+            self.fpn_blocks.append(fpn_layer)\n+\n+        # bottom-up pan\n+        self.downsample_convs = nn.ModuleList()\n+        self.pan_blocks = nn.ModuleList()\n+        for _ in range(len(self.in_channels) - 1):\n+            self.downsample_convs.append(DFineSCDown(config, 3, 2))\n+            num_blocks = round(3 * config.depth_mult)\n+            self.pan_blocks.append(DFineRepNCSPELAN4(config, numb_blocks=num_blocks))\n+\n+\n+__all__ = [\n+    \"DFineConfig\",\n+    \"DFineModel\",\n+    \"DFinePreTrainedModel\",\n+    \"DFineForObjectDetection\",\n+]"
        },
        {
            "sha": "1bf549dccf1d5b6bcb58f9337d4c97ca41536ce4",
            "filename": "src/transformers/models/hgnet_v2/__init__.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2F__init__.py?ref=4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63",
            "patch": "@@ -0,0 +1,29 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_hgnet_v2 import *\n+    from .modeling_hgnet_v2 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "80a59087395e9fe5f7c9bfb1eb2b94ea1c05e534",
            "filename": "src/transformers/models/hgnet_v2/configuration_hgnet_v2.py",
            "status": "added",
            "additions": 152,
            "deletions": 0,
            "changes": 152,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fconfiguration_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fconfiguration_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fconfiguration_hgnet_v2.py?ref=4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63",
            "patch": "@@ -0,0 +1,152 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/hgnet_v2/modular_hgnet_v2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_hgnet_v2.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 Baidu Inc and The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n+\n+\n+# TODO: Modular conversion for resnet must be fixed as\n+# it provides incorrect import for configuration like resnet_resnet\n+class HGNetV2Config(BackboneConfigMixin, PretrainedConfig):\n+    \"\"\"\n+    This is the configuration class to store the configuration of a [`HGNetV2Backbone`]. It is used to instantiate a HGNet-V2\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of D-FINE-X-COCO B4 \"[ustc-community/dfine_x_coco\"](https://huggingface.co/ustc-community/dfine_x_coco\").\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        num_channels (`int`, *optional*, defaults to 3):\n+            The number of input channels.\n+        embedding_size (`int`, *optional*, defaults to 64):\n+            Dimensionality (hidden size) for the embedding layer.\n+        depths (`List[int]`, *optional*, defaults to `[3, 4, 6, 3]`):\n+            Depth (number of layers) for each stage.\n+        hidden_sizes (`List[int]`, *optional*, defaults to `[256, 512, 1024, 2048]`):\n+            Dimensionality (hidden size) at each stage.\n+        hidden_act (`str`, *optional*, defaults to `\"relu\"`):\n+            The non-linear activation function in each block. If string, `\"gelu\"`, `\"relu\"`, `\"selu\"` and `\"gelu_new\"`\n+            are supported.\n+        out_features (`List[str]`, *optional*):\n+            If used as backbone, list of features to output. Can be any of `\"stem\"`, `\"stage1\"`, `\"stage2\"`, etc.\n+            (depending on how many stages the model has). If unset and `out_indices` is set, will default to the\n+            corresponding stages. If unset and `out_indices` is unset, will default to the last stage. Must be in the\n+            same order as defined in the `stage_names` attribute.\n+        out_indices (`List[int]`, *optional*):\n+            If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how\n+            many stages the model has). If unset and `out_features` is set, will default to the corresponding stages.\n+            If unset and `out_features` is unset, will default to the last stage. Must be in the\n+            same order as defined in the `stage_names` attribute.\n+        stem_channels (`List[int]`, *optional*, defaults to `[3, 32, 48]`):\n+            Channel dimensions for the stem layers:\n+            - First number (3) is input image channels\n+            - Second number (32) is intermediate stem channels\n+            - Third number (48) is output stem channels\n+        stage_in_channels (`List[int]`, *optional*, defaults to `[48, 128, 512, 1024]`):\n+            Input channel dimensions for each stage of the backbone.\n+            This defines how many channels the input to each stage will have.\n+        stage_mid_channels (`List[int]`, *optional*, defaults to `[48, 96, 192, 384]`):\n+            Mid-channel dimensions for each stage of the backbone.\n+            This defines the number of channels used in the intermediate layers of each stage.\n+        stage_out_channels (`List[int]`, *optional*, defaults to `[128, 512, 1024, 2048]`):\n+            Output channel dimensions for each stage of the backbone.\n+            This defines how many channels the output of each stage will have.\n+        stage_num_blocks (`List[int]`, *optional*, defaults to `[1, 1, 3, 1]`):\n+            Number of blocks to be used in each stage of the backbone.\n+            This controls the depth of each stage by specifying how many convolutional blocks to stack.\n+        stage_downsample (`List[bool]`, *optional*, defaults to `[False, True, True, True]`):\n+            Indicates whether to downsample the feature maps at each stage.\n+            If `True`, the spatial dimensions of the feature maps will be reduced.\n+        stage_light_block (`List[bool]`, *optional*, defaults to `[False, False, True, True]`):\n+            Indicates whether to use light blocks in each stage.\n+            Light blocks are a variant of convolutional blocks that may have fewer parameters.\n+        stage_kernel_size (`List[int]`, *optional*, defaults to `[3, 3, 5, 5]`):\n+            Kernel sizes for the convolutional layers in each stage.\n+        stage_numb_of_layers (`List[int]`, *optional*, defaults to `[6, 6, 6, 6]`):\n+            Number of layers to be used in each block of the stage.\n+        use_learnable_affine_block (`bool`, *optional*, defaults to `False`):\n+            Whether to use Learnable Affine Blocks (LAB) in the network.\n+            LAB adds learnable scale and bias parameters after certain operations.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+    \"\"\"\n+\n+    model_type = \"hgnet_v2\"\n+\n+    def __init__(\n+        self,\n+        num_channels=3,\n+        embedding_size=64,\n+        depths=[3, 4, 6, 3],\n+        hidden_sizes=[256, 512, 1024, 2048],\n+        hidden_act=\"relu\",\n+        out_features=None,\n+        out_indices=None,\n+        stem_channels=[3, 32, 48],\n+        stage_in_channels=[48, 128, 512, 1024],\n+        stage_mid_channels=[48, 96, 192, 384],\n+        stage_out_channels=[128, 512, 1024, 2048],\n+        stage_num_blocks=[1, 1, 3, 1],\n+        stage_downsample=[False, True, True, True],\n+        stage_light_block=[False, False, True, True],\n+        stage_kernel_size=[3, 3, 5, 5],\n+        stage_numb_of_layers=[6, 6, 6, 6],\n+        use_learnable_affine_block=False,\n+        initializer_range=0.02,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.num_channels = num_channels\n+        self.embedding_size = embedding_size\n+        self.depths = depths\n+        self.hidden_sizes = hidden_sizes\n+        self.hidden_act = hidden_act\n+        self.stage_names = [\"stem\"] + [f\"stage{idx}\" for idx in range(1, len(depths) + 1)]\n+        self._out_features, self._out_indices = get_aligned_output_features_output_indices(\n+            out_features=out_features, out_indices=out_indices, stage_names=self.stage_names\n+        )\n+        self.stem_channels = stem_channels\n+        self.stage_in_channels = stage_in_channels\n+        self.stage_mid_channels = stage_mid_channels\n+        self.stage_out_channels = stage_out_channels\n+        self.stage_num_blocks = stage_num_blocks\n+        self.stage_downsample = stage_downsample\n+        self.stage_light_block = stage_light_block\n+        self.stage_kernel_size = stage_kernel_size\n+        self.stage_numb_of_layers = stage_numb_of_layers\n+        self.use_learnable_affine_block = use_learnable_affine_block\n+        self.initializer_range = initializer_range\n+\n+        if not (\n+            len(stage_in_channels)\n+            == len(stage_mid_channels)\n+            == len(stage_out_channels)\n+            == len(stage_num_blocks)\n+            == len(stage_downsample)\n+            == len(stage_light_block)\n+            == len(stage_kernel_size)\n+            == len(stage_numb_of_layers)\n+        ):\n+            raise ValueError(\"All stage configuration lists must have the same length.\")\n+\n+\n+__all__ = [\"HGNetV2Config\"]"
        },
        {
            "sha": "d4cc2e929b43017f39221163a7ea9853c1f6fd88",
            "filename": "src/transformers/models/hgnet_v2/modeling_hgnet_v2.py",
            "status": "added",
            "additions": 541,
            "deletions": 0,
            "changes": 541,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py?ref=4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63",
            "patch": "@@ -0,0 +1,541 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/hgnet_v2/modular_hgnet_v2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_hgnet_v2.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 Baidu Inc and The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from typing import Optional\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import Tensor, nn\n+from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+\n+from ...activations import ACT2FN\n+from ...modeling_outputs import BackboneOutput, BaseModelOutputWithNoAttention, ImageClassifierOutputWithNoAttention\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, replace_return_docstrings\n+from ...utils.backbone_utils import BackboneMixin\n+from .configuration_hgnet_v2 import HGNetV2Config\n+\n+\n+# General docstring\n+_CONFIG_FOR_DOC = \"HGNetV2Config\"\n+\n+\n+class HGNetV2PreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = HGNetV2Config\n+    base_model_prefix = \"hgnetv2\"\n+    main_input_name = \"pixel_values\"\n+    _no_split_modules = [\"HGNetV2BasicLayer\"]\n+\n+    def _init_weights(self, module):\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.BatchNorm2d):\n+            module.weight.data.fill_(1.0)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+\n+\n+class HGNetV2LearnableAffineBlock(nn.Module):\n+    def __init__(self, scale_value: float = 1.0, bias_value: float = 0.0):\n+        super().__init__()\n+        self.scale = nn.Parameter(torch.tensor([scale_value]), requires_grad=True)\n+        self.bias = nn.Parameter(torch.tensor([bias_value]), requires_grad=True)\n+\n+    def forward(self, hidden_state: Tensor) -> Tensor:\n+        hidden_state = self.scale * hidden_state + self.bias\n+        return hidden_state\n+\n+\n+class HGNetV2ConvLayer(nn.Module):\n+    def __init__(\n+        self,\n+        in_channels: int,\n+        out_channels: int,\n+        kernel_size: int,\n+        stride: int = 1,\n+        groups: int = 1,\n+        activation: str = \"relu\",\n+        use_learnable_affine_block: bool = False,\n+    ):\n+        super().__init__()\n+        self.convolution = nn.Conv2d(\n+            in_channels,\n+            out_channels,\n+            kernel_size=kernel_size,\n+            stride=stride,\n+            groups=groups,\n+            padding=(kernel_size - 1) // 2,\n+            bias=False,\n+        )\n+        self.normalization = nn.BatchNorm2d(out_channels)\n+        self.activation = ACT2FN[activation] if activation is not None else nn.Identity()\n+        if activation and use_learnable_affine_block:\n+            self.lab = HGNetV2LearnableAffineBlock()\n+        else:\n+            self.lab = nn.Identity()\n+\n+    def forward(self, input: Tensor) -> Tensor:\n+        hidden_state = self.convolution(input)\n+        hidden_state = self.normalization(hidden_state)\n+        hidden_state = self.activation(hidden_state)\n+        hidden_state = self.lab(hidden_state)\n+        return hidden_state\n+\n+\n+class HGNetV2ConvLayerLight(nn.Module):\n+    def __init__(\n+        self, in_channels: int, out_channels: int, kernel_size: int, use_learnable_affine_block: bool = False\n+    ):\n+        super().__init__()\n+        self.conv1 = HGNetV2ConvLayer(\n+            in_channels,\n+            out_channels,\n+            kernel_size=1,\n+            activation=None,\n+            use_learnable_affine_block=use_learnable_affine_block,\n+        )\n+        self.conv2 = HGNetV2ConvLayer(\n+            out_channels,\n+            out_channels,\n+            kernel_size=kernel_size,\n+            groups=out_channels,\n+            use_learnable_affine_block=use_learnable_affine_block,\n+        )\n+\n+    def forward(self, hidden_state: Tensor) -> Tensor:\n+        hidden_state = self.conv1(hidden_state)\n+        hidden_state = self.conv2(hidden_state)\n+        return hidden_state\n+\n+\n+class HGNetV2Embeddings(nn.Module):\n+    def __init__(self, config: HGNetV2Config):\n+        super().__init__()\n+\n+        self.stem1 = HGNetV2ConvLayer(\n+            config.stem_channels[0],\n+            config.stem_channels[1],\n+            kernel_size=3,\n+            stride=2,\n+            activation=config.hidden_act,\n+            use_learnable_affine_block=config.use_learnable_affine_block,\n+        )\n+        self.stem2a = HGNetV2ConvLayer(\n+            config.stem_channels[1],\n+            config.stem_channels[1] // 2,\n+            kernel_size=2,\n+            stride=1,\n+            activation=config.hidden_act,\n+            use_learnable_affine_block=config.use_learnable_affine_block,\n+        )\n+        self.stem2b = HGNetV2ConvLayer(\n+            config.stem_channels[1] // 2,\n+            config.stem_channels[1],\n+            kernel_size=2,\n+            stride=1,\n+            activation=config.hidden_act,\n+            use_learnable_affine_block=config.use_learnable_affine_block,\n+        )\n+        self.stem3 = HGNetV2ConvLayer(\n+            config.stem_channels[1] * 2,\n+            config.stem_channels[1],\n+            kernel_size=3,\n+            stride=2,\n+            activation=config.hidden_act,\n+            use_learnable_affine_block=config.use_learnable_affine_block,\n+        )\n+        self.stem4 = HGNetV2ConvLayer(\n+            config.stem_channels[1],\n+            config.stem_channels[2],\n+            kernel_size=1,\n+            stride=1,\n+            activation=config.hidden_act,\n+            use_learnable_affine_block=config.use_learnable_affine_block,\n+        )\n+\n+        self.pool = nn.MaxPool2d(kernel_size=2, stride=1, ceil_mode=True)\n+        self.num_channels = config.num_channels\n+\n+    def forward(self, pixel_values: Tensor) -> Tensor:\n+        num_channels = pixel_values.shape[1]\n+        if num_channels != self.num_channels:\n+            raise ValueError(\n+                \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n+            )\n+        embedding = self.stem1(pixel_values)\n+        embedding = F.pad(embedding, (0, 1, 0, 1))\n+        emb_stem_2a = self.stem2a(embedding)\n+        emb_stem_2a = F.pad(emb_stem_2a, (0, 1, 0, 1))\n+        emb_stem_2a = self.stem2b(emb_stem_2a)\n+        pooled_emb = self.pool(embedding)\n+        embedding = torch.cat([pooled_emb, emb_stem_2a], dim=1)\n+        embedding = self.stem3(embedding)\n+        embedding = self.stem4(embedding)\n+        return embedding\n+\n+\n+class HGNetV2BasicLayer(nn.Module):\n+    def __init__(\n+        self,\n+        in_channels: int,\n+        middle_channels: int,\n+        out_channels: int,\n+        layer_num: int,\n+        kernel_size: int = 3,\n+        residual: bool = False,\n+        light_block: bool = False,\n+        drop_path: float = 0.0,\n+        use_learnable_affine_block: bool = False,\n+    ):\n+        super().__init__()\n+        self.residual = residual\n+\n+        self.layers = nn.ModuleList()\n+        for i in range(layer_num):\n+            temp_in_channels = in_channels if i == 0 else middle_channels\n+            if light_block:\n+                block = HGNetV2ConvLayerLight(\n+                    in_channels=temp_in_channels,\n+                    out_channels=middle_channels,\n+                    kernel_size=kernel_size,\n+                    use_learnable_affine_block=use_learnable_affine_block,\n+                )\n+            else:\n+                block = HGNetV2ConvLayer(\n+                    in_channels=temp_in_channels,\n+                    out_channels=middle_channels,\n+                    kernel_size=kernel_size,\n+                    use_learnable_affine_block=use_learnable_affine_block,\n+                    stride=1,\n+                )\n+            self.layers.append(block)\n+\n+        # feature aggregation\n+        total_channels = in_channels + layer_num * middle_channels\n+        aggregation_squeeze_conv = HGNetV2ConvLayer(\n+            total_channels,\n+            out_channels // 2,\n+            kernel_size=1,\n+            stride=1,\n+            use_learnable_affine_block=use_learnable_affine_block,\n+        )\n+        aggregation_excitation_conv = HGNetV2ConvLayer(\n+            out_channels // 2,\n+            out_channels,\n+            kernel_size=1,\n+            stride=1,\n+            use_learnable_affine_block=use_learnable_affine_block,\n+        )\n+        self.aggregation = nn.Sequential(\n+            aggregation_squeeze_conv,\n+            aggregation_excitation_conv,\n+        )\n+        self.drop_path = nn.Dropout(drop_path) if drop_path else nn.Identity()\n+\n+    def forward(self, hidden_state: Tensor) -> Tensor:\n+        identity = hidden_state\n+        output = [hidden_state]\n+        for layer in self.layers:\n+            hidden_state = layer(hidden_state)\n+            output.append(hidden_state)\n+        hidden_state = torch.cat(output, dim=1)\n+        hidden_state = self.aggregation(hidden_state)\n+        if self.residual:\n+            hidden_state = self.drop_path(hidden_state) + identity\n+        return hidden_state\n+\n+\n+class HGNetV2Stage(nn.Module):\n+    def __init__(self, config: HGNetV2Config, stage_index: int, drop_path: float = 0.0):\n+        super().__init__()\n+        in_channels = config.stage_in_channels[stage_index]\n+        mid_channels = config.stage_mid_channels[stage_index]\n+        out_channels = config.stage_out_channels[stage_index]\n+        num_blocks = config.stage_num_blocks[stage_index]\n+        num_layers = config.stage_numb_of_layers[stage_index]\n+        downsample = config.stage_downsample[stage_index]\n+        light_block = config.stage_light_block[stage_index]\n+        kernel_size = config.stage_kernel_size[stage_index]\n+        use_learnable_affine_block = config.use_learnable_affine_block\n+\n+        if downsample:\n+            self.downsample = HGNetV2ConvLayer(\n+                in_channels, in_channels, kernel_size=3, stride=2, groups=in_channels, activation=None\n+            )\n+        else:\n+            self.downsample = nn.Identity()\n+\n+        blocks_list = []\n+        for i in range(num_blocks):\n+            blocks_list.append(\n+                HGNetV2BasicLayer(\n+                    in_channels if i == 0 else out_channels,\n+                    mid_channels,\n+                    out_channels,\n+                    num_layers,\n+                    residual=False if i == 0 else True,\n+                    kernel_size=kernel_size,\n+                    light_block=light_block,\n+                    drop_path=drop_path,\n+                    use_learnable_affine_block=use_learnable_affine_block,\n+                )\n+            )\n+        self.blocks = nn.ModuleList(blocks_list)\n+\n+    def forward(self, hidden_state: Tensor) -> Tensor:\n+        hidden_state = self.downsample(hidden_state)\n+        for block in self.blocks:\n+            hidden_state = block(hidden_state)\n+        return hidden_state\n+\n+\n+class HGNetV2Encoder(nn.Module):\n+    def __init__(self, config: HGNetV2Config):\n+        super().__init__()\n+        self.stages = nn.ModuleList([])\n+        for stage_index in range(len(config.stage_in_channels)):\n+            resnet_stage = HGNetV2Stage(config, stage_index)\n+            self.stages.append(resnet_stage)\n+\n+    def forward(\n+        self, hidden_state: Tensor, output_hidden_states: bool = False, return_dict: bool = True\n+    ) -> BaseModelOutputWithNoAttention:\n+        hidden_states = () if output_hidden_states else None\n+\n+        for stage in self.stages:\n+            if output_hidden_states:\n+                hidden_states = hidden_states + (hidden_state,)\n+\n+            hidden_state = stage(hidden_state)\n+\n+        if output_hidden_states:\n+            hidden_states = hidden_states + (hidden_state,)\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_state, hidden_states] if v is not None)\n+\n+        return BaseModelOutputWithNoAttention(\n+            last_hidden_state=hidden_state,\n+            hidden_states=hidden_states,\n+        )\n+\n+\n+HGNet_V2_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n+            [`RTDetrImageProcessor.__call__`] for details.\n+\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+class HGNetV2Backbone(HGNetV2PreTrainedModel, BackboneMixin):\n+    def __init__(self, config: HGNetV2Config):\n+        super().__init__(config)\n+        super()._init_backbone(config)\n+        self.depths = config.depths\n+        self.num_features = [config.embedding_size] + config.hidden_sizes\n+        self.embedder = HGNetV2Embeddings(config)\n+        self.encoder = HGNetV2Encoder(config)\n+\n+        # initialize weights and apply final processing\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(HGNet_V2_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=BackboneOutput, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self, pixel_values: Tensor, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None\n+    ) -> BackboneOutput:\n+        \"\"\"\n+        Returns:\n+\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import RTDetrResNetConfig, RTDetrResNetBackbone\n+        >>> import torch\n+\n+        >>> config = RTDetrResNetConfig()\n+        >>> model = RTDetrResNetBackbone(config)\n+\n+        >>> pixel_values = torch.randn(1, 3, 224, 224)\n+\n+        >>> with torch.no_grad():\n+        ...     outputs = model(pixel_values)\n+\n+        >>> feature_maps = outputs.feature_maps\n+        >>> list(feature_maps[-1].shape)\n+        [1, 2048, 7, 7]\n+        ```\"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        embedding_output = self.embedder(pixel_values)\n+\n+        outputs = self.encoder(embedding_output, output_hidden_states=True, return_dict=True)\n+\n+        hidden_states = outputs.hidden_states\n+\n+        feature_maps = ()\n+        for idx, stage in enumerate(self.stage_names):\n+            if stage in self.out_features:\n+                feature_maps += (hidden_states[idx],)\n+\n+        if not return_dict:\n+            output = (feature_maps,)\n+            if output_hidden_states:\n+                output += (outputs.hidden_states,)\n+            return output\n+\n+        return BackboneOutput(\n+            feature_maps=feature_maps,\n+            hidden_states=outputs.hidden_states if output_hidden_states else None,\n+            attentions=None,\n+        )\n+\n+\n+HGNet_V2_START_DOCSTRING = r\"\"\"\n+    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n+    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n+    behavior.\n+\n+    Parameters:\n+        config ([`HGNetV2Config`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    HGNetV2 Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for\n+    ImageNet.\n+    \"\"\",\n+    HGNet_V2_START_DOCSTRING,\n+)\n+class HGNetV2ForImageClassification(HGNetV2PreTrainedModel):\n+    def __init__(self, config: HGNetV2Config):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+        self.embedder = HGNetV2Embeddings(config)\n+        self.encoder = HGNetV2Encoder(config)\n+        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n+        self.flatten = nn.Flatten()\n+        self.fc = nn.Linear(config.hidden_sizes[-1], config.num_labels) if config.num_labels > 0 else nn.Identity()\n+\n+        # classification head\n+        self.classifier = nn.ModuleList([self.avg_pool, self.flatten])\n+\n+        # initialize weights and apply final processing\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(HGNet_V2_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=ImageClassifierOutputWithNoAttention, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> ImageClassifierOutputWithNoAttention:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+\n+        Returns:\n+\n+        Examples:\n+        ```python\n+        >>> import torch\n+        >>> import requests\n+        >>> from transformers import HGNetV2ForImageClassification, AutoImageProcessor\n+        >>> from PIL import Image\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> model = HGNetV2ForImageClassification.from_pretrained(\"ustc-community/hgnet-v2\")\n+        >>> processor = AutoImageProcessor.from_pretrained(\"ustc-community/hgnet-v2\")\n+\n+        >>> inputs = processor(images=image, return_tensors=\"pt\")\n+        >>> with torch.no_grad():\n+        ...     outputs = model(**inputs)\n+        >>> outputs.logits.shape\n+        torch.Size([1, 2])\n+        ```\"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        embedding_output = self.embedder(pixel_values)\n+        outputs = self.encoder(embedding_output, output_hidden_states=output_hidden_states, return_dict=return_dict)\n+        last_hidden_state = outputs[0]\n+        for layer in self.classifier:\n+            last_hidden_state = layer(last_hidden_state)\n+        logits = self.fc(last_hidden_state)\n+        loss = None\n+\n+        if labels is not None:\n+            if self.config.problem_type is None:\n+                if self.num_labels == 1:\n+                    self.config.problem_type = \"regression\"\n+                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n+                    self.config.problem_type = \"single_label_classification\"\n+                else:\n+                    self.config.problem_type = \"multi_label_classification\"\n+            if self.config.problem_type == \"regression\":\n+                loss_fct = MSELoss()\n+                if self.num_labels == 1:\n+                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n+                else:\n+                    loss = loss_fct(logits, labels)\n+            elif self.config.problem_type == \"single_label_classification\":\n+                loss_fct = CrossEntropyLoss()\n+                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            elif self.config.problem_type == \"multi_label_classification\":\n+                loss_fct = BCEWithLogitsLoss()\n+                loss = loss_fct(logits, labels)\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[2:]\n+            return (loss,) + output if loss is not None else output\n+\n+        return ImageClassifierOutputWithNoAttention(loss=loss, logits=logits, hidden_states=outputs.hidden_states)\n+\n+\n+__all__ = [\"HGNetV2Backbone\", \"HGNetV2PreTrainedModel\", \"HGNetV2ForImageClassification\"]"
        },
        {
            "sha": "478aabbf7a022b2203aca710f276eb8819fccc1a",
            "filename": "src/transformers/models/hgnet_v2/modular_hgnet_v2.py",
            "status": "added",
            "additions": 661,
            "deletions": 0,
            "changes": 661,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py?ref=4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63",
            "patch": "@@ -0,0 +1,661 @@\n+# coding=utf-8\n+# Copyright 2025 Baidu Inc and The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from typing import Optional\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import Tensor, nn\n+from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_outputs import (\n+    BackboneOutput,\n+    BaseModelOutputWithNoAttention,\n+    ImageClassifierOutputWithNoAttention,\n+)\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, replace_return_docstrings\n+from ...utils.backbone_utils import BackboneConfigMixin, BackboneMixin, get_aligned_output_features_output_indices\n+from ..rt_detr.modeling_rt_detr_resnet import RTDetrResNetConvLayer\n+\n+\n+# TODO: Modular conversion for resnet must be fixed as\n+# it provides incorrect import for configuration like resnet_resnet\n+class HGNetV2Config(BackboneConfigMixin, PretrainedConfig):\n+    \"\"\"\n+    This is the configuration class to store the configuration of a [`HGNetV2Backbone`]. It is used to instantiate a HGNet-V2\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of D-FINE-X-COCO B4 \"[ustc-community/dfine_x_coco\"](https://huggingface.co/ustc-community/dfine_x_coco\").\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        num_channels (`int`, *optional*, defaults to 3):\n+            The number of input channels.\n+        embedding_size (`int`, *optional*, defaults to 64):\n+            Dimensionality (hidden size) for the embedding layer.\n+        depths (`List[int]`, *optional*, defaults to `[3, 4, 6, 3]`):\n+            Depth (number of layers) for each stage.\n+        hidden_sizes (`List[int]`, *optional*, defaults to `[256, 512, 1024, 2048]`):\n+            Dimensionality (hidden size) at each stage.\n+        hidden_act (`str`, *optional*, defaults to `\"relu\"`):\n+            The non-linear activation function in each block. If string, `\"gelu\"`, `\"relu\"`, `\"selu\"` and `\"gelu_new\"`\n+            are supported.\n+        out_features (`List[str]`, *optional*):\n+            If used as backbone, list of features to output. Can be any of `\"stem\"`, `\"stage1\"`, `\"stage2\"`, etc.\n+            (depending on how many stages the model has). If unset and `out_indices` is set, will default to the\n+            corresponding stages. If unset and `out_indices` is unset, will default to the last stage. Must be in the\n+            same order as defined in the `stage_names` attribute.\n+        out_indices (`List[int]`, *optional*):\n+            If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how\n+            many stages the model has). If unset and `out_features` is set, will default to the corresponding stages.\n+            If unset and `out_features` is unset, will default to the last stage. Must be in the\n+            same order as defined in the `stage_names` attribute.\n+        stem_channels (`List[int]`, *optional*, defaults to `[3, 32, 48]`):\n+            Channel dimensions for the stem layers:\n+            - First number (3) is input image channels\n+            - Second number (32) is intermediate stem channels\n+            - Third number (48) is output stem channels\n+        stage_in_channels (`List[int]`, *optional*, defaults to `[48, 128, 512, 1024]`):\n+            Input channel dimensions for each stage of the backbone.\n+            This defines how many channels the input to each stage will have.\n+        stage_mid_channels (`List[int]`, *optional*, defaults to `[48, 96, 192, 384]`):\n+            Mid-channel dimensions for each stage of the backbone.\n+            This defines the number of channels used in the intermediate layers of each stage.\n+        stage_out_channels (`List[int]`, *optional*, defaults to `[128, 512, 1024, 2048]`):\n+            Output channel dimensions for each stage of the backbone.\n+            This defines how many channels the output of each stage will have.\n+        stage_num_blocks (`List[int]`, *optional*, defaults to `[1, 1, 3, 1]`):\n+            Number of blocks to be used in each stage of the backbone.\n+            This controls the depth of each stage by specifying how many convolutional blocks to stack.\n+        stage_downsample (`List[bool]`, *optional*, defaults to `[False, True, True, True]`):\n+            Indicates whether to downsample the feature maps at each stage.\n+            If `True`, the spatial dimensions of the feature maps will be reduced.\n+        stage_light_block (`List[bool]`, *optional*, defaults to `[False, False, True, True]`):\n+            Indicates whether to use light blocks in each stage.\n+            Light blocks are a variant of convolutional blocks that may have fewer parameters.\n+        stage_kernel_size (`List[int]`, *optional*, defaults to `[3, 3, 5, 5]`):\n+            Kernel sizes for the convolutional layers in each stage.\n+        stage_numb_of_layers (`List[int]`, *optional*, defaults to `[6, 6, 6, 6]`):\n+            Number of layers to be used in each block of the stage.\n+        use_learnable_affine_block (`bool`, *optional*, defaults to `False`):\n+            Whether to use Learnable Affine Blocks (LAB) in the network.\n+            LAB adds learnable scale and bias parameters after certain operations.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+    \"\"\"\n+\n+    model_type = \"hgnet_v2\"\n+\n+    def __init__(\n+        self,\n+        num_channels=3,\n+        embedding_size=64,\n+        depths=[3, 4, 6, 3],\n+        hidden_sizes=[256, 512, 1024, 2048],\n+        hidden_act=\"relu\",\n+        out_features=None,\n+        out_indices=None,\n+        stem_channels=[3, 32, 48],\n+        stage_in_channels=[48, 128, 512, 1024],\n+        stage_mid_channels=[48, 96, 192, 384],\n+        stage_out_channels=[128, 512, 1024, 2048],\n+        stage_num_blocks=[1, 1, 3, 1],\n+        stage_downsample=[False, True, True, True],\n+        stage_light_block=[False, False, True, True],\n+        stage_kernel_size=[3, 3, 5, 5],\n+        stage_numb_of_layers=[6, 6, 6, 6],\n+        use_learnable_affine_block=False,\n+        initializer_range=0.02,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.num_channels = num_channels\n+        self.embedding_size = embedding_size\n+        self.depths = depths\n+        self.hidden_sizes = hidden_sizes\n+        self.hidden_act = hidden_act\n+        self.stage_names = [\"stem\"] + [f\"stage{idx}\" for idx in range(1, len(depths) + 1)]\n+        self._out_features, self._out_indices = get_aligned_output_features_output_indices(\n+            out_features=out_features, out_indices=out_indices, stage_names=self.stage_names\n+        )\n+        self.stem_channels = stem_channels\n+        self.stage_in_channels = stage_in_channels\n+        self.stage_mid_channels = stage_mid_channels\n+        self.stage_out_channels = stage_out_channels\n+        self.stage_num_blocks = stage_num_blocks\n+        self.stage_downsample = stage_downsample\n+        self.stage_light_block = stage_light_block\n+        self.stage_kernel_size = stage_kernel_size\n+        self.stage_numb_of_layers = stage_numb_of_layers\n+        self.use_learnable_affine_block = use_learnable_affine_block\n+        self.initializer_range = initializer_range\n+\n+        if not (\n+            len(stage_in_channels)\n+            == len(stage_mid_channels)\n+            == len(stage_out_channels)\n+            == len(stage_num_blocks)\n+            == len(stage_downsample)\n+            == len(stage_light_block)\n+            == len(stage_kernel_size)\n+            == len(stage_numb_of_layers)\n+        ):\n+            raise ValueError(\"All stage configuration lists must have the same length.\")\n+\n+\n+# General docstring\n+_CONFIG_FOR_DOC = \"HGNetV2Config\"\n+\n+\n+class HGNetV2PreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = HGNetV2Config\n+    base_model_prefix = \"hgnetv2\"\n+    main_input_name = \"pixel_values\"\n+    _no_split_modules = [\"HGNetV2BasicLayer\"]\n+\n+    def _init_weights(self, module):\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.BatchNorm2d):\n+            module.weight.data.fill_(1.0)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+\n+\n+class HGNetV2LearnableAffineBlock(nn.Module):\n+    def __init__(self, scale_value: float = 1.0, bias_value: float = 0.0):\n+        super().__init__()\n+        self.scale = nn.Parameter(torch.tensor([scale_value]), requires_grad=True)\n+        self.bias = nn.Parameter(torch.tensor([bias_value]), requires_grad=True)\n+\n+    def forward(self, hidden_state: Tensor) -> Tensor:\n+        hidden_state = self.scale * hidden_state + self.bias\n+        return hidden_state\n+\n+\n+class HGNetV2ConvLayer(RTDetrResNetConvLayer):\n+    def __init__(\n+        self,\n+        in_channels: int,\n+        out_channels: int,\n+        kernel_size: int,\n+        stride: int = 1,\n+        groups: int = 1,\n+        activation: str = \"relu\",\n+        use_learnable_affine_block: bool = False,\n+    ):\n+        super().__init__(in_channels, out_channels, kernel_size, stride, activation)\n+        self.convolution = nn.Conv2d(\n+            in_channels,\n+            out_channels,\n+            kernel_size=kernel_size,\n+            stride=stride,\n+            groups=groups,\n+            padding=(kernel_size - 1) // 2,\n+            bias=False,\n+        )\n+        if activation and use_learnable_affine_block:\n+            self.lab = HGNetV2LearnableAffineBlock()\n+        else:\n+            self.lab = nn.Identity()\n+\n+    def forward(self, input: Tensor) -> Tensor:\n+        hidden_state = self.convolution(input)\n+        hidden_state = self.normalization(hidden_state)\n+        hidden_state = self.activation(hidden_state)\n+        hidden_state = self.lab(hidden_state)\n+        return hidden_state\n+\n+\n+class HGNetV2ConvLayerLight(nn.Module):\n+    def __init__(\n+        self, in_channels: int, out_channels: int, kernel_size: int, use_learnable_affine_block: bool = False\n+    ):\n+        super().__init__()\n+        self.conv1 = HGNetV2ConvLayer(\n+            in_channels,\n+            out_channels,\n+            kernel_size=1,\n+            activation=None,\n+            use_learnable_affine_block=use_learnable_affine_block,\n+        )\n+        self.conv2 = HGNetV2ConvLayer(\n+            out_channels,\n+            out_channels,\n+            kernel_size=kernel_size,\n+            groups=out_channels,\n+            use_learnable_affine_block=use_learnable_affine_block,\n+        )\n+\n+    def forward(self, hidden_state: Tensor) -> Tensor:\n+        hidden_state = self.conv1(hidden_state)\n+        hidden_state = self.conv2(hidden_state)\n+        return hidden_state\n+\n+\n+class HGNetV2Embeddings(nn.Module):\n+    def __init__(self, config: HGNetV2Config):\n+        super().__init__()\n+\n+        self.stem1 = HGNetV2ConvLayer(\n+            config.stem_channels[0],\n+            config.stem_channels[1],\n+            kernel_size=3,\n+            stride=2,\n+            activation=config.hidden_act,\n+            use_learnable_affine_block=config.use_learnable_affine_block,\n+        )\n+        self.stem2a = HGNetV2ConvLayer(\n+            config.stem_channels[1],\n+            config.stem_channels[1] // 2,\n+            kernel_size=2,\n+            stride=1,\n+            activation=config.hidden_act,\n+            use_learnable_affine_block=config.use_learnable_affine_block,\n+        )\n+        self.stem2b = HGNetV2ConvLayer(\n+            config.stem_channels[1] // 2,\n+            config.stem_channels[1],\n+            kernel_size=2,\n+            stride=1,\n+            activation=config.hidden_act,\n+            use_learnable_affine_block=config.use_learnable_affine_block,\n+        )\n+        self.stem3 = HGNetV2ConvLayer(\n+            config.stem_channels[1] * 2,\n+            config.stem_channels[1],\n+            kernel_size=3,\n+            stride=2,\n+            activation=config.hidden_act,\n+            use_learnable_affine_block=config.use_learnable_affine_block,\n+        )\n+        self.stem4 = HGNetV2ConvLayer(\n+            config.stem_channels[1],\n+            config.stem_channels[2],\n+            kernel_size=1,\n+            stride=1,\n+            activation=config.hidden_act,\n+            use_learnable_affine_block=config.use_learnable_affine_block,\n+        )\n+\n+        self.pool = nn.MaxPool2d(kernel_size=2, stride=1, ceil_mode=True)\n+        self.num_channels = config.num_channels\n+\n+    def forward(self, pixel_values: Tensor) -> Tensor:\n+        num_channels = pixel_values.shape[1]\n+        if num_channels != self.num_channels:\n+            raise ValueError(\n+                \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n+            )\n+        embedding = self.stem1(pixel_values)\n+        embedding = F.pad(embedding, (0, 1, 0, 1))\n+        emb_stem_2a = self.stem2a(embedding)\n+        emb_stem_2a = F.pad(emb_stem_2a, (0, 1, 0, 1))\n+        emb_stem_2a = self.stem2b(emb_stem_2a)\n+        pooled_emb = self.pool(embedding)\n+        embedding = torch.cat([pooled_emb, emb_stem_2a], dim=1)\n+        embedding = self.stem3(embedding)\n+        embedding = self.stem4(embedding)\n+        return embedding\n+\n+\n+class HGNetV2BasicLayer(nn.Module):\n+    def __init__(\n+        self,\n+        in_channels: int,\n+        middle_channels: int,\n+        out_channels: int,\n+        layer_num: int,\n+        kernel_size: int = 3,\n+        residual: bool = False,\n+        light_block: bool = False,\n+        drop_path: float = 0.0,\n+        use_learnable_affine_block: bool = False,\n+    ):\n+        super().__init__()\n+        self.residual = residual\n+\n+        self.layers = nn.ModuleList()\n+        for i in range(layer_num):\n+            temp_in_channels = in_channels if i == 0 else middle_channels\n+            if light_block:\n+                block = HGNetV2ConvLayerLight(\n+                    in_channels=temp_in_channels,\n+                    out_channels=middle_channels,\n+                    kernel_size=kernel_size,\n+                    use_learnable_affine_block=use_learnable_affine_block,\n+                )\n+            else:\n+                block = HGNetV2ConvLayer(\n+                    in_channels=temp_in_channels,\n+                    out_channels=middle_channels,\n+                    kernel_size=kernel_size,\n+                    use_learnable_affine_block=use_learnable_affine_block,\n+                    stride=1,\n+                )\n+            self.layers.append(block)\n+\n+        # feature aggregation\n+        total_channels = in_channels + layer_num * middle_channels\n+        aggregation_squeeze_conv = HGNetV2ConvLayer(\n+            total_channels,\n+            out_channels // 2,\n+            kernel_size=1,\n+            stride=1,\n+            use_learnable_affine_block=use_learnable_affine_block,\n+        )\n+        aggregation_excitation_conv = HGNetV2ConvLayer(\n+            out_channels // 2,\n+            out_channels,\n+            kernel_size=1,\n+            stride=1,\n+            use_learnable_affine_block=use_learnable_affine_block,\n+        )\n+        self.aggregation = nn.Sequential(\n+            aggregation_squeeze_conv,\n+            aggregation_excitation_conv,\n+        )\n+        self.drop_path = nn.Dropout(drop_path) if drop_path else nn.Identity()\n+\n+    def forward(self, hidden_state: Tensor) -> Tensor:\n+        identity = hidden_state\n+        output = [hidden_state]\n+        for layer in self.layers:\n+            hidden_state = layer(hidden_state)\n+            output.append(hidden_state)\n+        hidden_state = torch.cat(output, dim=1)\n+        hidden_state = self.aggregation(hidden_state)\n+        if self.residual:\n+            hidden_state = self.drop_path(hidden_state) + identity\n+        return hidden_state\n+\n+\n+class HGNetV2Stage(nn.Module):\n+    def __init__(self, config: HGNetV2Config, stage_index: int, drop_path: float = 0.0):\n+        super().__init__()\n+        in_channels = config.stage_in_channels[stage_index]\n+        mid_channels = config.stage_mid_channels[stage_index]\n+        out_channels = config.stage_out_channels[stage_index]\n+        num_blocks = config.stage_num_blocks[stage_index]\n+        num_layers = config.stage_numb_of_layers[stage_index]\n+        downsample = config.stage_downsample[stage_index]\n+        light_block = config.stage_light_block[stage_index]\n+        kernel_size = config.stage_kernel_size[stage_index]\n+        use_learnable_affine_block = config.use_learnable_affine_block\n+\n+        if downsample:\n+            self.downsample = HGNetV2ConvLayer(\n+                in_channels, in_channels, kernel_size=3, stride=2, groups=in_channels, activation=None\n+            )\n+        else:\n+            self.downsample = nn.Identity()\n+\n+        blocks_list = []\n+        for i in range(num_blocks):\n+            blocks_list.append(\n+                HGNetV2BasicLayer(\n+                    in_channels if i == 0 else out_channels,\n+                    mid_channels,\n+                    out_channels,\n+                    num_layers,\n+                    residual=False if i == 0 else True,\n+                    kernel_size=kernel_size,\n+                    light_block=light_block,\n+                    drop_path=drop_path,\n+                    use_learnable_affine_block=use_learnable_affine_block,\n+                )\n+            )\n+        self.blocks = nn.ModuleList(blocks_list)\n+\n+    def forward(self, hidden_state: Tensor) -> Tensor:\n+        hidden_state = self.downsample(hidden_state)\n+        for block in self.blocks:\n+            hidden_state = block(hidden_state)\n+        return hidden_state\n+\n+\n+class HGNetV2Encoder(nn.Module):\n+    def __init__(self, config: HGNetV2Config):\n+        super().__init__()\n+        self.stages = nn.ModuleList([])\n+        for stage_index in range(len(config.stage_in_channels)):\n+            resnet_stage = HGNetV2Stage(config, stage_index)\n+            self.stages.append(resnet_stage)\n+\n+    def forward(\n+        self, hidden_state: Tensor, output_hidden_states: bool = False, return_dict: bool = True\n+    ) -> BaseModelOutputWithNoAttention:\n+        hidden_states = () if output_hidden_states else None\n+\n+        for stage in self.stages:\n+            if output_hidden_states:\n+                hidden_states = hidden_states + (hidden_state,)\n+\n+            hidden_state = stage(hidden_state)\n+\n+        if output_hidden_states:\n+            hidden_states = hidden_states + (hidden_state,)\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_state, hidden_states] if v is not None)\n+\n+        return BaseModelOutputWithNoAttention(\n+            last_hidden_state=hidden_state,\n+            hidden_states=hidden_states,\n+        )\n+\n+\n+HGNet_V2_START_DOCSTRING = r\"\"\"\n+    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n+    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n+    behavior.\n+\n+    Parameters:\n+        config ([`HGNetV2Config`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+HGNet_V2_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n+            [`RTDetrImageProcessor.__call__`] for details.\n+\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+class HGNetV2Backbone(HGNetV2PreTrainedModel, BackboneMixin):\n+    def __init__(self, config: HGNetV2Config):\n+        super().__init__(config)\n+        super()._init_backbone(config)\n+        self.depths = config.depths\n+        self.num_features = [config.embedding_size] + config.hidden_sizes\n+        self.embedder = HGNetV2Embeddings(config)\n+        self.encoder = HGNetV2Encoder(config)\n+\n+        # initialize weights and apply final processing\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(HGNet_V2_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=BackboneOutput, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self, pixel_values: Tensor, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None\n+    ) -> BackboneOutput:\n+        \"\"\"\n+        Returns:\n+\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import RTDetrResNetConfig, RTDetrResNetBackbone\n+        >>> import torch\n+\n+        >>> config = RTDetrResNetConfig()\n+        >>> model = RTDetrResNetBackbone(config)\n+\n+        >>> pixel_values = torch.randn(1, 3, 224, 224)\n+\n+        >>> with torch.no_grad():\n+        ...     outputs = model(pixel_values)\n+\n+        >>> feature_maps = outputs.feature_maps\n+        >>> list(feature_maps[-1].shape)\n+        [1, 2048, 7, 7]\n+        ```\"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        embedding_output = self.embedder(pixel_values)\n+\n+        outputs = self.encoder(embedding_output, output_hidden_states=True, return_dict=True)\n+\n+        hidden_states = outputs.hidden_states\n+\n+        feature_maps = ()\n+        for idx, stage in enumerate(self.stage_names):\n+            if stage in self.out_features:\n+                feature_maps += (hidden_states[idx],)\n+\n+        if not return_dict:\n+            output = (feature_maps,)\n+            if output_hidden_states:\n+                output += (outputs.hidden_states,)\n+            return output\n+\n+        return BackboneOutput(\n+            feature_maps=feature_maps,\n+            hidden_states=outputs.hidden_states if output_hidden_states else None,\n+            attentions=None,\n+        )\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    HGNetV2 Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for\n+    ImageNet.\n+    \"\"\",\n+    HGNet_V2_START_DOCSTRING,\n+)\n+class HGNetV2ForImageClassification(HGNetV2PreTrainedModel):\n+    def __init__(self, config: HGNetV2Config):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+        self.embedder = HGNetV2Embeddings(config)\n+        self.encoder = HGNetV2Encoder(config)\n+        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n+        self.flatten = nn.Flatten()\n+        self.fc = nn.Linear(config.hidden_sizes[-1], config.num_labels) if config.num_labels > 0 else nn.Identity()\n+\n+        # classification head\n+        self.classifier = nn.ModuleList([self.avg_pool, self.flatten])\n+\n+        # initialize weights and apply final processing\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(HGNet_V2_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=ImageClassifierOutputWithNoAttention, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> ImageClassifierOutputWithNoAttention:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+\n+        Returns:\n+\n+        Examples:\n+        ```python\n+        >>> import torch\n+        >>> import requests\n+        >>> from transformers import HGNetV2ForImageClassification, AutoImageProcessor\n+        >>> from PIL import Image\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> model = HGNetV2ForImageClassification.from_pretrained(\"ustc-community/hgnet-v2\")\n+        >>> processor = AutoImageProcessor.from_pretrained(\"ustc-community/hgnet-v2\")\n+\n+        >>> inputs = processor(images=image, return_tensors=\"pt\")\n+        >>> with torch.no_grad():\n+        ...     outputs = model(**inputs)\n+        >>> outputs.logits.shape\n+        torch.Size([1, 2])\n+        ```\"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        embedding_output = self.embedder(pixel_values)\n+        outputs = self.encoder(embedding_output, output_hidden_states=output_hidden_states, return_dict=return_dict)\n+        last_hidden_state = outputs[0]\n+        for layer in self.classifier:\n+            last_hidden_state = layer(last_hidden_state)\n+        logits = self.fc(last_hidden_state)\n+        loss = None\n+\n+        if labels is not None:\n+            if self.config.problem_type is None:\n+                if self.num_labels == 1:\n+                    self.config.problem_type = \"regression\"\n+                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n+                    self.config.problem_type = \"single_label_classification\"\n+                else:\n+                    self.config.problem_type = \"multi_label_classification\"\n+            if self.config.problem_type == \"regression\":\n+                loss_fct = MSELoss()\n+                if self.num_labels == 1:\n+                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n+                else:\n+                    loss = loss_fct(logits, labels)\n+            elif self.config.problem_type == \"single_label_classification\":\n+                loss_fct = CrossEntropyLoss()\n+                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            elif self.config.problem_type == \"multi_label_classification\":\n+                loss_fct = BCEWithLogitsLoss()\n+                loss = loss_fct(logits, labels)\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[2:]\n+            return (loss,) + output if loss is not None else output\n+\n+        return ImageClassifierOutputWithNoAttention(loss=loss, logits=logits, hidden_states=outputs.hidden_states)\n+\n+\n+__all__ = [\"HGNetV2Config\", \"HGNetV2Backbone\", \"HGNetV2PreTrainedModel\", \"HGNetV2ForImageClassification\"]"
        },
        {
            "sha": "d3dda3c12b6567ba98c52f02b5f321a4b74a388b",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr.py",
            "status": "modified",
            "additions": 24,
            "deletions": 2,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py?ref=4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63",
            "patch": "@@ -123,6 +123,10 @@ class RTDetrDecoderOutput(ModelOutput):\n             Stacked intermediate logits (logits of each layer of the decoder).\n         intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, hidden_size)`):\n             Stacked intermediate reference points (reference points of each layer of the decoder).\n+        intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+            Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n+        initial_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+            Stacked initial reference points (initial reference points of each layer of the decoder).\n         hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n             Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n             shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n@@ -141,6 +145,8 @@ class RTDetrDecoderOutput(ModelOutput):\n     intermediate_hidden_states: Optional[torch.FloatTensor] = None\n     intermediate_logits: Optional[torch.FloatTensor] = None\n     intermediate_reference_points: Optional[torch.FloatTensor] = None\n+    intermediate_predicted_corners: Optional[torch.FloatTensor] = None\n+    initial_reference_points: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n     cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -204,6 +210,8 @@ class RTDetrModelOutput(ModelOutput):\n     intermediate_hidden_states: Optional[torch.FloatTensor] = None\n     intermediate_logits: Optional[torch.FloatTensor] = None\n     intermediate_reference_points: Optional[torch.FloatTensor] = None\n+    intermediate_predicted_corners: Optional[torch.FloatTensor] = None\n+    initial_reference_points: Optional[torch.FloatTensor] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n     cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -249,6 +257,10 @@ class RTDetrObjectDetectionOutput(ModelOutput):\n             Stacked intermediate logits (logits of each layer of the decoder).\n         intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n             Stacked intermediate reference points (reference points of each layer of the decoder).\n+        intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+            Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n+        initial_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+            Stacked initial reference points (initial reference points of each layer of the decoder).\n         decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n             Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n             shape `(batch_size, num_queries, hidden_size)`. Hidden-states of the decoder at the output of each layer\n@@ -296,6 +308,8 @@ class RTDetrObjectDetectionOutput(ModelOutput):\n     intermediate_hidden_states: Optional[torch.FloatTensor] = None\n     intermediate_logits: Optional[torch.FloatTensor] = None\n     intermediate_reference_points: Optional[torch.FloatTensor] = None\n+    intermediate_predicted_corners: Optional[torch.FloatTensor] = None\n+    initial_reference_points: Optional[torch.FloatTensor] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n     cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -1468,8 +1482,8 @@ def forward(\n \n             # hack implementation for iterative bounding box refinement\n             if self.bbox_embed is not None:\n-                tmp = self.bbox_embed[idx](hidden_states)\n-                new_reference_points = F.sigmoid(tmp + inverse_sigmoid(reference_points))\n+                predicted_corners = self.bbox_embed[idx](hidden_states)\n+                new_reference_points = F.sigmoid(predicted_corners + inverse_sigmoid(reference_points))\n                 reference_points = new_reference_points.detach()\n \n             intermediate += (hidden_states,)\n@@ -1865,6 +1879,8 @@ def forward(\n             intermediate_hidden_states=decoder_outputs.intermediate_hidden_states,\n             intermediate_logits=decoder_outputs.intermediate_logits,\n             intermediate_reference_points=decoder_outputs.intermediate_reference_points,\n+            intermediate_predicted_corners=decoder_outputs.intermediate_predicted_corners,\n+            initial_reference_points=decoder_outputs.initial_reference_points,\n             decoder_hidden_states=decoder_outputs.hidden_states,\n             decoder_attentions=decoder_outputs.attentions,\n             cross_attentions=decoder_outputs.cross_attentions,\n@@ -2020,6 +2036,8 @@ def forward(\n \n         outputs_class = outputs.intermediate_logits if return_dict else outputs[2]\n         outputs_coord = outputs.intermediate_reference_points if return_dict else outputs[3]\n+        predicted_corners = outputs.intermediate_predicted_corners if return_dict else outputs[4]\n+        initial_reference_points = outputs.initial_reference_points if return_dict else outputs[5]\n \n         logits = outputs_class[:, -1]\n         pred_boxes = outputs_coord[:, -1]\n@@ -2039,6 +2057,8 @@ def forward(\n                 enc_topk_logits=enc_topk_logits,\n                 enc_topk_bboxes=enc_topk_bboxes,\n                 denoising_meta_values=denoising_meta_values,\n+                predicted_corners=predicted_corners,\n+                initial_reference_points=initial_reference_points,\n                 **loss_kwargs,\n             )\n \n@@ -2059,6 +2079,8 @@ def forward(\n             intermediate_hidden_states=outputs.intermediate_hidden_states,\n             intermediate_logits=outputs.intermediate_logits,\n             intermediate_reference_points=outputs.intermediate_reference_points,\n+            intermediate_predicted_corners=outputs.intermediate_predicted_corners,\n+            initial_reference_points=outputs.initial_reference_points,\n             decoder_hidden_states=outputs.decoder_hidden_states,\n             decoder_attentions=outputs.decoder_attentions,\n             cross_attentions=outputs.cross_attentions,"
        },
        {
            "sha": "ab47e5be689d3fb1ae78baed781f40180b62bd80",
            "filename": "src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 24,
            "deletions": 2,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py?ref=4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63",
            "patch": "@@ -472,6 +472,10 @@ class RTDetrV2DecoderOutput(ModelOutput):\n             Stacked intermediate logits (logits of each layer of the decoder).\n         intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, hidden_size)`):\n             Stacked intermediate reference points (reference points of each layer of the decoder).\n+        intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+            Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n+        initial_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+            Stacked initial reference points (initial reference points of each layer of the decoder).\n         hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n             Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n             shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n@@ -490,6 +494,8 @@ class RTDetrV2DecoderOutput(ModelOutput):\n     intermediate_hidden_states: Optional[torch.FloatTensor] = None\n     intermediate_logits: Optional[torch.FloatTensor] = None\n     intermediate_reference_points: Optional[torch.FloatTensor] = None\n+    intermediate_predicted_corners: Optional[torch.FloatTensor] = None\n+    initial_reference_points: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n     cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -553,6 +559,8 @@ class RTDetrV2ModelOutput(ModelOutput):\n     intermediate_hidden_states: Optional[torch.FloatTensor] = None\n     intermediate_logits: Optional[torch.FloatTensor] = None\n     intermediate_reference_points: Optional[torch.FloatTensor] = None\n+    intermediate_predicted_corners: Optional[torch.FloatTensor] = None\n+    initial_reference_points: Optional[torch.FloatTensor] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n     cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -598,6 +606,10 @@ class RTDetrV2ObjectDetectionOutput(ModelOutput):\n             Stacked intermediate logits (logits of each layer of the decoder).\n         intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n             Stacked intermediate reference points (reference points of each layer of the decoder).\n+        intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+            Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n+        initial_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+            Stacked initial reference points (initial reference points of each layer of the decoder).\n         decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n             Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n             shape `(batch_size, num_queries, hidden_size)`. Hidden-states of the decoder at the output of each layer\n@@ -645,6 +657,8 @@ class RTDetrV2ObjectDetectionOutput(ModelOutput):\n     intermediate_hidden_states: Optional[torch.FloatTensor] = None\n     intermediate_logits: Optional[torch.FloatTensor] = None\n     intermediate_reference_points: Optional[torch.FloatTensor] = None\n+    intermediate_predicted_corners: Optional[torch.FloatTensor] = None\n+    initial_reference_points: Optional[torch.FloatTensor] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n     cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -1476,8 +1490,8 @@ def forward(\n \n             # hack implementation for iterative bounding box refinement\n             if self.bbox_embed is not None:\n-                tmp = self.bbox_embed[idx](hidden_states)\n-                new_reference_points = F.sigmoid(tmp + inverse_sigmoid(reference_points))\n+                predicted_corners = self.bbox_embed[idx](hidden_states)\n+                new_reference_points = F.sigmoid(predicted_corners + inverse_sigmoid(reference_points))\n                 reference_points = new_reference_points.detach()\n \n             intermediate += (hidden_states,)\n@@ -1849,6 +1863,8 @@ def forward(\n             intermediate_hidden_states=decoder_outputs.intermediate_hidden_states,\n             intermediate_logits=decoder_outputs.intermediate_logits,\n             intermediate_reference_points=decoder_outputs.intermediate_reference_points,\n+            intermediate_predicted_corners=decoder_outputs.intermediate_predicted_corners,\n+            initial_reference_points=decoder_outputs.initial_reference_points,\n             decoder_hidden_states=decoder_outputs.hidden_states,\n             decoder_attentions=decoder_outputs.attentions,\n             cross_attentions=decoder_outputs.cross_attentions,\n@@ -2019,6 +2035,8 @@ def forward(\n \n         outputs_class = outputs.intermediate_logits if return_dict else outputs[2]\n         outputs_coord = outputs.intermediate_reference_points if return_dict else outputs[3]\n+        predicted_corners = outputs.intermediate_predicted_corners if return_dict else outputs[4]\n+        initial_reference_points = outputs.initial_reference_points if return_dict else outputs[5]\n \n         logits = outputs_class[:, -1]\n         pred_boxes = outputs_coord[:, -1]\n@@ -2038,6 +2056,8 @@ def forward(\n                 enc_topk_logits=enc_topk_logits,\n                 enc_topk_bboxes=enc_topk_bboxes,\n                 denoising_meta_values=denoising_meta_values,\n+                predicted_corners=predicted_corners,\n+                initial_reference_points=initial_reference_points,\n                 **loss_kwargs,\n             )\n \n@@ -2058,6 +2078,8 @@ def forward(\n             intermediate_hidden_states=outputs.intermediate_hidden_states,\n             intermediate_logits=outputs.intermediate_logits,\n             intermediate_reference_points=outputs.intermediate_reference_points,\n+            intermediate_predicted_corners=outputs.intermediate_predicted_corners,\n+            initial_reference_points=outputs.initial_reference_points,\n             decoder_hidden_states=outputs.decoder_hidden_states,\n             decoder_attentions=outputs.decoder_attentions,\n             cross_attentions=outputs.cross_attentions,"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/d_fine/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/tests%2Fmodels%2Fd_fine%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/tests%2Fmodels%2Fd_fine%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fd_fine%2F__init__.py?ref=4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63"
        },
        {
            "sha": "3590ed10355222ca1513d41dc4e429336b5ce814",
            "filename": "tests/models/d_fine/test_modeling_d_fine.py",
            "status": "added",
            "additions": 810,
            "deletions": 0,
            "changes": 810,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py?ref=4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63",
            "patch": "@@ -0,0 +1,810 @@\n+# coding = utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch D-FINE model.\"\"\"\n+\n+import inspect\n+import math\n+import tempfile\n+import unittest\n+\n+from parameterized import parameterized\n+\n+from transformers import (\n+    DFineConfig,\n+    HGNetV2Config,\n+    is_torch_available,\n+    is_vision_available,\n+)\n+from transformers.testing_utils import require_torch, require_torch_gpu, require_vision, slow, torch_device\n+from transformers.utils import cached_property\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import DFineForObjectDetection, DFineModel\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+from transformers import RTDetrImageProcessor\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+CHECKPOINT = \"ustc-community/dfine_s_coco\"\n+\n+\n+class DFineModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=3,\n+        is_training=True,\n+        use_labels=True,\n+        n_targets=3,\n+        num_labels=10,\n+        initializer_range=0.02,\n+        layer_norm_eps=1e-5,\n+        batch_norm_eps=1e-5,\n+        # backbone\n+        backbone_config=None,\n+        # encoder HybridEncoder\n+        encoder_hidden_dim=32,\n+        encoder_in_channels=[128, 256, 512],\n+        feat_strides=[8, 16, 32],\n+        encoder_layers=1,\n+        encoder_ffn_dim=64,\n+        encoder_attention_heads=2,\n+        dropout=0.0,\n+        activation_dropout=0.0,\n+        encode_proj_layers=[2],\n+        positional_encoding_temperature=10000,\n+        encoder_activation_function=\"gelu\",\n+        activation_function=\"silu\",\n+        eval_size=None,\n+        normalize_before=False,\n+        # decoder DFineTransformer\n+        d_model=32,\n+        num_queries=30,\n+        decoder_in_channels=[32, 32, 32],\n+        decoder_ffn_dim=64,\n+        num_feature_levels=3,\n+        decoder_n_points=[3, 6, 3],\n+        decoder_n_levels=3,\n+        decoder_layers=2,\n+        decoder_attention_heads=2,\n+        decoder_activation_function=\"relu\",\n+        attention_dropout=0.0,\n+        num_denoising=0,\n+        label_noise_ratio=0.5,\n+        box_noise_scale=1.0,\n+        learn_initial_query=False,\n+        anchor_image_size=None,\n+        image_size=64,\n+        disable_custom_kernels=True,\n+        with_box_refine=True,\n+        decoder_offset_scale=0.5,\n+        eval_idx=-1,\n+        layer_scale=1,\n+        reg_max=32,\n+        reg_scale=4.0,\n+        depth_mult=0.34,\n+        hidden_expansion=0.5,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = 3\n+        self.is_training = is_training\n+        self.use_labels = use_labels\n+        self.n_targets = n_targets\n+        self.num_labels = num_labels\n+        self.initializer_range = initializer_range\n+        self.layer_norm_eps = layer_norm_eps\n+        self.batch_norm_eps = batch_norm_eps\n+        self.backbone_config = backbone_config\n+        self.encoder_hidden_dim = encoder_hidden_dim\n+        self.encoder_in_channels = encoder_in_channels\n+        self.feat_strides = feat_strides\n+        self.encoder_layers = encoder_layers\n+        self.encoder_ffn_dim = encoder_ffn_dim\n+        self.encoder_attention_heads = encoder_attention_heads\n+        self.dropout = dropout\n+        self.activation_dropout = activation_dropout\n+        self.encode_proj_layers = encode_proj_layers\n+        self.positional_encoding_temperature = positional_encoding_temperature\n+        self.encoder_activation_function = encoder_activation_function\n+        self.activation_function = activation_function\n+        self.eval_size = eval_size\n+        self.normalize_before = normalize_before\n+        self.d_model = d_model\n+        self.num_queries = num_queries\n+        self.decoder_in_channels = decoder_in_channels\n+        self.decoder_ffn_dim = decoder_ffn_dim\n+        self.num_feature_levels = num_feature_levels\n+        self.decoder_n_points = decoder_n_points\n+        self.decoder_n_levels = decoder_n_levels\n+        self.decoder_layers = decoder_layers\n+        self.decoder_attention_heads = decoder_attention_heads\n+        self.decoder_activation_function = decoder_activation_function\n+        self.attention_dropout = attention_dropout\n+        self.decoder_offset_scale = decoder_offset_scale\n+        self.eval_idx = eval_idx\n+        self.layer_scale = layer_scale\n+        self.reg_max = reg_max\n+        self.reg_scale = reg_scale\n+        self.depth_mult = depth_mult\n+        self.num_denoising = num_denoising\n+        self.label_noise_ratio = label_noise_ratio\n+        self.box_noise_scale = box_noise_scale\n+        self.learn_initial_query = learn_initial_query\n+        self.anchor_image_size = anchor_image_size\n+        self.image_size = image_size\n+        self.disable_custom_kernels = disable_custom_kernels\n+        self.with_box_refine = with_box_refine\n+        self.hidden_expansion = hidden_expansion\n+\n+        self.encoder_seq_length = math.ceil(self.image_size / 32) * math.ceil(self.image_size / 32)\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+\n+        pixel_mask = torch.ones([self.batch_size, self.image_size, self.image_size], device=torch_device)\n+\n+        labels = None\n+        if self.use_labels:\n+            # labels is a list of Dict (each Dict being the labels for a given example in the batch)\n+            labels = []\n+            for i in range(self.batch_size):\n+                target = {}\n+                target[\"class_labels\"] = torch.randint(\n+                    high=self.num_labels, size=(self.n_targets,), device=torch_device\n+                )\n+                target[\"boxes\"] = torch.rand(self.n_targets, 4, device=torch_device)\n+                labels.append(target)\n+\n+        config = self.get_config()\n+        config.num_labels = self.num_labels\n+        return config, pixel_values, pixel_mask, labels\n+\n+    def get_config(self):\n+        hidden_sizes = [64, 128, 256, 512]\n+        backbone_config = HGNetV2Config(\n+            stage_in_channels=[16, 64, 128, 256],\n+            stage_mid_channels=[16, 32, 64, 128],\n+            stage_out_channels=[64, 128, 256, 512],\n+            stage_num_blocks=[1, 1, 2, 1],\n+            stage_downsample=[False, True, True, True],\n+            stage_light_block=[False, False, True, True],\n+            stage_kernel_size=[3, 3, 5, 5],\n+            stage_numb_of_layers=[3, 3, 3, 3],\n+            embeddings_size=10,\n+            hidden_sizes=hidden_sizes,\n+            depths=[1, 1, 2, 1],\n+            out_features=[\"stage2\", \"stage3\", \"stage4\"],\n+            out_indices=[2, 3, 4],\n+            stem_channels=[3, 16, 16],\n+            use_lab=True,\n+        )\n+        return DFineConfig.from_backbone_configs(\n+            backbone_config=backbone_config,\n+            encoder_hidden_dim=self.encoder_hidden_dim,\n+            encoder_in_channels=self.encoder_in_channels,\n+            feat_strides=self.feat_strides,\n+            encoder_layers=self.encoder_layers,\n+            encoder_ffn_dim=self.encoder_ffn_dim,\n+            encoder_attention_heads=self.encoder_attention_heads,\n+            dropout=self.dropout,\n+            activation_dropout=self.activation_dropout,\n+            encode_proj_layers=self.encode_proj_layers,\n+            positional_encoding_temperature=self.positional_encoding_temperature,\n+            encoder_activation_function=self.encoder_activation_function,\n+            activation_function=self.activation_function,\n+            eval_size=self.eval_size,\n+            normalize_before=self.normalize_before,\n+            d_model=self.d_model,\n+            num_queries=self.num_queries,\n+            decoder_in_channels=self.decoder_in_channels,\n+            decoder_ffn_dim=self.decoder_ffn_dim,\n+            num_feature_levels=self.num_feature_levels,\n+            decoder_n_points=self.decoder_n_points,\n+            decoder_n_levels=self.decoder_n_levels,\n+            decoder_layers=self.decoder_layers,\n+            decoder_attention_heads=self.decoder_attention_heads,\n+            decoder_activation_function=self.decoder_activation_function,\n+            decoder_offset_scale=self.decoder_offset_scale,\n+            eval_idx=self.eval_idx,\n+            layer_scale=self.layer_scale,\n+            reg_max=self.reg_max,\n+            reg_scale=self.reg_scale,\n+            depth_mult=self.depth_mult,\n+            attention_dropout=self.attention_dropout,\n+            num_denoising=self.num_denoising,\n+            label_noise_ratio=self.label_noise_ratio,\n+            box_noise_scale=self.box_noise_scale,\n+            learn_initial_query=self.learn_initial_query,\n+            anchor_image_size=self.anchor_image_size,\n+            image_size=self.image_size,\n+            disable_custom_kernels=self.disable_custom_kernels,\n+            with_box_refine=self.with_box_refine,\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config, pixel_values, pixel_mask, labels = self.prepare_config_and_inputs()\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+    def create_and_check_d_fine_model(self, config, pixel_values, pixel_mask, labels):\n+        model = DFineModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+\n+        result = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n+        result = model(pixel_values)\n+\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.num_queries, self.d_model))\n+\n+    def create_and_check_d_fine_object_detection_head_model(self, config, pixel_values, pixel_mask, labels):\n+        model = DFineForObjectDetection(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+\n+        result = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n+        result = model(pixel_values)\n+\n+        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_queries, self.num_labels))\n+        self.parent.assertEqual(result.pred_boxes.shape, (self.batch_size, self.num_queries, 4))\n+\n+        result = model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n+\n+        self.parent.assertEqual(result.loss.shape, ())\n+        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_queries, self.num_labels))\n+        self.parent.assertEqual(result.pred_boxes.shape, (self.batch_size, self.num_queries, 4))\n+\n+\n+@require_torch\n+class DFineModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (DFineModel, DFineForObjectDetection) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\"image-feature-extraction\": DFineModel, \"object-detection\": DFineForObjectDetection}\n+        if is_torch_available()\n+        else {}\n+    )\n+    is_encoder_decoder = True\n+    test_torchscript = False\n+    test_pruning = False\n+    test_head_masking = False\n+    test_missing_keys = False\n+    test_torch_exportable = True\n+\n+    # special case for head models\n+    def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n+        inputs_dict = super()._prepare_for_class(inputs_dict, model_class, return_labels=return_labels)\n+\n+        if return_labels:\n+            if model_class.__name__ == \"DFineForObjectDetection\":\n+                labels = []\n+                for i in range(self.model_tester.batch_size):\n+                    target = {}\n+                    target[\"class_labels\"] = torch.ones(\n+                        size=(self.model_tester.n_targets,), device=torch_device, dtype=torch.long\n+                    )\n+                    target[\"boxes\"] = torch.ones(\n+                        self.model_tester.n_targets, 4, device=torch_device, dtype=torch.float\n+                    )\n+                    labels.append(target)\n+                inputs_dict[\"labels\"] = labels\n+\n+        return inputs_dict\n+\n+    def setUp(self):\n+        self.model_tester = DFineModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self,\n+            config_class=DFineConfig,\n+            has_text_modality=False,\n+            common_properties=[\"hidden_size\", \"num_attention_heads\"],\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_d_fine_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_d_fine_model(*config_and_inputs)\n+\n+    def test_d_fine_object_detection_head_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_d_fine_object_detection_head_model(*config_and_inputs)\n+\n+    @unittest.skip(reason=\"DFine doesn't work well with `nn.DataParallel\")\n+    def test_multi_gpu_data_parallel_forward(self):\n+        pass\n+\n+    @unittest.skip(reason=\"DFine does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"DFine does not use test_inputs_embeds_matches_input_ids\")\n+    def test_inputs_embeds_matches_input_ids(self):\n+        pass\n+\n+    @unittest.skip(reason=\"DFine does not support input and output embeddings\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"DFine does not support input and output embeddings\")\n+    def test_model_common_attributes(self):\n+        pass\n+\n+    @unittest.skip(reason=\"DFine does not use token embeddings\")\n+    def test_resize_tokens_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Not relevant for the model\")\n+    def test_can_init_all_missing_weights(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Feed forward chunking is not implemented\")\n+    def test_feed_forward_chunking(self):\n+        pass\n+\n+    def test_attention_outputs(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.encoder_attentions\n+            self.assertEqual(len(attentions), self.model_tester.encoder_layers)\n+\n+            # check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.encoder_attentions\n+            self.assertEqual(len(attentions), self.model_tester.encoder_layers)\n+\n+            self.assertListEqual(\n+                list(attentions[0].shape[-3:]),\n+                [\n+                    self.model_tester.encoder_attention_heads,\n+                    self.model_tester.encoder_seq_length,\n+                    self.model_tester.encoder_seq_length,\n+                ],\n+            )\n+            out_len = len(outputs)\n+\n+            correct_outlen = 15\n+\n+            # loss is at first position\n+            if \"labels\" in inputs_dict:\n+                correct_outlen += 1  # loss is added to beginning\n+            # Object Detection model returns pred_logits and pred_boxes\n+            if model_class.__name__ == \"DFineForObjectDetection\":\n+                correct_outlen += 2\n+\n+            self.assertEqual(out_len, correct_outlen)\n+\n+            # decoder attentions\n+            decoder_attentions = outputs.decoder_attentions\n+            self.assertIsInstance(decoder_attentions, (list, tuple))\n+            self.assertEqual(len(decoder_attentions), self.model_tester.decoder_layers)\n+            self.assertListEqual(\n+                list(decoder_attentions[0].shape[-3:]),\n+                [\n+                    self.model_tester.decoder_attention_heads,\n+                    self.model_tester.num_queries,\n+                    self.model_tester.num_queries,\n+                ],\n+            )\n+\n+            # cross attentions\n+            cross_attentions = outputs.cross_attentions\n+            self.assertIsInstance(cross_attentions, (list, tuple))\n+            self.assertEqual(len(cross_attentions), self.model_tester.decoder_layers)\n+            self.assertListEqual(\n+                list(cross_attentions[0].shape[-3:]),\n+                [\n+                    self.model_tester.num_queries,\n+                    self.model_tester.decoder_attention_heads,\n+                    self.model_tester.decoder_n_levels * self.model_tester.decoder_n_points\n+                    if isinstance(self.model_tester.decoder_n_points, int)\n+                    else sum(self.model_tester.decoder_n_points),\n+                ],\n+            )\n+\n+            # Check attention is always last and order is fine\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            if hasattr(self.model_tester, \"num_hidden_states_types\"):\n+                added_hidden_states = self.model_tester.num_hidden_states_types\n+            else:\n+                # DFine should maintin encoder_hidden_states output\n+                added_hidden_states = 2\n+            self.assertEqual(out_len + added_hidden_states, len(outputs))\n+\n+            self_attentions = outputs.encoder_attentions\n+\n+            self.assertEqual(len(self_attentions), self.model_tester.encoder_layers)\n+            self.assertListEqual(\n+                list(self_attentions[0].shape[-3:]),\n+                [\n+                    self.model_tester.encoder_attention_heads,\n+                    self.model_tester.encoder_seq_length,\n+                    self.model_tester.encoder_seq_length,\n+                ],\n+            )\n+\n+    def test_hidden_states_output(self):\n+        def check_hidden_states_output(inputs_dict, config, model_class):\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            hidden_states = outputs.encoder_hidden_states if config.is_encoder_decoder else outputs.hidden_states\n+\n+            expected_num_layers = getattr(\n+                self.model_tester, \"expected_num_hidden_layers\", len(self.model_tester.encoder_in_channels) - 1\n+            )\n+            self.assertEqual(len(hidden_states), expected_num_layers)\n+\n+            self.assertListEqual(\n+                list(hidden_states[1].shape[-2:]),\n+                [\n+                    self.model_tester.image_size // self.model_tester.feat_strides[-1],\n+                    self.model_tester.image_size // self.model_tester.feat_strides[-1],\n+                ],\n+            )\n+\n+            if config.is_encoder_decoder:\n+                hidden_states = outputs.decoder_hidden_states\n+\n+                expected_num_layers = getattr(\n+                    self.model_tester, \"expected_num_hidden_layers\", self.model_tester.decoder_layers + 1\n+                )\n+\n+                self.assertIsInstance(hidden_states, (list, tuple))\n+                self.assertEqual(len(hidden_states), expected_num_layers)\n+\n+                self.assertListEqual(\n+                    list(hidden_states[0].shape[-2:]),\n+                    [self.model_tester.num_queries, self.model_tester.d_model],\n+                )\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_hidden_states\"] = True\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+            # check that output_hidden_states also work using config\n+            del inputs_dict[\"output_hidden_states\"]\n+            config.output_hidden_states = True\n+\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+    def test_retain_grad_hidden_states_attentions(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.output_hidden_states = True\n+        config.output_attentions = True\n+\n+        model_class = self.all_model_classes[0]\n+        model = model_class(config)\n+        model.to(torch_device)\n+\n+        inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+        outputs = model(**inputs)\n+\n+        # we take the first output since last_hidden_state is the first item\n+        output = outputs[0]\n+\n+        encoder_hidden_states = outputs.encoder_hidden_states[0]\n+        encoder_attentions = outputs.encoder_attentions[0]\n+        encoder_hidden_states.retain_grad()\n+        encoder_attentions.retain_grad()\n+\n+        decoder_attentions = outputs.decoder_attentions[0]\n+        decoder_attentions.retain_grad()\n+\n+        cross_attentions = outputs.cross_attentions[0]\n+        cross_attentions.retain_grad()\n+\n+        output.flatten()[0].backward(retain_graph=True)\n+\n+        self.assertIsNotNone(encoder_hidden_states.grad)\n+        self.assertIsNotNone(encoder_attentions.grad)\n+        self.assertIsNotNone(decoder_attentions.grad)\n+        self.assertIsNotNone(cross_attentions.grad)\n+\n+    def test_forward_signature(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            signature = inspect.signature(model.forward)\n+            arg_names = [*signature.parameters.keys()]\n+            expected_arg_names = [\"pixel_values\"]\n+            self.assertListEqual(arg_names[:1], expected_arg_names)\n+\n+    def test_different_timm_backbone(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        # let's pick a random timm backbone\n+        config.encoder_in_channels = [24, 40, 432]\n+        config.backbone = \"tf_mobilenetv3_small_075\"\n+        config.backbone_config = None\n+        config.use_timm_backbone = True\n+        config.backbone_kwargs = {\"out_indices\": [2, 3, 4]}\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            if model_class.__name__ == \"DFineForObjectDetection\":\n+                expected_shape = (\n+                    self.model_tester.batch_size,\n+                    self.model_tester.num_queries,\n+                    self.model_tester.num_labels,\n+                )\n+                self.assertEqual(outputs.logits.shape, expected_shape)\n+                # Confirm out_indices was propogated to backbone\n+                self.assertEqual(len(model.model.backbone.intermediate_channel_sizes), 3)\n+            else:\n+                # Confirm out_indices was propogated to backbone\n+                self.assertEqual(len(model.backbone.intermediate_channel_sizes), 3)\n+\n+            self.assertTrue(outputs)\n+\n+    def test_hf_backbone(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        # Load a pretrained HF checkpoint as backbone\n+        config.backbone = \"microsoft/resnet-18\"\n+        config.backbone_config = None\n+        config.use_timm_backbone = False\n+        config.use_pretrained_backbone = True\n+        config.backbone_kwargs = {\"out_indices\": [2, 3, 4]}\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            if model_class.__name__ == \"DFineForObjectDetection\":\n+                expected_shape = (\n+                    self.model_tester.batch_size,\n+                    self.model_tester.num_queries,\n+                    self.model_tester.num_labels,\n+                )\n+                self.assertEqual(outputs.logits.shape, expected_shape)\n+                # Confirm out_indices was propogated to backbone\n+                self.assertEqual(len(model.model.backbone.intermediate_channel_sizes), 3)\n+            else:\n+                # Confirm out_indices was propogated to backbone\n+                self.assertEqual(len(model.backbone.intermediate_channel_sizes), 3)\n+\n+            self.assertTrue(outputs)\n+\n+    def test_initialization(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        configs_no_init = _config_zero_init(config)\n+        configs_no_init.initializer_bias_prior_prob = 0.2\n+        bias_value = -1.3863  # log_e ((1 - 0.2) / 0.2)\n+\n+        failed_cases = []\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            # Skip the check for the backbone\n+            for name, module in model.named_modules():\n+                if module.__class__.__name__ == \"DFineConvEncoder\":\n+                    backbone_params = [f\"{name}.{key}\" for key in module.state_dict().keys()]\n+                    break\n+\n+            for name, param in model.named_parameters():\n+                if param.requires_grad:\n+                    if (\"class_embed\" in name and \"bias\" in name) or \"enc_score_head.bias\" in name:\n+                        bias_tensor = torch.full_like(param.data, bias_value)\n+                        try:\n+                            torch.testing.assert_close(param.data, bias_tensor, atol=1e-4, rtol=1e-4)\n+                        except AssertionError:\n+                            failed_cases.append(\n+                                f\"Parameter {name} of model {model_class} seems not properly initialized. \"\n+                                f\"Biases should be initialized to {bias_value}, got {param.data}\"\n+                            )\n+                    elif (\n+                        \"level_embed\" in name\n+                        or \"sampling_offsets.bias\" in name\n+                        or \"value_proj\" in name\n+                        or \"output_proj\" in name\n+                        or \"reference_points\" in name\n+                        or \"enc_score_head.weight\" in name\n+                        or (\"class_embed\" in name and \"weight\" in name)\n+                        or name in backbone_params\n+                    ):\n+                        continue\n+                    else:\n+                        mean = param.data.mean()\n+                        round_mean = (mean * 1e9).round() / 1e9\n+                        round_mean = round_mean.item()\n+                        if round_mean not in [0.0, 1.0]:\n+                            failed_cases.append(\n+                                f\"Parameter {name} of model {model_class} seems not properly initialized. \"\n+                                f\"Mean is {round_mean}, but should be in [0, 1]\"\n+                            )\n+\n+        message = \"\\n\" + \"\\n\".join(failed_cases)\n+        self.assertTrue(not failed_cases, message)\n+\n+    @parameterized.expand([\"float32\", \"float16\", \"bfloat16\"])\n+    @require_torch_gpu\n+    @slow\n+    def test_inference_with_different_dtypes(self, torch_dtype_str):\n+        torch_dtype = {\n+            \"float32\": torch.float32,\n+            \"float16\": torch.float16,\n+            \"bfloat16\": torch.bfloat16,\n+        }[torch_dtype_str]\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device).to(torch_dtype)\n+            model.eval()\n+            for key, tensor in inputs_dict.items():\n+                if tensor.dtype == torch.float32:\n+                    inputs_dict[key] = tensor.to(torch_dtype)\n+            with torch.no_grad():\n+                _ = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+    @parameterized.expand([\"float32\", \"float16\", \"bfloat16\"])\n+    @require_torch_gpu\n+    @slow\n+    def test_inference_equivalence_for_static_and_dynamic_anchors(self, torch_dtype_str):\n+        torch_dtype = {\n+            \"float32\": torch.float32,\n+            \"float16\": torch.float16,\n+            \"bfloat16\": torch.bfloat16,\n+        }[torch_dtype_str]\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        h, w = inputs_dict[\"pixel_values\"].shape[-2:]\n+\n+        # convert inputs to the desired dtype\n+        for key, tensor in inputs_dict.items():\n+            if tensor.dtype == torch.float32:\n+                inputs_dict[key] = tensor.to(torch_dtype)\n+\n+        for model_class in self.all_model_classes:\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model_class(config).save_pretrained(tmpdirname)\n+                model_static = model_class.from_pretrained(\n+                    tmpdirname, anchor_image_size=[h, w], device_map=torch_device, torch_dtype=torch_dtype\n+                ).eval()\n+                model_dynamic = model_class.from_pretrained(\n+                    tmpdirname, anchor_image_size=None, device_map=torch_device, torch_dtype=torch_dtype\n+                ).eval()\n+\n+            self.assertIsNotNone(model_static.config.anchor_image_size)\n+            self.assertIsNone(model_dynamic.config.anchor_image_size)\n+\n+            with torch.no_grad():\n+                outputs_static = model_static(**self._prepare_for_class(inputs_dict, model_class))\n+                outputs_dynamic = model_dynamic(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            torch.testing.assert_close(\n+                outputs_static.last_hidden_state, outputs_dynamic.last_hidden_state, rtol=1e-4, atol=1e-4\n+            )\n+\n+\n+TOLERANCE = 1e-4\n+\n+\n+# We will verify our results on an image of cute cats\n+def prepare_img():\n+    image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+    return image\n+\n+\n+@require_torch\n+@require_vision\n+class DFineModelIntegrationTest(unittest.TestCase):\n+    @cached_property\n+    def default_image_processor(self):\n+        return RTDetrImageProcessor.from_pretrained(CHECKPOINT) if is_vision_available() else None\n+\n+    def test_inference_object_detection_head(self):\n+        model = DFineForObjectDetection.from_pretrained(CHECKPOINT).to(torch_device)\n+\n+        image_processor = self.default_image_processor\n+        image = prepare_img()\n+        inputs = image_processor(images=image, return_tensors=\"pt\").to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+\n+        expected_shape_logits = torch.Size((1, 300, model.config.num_labels))\n+        self.assertEqual(outputs.logits.shape, expected_shape_logits)\n+\n+        expected_logits = torch.tensor(\n+            [\n+                [-3.8097816, -4.7724586, -5.994499],\n+                [-5.2974715, -9.499067, -6.1653666],\n+                [-5.3502765, -3.9530406, -6.3630295],\n+            ]\n+        ).to(torch_device)\n+        expected_boxes = torch.tensor(\n+            [\n+                [0.7677696, 0.41479152, 0.46441072],\n+                [0.16912134, 0.19869131, 0.2123824],\n+                [0.2581653, 0.54818195, 0.47512347],\n+            ]\n+        ).to(torch_device)\n+\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_logits, atol=1e-4, rtol=1e-4)\n+\n+        expected_shape_boxes = torch.Size((1, 300, 4))\n+        self.assertEqual(outputs.pred_boxes.shape, expected_shape_boxes)\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_boxes, atol=1e-4, rtol=1e-4)\n+\n+        # verify postprocessing\n+        results = image_processor.post_process_object_detection(\n+            outputs, threshold=0.0, target_sizes=[image.size[::-1]]\n+        )[0]\n+        expected_scores = torch.tensor([0.9642, 0.9542, 0.9536, 0.8548], device=torch_device)\n+        expected_labels = [15, 65, 15, 57]\n+        expected_slice_boxes = torch.tensor(\n+            [\n+                [1.3186283e01, 5.4130211e01, 3.1726535e02, 4.7212445e02],\n+                [4.0275269e01, 7.2975174e01, 1.7620003e02, 1.1776848e02],\n+                [3.4276117e02, 2.3427944e01, 6.3998401e02, 3.7477191e02],\n+                [5.8418274e-01, 1.1794567e00, 6.3933154e02, 4.7485995e02],\n+            ],\n+            device=torch_device,\n+        )\n+        torch.testing.assert_close(results[\"scores\"][:4], expected_scores, atol=1e-3, rtol=1e-4)\n+        self.assertSequenceEqual(results[\"labels\"][:4].tolist(), expected_labels)\n+        torch.testing.assert_close(results[\"boxes\"][:4], expected_slice_boxes[:4], atol=1e-3, rtol=1e-4)"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/hgnet_v2/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/tests%2Fmodels%2Fhgnet_v2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/tests%2Fmodels%2Fhgnet_v2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhgnet_v2%2F__init__.py?ref=4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63"
        },
        {
            "sha": "2dad713308b47353f7da0a103b46116c43ec414d",
            "filename": "tests/models/hgnet_v2/test_modeling_hgnet_v2.py",
            "status": "added",
            "additions": 286,
            "deletions": 0,
            "changes": 286,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/tests%2Fmodels%2Fhgnet_v2%2Ftest_modeling_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/tests%2Fmodels%2Fhgnet_v2%2Ftest_modeling_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhgnet_v2%2Ftest_modeling_hgnet_v2.py?ref=4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63",
            "patch": "@@ -0,0 +1,286 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import torch\n+from torch import nn\n+\n+from transformers import HGNetV2Config\n+from transformers.testing_utils import require_torch, torch_device\n+from transformers.utils.import_utils import is_torch_available\n+\n+from ...test_backbone_common import BackboneTesterMixin\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    from transformers import HGNetV2Backbone, HGNetV2ForImageClassification\n+\n+\n+class HGNetV2ModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=3,\n+        image_size=32,\n+        num_channels=3,\n+        embeddings_size=10,\n+        hidden_sizes=[64, 128, 256, 512],\n+        stage_in_channels=[16, 64, 128, 256],\n+        stage_mid_channels=[16, 32, 64, 128],\n+        stage_out_channels=[64, 128, 256, 512],\n+        stage_num_blocks=[1, 1, 2, 1],\n+        stage_downsample=[False, True, True, True],\n+        stage_light_block=[False, False, True, True],\n+        stage_kernel_size=[3, 3, 5, 5],\n+        stage_numb_of_layers=[3, 3, 3, 3],\n+        stem_channels=[3, 16, 16],\n+        depths=[1, 1, 2, 1],\n+        is_training=True,\n+        use_labels=True,\n+        hidden_act=\"relu\",\n+        num_labels=3,\n+        scope=None,\n+        out_features=[\"stage2\", \"stage3\", \"stage4\"],\n+        out_indices=[2, 3, 4],\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.image_size = image_size\n+        self.num_channels = num_channels\n+        self.embeddings_size = embeddings_size\n+        self.hidden_sizes = hidden_sizes\n+        self.stage_in_channels = stage_in_channels\n+        self.stage_mid_channels = stage_mid_channels\n+        self.stage_out_channels = stage_out_channels\n+        self.stage_num_blocks = stage_num_blocks\n+        self.stage_downsample = stage_downsample\n+        self.stage_light_block = stage_light_block\n+        self.stage_kernel_size = stage_kernel_size\n+        self.stage_numb_of_layers = stage_numb_of_layers\n+        self.stem_channels = stem_channels\n+        self.depths = depths\n+        self.is_training = is_training\n+        self.use_labels = use_labels\n+        self.hidden_act = hidden_act\n+        self.num_labels = num_labels\n+        self.scope = scope\n+        self.num_stages = len(hidden_sizes)\n+        self.out_features = out_features\n+        self.out_indices = out_indices\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+\n+        labels = None\n+        if self.use_labels:\n+            labels = ids_tensor([self.batch_size], self.num_labels)\n+\n+        config = self.get_config()\n+\n+        return config, pixel_values, labels\n+\n+    def get_config(self):\n+        return HGNetV2Config(\n+            num_channels=self.num_channels,\n+            embeddings_size=self.embeddings_size,\n+            hidden_sizes=self.hidden_sizes,\n+            stage_in_channels=self.stage_in_channels,\n+            stage_mid_channels=self.stage_mid_channels,\n+            stage_out_channels=self.stage_out_channels,\n+            stage_num_blocks=self.stage_num_blocks,\n+            stage_downsample=self.stage_downsample,\n+            stage_light_block=self.stage_light_block,\n+            stage_kernel_size=self.stage_kernel_size,\n+            stage_numb_of_layers=self.stage_numb_of_layers,\n+            stem_channels=self.stem_channels,\n+            depths=self.depths,\n+            hidden_act=self.hidden_act,\n+            num_labels=self.num_labels,\n+            out_features=self.out_features,\n+            out_indices=self.out_indices,\n+        )\n+\n+    def create_and_check_backbone(self, config, pixel_values, labels):\n+        model = HGNetV2Backbone(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+\n+        # verify feature maps\n+        self.parent.assertEqual(len(result.feature_maps), len(config.out_features))\n+        self.parent.assertListEqual(list(result.feature_maps[0].shape), [self.batch_size, self.hidden_sizes[1], 4, 4])\n+\n+        # verify channels\n+        self.parent.assertEqual(len(model.channels), len(config.out_features))\n+        self.parent.assertListEqual(model.channels, config.hidden_sizes[1:])\n+\n+        # verify backbone works with out_features=None\n+        config.out_features = None\n+        model = HGNetV2Backbone(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+\n+        # verify feature maps\n+        self.parent.assertEqual(len(result.feature_maps), 1)\n+        self.parent.assertListEqual(list(result.feature_maps[0].shape), [self.batch_size, self.hidden_sizes[-1], 1, 1])\n+\n+        # verify channels\n+        self.parent.assertEqual(len(model.channels), 1)\n+        self.parent.assertListEqual(model.channels, [config.hidden_sizes[-1]])\n+\n+    def create_and_check_for_image_classification(self, config, pixel_values, labels):\n+        config.num_labels = self.num_labels\n+        model = HGNetV2ForImageClassification(config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values, labels=labels)\n+        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values, labels = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class RTDetrResNetBackboneTest(BackboneTesterMixin, unittest.TestCase):\n+    all_model_classes = (HGNetV2Backbone,) if is_torch_available() else ()\n+    has_attentions = False\n+    config_class = HGNetV2Config\n+\n+    def setUp(self):\n+        self.model_tester = HGNetV2ModelTester(self)\n+\n+\n+@require_torch\n+class HGNetV2ForImageClassificationTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Here we also overwrite some tests of test_modeling_common.py, as TextNet does not use input_ids, inputs_embeds,\n+    attention_mask and seq_length.\n+    \"\"\"\n+\n+    all_model_classes = (HGNetV2ForImageClassification, HGNetV2Backbone) if is_torch_available() else ()\n+    pipeline_model_mapping = {\"image-classification\": HGNetV2ForImageClassification} if is_torch_available() else {}\n+\n+    fx_compatible = False\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+    test_torch_exportable = True\n+    has_attentions = False\n+\n+    def setUp(self):\n+        self.model_tester = HGNetV2ModelTester(self)\n+\n+    @unittest.skip(reason=\"Does not work on the tiny model.\")\n+    def test_model_parallelism(self):\n+        super().test_model_parallelism()\n+\n+    @unittest.skip(reason=\"HGNetV2 does not output attentions\")\n+    def test_attention_outputs(self):\n+        pass\n+\n+    @unittest.skip(reason=\"HGNetV2 does not have input/output embeddings\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"HGNetV2 does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"HGNetV2 does not support input and output embeddings\")\n+    def test_model_common_attributes(self):\n+        pass\n+\n+    @unittest.skip(reason=\"HGNetV2 does not have a model\")\n+    def test_model(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Not relevant for the model\")\n+    def test_can_init_all_missing_weights(self):\n+        pass\n+\n+    def test_backbone(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_backbone(*config_and_inputs)\n+\n+    def test_initialization(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=config)\n+            for name, module in model.named_modules():\n+                if isinstance(module, (nn.BatchNorm2d, nn.GroupNorm)):\n+                    self.assertTrue(\n+                        torch.all(module.weight == 1),\n+                        msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                    )\n+                    self.assertTrue(\n+                        torch.all(module.bias == 0),\n+                        msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                    )\n+\n+    def test_hidden_states_output(self):\n+        def check_hidden_states_output(inputs_dict, config, model_class):\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            hidden_states = outputs.encoder_hidden_states if config.is_encoder_decoder else outputs.hidden_states\n+\n+            self.assertEqual(len(hidden_states), self.model_tester.num_stages + 1)\n+\n+            self.assertListEqual(\n+                list(hidden_states[0].shape[-2:]),\n+                [self.model_tester.image_size // 4, self.model_tester.image_size // 4],\n+            )\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        layers_type = [\"preactivation\", \"bottleneck\"]\n+        for model_class in self.all_model_classes:\n+            for layer_type in layers_type:\n+                config.layer_type = layer_type\n+                inputs_dict[\"output_hidden_states\"] = True\n+                check_hidden_states_output(inputs_dict, config, model_class)\n+\n+                # check that output_hidden_states also work using config\n+                del inputs_dict[\"output_hidden_states\"]\n+                config.output_hidden_states = True\n+\n+                check_hidden_states_output(inputs_dict, config, model_class)\n+\n+    @unittest.skip(reason=\"Retain_grad is not supposed to be tested\")\n+    def test_retain_grad_hidden_states_attentions(self):\n+        pass\n+\n+    @unittest.skip(reason=\"TextNet does not use feedforward chunking\")\n+    def test_feed_forward_chunking(self):\n+        pass\n+\n+    def test_for_image_classification(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_for_image_classification(*config_and_inputs)\n+\n+    @unittest.skip(reason=\"HGNetV2 does not use model\")\n+    def test_model_from_pretrained(self):\n+        pass"
        },
        {
            "sha": "dc0dac82da2647b893fd4e3b76adb67ce6f97fb3",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=4abeb50f6e7b5ca7e63f80686cdd74e7b6b03e63",
            "patch": "@@ -183,6 +183,22 @@\n         \"giou_loss_coefficient\",\n         \"mask_loss_coefficient\",\n     ],\n+    \"DFineConfig\": [\n+        \"eos_coefficient\",\n+        \"focal_loss_alpha\",\n+        \"focal_loss_gamma\",\n+        \"matcher_alpha\",\n+        \"matcher_bbox_cost\",\n+        \"matcher_class_cost\",\n+        \"matcher_gamma\",\n+        \"matcher_giou_cost\",\n+        \"use_focal_loss\",\n+        \"weight_loss_bbox\",\n+        \"weight_loss_giou\",\n+        \"weight_loss_vfl\",\n+        \"weight_loss_fgl\",\n+        \"weight_loss_ddf\",\n+    ],\n     \"GroundingDinoConfig\": [\n         \"bbox_cost\",\n         \"bbox_loss_coefficient\","
        }
    ],
    "stats": {
        "total": 7715,
        "additions": 7711,
        "deletions": 4
    }
}