{
    "author": "ydshieh",
    "message": "update gemma tests (#38384)\n\n* update\n\n* update\n\n* update\n\n* update\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "07848a84054f377fca4fbce357a92ccb94ec42ff",
    "files": [
        {
            "sha": "058ccd74cd7a49ab52e39c8ef2b3a12ad9d1ae67",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 4,
            "deletions": 27,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/07848a84054f377fca4fbce357a92ccb94ec42ff/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07848a84054f377fca4fbce357a92ccb94ec42ff/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=07848a84054f377fca4fbce357a92ccb94ec42ff",
            "patch": "@@ -30,7 +30,6 @@\n     require_torch,\n     require_torch_accelerator,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -147,7 +146,7 @@ def test_model_2b_bf16(self):\n \n         EXPECTED_TEXTS = [\n             \"Hello I am doing a project on the 1990s and I need to know what the most popular music\",\n-            \"Hi today I am going to share with you a very easy and simple recipe of <strong><em>Khichdi\",\n+            \"Hi today I am going to share with you a very easy and simple recipe of <strong><em>Kaju Kat\",\n         ]\n \n         model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16).to(\n@@ -168,34 +167,12 @@ def test_model_2b_eager(self):\n \n         EXPECTED_TEXTS = [\n             \"Hello I am doing a project on the 1990s and I need to know what the most popular music\",\n-            \"Hi today I am going to share with you a very easy and simple recipe of <strong><em>Khichdi\",\n-        ]\n-\n-        model = AutoModelForCausalLM.from_pretrained(\n-            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16, attn_implementation=\"eager\"\n-        )\n-        model.to(torch_device)\n-\n-        tokenizer = AutoTokenizer.from_pretrained(model_id)\n-        inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n-\n-        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n-        output_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n-\n-        self.assertEqual(output_text, EXPECTED_TEXTS)\n-\n-    @require_torch_sdpa\n-    @require_read_token\n-    def test_model_2b_sdpa(self):\n-        model_id = \"google/gemma-2b\"\n-\n-        EXPECTED_TEXTS = [\n-            \"Hello I am doing a project on the 1990s and I need to know what the most popular music\",\n-            \"Hi today I am going to share with you a very easy and simple recipe of <strong><em>Khichdi\",\n+            \"Hi today I am going to share with you a very easy and simple recipe of <strong><em>Kaju Kat\",\n         ]\n \n+        # bfloat16 gives strange values, likely due to it has lower precision + very short prompts\n         model = AutoModelForCausalLM.from_pretrained(\n-            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16, attn_implementation=\"sdpa\"\n+            model_id, low_cpu_mem_usage=True, torch_dtype=torch.float16, attn_implementation=\"eager\"\n         )\n         model.to(torch_device)\n "
        }
    ],
    "stats": {
        "total": 31,
        "additions": 4,
        "deletions": 27
    }
}