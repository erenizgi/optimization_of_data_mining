{
    "author": "yonigozlan",
    "message": "Refactor siglip2 fast image processor (#36406)\n\n* refactor siglip2 fast image processor, add unused_kwargs in base fast image processor\n\n* nits\n\n* change unused_kwargs default to None\n\n* update siglip2 fast image proc",
    "sha": "48292a9848fffc199c487c2b0b34f21c789acabb",
    "files": [
        {
            "sha": "7d3065fda4954bd13280df3dbee66a520b0a340a",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/48292a9848fffc199c487c2b0b34f21c789acabb/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48292a9848fffc199c487c2b0b34f21c789acabb/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=48292a9848fffc199c487c2b0b34f21c789acabb",
            "patch": "@@ -265,12 +265,14 @@ class BaseImageProcessorFast(BaseImageProcessor):\n     device = None\n     model_input_names = [\"pixel_values\"]\n     valid_kwargs = DefaultFastImageProcessorKwargs\n+    unused_kwargs = None\n \n     def __init__(\n         self,\n         **kwargs: Unpack[DefaultFastImageProcessorKwargs],\n     ) -> None:\n         super().__init__(**kwargs)\n+        kwargs = self.filter_out_unused_kwargs(kwargs)\n         size = kwargs.pop(\"size\", self.size)\n         self.size = (\n             get_size_dict(size=size, default_to_square=kwargs.pop(\"default_to_square\", self.default_to_square))\n@@ -438,6 +440,19 @@ def convert_to_rgb(\n         \"\"\"\n         return convert_to_rgb(image)\n \n+    def filter_out_unused_kwargs(self, kwargs: dict):\n+        \"\"\"\n+        Filter out the unused kwargs from the kwargs dictionary.\n+        \"\"\"\n+        if self.unused_kwargs is None:\n+            return kwargs\n+\n+        for kwarg_name in self.unused_kwargs:\n+            if kwarg_name in kwargs:\n+                logger.warning_once(f\"This processor does not use the `{kwarg_name}` parameter. It will be ignored.\")\n+                kwargs.pop(kwarg_name)\n+        return kwargs\n+\n     def _prepare_images_structure(\n         self,\n         images: ImageInput,\n@@ -634,6 +649,7 @@ def _preprocess(\n         image_mean: Optional[Union[float, List[float]]],\n         image_std: Optional[Union[float, List[float]]],\n         return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched resizing\n         grouped_images, grouped_images_index = group_images_by_shape(images)"
        },
        {
            "sha": "26948a507f405e5030807a3365fe2a1c611c14ab",
            "filename": "src/transformers/models/siglip2/image_processing_siglip2_fast.py",
            "status": "modified",
            "additions": 73,
            "deletions": 198,
            "changes": 271,
            "blob_url": "https://github.com/huggingface/transformers/blob/48292a9848fffc199c487c2b0b34f21c789acabb/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48292a9848fffc199c487c2b0b34f21c789acabb/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2_fast.py?ref=48292a9848fffc199c487c2b0b34f21c789acabb",
            "patch": "@@ -14,78 +14,46 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for SigLIP2.\"\"\"\n \n-import math\n from functools import lru_cache\n from typing import List, Optional, Tuple, Union\n \n import torch\n \n from ...image_processing_utils import BatchFeature\n-from ...image_processing_utils_fast import BaseImageProcessorFast\n+from ...image_processing_utils_fast import (\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    SizeDict,\n+)\n from ...image_utils import (\n-    ChannelDimension,\n     ImageInput,\n     PILImageResampling,\n-    SizeDict,\n-    TensorType,\n )\n+from ...processing_utils import Unpack\n from ...utils import (\n-    filter_out_non_signature_kwargs,\n+    TensorType,\n+    add_start_docstrings,\n     is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    logging,\n )\n+from .image_processing_siglip2 import get_image_size_for_max_num_patches\n \n \n if is_torch_available():\n     import torch\n \n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n \n-@lru_cache(maxsize=256)\n-# Copied from transformers.models.siglip2.image_processing_siglip2.get_image_size_for_max_num_patches\n-def get_image_size_for_max_num_patches(\n-    image_height: int, image_width: int, patch_size: int, max_num_patches: int, eps: float = 1e-5\n-) -> Tuple[int, int]:\n-    \"\"\"\n-    Determine image size based on max number of patches, ensure dimensions are divisible by patch size and image is at least 1 patch.\n-\n-    Args:\n-        image_height (`int`):\n-            Original image height.\n-        image_width (`int`):\n-            Original image width.\n-        patch_size (`int`):\n-            Patch size for processing.\n-        max_num_patches (`int`):\n-            Maximum number of patches.\n-        eps (`float`):\n-            Small threshold for binary search.\n-\n-    Returns:\n-        Tuple: (target_height, target_width)\n-    \"\"\"\n-\n-    def get_scaled_image_size(scale: float, size: int, patch_size: int) -> int:\n-        scaled_size = size * scale\n-        scaled_size = math.ceil(scaled_size / patch_size) * patch_size  # make divisible by patch_size\n-        scaled_size = max(patch_size, scaled_size)  # ensure at least 1 patch\n-        return int(scaled_size)\n-\n-    # Binary search for optimal scale\n-    scale_min, scale_max = eps / 10, 100.0\n-    while (scale_max - scale_min) >= eps:\n-        scale = (scale_min + scale_max) / 2\n-        target_height = get_scaled_image_size(scale, image_height, patch_size)\n-        target_width = get_scaled_image_size(scale, image_width, patch_size)\n-        num_patches = (target_height / patch_size) * (target_width / patch_size)\n-\n-        if num_patches <= max_num_patches:\n-            scale_min = scale\n-        else:\n-            scale_max = scale\n \n-    scale = scale_min\n-    target_height = get_scaled_image_size(scale, image_height, patch_size)\n-    target_width = get_scaled_image_size(scale, image_width, patch_size)\n-    return target_height, target_width\n+logger = logging.get_logger(__name__)\n \n \n def convert_image_to_patches(image: \"torch.Tensor\", patch_size: int) -> \"torch.Tensor\":\n@@ -118,164 +86,71 @@ def pad_along_first_dim(\n     return tensor, mask\n \n \n-class Siglip2ImageProcessorFast(BaseImageProcessorFast):\n-    r\"\"\"\n-    Constructs a fast SigLIP2 image processor.\n+class Siglip2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    patch_size: Optional[int]\n+    max_num_patches: Optional[int]\n+\n \n-    Args:\n-        do_resize (`bool`, *optional*, defaults to `True`):\n-            Whether to resize the image's dimensions to fit `max_num_patches` according to given `patch_size`.\n-            Can be overridden by `do_resize` in the `preprocess` method.\n-        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`):\n-            Resampling filter to use if resizing the image. Can be overridden by `resample` in the `preprocess` method.\n-        do_rescale (`bool`, *optional*, defaults to `True`):\n-            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n-            the `preprocess` method.\n-        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n-            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in the `preprocess`\n-            method.\n-        do_normalize (`bool`, *optional*, defaults to `True`):\n-            Whether to normalize the image by the specified mean and standard deviation. Can be overridden by\n-            `do_normalize` in the `preprocess` method.\n-        image_mean (`float` or `List[float]`, *optional*, defaults to `[0.5, 0.5, 0.5]`):\n-            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n-            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n-        image_std (`float` or `List[float]`, *optional*, defaults to `[0.5, 0.5, 0.5]`):\n-            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n-            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n-            Can be overridden by the `image_std` parameter in the `preprocess` method.\n-        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n-            Whether to convert the image to RGB.\n+@add_start_docstrings(\n+    r\"Constructs a fast Siglip2 image processor.\",\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    \"\"\"\n         patch_size (`int`, *optional*, defaults to 16):\n             The size (resolution) of each patch the image will be split to.\n         max_num_patches (`int`, *optional*, defaults to 256):\n             The image will be resized to have at most this number of patches,\n             and then padded in \"patch\" dimension to match this number exactly.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        do_resize: bool = True,\n-        resample: PILImageResampling = PILImageResampling.BILINEAR,\n-        do_rescale: bool = True,\n-        rescale_factor: float = 1 / 255,\n-        do_normalize: bool = True,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        patch_size: int = 16,\n-        max_num_patches: int = 256,\n-        **kwargs,\n-    ):\n+    \"\"\",\n+)\n+class Siglip2ImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = [0.5, 0.5, 0.5]\n+    image_std = [0.5, 0.5, 0.5]\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    patch_size = 16\n+    max_num_patches = 256\n+    valid_kwargs = Siglip2FastImageProcessorKwargs\n+    unused_kwargs = [\"size\", \"do_center_crop\", \"crop_size\"]\n+\n+    def __init__(self, **kwargs: Unpack[Siglip2FastImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n-        image_mean = image_mean if image_mean is not None else [0.5, 0.5, 0.5]\n-        image_std = image_std if image_std is not None else [0.5, 0.5, 0.5]\n+    @lru_cache(maxsize=10)\n+    def _prepare_process_arguments(self, **kwargs) -> tuple:\n+        # Remove do_resize from kwargs to not raise an error as size is None\n+        kwargs.pop(\"do_resize\", None)\n+        return super()._prepare_process_arguments(**kwargs)\n \n-        self.do_resize = do_resize\n-        self.resample = resample\n-        self.do_rescale = do_rescale\n-        self.rescale_factor = rescale_factor\n-        self.do_normalize = do_normalize\n-        self.image_mean = image_mean\n-        self.image_std = image_std\n-        self.do_convert_rgb = do_convert_rgb\n-        self.patch_size = patch_size\n-        self.max_num_patches = max_num_patches\n+    @add_start_docstrings(\n+        BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+        \"\"\"\n+        patch_size (`int`, *optional*, defaults to `self.patch_size`):\n+            The size (resolution) of each patch the image will be split to.\n+        max_num_patches (`int`, *optional*, defaults to `self.max_num_patches`):\n+            The image will be resized to have at most this number of patches,\n+            and then padded in \"patch\" dimension to match this number exactly.\n+        \"\"\",\n+    )\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[Siglip2FastImageProcessorKwargs]) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n \n-    @filter_out_non_signature_kwargs()\n-    def preprocess(\n+    def _preprocess(\n         self,\n-        images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        patch_size: Optional[int] = None,\n-        max_num_patches: Optional[int] = None,\n-        device: Union[\"torch.device\", str] = \"cpu\",\n+        images: List[\"torch.Tensor\"],\n+        do_resize: bool,\n+        patch_size: int,\n+        max_num_patches: int,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, List[float]]],\n+        image_std: Optional[Union[float, List[float]]],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n     ) -> BatchFeature:\n-        \"\"\"\n-        Preprocess an image or batch of images.\n-\n-        Args:\n-            images (`ImageInput`):\n-                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n-                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n-                Whether to resize the image.\n-            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n-                Size of the image after resizing.\n-            resample (`int`, *optional*, defaults to `self.resample`):\n-                Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n-                has an effect if `do_resize` is set to `True`.\n-            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n-                Whether to rescale the image.\n-            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n-                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n-            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n-                Whether to normalize the image.\n-            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n-                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n-            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n-                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n-                `True`.\n-            return_tensors (`str` or `TensorType`, *optional*):\n-                The type of tensors to return. Can be one of:\n-                - Unset: Return a list of `np.ndarray`.\n-                - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n-                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n-                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n-                - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n-                from the input image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n-                Whether to convert the image to RGB.\n-            patch_size (`int`, *optional*, defaults to `self.patch_size`):\n-                Patch size for processing, same as the patch size used in the model.\n-            max_num_patches (`int`, *optional*, defaults to `self.max_num_patches`):\n-                Maximum number of patches per image, the image will be resized to have at most this number of patches.\n-        \"\"\"\n-        do_resize = do_resize if do_resize is not None else self.do_resize\n-        resample = resample if resample is not None else self.resample\n-        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n-        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n-        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n-        image_mean = image_mean if image_mean is not None else self.image_mean\n-        image_std = image_std if image_std is not None else self.image_std\n-        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n-        patch_size = patch_size if patch_size is not None else self.patch_size\n-        max_num_patches = max_num_patches if max_num_patches is not None else self.max_num_patches\n-\n-        image_mean = tuple(image_mean) if isinstance(image_mean, list) else image_mean\n-        image_std = tuple(image_std) if isinstance(image_std, list) else image_std\n-\n-        image_mean, image_std, interpolation = self._prepare_process_arguments(\n-            do_normalize=do_normalize,\n-            do_rescale=do_rescale,\n-            rescale_factor=rescale_factor,\n-            image_mean=image_mean,\n-            image_std=image_std,\n-            resample=resample,\n-        )\n-\n-        images = self._prepare_input_images(\n-            images=images,\n-            do_convert_rgb=do_convert_rgb,\n-            input_data_format=input_data_format,\n-            device=device,\n-        )\n-\n         pixel_masks = []\n         pixel_values = []\n         spatial_shapes = []"
        }
    ],
    "stats": {
        "total": 287,
        "additions": 89,
        "deletions": 198
    }
}