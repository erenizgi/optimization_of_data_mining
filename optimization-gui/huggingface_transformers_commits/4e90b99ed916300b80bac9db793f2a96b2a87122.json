{
    "author": "Cyrilvallez",
    "message": "Refactor StarCoder2 using modular (#34015)\n\n* Create modular_starcoder2.py\r\n\r\n* Update modular_starcoder2.py\r\n\r\n* update\r\n\r\n* finalize modular\r\n\r\n* revert # no-unravel\r\n\r\n* Add support\r\n\r\n* style\r\n\r\n* Update modular_model_converter.py\r\n\r\n* update docstring",
    "sha": "4e90b99ed916300b80bac9db793f2a96b2a87122",
    "files": [
        {
            "sha": "93adc80d161aa7d0edc1aef221b7d82fc136f7de",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 31,
            "deletions": 53,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e90b99ed916300b80bac9db793f2a96b2a87122/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e90b99ed916300b80bac9db793f2a96b2a87122/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=4e90b99ed916300b80bac9db793f2a96b2a87122",
            "patch": "@@ -1,3 +1,9 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/starcoder2/modular_starcoder2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_starcoder2.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2024 BigCode and the HuggingFace Inc. team. All rights reserved.\n #\n@@ -17,20 +23,18 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"PyTorch Starcoder2 model.\"\"\"\n \n import math\n from typing import List, Optional, Tuple, Union\n \n import torch\n-import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import _flash_attention_forward\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -56,12 +60,10 @@\n \n \n logger = logging.get_logger(__name__)\n-\n _CHECKPOINT_FOR_DOC = \"bigcode/starcoder2-7b\"\n _CONFIG_FOR_DOC = \"Starcoder2Config\"\n \n \n-# Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Starcoder2\n class Starcoder2RotaryEmbedding(nn.Module):\n     def __init__(\n         self,\n@@ -149,15 +151,30 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-# Copied from transformers.models.llama.modeling_llama.rotate_half\n+class Starcoder2MLP(nn.Module):\n+    def __init__(self, config: Starcoder2Config):\n+        super().__init__()\n+        embed_dim = config.hidden_size\n+        self.c_fc = nn.Linear(embed_dim, config.intermediate_size, bias=config.use_bias)\n+        self.c_proj = nn.Linear(config.intermediate_size, embed_dim, bias=config.use_bias)\n+        self.act = ACT2FN[config.hidden_act]\n+        self.residual_dropout = config.residual_dropout\n+\n+    def forward(self, hidden_states: Optional[Tuple[torch.FloatTensor]]) -> torch.FloatTensor:\n+        hidden_states = self.c_fc(hidden_states)\n+        hidden_states = self.act(hidden_states)\n+        hidden_states = self.c_proj(hidden_states)\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.residual_dropout, training=self.training)\n+        return hidden_states\n+\n+\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n     x2 = x[..., x.shape[-1] // 2 :]\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -185,24 +202,6 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n-class Starcoder2MLP(nn.Module):\n-    def __init__(self, config: Starcoder2Config):\n-        super().__init__()\n-        embed_dim = config.hidden_size\n-        self.c_fc = nn.Linear(embed_dim, config.intermediate_size, bias=config.use_bias)\n-        self.c_proj = nn.Linear(config.intermediate_size, embed_dim, bias=config.use_bias)\n-        self.act = ACT2FN[config.hidden_act]\n-        self.residual_dropout = config.residual_dropout\n-\n-    def forward(self, hidden_states: Optional[Tuple[torch.FloatTensor]]) -> torch.FloatTensor:\n-        hidden_states = self.c_fc(hidden_states)\n-        hidden_states = self.act(hidden_states)\n-        hidden_states = self.c_proj(hidden_states)\n-        hidden_states = nn.functional.dropout(hidden_states, p=self.residual_dropout, training=self.training)\n-        return hidden_states\n-\n-\n-# Copied from transformers.models.llama.modeling_llama.repeat_kv\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -331,7 +330,6 @@ class Starcoder2FlashAttention2(Starcoder2Attention):\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n@@ -340,7 +338,6 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n-    # Ignore copy\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -406,7 +403,7 @@ def forward(\n             key_states = key_states.to(target_dtype)\n             value_states = value_states.to(target_dtype)\n \n-        # Reashape to the expected shape for Flash Attention\n+        # Reshape to the expected shape for Flash Attention\n         query_states = query_states.transpose(1, 2)\n         key_states = key_states.transpose(1, 2)\n         value_states = value_states.transpose(1, 2)\n@@ -434,15 +431,13 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-# Copied from transformers.models.mixtral.modeling_mixtral.MixtralSdpaAttention with Mixtral->Starcoder2\n class Starcoder2SdpaAttention(Starcoder2Attention):\n     \"\"\"\n     Starcoder2 attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n     `Starcoder2Attention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n     SDPA API.\n     \"\"\"\n \n-    # Ignore copy\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -552,7 +547,6 @@ def __init__(self, config: Starcoder2Config, layer_idx: int):\n         self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.norm_epsilon)\n         self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.norm_epsilon)\n \n-    # Copied from transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer.forward\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -642,7 +636,6 @@ def forward(\n     \"The bare Starcoder2 Model outputting raw hidden-states without any specific head on top.\",\n     STARCODER2_START_DOCSTRING,\n )\n-# Copied from transformers.models.qwen2.modeling_qwen2.Qwen2PreTrainedModel with Qwen2->Starcoder2\n class Starcoder2PreTrainedModel(PreTrainedModel):\n     config_class = Starcoder2Config\n     base_model_prefix = \"model\"\n@@ -760,14 +753,15 @@ def __init__(self, config: Starcoder2Config):\n         self.vocab_size = config.vocab_size\n \n         self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n-        self.embedding_dropout = config.embedding_dropout\n         self.layers = nn.ModuleList(\n             [Starcoder2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self._attn_implementation = config._attn_implementation\n         self.norm = nn.LayerNorm(config.hidden_size, eps=config.norm_epsilon)\n         self.rotary_emb = Starcoder2RotaryEmbedding(config=config)\n+\n         self.gradient_checkpointing = False\n+        self.embedding_dropout = config.embedding_dropout\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -904,7 +898,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,\n@@ -981,7 +974,6 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.mistral.modeling_mistral.MistralModel._prepare_4d_causal_attention_mask_with_cache_position with Mistral->Starcoder2\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,\n@@ -1049,7 +1041,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n-# Copied from transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM with QWEN2->STARCODER2,Qwen2->Starcoder2\n class Starcoder2ForCausalLM(Starcoder2PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n@@ -1082,7 +1073,6 @@ def get_decoder(self):\n \n     @add_start_docstrings_to_model_forward(STARCODER2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n-    # Ignore copy\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -1097,6 +1087,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         num_logits_to_keep: int = 0,\n+        **loss_kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1117,8 +1108,8 @@ def forward(\n         ```python\n         >>> from transformers import AutoTokenizer, Starcoder2ForCausalLM\n \n-        >>> model = Starcoder2ForCausalLM.from_pretrained(\"bigcode/starcoder2-7b\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoder2-7b\")\n+        >>> model = Starcoder2ForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n+        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n \n         >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n         >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n@@ -1155,18 +1146,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Ensure tensors are on the same device\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]\n@@ -1196,7 +1176,6 @@ def forward(\n     \"\"\",\n     STARCODER2_START_DOCSTRING,\n )\n-# Copied from transformers.models.llama.modeling_llama.LlamaForSequenceClassification with Llama->Starcoder2, LLAMA->STARCODER2\n class Starcoder2ForSequenceClassification(Starcoder2PreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1293,7 +1272,6 @@ def forward(\n     \"\"\",\n     STARCODER2_START_DOCSTRING,\n )\n-# Copied from transformers.models.llama.modeling_llama.LlamaForTokenClassification with Llama->Starcoder2, LLAMA->STARCODER2\n class Starcoder2ForTokenClassification(Starcoder2PreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "b323a3ce9e4d5b1148789a27e3923d3964c3240f",
            "filename": "src/transformers/models/starcoder2/modular_starcoder2.py",
            "status": "added",
            "additions": 573,
            "deletions": 0,
            "changes": 573,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e90b99ed916300b80bac9db793f2a96b2a87122/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e90b99ed916300b80bac9db793f2a96b2a87122/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py?ref=4e90b99ed916300b80bac9db793f2a96b2a87122",
            "patch": "@@ -0,0 +1,573 @@\n+# coding=utf-8\n+# Copyright 2024 BigCode and the HuggingFace Inc. team. All rights reserved.\n+#\n+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n+# and OPT implementations in this library. It has been modified from its\n+# original forms to accommodate minor architectural differences compared\n+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch Starcoder2 model.\"\"\"\n+\n+import math\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n+from ...modeling_outputs import (\n+    BaseModelOutputWithPast,\n+)\n+from ...utils import (\n+    add_start_docstrings_to_model_forward,\n+    is_flash_attn_2_available,\n+    is_flash_attn_greater_or_equal_2_10,\n+    logging,\n+)\n+from ..llama.modeling_llama import (\n+    LlamaForSequenceClassification,\n+    LlamaForTokenClassification,\n+    LlamaRotaryEmbedding,\n+    apply_rotary_pos_emb,\n+    repeat_kv,\n+)\n+from ..qwen2.modeling_qwen2 import Qwen2DecoderLayer, Qwen2ForCausalLM, Qwen2Model, Qwen2PreTrainedModel\n+from .configuration_starcoder2 import Starcoder2Config\n+\n+\n+if is_flash_attn_2_available():\n+    from ...modeling_flash_attention_utils import _flash_attention_forward\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+_CONFIG_FOR_DOC = \"Starcoder2Config\"\n+_CHECKPOINT_FOR_DOC = \"bigcode/starcoder2-7b\"\n+\n+\n+class Starcoder2RotaryEmbedding(LlamaRotaryEmbedding):\n+    pass\n+\n+\n+class Starcoder2MLP(nn.Module):\n+    def __init__(self, config: Starcoder2Config):\n+        super().__init__()\n+        embed_dim = config.hidden_size\n+        self.c_fc = nn.Linear(embed_dim, config.intermediate_size, bias=config.use_bias)\n+        self.c_proj = nn.Linear(config.intermediate_size, embed_dim, bias=config.use_bias)\n+        self.act = ACT2FN[config.hidden_act]\n+        self.residual_dropout = config.residual_dropout\n+\n+    def forward(self, hidden_states: Optional[Tuple[torch.FloatTensor]]) -> torch.FloatTensor:\n+        hidden_states = self.c_fc(hidden_states)\n+        hidden_states = self.act(hidden_states)\n+        hidden_states = self.c_proj(hidden_states)\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.residual_dropout, training=self.training)\n+        return hidden_states\n+\n+\n+class Starcoder2Attention(nn.Module):\n+    \"\"\"\n+    Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer\n+    and \"Generating Long Sequences with Sparse Transformers\".\n+    \"\"\"\n+\n+    def __init__(self, config: Starcoder2Config, layer_idx: Optional[int] = None):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        if layer_idx is None:\n+            logger.warning_once(\n+                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n+                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n+\n+        self.hidden_size = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = self.hidden_size // self.num_heads\n+        self.num_key_value_heads = config.num_key_value_heads\n+        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n+        self.rope_theta = config.rope_theta\n+        self.use_bias = config.use_bias\n+        self.is_causal = True\n+        self.attention_dropout = config.attention_dropout\n+        self.residual_dropout = config.residual_dropout\n+\n+        if (self.head_dim * self.num_heads) != self.hidden_size:\n+            raise ValueError(\n+                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n+                f\" and `num_heads`: {self.num_heads}).\"\n+            )\n+        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=self.use_bias)\n+        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=self.use_bias)\n+        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=self.use_bias)\n+        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=self.use_bias)\n+\n+        self.rotary_emb = Starcoder2RotaryEmbedding(config=self.config)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        # repeat k/v heads if n_kv_heads < n_heads\n+        key_states = repeat_kv(key_states, self.num_key_value_groups)\n+        value_states = repeat_kv(value_states, self.num_key_value_groups)\n+\n+        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n+        if attention_mask is not None:  # no matter the length, we just slice it\n+            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+            attn_weights += causal_mask\n+\n+        # upcast attention to fp32\n+        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n+        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n+        attn_output = torch.matmul(attn_weights, value_states)\n+\n+        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n+            raise ValueError(\n+                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n+                f\" {attn_output.size()}\"\n+            )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n+\n+        attn_output = self.o_proj(attn_output)\n+        attn_output = nn.functional.dropout(attn_output, p=self.residual_dropout, training=self.training)\n+\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights, past_key_value\n+\n+\n+class Starcoder2FlashAttention2(Starcoder2Attention):\n+    \"\"\"\n+    Starcoder2 flash attention module. This module inherits from `Starcoder2Attention` as the weights of the module stays\n+    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n+    flash attention and deal with padding tokens in case the input contains any of them.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+\n+        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n+        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+    ):\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        # repeat k/v heads if n_kv_heads < n_heads\n+        key_states = repeat_kv(key_states, self.num_key_value_groups)\n+        value_states = repeat_kv(value_states, self.num_key_value_groups)\n+        dropout_rate = 0.0 if not self.training else self.attention_dropout\n+\n+        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n+        # therefore the input hidden states gets silently casted in float32. Hence, we need\n+        # cast them back in float16 just to be sure everything works as expected.\n+        input_dtype = query_states.dtype\n+        if input_dtype == torch.float32:\n+            if torch.is_autocast_enabled():\n+                target_dtype = torch.get_autocast_gpu_dtype()\n+            # Handle the case where the model is quantized\n+            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n+                target_dtype = self.config._pre_quantization_dtype\n+            else:\n+                target_dtype = self.q_proj.weight.dtype\n+\n+            logger.warning_once(\n+                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n+                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n+                f\" {target_dtype}.\"\n+            )\n+\n+            query_states = query_states.to(target_dtype)\n+            key_states = key_states.to(target_dtype)\n+            value_states = value_states.to(target_dtype)\n+\n+        # Reshape to the expected shape for Flash Attention\n+        query_states = query_states.transpose(1, 2)\n+        key_states = key_states.transpose(1, 2)\n+        value_states = value_states.transpose(1, 2)\n+\n+        attn_output = _flash_attention_forward(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            q_len,\n+            position_ids=position_ids,\n+            dropout=dropout_rate,\n+            sliding_window=getattr(self.config, \"sliding_window\", None),\n+            is_causal=self.is_causal,\n+            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+        )\n+\n+        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        attn_output = nn.functional.dropout(attn_output, p=self.residual_dropout, training=self.training)\n+\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights, past_key_value\n+\n+\n+class Starcoder2SdpaAttention(Starcoder2Attention):\n+    \"\"\"\n+    Starcoder2 attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n+    `Starcoder2Attention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n+    SDPA API.\n+    \"\"\"\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        if output_attentions:\n+            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n+            logger.warning_once(\n+                \"Starcoder2Model is using Starcoder2SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n+                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+            )\n+\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        key_states = repeat_kv(key_states, self.num_key_value_groups)\n+        value_states = repeat_kv(value_states, self.num_key_value_groups)\n+\n+        causal_mask = attention_mask\n+        if attention_mask is not None:  # no matter the length, we just slice it\n+            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+\n+        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n+        if query_states.device.type == \"cuda\" and attention_mask is not None:\n+            query_states = query_states.contiguous()\n+            key_states = key_states.contiguous()\n+            value_states = value_states.contiguous()\n+\n+        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+        # # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n+        is_causal = True if causal_mask is None and q_len > 1 else False\n+\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attn_mask=causal_mask,\n+            dropout_p=self.attention_dropout if self.training else 0.0,\n+            is_causal=is_causal,\n+        )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n+\n+        attn_output = self.o_proj(attn_output)\n+        # The difference with Mistral is that here it uses dropout\n+        attn_output = nn.functional.dropout(attn_output, p=self.residual_dropout, training=self.training)\n+\n+        return attn_output, None, past_key_value\n+\n+\n+STARCODER2_ATTENTION_CLASSES = {\n+    \"eager\": Starcoder2Attention,\n+    \"flash_attention_2\": Starcoder2FlashAttention2,\n+    \"sdpa\": Starcoder2SdpaAttention,\n+}\n+\n+\n+class Starcoder2DecoderLayer(Qwen2DecoderLayer, nn.Module):\n+    def __init__(self, config: Starcoder2Config, layer_idx: int):\n+        nn.Module.__init__(self)\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = STARCODER2_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx)\n+\n+        self.mlp = Starcoder2MLP(config)\n+\n+        self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.norm_epsilon)\n+        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.norm_epsilon)\n+\n+\n+class Starcoder2PreTrainedModel(Qwen2PreTrainedModel):\n+    pass\n+\n+\n+STARCODER2_INPUTS_DOCSTRING = None  # will be automatically redefined\n+\n+\n+class Starcoder2Model(Qwen2Model):\n+    \"\"\"\n+    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`Starcoder2DecoderLayer`]\n+\n+    Args:\n+        config: Starcoder2Config\n+    \"\"\"\n+\n+    def __init__(self, config: Starcoder2Config):\n+        super().__init__(config)\n+        self.embedding_dropout = config.embedding_dropout\n+        self.norm = nn.LayerNorm(config.hidden_size, eps=config.norm_epsilon)\n+\n+    @add_start_docstrings_to_model_forward(STARCODER2_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        # kept for BC (non `Cache` `past_key_values` inputs)\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = self._update_causal_mask(\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        )\n+\n+        hidden_states = inputs_embeds\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.embedding_dropout, training=self.training)\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+        next_decoder_cache = None\n+\n+        for decoder_layer in self.layers:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    decoder_layer.__call__,\n+                    hidden_states,\n+                    causal_mask,\n+                    position_ids,\n+                    past_key_values,\n+                    output_attentions,\n+                    use_cache,\n+                    cache_position,\n+                    position_embeddings,\n+                )\n+            else:\n+                layer_outputs = decoder_layer(\n+                    hidden_states,\n+                    attention_mask=causal_mask,\n+                    position_ids=position_ids,\n+                    past_key_value=past_key_values,\n+                    output_attentions=output_attentions,\n+                    use_cache=use_cache,\n+                    cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if use_cache:\n+                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = next_cache.to_legacy_cache()\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=next_cache,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+        )\n+\n+\n+class Starcoder2ForCausalLM(Qwen2ForCausalLM):\n+    pass\n+\n+\n+class Starcoder2ForSequenceClassification(LlamaForSequenceClassification):\n+    pass\n+\n+\n+class Starcoder2ForTokenClassification(LlamaForTokenClassification):\n+    pass"
        },
        {
            "sha": "8d6c6782a572f44e1eb71e709a67e20cf2176e13",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 39,
            "deletions": 13,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e90b99ed916300b80bac9db793f2a96b2a87122/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e90b99ed916300b80bac9db793f2a96b2a87122/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=4e90b99ed916300b80bac9db793f2a96b2a87122",
            "patch": "@@ -145,45 +145,69 @@ def is_call_to_super(node, func_name):\n     )\n \n \n+def get_full_attribute_name(node: cst.Attribute | cst.Name) -> str | None:\n+    \"\"\"Get the full name of an Attribute or Name node (e.g. `\"nn.Module\"` for an Attribute representing it). If the\n+    successive value of an Attribute are not Name nodes, return `None`.\"\"\"\n+    if m.matches(node, m.Name()):\n+        return node.value\n+    elif m.matches(node, m.Attribute()):\n+        if not m.matches(node.attr, m.Name()):\n+            return None\n+        name = node.attr.value\n+        new_node = node.value\n+        while m.matches(new_node, m.Attribute()):\n+            if not m.matches(new_node.attr, m.Name()):\n+                return None\n+            name = new_node.attr.value + \".\" + name\n+            new_node = new_node.value\n+        if not m.matches(new_node, m.Name()):\n+            return None\n+        return new_node.value + \".\" + name\n+    return None\n+\n+\n # Transformer class to replace ClassB.call_to_method and ClassB().call_to_method with super().call_to_method\n class ReplaceMethodCallTransformer(cst.CSTTransformer):\n     def __init__(self, all_bases: Set[str]):\n         self.all_bases = all_bases\n \n     def leave_Attribute(self, original_node: cst.Attribute, updated_node: cst.Attribute) -> cst.CSTNode:\n-        # Handle ClassB.call_to_method\n+        # Handle ClassB.call_to_method or module.classB.call_to_method\n         if (\n-            m.matches(original_node.value, m.Name())\n-            and original_node.value.value in self.all_bases\n+            m.matches(original_node.value, m.Name() | m.Attribute())\n+            and get_full_attribute_name(original_node.value) in self.all_bases\n             and m.matches(original_node.attr, m.Name())\n         ):\n             # Replace with super().call_to_method\n             return updated_node.with_changes(\n                 value=cst.Call(cst.Name(\"super\")),\n             )\n-        # Handle ClassB().call_to_method\n+        # Handle ClassB().call_to_method or module.ClassB().call_to_method\n         elif (\n             m.matches(original_node.value, m.Call())\n-            and m.matches(original_node.value.func, m.Name())\n-            and original_node.value.func.value in self.all_bases\n+            and m.matches(original_node.value.func, m.Name() | m.Attribute())\n+            and get_full_attribute_name(original_node.value.func) in self.all_bases\n             and m.matches(original_node.attr, m.Name())\n         ):\n             # Replace with super().call_to_method\n-            return updated_node.with_changes(func=cst.Attribute(value=cst.Call(func=cst.Name(\"super\"))))\n+            return updated_node.with_changes(value=cst.Call(cst.Name(\"super\")))\n         return updated_node\n \n     def leave_Call(self, original_node: cst.Call, updated_node: cst.Call) -> cst.CSTNode:\n         # Check if the function being called is of the form ClassB().func_a or ClassB.func_a\n         if m.matches(original_node.func, m.Attribute()) and (\n-            # Match ClassB().func_a(...)\n+            # Match ClassB().func_a(...) or module\n             (\n                 m.matches(original_node.func.value, m.Call())\n-                and m.matches(original_node.func.value.func, m.Name())\n-                and original_node.func.value.func.value in self.all_bases\n+                and m.matches(original_node.func.value.func, m.Name() | m.Attribute())\n+                and get_full_attribute_name(original_node.func.value.func) in self.all_bases\n             )\n             or\n             # Match ClassB.func_a(...)\n-            (m.matches(original_node.func.value, m.Name()) and original_node.func.value.value in self.all_bases)\n+            (\n+                m.matches(original_node.func.value, m.Name() | m.Attribute())\n+                and get_full_attribute_name(original_node.func.value) in self.all_bases\n+            )\n         ):\n             # Check if the first argument is 'self', and remove it\n             if len(original_node.args) > 0 and m.matches(original_node.args[0].value, m.Name(\"self\")):\n@@ -860,7 +884,9 @@ def replace_class_node(mapper: ModelFileMapper, class_node: cst.ClassDef, rename\n                                                                             |               self.post_init()\n                                                                             |     ```\n     \"\"\"\n-    all_bases = [k.value.value for k in class_node.bases]\n+    all_bases = [get_full_attribute_name(k.value) for k in class_node.bases]\n+    if any(base is None for base in all_bases):\n+        raise ValueError(f\"Could not parse the name of the bases for {class_node.name.value}\")\n \n     original_node = mapper.classes[renamed_super_class]\n     original_methods = {\n@@ -1496,7 +1522,7 @@ def save_modeling_file(modular_file, converted_file):\n     parser = argparse.ArgumentParser()\n     parser.add_argument(\n         \"--files_to_parse\",\n-        default=[\"src/transformers/models/gemma2/modular_gemma2.py\"],\n+        default=[\"src/transformers/models/starcoder2/modular_starcoder2.py\"],\n         nargs=\"+\",\n         help=\"A list of `modular_xxxx` files that should be converted to single model file\",\n     )"
        }
    ],
    "stats": {
        "total": 709,
        "additions": 643,
        "deletions": 66
    }
}