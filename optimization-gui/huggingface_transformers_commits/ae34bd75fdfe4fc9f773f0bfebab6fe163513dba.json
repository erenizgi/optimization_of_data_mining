{
    "author": "guangy10",
    "message": "Use public export API on torch 2.5 and future (#36781)\n\nCo-authored-by: Guang Yang <guangyang@fb.com>\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
    "sha": "ae34bd75fdfe4fc9f773f0bfebab6fe163513dba",
    "files": [
        {
            "sha": "b0a7f904c961c45892020ca454fdd9c174d168b9",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 20,
            "deletions": 11,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/ae34bd75fdfe4fc9f773f0bfebab6fe163513dba/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ae34bd75fdfe4fc9f773f0bfebab6fe163513dba/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=ae34bd75fdfe4fc9f773f0bfebab6fe163513dba",
            "patch": "@@ -19,7 +19,7 @@\n \n if is_torch_available():\n     from transformers import PreTrainedModel, StaticCache\n-    from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_3\n+    from transformers.pytorch_utils import is_torch_greater_or_equal, is_torch_greater_or_equal_than_2_3\n \n \n class TorchExportableModuleWithStaticCache(torch.nn.Module):\n@@ -193,7 +193,6 @@ def convert_and_export_with_cache(\n     Returns:\n         Exported program (`torch.export.ExportedProgram`): The exported program generated via `torch.export`.\n     \"\"\"\n-\n     if not is_torch_greater_or_equal_than_2_3:\n         raise ImportError(\"torch >= 2.3 is required.\")\n \n@@ -208,15 +207,25 @@ def convert_and_export_with_cache(\n             example_cache_position if example_cache_position is not None else torch.tensor([0], dtype=torch.long)\n         )\n \n-        # Due to issue https://github.com/pytorch/pytorch/issues/128394, we need to switch to use an internal\n-        # export API and pre_dispatch=False. Switch to use the public API once the issue is included in 2.5 release.\n-        exported_program = torch.export._trace._export(\n-            TorchExportableModuleWithStaticCache(model),\n-            args=(example_input_ids,),\n-            kwargs={\"cache_position\": example_cache_position},\n-            pre_dispatch=False,\n-            strict=True,\n-        )\n+        if is_torch_greater_or_equal(\"2.5.0\"):\n+            exported_program = torch.export.export(\n+                TorchExportableModuleWithStaticCache(model),\n+                args=(example_input_ids,),\n+                kwargs={\"cache_position\": example_cache_position},\n+                strict=True,\n+            )\n+        else:\n+            # We have to keep this path for BC.\n+            #\n+            # Due to issue https://github.com/pytorch/pytorch/issues/128394, we need to switch to use an internal\n+            # export API and pre_dispatch=False. Switch to use the public API once the issue is included in 2.5 release.\n+            exported_program = torch.export._trace._export(\n+                TorchExportableModuleWithStaticCache(model),\n+                args=(example_input_ids,),\n+                kwargs={\"cache_position\": example_cache_position},\n+                pre_dispatch=False,\n+                strict=True,\n+            )\n         return exported_program\n \n "
        }
    ],
    "stats": {
        "total": 31,
        "additions": 20,
        "deletions": 11
    }
}