{
    "author": "SunMarc",
    "message": "[Trainer] [Breaking change] `use_cache` default to `False` (#41585)\n\n* use_cache default to `False` when training\n\n* style\n\n* Fix comment\n\n* add checks\n\n* style\n\n* set\n\n* switch",
    "sha": "bdbc2d037b1751c71b30a488f0a168deabef9a90",
    "files": [
        {
            "sha": "bab3230b680fc94418ba6d7d68b411a1471eeea2",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bdbc2d037b1751c71b30a488f0a168deabef9a90/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bdbc2d037b1751c71b30a488f0a168deabef9a90/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=bdbc2d037b1751c71b30a488f0a168deabef9a90",
            "patch": "@@ -738,6 +738,10 @@ def __init__(\n         self._train_batch_size = args.train_batch_size\n         self._created_lr_scheduler = False\n \n+        # Set use_cache for the model\n+        if getattr(self.model, \"config\", None) is not None:\n+            self.model.config.use_cache = self.args.use_cache\n+\n         # very last\n         self._memory_tracker.stop_and_update_metrics()\n "
        },
        {
            "sha": "3c541d2519d404e0ea31d271223d793130226635",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/bdbc2d037b1751c71b30a488f0a168deabef9a90/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bdbc2d037b1751c71b30a488f0a168deabef9a90/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=bdbc2d037b1751c71b30a488f0a168deabef9a90",
            "patch": "@@ -752,6 +752,10 @@ class TrainingArguments:\n             Whether or not to average tokens across devices. If enabled, will use all_reduce to synchronize\n             num_tokens_in_batch for precise loss calculation. Reference:\n             https://github.com/huggingface/transformers/issues/34242\n+\n+        use_cache (`bool`, *optional*, defaults to `False`):\n+            Whether or not to enable cache for the model. For training, this is usually not needed apart from some PEFT methods that uses `past_key_values`.\n+\n     \"\"\"\n \n     # Sometimes users will pass in a `str` repr of a dict in the CLI\n@@ -1382,6 +1386,13 @@ class TrainingArguments:\n         },\n     )\n \n+    use_cache: bool = field(\n+        default=False,\n+        metadata={\n+            \"help\": \"Whether or not to use cache for the model For training, this is usually not needed apart from some PEFT methods that uses `past_key_values`.\"\n+        },\n+    )\n+\n     def __post_init__(self):\n         # Set default output_dir if not provided\n         if self.output_dir is None:"
        }
    ],
    "stats": {
        "total": 15,
        "additions": 15,
        "deletions": 0
    }
}