{
    "author": "yiding",
    "message": "[flax/mistral] support sliding_window: null in config (#37402)\n\nflax/mistral: Allow sliding_window to be set to none",
    "sha": "cceab972bafde7399a02ffcbf824038ead443331",
    "files": [
        {
            "sha": "f50c55bbd3b1f06df0c1b5313ffac369b7e6463e",
            "filename": "src/transformers/models/mistral/modeling_flax_mistral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/cceab972bafde7399a02ffcbf824038ead443331/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_flax_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cceab972bafde7399a02ffcbf824038ead443331/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_flax_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_flax_mistral.py?ref=cceab972bafde7399a02ffcbf824038ead443331",
            "patch": "@@ -240,8 +240,8 @@ def setup(self):\n         self.v_proj = nn.Dense(self.num_key_value_heads * self.head_dim, use_bias=False, dtype=self.dtype)\n         self.o_proj = nn.Dense(self.hidden_size, use_bias=False, dtype=self.dtype)\n         casual_mask = make_causal_mask(jnp.ones((1, config.max_position_embeddings), dtype=\"bool\"), dtype=\"bool\")\n-        self.causal_mask = jnp.triu(casual_mask, k=-config.sliding_window)\n-        self.rotary_emb = FlaxMistralRotaryEmbedding(config, dtype=self.dtype)\n+        self.causal_mask = jnp.triu(casual_mask, k=-(config.sliding_window or 0))\n+        self.rotary_emb = FlaxMistralRotaryEmbedding(self.config, dtype=self.dtype)\n \n     def _split_heads(self, hidden_states, num_heads):\n         return hidden_states.reshape(hidden_states.shape[:2] + (num_heads, self.head_dim))"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}