{
    "author": "zucchini-nlp",
    "message": "[flash attn 3] bring back flags (#39294)\n\n* flash attn 3 flag\n\n* fix copies",
    "sha": "0687d481e2c71544501ef9cb3eef795a6e79b1de",
    "files": [
        {
            "sha": "d9434e610b7c46953ef73185b430e2bd859737f7",
            "filename": "src/transformers/models/aimv2/modeling_aimv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -445,6 +445,7 @@ class Aimv2PreTrainedModel(PreTrainedModel):\n     ]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "25156fbd1a4f20a49cbe381031bfe4594a08f03e",
            "filename": "src/transformers/models/aimv2/modular_aimv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -442,6 +442,7 @@ class Aimv2PreTrainedModel(PreTrainedModel):\n     ]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "a7902c75eccca1b620ed467fd4331116a850d621",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -314,6 +314,7 @@ class ArceePreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"ArceeDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "1212308eb1958507d734abdd3860d22c6d57a859",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -658,6 +658,7 @@ class AriaPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"AriaDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "a6baf638143b4151caea6421fde4c406238f2cb7",
            "filename": "src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -376,6 +376,7 @@ class ASTPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "7b6e8efb1f26e0c976bd9abb87beb119d25f713e",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -94,6 +94,7 @@ class AyaVisionPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_quantized_cache = False\n     _supports_static_cache = False"
        },
        {
            "sha": "eba5449a396849609b775e0975a134ef6430f238",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -1040,6 +1040,7 @@ class BambaPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"BambaDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True  # Note: only supports HybridMambaAttentionDynamicCache\n     _is_stateful = True"
        },
        {
            "sha": "a80796cf6e437ebb7404eec08cd39b8fe32927fb",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -811,6 +811,7 @@ class BambaPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"BambaDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True  # Note: only supports HybridMambaAttentionDynamicCache\n     _is_stateful = True"
        },
        {
            "sha": "362f9d2ba76bad93888c58f5810b0297ef43f08b",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -356,6 +356,7 @@ class BarkPreTrainedModel(PreTrainedModel):\n     config_class = BarkConfig\n     supports_gradient_checkpointing = False\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\""
        },
        {
            "sha": "ba7e8603f2148ae807dde0aaebf2331a8e869836",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -494,6 +494,7 @@ class BartPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [r\"BartEncoderLayer\", r\"BartDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "a9fa3584d424fe2466bd60ba0b5462bfb6c59097",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -347,6 +347,7 @@ class BioGptPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"biogpt\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "3fc8205f75602660d1993b621eb362f2d32eefbc",
            "filename": "src/transformers/models/biogpt/modular_biogpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -172,6 +172,7 @@ class BioGptPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"biogpt\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "ff3a17998f765cfc7051a28123122135b817e804",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -309,6 +309,7 @@ class BitNetPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"BitNetDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "9d31049a17152a258774239dac13e354ff7e378b",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -464,6 +464,7 @@ class BlenderbotPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "930233413769ce37a33cd1711498e2199acd9b71",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -452,6 +452,7 @@ class BlenderbotSmallPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "80615fd9b940e00d624aa04471f62d0db1038adb",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -408,6 +408,7 @@ class Blip2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_attention_backend = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "4f699914b9db54f77d7083f8b5f2cd5d6118ae78",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -822,6 +822,7 @@ class ChameleonPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"ChameleonDecoderLayer\", \"ChameleonSwinDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_cache_class = True"
        },
        {
            "sha": "e453c1033989951da3e6a8460ac5b6ea61423324",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -429,6 +429,7 @@ class CLIPPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "f457b4e8ac6583404eef37ca9747649533ef109b",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -342,6 +342,7 @@ class CoherePreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"CohereDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "d2cb22132b67a570cc82c7af844198ef0c297a18",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -319,6 +319,7 @@ class Cohere2PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Cohere2DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "3d259212c50976d05c8b0308fc3706eccafe0c36",
            "filename": "src/transformers/models/colqwen2/modeling_colqwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -42,6 +42,7 @@ class ColQwen2PreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     _no_split_modules = []\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n "
        },
        {
            "sha": "881cb8af662ebe99aaa92ae867527367f232fda0",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -226,6 +226,7 @@ def __call__(\n \n class ColQwen2PreTrainedModel(ColPaliPreTrainedModel):\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n "
        },
        {
            "sha": "3d2690568058f496496401f40f3fb9e076d41788",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -367,6 +367,7 @@ class CsmPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"CsmDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     # does not because of Mimi codec model\n     # _supports_flex_attn = True"
        },
        {
            "sha": "15e7a48680111a68d96d87f340d93ab5298a9b37",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -130,6 +130,7 @@ class CsmPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"CsmDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     # does not because of Mimi codec model\n     # _supports_flex_attn = True"
        },
        {
            "sha": "bd3cc865aced6ab284cecb3d2813bf7018103644",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -506,6 +506,7 @@ class Data2VecAudioPreTrainedModel(PreTrainedModel):\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "2de816e80eeac97970b6aa438d539e722b12438b",
            "filename": "src/transformers/models/data2vec/modular_data2vec_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -140,6 +140,7 @@ class Data2VecAudioPreTrainedModel(PreTrainedModel, Wav2Vec2PreTrainedModel):\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "1dcf34a91c3eb6aa512f1f5a1a89dc8fd5a11041",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -813,6 +813,7 @@ class DbrxPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"DbrxBlock\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True"
        },
        {
            "sha": "d686c8297b8d914ef1ee67ff3d90bf54d503e55f",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -495,6 +495,7 @@ class DeepseekV3PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"DeepseekV3DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "90b678466c490f3f0d7aa9e42f08d7968bcbe5b2",
            "filename": "src/transformers/models/deit/modeling_deit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -443,6 +443,7 @@ class DeiTPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"DeiTLayer\"]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "38bc0715fc6dac36ac3e1e4c8acd0eecbf39dae1",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -65,6 +65,7 @@ class DiaPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "18512856dd43c44cf5b3eaa249d99e16d6075dd0",
            "filename": "src/transformers/models/dia/modular_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -60,6 +60,7 @@ class DiaPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "1e19c226e2ce0b44ff6267eaa8e37c9ed1f129b6",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -531,6 +531,7 @@ class DiffLlamaPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"DiffLlamaDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = False\n     _supports_cache_class = True"
        },
        {
            "sha": "6b98e3fa8d6523779c113a84e781a2ab752f3044",
            "filename": "src/transformers/models/dinov2/modeling_dinov2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -487,6 +487,7 @@ class Dinov2PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Dinov2Layer\"]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "17dfafd3197e212f6c256833f8525a33c9ebad51",
            "filename": "src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -505,6 +505,7 @@ class Dinov2WithRegistersPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Dinov2WithRegistersLayer\"]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "a047a676be09608933042051ca401c354e6aa43c",
            "filename": "src/transformers/models/distilbert/modeling_distilbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -576,6 +576,7 @@ class DistilBertPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"distilbert\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n \n     def _init_weights(self, module: nn.Module):"
        },
        {
            "sha": "8aaf546481500817954c9dcadc74cf8dcf5e9bbd",
            "filename": "src/transformers/models/doge/modeling_doge.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -492,6 +492,7 @@ class DogePreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"DogeDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = False\n+    _supports_flash_attn_3 = False\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True\n@@ -503,7 +504,6 @@ class DogePreTrainedModel(PreTrainedModel):\n         \"hidden_states\": DogeDecoderLayer,\n         \"attentions\": DogeAttention,\n     }\n-    _supports_flash_attn_3 = False\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "0d2409f8e8cb38fd99e8eb7e87914a18b3acccd7",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -415,6 +415,7 @@ class Dots1PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Dots1DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "4d0a072dcc2d6640188c4a1156115e0833febcd6",
            "filename": "src/transformers/models/dpt/modeling_dpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -810,6 +810,7 @@ class DPTPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "c3dd0c1870b9aff01a8bf008bd86d3882352c4af",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -927,6 +927,7 @@ class Emu3VQVAE(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n     _no_split_modules = [\n@@ -1096,6 +1097,7 @@ class Emu3PreTrainedModel(PreTrainedModel):\n     ]\n     _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_cache_class = True"
        },
        {
            "sha": "6df946949819d15519f97ad91b7b5b9b2a6f2813",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -679,6 +679,7 @@ class Emu3VQVAE(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n     _no_split_modules = ["
        },
        {
            "sha": "7bf808db2222636ac23253a29d4a9dc2729928c7",
            "filename": "src/transformers/models/encoder_decoder/modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -79,6 +79,7 @@ class EncoderDecoderModel(PreTrainedModel, GenerationMixin):\n     supports_gradient_checkpointing = True\n     _supports_param_buffer_assignment = False\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n \n     def __init__("
        },
        {
            "sha": "025d28350fffaf3ba4c2a82922057b60e3ef9b35",
            "filename": "src/transformers/models/eomt/modeling_eomt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -1002,6 +1002,7 @@ class EomtPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"EomtLayer\"]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n \n     def _init_weights(self, module: nn.Module) -> None:\n         std = self.config.initializer_range"
        },
        {
            "sha": "aeb562fc26b4751d8f7e1eed5ec584dada929084",
            "filename": "src/transformers/models/eomt/modular_eomt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -374,6 +374,7 @@ class EomtPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"EomtLayer\"]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n \n     def _init_weights(self, module: nn.Module) -> None:\n         std = self.config.initializer_range"
        },
        {
            "sha": "f448be910fe65d76f6340ad5b8b04fcfacbbeb82",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -739,6 +739,7 @@ class EsmPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"EsmLayer\", \"EsmFoldTriangularSelfAttentionBlock\", \"EsmEmbeddings\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n \n     # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights with BertLMPredictionHead->EsmLMHead\n     def _init_weights(self, module):"
        },
        {
            "sha": "78792318a16ad1cf0091ea8ec4ccf22eabbe0711",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -659,6 +659,7 @@ class FalconPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"FalconDecoderLayer\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True"
        },
        {
            "sha": "71c06ce6b356d0281e07cf884791dc425facb7ad",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -1160,6 +1160,7 @@ class FalconH1PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"FalconH1DecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True  # Note: only supports FalconHybridMambaAttentionDynamicCache\n     _is_stateful = True"
        },
        {
            "sha": "8bfbe328a26253d50be324cd0339fa40baf5820a",
            "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -935,6 +935,7 @@ class FalconH1PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"FalconH1DecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True  # Note: only supports FalconHybridMambaAttentionDynamicCache\n     _is_stateful = True"
        },
        {
            "sha": "8c445571ac34e2bd831bc23886dfa74d98e591d4",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -38,6 +38,7 @@ class FuyuPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_attention_backend = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _no_split_modules = []"
        },
        {
            "sha": "f79f62977d18501a987eccee291672bff9fb7dd8",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -311,6 +311,7 @@ class GemmaPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"GemmaDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "14fff44d8a19ff7d296147cc3c4b764b9ae6189a",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -341,6 +341,7 @@ class Gemma2PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Gemma2DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "b8f4a88da3a6507a7a6de6e4fa9cc3442cdede55",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -431,6 +431,7 @@ class Gemma3PreTrainedModel(PreTrainedModel):\n     ]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "1e3641ddc6a23e07b04e576ae7b541756d2ba940",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -1487,6 +1487,7 @@ class Gemma3nPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Gemma3nTextDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "28e629529257c6bd06a725919e9b3f0ba45cd693",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -328,6 +328,7 @@ class GlmPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"GlmDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "ceb9f2d4dd1c86515bb48f367466c8c6e510ced9",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -332,6 +332,7 @@ class Glm4PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Glm4DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "fcf36a826099f040fab38b756bdf25e172c9382b",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -405,6 +405,7 @@ class Glm4vPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Glm4vTextDecoderLayer\", \"Glm4vVisionBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_static_cache = True"
        },
        {
            "sha": "d5cdeff2f33b6876359729b169ef1e4b20061333",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -281,6 +281,7 @@ class GotOcr2PreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "00d21c7d603a63bb45f103819c7a297220859885",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -560,6 +560,7 @@ class GPT2PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"GPT2Block\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_attention_backend = True\n     _supports_cache_class = True"
        },
        {
            "sha": "711c29a0294c0447957b45bec9f144d18c3655c5",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -656,6 +656,7 @@ class GPTBigCodePreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"GPTBigCodeBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n \n     def __init__(self, *inputs, **kwargs):"
        },
        {
            "sha": "688f368015e97766cc04b2c8ee59e849b6b9dd5e",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -491,6 +491,7 @@ class GPTNeoPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"GPTNeoBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = False  # TODO: needs a HybridCache"
        },
        {
            "sha": "35dc1963d86f8464e02f11484153c524abfd3b44",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -361,6 +361,7 @@ class GPTNeoXPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"GPTNeoXLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "a8faccb5d80c2ad8a13f03eaa6a6336ef5f425f9",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -489,6 +489,7 @@ class GPTJPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"GPTJBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "e94ef030f07394be27c7dee51b009171246b15c8",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -306,6 +306,7 @@ class GranitePreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"GraniteDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "99ca5f50a1b7723598bf46b660435cd9e8ba8e43",
            "filename": "src/transformers/models/granite_speech/modeling_granite_speech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -284,6 +284,7 @@ class GraniteSpeechPreTrainedModel(PreTrainedModel):\n     config_class = GraniteSpeechConfig\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n \n     def _init_weights(self, module: nn.Module):"
        },
        {
            "sha": "c78c97ade3b93e1729f28970b74027e5ba649d85",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -593,6 +593,7 @@ class GraniteMoePreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"GraniteMoeDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True"
        },
        {
            "sha": "4160dd82e7e40d0bae4bcbf41985d931c17a1dfe",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -1166,6 +1166,7 @@ class GraniteMoeHybridPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"GraniteMoeHybridDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True"
        },
        {
            "sha": "5e10ed2552f7e9f03d9b2d59ab620a80f51288ee",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -511,6 +511,7 @@ class GraniteMoeSharedPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"GraniteMoeSharedDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True"
        },
        {
            "sha": "7cab557ac9396f8e23424c520da3591be1a617b4",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -313,6 +313,7 @@ class HeliumPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"HeliumDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "d828e0a49d57cf24d809508350b5f60cab319d9c",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -685,6 +685,7 @@ class HubertPreTrainedModel(PreTrainedModel):\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "5e0102c1d79522c91ce9e57a414877c1e6ec9f7c",
            "filename": "src/transformers/models/hubert/modular_hubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -130,6 +130,7 @@ class HubertPreTrainedModel(PreTrainedModel):\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "706584ccf3ba68e09c5518095900836176c07636",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -886,6 +886,7 @@ class IdeficsPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_static_cache = False  # IDEFICS cannot compile due to dynamic control flow when checking inputs\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "50d1489c2d77936716755edb06c309bb246e02d2",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -457,6 +457,7 @@ class Idefics2PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Idefics2VisionAttention\", \"Idefics2MLP\", \"Idefics2PerceiverLayer\", \"Idefics2DecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True\n@@ -494,7 +495,8 @@ def _init_weights(self, module):\n class Idefics2VisionTransformer(Idefics2PreTrainedModel):\n     config_class = Idefics2VisionConfig\n     _supports_sdpa = True\n-    _supports_flash_attention_2 = True\n+    _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n \n     def __init__(self, config: Idefics2VisionConfig):"
        },
        {
            "sha": "14381c6d68b8e05298b1f85ac11df169907fa1bf",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -474,6 +474,7 @@ class Idefics3PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Idefics3VisionAttention\", \"Idefics3DecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True\n@@ -505,7 +506,8 @@ def _init_weights(self, module):\n class Idefics3VisionTransformer(Idefics3PreTrainedModel):\n     config_class = Idefics3VisionConfig\n     _supports_sdpa = True\n-    _supports_flash_attention_2 = True\n+    _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n \n     def __init__(self, config: Idefics3VisionConfig):"
        },
        {
            "sha": "6c2df8d2bbe75ed86fcef0e69cf5663e015f91dd",
            "filename": "src/transformers/models/ijepa/modeling_ijepa.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -152,6 +152,7 @@ class IJepaPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"IJepaEmbeddings\", \"IJepaLayer\"]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "e3d0d79081d118b7ae0d491e8f3a2c796ed49305",
            "filename": "src/transformers/models/ijepa/modular_ijepa.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -95,6 +95,7 @@ class IJepaPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"IJepaEmbeddings\", \"IJepaLayer\"]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "afcc4c9bf149a7584bbc54c3674df209feb256a5",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -335,6 +335,7 @@ class InstructBlipPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_attention_backend = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "c9b96db20a0e149dd75ab61e436ec4c434049ca6",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -847,6 +847,7 @@ class InstructBlipVideoPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_attention_backend = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "9d7de418f920ef82ee0b4fff51091a530cb4433b",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -179,6 +179,7 @@ class InternVLVisionPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"InternVLVisionLayer\"]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n \n@@ -521,6 +522,7 @@ class InternVLPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "84edfe9e2ed82215ca3cbc73963e6bade62c3f6a",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -141,6 +141,7 @@ class InternVLVisionPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"InternVLVisionLayer\"]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "c635927c9e3808953aa41d87a656773f0b88146c",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -1070,6 +1070,7 @@ class JambaPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"JambaAttentionDecoderLayer\", \"JambaMambaDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True  # Note: only supports HybridMambaAttentionDynamicCache\n     _is_stateful = True"
        },
        {
            "sha": "ed8ea178744387e0c4558c90abd536e88413f06c",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -61,6 +61,7 @@ class JanusPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"LlamaDecoderLayer\", \"JanusVisionEncoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_cache_class = True"
        },
        {
            "sha": "37be985a425b9715d6aee75979b545ba21c11461",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -388,6 +388,7 @@ class JanusPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"LlamaDecoderLayer\", \"JanusVisionEncoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_cache_class = True"
        },
        {
            "sha": "acfe7dbd48ae7bb8e05b9266dd2b1a4cb776bf02",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -830,6 +830,7 @@ class JetMoePreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"JetMoeBlock\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n "
        },
        {
            "sha": "d9bfb7db132e8ee7229e3b057c147462de1b97d2",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -1152,6 +1152,7 @@ class Kosmos2PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Kosmos2VisionEncoderLayer\", \"Kosmos2TextBlock\"]\n     _supports_attention_backend = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "82fe496ea257c2acdd67c05dcb139b0ca7235b41",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -118,6 +118,7 @@ class KyutaiSpeechToTextPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"KyutaiSpeechToTextDecoderLayer\", \"MimiTransformerLayer\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     main_input_name = \"input_ids\""
        },
        {
            "sha": "37d6502e5f44b6e4f6fcefa4e2bbc56519b1e99d",
            "filename": "src/transformers/models/lightglue/modeling_lightglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -424,6 +424,7 @@ class LightGluePreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = False\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n \n     def _init_weights(self, module: nn.Module) -> None:"
        },
        {
            "sha": "beacacab02de7dd83c230a62960ddd8019f806f1",
            "filename": "src/transformers/models/lightglue/modular_lightglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -509,6 +509,7 @@ class LightGluePreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = False\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n \n     def _init_weights(self, module: nn.Module) -> None:"
        },
        {
            "sha": "cded4e58b392c731b0c8ee79d28d9ec72d1b3ecb",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -312,6 +312,7 @@ class LlamaPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"LlamaDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "62d80fa5f08f14f19161a0eaf1a54ea0b105df76",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -118,6 +118,7 @@ class LlavaPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "5d74255fe65b87f9df2a1ef5c34b540b0dcd5c08",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -229,6 +229,7 @@ class LlavaNextPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "37e11e262e8af263b7709de6b3772cf5aa5fb332",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -170,6 +170,7 @@ class LlavaNextVideoPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "d07e7c28b6d8ebbbe03410e2e3948b8f1e396ab3",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -283,6 +283,7 @@ class LlavaOnevisionPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "8b8ce850339a5add673f3ea30e3e592a6be6ec2c",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -530,6 +530,7 @@ class M2M100PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"M2M100EncoderLayer\", \"M2M100DecoderLayer\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "0d7fa3a7b9cc64b45d61333899969bb5c145d419",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -468,6 +468,7 @@ class MarianPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "361e1fabda9d5118d4d11884ee140eaa0ee44abe",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -498,6 +498,7 @@ class MBartPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MBartDecoderLayer\", \"MBartEncoderLayer\", \"MBartAttention\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "8f42a31521539628c97f76c54d79c9be7804d53f",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -1385,6 +1385,7 @@ class MimiPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"MimiDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_static_cache = True"
        },
        {
            "sha": "8379e54e952081a8abcb9efc744f2866fa4688b3",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -586,6 +586,7 @@ class MiniMaxPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"MiniMaxDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True  # Note: only supports MiniMaxCache"
        },
        {
            "sha": "8724a0b90449784309af7281f5be9edd2b4f350f",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -257,6 +257,7 @@ class MistralPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"MistralDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "9c9f10fde59a8882cf20d0355c11ba2b5bc7c413",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -183,6 +183,7 @@ class Mistral3PreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "c9e953b8c936855f344c6433ee971447f61afd09",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -386,6 +386,7 @@ class MixtralPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"MixtralDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "48732174105a41f4057e97a54429607fdb738884",
            "filename": "src/transformers/models/mlcd/modeling_mlcd.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -509,6 +509,7 @@ class MLCDPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"mlcd\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "58fd45c9b86521ce7ddcfda4dd9ff56834ac5ab7",
            "filename": "src/transformers/models/mlcd/modular_mlcd.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -443,6 +443,7 @@ class MLCDPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"mlcd\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "e2d70b6d9b1d752a7818e6610624676610f94be7",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -859,6 +859,7 @@ class MllamaPreTrainedModel(PreTrainedModel):\n     _supports_static_cache = False  # static cache cannot have different shapes for each layer\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_quantized_cache = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "19a8e68d0e655b9c3f98ecac2ff76d24abb27db1",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -562,6 +562,7 @@ class ModernBertPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ModernBertEmbeddings\", \"ModernBertEncoderLayer\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = False\n "
        },
        {
            "sha": "8cd853aa5cc323d66e6daab30337d5a54ae7182d",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -764,6 +764,7 @@ class ModernBertPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ModernBertEmbeddings\", \"ModernBertEncoderLayer\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = False\n "
        },
        {
            "sha": "ea888df2aab86d8e3e2f0e9498b0c1a805d58332",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -460,6 +460,7 @@ class MoonshinePreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MoonshineEncoderLayer\", \"MoonshineDecoderLayer\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_static_cache = True\n@@ -928,6 +929,10 @@ def forward(\n             the soundfile library (`pip install soundfile`). To prepare the array into\n             `input_values`, the [`AutoFeatureExtractor`] should be used for padding\n             and conversion into a tensor of type `torch.FloatTensor`.\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`):\n+            Indices of positions of each input sequence tokens in the position embeddings.\n+            Used to calculate the position embeddings up to `config.decoder_config.max_position_embeddings`\n+\n         Example:\n \n         ```python\n@@ -1046,6 +1051,9 @@ def forward(\n             the soundfile library (`pip install soundfile`). To prepare the array into\n             `input_values`, the [`AutoFeatureExtractor`] should be used for padding\n             and conversion into a tensor of type `torch.FloatTensor`.\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`):\n+            Indices of positions of each input sequence tokens in the position embeddings.\n+            Used to calculate the position embeddings up to `config.decoder_config.max_position_embeddings`\n \n         Example:\n "
        },
        {
            "sha": "f2ac060dc22fd9fa106c61555f439936ed4fc259",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -495,6 +495,7 @@ class MoonshinePreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MoonshineEncoderLayer\", \"MoonshineDecoderLayer\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_static_cache = True\n@@ -757,6 +758,10 @@ def forward(\n             the soundfile library (`pip install soundfile`). To prepare the array into\n             `input_values`, the [`AutoFeatureExtractor`] should be used for padding\n             and conversion into a tensor of type `torch.FloatTensor`.\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`):\n+            Indices of positions of each input sequence tokens in the position embeddings.\n+            Used to calculate the position embeddings up to `config.decoder_config.max_position_embeddings`\n+\n         Example:\n \n         ```python\n@@ -859,6 +864,9 @@ def forward(\n             the soundfile library (`pip install soundfile`). To prepare the array into\n             `input_values`, the [`AutoFeatureExtractor`] should be used for padding\n             and conversion into a tensor of type `torch.FloatTensor`.\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`):\n+            Indices of positions of each input sequence tokens in the position embeddings.\n+            Used to calculate the position embeddings up to `config.decoder_config.max_position_embeddings`\n \n         Example:\n "
        },
        {
            "sha": "a0cf3412900a4342d6e27cdf07dfdea7728d3d5b",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -809,6 +809,7 @@ class MoshiPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MoshiDecoderLayer\", \"MimiTransformerLayer\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     main_input_name = \"input_ids\"\n@@ -1658,6 +1659,7 @@ class MoshiForConditionalGeneration(MoshiPreTrainedModel, GenerationMixin):\n     main_input_name = \"input_ids\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n \n     def __init__(self, config: MoshiConfig):"
        },
        {
            "sha": "31161cb54a754700dd6d8a6873e4d72a699ab4c6",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -435,6 +435,7 @@ class MusicgenPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MusicgenDecoderLayer\", \"MusicgenAttention\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n@@ -1346,6 +1347,7 @@ class MusicgenForConditionalGeneration(PreTrainedModel, GenerationMixin):\n     main_input_name = \"input_ids\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "6fd14cb842128e6bcc7c50897755de043c433bc2",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -401,6 +401,7 @@ class MusicgenMelodyPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MusicgenMelodyDecoderLayer\", \"MusicgenMelodyAttention\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n@@ -1274,6 +1275,7 @@ class MusicgenMelodyForConditionalGeneration(PreTrainedModel, GenerationMixin):\n     main_input_name = \"input_ids\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "f84096445f453471760ec31039ce213a92fec0a9",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -590,6 +590,7 @@ class NemotronPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"NemotronDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True"
        },
        {
            "sha": "85a837ad3800dc8bfb377813e4e09e11855ad4df",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -291,6 +291,7 @@ class OlmoPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"OlmoDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "8f0aa68d8b03a722d37cf245e84d4acb446170ab",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -296,6 +296,7 @@ class Olmo2PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Olmo2DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "0408ceb62e5a5be408521030d7e9950b96fd891d",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -707,6 +707,7 @@ class OlmoePreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"OlmoeDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True"
        },
        {
            "sha": "33caa5a127b0285980803f6e9f7c48cafe221b8d",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -313,6 +313,7 @@ class OPTPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"OPTDecoderLayer\"]\n     _supports_attention_backend = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "847f31da71e2dbd84b757dabc286067565324379",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -117,6 +117,7 @@ class PaliGemmaPreTrainedModel(PreTrainedModel):\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "24c61a74660573d90942ef2cd74bb9853515d37c",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -463,6 +463,7 @@ class PegasusPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "c3e9d0fa5a962bdc86d1f8a2d18f240396bb3a4e",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -758,6 +758,7 @@ class PegasusXPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [r\"PegasusXEncoderLayer\", r\"PegasusXDecoderLayer\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     # Flaky logits\n     _supports_sdpa = False\n     _supports_flex_attn = True"
        },
        {
            "sha": "6b8da44b7424c5254c7209674db4dfe241758362",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -396,6 +396,7 @@ class PersimmonPreTrainedModel(PreTrainedModel):\n     _supports_static_cache = True\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "1782d80b4b4e3332f4e913a2103576b2b6b6b421",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -296,6 +296,7 @@ class PhiPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"PhiDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "6fd74305081dcdbfc832e567511330b3f62772fd",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -288,6 +288,7 @@ class Phi3PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Phi3DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "67c8129c0fa4cb44c5709a38a3a268d41aec0669",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -371,6 +371,7 @@ class Phi4MultimodalVisionPreTrainedModel(PreTrainedModel):\n \n     _no_split_modules = [\"Phi4MultimodalVisionEncoderLayer\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n@@ -996,6 +997,7 @@ class Phi4MultimodalAudioPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Phi4MultimodalAudioConformerEncoderLayer\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n@@ -1590,6 +1592,7 @@ class Phi4MultimodalPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Phi4MultimodalDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "2477f52f68ac5518adec59953057d353fc3e53a9",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -541,6 +541,7 @@ class Phi4MultimodalVisionPreTrainedModel(SiglipPreTrainedModel):\n \n     _no_split_modules = [\"Phi4MultimodalVisionEncoderLayer\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n@@ -1121,6 +1122,7 @@ class Phi4MultimodalAudioPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Phi4MultimodalAudioConformerEncoderLayer\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "09528a8f8eb6891452100c3e9144d7926b9db9a0",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -891,6 +891,7 @@ class PhimoePreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"PhimoeDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True"
        },
        {
            "sha": "5f9e1353263869532b24bd8c3191fbbcd89df755",
            "filename": "src/transformers/models/pixtral/modeling_pixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -406,10 +406,12 @@ class PixtralPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_attention_backend = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _no_split_modules = [\"PixtralAttentionLayer\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "bcad5f8f7253f3c7744fbb5324e01085f91f77f5",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -78,6 +78,7 @@ class PLBartPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"PLBartDecoderLayer\", \"PLBartEncoderLayer\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "f998a0136c211ed964a4c9f8bd4ffadf28d087ed",
            "filename": "src/transformers/models/plbart/modular_plbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -63,6 +63,7 @@ class PLBartPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"PLBartDecoderLayer\", \"PLBartEncoderLayer\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "d3882e22e638f2c0f84b1371e661c1ea0af53047",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -260,6 +260,7 @@ class Qwen2PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Qwen2DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "db2ac33eae56a3af79f432e97df1056477e61d86",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -86,6 +86,7 @@ class Qwen2_5OmniPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Qwen2_5OmniDecoderLayer\", \"Qwen2_5OmniVisionBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_static_cache = False"
        },
        {
            "sha": "aa8e0e44875868e14c81eda5f36989e4729ff2d2",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -324,6 +324,7 @@ class Qwen2_5_VLPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Qwen2_5_VLDecoderLayer\", \"Qwen2_5_VLVisionBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_static_cache = True"
        },
        {
            "sha": "31cda06324bf6149c12047c0db329c3ffbd30a25",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -268,6 +268,7 @@ class Qwen2AudioPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Qwen2AudioAttention\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "dc9f775ae995c9ced1986f358488560fa72f3cd3",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -748,6 +748,7 @@ class Qwen2MoePreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Qwen2MoeDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n "
        },
        {
            "sha": "9c9aaf6467636c3f77ae685eef713acebbbd5f7d",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -660,6 +660,7 @@ class Qwen2VLPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Qwen2VLDecoderLayer\", \"Qwen2VLVisionBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_static_cache = True"
        },
        {
            "sha": "a0c6d30239b93e6ac7efbfd5835b1df20b370720",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -286,6 +286,7 @@ class Qwen3PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Qwen3DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "c1201269ba21aafb0453cd12d70024987771696e",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -409,6 +409,7 @@ class Qwen3MoePreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Qwen3MoeDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "77ddfdaa01adf90715af8d1cd43f4452175795bc",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -234,6 +234,7 @@ class RagPreTrainedModel(PreTrainedModel):\n     config_class = RagConfig\n     base_model_prefix = \"rag\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n \n     @classmethod"
        },
        {
            "sha": "863462eed675a1aeb620fdb8aa9e55dc5d3516d8",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -519,6 +519,7 @@ class SEWPreTrainedModel(PreTrainedModel):\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = False  # needs a proper look into the mask creation\n "
        },
        {
            "sha": "2eba4010d6e6614b60a67c08800f46145861a08f",
            "filename": "src/transformers/models/sew/modular_sew.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fsew%2Fmodular_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fsew%2Fmodular_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodular_sew.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -264,6 +264,7 @@ class SEWPreTrainedModel(PreTrainedModel):\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = False  # needs a proper look into the mask creation\n "
        },
        {
            "sha": "6b9b752ba4140c1167d1975a77af784e7ba1a33b",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -480,6 +480,7 @@ class SiglipPreTrainedModel(PreTrainedModel):\n         \"SiglipMultiheadAttentionPoolingHead\",\n     ]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "a9e1885dd9b89ce044b86b73ddc2dd6a684ee857",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -713,6 +713,7 @@ class Siglip2PreTrainedModel(PreTrainedModel):\n         \"Siglip2MultiheadAttentionPoolingHead\",\n     ]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "ed0233940d39bdbf615548cc08c566909228d748",
            "filename": "src/transformers/models/smollm3/modeling_smollm3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -290,6 +290,7 @@ class SmolLM3PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"SmolLM3DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "2c9878df6c010a0c6d5a5232138d1bda4fc22784",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -55,6 +55,7 @@ class SmolVLMPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"SmolVLMVisionAttention\", \"SmolVLMDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True\n@@ -372,7 +373,8 @@ def forward(\n class SmolVLMVisionTransformer(SmolVLMPreTrainedModel):\n     config_class = SmolVLMVisionConfig\n     _supports_sdpa = True\n-    _supports_flash_attention_2 = True\n+    _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n \n     def __init__(self, config: SmolVLMVisionConfig):"
        },
        {
            "sha": "703584d43c68dd876cc28e348effe1cd76e3c23d",
            "filename": "src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -67,6 +67,7 @@ class SpeechEncoderDecoderModel(PreTrainedModel, GenerationMixin):\n     supports_gradient_checkpointing = True\n     _supports_param_buffer_assignment = False\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n \n     def __init__("
        },
        {
            "sha": "b7e99abe6350cd22a47880488dd12df72a16e182",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -622,6 +622,7 @@ class StableLmPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"StableLmDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_cache_class = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True"
        },
        {
            "sha": "f4f805c016dc3fa9ba2d6e6f4ba0e1b66eca1e43",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -294,6 +294,7 @@ class Starcoder2PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Starcoder2DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "6e31f181327d80babdece04b33d2e823fa7af72a",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -582,6 +582,7 @@ class T5GemmaPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"T5GemmaBlock\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "e995b8c1dd2583bc7b7991faaf787933485ddc91",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -787,6 +787,7 @@ class UniSpeechPreTrainedModel(PreTrainedModel):\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "ae7a37e093fbc6e6f83c3fd98028f203ac44b0ab",
            "filename": "src/transformers/models/unispeech/modular_unispeech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -143,6 +143,7 @@ class UniSpeechPreTrainedModel(PreTrainedModel):\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "86d4de0fe5fb590dbf3f3384080eee172485a877",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -792,6 +792,7 @@ class UniSpeechSatPreTrainedModel(PreTrainedModel):\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "a78b5bf2b0fac7f060869ffa91a212c8f2b516df",
            "filename": "src/transformers/models/unispeech_sat/modular_unispeech_sat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -155,6 +155,7 @@ class UniSpeechSatPreTrainedModel(PreTrainedModel):\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "3a906c4563373da2418ef868eef49b6be9e5e914",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -132,6 +132,7 @@ class VideoLlavaPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "74293b39716b2fdeb9371908523b62d04a914a76",
            "filename": "src/transformers/models/videomae/modeling_videomae.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -472,6 +472,7 @@ class VideoMAEPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "326fc4a35333fc4ca65bd420f119d7d9d7194cff",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -119,6 +119,7 @@ class VipLlavaPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "fd92b19e5ebedd9cf5865c4d75b9806ef5f13d91",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -69,6 +69,7 @@ class VisionEncoderDecoderModel(PreTrainedModel, GenerationMixin):\n     supports_gradient_checkpointing = True\n     _supports_param_buffer_assignment = False\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n \n     def __init__("
        },
        {
            "sha": "d8fa506d7c49a7ac2a8d73125689e006ed96a8a1",
            "filename": "src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -47,6 +47,7 @@ class VisionTextDualEncoderModel(PreTrainedModel):\n     config_class = VisionTextDualEncoderConfig\n     base_model_prefix = \"vision_text_dual_encoder\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n \n     def __init__("
        },
        {
            "sha": "f898b81382641994a96503a8e3f78b22f37c05e6",
            "filename": "src/transformers/models/vit/modeling_vit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -441,6 +441,7 @@ class ViTPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"ViTEmbeddings\", \"ViTLayer\"]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "8f0376863934aa1bcd77dc84c4756f11de5d9a9e",
            "filename": "src/transformers/models/vit_mae/modeling_vit_mae.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -606,6 +606,7 @@ class ViTMAEPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "c8b1b6f6cf6762be69d0936e3d2c5289a814822d",
            "filename": "src/transformers/models/vit_msn/modeling_vit_msn.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -445,6 +445,7 @@ class ViTMSNPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"ViTMSNAttention\", \"ViTMSNSdpaAttention\"]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "dad8dfe9c4f7abc79025c3ff473de6a34df6dbed",
            "filename": "src/transformers/models/vitpose_backbone/modeling_vitpose_backbone.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -406,6 +406,7 @@ class VitPoseBackbonePreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"VitPoseBackboneEmbeddings\", \"VitPoseBackboneLayer\"]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm, VitPoseBackboneEmbeddings]) -> None:\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "8a2f3177604ec35b76c25c12bcfc12048f41bfeb",
            "filename": "src/transformers/models/vivit/modeling_vivit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -449,6 +449,7 @@ class VivitPreTrainedModel(PreTrainedModel):\n     _no_split_modules = []\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "efbf452d1fc1082c37502b28b9ac9859aedbf2e9",
            "filename": "src/transformers/models/vjepa2/modeling_vjepa2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -990,6 +990,7 @@ class VJEPA2PreTrainedModel(PreTrainedModel):\n     ]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "a06b74bfd65024669ab2068f70c919b113eb22a8",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -1036,6 +1036,7 @@ class Wav2Vec2PreTrainedModel(PreTrainedModel):\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "ba5fba91d44f384d9e3d2ca43b2f0c1e5226a10f",
            "filename": "src/transformers/models/wavlm/modeling_wavlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -599,6 +599,7 @@ class WavLMPreTrainedModel(PreTrainedModel):\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = False\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = False\n     _supports_flex_attn = False\n "
        },
        {
            "sha": "123376a58a39d7396794ed691d0e5f342c6bb9a9",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -560,6 +560,7 @@ class WhisperPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"WhisperEncoderLayer\", \"WhisperDecoderLayer\"]\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "bb9f034e81dd969e804cc351f7297031efe6cb01",
            "filename": "src/transformers/models/yolos/modeling_yolos.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -519,6 +519,7 @@ class YolosPreTrainedModel(PreTrainedModel):\n     _no_split_modules = []\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "b7c391887f1ab954925ea9be56d86cb003b84efa",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -1183,6 +1183,7 @@ class Zamba2PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Zamba2AttentionDecoderLayer\", \"Zamba2MambaDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True  # Note: only supports Zamba2HybridDynamicCache"
        },
        {
            "sha": "fc1d76816101e452d039151d2672508391d9772e",
            "filename": "src/transformers/models/zamba2/modular_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0687d481e2c71544501ef9cb3eef795a6e79b1de/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py?ref=0687d481e2c71544501ef9cb3eef795a6e79b1de",
            "patch": "@@ -909,6 +909,7 @@ class Zamba2PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Zamba2AttentionDecoderLayer\", \"Zamba2MambaDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n     _supports_flex_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True  # Note: only supports Zamba2HybridDynamicCache"
        }
    ],
    "stats": {
        "total": 199,
        "additions": 195,
        "deletions": 4
    }
}