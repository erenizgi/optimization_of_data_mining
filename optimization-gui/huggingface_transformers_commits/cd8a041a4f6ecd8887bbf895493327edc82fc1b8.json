{
    "author": "ydshieh",
    "message": "Update expected values (after switching to A10) - part 7 (#39218)\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "cd8a041a4f6ecd8887bbf895493327edc82fc1b8",
    "files": [
        {
            "sha": "a621881098023727dc7d0f2976d1f5ef9c9aa0b2",
            "filename": "tests/models/cohere2/test_modeling_cohere2.py",
            "status": "modified",
            "additions": 14,
            "deletions": 5,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd8a041a4f6ecd8887bbf895493327edc82fc1b8/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd8a041a4f6ecd8887bbf895493327edc82fc1b8/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py?ref=cd8a041a4f6ecd8887bbf895493327edc82fc1b8",
            "patch": "@@ -165,7 +165,8 @@ def test_model_fp16(self):\n         EXPECTED_TEXTS = Expectations(\n             {\n                 (\"xpu\", 3): [\"<BOS_TOKEN>Hello I am doing a project for my school and I need to create a website for a fictional company. I have the\", \"<PAD><PAD><BOS_TOKEN>Hi today I'm going to show you how to make a simple and easy to make a chocolate cake.\\n\"],\n-                (\"cuda\", 7): [\"<BOS_TOKEN>Hello I am doing a project for a school assignment and I need to create a website for a fictional company. I have\", \"<PAD><PAD><BOS_TOKEN>Hi today I'm going to show you how to make a simple and easy to make a chocolate cake.\\n\",],\n+                (None, None): [\"<BOS_TOKEN>Hello I am doing a project for a school assignment and I need to create a website for a fictional company. I have\", \"<PAD><PAD><BOS_TOKEN>Hi today I'm going to show you how to make a simple and easy to make a chocolate cake.\\n\"],\n+                (\"cuda\", 8): ['<BOS_TOKEN>Hello I am doing a project for my school and I need to create a website for a fictional company. I have the', \"<PAD><PAD><BOS_TOKEN>Hi today I'm going to show you how to make a simple and easy to make a chocolate cake.\\n\"],\n             }\n         )\n         EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n@@ -238,7 +239,8 @@ def test_export_static_cache(self):\n         EXPECTED_TEXT_COMPLETIONS = Expectations(\n             {\n                 (\"xpu\", 3): [\"Hello I am doing a project for a friend and I am stuck on a few things. I have a 2004 Ford F-\"],\n-                (\"cuda\", 7): [\"Hello I am doing a project on the effects of social media on mental health. I have a few questions. 1. What is the relationship\",],\n+                (None, None): [\"Hello I am doing a project on the effects of social media on mental health. I have a few questions. 1. What is the relationship\"],\n+                (\"cuda\", 8): ['Hello I am doing a project for a friend and I am stuck on a few things. I have a 2004 Ford F-'],\n             }\n         )\n         EXPECTED_TEXT_COMPLETION = EXPECTED_TEXT_COMPLETIONS.get_expectation()\n@@ -290,24 +292,31 @@ def test_generation_beyond_sliding_window(self, attn_implementation: str):\n         if attn_implementation == \"flash_attention_2\" and not is_flash_attn_2_available():\n             self.skipTest(\"FlashAttention2 is required for this test.\")\n \n+        # TODO: if we can specify not to compile when `flex` attention is used?\n+        if attn_implementation == \"flex_attention\":\n+            self.skipTest(\n+                \"Flex attention will compile (see `compile_friendly_flex_attention`) which causes triton issue.\"\n+            )\n+\n         if torch_device == \"xpu\" and attn_implementation == \"flash_attention_2\":\n             self.skipTest(reason=\"Intel XPU doesn't support falsh_attention_2 as of now.\")\n \n         model_id = \"CohereForAI/c4ai-command-r7b-12-2024\"\n         EXPECTED_COMPLETIONS = [\n-            \" the mountains, the lakes, the rivers, the waterfalls, the waterfalls, the waterfalls, the waterfalls\",\n+            \" the mountains, the lakes, the rivers, the forests, the trees, the birds, the animals\",\n             \", green, yellow, orange, purple, pink, brown, black, white, grey, silver\",\n         ]\n \n         input_text = [\n-            \"This is a nice place. \" * 800 + \"I really enjoy the scenery,\",  # This is larger than 4096 tokens\n+            \"This is a nice place. \" * 200 + \"I really enjoy the scenery,\",  # This is larger than 1024 tokens\n             \"A list of colors: red, blue\",  # This will almost all be padding tokens\n         ]\n         tokenizer = AutoTokenizer.from_pretrained(model_id, padding=\"left\")\n         inputs = tokenizer(input_text, padding=True, return_tensors=\"pt\").to(torch_device)\n \n+        # We use `sliding_window=1024` instead of the origin value `4096` in the config to avoid GPU OOM\n         model = AutoModelForCausalLM.from_pretrained(\n-            model_id, attn_implementation=attn_implementation, torch_dtype=torch.float16\n+            model_id, attn_implementation=attn_implementation, torch_dtype=torch.float16, sliding_window=1024\n         ).to(torch_device)\n \n         # Make sure prefill is larger than sliding window"
        },
        {
            "sha": "a1dde3d31b96940575b3c52d7e378eafbbb4c753",
            "filename": "tests/models/data2vec/test_modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd8a041a4f6ecd8887bbf895493327edc82fc1b8/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd8a041a4f6ecd8887bbf895493327edc82fc1b8/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py?ref=cd8a041a4f6ecd8887bbf895493327edc82fc1b8",
            "patch": "@@ -211,6 +211,12 @@ def setUp(self):\n     def test_config(self):\n         self.config_tester.run_common_tests()\n \n+    @unittest.skip(\n+        reason=\"Will fix only if requested by the community: it fails with `torch._dynamo.exc.InternalTorchDynamoError: IndexError: list index out of range`. Without compile, the test pass.\"\n+    )\n+    def test_sdpa_can_compile_dynamic(self):\n+        pass\n+\n     @unittest.skip(reason=\"Data2VecVision does not use inputs_embeds\")\n     def test_inputs_embeds(self):\n         pass"
        },
        {
            "sha": "50ac4cd1d28fb9e9dafe85cae49fec838c8550af",
            "filename": "tests/models/depth_pro/test_modeling_depth_pro.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd8a041a4f6ecd8887bbf895493327edc82fc1b8/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd8a041a4f6ecd8887bbf895493327edc82fc1b8/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py?ref=cd8a041a4f6ecd8887bbf895493327edc82fc1b8",
            "patch": "@@ -220,6 +220,10 @@ def setUp(self):\n     def test_config(self):\n         self.config_tester.run_common_tests()\n \n+    @unittest.skip(reason=\"Inductor error: name 'OpaqueUnaryFn_log2' is not defined\")\n+    def test_sdpa_can_compile_dynamic(self):\n+        pass\n+\n     @unittest.skip(reason=\"DepthPro does not use inputs_embeds\")\n     def test_inputs_embeds(self):\n         pass"
        },
        {
            "sha": "b5147a83cc1840f6e2a978597c5d38ed88133e65",
            "filename": "tests/models/olmo2/test_modeling_olmo2.py",
            "status": "modified",
            "additions": 36,
            "deletions": 9,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd8a041a4f6ecd8887bbf895493327edc82fc1b8/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd8a041a4f6ecd8887bbf895493327edc82fc1b8/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py?ref=cd8a041a4f6ecd8887bbf895493327edc82fc1b8",
            "patch": "@@ -22,6 +22,8 @@\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.models.auto.tokenization_auto import AutoTokenizer\n from transformers.testing_utils import (\n+    Expectations,\n+    cleanup,\n     require_tokenizers,\n     require_torch,\n     slow,\n@@ -232,30 +234,55 @@ def test_model_rope_scaling(self, scaling_type):\n \n @require_torch\n class Olmo2IntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n     @slow\n     def test_model_1b_logits_bfloat16(self):\n         input_ids = [[1, 306, 4658, 278, 6593, 310, 2834, 338]]\n-        model = Olmo2ForCausalLM.from_pretrained(\"allenai/OLMo-2-0425-1B\").to(torch.bfloat16)\n-        out = model(torch.tensor(input_ids)).logits.float()\n+        model = Olmo2ForCausalLM.from_pretrained(\"allenai/OLMo-2-0425-1B\").to(torch_device, torch.bfloat16)\n+        out = model(torch.tensor(input_ids, device=torch_device)).logits.float()\n         # Expected mean on dim = -1\n-        EXPECTED_MEAN = torch.tensor([[-5.7094, -6.5548, -3.2527, -2.7847, -5.5092, -4.5223, -4.8427, -4.6867]])\n+        expectations = Expectations(\n+            {\n+                (\"cuda\", 8): [[-5.6700, -6.5557, -3.1545, -2.7418, -5.5887, -4.5179, -4.9077, -4.6530]],\n+            }\n+        )\n+        EXPECTED_MEAN = torch.tensor(expectations.get_expectation(), device=torch_device)\n         torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n+\n         # slicing logits[0, 0, 0:30]\n-        EXPECTED_SLICE = torch.tensor([2.4531, -5.7188, -5.1562, -4.8750, -6.7812, -4.0625, -4.4375, -4.5938, -7.5938, -5.0938, -3.9375, -3.6875, -5.0938, -3.1875, -5.6875, 0.2266, 1.2578, 1.1016, 0.8945, 0.4785, 0.2256, -0.3613, -0.4258, 0.1377, -0.1104, -7.1875, -5.2188, -6.8125, -0.9062, -2.9062])  # fmt: skip\n+        expectations = Expectations(\n+            {\n+                (\"cuda\", 8): [2.65625, -5.25, -4.9375, -4.53125, -6.5, -3.828125, -4.15625, -4.1875, -7.0625, -4.71875, -3.609375, -3.09375, -4.59375, -2.640625, -5.25, 0.39453125, 1.3828125, 1.2265625, 1.0078125, 0.57421875, 0.330078125, -0.287109375, -0.3671875, 0.1943359375, -0.0732421875, -6.6875, -4.75, -6.4375, -0.625, -2.625],\n+            }\n+        )  # fmt: skip\n+        EXPECTED_SLICE = torch.tensor(expectations.get_expectation(), device=torch_device)\n         torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-2, atol=1e-2)\n \n     @slow\n     def test_model_7b_logits(self):\n         input_ids = [[1, 306, 4658, 278, 6593, 310, 2834, 338]]\n-        model = Olmo2ForCausalLM.from_pretrained(\"shanearora/OLMo2-7B-1124-hf\", device_map=\"auto\")\n-        out = model(torch.tensor(input_ids)).logits.float()\n+        model = Olmo2ForCausalLM.from_pretrained(\"shanearora/OLMo2-7B-1124-hf\").to(torch_device, dtype=torch.bfloat16)\n+        out = model(torch.tensor(input_ids, device=torch_device)).logits.float()\n         # Expected mean on dim = -1\n-        EXPECTED_MEAN = torch.tensor(\n-            [[-13.0244, -13.9564, -11.8270, -11.3047, -12.3794, -12.4215, -15.6030, -12.7962]]\n+        expectations = Expectations(\n+            {\n+                (\"cuda\", 8): [[-13.0518, -13.8897, -11.7999, -11.3222, -12.3441, -12.3884, -15.4874, -12.7365]],\n+            }\n         )\n+        EXPECTED_MEAN = torch.tensor(expectations.get_expectation(), device=torch_device)\n         torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n         # slicing logits[0, 0, 0:30]\n-        EXPECTED_SLICE = torch.tensor([-5.3909, -13.9841, -13.6123, -14.5780, -13.9455, -13.2265, -13.4734, -11.9079, -9.2879, -12.6139, -11.4819, -5.9607, -11.9657, -6.3618, -11.1065, -7.3075, -6.5674, -6.7154, -7.3409, -7.9662, -8.0863, -8.1682, -8.7341, -8.7665, -8.8742, -9.7813, -8.0620, -12.5937, -7.6440, -11.3966])  # fmt: skip\n+        expectations = Expectations(\n+            {\n+                (\"cuda\", 8): [-5.5, -14.4375, -13.8125, -14.875, -14.125, -13.4375, -13.8125, -12.25, -9.5, -12.9375, -11.6875, -6.09375, -12.1875, -6.5, -11.3125, -7.34375, -6.5625, -6.71875, -7.375, -7.96875, -8.0625, -8.1875, -8.75, -8.75, -8.875, -9.9375, -8.1875, -12.875, -7.84375, -11.625],\n+            }\n+        )  # fmt: skip\n+        EXPECTED_SLICE = torch.tensor(expectations.get_expectation(), device=torch_device)\n         torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-2, atol=1e-2)\n \n     @slow"
        }
    ],
    "stats": {
        "total": 74,
        "additions": 60,
        "deletions": 14
    }
}