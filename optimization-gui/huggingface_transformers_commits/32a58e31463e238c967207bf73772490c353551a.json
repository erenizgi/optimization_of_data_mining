{
    "author": "zucchini-nlp",
    "message": "ðŸš¨ Delete deprecations with end-cycle in v4.xx and v5.0 (#41681)\n\n* remove deprecations from v4\n\n* delete those for v5\n\n* delete these also\n\n* fix tests\n\n* add dummy test config\n\n* fix copies\n\n* SDPA raises warning but doesn't automatically change to eager\n\n* max size can't be deleted, sadly\n\n* oke, this should allow loading from-pretrained, but delete everything else\n\n* style\n\n* fix popping from kwargs\n\n* audios rename\n\n* padding defaults to self\n\n* modular fix\n\n* address comment\n\n* style",
    "sha": "32a58e31463e238c967207bf73772490c353551a",
    "files": [
        {
            "sha": "8153589e5cca717c0f8bd7f588448c29fd3b0578",
            "filename": "docs/source/en/model_doc/qwen2_5_omni.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_omni.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_omni.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_omni.md?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -136,7 +136,7 @@ inputs = processor.apply_chat_template(\n     tokenize=True,\n     return_dict=True,\n     return_tensors=\"pt\",\n-    video_fps=1,\n+    fps=1,\n \n     # kwargs to be passed to `Qwen2-5-OmniProcessor`\n     padding=True,\n@@ -245,7 +245,7 @@ inputs = processor.apply_chat_template(\n     tokenize=True,\n     return_dict=True,\n     return_tensors=\"pt\",\n-    video_fps=1,\n+    fps=1,\n \n     # kwargs to be passed to `Qwen2-5-OmniProcessor`\n     padding=True,"
        },
        {
            "sha": "76e16315c3e5631b20f6def3a6570e8654ffdb14",
            "filename": "docs/source/en/model_doc/qwen2_audio.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -54,7 +54,7 @@ processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B\", trust_remote_co\n prompt = \"<|audio_bos|><|AUDIO|><|audio_eos|>Generate the caption in English:\"\n url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Audio/glass-breaking-151256.mp3\"\n audio, sr = librosa.load(BytesIO(urlopen(url).read()), sr=processor.feature_extractor.sampling_rate)\n-inputs = processor(text=prompt, audios=audio, return_tensors=\"pt\").to(model.device)\n+inputs = processor(text=prompt, audio=audio, return_tensors=\"pt\").to(model.device)\n \n generate_ids = model.generate(**inputs, max_length=256)\n generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n@@ -63,7 +63,7 @@ response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_\n \n # We can also omit the audio_bos and audio_eos tokens\n prompt = \"<|AUDIO|>Generate the caption in English:\"\n-inputs = processor(text=prompt, audios=audio, return_tensors=\"pt\").to(model.device)\n+inputs = processor(text=prompt, audio=audio, return_tensors=\"pt\").to(model.device)\n \n generate_ids = model.generate(**inputs, max_length=256)\n generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n@@ -106,7 +106,7 @@ for message in conversation:\n                     sr=processor.feature_extractor.sampling_rate)[0]\n                 )\n \n-inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n+inputs = processor(text=text, audio=audios, return_tensors=\"pt\", padding=True)\n inputs.input_ids = inputs.input_ids.to(model.device)\n \n generate_ids = model.generate(**inputs, max_length=256)\n@@ -156,7 +156,7 @@ for message in conversation:\n                         sr=processor.feature_extractor.sampling_rate)[0]\n                 )\n \n-inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n+inputs = processor(text=text, audio=audios, return_tensors=\"pt\", padding=True)\n inputs.input_ids = inputs.input_ids.to(model.device)\n \n generate_ids = model.generate(**inputs, max_length=256)\n@@ -213,7 +213,7 @@ for conversation in conversations:\n                             sr=processor.feature_extractor.sampling_rate)[0]\n                     )\n \n-inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n+inputs = processor(text=text, audio=audios, return_tensors=\"pt\", padding=True)\n inputs['input_ids'] = inputs['input_ids'].to(model.device)\n inputs.input_ids = inputs.input_ids.to(model.device)\n "
        },
        {
            "sha": "75dea355e4bb8c5626e2cf1b40674585d66c019e",
            "filename": "docs/source/en/model_doc/qwen3_omni_moe.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_omni_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_omni_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_omni_moe.md?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -80,7 +80,7 @@ inputs = processor.apply_chat_template(\n     tokenize=True,\n     return_dict=True,\n     return_tensors=\"pt\",\n-    video_fps=1,\n+    fps=1,\n \n     # kwargs to be passed to `Qwen3OmniMoeProcessor`\n     padding=True,\n@@ -136,7 +136,7 @@ inputs = processor.apply_chat_template(\n     tokenize=True,\n     return_dict=True,\n     return_tensors=\"pt\",\n-    video_fps=1,\n+    fps=1,\n \n     # kwargs to be passed to `Qwen3OmniMoeProcessor`\n     padding=True,\n@@ -245,7 +245,7 @@ inputs = processor.apply_chat_template(\n     tokenize=True,\n     return_dict=True,\n     return_tensors=\"pt\",\n-    video_fps=1,\n+    fps=1,\n \n     # kwargs to be passed to `Qwen3OmniMoeProcessor`\n     padding=True,"
        },
        {
            "sha": "8415c94f85010972467b7f7bed4ea4c24f15b4f3",
            "filename": "docs/source/en/model_doc/seamless_m4t.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t.md?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -61,7 +61,7 @@ Here is how to use the processor to process text and audio:\n >>> audio_sample = next(iter(dataset))[\"audio\"]\n \n >>> # now, process it\n->>> audio_inputs = processor(audios=audio_sample[\"array\"], return_tensors=\"pt\")\n+>>> audio_inputs = processor(audio=audio_sample[\"array\"], return_tensors=\"pt\")\n \n >>> # now, process some English test as well\n >>> text_inputs = processor(text = \"Hello, my dog is cute\", src_lang=\"eng\", return_tensors=\"pt\")"
        },
        {
            "sha": "c92e5c00527bfd8095819df6e38071eeb3dd6763",
            "filename": "docs/source/en/model_doc/seamless_m4t_v2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t_v2.md?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -61,7 +61,7 @@ Here is how to use the processor to process text and audio:\n >>> audio_sample = next(iter(dataset))[\"audio\"]\n \n >>> # now, process it\n->>> audio_inputs = processor(audios=audio_sample[\"array\"], return_tensors=\"pt\")\n+>>> audio_inputs = processor(audio=audio_sample[\"array\"], return_tensors=\"pt\")\n \n >>> # now, process some English text as well\n >>> text_inputs = processor(text = \"Hello, my dog is cute\", src_lang=\"eng\", return_tensors=\"pt\")"
        },
        {
            "sha": "7f5634e625fa1f97db28bc494a89bec6144fbd5c",
            "filename": "examples/pytorch/audio-classification/run_audio_classification.py",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/examples%2Fpytorch%2Faudio-classification%2Frun_audio_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/examples%2Fpytorch%2Faudio-classification%2Frun_audio_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Faudio-classification%2Frun_audio_classification.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -27,7 +27,6 @@\n import logging\n import os\n import sys\n-import warnings\n from dataclasses import dataclass, field\n from random import randint\n from typing import Optional\n@@ -180,29 +179,11 @@ class ModelArguments:\n             )\n         },\n     )\n-    freeze_feature_extractor: Optional[bool] = field(\n-        default=None, metadata={\"help\": \"Whether to freeze the feature extractor layers of the model.\"}\n-    )\n     ignore_mismatched_sizes: bool = field(\n         default=False,\n         metadata={\"help\": \"Will enable to load a pretrained model whose head dimensions are different.\"},\n     )\n \n-    def __post_init__(self):\n-        if not self.freeze_feature_extractor and self.freeze_feature_encoder:\n-            warnings.warn(\n-                \"The argument `--freeze_feature_extractor` is deprecated and \"\n-                \"will be removed in a future version. Use `--freeze_feature_encoder` \"\n-                \"instead. Setting `freeze_feature_encoder==True`.\",\n-                FutureWarning,\n-            )\n-        if self.freeze_feature_extractor and not self.freeze_feature_encoder:\n-            raise ValueError(\n-                \"The argument `--freeze_feature_extractor` is deprecated and \"\n-                \"should not be used in combination with `--freeze_feature_encoder`. \"\n-                \"Only make use of `--freeze_feature_encoder`.\"\n-            )\n-\n \n def main():\n     # See all possible arguments in src/transformers/training_args.py"
        },
        {
            "sha": "b56f70d1e90dc42280ca752cbca660921efc888e",
            "filename": "src/transformers/models/aimv2/modeling_aimv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -37,7 +37,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, filter_out_non_signature_kwargs\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_aimv2 import Aimv2Config, Aimv2TextConfig, Aimv2VisionConfig\n \n@@ -446,13 +445,11 @@ def __init__(self, config: Aimv2VisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.embeddings.patch_embed\n \n-    @deprecate_kwarg(\"attention_mask\", version=\"v4.58.0\")\n     @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values,\n-        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\""
        },
        {
            "sha": "e65f5ab5a6121406afdcbfcc2d9a0f1d9bee0087",
            "filename": "src/transformers/models/aimv2/modular_aimv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -32,7 +32,6 @@\n     auto_docstring,\n     can_return_tuple,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from ..clip.modeling_clip import CLIPModel, CLIPTextEmbeddings, _get_vector_norm\n from ..llama.modeling_llama import LlamaMLP, LlamaRMSNorm\n@@ -489,13 +488,11 @@ def __init__(self, config: Aimv2VisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.embeddings.patch_embed\n \n-    @deprecate_kwarg(\"attention_mask\", version=\"v4.58.0\")\n     @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values,\n-        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\""
        },
        {
            "sha": "e47b5281010f928902fe27b862aa2fc46d73f54f",
            "filename": "src/transformers/models/altclip/processing_altclip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -17,7 +17,6 @@\n \"\"\"\n \n from ...processing_utils import ProcessorMixin\n-from ...utils.deprecation import deprecate_kwarg\n \n \n class AltCLIPProcessor(ProcessorMixin):\n@@ -35,7 +34,6 @@ class AltCLIPProcessor(ProcessorMixin):\n             The tokenizer is a required input.\n     \"\"\"\n \n-    @deprecate_kwarg(old_name=\"feature_extractor\", version=\"5.0.0\", new_name=\"image_processor\")\n     def __init__(self, image_processor=None, tokenizer=None):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "1d83a26a4b3f7a19658c25397f0b9e8f759977b7",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 20,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -16,7 +16,6 @@\n \n import collections.abc\n import math\n-import warnings\n from dataclasses import dataclass\n from typing import Optional, Union\n \n@@ -163,14 +162,7 @@ def forward(\n         self,\n         pixel_values: torch.Tensor,\n         bool_masked_pos: Optional[torch.BoolTensor] = None,\n-        interpolate_pos_encoding: Optional[bool] = None,\n     ) -> torch.Tensor:\n-        if self.position_embeddings is not None and interpolate_pos_encoding is not None:\n-            warnings.warn(\n-                \"`interpolate_pos_encoding` argument has no effect for BEiTEmbeddings, embeddings are always \"\n-                \"interpolated to the input image size. The argument will be removed in transformers v4.51.0.\"\n-            )\n-\n         _, _, height, width = pixel_values.shape\n         embeddings, (patch_height, patch_width) = self.patch_embeddings(pixel_values)\n         batch_size, seq_len, _ = embeddings.size()\n@@ -325,19 +317,9 @@ def forward(\n     ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n         if output_attentions:\n             logger.warning_once(\n-                \"`BeitSdpaSelfAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not \"\n-                \"support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                \"but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n-                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                output_attentions=output_attentions,\n-                relative_position_bias=relative_position_bias,\n-                interpolate_pos_encoding=interpolate_pos_encoding,\n-                resolution=resolution,\n+                f\"{self.__class__.__name__} does not support `output_attentions=True`. The returned attention weights will \"\n+                \"be `None`. If you want to get attention weights, please set `attn_implementation='eager'` when loading the model.\"\n             )\n-\n         batch_size, seq_length, _ = hidden_states.shape\n         query_layer = (\n             self.query(hidden_states)"
        },
        {
            "sha": "45297cb4e57c6c398bc6ade098141cd232ff5eca",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 43,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"PyTorch BLIP-2 model.\"\"\"\n \n import math\n-import warnings\n from collections.abc import Callable\n from dataclasses import dataclass\n from typing import Any, Optional, Union\n@@ -1082,7 +1081,6 @@ def get_text_features(\n         decoder_input_ids: Optional[torch.Tensor] = None,\n         decoder_attention_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        legacy_output: bool = True,\n     ) -> Union[torch.FloatTensor, CausalLMOutputWithPast]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1101,12 +1099,10 @@ def get_text_features(\n         decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n             be used by default.\n-        legacy_output (`bool`, *optional*, defaults to `True`):\n-            Whether to return a model output object or a tensor of features.\n \n         Returns:\n-            text_outputs (`CausalLMOutputWithPast` or `torch.FloatTensor`):\n-                The language model outputs. If `legacy_output=False`, the output is a `torch.FloatTensor`.\n+            text_outputs (``torch.FloatTensor`):\n+                The language model's last hidden states.\n \n         Examples:\n         ```python\n@@ -1121,13 +1117,6 @@ def get_text_features(\n         ...     text_features = model.get_text_features(**inputs)\n         ```\"\"\"\n \n-        if legacy_output:\n-            warnings.warn(\n-                \"Deprecation notice: In Transformers v4.59, the default return value of `get_text_features` will change. \"\n-                \"Currently, this method returns a model output object, but starting in v4.59, it will return a tensor instead. \"\n-                \"To opt in to the new behavior now, set `legacy_output=False`.\"\n-            )\n-\n         if self.config.use_decoder_only_language_model:\n             text_outputs: CausalLMOutputWithPast = self.language_model(\n                 input_ids=input_ids,\n@@ -1145,23 +1134,19 @@ def get_text_features(\n                 return_dict=True,\n             )\n \n-        return text_outputs if legacy_output else text_outputs.logits\n+        return text_outputs.logits\n \n     @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n         interpolate_pos_encoding: bool = False,\n-        legacy_output: bool = True,\n     ) -> Union[torch.FloatTensor, CausalLMOutputWithPast]:\n         r\"\"\"\n-        legacy_output (`bool`, *optional*, defaults to `True`):\n-            Whether to return a model output object or a tensor of features.\n-\n         Returns:\n-            vision_outputs (`BaseModelOutputWithPooling` or `torch.FloatTensor`):\n-                The vision model outputs. If `legacy_output=False`, the output is a `torch.FloatTensor`.\n+            vision_outputs (`torch.FloatTensor`):\n+                The vision model's last layer pooled logits.\n \n         Examples:\n         ```python\n@@ -1179,36 +1164,25 @@ def get_image_features(\n         >>> with torch.inference_mode():\n         ...     image_outputs = model.get_image_features(**inputs)\n         ```\"\"\"\n-        if legacy_output:\n-            warnings.warn(\n-                \"Deprecation notice: In Transformers v4.59, the default return value of `get_text_features` will change. \"\n-                \"Currently, this method returns a model output object, but starting in v4.59, it will return a tensor instead. \"\n-                \"To opt in to the new behavior now, set `legacy_output=False`.\"\n-            )\n-\n         vision_outputs = self.vision_model(\n             pixel_values=pixel_values,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=True,\n         )\n \n-        return vision_outputs if legacy_output else vision_outputs.pooler_output\n+        return vision_outputs.pooler_output\n \n     @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_qformer_features(\n         self,\n         pixel_values: torch.FloatTensor,\n         interpolate_pos_encoding: bool = False,\n-        legacy_output: bool = True,\n     ) -> Union[torch.FloatTensor, BaseModelOutputWithPooling]:\n         r\"\"\"\n-        legacy_output (`bool`, *optional*, defaults to `True`):\n-            Whether to return a model output object or a tensor of features.\n-\n         Returns:\n-            qformer_outputs (`BaseModelOutputWithPooling` or `torch.FloatTensor`):\n-                The Q-Former outputs. If `legacy_output=False`, the output is a `torch.FloatTensor`.\n+            qformer_outputs (`torch.FloatTensor`):\n+                The Q-Former model's last layer hidden states.\n \n         Examples:\n \n@@ -1227,14 +1201,6 @@ def get_qformer_features(\n         >>> with torch.inference_mode():\n         ...     qformer_outputs = model.get_qformer_features(**inputs)\n         ```\"\"\"\n-\n-        if legacy_output:\n-            warnings.warn(\n-                \"Deprecation notice: In Transformers v4.59, the default return value of `get_qformer_features` will change. \"\n-                \"Currently, this method returns a model output object, but starting in v4.59, it will return a tensor instead. \"\n-                \"To opt in to the new behavior now, set `legacy_output=False`.\"\n-            )\n-\n         vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n@@ -1254,7 +1220,7 @@ def get_qformer_features(\n             return_dict=True,\n         )\n \n-        return query_outputs if legacy_output else query_outputs.last_hidden_state\n+        return query_outputs.last_hidden_state\n \n     def get_placeholder_mask(self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor):\n         \"\"\""
        },
        {
            "sha": "3b0be0bef90a7bd6ca6a1ec79554d4589fe8cc6f",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 2,
            "deletions": 50,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"PyTorch BLOOM model.\"\"\"\n \n import math\n-import warnings\n from typing import Optional, Union\n \n import torch\n@@ -485,7 +484,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **deprecated_arguments,\n     ) -> Union[tuple[torch.Tensor, ...], BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n@@ -500,16 +498,6 @@ def forward(\n \n             [What are input IDs?](../glossary#input-ids)\n         \"\"\"\n-        if deprecated_arguments.pop(\"position_ids\", False) is not False:\n-            # `position_ids` could have been `torch.Tensor` or `None` so defaulting pop to `False` allows to detect if users were passing explicitly `None`\n-            warnings.warn(\n-                \"`position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore\"\n-                \" passing `position_ids`.\",\n-                FutureWarning,\n-            )\n-        if len(deprecated_arguments) > 0:\n-            raise ValueError(f\"Got unexpected arguments: {deprecated_arguments}\")\n-\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -818,7 +806,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **deprecated_arguments,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n@@ -837,18 +825,6 @@ def forward(\n             `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n             are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n         \"\"\"\n-        # Bloom has deprecated kwargs, so we need to pop num_items_in_batch explicitly\n-        num_items_in_batch = deprecated_arguments.pop(\"num_items_in_batch\", None)\n-        if deprecated_arguments.pop(\"position_ids\", False) is not False:\n-            # `position_ids` could have been `torch.Tensor` or `None` so defaulting pop to `False` allows to detect if users were passing explicitly `None`\n-            warnings.warn(\n-                \"`position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore\"\n-                \" passing `position_ids`.\",\n-                FutureWarning,\n-            )\n-        if len(deprecated_arguments) > 0:\n-            raise ValueError(f\"Got unexpected arguments: {deprecated_arguments}\")\n-\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         transformer_outputs = self.transformer(\n@@ -874,7 +850,7 @@ def forward(\n                 logits,\n                 labels,\n                 vocab_size=self.config.vocab_size,\n-                num_items_in_batch=num_items_in_batch,\n+                num_items_in_batch=kwargs.get(\"num_items_in_batch\"),\n             )\n \n         if not return_dict:\n@@ -926,7 +902,6 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-        **deprecated_arguments,\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n@@ -945,16 +920,6 @@ def forward(\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        if deprecated_arguments.pop(\"position_ids\", False) is not False:\n-            # `position_ids` could have been `torch.Tensor` or `None` so defaulting pop to `False` allows to detect if users were passing explicitly `None`\n-            warnings.warn(\n-                \"`position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore\"\n-                \" passing `position_ids`.\",\n-                FutureWarning,\n-            )\n-        if len(deprecated_arguments) > 0:\n-            raise ValueError(f\"Got unexpected arguments: {deprecated_arguments}\")\n-\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         transformer_outputs = self.transformer(\n@@ -1060,7 +1025,6 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-        **deprecated_arguments,\n     ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n@@ -1079,16 +1043,6 @@ def forward(\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        if deprecated_arguments.pop(\"position_ids\", False) is not False:\n-            # `position_ids` could have been `torch.Tensor` or `None` so defaulting pop to `False` allows to detect if users were passing explicitly `None`\n-            warnings.warn(\n-                \"`position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore\"\n-                \" passing `position_ids`.\",\n-                FutureWarning,\n-            )\n-        if len(deprecated_arguments) > 0:\n-            raise ValueError(f\"Got unexpected arguments: {deprecated_arguments}\")\n-\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         transformer_outputs = self.transformer(\n@@ -1143,7 +1097,6 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -1169,7 +1122,6 @@ def forward(\n         outputs = self.transformer(\n             input_ids,\n             attention_mask=attention_mask,\n-            position_ids=position_ids,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "c167817937b74249d631d7535ca0fc6666c16d7d",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -192,9 +192,6 @@ def __init__(\n         do_pad: bool = True,\n         **kwargs,\n     ) -> None:\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            do_pad = kwargs.pop(\"pad_and_return_pixel_mask\")\n-\n         super().__init__(**kwargs)\n         size = size if size is not None else {\"shortest_edge\": 288}\n         size = get_size_dict(size, default_to_square=False)\n@@ -208,7 +205,7 @@ def __init__(\n         self.do_normalize = do_normalize\n         self.image_mean = image_mean if image_mean is not None else OPENAI_CLIP_MEAN\n         self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n-        self.do_pad = do_pad\n+        self.do_pad = kwargs.pop(\"pad_and_return_pixel_mask\", do_pad)\n         self.do_center_crop = do_center_crop\n         self.crop_size = crop_size\n "
        },
        {
            "sha": "420d14cb816ca70d7bf5676f59629bb73242fffa",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -1373,7 +1373,7 @@ def forward(\n         >>> model = ClapAudioModel.from_pretrained(\"laion/clap-htsat-fused\")\n         >>> processor = AutoProcessor.from_pretrained(\"laion/clap-htsat-fused\")\n \n-        >>> inputs = processor(audios=audio_sample, return_tensors=\"pt\")\n+        >>> inputs = processor(audio=audio_sample, return_tensors=\"pt\")\n \n         >>> outputs = model(**inputs)\n         >>> last_hidden_state = outputs.last_hidden_state\n@@ -1648,7 +1648,7 @@ def forward(\n \n         >>> input_text = [\"Sound of a dog\", \"Sound of vacuum cleaner\"]\n \n-        >>> inputs = processor(text=input_text, audios=audio_sample, return_tensors=\"pt\", padding=True)\n+        >>> inputs = processor(text=input_text, audio=audio_sample, return_tensors=\"pt\", padding=True)\n \n         >>> outputs = model(**inputs)\n         >>> logits_per_audio = outputs.logits_per_audio  # this is the audio-text similarity score\n@@ -1820,7 +1820,7 @@ def forward(\n         >>> dataset = load_dataset(\"hf-internal-testing/ashraq-esc50-1-dog-example\")\n         >>> audio_sample = dataset[\"train\"][\"audio\"][0][\"array\"]\n \n-        >>> inputs = processor(audios=audio_sample, return_tensors=\"pt\")\n+        >>> inputs = processor(audio=audio_sample, return_tensors=\"pt\")\n         >>> outputs = model(**inputs)\n         >>> audio_embeds = outputs.audio_embeds\n         ```\"\"\""
        },
        {
            "sha": "cbec5473e4589dd3024cc5b0cdb8f3e104c6d450",
            "filename": "src/transformers/models/clap/processing_clap.py",
            "status": "modified",
            "additions": 1,
            "deletions": 29,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fclap%2Fprocessing_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fclap%2Fprocessing_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fprocessing_clap.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -16,13 +16,8 @@\n Audio/Text processor class for CLAP\n \"\"\"\n \n-from typing import Optional, Union\n-\n-from ...audio_utils import AudioInput\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n-from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...processing_utils import ProcessorMixin\n from ...utils import logging\n-from ...utils.deprecation import deprecate_kwarg\n \n \n logger = logging.get_logger(__name__)\n@@ -45,28 +40,5 @@ class ClapProcessor(ProcessorMixin):\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n \n-    @deprecate_kwarg(\"audios\", version=\"v4.59.0\", new_name=\"audio\")\n-    def __call__(\n-        self,\n-        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n-        audios: Optional[AudioInput] = None,\n-        audio: Optional[AudioInput] = None,\n-        **kwargs: Unpack[ProcessingKwargs],\n-    ):\n-        \"\"\"\n-        Forwards the `audio` and `sampling_rate` arguments to [`~ClapFeatureExtractor.__call__`] and the `text`\n-        argument to [`~RobertaTokenizerFast.__call__`]. Please refer to the docstring of the above two methods for more\n-        information.\n-        \"\"\"\n-        # The `deprecate_kwarg` will not work if the inputs are passed as arguments, so we check\n-        # again that the correct naming is used\n-        if audios is not None and audio is None:\n-            logger.warning(\n-                \"Using `audios` keyword argument is deprecated when calling ClapProcessor, instead use `audio`.\"\n-            )\n-            audio = audios\n-\n-        return super().__call__(text=text, audio=audio, **kwargs)\n-\n \n __all__ = [\"ClapProcessor\"]"
        },
        {
            "sha": "7d7aff908db866680683ecfb62d68aa58fd98f9c",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 94,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -826,18 +826,7 @@ def __init__(\n         pad_size: Optional[dict[str, int]] = None,\n         **kwargs,\n     ) -> None:\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            do_pad = kwargs.pop(\"pad_and_return_pixel_mask\")\n-\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` parameter is deprecated and will be removed in v4.26. \"\n-                \"Please specify in `size['longest_edge'] instead`.\",\n-            )\n-            max_size = kwargs.pop(\"max_size\")\n-        else:\n-            max_size = None if size is None else 1333\n-\n+        max_size = None if size is None else kwargs.pop(\"max_size\", 1333)\n         size = size if size is not None else {\"shortest_edge\": 800, \"longest_edge\": 1333}\n         size = get_size_dict(size, max_size=max_size, default_to_square=False)\n \n@@ -856,7 +845,7 @@ def __init__(\n         self.do_convert_annotations = do_convert_annotations\n         self.image_mean = image_mean if image_mean is not None else IMAGENET_DEFAULT_MEAN\n         self.image_std = image_std if image_std is not None else IMAGENET_DEFAULT_STD\n-        self.do_pad = do_pad\n+        self.do_pad = kwargs.pop(\"pad_and_return_pixel_mask\", do_pad)\n         self.pad_size = pad_size\n         self._valid_processor_keys = [\n             \"images\",\n@@ -880,21 +869,6 @@ def __init__(\n             \"input_data_format\",\n         ]\n \n-    @classmethod\n-    # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.from_dict with Detr->ConditionalDetr\n-    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n-        \"\"\"\n-        Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n-        created using from_dict and kwargs e.g. `ConditionalDetrImageProcessor.from_pretrained(checkpoint, size=600,\n-        max_size=800)`\n-        \"\"\"\n-        image_processor_dict = image_processor_dict.copy()\n-        if \"max_size\" in kwargs:\n-            image_processor_dict[\"max_size\"] = kwargs.pop(\"max_size\")\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            image_processor_dict[\"pad_and_return_pixel_mask\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n-        return super().from_dict(image_processor_dict, **kwargs)\n-\n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.prepare_annotation with DETR->ConditionalDetr\n     def prepare_annotation(\n         self,\n@@ -963,15 +937,7 @@ def resize(\n             input_data_format (`ChannelDimension` or `str`, *optional*):\n                 The channel dimension format of the input image. If not provided, it will be inferred.\n         \"\"\"\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` parameter is deprecated and will be removed in v4.26. \"\n-                \"Please specify in `size['longest_edge'] instead`.\",\n-            )\n-            max_size = kwargs.pop(\"max_size\")\n-        else:\n-            max_size = None\n-        size = get_size_dict(size, max_size=max_size, default_to_square=False)\n+        size = get_size_dict(size, max_size=None, default_to_square=False)\n         if \"shortest_edge\" in size and \"longest_edge\" in size:\n             new_size = get_resize_output_image_size(\n                 image, size[\"shortest_edge\"], size[\"longest_edge\"], input_data_format=input_data_format\n@@ -1308,19 +1274,6 @@ def preprocess(\n                 provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n                 height and width in the batch.\n         \"\"\"\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            logger.warning_once(\n-                \"The `pad_and_return_pixel_mask` argument is deprecated and will be removed in a future version, \"\n-                \"use `do_pad` instead.\"\n-            )\n-            do_pad = kwargs.pop(\"pad_and_return_pixel_mask\")\n-\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` argument is deprecated and will be removed in a future version, use\"\n-                \" `size['longest_edge']` instead.\"\n-            )\n-            size = kwargs.pop(\"max_size\")\n \n         do_resize = self.do_resize if do_resize is None else do_resize\n         size = self.size if size is None else size\n@@ -1472,50 +1425,6 @@ def preprocess(\n \n         return encoded_inputs\n \n-    def post_process(self, outputs, target_sizes):\n-        \"\"\"\n-        Converts the output of [`ConditionalDetrForObjectDetection`] into the format expected by the Pascal VOC format (xmin, ymin, xmax, ymax).\n-\n-        Args:\n-            outputs ([`ConditionalDetrObjectDetectionOutput`]):\n-                Raw outputs of the model.\n-            target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n-                Tensor containing the size (h, w) of each image of the batch. For evaluation, this must be the original\n-                image size (before any data augmentation). For visualization, this should be the image size after data\n-                augment, but before padding.\n-        Returns:\n-            `list[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n-            in the batch as predicted by the model.\n-        \"\"\"\n-        logging.warning_once(\n-            \"`post_process` is deprecated and will be removed in v5 of Transformers, please use\"\n-            \" `post_process_object_detection` instead, with `threshold=0.` for equivalent results.\",\n-        )\n-\n-        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n-\n-        if len(out_logits) != len(target_sizes):\n-            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n-        if target_sizes.shape[1] != 2:\n-            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n-\n-        prob = out_logits.sigmoid()\n-        topk_values, topk_indexes = torch.topk(prob.view(out_logits.shape[0], -1), 300, dim=1)\n-        scores = topk_values\n-        topk_boxes = torch.div(topk_indexes, out_logits.shape[2], rounding_mode=\"floor\")\n-        labels = topk_indexes % out_logits.shape[2]\n-        boxes = center_to_corners_format(out_bbox)\n-        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n-\n-        # and from relative [0, 1] to absolute [0, height] coordinates\n-        img_h, img_w = target_sizes.unbind(1)\n-        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n-        boxes = boxes * scale_fct[:, None, :]\n-\n-        results = [{\"scores\": s, \"labels\": l, \"boxes\": b} for s, l, b in zip(scores, labels, boxes)]\n-\n-        return results\n-\n     # Copied from transformers.models.deformable_detr.image_processing_deformable_detr.DeformableDetrImageProcessor.post_process_object_detection with DeformableDetr->ConditionalDetr\n     def post_process_object_detection(\n         self, outputs, threshold: float = 0.5, target_sizes: Union[TensorType, list[tuple]] = None, top_k: int = 100"
        },
        {
            "sha": "51afbd98bb8525f8d116c6f2947ed93245ddf973",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 93,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -27,7 +27,6 @@\n     AnnotationFormat,\n     AnnotationType,\n     ChannelDimension,\n-    ImageInput,\n     PILImageResampling,\n     get_image_size,\n     validate_annotations,\n@@ -263,19 +262,10 @@ class ConditionalDetrImageProcessorFast(BaseImageProcessorFast):\n     valid_kwargs = ConditionalDetrImageProcessorKwargs\n \n     def __init__(self, **kwargs: Unpack[ConditionalDetrImageProcessorKwargs]) -> None:\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n+        kwargs.setdefault(\"do_pad\", kwargs.pop(\"pad_and_return_pixel_mask\", self.do_pad))\n \n         size = kwargs.pop(\"size\", None)\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` parameter is deprecated and will be removed in v4.26. \"\n-                \"Please specify in `size['longest_edge'] instead`.\",\n-            )\n-            max_size = kwargs.pop(\"max_size\")\n-        else:\n-            max_size = None if size is None else 1333\n-\n+        max_size = None if size is None else kwargs.pop(\"max_size\", 1333)\n         size = size if size is not None else {\"shortest_edge\": 800, \"longest_edge\": 1333}\n         self.size = get_size_dict(size, max_size=max_size, default_to_square=False)\n \n@@ -287,20 +277,6 @@ def __init__(self, **kwargs: Unpack[ConditionalDetrImageProcessorKwargs]) -> Non\n \n         super().__init__(**kwargs)\n \n-    @classmethod\n-    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n-        \"\"\"\n-        Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n-        created using from_dict and kwargs e.g. `ConditionalDetrImageProcessorFast.from_pretrained(checkpoint, size=600,\n-        max_size=800)`\n-        \"\"\"\n-        image_processor_dict = image_processor_dict.copy()\n-        if \"max_size\" in kwargs:\n-            image_processor_dict[\"max_size\"] = kwargs.pop(\"max_size\")\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            image_processor_dict[\"pad_and_return_pixel_mask\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n-        return super().from_dict(image_processor_dict, **kwargs)\n-\n     def prepare_annotation(\n         self,\n         image: torch.Tensor,\n@@ -520,28 +496,6 @@ def pad(\n \n         return image, pixel_mask, annotation\n \n-    @auto_docstring\n-    def preprocess(\n-        self,\n-        images: ImageInput,\n-        **kwargs: Unpack[ConditionalDetrImageProcessorKwargs],\n-    ) -> BatchFeature:\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n-            logger.warning_once(\n-                \"The `pad_and_return_pixel_mask` argument is deprecated and will be removed in a future version, \"\n-                \"use `do_pad` instead.\"\n-            )\n-\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` argument is deprecated and will be removed in a future version, use\"\n-                \" `size['longest_edge']` instead.\"\n-            )\n-            kwargs[\"size\"] = kwargs.pop(\"max_size\")\n-\n-        return super().preprocess(images, **kwargs)\n-\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n@@ -658,51 +612,6 @@ def _preprocess(\n             ]\n         return encoded_inputs\n \n-    def post_process(self, outputs, target_sizes):\n-        \"\"\"\n-        Converts the output of [`ConditionalDetrForObjectDetection`] into the format expected by the Pascal VOC format (xmin, ymin, xmax, ymax).\n-        Only supports PyTorch.\n-\n-        Args:\n-            outputs ([`ConditionalDetrObjectDetectionOutput`]):\n-                Raw outputs of the model.\n-            target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n-                Tensor containing the size (h, w) of each image of the batch. For evaluation, this must be the original\n-                image size (before any data augmentation). For visualization, this should be the image size after data\n-                augment, but before padding.\n-        Returns:\n-            `list[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n-            in the batch as predicted by the model.\n-        \"\"\"\n-        logging.warning_once(\n-            \"`post_process` is deprecated and will be removed in v5 of Transformers, please use\"\n-            \" `post_process_object_detection` instead, with `threshold=0.` for equivalent results.\",\n-        )\n-\n-        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n-\n-        if len(out_logits) != len(target_sizes):\n-            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n-        if target_sizes.shape[1] != 2:\n-            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n-\n-        prob = out_logits.sigmoid()\n-        topk_values, topk_indexes = torch.topk(prob.view(out_logits.shape[0], -1), 300, dim=1)\n-        scores = topk_values\n-        topk_boxes = torch.div(topk_indexes, out_logits.shape[2], rounding_mode=\"floor\")\n-        labels = topk_indexes % out_logits.shape[2]\n-        boxes = center_to_corners_format(out_bbox)\n-        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n-\n-        # and from relative [0, 1] to absolute [0, height] coordinates\n-        img_h, img_w = target_sizes.unbind(1)\n-        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n-        boxes = boxes * scale_fct[:, None, :]\n-\n-        results = [{\"scores\": s, \"labels\": l, \"boxes\": b} for s, l, b in zip(scores, labels, boxes)]\n-\n-        return results\n-\n     def post_process_object_detection(\n         self, outputs, threshold: float = 0.5, target_sizes: Union[TensorType, list[tuple]] = None, top_k: int = 100\n     ):"
        },
        {
            "sha": "c2cbdd26e31dd00806a44cdd639d3fcc6c57094b",
            "filename": "src/transformers/models/conditional_detr/modular_conditional_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 54,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodular_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodular_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodular_conditional_detr.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -17,51 +17,6 @@\n \n \n class ConditionalDetrImageProcessorFast(DetrImageProcessorFast):\n-    def post_process(self, outputs, target_sizes):\n-        \"\"\"\n-        Converts the output of [`ConditionalDetrForObjectDetection`] into the format expected by the Pascal VOC format (xmin, ymin, xmax, ymax).\n-        Only supports PyTorch.\n-\n-        Args:\n-            outputs ([`ConditionalDetrObjectDetectionOutput`]):\n-                Raw outputs of the model.\n-            target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n-                Tensor containing the size (h, w) of each image of the batch. For evaluation, this must be the original\n-                image size (before any data augmentation). For visualization, this should be the image size after data\n-                augment, but before padding.\n-        Returns:\n-            `list[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n-            in the batch as predicted by the model.\n-        \"\"\"\n-        logging.warning_once(\n-            \"`post_process` is deprecated and will be removed in v5 of Transformers, please use\"\n-            \" `post_process_object_detection` instead, with `threshold=0.` for equivalent results.\",\n-        )\n-\n-        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n-\n-        if len(out_logits) != len(target_sizes):\n-            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n-        if target_sizes.shape[1] != 2:\n-            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n-\n-        prob = out_logits.sigmoid()\n-        topk_values, topk_indexes = torch.topk(prob.view(out_logits.shape[0], -1), 300, dim=1)\n-        scores = topk_values\n-        topk_boxes = torch.div(topk_indexes, out_logits.shape[2], rounding_mode=\"floor\")\n-        labels = topk_indexes % out_logits.shape[2]\n-        boxes = center_to_corners_format(out_bbox)\n-        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n-\n-        # and from relative [0, 1] to absolute [0, height] coordinates\n-        img_h, img_w = target_sizes.unbind(1)\n-        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n-        boxes = boxes * scale_fct[:, None, :]\n-\n-        results = [{\"scores\": s, \"labels\": l, \"boxes\": b} for s, l, b in zip(scores, labels, boxes)]\n-\n-        return results\n-\n     def post_process_object_detection(\n         self, outputs, threshold: float = 0.5, target_sizes: Union[TensorType, list[tuple]] = None, top_k: int = 100\n     ):\n@@ -121,14 +76,5 @@ def post_process_object_detection(\n \n         return results\n \n-    def post_process_segmentation(self):\n-        raise NotImplementedError(\"Segmentation post-processing is not implemented for Conditional DETR yet.\")\n-\n-    def post_process_instance(self):\n-        raise NotImplementedError(\"Instance post-processing is not implemented for Conditional DETR yet.\")\n-\n-    def post_process_panoptic(self):\n-        raise NotImplementedError(\"Panoptic post-processing is not implemented for Conditional DETR yet.\")\n-\n \n __all__ = [\"ConditionalDetrImageProcessorFast\"]"
        },
        {
            "sha": "5ce39db6f4032488de5beb7dac2d5b74fc35f629",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 48,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -839,18 +839,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -954,18 +942,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameters will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -1069,18 +1045,6 @@ def __init__(self, config):\n \n         self.init_weights()\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -1237,18 +1201,6 @@ def __init__(self, config):\n \n         self.init_weights()\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will"
        },
        {
            "sha": "4a906aac83a3c42cbce266c236d7d651bae114d0",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 20,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -16,7 +16,6 @@\n \n import collections.abc\n import math\n-import warnings\n from dataclasses import dataclass\n from typing import Optional, Union\n \n@@ -162,14 +161,7 @@ def forward(\n         self,\n         pixel_values: torch.Tensor,\n         bool_masked_pos: Optional[torch.BoolTensor] = None,\n-        interpolate_pos_encoding: Optional[bool] = None,\n     ) -> torch.Tensor:\n-        if self.position_embeddings is not None and interpolate_pos_encoding is not None:\n-            warnings.warn(\n-                \"`interpolate_pos_encoding` argument has no effect for BEiTEmbeddings, embeddings are always \"\n-                \"interpolated to the input image size. The argument will be removed in transformers v4.51.0.\"\n-            )\n-\n         _, _, height, width = pixel_values.shape\n         embeddings, (patch_height, patch_width) = self.patch_embeddings(pixel_values)\n         batch_size, seq_len, _ = embeddings.size()\n@@ -327,19 +319,9 @@ def forward(\n     ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n         if output_attentions:\n             logger.warning_once(\n-                \"`Data2VecVisionSdpaSelfAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not \"\n-                \"support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                \"but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n-                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                output_attentions=output_attentions,\n-                relative_position_bias=relative_position_bias,\n-                interpolate_pos_encoding=interpolate_pos_encoding,\n-                resolution=resolution,\n+                f\"{self.__class__.__name__} does not support `output_attentions=True`. The returned attention weights will \"\n+                \"be `None`. If you want to get attention weights, please set `attn_implementation='eager'` when loading the model.\"\n             )\n-\n         batch_size, seq_length, _ = hidden_states.shape\n         query_layer = (\n             self.query(hidden_states)"
        },
        {
            "sha": "070439d7de4ee33bc0725b0bd9b94132fabbc8da",
            "filename": "src/transformers/models/data2vec/modular_data2vec_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -201,9 +201,6 @@ def __init__(self, config: Data2VecAudioConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def freeze_feature_extractor(self):\n-        raise AttributeError(\"Not needed for Data2VecAudio\")\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will"
        },
        {
            "sha": "e1785ef9a851627e25cf8b4abaae04e838185708",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -19,7 +19,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import warnings\n from collections.abc import Callable\n from typing import Optional, Union\n \n@@ -346,10 +345,6 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        if \"padding_mask\" in kwargs:\n-            warnings.warn(\n-                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n-            )\n         batch_size, seq_length = hidden_states.shape[:-1]\n         query_shape = (batch_size, seq_length, -1, self.qk_head_dim)\n         key_shape = (batch_size, seq_length, -1, self.qk_nope_head_dim + self.v_head_dim)"
        },
        {
            "sha": "6994ba582e5afc43d1298928e87ddd39b6fd6936",
            "filename": "src/transformers/models/deepseek_v2/modular_deepseek_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -13,7 +13,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import warnings\n from collections.abc import Callable\n from typing import Optional\n \n@@ -365,10 +364,6 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        if \"padding_mask\" in kwargs:\n-            warnings.warn(\n-                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n-            )\n         batch_size, seq_length = hidden_states.shape[:-1]\n         query_shape = (batch_size, seq_length, -1, self.qk_head_dim)\n         key_shape = (batch_size, seq_length, -1, self.qk_nope_head_dim + self.v_head_dim)"
        },
        {
            "sha": "14b9aa31b5eb6c91b331cced524c35ac8724efc2",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 95,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -831,18 +831,7 @@ def __init__(\n         pad_size: Optional[dict[str, int]] = None,\n         **kwargs,\n     ) -> None:\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            do_pad = kwargs.pop(\"pad_and_return_pixel_mask\")\n-\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` parameter is deprecated and will be removed in v4.26. \"\n-                \"Please specify in `size['longest_edge'] instead`.\",\n-            )\n-            max_size = kwargs.pop(\"max_size\")\n-        else:\n-            max_size = None if size is None else 1333\n-\n+        max_size = None if size is None else kwargs.pop(\"max_size\", 1333)\n         size = size if size is not None else {\"shortest_edge\": 800, \"longest_edge\": 1333}\n         size = get_size_dict(size, max_size=max_size, default_to_square=False)\n \n@@ -861,7 +850,7 @@ def __init__(\n         self.do_convert_annotations = do_convert_annotations\n         self.image_mean = image_mean if image_mean is not None else IMAGENET_DEFAULT_MEAN\n         self.image_std = image_std if image_std is not None else IMAGENET_DEFAULT_STD\n-        self.do_pad = do_pad\n+        self.do_pad = kwargs.pop(\"pad_and_return_pixel_mask\", do_pad)\n         self.pad_size = pad_size\n         self._valid_processor_keys = [\n             \"images\",\n@@ -885,21 +874,6 @@ def __init__(\n             \"input_data_format\",\n         ]\n \n-    @classmethod\n-    # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.from_dict with Detr->DeformableDetr\n-    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n-        \"\"\"\n-        Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n-        created using from_dict and kwargs e.g. `DeformableDetrImageProcessor.from_pretrained(checkpoint, size=600,\n-        max_size=800)`\n-        \"\"\"\n-        image_processor_dict = image_processor_dict.copy()\n-        if \"max_size\" in kwargs:\n-            image_processor_dict[\"max_size\"] = kwargs.pop(\"max_size\")\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            image_processor_dict[\"pad_and_return_pixel_mask\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n-        return super().from_dict(image_processor_dict, **kwargs)\n-\n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.prepare_annotation with DETR->DeformableDetr\n     def prepare_annotation(\n         self,\n@@ -968,15 +942,7 @@ def resize(\n             input_data_format (`ChannelDimension` or `str`, *optional*):\n                 The channel dimension format of the input image. If not provided, it will be inferred.\n         \"\"\"\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` parameter is deprecated and will be removed in v4.26. \"\n-                \"Please specify in `size['longest_edge'] instead`.\",\n-            )\n-            max_size = kwargs.pop(\"max_size\")\n-        else:\n-            max_size = None\n-        size = get_size_dict(size, max_size=max_size, default_to_square=False)\n+        size = get_size_dict(size, max_size=None, default_to_square=False)\n         if \"shortest_edge\" in size and \"longest_edge\" in size:\n             new_size = get_resize_output_image_size(\n                 image, size[\"shortest_edge\"], size[\"longest_edge\"], input_data_format=input_data_format\n@@ -1313,19 +1279,6 @@ def preprocess(\n                 provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n                 height and width in the batch.\n         \"\"\"\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            logger.warning_once(\n-                \"The `pad_and_return_pixel_mask` argument is deprecated and will be removed in a future version, \"\n-                \"use `do_pad` instead.\"\n-            )\n-            do_pad = kwargs.pop(\"pad_and_return_pixel_mask\")\n-\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` argument is deprecated and will be removed in a future version, use\"\n-                \" `size['longest_edge']` instead.\"\n-            )\n-            size = kwargs.pop(\"max_size\")\n \n         do_resize = self.do_resize if do_resize is None else do_resize\n         size = self.size if size is None else size\n@@ -1477,51 +1430,6 @@ def preprocess(\n \n         return encoded_inputs\n \n-    def post_process(self, outputs, target_sizes):\n-        \"\"\"\n-        Converts the raw output of [`DeformableDetrForObjectDetection`] into final bounding boxes in (top_left_x,\n-        top_left_y, bottom_right_x, bottom_right_y) format.\n-\n-        Args:\n-            outputs ([`DeformableDetrObjectDetectionOutput`]):\n-                Raw outputs of the model.\n-            target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n-                Tensor containing the size (height, width) of each image of the batch. For evaluation, this must be the\n-                original image size (before any data augmentation). For visualization, this should be the image size\n-                after data augment, but before padding.\n-        Returns:\n-            `list[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n-            in the batch as predicted by the model.\n-        \"\"\"\n-        logger.warning_once(\n-            \"`post_process` is deprecated and will be removed in v5 of Transformers, please use\"\n-            \" `post_process_object_detection` instead, with `threshold=0.` for equivalent results.\",\n-        )\n-\n-        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n-\n-        if len(out_logits) != len(target_sizes):\n-            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n-        if target_sizes.shape[1] != 2:\n-            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n-\n-        prob = out_logits.sigmoid()\n-        topk_values, topk_indexes = torch.topk(prob.view(out_logits.shape[0], -1), 100, dim=1)\n-        scores = topk_values\n-        topk_boxes = torch.div(topk_indexes, out_logits.shape[2], rounding_mode=\"floor\")\n-        labels = topk_indexes % out_logits.shape[2]\n-        boxes = center_to_corners_format(out_bbox)\n-        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n-\n-        # and from relative [0, 1] to absolute [0, height] coordinates\n-        img_h, img_w = target_sizes.unbind(1)\n-        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n-        boxes = boxes * scale_fct[:, None, :]\n-\n-        results = [{\"scores\": s, \"labels\": l, \"boxes\": b} for s, l, b in zip(scores, labels, boxes)]\n-\n-        return results\n-\n     def post_process_object_detection(\n         self, outputs, threshold: float = 0.5, target_sizes: Union[TensorType, list[tuple]] = None, top_k: int = 100\n     ):"
        },
        {
            "sha": "d4a7ca2a8380e0b79455db1cb6ef9e5cd361419d",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 96,
            "changes": 99,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -26,19 +26,16 @@\n     AnnotationFormat,\n     AnnotationType,\n     ChannelDimension,\n-    ImageInput,\n     PILImageResampling,\n     get_image_size,\n     validate_annotations,\n )\n from ...processing_utils import Unpack\n-from ...utils import TensorType, auto_docstring, logging\n+from ...utils import TensorType, auto_docstring\n from ...utils.import_utils import requires\n from .image_processing_deformable_detr import DeformableDetrImageProcessorKwargs, get_size_with_aspect_ratio\n \n \n-logger = logging.get_logger(__name__)\n-\n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n \n \n@@ -256,19 +253,10 @@ class DeformableDetrImageProcessorFast(BaseImageProcessorFast):\n     valid_kwargs = DeformableDetrImageProcessorKwargs\n \n     def __init__(self, **kwargs: Unpack[DeformableDetrImageProcessorKwargs]) -> None:\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n+        kwargs.setdefault(\"do_pad\", kwargs.pop(\"pad_and_return_pixel_mask\", self.do_pad))\n \n         size = kwargs.pop(\"size\", None)\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` parameter is deprecated and will be removed in v4.26. \"\n-                \"Please specify in `size['longest_edge'] instead`.\",\n-            )\n-            max_size = kwargs.pop(\"max_size\")\n-        else:\n-            max_size = None if size is None else 1333\n-\n+        max_size = None if size is None else kwargs.pop(\"max_size\", 1333)\n         size = size if size is not None else {\"shortest_edge\": 800, \"longest_edge\": 1333}\n         self.size = get_size_dict(size, max_size=max_size, default_to_square=False)\n \n@@ -280,20 +268,6 @@ def __init__(self, **kwargs: Unpack[DeformableDetrImageProcessorKwargs]) -> None\n \n         super().__init__(**kwargs)\n \n-    @classmethod\n-    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n-        \"\"\"\n-        Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n-        created using from_dict and kwargs e.g. `DeformableDetrImageProcessorFast.from_pretrained(checkpoint, size=600,\n-        max_size=800)`\n-        \"\"\"\n-        image_processor_dict = image_processor_dict.copy()\n-        if \"max_size\" in kwargs:\n-            image_processor_dict[\"max_size\"] = kwargs.pop(\"max_size\")\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            image_processor_dict[\"pad_and_return_pixel_mask\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n-        return super().from_dict(image_processor_dict, **kwargs)\n-\n     def prepare_annotation(\n         self,\n         image: torch.Tensor,\n@@ -513,28 +487,6 @@ def pad(\n \n         return image, pixel_mask, annotation\n \n-    @auto_docstring\n-    def preprocess(\n-        self,\n-        images: ImageInput,\n-        **kwargs: Unpack[DeformableDetrImageProcessorKwargs],\n-    ) -> BatchFeature:\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n-            logger.warning_once(\n-                \"The `pad_and_return_pixel_mask` argument is deprecated and will be removed in a future version, \"\n-                \"use `do_pad` instead.\"\n-            )\n-\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` argument is deprecated and will be removed in a future version, use\"\n-                \" `size['longest_edge']` instead.\"\n-            )\n-            kwargs[\"size\"] = kwargs.pop(\"max_size\")\n-\n-        return super().preprocess(images, **kwargs)\n-\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n@@ -651,51 +603,6 @@ def _preprocess(\n             ]\n         return encoded_inputs\n \n-    def post_process(self, outputs, target_sizes):\n-        \"\"\"\n-        Converts the raw output of [`DeformableDetrForObjectDetection`] into final bounding boxes in (top_left_x,\n-        top_left_y, bottom_right_x, bottom_right_y) format. Only supports PyTorch.\n-\n-        Args:\n-            outputs ([`DeformableDetrObjectDetectionOutput`]):\n-                Raw outputs of the model.\n-            target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n-                Tensor containing the size (height, width) of each image of the batch. For evaluation, this must be the\n-                original image size (before any data augmentation). For visualization, this should be the image size\n-                after data augment, but before padding.\n-        Returns:\n-            `list[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n-            in the batch as predicted by the model.\n-        \"\"\"\n-        logger.warning_once(\n-            \"`post_process` is deprecated and will be removed in v5 of Transformers, please use\"\n-            \" `post_process_object_detection` instead, with `threshold=0.` for equivalent results.\",\n-        )\n-\n-        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n-\n-        if len(out_logits) != len(target_sizes):\n-            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n-        if target_sizes.shape[1] != 2:\n-            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n-\n-        prob = out_logits.sigmoid()\n-        topk_values, topk_indexes = torch.topk(prob.view(out_logits.shape[0], -1), 100, dim=1)\n-        scores = topk_values\n-        topk_boxes = torch.div(topk_indexes, out_logits.shape[2], rounding_mode=\"floor\")\n-        labels = topk_indexes % out_logits.shape[2]\n-        boxes = center_to_corners_format(out_bbox)\n-        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n-\n-        # and from relative [0, 1] to absolute [0, height] coordinates\n-        img_h, img_w = target_sizes.unbind(1)\n-        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n-        boxes = boxes * scale_fct[:, None, :]\n-\n-        results = [{\"scores\": s, \"labels\": l, \"boxes\": b} for s, l, b in zip(scores, labels, boxes)]\n-\n-        return results\n-\n     def post_process_object_detection(\n         self, outputs, threshold: float = 0.5, target_sizes: Union[TensorType, list[tuple]] = None, top_k: int = 100\n     ):"
        },
        {
            "sha": "450297519e1f77d8609adaaf1e58b8513e6d5317",
            "filename": "src/transformers/models/deformable_detr/modular_deformable_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 54,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodular_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodular_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodular_deformable_detr.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -15,51 +15,6 @@\n \n \n class DeformableDetrImageProcessorFast(DetrImageProcessorFast):\n-    def post_process(self, outputs, target_sizes):\n-        \"\"\"\n-        Converts the raw output of [`DeformableDetrForObjectDetection`] into final bounding boxes in (top_left_x,\n-        top_left_y, bottom_right_x, bottom_right_y) format. Only supports PyTorch.\n-\n-        Args:\n-            outputs ([`DeformableDetrObjectDetectionOutput`]):\n-                Raw outputs of the model.\n-            target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n-                Tensor containing the size (height, width) of each image of the batch. For evaluation, this must be the\n-                original image size (before any data augmentation). For visualization, this should be the image size\n-                after data augment, but before padding.\n-        Returns:\n-            `list[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n-            in the batch as predicted by the model.\n-        \"\"\"\n-        logger.warning_once(\n-            \"`post_process` is deprecated and will be removed in v5 of Transformers, please use\"\n-            \" `post_process_object_detection` instead, with `threshold=0.` for equivalent results.\",\n-        )\n-\n-        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n-\n-        if len(out_logits) != len(target_sizes):\n-            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n-        if target_sizes.shape[1] != 2:\n-            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n-\n-        prob = out_logits.sigmoid()\n-        topk_values, topk_indexes = torch.topk(prob.view(out_logits.shape[0], -1), 100, dim=1)\n-        scores = topk_values\n-        topk_boxes = torch.div(topk_indexes, out_logits.shape[2], rounding_mode=\"floor\")\n-        labels = topk_indexes % out_logits.shape[2]\n-        boxes = center_to_corners_format(out_bbox)\n-        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n-\n-        # and from relative [0, 1] to absolute [0, height] coordinates\n-        img_h, img_w = target_sizes.unbind(1)\n-        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n-        boxes = boxes * scale_fct[:, None, :]\n-\n-        results = [{\"scores\": s, \"labels\": l, \"boxes\": b} for s, l, b in zip(scores, labels, boxes)]\n-\n-        return results\n-\n     def post_process_object_detection(\n         self, outputs, threshold: float = 0.5, target_sizes: Union[TensorType, list[tuple]] = None, top_k: int = 100\n     ):\n@@ -119,15 +74,6 @@ def post_process_object_detection(\n \n         return results\n \n-    def post_process_segmentation(self):\n-        raise NotImplementedError(\"Segmentation post-processing is not implemented for Deformable DETR yet.\")\n-\n-    def post_process_instance(self):\n-        raise NotImplementedError(\"Instance post-processing is not implemented for Deformable DETR yet.\")\n-\n-    def post_process_panoptic(self):\n-        raise NotImplementedError(\"Panoptic post-processing is not implemented for Deformable DETR yet.\")\n-\n     def post_process_instance_segmentation(self):\n         raise NotImplementedError(\"Segmentation post-processing is not implemented for Deformable DETR yet.\")\n "
        },
        {
            "sha": "b3fa7169c1105224f57cdcc7d6a3c547c68470a7",
            "filename": "src/transformers/models/deprecated/deta/image_processing_deta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fimage_processing_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fimage_processing_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fimage_processing_deta.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -499,9 +499,6 @@ def __init__(\n         pad_size: Optional[dict[str, int]] = None,\n         **kwargs,\n     ) -> None:\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            do_pad = kwargs.pop(\"pad_and_return_pixel_mask\")\n-\n         size = size if size is not None else {\"shortest_edge\": 800, \"longest_edge\": 1333}\n         size = get_size_dict(size, default_to_square=False)\n \n@@ -519,7 +516,7 @@ def __init__(\n         self.do_convert_annotations = do_convert_annotations\n         self.image_mean = image_mean if image_mean is not None else IMAGENET_DEFAULT_MEAN\n         self.image_std = image_std if image_std is not None else IMAGENET_DEFAULT_STD\n-        self.do_pad = do_pad\n+        self.do_pad = kwargs.pop(\"pad_and_return_pixel_mask\", do_pad)\n         self.pad_size = pad_size\n \n     def prepare_annotation("
        },
        {
            "sha": "370ea470291ecf8d429f4de4e62747c2c7aff762",
            "filename": "src/transformers/models/detr/image_processing_detr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 319,
            "changes": 322,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -815,18 +815,7 @@ def __init__(\n         pad_size: Optional[dict[str, int]] = None,\n         **kwargs,\n     ) -> None:\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            do_pad = kwargs.pop(\"pad_and_return_pixel_mask\")\n-\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` parameter is deprecated and will be removed in v4.26. \"\n-                \"Please specify in `size['longest_edge'] instead`.\",\n-            )\n-            max_size = kwargs.pop(\"max_size\")\n-        else:\n-            max_size = None if size is None else 1333\n-\n+        max_size = None if size is None else kwargs.pop(\"max_size\", 1333)\n         size = size if size is not None else {\"shortest_edge\": 800, \"longest_edge\": 1333}\n         size = get_size_dict(size, max_size=max_size, default_to_square=False)\n \n@@ -845,7 +834,7 @@ def __init__(\n         self.do_convert_annotations = do_convert_annotations\n         self.image_mean = image_mean if image_mean is not None else IMAGENET_DEFAULT_MEAN\n         self.image_std = image_std if image_std is not None else IMAGENET_DEFAULT_STD\n-        self.do_pad = do_pad\n+        self.do_pad = kwargs.pop(\"pad_and_return_pixel_mask\", do_pad)\n         self.pad_size = pad_size\n         self._valid_processor_keys = [\n             \"images\",\n@@ -869,20 +858,6 @@ def __init__(\n             \"input_data_format\",\n         ]\n \n-    @classmethod\n-    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n-        \"\"\"\n-        Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n-        created using from_dict and kwargs e.g. `DetrImageProcessor.from_pretrained(checkpoint, size=600,\n-        max_size=800)`\n-        \"\"\"\n-        image_processor_dict = image_processor_dict.copy()\n-        if \"max_size\" in kwargs:\n-            image_processor_dict[\"max_size\"] = kwargs.pop(\"max_size\")\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            image_processor_dict[\"pad_and_return_pixel_mask\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n-        return super().from_dict(image_processor_dict, **kwargs)\n-\n     def prepare_annotation(\n         self,\n         image: np.ndarray,\n@@ -949,15 +924,7 @@ def resize(\n             input_data_format (`ChannelDimension` or `str`, *optional*):\n                 The channel dimension format of the input image. If not provided, it will be inferred.\n         \"\"\"\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` parameter is deprecated and will be removed in v4.26. \"\n-                \"Please specify in `size['longest_edge'] instead`.\",\n-            )\n-            max_size = kwargs.pop(\"max_size\")\n-        else:\n-            max_size = None\n-        size = get_size_dict(size, max_size=max_size, default_to_square=False)\n+        size = get_size_dict(size, max_size=None, default_to_square=False)\n         if \"shortest_edge\" in size and \"longest_edge\" in size:\n             new_size = get_resize_output_image_size(\n                 image, size[\"shortest_edge\"], size[\"longest_edge\"], input_data_format=input_data_format\n@@ -1288,19 +1255,6 @@ def preprocess(\n                 provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n                 height and width in the batch.\n         \"\"\"\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            logger.warning_once(\n-                \"The `pad_and_return_pixel_mask` argument is deprecated and will be removed in a future version, \"\n-                \"use `do_pad` instead.\"\n-            )\n-            do_pad = kwargs.pop(\"pad_and_return_pixel_mask\")\n-\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` argument is deprecated and will be removed in a future version, use\"\n-                \" `size['longest_edge']` instead.\"\n-            )\n-            size = kwargs.pop(\"max_size\")\n \n         do_resize = self.do_resize if do_resize is None else do_resize\n         size = self.size if size is None else size\n@@ -1452,276 +1406,6 @@ def preprocess(\n \n         return encoded_inputs\n \n-    # inspired by https://github.com/facebookresearch/detr/blob/master/models/detr.py#L258\n-    def post_process(self, outputs, target_sizes):\n-        \"\"\"\n-        Converts the raw output of [`DetrForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n-        bottom_right_x, bottom_right_y) format.\n-\n-        Args:\n-            outputs ([`DetrObjectDetectionOutput`]):\n-                Raw outputs of the model.\n-            target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n-                Tensor containing the size (height, width) of each image of the batch. For evaluation, this must be the\n-                original image size (before any data augmentation). For visualization, this should be the image size\n-                after data augment, but before padding.\n-        Returns:\n-            `list[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n-            in the batch as predicted by the model.\n-        \"\"\"\n-        logger.warning_once(\n-            \"`post_process` is deprecated and will be removed in v5 of Transformers, please use\"\n-            \" `post_process_object_detection` instead, with `threshold=0.` for equivalent results.\",\n-        )\n-\n-        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n-\n-        if len(out_logits) != len(target_sizes):\n-            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n-        if target_sizes.shape[1] != 2:\n-            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n-\n-        prob = nn.functional.softmax(out_logits, -1)\n-        scores, labels = prob[..., :-1].max(-1)\n-\n-        # convert to [x0, y0, x1, y1] format\n-        boxes = center_to_corners_format(out_bbox)\n-        # and from relative [0, 1] to absolute [0, height] coordinates\n-        img_h, img_w = target_sizes.unbind(1)\n-        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n-        boxes = boxes * scale_fct[:, None, :]\n-\n-        results = [{\"scores\": s, \"labels\": l, \"boxes\": b} for s, l, b in zip(scores, labels, boxes)]\n-        return results\n-\n-    def post_process_segmentation(self, outputs, target_sizes, threshold=0.9, mask_threshold=0.5):\n-        \"\"\"\n-        Converts the output of [`DetrForSegmentation`] into image segmentation predictions. Only supports PyTorch.\n-\n-        Args:\n-            outputs ([`DetrSegmentationOutput`]):\n-                Raw outputs of the model.\n-            target_sizes (`torch.Tensor` of shape `(batch_size, 2)` or `list[Tuple]` of length `batch_size`):\n-                Torch Tensor (or list) corresponding to the requested final size (h, w) of each prediction.\n-            threshold (`float`, *optional*, defaults to 0.9):\n-                Threshold to use to filter out queries.\n-            mask_threshold (`float`, *optional*, defaults to 0.5):\n-                Threshold to use when turning the predicted masks into binary values.\n-        Returns:\n-            `list[Dict]`: A list of dictionaries, each dictionary containing the scores, labels, and masks for an image\n-            in the batch as predicted by the model.\n-        \"\"\"\n-        logger.warning_once(\n-            \"`post_process_segmentation` is deprecated and will be removed in v5 of Transformers, please use\"\n-            \" `post_process_semantic_segmentation`.\",\n-        )\n-        out_logits, raw_masks = outputs.logits, outputs.pred_masks\n-        empty_label = out_logits.shape[-1] - 1\n-        preds = []\n-\n-        def to_tuple(tup):\n-            if isinstance(tup, tuple):\n-                return tup\n-            return tuple(tup.tolist())\n-\n-        for cur_logits, cur_masks, size in zip(out_logits, raw_masks, target_sizes):\n-            # we filter empty queries and detection below threshold\n-            cur_scores, cur_labels = cur_logits.softmax(-1).max(-1)\n-            keep = cur_labels.ne(empty_label) & (cur_scores > threshold)\n-            cur_scores = cur_scores[keep]\n-            cur_labels = cur_labels[keep]\n-            cur_masks = cur_masks[keep]\n-            cur_masks = nn.functional.interpolate(cur_masks[:, None], to_tuple(size), mode=\"bilinear\").squeeze(1)\n-            cur_masks = (cur_masks.sigmoid() > mask_threshold) * 1\n-\n-            predictions = {\"scores\": cur_scores, \"labels\": cur_labels, \"masks\": cur_masks}\n-            preds.append(predictions)\n-        return preds\n-\n-    # inspired by https://github.com/facebookresearch/detr/blob/master/models/segmentation.py#L218\n-    def post_process_instance(self, results, outputs, orig_target_sizes, max_target_sizes, threshold=0.5):\n-        \"\"\"\n-        Converts the output of [`DetrForSegmentation`] into actual instance segmentation predictions. Only supports\n-        PyTorch.\n-\n-        Args:\n-            results (`list[Dict]`):\n-                Results list obtained by [`~DetrImageProcessor.post_process`], to which \"masks\" results will be added.\n-            outputs ([`DetrSegmentationOutput`]):\n-                Raw outputs of the model.\n-            orig_target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n-                Tensor containing the size (h, w) of each image of the batch. For evaluation, this must be the original\n-                image size (before any data augmentation).\n-            max_target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n-                Tensor containing the maximum size (h, w) of each image of the batch. For evaluation, this must be the\n-                original image size (before any data augmentation).\n-            threshold (`float`, *optional*, defaults to 0.5):\n-                Threshold to use when turning the predicted masks into binary values.\n-        Returns:\n-            `list[Dict]`: A list of dictionaries, each dictionary containing the scores, labels, boxes and masks for an\n-            image in the batch as predicted by the model.\n-        \"\"\"\n-        logger.warning_once(\n-            \"`post_process_instance` is deprecated and will be removed in v5 of Transformers, please use\"\n-            \" `post_process_instance_segmentation`.\",\n-        )\n-\n-        if len(orig_target_sizes) != len(max_target_sizes):\n-            raise ValueError(\"Make sure to pass in as many orig_target_sizes as max_target_sizes\")\n-        max_h, max_w = max_target_sizes.max(0)[0].tolist()\n-        outputs_masks = outputs.pred_masks.squeeze(2)\n-        outputs_masks = nn.functional.interpolate(\n-            outputs_masks, size=(max_h, max_w), mode=\"bilinear\", align_corners=False\n-        )\n-        outputs_masks = (outputs_masks.sigmoid() > threshold).cpu()\n-\n-        for i, (cur_mask, t, tt) in enumerate(zip(outputs_masks, max_target_sizes, orig_target_sizes)):\n-            img_h, img_w = t[0], t[1]\n-            results[i][\"masks\"] = cur_mask[:, :img_h, :img_w].unsqueeze(1)\n-            results[i][\"masks\"] = nn.functional.interpolate(\n-                results[i][\"masks\"].float(), size=tuple(tt.tolist()), mode=\"nearest\"\n-            ).byte()\n-\n-        return results\n-\n-    # inspired by https://github.com/facebookresearch/detr/blob/master/models/segmentation.py#L241\n-    def post_process_panoptic(self, outputs, processed_sizes, target_sizes=None, is_thing_map=None, threshold=0.85):\n-        \"\"\"\n-        Converts the output of [`DetrForSegmentation`] into actual panoptic predictions. Only supports PyTorch.\n-\n-        Args:\n-            outputs ([`DetrSegmentationOutput`]):\n-                Raw outputs of the model.\n-            processed_sizes (`torch.Tensor` of shape `(batch_size, 2)` or `list[Tuple]` of length `batch_size`):\n-                Torch Tensor (or list) containing the size (h, w) of each image of the batch, i.e. the size after data\n-                augmentation but before batching.\n-            target_sizes (`torch.Tensor` of shape `(batch_size, 2)` or `list[Tuple]` of length `batch_size`, *optional*):\n-                Torch Tensor (or list) corresponding to the requested final size `(height, width)` of each prediction.\n-                If left to None, it will default to the `processed_sizes`.\n-            is_thing_map (`torch.Tensor` of shape `(batch_size, 2)`, *optional*):\n-                Dictionary mapping class indices to either True or False, depending on whether or not they are a thing.\n-                If not set, defaults to the `is_thing_map` of COCO panoptic.\n-            threshold (`float`, *optional*, defaults to 0.85):\n-                Threshold to use to filter out queries.\n-        Returns:\n-            `list[Dict]`: A list of dictionaries, each dictionary containing a PNG string and segments_info values for\n-            an image in the batch as predicted by the model.\n-        \"\"\"\n-        logger.warning_once(\n-            \"`post_process_panoptic is deprecated and will be removed in v5 of Transformers, please use\"\n-            \" `post_process_panoptic_segmentation`.\",\n-        )\n-        if target_sizes is None:\n-            target_sizes = processed_sizes\n-        if len(processed_sizes) != len(target_sizes):\n-            raise ValueError(\"Make sure to pass in as many processed_sizes as target_sizes\")\n-\n-        if is_thing_map is None:\n-            # default to is_thing_map of COCO panoptic\n-            is_thing_map = {i: i <= 90 for i in range(201)}\n-\n-        out_logits, raw_masks, raw_boxes = outputs.logits, outputs.pred_masks, outputs.pred_boxes\n-        if not len(out_logits) == len(raw_masks) == len(target_sizes):\n-            raise ValueError(\n-                \"Make sure that you pass in as many target sizes as the batch dimension of the logits and masks\"\n-            )\n-        empty_label = out_logits.shape[-1] - 1\n-        preds = []\n-\n-        def to_tuple(tup):\n-            if isinstance(tup, tuple):\n-                return tup\n-            return tuple(tup.tolist())\n-\n-        for cur_logits, cur_masks, cur_boxes, size, target_size in zip(\n-            out_logits, raw_masks, raw_boxes, processed_sizes, target_sizes\n-        ):\n-            # we filter empty queries and detection below threshold\n-            cur_scores, cur_labels = cur_logits.softmax(-1).max(-1)\n-            keep = cur_labels.ne(empty_label) & (cur_scores > threshold)\n-            cur_scores = cur_scores[keep]\n-            cur_labels = cur_labels[keep]\n-            cur_masks = cur_masks[keep]\n-            cur_masks = nn.functional.interpolate(cur_masks[:, None], to_tuple(size), mode=\"bilinear\").squeeze(1)\n-            cur_boxes = center_to_corners_format(cur_boxes[keep])\n-\n-            h, w = cur_masks.shape[-2:]\n-            if len(cur_boxes) != len(cur_labels):\n-                raise ValueError(\"Not as many boxes as there are classes\")\n-\n-            # It may be that we have several predicted masks for the same stuff class.\n-            # In the following, we track the list of masks ids for each stuff class (they are merged later on)\n-            cur_masks = cur_masks.flatten(1)\n-            stuff_equiv_classes = defaultdict(list)\n-            for k, label in enumerate(cur_labels):\n-                if not is_thing_map[label.item()]:\n-                    stuff_equiv_classes[label.item()].append(k)\n-\n-            def get_ids_area(masks, scores, dedup=False):\n-                # This helper function creates the final panoptic segmentation image\n-                # It also returns the area of the masks that appears on the image\n-\n-                m_id = masks.transpose(0, 1).softmax(-1)\n-\n-                if m_id.shape[-1] == 0:\n-                    # We didn't detect any mask :(\n-                    m_id = torch.zeros((h, w), dtype=torch.long, device=m_id.device)\n-                else:\n-                    m_id = m_id.argmax(-1).view(h, w)\n-\n-                if dedup:\n-                    # Merge the masks corresponding to the same stuff class\n-                    for equiv in stuff_equiv_classes.values():\n-                        if len(equiv) > 1:\n-                            for eq_id in equiv:\n-                                m_id.masked_fill_(m_id.eq(eq_id), equiv[0])\n-\n-                final_h, final_w = to_tuple(target_size)\n-\n-                seg_img = PIL.Image.fromarray(id_to_rgb(m_id.view(h, w).cpu().numpy()))\n-                seg_img = seg_img.resize(size=(final_w, final_h), resample=PILImageResampling.NEAREST)\n-\n-                np_seg_img = torch.ByteTensor(torch.ByteStorage.from_buffer(seg_img.tobytes()))\n-                np_seg_img = np_seg_img.view(final_h, final_w, 3)\n-                np_seg_img = np_seg_img.numpy()\n-\n-                m_id = torch.from_numpy(rgb_to_id(np_seg_img))\n-\n-                area = []\n-                for i in range(len(scores)):\n-                    area.append(m_id.eq(i).sum().item())\n-                return area, seg_img\n-\n-            area, seg_img = get_ids_area(cur_masks, cur_scores, dedup=True)\n-            if cur_labels.numel() > 0:\n-                # We know filter empty masks as long as we find some\n-                while True:\n-                    filtered_small = torch.as_tensor(\n-                        [area[i] <= 4 for i, c in enumerate(cur_labels)], dtype=torch.bool, device=keep.device\n-                    )\n-                    if filtered_small.any().item():\n-                        cur_scores = cur_scores[~filtered_small]\n-                        cur_labels = cur_labels[~filtered_small]\n-                        cur_masks = cur_masks[~filtered_small]\n-                        area, seg_img = get_ids_area(cur_masks, cur_scores)\n-                    else:\n-                        break\n-\n-            else:\n-                cur_labels = torch.ones(1, dtype=torch.long, device=cur_labels.device)\n-\n-            segments_info = []\n-            for i, a in enumerate(area):\n-                cat = cur_labels[i].item()\n-                segments_info.append({\"id\": i, \"isthing\": is_thing_map[cat], \"category_id\": cat, \"area\": a})\n-            del cur_labels\n-\n-            with io.BytesIO() as out:\n-                seg_img.save(out, format=\"PNG\")\n-                predictions = {\"png_string\": out.getvalue(), \"segments_info\": segments_info}\n-            preds.append(predictions)\n-        return preds\n-\n     # inspired by https://github.com/facebookresearch/detr/blob/master/models/detr.py#L258\n     def post_process_object_detection(\n         self, outputs, threshold: float = 0.5, target_sizes: Optional[Union[TensorType, list[tuple]]] = None"
        },
        {
            "sha": "61a68f74267ac27291f85e3e85dc62323fd9aa05",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 323,
            "changes": 326,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -14,12 +14,9 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for DETR.\"\"\"\n \n-import io\n import pathlib\n-from collections import defaultdict\n from typing import Any, Optional, Union\n \n-import PIL\n import torch\n from torch import nn\n from torchvision.io import read_image\n@@ -33,14 +30,13 @@\n     get_max_height_width,\n     safe_squeeze,\n )\n-from ...image_transforms import center_to_corners_format, corners_to_center_format, id_to_rgb\n+from ...image_transforms import center_to_corners_format, corners_to_center_format\n from ...image_utils import (\n     IMAGENET_DEFAULT_MEAN,\n     IMAGENET_DEFAULT_STD,\n     AnnotationFormat,\n     AnnotationType,\n     ChannelDimension,\n-    ImageInput,\n     PILImageResampling,\n     get_image_size,\n     validate_annotations,\n@@ -280,19 +276,10 @@ class DetrImageProcessorFast(BaseImageProcessorFast):\n     valid_kwargs = DetrImageProcessorKwargs\n \n     def __init__(self, **kwargs: Unpack[DetrImageProcessorKwargs]) -> None:\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n+        kwargs.setdefault(\"do_pad\", kwargs.pop(\"pad_and_return_pixel_mask\", self.do_pad))\n \n         size = kwargs.pop(\"size\", None)\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` parameter is deprecated and will be removed in v4.26. \"\n-                \"Please specify in `size['longest_edge'] instead`.\",\n-            )\n-            max_size = kwargs.pop(\"max_size\")\n-        else:\n-            max_size = None if size is None else 1333\n-\n+        max_size = None if size is None else kwargs.pop(\"max_size\", 1333)\n         size = size if size is not None else {\"shortest_edge\": 800, \"longest_edge\": 1333}\n         self.size = get_size_dict(size, max_size=max_size, default_to_square=False)\n \n@@ -304,20 +291,6 @@ def __init__(self, **kwargs: Unpack[DetrImageProcessorKwargs]) -> None:\n \n         super().__init__(**kwargs)\n \n-    @classmethod\n-    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n-        \"\"\"\n-        Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n-        created using from_dict and kwargs e.g. `DetrImageProcessorFast.from_pretrained(checkpoint, size=600,\n-        max_size=800)`\n-        \"\"\"\n-        image_processor_dict = image_processor_dict.copy()\n-        if \"max_size\" in kwargs:\n-            image_processor_dict[\"max_size\"] = kwargs.pop(\"max_size\")\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            image_processor_dict[\"pad_and_return_pixel_mask\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n-        return super().from_dict(image_processor_dict, **kwargs)\n-\n     def prepare_annotation(\n         self,\n         image: torch.Tensor,\n@@ -537,28 +510,6 @@ def pad(\n \n         return image, pixel_mask, annotation\n \n-    @auto_docstring\n-    def preprocess(\n-        self,\n-        images: ImageInput,\n-        **kwargs: Unpack[DetrImageProcessorKwargs],\n-    ) -> BatchFeature:\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n-            logger.warning_once(\n-                \"The `pad_and_return_pixel_mask` argument is deprecated and will be removed in a future version, \"\n-                \"use `do_pad` instead.\"\n-            )\n-\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` argument is deprecated and will be removed in a future version, use\"\n-                \" `size['longest_edge']` instead.\"\n-            )\n-            kwargs[\"size\"] = kwargs.pop(\"max_size\")\n-\n-        return super().preprocess(images, **kwargs)\n-\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n@@ -675,277 +626,6 @@ def _preprocess(\n             ]\n         return encoded_inputs\n \n-    # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process\n-    def post_process(self, outputs, target_sizes):\n-        \"\"\"\n-        Converts the raw output of [`DetrForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n-        bottom_right_x, bottom_right_y) format.\n-\n-        Args:\n-            outputs ([`DetrObjectDetectionOutput`]):\n-                Raw outputs of the model.\n-            target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n-                Tensor containing the size (height, width) of each image of the batch. For evaluation, this must be the\n-                original image size (before any data augmentation). For visualization, this should be the image size\n-                after data augment, but before padding.\n-        Returns:\n-            `list[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n-            in the batch as predicted by the model.\n-        \"\"\"\n-        logger.warning_once(\n-            \"`post_process` is deprecated and will be removed in v5 of Transformers, please use\"\n-            \" `post_process_object_detection` instead, with `threshold=0.` for equivalent results.\",\n-        )\n-\n-        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n-\n-        if len(out_logits) != len(target_sizes):\n-            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n-        if target_sizes.shape[1] != 2:\n-            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n-\n-        prob = nn.functional.softmax(out_logits, -1)\n-        scores, labels = prob[..., :-1].max(-1)\n-\n-        # convert to [x0, y0, x1, y1] format\n-        boxes = center_to_corners_format(out_bbox)\n-        # and from relative [0, 1] to absolute [0, height] coordinates\n-        img_h, img_w = target_sizes.unbind(1)\n-        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n-        boxes = boxes * scale_fct[:, None, :]\n-\n-        results = [{\"scores\": s, \"labels\": l, \"boxes\": b} for s, l, b in zip(scores, labels, boxes)]\n-        return results\n-\n-    # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_segmentation\n-    def post_process_segmentation(self, outputs, target_sizes, threshold=0.9, mask_threshold=0.5):\n-        \"\"\"\n-        Converts the output of [`DetrForSegmentation`] into image segmentation predictions. Only supports PyTorch.\n-\n-        Args:\n-            outputs ([`DetrSegmentationOutput`]):\n-                Raw outputs of the model.\n-            target_sizes (`torch.Tensor` of shape `(batch_size, 2)` or `list[Tuple]` of length `batch_size`):\n-                Torch Tensor (or list) corresponding to the requested final size (h, w) of each prediction.\n-            threshold (`float`, *optional*, defaults to 0.9):\n-                Threshold to use to filter out queries.\n-            mask_threshold (`float`, *optional*, defaults to 0.5):\n-                Threshold to use when turning the predicted masks into binary values.\n-        Returns:\n-            `list[Dict]`: A list of dictionaries, each dictionary containing the scores, labels, and masks for an image\n-            in the batch as predicted by the model.\n-        \"\"\"\n-        logger.warning_once(\n-            \"`post_process_segmentation` is deprecated and will be removed in v5 of Transformers, please use\"\n-            \" `post_process_semantic_segmentation`.\",\n-        )\n-        out_logits, raw_masks = outputs.logits, outputs.pred_masks\n-        empty_label = out_logits.shape[-1] - 1\n-        preds = []\n-\n-        def to_tuple(tup):\n-            if isinstance(tup, tuple):\n-                return tup\n-            return tuple(tup.tolist())\n-\n-        for cur_logits, cur_masks, size in zip(out_logits, raw_masks, target_sizes):\n-            # we filter empty queries and detection below threshold\n-            cur_scores, cur_labels = cur_logits.softmax(-1).max(-1)\n-            keep = cur_labels.ne(empty_label) & (cur_scores > threshold)\n-            cur_scores = cur_scores[keep]\n-            cur_labels = cur_labels[keep]\n-            cur_masks = cur_masks[keep]\n-            cur_masks = nn.functional.interpolate(cur_masks[:, None], to_tuple(size), mode=\"bilinear\").squeeze(1)\n-            cur_masks = (cur_masks.sigmoid() > mask_threshold) * 1\n-\n-            predictions = {\"scores\": cur_scores, \"labels\": cur_labels, \"masks\": cur_masks}\n-            preds.append(predictions)\n-        return preds\n-\n-    # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_instance\n-    def post_process_instance(self, results, outputs, orig_target_sizes, max_target_sizes, threshold=0.5):\n-        \"\"\"\n-        Converts the output of [`DetrForSegmentation`] into actual instance segmentation predictions. Only supports\n-        PyTorch.\n-\n-        Args:\n-            results (`list[Dict]`):\n-                Results list obtained by [`~DetrImageProcessor.post_process`], to which \"masks\" results will be added.\n-            outputs ([`DetrSegmentationOutput`]):\n-                Raw outputs of the model.\n-            orig_target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n-                Tensor containing the size (h, w) of each image of the batch. For evaluation, this must be the original\n-                image size (before any data augmentation).\n-            max_target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n-                Tensor containing the maximum size (h, w) of each image of the batch. For evaluation, this must be the\n-                original image size (before any data augmentation).\n-            threshold (`float`, *optional*, defaults to 0.5):\n-                Threshold to use when turning the predicted masks into binary values.\n-        Returns:\n-            `list[Dict]`: A list of dictionaries, each dictionary containing the scores, labels, boxes and masks for an\n-            image in the batch as predicted by the model.\n-        \"\"\"\n-        logger.warning_once(\n-            \"`post_process_instance` is deprecated and will be removed in v5 of Transformers, please use\"\n-            \" `post_process_instance_segmentation`.\",\n-        )\n-\n-        if len(orig_target_sizes) != len(max_target_sizes):\n-            raise ValueError(\"Make sure to pass in as many orig_target_sizes as max_target_sizes\")\n-        max_h, max_w = max_target_sizes.max(0)[0].tolist()\n-        outputs_masks = outputs.pred_masks.squeeze(2)\n-        outputs_masks = nn.functional.interpolate(\n-            outputs_masks, size=(max_h, max_w), mode=\"bilinear\", align_corners=False\n-        )\n-        outputs_masks = (outputs_masks.sigmoid() > threshold).cpu()\n-\n-        for i, (cur_mask, t, tt) in enumerate(zip(outputs_masks, max_target_sizes, orig_target_sizes)):\n-            img_h, img_w = t[0], t[1]\n-            results[i][\"masks\"] = cur_mask[:, :img_h, :img_w].unsqueeze(1)\n-            results[i][\"masks\"] = nn.functional.interpolate(\n-                results[i][\"masks\"].float(), size=tuple(tt.tolist()), mode=\"nearest\"\n-            ).byte()\n-\n-        return results\n-\n-    # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_panoptic\n-    def post_process_panoptic(self, outputs, processed_sizes, target_sizes=None, is_thing_map=None, threshold=0.85):\n-        \"\"\"\n-        Converts the output of [`DetrForSegmentation`] into actual panoptic predictions. Only supports PyTorch.\n-\n-        Args:\n-            outputs ([`DetrSegmentationOutput`]):\n-                Raw outputs of the model.\n-            processed_sizes (`torch.Tensor` of shape `(batch_size, 2)` or `list[Tuple]` of length `batch_size`):\n-                Torch Tensor (or list) containing the size (h, w) of each image of the batch, i.e. the size after data\n-                augmentation but before batching.\n-            target_sizes (`torch.Tensor` of shape `(batch_size, 2)` or `list[Tuple]` of length `batch_size`, *optional*):\n-                Torch Tensor (or list) corresponding to the requested final size `(height, width)` of each prediction.\n-                If left to None, it will default to the `processed_sizes`.\n-            is_thing_map (`torch.Tensor` of shape `(batch_size, 2)`, *optional*):\n-                Dictionary mapping class indices to either True or False, depending on whether or not they are a thing.\n-                If not set, defaults to the `is_thing_map` of COCO panoptic.\n-            threshold (`float`, *optional*, defaults to 0.85):\n-                Threshold to use to filter out queries.\n-        Returns:\n-            `list[Dict]`: A list of dictionaries, each dictionary containing a PNG string and segments_info values for\n-            an image in the batch as predicted by the model.\n-        \"\"\"\n-        logger.warning_once(\n-            \"`post_process_panoptic is deprecated and will be removed in v5 of Transformers, please use\"\n-            \" `post_process_panoptic_segmentation`.\",\n-        )\n-        if target_sizes is None:\n-            target_sizes = processed_sizes\n-        if len(processed_sizes) != len(target_sizes):\n-            raise ValueError(\"Make sure to pass in as many processed_sizes as target_sizes\")\n-\n-        if is_thing_map is None:\n-            # default to is_thing_map of COCO panoptic\n-            is_thing_map = {i: i <= 90 for i in range(201)}\n-\n-        out_logits, raw_masks, raw_boxes = outputs.logits, outputs.pred_masks, outputs.pred_boxes\n-        if not len(out_logits) == len(raw_masks) == len(target_sizes):\n-            raise ValueError(\n-                \"Make sure that you pass in as many target sizes as the batch dimension of the logits and masks\"\n-            )\n-        empty_label = out_logits.shape[-1] - 1\n-        preds = []\n-\n-        def to_tuple(tup):\n-            if isinstance(tup, tuple):\n-                return tup\n-            return tuple(tup.tolist())\n-\n-        for cur_logits, cur_masks, cur_boxes, size, target_size in zip(\n-            out_logits, raw_masks, raw_boxes, processed_sizes, target_sizes\n-        ):\n-            # we filter empty queries and detection below threshold\n-            cur_scores, cur_labels = cur_logits.softmax(-1).max(-1)\n-            keep = cur_labels.ne(empty_label) & (cur_scores > threshold)\n-            cur_scores = cur_scores[keep]\n-            cur_labels = cur_labels[keep]\n-            cur_masks = cur_masks[keep]\n-            cur_masks = nn.functional.interpolate(cur_masks[:, None], to_tuple(size), mode=\"bilinear\").squeeze(1)\n-            cur_boxes = center_to_corners_format(cur_boxes[keep])\n-\n-            h, w = cur_masks.shape[-2:]\n-            if len(cur_boxes) != len(cur_labels):\n-                raise ValueError(\"Not as many boxes as there are classes\")\n-\n-            # It may be that we have several predicted masks for the same stuff class.\n-            # In the following, we track the list of masks ids for each stuff class (they are merged later on)\n-            cur_masks = cur_masks.flatten(1)\n-            stuff_equiv_classes = defaultdict(list)\n-            for k, label in enumerate(cur_labels):\n-                if not is_thing_map[label.item()]:\n-                    stuff_equiv_classes[label.item()].append(k)\n-\n-            def get_ids_area(masks, scores, dedup=False):\n-                # This helper function creates the final panoptic segmentation image\n-                # It also returns the area of the masks that appears on the image\n-\n-                m_id = masks.transpose(0, 1).softmax(-1)\n-\n-                if m_id.shape[-1] == 0:\n-                    # We didn't detect any mask :(\n-                    m_id = torch.zeros((h, w), dtype=torch.long, device=m_id.device)\n-                else:\n-                    m_id = m_id.argmax(-1).view(h, w)\n-\n-                if dedup:\n-                    # Merge the masks corresponding to the same stuff class\n-                    for equiv in stuff_equiv_classes.values():\n-                        if len(equiv) > 1:\n-                            for eq_id in equiv:\n-                                m_id.masked_fill_(m_id.eq(eq_id), equiv[0])\n-\n-                final_h, final_w = to_tuple(target_size)\n-\n-                seg_img = PIL.Image.fromarray(id_to_rgb(m_id.view(h, w).cpu().numpy()))\n-                seg_img = seg_img.resize(size=(final_w, final_h), resample=PILImageResampling.NEAREST)\n-\n-                np_seg_img = torch.ByteTensor(torch.ByteStorage.from_buffer(seg_img.tobytes()))\n-                np_seg_img = np_seg_img.view(final_h, final_w, 3)\n-                np_seg_img = np_seg_img.numpy()\n-\n-                m_id = torch.from_numpy(rgb_to_id(np_seg_img))\n-\n-                area = []\n-                for i in range(len(scores)):\n-                    area.append(m_id.eq(i).sum().item())\n-                return area, seg_img\n-\n-            area, seg_img = get_ids_area(cur_masks, cur_scores, dedup=True)\n-            if cur_labels.numel() > 0:\n-                # We know filter empty masks as long as we find some\n-                while True:\n-                    filtered_small = torch.as_tensor(\n-                        [area[i] <= 4 for i, c in enumerate(cur_labels)], dtype=torch.bool, device=keep.device\n-                    )\n-                    if filtered_small.any().item():\n-                        cur_scores = cur_scores[~filtered_small]\n-                        cur_labels = cur_labels[~filtered_small]\n-                        cur_masks = cur_masks[~filtered_small]\n-                        area, seg_img = get_ids_area(cur_masks, cur_scores)\n-                    else:\n-                        break\n-\n-            else:\n-                cur_labels = torch.ones(1, dtype=torch.long, device=cur_labels.device)\n-\n-            segments_info = []\n-            for i, a in enumerate(area):\n-                cat = cur_labels[i].item()\n-                segments_info.append({\"id\": i, \"isthing\": is_thing_map[cat], \"category_id\": cat, \"area\": a})\n-            del cur_labels\n-\n-            with io.BytesIO() as out:\n-                seg_img.save(out, format=\"PNG\")\n-                predictions = {\"png_string\": out.getvalue(), \"segments_info\": segments_info}\n-            preds.append(predictions)\n-        return preds\n-\n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_object_detection\n     def post_process_object_detection(\n         self, outputs, threshold: float = 0.5, target_sizes: Optional[Union[TensorType, list[tuple]]] = None"
        },
        {
            "sha": "b3db644ed518984043505b4a5a4fd1bec5e501cd",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino.py",
            "status": "modified",
            "additions": 3,
            "deletions": 50,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -868,18 +868,7 @@ def __init__(\n         pad_size: Optional[dict[str, int]] = None,\n         **kwargs,\n     ) -> None:\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            do_pad = kwargs.pop(\"pad_and_return_pixel_mask\")\n-\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` parameter is deprecated and will be removed in v4.26. \"\n-                \"Please specify in `size['longest_edge'] instead`.\",\n-            )\n-            max_size = kwargs.pop(\"max_size\")\n-        else:\n-            max_size = None if size is None else 1333\n-\n+        max_size = None if size is None else kwargs.pop(\"max_size\", 1333)\n         size = size if size is not None else {\"shortest_edge\": 800, \"longest_edge\": 1333}\n         size = get_size_dict(size, max_size=max_size, default_to_square=False)\n \n@@ -898,7 +887,7 @@ def __init__(\n         self.do_convert_annotations = do_convert_annotations\n         self.image_mean = image_mean if image_mean is not None else IMAGENET_DEFAULT_MEAN\n         self.image_std = image_std if image_std is not None else IMAGENET_DEFAULT_STD\n-        self.do_pad = do_pad\n+        self.do_pad = kwargs.pop(\"pad_and_return_pixel_mask\", do_pad)\n         self.pad_size = pad_size\n         self._valid_processor_keys = [\n             \"images\",\n@@ -922,21 +911,6 @@ def __init__(\n             \"input_data_format\",\n         ]\n \n-    @classmethod\n-    # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.from_dict with Detr->GroundingDino\n-    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n-        \"\"\"\n-        Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n-        created using from_dict and kwargs e.g. `GroundingDinoImageProcessor.from_pretrained(checkpoint, size=600,\n-        max_size=800)`\n-        \"\"\"\n-        image_processor_dict = image_processor_dict.copy()\n-        if \"max_size\" in kwargs:\n-            image_processor_dict[\"max_size\"] = kwargs.pop(\"max_size\")\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            image_processor_dict[\"pad_and_return_pixel_mask\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n-        return super().from_dict(image_processor_dict, **kwargs)\n-\n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.prepare_annotation with DETR->GroundingDino\n     def prepare_annotation(\n         self,\n@@ -1005,15 +979,7 @@ def resize(\n             input_data_format (`ChannelDimension` or `str`, *optional*):\n                 The channel dimension format of the input image. If not provided, it will be inferred.\n         \"\"\"\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` parameter is deprecated and will be removed in v4.26. \"\n-                \"Please specify in `size['longest_edge'] instead`.\",\n-            )\n-            max_size = kwargs.pop(\"max_size\")\n-        else:\n-            max_size = None\n-        size = get_size_dict(size, max_size=max_size, default_to_square=False)\n+        size = get_size_dict(size, max_size=None, default_to_square=False)\n         if \"shortest_edge\" in size and \"longest_edge\" in size:\n             new_size = get_resize_output_image_size(\n                 image, size[\"shortest_edge\"], size[\"longest_edge\"], input_data_format=input_data_format\n@@ -1350,19 +1316,6 @@ def preprocess(\n                 provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n                 height and width in the batch.\n         \"\"\"\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            logger.warning_once(\n-                \"The `pad_and_return_pixel_mask` argument is deprecated and will be removed in a future version, \"\n-                \"use `do_pad` instead.\"\n-            )\n-            do_pad = kwargs.pop(\"pad_and_return_pixel_mask\")\n-\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` argument is deprecated and will be removed in a future version, use\"\n-                \" `size['longest_edge']` instead.\"\n-            )\n-            size = kwargs.pop(\"max_size\")\n \n         do_resize = self.do_resize if do_resize is None else do_resize\n         size = self.size if size is None else size"
        },
        {
            "sha": "bb911ac27b86702e45608755012121a6a7a0418e",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 52,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -46,23 +46,19 @@\n     AnnotationFormat,\n     AnnotationType,\n     ChannelDimension,\n-    ImageInput,\n     PILImageResampling,\n     get_image_size,\n     validate_annotations,\n )\n from ...processing_utils import Unpack\n-from ...utils import TensorType, auto_docstring, logging\n+from ...utils import TensorType, auto_docstring\n from ...utils.import_utils import requires\n from .image_processing_grounding_dino import GroundingDinoImageProcessorKwargs, get_size_with_aspect_ratio\n \n \n if TYPE_CHECKING:\n     from .modeling_grounding_dino import GroundingDinoObjectDetectionOutput\n \n-\n-logger = logging.get_logger(__name__)\n-\n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n \n \n@@ -308,19 +304,10 @@ class GroundingDinoImageProcessorFast(BaseImageProcessorFast):\n     valid_kwargs = GroundingDinoImageProcessorKwargs\n \n     def __init__(self, **kwargs: Unpack[GroundingDinoImageProcessorKwargs]) -> None:\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n+        kwargs.setdefault(\"do_pad\", kwargs.pop(\"pad_and_return_pixel_mask\", self.do_pad))\n \n         size = kwargs.pop(\"size\", None)\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` parameter is deprecated and will be removed in v4.26. \"\n-                \"Please specify in `size['longest_edge'] instead`.\",\n-            )\n-            max_size = kwargs.pop(\"max_size\")\n-        else:\n-            max_size = None if size is None else 1333\n-\n+        max_size = None if size is None else kwargs.pop(\"max_size\", 1333)\n         size = size if size is not None else {\"shortest_edge\": 800, \"longest_edge\": 1333}\n         self.size = get_size_dict(size, max_size=max_size, default_to_square=False)\n \n@@ -332,20 +319,6 @@ def __init__(self, **kwargs: Unpack[GroundingDinoImageProcessorKwargs]) -> None:\n \n         super().__init__(**kwargs)\n \n-    @classmethod\n-    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n-        \"\"\"\n-        Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n-        created using from_dict and kwargs e.g. `GroundingDinoImageProcessorFast.from_pretrained(checkpoint, size=600,\n-        max_size=800)`\n-        \"\"\"\n-        image_processor_dict = image_processor_dict.copy()\n-        if \"max_size\" in kwargs:\n-            image_processor_dict[\"max_size\"] = kwargs.pop(\"max_size\")\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            image_processor_dict[\"pad_and_return_pixel_mask\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n-        return super().from_dict(image_processor_dict, **kwargs)\n-\n     def prepare_annotation(\n         self,\n         image: torch.Tensor,\n@@ -565,28 +538,6 @@ def pad(\n \n         return image, pixel_mask, annotation\n \n-    @auto_docstring\n-    def preprocess(\n-        self,\n-        images: ImageInput,\n-        **kwargs: Unpack[GroundingDinoImageProcessorKwargs],\n-    ) -> BatchFeature:\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n-            logger.warning_once(\n-                \"The `pad_and_return_pixel_mask` argument is deprecated and will be removed in a future version, \"\n-                \"use `do_pad` instead.\"\n-            )\n-\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` argument is deprecated and will be removed in a future version, use\"\n-                \" `size['longest_edge']` instead.\"\n-            )\n-            kwargs[\"size\"] = kwargs.pop(\"max_size\")\n-\n-        return super().preprocess(images, **kwargs)\n-\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],"
        },
        {
            "sha": "2ea1a4e4393074d92a8c4ddb48ae3e3ae0ee8e9c",
            "filename": "src/transformers/models/grounding_dino/modular_grounding_dino.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodular_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodular_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodular_grounding_dino.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -120,18 +120,6 @@ def post_process_object_detection(\n \n         return results\n \n-    def post_process(self):\n-        raise NotImplementedError(\"Post-processing is not implemented for Grounding-Dino yet.\")\n-\n-    def post_process_segmentation(self):\n-        raise NotImplementedError(\"Segmentation post-processing is not implemented for Grounding-Dino yet.\")\n-\n-    def post_process_instance(self):\n-        raise NotImplementedError(\"Instance post-processing is not implemented for Grounding-Dino yet.\")\n-\n-    def post_process_panoptic(self):\n-        raise NotImplementedError(\"Panoptic post-processing is not implemented for Grounding-Dino yet.\")\n-\n     def post_process_instance_segmentation(self):\n         raise NotImplementedError(\"Segmentation post-processing is not implemented for Grounding-Dino yet.\")\n "
        },
        {
            "sha": "9967c09b2fbe3872d080695ecc7ba414d24c3c21",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -19,7 +19,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import warnings\n from collections.abc import Callable\n from typing import Optional, Union\n \n@@ -1014,18 +1013,6 @@ def tie_weights(self, missing_keys=None):\n         elif target_lang is not None:\n             self.load_adapter(target_lang, force_load=True)\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -1137,18 +1124,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameters will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will"
        },
        {
            "sha": "a906db997877cbaa3052f66b97d1dc7872ae6473",
            "filename": "src/transformers/models/hubert/modular_hubert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -214,9 +214,6 @@ def __init__(self, config: HubertConfig):\n \n         del self.adapter\n \n-    def freeze_feature_extractor(self):\n-        raise AttributeError(\"Not needed for Hubert\")\n-\n     def freeze_feature_encoder(self):\n         raise AttributeError(\"Not needed for Hubert\")\n "
        },
        {
            "sha": "7e128e4a235099c865cd6a0878625b71c0ab6e38",
            "filename": "src/transformers/models/idefics/processing_idefics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -29,7 +29,6 @@\n )\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import is_torch_available\n-from ...utils.deprecation import deprecate_kwarg\n \n \n if is_torch_available():\n@@ -171,7 +170,6 @@ def __init__(self, image_processor, tokenizer=None, image_size=224, add_end_of_u\n             \"<end_of_utterance>\" in self.tokenizer.special_tokens_map.get(\"additional_special_tokens\", [])\n         )\n \n-    @deprecate_kwarg(old_name=\"prompts\", version=\"5.0.0\", new_name=\"text\", raise_if_both_names=True)\n     def __call__(\n         self,\n         images: Union[ImageInput, list[ImageInput], str, list[str], list[list[str]]] = None,"
        },
        {
            "sha": "10d4b4c7eb576800a7ba8fd874c998ec027090f7",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 15,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -645,8 +645,6 @@ def forward(\n         return attn_output, attn_weights\n \n \n-# NO LONGER EXIST Copied from transformers.models.gemma.modeling_gemma.GemmaSdpaAttention with Gemma->KyutaiSpeechToText\n-# TODO cyril: modular\n class KyutaiSpeechToTextSdpaAttention(KyutaiSpeechToTextAttention):\n     \"\"\"\n     KyutaiSpeechToText attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n@@ -667,21 +665,10 @@ def forward(\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n             logger.warning_once(\n-                \"KyutaiSpeechToTextModel is using KyutaiSpeechToTextSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n+                f\"{self.__class__.__name__} does not support `output_attentions=True`. The returned attention weights will \"\n+                \"be `None`. If you want to get attention weights, please set `attn_implementation='eager'` when loading the model.\"\n             )\n-\n         bsz, q_len, _ = hidden_states.size()\n \n         query_states = self.q_proj(hidden_states, cache_position)  # Ignore copy"
        },
        {
            "sha": "88a0f4f82e533b82bd8051d71330c0a051cf238d",
            "filename": "src/transformers/models/llama4/configuration_llama4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -14,7 +14,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import warnings\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n@@ -133,19 +132,6 @@ def __init__(\n         rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n         standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self)\n-\n-        @property\n-        def vision_feature_layer(self):\n-            warnings.warn(\n-                \"The `vision_feature_layer` attribute is deprecated and will be removed in v4.58.0.\",\n-                FutureWarning,\n-            )\n-            return self._vision_feature_layer\n-\n-        @vision_feature_layer.setter\n-        def vision_feature_layer(self, value):\n-            self._vision_feature_layer = value\n-\n         super().__init__(**kwargs)\n \n "
        },
        {
            "sha": "b558135f2ef48c678626bf7f138d4ea69cf1447e",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -39,7 +39,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_llama4 import Llama4Config, Llama4TextConfig\n \n@@ -1259,7 +1258,6 @@ def get_placeholder_mask(\n         return special_image_mask\n \n     @auto_docstring\n-    @deprecate_kwarg(\"vision_feature_layer\", version=\"4.58\")\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1268,7 +1266,6 @@ def forward(\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "9d6a021a61557774f3fecb54dc1983ddd0ef04b4",
            "filename": "src/transformers/models/mask2former/modular_mask2former.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodular_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodular_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodular_mask2former.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -305,8 +305,5 @@ def post_process_panoptic_segmentation(\n             results.append({\"segmentation\": segmentation, \"segments_info\": segments})\n         return results\n \n-    def post_process_segmentation(self):\n-        raise NotImplementedError(\"Segmentation post-processing is not implemented for Mask2Former yet.\")\n-\n \n __all__ = [\"Mask2FormerImageProcessorFast\"]"
        },
        {
            "sha": "586913c36f6894c83b815e99c5ceb52be2265d82",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 54,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -15,9 +15,8 @@\n \"\"\"Image processor class for MaskFormer.\"\"\"\n \n import math\n-import warnings\n from collections.abc import Iterable\n-from typing import TYPE_CHECKING, Any, Optional, Union\n+from typing import Any, Optional, Union\n \n import numpy as np\n \n@@ -58,10 +57,6 @@\n logger = logging.get_logger(__name__)\n \n \n-if TYPE_CHECKING:\n-    from transformers import MaskFormerForInstanceSegmentationOutput\n-\n-\n if is_torch_available():\n     import torch\n     from torch import nn\n@@ -989,54 +984,6 @@ def encode_inputs(\n \n         return encoded_inputs\n \n-    def post_process_segmentation(\n-        self, outputs: \"MaskFormerForInstanceSegmentationOutput\", target_size: Optional[tuple[int, int]] = None\n-    ) -> \"torch.Tensor\":\n-        \"\"\"\n-        Converts the output of [`MaskFormerForInstanceSegmentationOutput`] into image segmentation predictions. Only\n-        supports PyTorch.\n-\n-        Args:\n-            outputs ([`MaskFormerForInstanceSegmentationOutput`]):\n-                The outputs from [`MaskFormerForInstanceSegmentation`].\n-\n-            target_size (`tuple[int, int]`, *optional*):\n-                If set, the `masks_queries_logits` will be resized to `target_size`.\n-\n-        Returns:\n-            `torch.Tensor`:\n-                A tensor of shape (`batch_size, num_class_labels, height, width`).\n-        \"\"\"\n-        warnings.warn(\n-            \"`post_process_segmentation` is deprecated and will be removed in v5 of Transformers, please use\"\n-            \" `post_process_instance_segmentation`\",\n-            FutureWarning,\n-        )\n-\n-        # class_queries_logits has shape [BATCH, QUERIES, CLASSES + 1]\n-        class_queries_logits = outputs.class_queries_logits\n-        # masks_queries_logits has shape [BATCH, QUERIES, HEIGHT, WIDTH]\n-        masks_queries_logits = outputs.masks_queries_logits\n-        if target_size is not None:\n-            masks_queries_logits = torch.nn.functional.interpolate(\n-                masks_queries_logits,\n-                size=target_size,\n-                mode=\"bilinear\",\n-                align_corners=False,\n-            )\n-        # remove the null class `[..., :-1]`\n-        masks_classes = class_queries_logits.softmax(dim=-1)[..., :-1]\n-        # mask probs has shape [BATCH, QUERIES, HEIGHT, WIDTH]\n-        masks_probs = masks_queries_logits.sigmoid()\n-        # now we want to sum over the queries,\n-        # $ out_{c,h,w} =  \\sum_q p_{q,c} * m_{q,h,w} $\n-        # where $ softmax(p) \\in R^{q, c} $ is the mask classes\n-        # and $ sigmoid(m) \\in R^{q, h, w}$ is the mask probabilities\n-        # b(atch)q(uery)c(lasses), b(atch)q(uery)h(eight)w(idth)\n-        segmentation = torch.einsum(\"bqc, bqhw -> bchw\", masks_classes, masks_probs)\n-\n-        return segmentation\n-\n     def post_process_semantic_segmentation(\n         self, outputs, target_sizes: Optional[list[tuple[int, int]]] = None\n     ) -> \"torch.Tensor\":"
        },
        {
            "sha": "2be8ca8f16a91eebf46ee20e396513d8c836bcea",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 51,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"Fast Image processor class for MaskFormer.\"\"\"\n \n import math\n-import warnings\n from typing import TYPE_CHECKING, Any, Optional, Union\n \n import torch\n@@ -58,7 +57,7 @@\n \n \n if TYPE_CHECKING:\n-    from transformers import MaskFormerForInstanceSegmentationOutput\n+    pass\n \n \n def convert_segmentation_map_to_binary_masks_fast(\n@@ -406,55 +405,6 @@ def _preprocess(\n \n         return encoded_inputs\n \n-    # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.post_process_segmentation\n-    def post_process_segmentation(\n-        self, outputs: \"MaskFormerForInstanceSegmentationOutput\", target_size: Optional[tuple[int, int]] = None\n-    ) -> \"torch.Tensor\":\n-        \"\"\"\n-        Converts the output of [`MaskFormerForInstanceSegmentationOutput`] into image segmentation predictions. Only\n-        supports PyTorch.\n-\n-        Args:\n-            outputs ([`MaskFormerForInstanceSegmentationOutput`]):\n-                The outputs from [`MaskFormerForInstanceSegmentation`].\n-\n-            target_size (`tuple[int, int]`, *optional*):\n-                If set, the `masks_queries_logits` will be resized to `target_size`.\n-\n-        Returns:\n-            `torch.Tensor`:\n-                A tensor of shape (`batch_size, num_class_labels, height, width`).\n-        \"\"\"\n-        warnings.warn(\n-            \"`post_process_segmentation` is deprecated and will be removed in v5 of Transformers, please use\"\n-            \" `post_process_instance_segmentation`\",\n-            FutureWarning,\n-        )\n-\n-        # class_queries_logits has shape [BATCH, QUERIES, CLASSES + 1]\n-        class_queries_logits = outputs.class_queries_logits\n-        # masks_queries_logits has shape [BATCH, QUERIES, HEIGHT, WIDTH]\n-        masks_queries_logits = outputs.masks_queries_logits\n-        if target_size is not None:\n-            masks_queries_logits = torch.nn.functional.interpolate(\n-                masks_queries_logits,\n-                size=target_size,\n-                mode=\"bilinear\",\n-                align_corners=False,\n-            )\n-        # remove the null class `[..., :-1]`\n-        masks_classes = class_queries_logits.softmax(dim=-1)[..., :-1]\n-        # mask probs has shape [BATCH, QUERIES, HEIGHT, WIDTH]\n-        masks_probs = masks_queries_logits.sigmoid()\n-        # now we want to sum over the queries,\n-        # $ out_{c,h,w} =  \\sum_q p_{q,c} * m_{q,h,w} $\n-        # where $ softmax(p) \\in R^{q, c} $ is the mask classes\n-        # and $ sigmoid(m) \\in R^{q, h, w}$ is the mask probabilities\n-        # b(atch)q(uery)c(lasses), b(atch)q(uery)h(eight)w(idth)\n-        segmentation = torch.einsum(\"bqc, bqhw -> bchw\", masks_classes, masks_probs)\n-\n-        return segmentation\n-\n     # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.post_process_semantic_segmentation\n     def post_process_semantic_segmentation(\n         self, outputs, target_sizes: Optional[list[tuple[int, int]]] = None"
        },
        {
            "sha": "46d5817bed769e24d26a8a151a1919b8f463ef6f",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -870,21 +870,10 @@ def forward(\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n             logger.warning_once(\n-                \"MimiModel is using MimiSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                f\"{self.__class__.__name__} does not support `output_attentions=True`. The returned attention weights will \"\n+                \"be `None`. If you want to get attention weights, please set `attn_implementation='eager'` when loading the model.\"\n             )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-            )\n-\n         bsz, q_len, _ = hidden_states.size()\n \n         query_states = self.q_proj(hidden_states)"
        },
        {
            "sha": "edfb33cf756b33569e6b1625429e807c94b22720",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 3,
            "deletions": 15,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -643,8 +643,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-# NO LONGER EXIST Copied from transformers.models.gemma.modeling_gemma.GemmaSdpaAttention with Gemma->Moshi\n-# TODO cyril: modular\n+# Copied from transformers.models.mimi.modeling_mimi.MimiSdpaAttention with Mimi->Moshi\n class MoshiSdpaAttention(MoshiAttention):\n     \"\"\"\n     Moshi attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n@@ -665,21 +664,10 @@ def forward(\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n             logger.warning_once(\n-                \"MoshiModel is using MoshiSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                f\"{self.__class__.__name__} does not support `output_attentions=True`. The returned attention weights will \"\n+                \"be `None`. If you want to get attention weights, please set `attn_implementation='eager'` when loading the model.\"\n             )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-            )\n-\n         bsz, q_len, _ = hidden_states.size()\n \n         query_states = self.q_proj(hidden_states, cache_position)  # Ignore copy"
        },
        {
            "sha": "88643bbc01336dc3af65d09be19be5b8a089f302",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 2,
            "deletions": 14,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -454,22 +454,10 @@ def forward(\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n             logger.warning_once(\n-                \"NemotronModel is using NemotronSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                f\"{self.__class__.__name__} does not support `output_attentions=True`. The returned attention weights will \"\n+                \"be `None`. If you want to get attention weights, please set `attn_implementation='eager'` when loading the model.\"\n             )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n-            )\n-\n         bsz, q_len, _ = hidden_states.size()\n \n         query_states = self.q_proj(hidden_states)"
        },
        {
            "sha": "d31173c997c4170d932dac1f597073286968fb86",
            "filename": "src/transformers/models/owlv2/image_processing_owlv2_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 45,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2_fast.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -53,51 +53,6 @@ class Owlv2ImageProcessorFast(BaseImageProcessorFast):\n     rescale_factor = 1 / 255\n     do_pad = True\n \n-    def post_process(self, outputs, target_sizes):\n-        \"\"\"\n-        Converts the raw output of [`Owlv2ForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n-        bottom_right_x, bottom_right_y) format.\n-\n-        Args:\n-            outputs ([`Owlv2ObjectDetectionOutput`]):\n-                Raw outputs of the model.\n-            target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n-                Tensor containing the size (h, w) of each image of the batch. For evaluation, this must be the original\n-                image size (before any data augmentation). For visualization, this should be the image size after data\n-                augment, but before padding.\n-        Returns:\n-            `list[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n-            in the batch as predicted by the model.\n-        \"\"\"\n-        warnings.warn(\n-            \"`post_process` is deprecated and will be removed in v5 of Transformers, please use\"\n-            \" `post_process_object_detection` instead, with `threshold=0.` for equivalent results.\",\n-            FutureWarning,\n-        )\n-\n-        logits, boxes = outputs.logits, outputs.pred_boxes\n-\n-        if len(logits) != len(target_sizes):\n-            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n-        if target_sizes.shape[1] != 2:\n-            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n-\n-        probs = torch.max(logits, dim=-1)\n-        scores = torch.sigmoid(probs.values)\n-        labels = probs.indices\n-\n-        # Convert to [x0, y0, x1, y1] format\n-        boxes = center_to_corners_format(boxes)\n-\n-        # Convert from relative [0, 1] to absolute [0, height] coordinates\n-        img_h, img_w = target_sizes.unbind(1)\n-        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n-        boxes = boxes * scale_fct[:, None, :]\n-\n-        results = [{\"scores\": s, \"labels\": l, \"boxes\": b} for s, l, b in zip(scores, labels, boxes)]\n-\n-        return results\n-\n     def post_process_object_detection(\n         self,\n         outputs: \"Owlv2ObjectDetectionOutput\","
        },
        {
            "sha": "57df27ef5a00fecb51d9d998282fcffb4c77bb5b",
            "filename": "src/transformers/models/owlv2/processing_owlv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -16,7 +16,6 @@\n Image/Text processor class for OWLv2\n \"\"\"\n \n-import warnings\n from typing import TYPE_CHECKING, Optional, Union\n \n import numpy as np\n@@ -170,19 +169,6 @@ def __call__(\n \n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n-    # Copied from transformers.models.owlvit.processing_owlvit.OwlViTProcessor.post_process_object_detection with OwlViT->Owlv2\n-    def post_process_object_detection(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to [`Owlv2ImageProcessor.post_process_object_detection`]. Please refer\n-        to the docstring of this method for more information.\n-        \"\"\"\n-        warnings.warn(\n-            \"`post_process_object_detection` method is deprecated for OwlVitProcessor and will be removed in v5. \"\n-            \"Use `post_process_grounded_object_detection` instead.\",\n-            FutureWarning,\n-        )\n-        return self.image_processor.post_process_object_detection(*args, **kwargs)\n-\n     # Copied from transformers.models.owlvit.processing_owlvit.OwlViTProcessor.post_process_grounded_object_detection with OwlViT->Owlv2\n     def post_process_grounded_object_detection(\n         self,"
        },
        {
            "sha": "d72d1a933b59d027c19a2c5f80b44d01d40bce98",
            "filename": "src/transformers/models/owlvit/image_processing_owlvit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 46,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Image processor class for OwlViT\"\"\"\n \n-import warnings\n from typing import TYPE_CHECKING, Optional, Union\n \n import numpy as np\n@@ -440,51 +439,6 @@ def preprocess(\n         encoded_inputs = BatchFeature(data={\"pixel_values\": images}, tensor_type=return_tensors)\n         return encoded_inputs\n \n-    def post_process(self, outputs, target_sizes):\n-        \"\"\"\n-        Converts the raw output of [`OwlViTForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n-        bottom_right_x, bottom_right_y) format.\n-\n-        Args:\n-            outputs ([`OwlViTObjectDetectionOutput`]):\n-                Raw outputs of the model.\n-            target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n-                Tensor containing the size (h, w) of each image of the batch. For evaluation, this must be the original\n-                image size (before any data augmentation). For visualization, this should be the image size after data\n-                augment, but before padding.\n-        Returns:\n-            `list[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n-            in the batch as predicted by the model.\n-        \"\"\"\n-        warnings.warn(\n-            \"`post_process` is deprecated and will be removed in v5 of Transformers, please use\"\n-            \" `post_process_object_detection` instead, with `threshold=0.` for equivalent results.\",\n-            FutureWarning,\n-        )\n-\n-        logits, boxes = outputs.logits, outputs.pred_boxes\n-\n-        if len(logits) != len(target_sizes):\n-            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n-        if target_sizes.shape[1] != 2:\n-            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n-\n-        probs = torch.max(logits, dim=-1)\n-        scores = torch.sigmoid(probs.values)\n-        labels = probs.indices\n-\n-        # Convert to [x0, y0, x1, y1] format\n-        boxes = center_to_corners_format(boxes)\n-\n-        # Convert from relative [0, 1] to absolute [0, height] coordinates\n-        img_h, img_w = target_sizes.unbind(1)\n-        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n-        boxes = boxes * scale_fct[:, None, :]\n-\n-        results = [{\"scores\": s, \"labels\": l, \"boxes\": b} for s, l, b in zip(scores, labels, boxes)]\n-\n-        return results\n-\n     def post_process_object_detection(\n         self,\n         outputs: \"OwlViTObjectDetectionOutput\","
        },
        {
            "sha": "6e90d2bcb0beb376d2e5a5a76eb55cf1376f0fe1",
            "filename": "src/transformers/models/owlvit/image_processing_owlvit_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 47,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit_fast.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for OwlViT\"\"\"\n \n-import warnings\n from typing import TYPE_CHECKING, Optional, Union\n \n import torch\n@@ -48,52 +47,6 @@ class OwlViTImageProcessorFast(BaseImageProcessorFast):\n     do_convert_rgb = None\n     model_input_names = [\"pixel_values\"]\n \n-    # Copied from transformers.models.owlvit.image_processing_owlvit.OwlViTImageProcessor.post_process\n-    def post_process(self, outputs, target_sizes):\n-        \"\"\"\n-        Converts the raw output of [`OwlViTForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n-        bottom_right_x, bottom_right_y) format.\n-\n-        Args:\n-            outputs ([`OwlViTObjectDetectionOutput`]):\n-                Raw outputs of the model.\n-            target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n-                Tensor containing the size (h, w) of each image of the batch. For evaluation, this must be the original\n-                image size (before any data augmentation). For visualization, this should be the image size after data\n-                augment, but before padding.\n-        Returns:\n-            `list[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n-            in the batch as predicted by the model.\n-        \"\"\"\n-        warnings.warn(\n-            \"`post_process` is deprecated and will be removed in v5 of Transformers, please use\"\n-            \" `post_process_object_detection` instead, with `threshold=0.` for equivalent results.\",\n-            FutureWarning,\n-        )\n-\n-        logits, boxes = outputs.logits, outputs.pred_boxes\n-\n-        if len(logits) != len(target_sizes):\n-            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n-        if target_sizes.shape[1] != 2:\n-            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n-\n-        probs = torch.max(logits, dim=-1)\n-        scores = torch.sigmoid(probs.values)\n-        labels = probs.indices\n-\n-        # Convert to [x0, y0, x1, y1] format\n-        boxes = center_to_corners_format(boxes)\n-\n-        # Convert from relative [0, 1] to absolute [0, height] coordinates\n-        img_h, img_w = target_sizes.unbind(1)\n-        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n-        boxes = boxes * scale_fct[:, None, :]\n-\n-        results = [{\"scores\": s, \"labels\": l, \"boxes\": b} for s, l, b in zip(scores, labels, boxes)]\n-\n-        return results\n-\n     # Copied from transformers.models.owlvit.image_processing_owlvit.OwlViTImageProcessor.post_process_object_detection\n     def post_process_object_detection(\n         self,"
        },
        {
            "sha": "3c6dd617c2144340f56103be137c3730b53c39fa",
            "filename": "src/transformers/models/owlvit/processing_owlvit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -16,7 +16,6 @@\n Image/Text processor class for OWL-ViT\n \"\"\"\n \n-import warnings\n from typing import TYPE_CHECKING, Optional, Union\n \n import numpy as np\n@@ -175,18 +174,6 @@ def post_process(self, *args, **kwargs):\n         \"\"\"\n         return self.image_processor.post_process(*args, **kwargs)\n \n-    def post_process_object_detection(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to [`OwlViTImageProcessor.post_process_object_detection`]. Please refer\n-        to the docstring of this method for more information.\n-        \"\"\"\n-        warnings.warn(\n-            \"`post_process_object_detection` method is deprecated for OwlVitProcessor and will be removed in v5. \"\n-            \"Use `post_process_grounded_object_detection` instead.\",\n-            FutureWarning,\n-        )\n-        return self.image_processor.post_process_object_detection(*args, **kwargs)\n-\n     def post_process_grounded_object_detection(\n         self,\n         outputs: \"OwlViTObjectDetectionOutput\","
        },
        {
            "sha": "0826873a8f98611034ea0e463e603dcbed51c4c9",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -1903,7 +1903,7 @@ def forward(\n         >>> text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n         >>> audios = [ librosa.load(BytesIO(urlopen( conversations[1]['content'][1]['audio_url'] ).read()), sr=self.processor.feature_extractor.sampling_rate) ]\n         >>> images, videos = process_vision_info(conversations)\n-        >>> inputs = processor(text=text, audios=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True)\n+        >>> inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True)\n \n         >>> # Generate\n         >>> inputs['use_audio_in_video'] = `True` or `False`\n@@ -2349,7 +2349,7 @@ def forward(\n         >>> url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"\n         >>> audio, _ = librosa.load(BytesIO(urlopen(url).read()), sr=self.processor.feature_extractor.sampling_rate)\n \n-        >>> inputs = processor(text=prompt, audios=audio, return_tensors=\"pt\")\n+        >>> inputs = processor(text=prompt, audio=audio, return_tensors=\"pt\")\n \n         >>> # Generate\n         >>> generate_ids = model.generate(**inputs, max_length=30)"
        },
        {
            "sha": "5b26bef72601f2908659d7ba3d636d5d8f08ee30",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -2267,7 +2267,7 @@ def forward(\n         >>> text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n         >>> audios = [ librosa.load(BytesIO(urlopen( conversations[1]['content'][1]['audio_url'] ).read()), sr=self.processor.feature_extractor.sampling_rate) ]\n         >>> images, videos = process_vision_info(conversations)\n-        >>> inputs = processor(text=text, audios=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True)\n+        >>> inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True)\n \n         >>> # Generate\n         >>> inputs['use_audio_in_video'] = `True` or `False`\n@@ -2566,7 +2566,7 @@ def forward(\n         >>> url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"\n         >>> audio, _ = librosa.load(BytesIO(urlopen(url).read()), sr=self.processor.feature_extractor.sampling_rate)\n \n-        >>> inputs = processor(text=prompt, audios=audio, return_tensors=\"pt\")\n+        >>> inputs = processor(text=prompt, audio=audio, return_tensors=\"pt\")\n \n         >>> # Generate\n         >>> generate_ids = model.generate(**inputs, max_length=30)"
        },
        {
            "sha": "94114633ef47e485efbba1d06d6f68e9a7b740ca",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -560,7 +560,7 @@ def _merge_input_ids_with_audio_features(\n                     \"[INST] <|AUDIO|>\\nWhat is that in this audio? [/INST]\",\n                     \"[INST] <|AUDIO|>\\nWhat is that in this audio? [/INST]\",\n                 ]\n-                inputs = processor(text=prompts, audios=[audio1, audio2], return_tensors='pt', padding=True).to(\"cuda\")\n+                inputs = processor(text=prompts, audio=[audio1, audio2], return_tensors='pt', padding=True).to(\"cuda\")\n                     audio1 has 101 tokens, while audio2 has 72 tokens\n                 ```\n \n@@ -734,7 +734,7 @@ def forward(\n         >>> url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"\n         >>> audio, _ = librosa.load(BytesIO(urlopen(url).read()), sr=self.processor.feature_extractor.sampling_rate)\n \n-        >>> inputs = processor(text=prompt, audios=audio, return_tensors=\"pt\")\n+        >>> inputs = processor(text=prompt, audio=audio, return_tensors=\"pt\")\n \n         >>> # Generate\n         >>> generate_ids = model.generate(**inputs, max_length=30)"
        },
        {
            "sha": "aa9d41366be03256fd30027e764da6187e70a6c3",
            "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -2087,7 +2087,7 @@ def forward(\n         >>> text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n         >>> audios = [ librosa.load(BytesIO(urlopen( conversations[1]['content'][1]['audio_url'] ).read()), sr=self.processor.feature_extractor.sampling_rate) ]\n         >>> images, videos = process_vision_info(conversations)\n-        >>> inputs = processor(text=text, audios=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True)\n+        >>> inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True)\n \n         >>> # Generate\n         >>> inputs['use_audio_in_video'] = `True` or `False`"
        },
        {
            "sha": "966051c535d78a1ce6eab5f0826a1c1fa5bbc8e1",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -486,15 +486,7 @@ def resize(\n             input_data_format (`ChannelDimension` or `str`, *optional*):\n                 The channel dimension format of the input image. If not provided, it will be inferred.\n         \"\"\"\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` parameter is deprecated and will be removed in v4.26. \"\n-                \"Please specify in `size['longest_edge'] instead`.\",\n-            )\n-            max_size = kwargs.pop(\"max_size\")\n-        else:\n-            max_size = None\n-        size = get_size_dict(size, max_size=max_size, default_to_square=False)\n+        size = get_size_dict(size, max_size=None, default_to_square=False)\n         if \"shortest_edge\" in size and \"longest_edge\" in size:\n             new_size = get_resize_output_image_size(\n                 image, size[\"shortest_edge\"], size[\"longest_edge\"], input_data_format=input_data_format"
        },
        {
            "sha": "6668449112492d4a924f0d0ab31b865428434e8a",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -25,7 +25,6 @@\n     AnnotationFormat,\n     AnnotationType,\n     ChannelDimension,\n-    ImageInput,\n     PILImageResampling,\n     get_image_size,\n     validate_annotations,\n@@ -334,14 +333,6 @@ def pad(\n \n         return image, pixel_mask, annotation\n \n-    @auto_docstring\n-    def preprocess(\n-        self,\n-        images: ImageInput,\n-        **kwargs: Unpack[RTDetrImageProcessorKwargs],\n-    ) -> BatchFeature:\n-        return super().preprocess(images, **kwargs)\n-\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],"
        },
        {
            "sha": "3083bf9754745a5396f36c24e6bd7168e10e1504",
            "filename": "src/transformers/models/rt_detr/modular_rt_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -15,7 +15,6 @@\n     AnnotationFormat,\n     AnnotationType,\n     ChannelDimension,\n-    ImageInput,\n     PILImageResampling,\n     get_image_size,\n     validate_annotations,\n@@ -117,13 +116,6 @@ def __init__(self, **kwargs: Unpack[RTDetrImageProcessorKwargs]) -> None:\n \n         BaseImageProcessorFast.__init__(self, **kwargs)\n \n-    def preprocess(\n-        self,\n-        images: ImageInput,\n-        **kwargs: Unpack[RTDetrImageProcessorKwargs],\n-    ) -> BatchFeature:\n-        return BaseImageProcessorFast.preprocess(self, images, **kwargs)\n-\n     def prepare_annotation(\n         self,\n         image: torch.Tensor,\n@@ -322,21 +314,6 @@ def post_process_object_detection(\n \n         return results\n \n-    def from_dict(self):\n-        raise NotImplementedError(\"No need to override this method for RT-DETR yet.\")\n-\n-    def post_process(self):\n-        raise NotImplementedError(\"Post-processing is not implemented for RT-DETR yet.\")\n-\n-    def post_process_segmentation(self):\n-        raise NotImplementedError(\"Segmentation post-processing is not implemented for RT-DETR yet.\")\n-\n-    def post_process_instance(self):\n-        raise NotImplementedError(\"Instance post-processing is not implemented for RT-DETR yet.\")\n-\n-    def post_process_panoptic(self):\n-        raise NotImplementedError(\"Panoptic post-processing is not implemented for RT-DETR yet.\")\n-\n     def post_process_instance_segmentation(self):\n         raise NotImplementedError(\"Segmentation post-processing is not implemented for RT-DETR yet.\")\n "
        },
        {
            "sha": "138a59716b2f353f0bd7520830eb087444cf3f72",
            "filename": "src/transformers/models/sam/modeling_sam.py",
            "status": "modified",
            "additions": 3,
            "deletions": 14,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -31,11 +31,7 @@\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    ModelOutput,\n-    auto_docstring,\n-    logging,\n-)\n+from ...utils import ModelOutput, auto_docstring, logging\n from .configuration_sam import SamConfig, SamMaskDecoderConfig, SamPromptEncoderConfig, SamVisionConfig\n \n \n@@ -848,16 +844,9 @@ def __init__(self, config, window_size):\n     def forward(self, hidden_states: torch.Tensor, output_attentions=False) -> torch.Tensor:\n         if output_attentions:\n             logger.warning_once(\n-                \"`SamVisionSdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n-                \"`output_attentions=True`. Falling back to the manual attention implementation, but \"\n-                \"specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n-                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                output_attentions=output_attentions,\n+                f\"{self.__class__.__name__} does not support `output_attentions=True`. The returned attention weights will \"\n+                \"be `None`. If you want to get attention weights, please set `attn_implementation='eager'` when loading the model.\"\n             )\n-\n         batch_size, height, width, _ = hidden_states.shape\n         # qkv with shape (3, B, nHead, H * W, C)\n         qkv = ("
        },
        {
            "sha": "7bca21b31892bd4ce4b131104535d761cba08414",
            "filename": "src/transformers/models/sam_hq/modeling_sam_hq.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -283,16 +283,9 @@ def __init__(self, config, window_size):\n     def forward(self, hidden_states: torch.Tensor, output_attentions=False) -> torch.Tensor:\n         if output_attentions:\n             logger.warning_once(\n-                \"`SamHQVisionSdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n-                \"`output_attentions=True`. Falling back to the manual attention implementation, but \"\n-                \"specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n-                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                f\"{self.__class__.__name__} does not support `output_attentions=True`. The returned attention weights will \"\n+                \"be `None`. If you want to get attention weights, please set `attn_implementation='eager'` when loading the model.\"\n             )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                output_attentions=output_attentions,\n-            )\n-\n         batch_size, height, width, _ = hidden_states.shape\n         # qkv with shape (3, B, nHead, H * W, C)\n         qkv = ("
        },
        {
            "sha": "e210da1c9a8ec765aa58e6fca5599cc7a34325c7",
            "filename": "src/transformers/models/seamless_m4t/processing_seamless_m4t.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fprocessing_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fprocessing_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fprocessing_seamless_m4t.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -22,7 +22,6 @@\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import logging\n-from ...utils.deprecation import deprecate_kwarg\n \n \n logger = logging.get_logger(__name__)\n@@ -59,11 +58,9 @@ class SeamlessM4TProcessor(ProcessorMixin):\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n \n-    @deprecate_kwarg(\"audios\", version=\"v4.59.0\", new_name=\"audio\")\n     def __call__(\n         self,\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n-        audios: Optional[AudioInput] = None,\n         audio: Optional[AudioInput] = None,\n         **kwargs: Unpack[ProcessingKwargs],\n     ):\n@@ -92,15 +89,10 @@ def __call__(\n               `None`).\n             - **input_features** -- Audio input features to be fed to a model. Returned when `audios` is not `None`.\n         \"\"\"\n-        if text is not None and audios is not None:\n+        if text is not None and audio is not None:\n             raise ValueError(\n                 \"Text and audios are mututally exclusive when passed to `SeamlessM4T`. Specify one or another.\"\n             )\n-        if audio is None and audios is not None:\n-            logger.warning(\n-                \"Passing `audios` as keyword argument is deprecated and will be removed in v4.63, please pass `audio` instead.\"\n-            )\n-            audio = audios\n         return super().__call__(text=text, audio=audio, **kwargs)\n \n "
        },
        {
            "sha": "12870e4817db0c346788c400bc8c1e7527ed7a29",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -20,7 +20,6 @@\n # limitations under the License.\n \n import math\n-import warnings\n from collections.abc import Callable\n from typing import Optional, Union\n \n@@ -878,18 +877,6 @@ def tie_weights(self, missing_keys=None):\n         elif target_lang is not None:\n             self.load_adapter(target_lang, force_load=True)\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -1001,18 +988,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameters will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will"
        },
        {
            "sha": "a86f800027c5f91d47023a68939da766e7db0d73",
            "filename": "src/transformers/models/sew_d/modeling_sew_d.py",
            "status": "modified",
            "additions": 0,
            "deletions": 36,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"PyTorch SEW model.\"\"\"\n \n import math\n-import warnings\n from collections.abc import Sequence\n from typing import Optional, Union\n \n@@ -434,17 +433,6 @@ def forward(self, input_values):\n         return hidden_states\n \n \n-class SEWDFeatureExtractor(SEWDFeatureEncoder):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        warnings.warn(\n-            f\"The class `{self.__class__.__name__}` has been depreciated \"\n-            \"and will be removed in Transformers v5. \"\n-            f\"Use `{self.__class__.__bases__[0].__name__}` instead.\",\n-            FutureWarning,\n-        )\n-\n-\n class ContextPooler(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -1431,18 +1419,6 @@ def tie_weights(self, missing_keys=None):\n         elif target_lang is not None:\n             self.load_adapter(target_lang, force_load=True)\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -1555,18 +1531,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameters will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will"
        },
        {
            "sha": "8a849499107f0484bebad876082b69ead2ffa8d2",
            "filename": "src/transformers/models/swin2sr/image_processing_swin2sr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -32,7 +32,6 @@\n )\n from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, logging\n-from ...utils.deprecation import deprecate_kwarg\n \n \n logger = logging.get_logger(__name__)\n@@ -74,20 +73,6 @@ def __init__(\n         pad_size = kwargs.get(\"pad_size\")\n         self.size_divisor = size_divisor if size_divisor is not None else pad_size\n \n-    @property\n-    def pad_size(self):\n-        logger.warning(\n-            \"`self.pad_size` attribute is deprecated and will be removed in v5. Use `self.size_divisor` instead\",\n-        )\n-        return self.size_divisor\n-\n-    @pad_size.setter\n-    def pad_size(self, value):\n-        logger.warning(\n-            \"`self.pad_size` attribute is deprecated and will be removed in v5. Use `self.size_divisor` instead\",\n-        )\n-        self.size_divisor = value\n-\n     def pad(\n         self,\n         image: np.ndarray,\n@@ -130,7 +115,6 @@ def pad(\n         )\n \n     @filter_out_non_signature_kwargs()\n-    @deprecate_kwarg(\"pad_size\", version=\"v5\", new_name=\"size_divisor\")\n     def preprocess(\n         self,\n         images: ImageInput,"
        },
        {
            "sha": "f85c124041bb2c51a062f9b61500cf640d29c271",
            "filename": "src/transformers/models/swin2sr/image_processing_swin2sr_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr_fast.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -32,7 +32,6 @@\n     auto_docstring,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .image_processing_swin2sr import Swin2SRImageProcessorKwargs\n \n \n@@ -52,24 +51,9 @@ def __init__(self, **kwargs: Unpack[Swin2SRImageProcessorKwargs]):\n         kwargs.setdefault(\"size_divisor\", pad_size)\n         super().__init__(**kwargs)\n \n-    @property\n-    def pad_size(self):\n-        logger.warning(\n-            \"`self.pad_size` attribute is deprecated and will be removed in v5. Use `self.size_divisor` instead\",\n-        )\n-        return self.size_divisor\n-\n-    @pad_size.setter\n-    def pad_size(self, value):\n-        logger.warning(\n-            \"`self.pad_size` attribute is deprecated and will be removed in v5. Use `self.size_divisor` instead\",\n-        )\n-        self.size_divisor = value\n-\n     def preprocess(self, images: ImageInput, **kwargs: Unpack[Swin2SRImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n-    @deprecate_kwarg(\"size\", version=\"v5\", new_name=\"size_divisor\")\n     def pad(self, images: \"torch.Tensor\", size_divisor: int) -> \"torch.Tensor\":\n         \"\"\"\n         Pad an image to make the height and width divisible by `size_divisor`.\n@@ -93,7 +77,6 @@ def pad(self, images: \"torch.Tensor\", size_divisor: int) -> \"torch.Tensor\":\n             padding_mode=\"symmetric\",\n         )\n \n-    @deprecate_kwarg(\"pad_size\", version=\"v5\", new_name=\"size_divisor\")\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],"
        },
        {
            "sha": "b6f2590e2be5a3c84faadf1387963ad5d8c5f099",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -20,7 +20,6 @@\n # limitations under the License.\n \n import math\n-import warnings\n from collections.abc import Callable\n from dataclasses import dataclass\n from typing import Optional, Union\n@@ -1073,18 +1072,6 @@ def set_gumbel_temperature(self, temperature: int):\n         \"\"\"\n         self.quantizer.temperature = temperature\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameters will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -1243,18 +1230,6 @@ def tie_weights(self, missing_keys=None):\n         elif target_lang is not None:\n             self.load_adapter(target_lang, force_load=True)\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -1366,18 +1341,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameters will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will"
        },
        {
            "sha": "84b962e4b1d1cc63cede29b82ca7477a1208d90d",
            "filename": "src/transformers/models/unispeech/modular_unispeech.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"PyTorch UniSpeech model.\"\"\"\n \n import math\n-import warnings\n from dataclasses import dataclass\n from typing import Optional, Union\n \n@@ -233,9 +232,6 @@ def __init__(self, config: UniSpeechConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def freeze_feature_extractor(self):\n-        raise AttributeError(\"Not needed for UniSpeech\")\n-\n     def freeze_feature_encoder(self):\n         raise AttributeError(\"Not needed for UniSpeech\")\n \n@@ -319,18 +315,6 @@ def set_gumbel_temperature(self, temperature: int):\n         \"\"\"\n         self.quantizer.temperature = temperature\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameters will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will"
        },
        {
            "sha": "e1906639f2dcf802aa66fcfe0c5dea053f6bf16b",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 0,
            "deletions": 60,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -1084,18 +1084,6 @@ def set_gumbel_temperature(self, temperature: int):\n         \"\"\"\n         self.quantizer.temperature = temperature\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameters will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -1238,18 +1226,6 @@ def tie_weights(self, missing_keys=None):\n         elif target_lang is not None:\n             self.load_adapter(target_lang, force_load=True)\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -1361,18 +1337,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameters will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -1476,18 +1440,6 @@ def __init__(self, config):\n \n         self.init_weights()\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -1644,18 +1596,6 @@ def __init__(self, config):\n \n         self.init_weights()\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will"
        },
        {
            "sha": "abcdb0364810ce25b9afbf965956f7e03899d8a2",
            "filename": "src/transformers/models/unispeech_sat/modular_unispeech_sat.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"PyTorch UniSpeechSat model.\"\"\"\n \n import math\n-import warnings\n from dataclasses import dataclass\n from typing import Optional, Union\n \n@@ -244,9 +243,6 @@ def __init__(self, config: UniSpeechSatConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def freeze_feature_extractor(self):\n-        raise AttributeError(\"Not needed for UniSpeechSat\")\n-\n     def freeze_feature_encoder(self):\n         raise AttributeError(\"Not needed for UniSpeechSat\")\n \n@@ -337,18 +333,6 @@ def set_gumbel_temperature(self, temperature: int):\n         \"\"\"\n         self.quantizer.temperature = temperature\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameters will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will"
        },
        {
            "sha": "c74885375399f4a754e1502c03aa7ac4fb5d26d0",
            "filename": "src/transformers/models/vilt/image_processing_vilt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 16,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -183,9 +183,6 @@ def __init__(\n         do_pad: bool = True,\n         **kwargs,\n     ) -> None:\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            do_pad = kwargs.pop(\"pad_and_return_pixel_mask\")\n-\n         super().__init__(**kwargs)\n         size = size if size is not None else {\"shortest_edge\": 384}\n         size = get_size_dict(size, default_to_square=False)\n@@ -199,19 +196,7 @@ def __init__(\n         self.do_normalize = do_normalize\n         self.image_mean = image_mean if image_mean is not None else IMAGENET_STANDARD_MEAN\n         self.image_std = image_std if image_std is not None else IMAGENET_STANDARD_STD\n-        self.do_pad = do_pad\n-\n-    @classmethod\n-    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n-        \"\"\"\n-        Overrides the `from_dict` method from the base class to make sure `pad_and_return_pixel_mask` is updated if image processor\n-        is created using from_dict and kwargs e.g. `ViltImageProcessor.from_pretrained(checkpoint,\n-        pad_and_return_pixel_mask=False)`\n-        \"\"\"\n-        image_processor_dict = image_processor_dict.copy()\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            image_processor_dict[\"pad_and_return_pixel_mask\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n-        return super().from_dict(image_processor_dict, **kwargs)\n+        self.do_pad = kwargs.pop(\"pad_and_return_pixel_mask\", do_pad)\n \n     def resize(\n         self,"
        },
        {
            "sha": "7a6b847b34b649e20345f4e87e07cc53381e77ac",
            "filename": "src/transformers/models/vitmatte/image_processing_vitmatte.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -35,7 +35,6 @@\n )\n from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, logging\n-from ...utils.deprecation import deprecate_kwarg\n \n \n logger = logging.get_logger(__name__)\n@@ -96,20 +95,6 @@ def __init__(\n         size_divisibility = kwargs.get(\"size_divisibility\")\n         self.size_divisor = size_divisibility if size_divisibility is not None else size_divisor\n \n-    @property\n-    def size_divisibility(self):\n-        logger.warning(\n-            \"`self.size_divisibility` attribute is deprecated and will be removed in v5. Use `self.size_divisor` instead\"\n-        )\n-        return self.size_divisor\n-\n-    @size_divisibility.setter\n-    def size_divisibility(self, value):\n-        logger.warning(\n-            \"`self.size_divisibility` attribute is deprecated and will be removed in v5. Use `self.size_divisor` instead\"\n-        )\n-        self.size_divisor = value\n-\n     def pad_image(\n         self,\n         image: np.ndarray,\n@@ -152,7 +137,6 @@ def pad_image(\n         return image\n \n     @filter_out_non_signature_kwargs()\n-    @deprecate_kwarg(\"size_divisibility\", version=\"v5\", new_name=\"size_divisor\")\n     def preprocess(\n         self,\n         images: ImageInput,"
        },
        {
            "sha": "54f54d18cc89b907e7437d7bd8791085877bb145",
            "filename": "src/transformers/models/vitmatte/image_processing_vitmatte_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -61,20 +61,6 @@ def __init__(self, **kwargs: Unpack[VitMatteImageProcessorKwargs]) -> None:\n         kwargs.setdefault(\"size_divisor\", size_divisibility)\n         super().__init__(**kwargs)\n \n-    @property\n-    def size_divisibility(self):\n-        logger.warning(\n-            \"`self.size_divisibility` attribute is deprecated and will be removed in v5. Use `self.size_divisor` instead\"\n-        )\n-        return self.size_divisor\n-\n-    @size_divisibility.setter\n-    def size_divisibility(self, value):\n-        logger.warning(\n-            \"`self.size_divisibility` attribute is deprecated and will be removed in v5. Use `self.size_divisor` instead\"\n-        )\n-        self.size_divisor = value\n-\n     def _pad_image(\n         self,\n         images: torch.Tensor,"
        },
        {
            "sha": "af33fdad10dea24262f1c6999ee40e7093c713f1",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 84,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -423,17 +423,6 @@ def forward(self, input_values):\n         return hidden_states\n \n \n-class Wav2Vec2FeatureExtractor(Wav2Vec2FeatureEncoder):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        warnings.warn(\n-            f\"The class `{self.__class__.__name__}` has been depreciated \"\n-            \"and will be removed in Transformers v5. \"\n-            f\"Use `{self.__class__.__bases__[0].__name__}` instead.\",\n-            FutureWarning,\n-        )\n-\n-\n class Wav2Vec2FeatureProjection(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -1166,7 +1155,6 @@ def load_adapter(self, target_lang: str, force_load=True, **kwargs):\n         token = kwargs.pop(\"token\", None)\n         revision = kwargs.pop(\"revision\", None)\n         use_safetensors = kwargs.pop(\"use_safetensors\", None)\n-\n         model_path_or_id = self.config._name_or_path\n         state_dict = None\n \n@@ -1291,18 +1279,6 @@ def __init__(self, config: Wav2Vec2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameters will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -1440,18 +1416,6 @@ def set_gumbel_temperature(self, temperature: int):\n         \"\"\"\n         self.quantizer.temperature = temperature\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameters will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -1742,18 +1706,6 @@ def tie_weights(self, missing_keys=None):\n         elif target_lang is not None:\n             self.load_adapter(target_lang, force_load=True)\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -1865,18 +1817,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameters will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -1980,18 +1920,6 @@ def __init__(self, config):\n \n         self.init_weights()\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -2148,18 +2076,6 @@ def __init__(self, config):\n \n         self.init_weights()\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will"
        },
        {
            "sha": "6b865f708a5f82c606c324809216bf38bdc5be4f",
            "filename": "src/transformers/models/wav2vec2/processing_wav2vec2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -21,8 +21,6 @@\n \n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import AudioInput, PreTokenizedInput, TextInput\n-from .feature_extraction_wav2vec2 import Wav2Vec2FeatureExtractor\n-from .tokenization_wav2vec2 import Wav2Vec2CTCTokenizer\n \n \n class Wav2Vec2ProcessorKwargs(ProcessingKwargs, total=False):\n@@ -47,25 +45,6 @@ class Wav2Vec2Processor(ProcessorMixin):\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n-        try:\n-            return super().from_pretrained(pretrained_model_name_or_path, **kwargs)\n-        except (OSError, ValueError):\n-            warnings.warn(\n-                f\"Loading a tokenizer inside {cls.__name__} from a config that does not\"\n-                \" include a `tokenizer_class` attribute is deprecated and will be \"\n-                \"removed in v5. Please add `'tokenizer_class': 'Wav2Vec2CTCTokenizer'`\"\n-                \" attribute to either your `config.json` or `tokenizer_config.json` \"\n-                \"file to suppress this warning: \",\n-                FutureWarning,\n-            )\n-\n-            feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(pretrained_model_name_or_path, **kwargs)\n-            tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(pretrained_model_name_or_path, **kwargs)\n-\n-            return cls(feature_extractor=feature_extractor, tokenizer=tokenizer)\n-\n     def __call__(\n         self,\n         audio: Optional[AudioInput] = None,"
        },
        {
            "sha": "4cd67b9db2521f410940a843bdba7c9b589bfa41",
            "filename": "src/transformers/models/wav2vec2_bert/modular_wav2vec2_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -690,9 +690,6 @@ def __init__(self, config: Wav2Vec2BertConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def freeze_feature_extractor(self):\n-        raise AttributeError(\"Not needed for Wav2Vec2Bert\")\n-\n     def freeze_feature_encoder(self):\n         raise AttributeError(\"Not needed for Wav2Vec2Bert\")\n \n@@ -839,9 +836,6 @@ class Wav2Vec2BertForSequenceClassification(Wav2Vec2ForSequenceClassification):\n     def __init__(self, config):\n         super().__init__(config)\n \n-    def freeze_feature_extractor(self):\n-        raise AttributeError(\"Not needed for Wav2Vec2Bert\")\n-\n     def freeze_feature_encoder(self):\n         raise AttributeError(\"Not needed for Wav2Vec2Bert\")\n "
        },
        {
            "sha": "09f27f39ff865d8ee4910b7be7b2a4d69e767caa",
            "filename": "src/transformers/models/wav2vec2_bert/processing_wav2vec2_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -16,13 +16,10 @@\n Speech processor class for Wav2Vec2-BERT\n \"\"\"\n \n-import warnings\n from typing import Optional, Union\n \n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import AudioInput, PreTokenizedInput, TextInput\n-from ..seamless_m4t.feature_extraction_seamless_m4t import SeamlessM4TFeatureExtractor\n-from ..wav2vec2.tokenization_wav2vec2 import Wav2Vec2CTCTokenizer\n \n \n class Wav2Vec2BertProcessorKwargs(ProcessingKwargs, total=False):\n@@ -47,25 +44,6 @@ class Wav2Vec2BertProcessor(ProcessorMixin):\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n-        try:\n-            return super().from_pretrained(pretrained_model_name_or_path, **kwargs)\n-        except OSError:\n-            warnings.warn(\n-                f\"Loading a tokenizer inside {cls.__name__} from a config that does not\"\n-                \" include a `tokenizer_class` attribute is deprecated and will be \"\n-                \"removed in v5. Please add `'tokenizer_class': 'Wav2Vec2CTCTokenizer'`\"\n-                \" attribute to either your `config.json` or `tokenizer_config.json` \"\n-                \"file to suppress this warning: \",\n-                FutureWarning,\n-            )\n-\n-            feature_extractor = SeamlessM4TFeatureExtractor.from_pretrained(pretrained_model_name_or_path, **kwargs)\n-            tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(pretrained_model_name_or_path, **kwargs)\n-\n-            return cls(feature_extractor=feature_extractor, tokenizer=tokenizer)\n-\n     def __call__(\n         self,\n         audio: Optional[AudioInput] = None,"
        },
        {
            "sha": "7b80807984711532a3631bfb410fe90c8efc4aab",
            "filename": "src/transformers/models/wav2vec2_conformer/modular_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -661,17 +661,11 @@ def __init__(self, config: Wav2Vec2ConformerConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def freeze_feature_extractor(self):\n-        raise AttributeError(\"Not needed for Wav2Vec2Conformer\")\n-\n \n class Wav2Vec2ConformerForPreTraining(Wav2Vec2ForPreTraining):\n     def __init__(self, config: Wav2Vec2ConformerConfig):\n         super().__init__(config)\n \n-    def freeze_feature_extractor(self):\n-        raise AttributeError(\"Not needed for Wav2Vec2Conformer\")\n-\n \n class Wav2Vec2ConformerForCTC(Wav2Vec2ForCTC):\n     def __init__(self, config, target_lang: Optional[str] = None):\n@@ -686,9 +680,6 @@ def __init__(self, config, target_lang: Optional[str] = None):\n     def tie_weights(self):\n         raise AttributeError(\"Not needed for Wav2Vec2Conformer\")\n \n-    def freeze_feature_extractor(self):\n-        raise AttributeError(\"Not needed for Wav2Vec2Conformer\")\n-\n     def freeze_base_model(self):\n         raise AttributeError(\"Not needed for Wav2Vec2Conformer\")\n \n@@ -697,25 +688,16 @@ class Wav2Vec2ConformerForSequenceClassification(Wav2Vec2ForSequenceClassificati\n     def __init__(self, config):\n         super().__init__(config)\n \n-    def freeze_feature_extractor(self):\n-        raise AttributeError(\"Not needed for Wav2Vec2Conformer\")\n-\n \n class Wav2Vec2ConformerForAudioFrameClassification(Wav2Vec2ForAudioFrameClassification):\n     def __init__(self, config):\n         super().__init__(config)\n \n-    def freeze_feature_extractor(self):\n-        raise AttributeError(\"Not needed for Wav2Vec2Conformer\")\n-\n \n class Wav2Vec2ConformerForXVector(Wav2Vec2ForXVector):\n     def __init__(self, config):\n         super().__init__(config)\n \n-    def freeze_feature_extractor(self):\n-        raise AttributeError(\"Not needed for Wav2Vec2Conformer\")\n-\n \n __all__ = [\n     \"Wav2Vec2ConformerForAudioFrameClassification\","
        },
        {
            "sha": "958337d3bc3d5372c076b93dd96dbde78c62b50a",
            "filename": "src/transformers/models/wavlm/modeling_wavlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 60,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -984,18 +984,6 @@ def __init__(self, config: WavLMConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameters will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -1167,18 +1155,6 @@ def tie_weights(self, missing_keys=None):\n         elif target_lang is not None:\n             self.load_adapter(target_lang, force_load=True)\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -1290,18 +1266,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameters will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -1405,18 +1369,6 @@ def __init__(self, config):\n \n         self.init_weights()\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -1573,18 +1525,6 @@ def __init__(self, config):\n \n         self.init_weights()\n \n-    def freeze_feature_extractor(self):\n-        \"\"\"\n-        Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n-        not be updated during training.\n-        \"\"\"\n-        warnings.warn(\n-            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n-            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n-            FutureWarning,\n-        )\n-        self.freeze_feature_encoder()\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will"
        },
        {
            "sha": "d2a9914aa822dcdfa4f5a8db2265e624e5d126f3",
            "filename": "src/transformers/models/whisper/tokenization_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -16,7 +16,6 @@\n \n import json\n import os\n-import warnings\n from functools import lru_cache\n from typing import Optional, Union\n \n@@ -499,20 +498,6 @@ def _convert_id_to_token(self, index):\n         \"\"\"\n         return self.decoder.get(index, \"\")\n \n-    def _normalize(self, text):\n-        warnings.warn(\n-            \"The private method `_normalize` is deprecated and will be removed in v5 of Transformers.\"\n-            \"You can normalize an input string using the Whisper English normalizer using the `normalize` method.\"\n-        )\n-        return self.normalize(text)\n-\n-    def _basic_normalize(self, text, remove_diacritics=False):\n-        warnings.warn(\n-            \"The private method `_basic_normalize` is deprecated and will be removed in v5 of Transformers.\"\n-            \"You can normalize an input string using the Whisper basic normalizer using the `basic_normalize` method.\"\n-        )\n-        return self.basic_normalize(text, remove_diacritics=remove_diacritics)\n-\n     def normalize(self, text):\n         \"\"\"\n         Normalize a given string using the `EnglishTextNormalizer` class, which performs commons transformation on"
        },
        {
            "sha": "904f099243f916ef91e57d987f1285c1204efc23",
            "filename": "src/transformers/models/whisper/tokenization_whisper_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 19,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper_fast.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -17,7 +17,6 @@\n import json\n import os\n import re\n-import warnings\n from functools import lru_cache\n from typing import Optional\n \n@@ -393,30 +392,14 @@ def _decode(\n         text = super()._decode(*args, **kwargs)\n \n         if normalize:\n-            clean_text = self._normalize(text)\n+            clean_text = self.normalize(text)\n             return clean_text\n         elif basic_normalize:\n-            clean_text = self._basic_normalize(text, remove_diacritics=remove_diacritics)\n+            clean_text = self.basic_normalize(text, remove_diacritics=remove_diacritics)\n             return clean_text\n         else:\n             return text\n \n-    # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer._normalize\n-    def _normalize(self, text):\n-        warnings.warn(\n-            \"The private method `_normalize` is deprecated and will be removed in v5 of Transformers.\"\n-            \"You can normalize an input string using the Whisper English normalizer using the `normalize` method.\"\n-        )\n-        return self.normalize(text)\n-\n-    # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer._basic_normalize\n-    def _basic_normalize(self, text, remove_diacritics=False):\n-        warnings.warn(\n-            \"The private method `_basic_normalize` is deprecated and will be removed in v5 of Transformers.\"\n-            \"You can normalize an input string using the Whisper basic normalizer using the `basic_normalize` method.\"\n-        )\n-        return self.basic_normalize(text, remove_diacritics=remove_diacritics)\n-\n     # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer.normalize\n     def normalize(self, text):\n         \"\"\""
        },
        {
            "sha": "ed4ff14f6f918b2be862eae157941eb245465336",
            "filename": "src/transformers/models/yolos/image_processing_yolos.py",
            "status": "modified",
            "additions": 3,
            "deletions": 93,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -786,18 +786,7 @@ def __init__(\n         pad_size: Optional[dict[str, int]] = None,\n         **kwargs,\n     ) -> None:\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            do_pad = kwargs.pop(\"pad_and_return_pixel_mask\")\n-\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` parameter is deprecated and will be removed in v4.26. \"\n-                \"Please specify in `size['longest_edge'] instead`.\",\n-            )\n-            max_size = kwargs.pop(\"max_size\")\n-        else:\n-            max_size = None if size is None else 1333\n-\n+        max_size = None if size is None else kwargs.pop(\"max_size\", 1333)\n         size = size if size is not None else {\"shortest_edge\": 800, \"longest_edge\": 1333}\n         size = get_size_dict(size, max_size=max_size, default_to_square=False)\n \n@@ -816,7 +805,7 @@ def __init__(\n         self.do_convert_annotations = do_convert_annotations\n         self.image_mean = image_mean if image_mean is not None else IMAGENET_DEFAULT_MEAN\n         self.image_std = image_std if image_std is not None else IMAGENET_DEFAULT_STD\n-        self.do_pad = do_pad\n+        self.do_pad = kwargs.pop(\"pad_and_return_pixel_mask\", do_pad)\n         self.pad_size = pad_size\n         self._valid_processor_keys = [\n             \"images\",\n@@ -840,21 +829,6 @@ def __init__(\n             \"input_data_format\",\n         ]\n \n-    @classmethod\n-    # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.from_dict with Detr->Yolos\n-    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n-        \"\"\"\n-        Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n-        created using from_dict and kwargs e.g. `YolosImageProcessor.from_pretrained(checkpoint, size=600,\n-        max_size=800)`\n-        \"\"\"\n-        image_processor_dict = image_processor_dict.copy()\n-        if \"max_size\" in kwargs:\n-            image_processor_dict[\"max_size\"] = kwargs.pop(\"max_size\")\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            image_processor_dict[\"pad_and_return_pixel_mask\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n-        return super().from_dict(image_processor_dict, **kwargs)\n-\n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.prepare_annotation\n     def prepare_annotation(\n         self,\n@@ -923,15 +897,7 @@ def resize(\n             input_data_format (`ChannelDimension` or `str`, *optional*):\n                 The channel dimension format of the input image. If not provided, it will be inferred.\n         \"\"\"\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` parameter is deprecated and will be removed in v4.26. \"\n-                \"Please specify in `size['longest_edge'] instead`.\",\n-            )\n-            max_size = kwargs.pop(\"max_size\")\n-        else:\n-            max_size = None\n-        size = get_size_dict(size, max_size=max_size, default_to_square=False)\n+        size = get_size_dict(size, max_size=None, default_to_square=False)\n         if \"shortest_edge\" in size and \"longest_edge\" in size:\n             new_size = get_resize_output_image_size(\n                 image, size[\"shortest_edge\"], size[\"longest_edge\"], input_data_format=input_data_format\n@@ -1264,20 +1230,6 @@ def preprocess(\n                 provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n                 height and width in the batch.\n         \"\"\"\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            logger.warning_once(\n-                \"The `pad_and_return_pixel_mask` argument is deprecated and will be removed in v4.33, \"\n-                \"use `do_pad` instead.\",\n-            )\n-            do_pad = kwargs.pop(\"pad_and_return_pixel_mask\")\n-\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` argument is deprecated and will be removed in v4.33, use\"\n-                \" `size['longest_edge']` instead.\",\n-            )\n-            size = kwargs.pop(\"max_size\")\n-\n         do_resize = self.do_resize if do_resize is None else do_resize\n         size = self.size if size is None else size\n         size = get_size_dict(size=size, default_to_square=False)\n@@ -1427,48 +1379,6 @@ def preprocess(\n \n         return encoded_inputs\n \n-    # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process  with Detr->Yolos\n-    def post_process(self, outputs, target_sizes):\n-        \"\"\"\n-        Converts the raw output of [`YolosForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n-        bottom_right_x, bottom_right_y) format.\n-\n-        Args:\n-            outputs ([`YolosObjectDetectionOutput`]):\n-                Raw outputs of the model.\n-            target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n-                Tensor containing the size (height, width) of each image of the batch. For evaluation, this must be the\n-                original image size (before any data augmentation). For visualization, this should be the image size\n-                after data augment, but before padding.\n-        Returns:\n-            `list[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n-            in the batch as predicted by the model.\n-        \"\"\"\n-        logger.warning_once(\n-            \"`post_process` is deprecated and will be removed in v5 of Transformers, please use\"\n-            \" `post_process_object_detection` instead, with `threshold=0.` for equivalent results.\",\n-        )\n-\n-        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n-\n-        if len(out_logits) != len(target_sizes):\n-            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n-        if target_sizes.shape[1] != 2:\n-            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n-\n-        prob = nn.functional.softmax(out_logits, -1)\n-        scores, labels = prob[..., :-1].max(-1)\n-\n-        # convert to [x0, y0, x1, y1] format\n-        boxes = center_to_corners_format(out_bbox)\n-        # and from relative [0, 1] to absolute [0, height] coordinates\n-        img_h, img_w = target_sizes.unbind(1)\n-        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n-        boxes = boxes * scale_fct[:, None, :]\n-\n-        results = [{\"scores\": s, \"labels\": l, \"boxes\": b} for s, l, b in zip(scores, labels, boxes)]\n-        return results\n-\n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_object_detection with Detr->Yolos\n     def post_process_object_detection(\n         self, outputs, threshold: float = 0.5, target_sizes: Optional[Union[TensorType, list[tuple]]] = None"
        },
        {
            "sha": "53e7318c6285c7d60000f045d8cb1570f52a1044",
            "filename": "src/transformers/models/yolos/image_processing_yolos_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 96,
            "changes": 99,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -26,19 +26,16 @@\n     AnnotationFormat,\n     AnnotationType,\n     ChannelDimension,\n-    ImageInput,\n     PILImageResampling,\n     get_image_size,\n     validate_annotations,\n )\n from ...processing_utils import Unpack\n-from ...utils import TensorType, auto_docstring, logging\n+from ...utils import TensorType, auto_docstring\n from ...utils.import_utils import requires\n from .image_processing_yolos import YolosImageProcessorKwargs\n \n \n-logger = logging.get_logger(__name__)\n-\n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n \n \n@@ -305,19 +302,10 @@ class YolosImageProcessorFast(BaseImageProcessorFast):\n     valid_kwargs = YolosImageProcessorKwargs\n \n     def __init__(self, **kwargs: Unpack[YolosImageProcessorKwargs]) -> None:\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n+        kwargs.setdefault(\"do_pad\", kwargs.pop(\"pad_and_return_pixel_mask\", self.do_pad))\n \n         size = kwargs.pop(\"size\", None)\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` parameter is deprecated and will be removed in v4.26. \"\n-                \"Please specify in `size['longest_edge'] instead`.\",\n-            )\n-            max_size = kwargs.pop(\"max_size\")\n-        else:\n-            max_size = None if size is None else 1333\n-\n+        max_size = None if size is None else kwargs.pop(\"max_size\", 1333)\n         size = size if size is not None else {\"shortest_edge\": 800, \"longest_edge\": 1333}\n         self.size = get_size_dict(size, max_size=max_size, default_to_square=False)\n \n@@ -329,20 +317,6 @@ def __init__(self, **kwargs: Unpack[YolosImageProcessorKwargs]) -> None:\n \n         super().__init__(**kwargs)\n \n-    @classmethod\n-    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n-        \"\"\"\n-        Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n-        created using from_dict and kwargs e.g. `YolosImageProcessorFast.from_pretrained(checkpoint, size=600,\n-        max_size=800)`\n-        \"\"\"\n-        image_processor_dict = image_processor_dict.copy()\n-        if \"max_size\" in kwargs:\n-            image_processor_dict[\"max_size\"] = kwargs.pop(\"max_size\")\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            image_processor_dict[\"pad_and_return_pixel_mask\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n-        return super().from_dict(image_processor_dict, **kwargs)\n-\n     def prepare_annotation(\n         self,\n         image: torch.Tensor,\n@@ -562,28 +536,6 @@ def pad(\n \n         return image, pixel_mask, annotation\n \n-    @auto_docstring\n-    def preprocess(\n-        self,\n-        images: ImageInput,\n-        **kwargs: Unpack[YolosImageProcessorKwargs],\n-    ) -> BatchFeature:\n-        if \"pad_and_return_pixel_mask\" in kwargs:\n-            kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n-            logger.warning_once(\n-                \"The `pad_and_return_pixel_mask` argument is deprecated and will be removed in a future version, \"\n-                \"use `do_pad` instead.\"\n-            )\n-\n-        if \"max_size\" in kwargs:\n-            logger.warning_once(\n-                \"The `max_size` argument is deprecated and will be removed in a future version, use\"\n-                \" `size['longest_edge']` instead.\"\n-            )\n-            kwargs[\"size\"] = kwargs.pop(\"max_size\")\n-\n-        return super().preprocess(images, **kwargs)\n-\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n@@ -700,51 +652,6 @@ def _preprocess(\n             ]\n         return encoded_inputs\n \n-    def post_process(self, outputs, target_sizes):\n-        \"\"\"\n-        Converts the raw output of [`YolosForObjectDetection`] into final bounding boxes in (top_left_x,\n-        top_left_y, bottom_right_x, bottom_right_y) format. Only supports PyTorch.\n-\n-        Args:\n-            outputs ([`YolosObjectDetectionOutput`]):\n-                Raw outputs of the model.\n-            target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n-                Tensor containing the size (height, width) of each image of the batch. For evaluation, this must be the\n-                original image size (before any data augmentation). For visualization, this should be the image size\n-                after data augment, but before padding.\n-        Returns:\n-            `list[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n-            in the batch as predicted by the model.\n-        \"\"\"\n-        logger.warning_once(\n-            \"`post_process` is deprecated and will be removed in v5 of Transformers, please use\"\n-            \" `post_process_object_detection` instead, with `threshold=0.` for equivalent results.\",\n-        )\n-\n-        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n-\n-        if len(out_logits) != len(target_sizes):\n-            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n-        if target_sizes.shape[1] != 2:\n-            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n-\n-        prob = out_logits.sigmoid()\n-        topk_values, topk_indexes = torch.topk(prob.view(out_logits.shape[0], -1), 100, dim=1)\n-        scores = topk_values\n-        topk_boxes = torch.div(topk_indexes, out_logits.shape[2], rounding_mode=\"floor\")\n-        labels = topk_indexes % out_logits.shape[2]\n-        boxes = center_to_corners_format(out_bbox)\n-        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n-\n-        # and from relative [0, 1] to absolute [0, height] coordinates\n-        img_h, img_w = target_sizes.unbind(1)\n-        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n-        boxes = boxes * scale_fct[:, None, :]\n-\n-        results = [{\"scores\": s, \"labels\": l, \"boxes\": b} for s, l, b in zip(scores, labels, boxes)]\n-\n-        return results\n-\n     def post_process_object_detection(\n         self, outputs, threshold: float = 0.5, target_sizes: Union[TensorType, list[tuple]] = None, top_k: int = 100\n     ):"
        },
        {
            "sha": "5de64f7cf5cf4149aa65519b98bab2dd82041809",
            "filename": "src/transformers/models/yolos/modular_yolos.py",
            "status": "modified",
            "additions": 0,
            "deletions": 54,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodular_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodular_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodular_yolos.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -64,51 +64,6 @@ def get_size_with_aspect_ratio(\n \n \n class YolosImageProcessorFast(DetrImageProcessorFast):\n-    def post_process(self, outputs, target_sizes):\n-        \"\"\"\n-        Converts the raw output of [`YolosForObjectDetection`] into final bounding boxes in (top_left_x,\n-        top_left_y, bottom_right_x, bottom_right_y) format. Only supports PyTorch.\n-\n-        Args:\n-            outputs ([`YolosObjectDetectionOutput`]):\n-                Raw outputs of the model.\n-            target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n-                Tensor containing the size (height, width) of each image of the batch. For evaluation, this must be the\n-                original image size (before any data augmentation). For visualization, this should be the image size\n-                after data augment, but before padding.\n-        Returns:\n-            `list[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n-            in the batch as predicted by the model.\n-        \"\"\"\n-        logger.warning_once(\n-            \"`post_process` is deprecated and will be removed in v5 of Transformers, please use\"\n-            \" `post_process_object_detection` instead, with `threshold=0.` for equivalent results.\",\n-        )\n-\n-        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n-\n-        if len(out_logits) != len(target_sizes):\n-            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n-        if target_sizes.shape[1] != 2:\n-            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n-\n-        prob = out_logits.sigmoid()\n-        topk_values, topk_indexes = torch.topk(prob.view(out_logits.shape[0], -1), 100, dim=1)\n-        scores = topk_values\n-        topk_boxes = torch.div(topk_indexes, out_logits.shape[2], rounding_mode=\"floor\")\n-        labels = topk_indexes % out_logits.shape[2]\n-        boxes = center_to_corners_format(out_bbox)\n-        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n-\n-        # and from relative [0, 1] to absolute [0, height] coordinates\n-        img_h, img_w = target_sizes.unbind(1)\n-        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n-        boxes = boxes * scale_fct[:, None, :]\n-\n-        results = [{\"scores\": s, \"labels\": l, \"boxes\": b} for s, l, b in zip(scores, labels, boxes)]\n-\n-        return results\n-\n     def post_process_object_detection(\n         self, outputs, threshold: float = 0.5, target_sizes: Union[TensorType, list[tuple]] = None, top_k: int = 100\n     ):\n@@ -168,15 +123,6 @@ def post_process_object_detection(\n \n         return results\n \n-    def post_process_segmentation(self):\n-        raise NotImplementedError(\"Segmentation post-processing is not implemented for Deformable DETR yet.\")\n-\n-    def post_process_instance(self):\n-        raise NotImplementedError(\"Instance post-processing is not implemented for Deformable DETR yet.\")\n-\n-    def post_process_panoptic(self):\n-        raise NotImplementedError(\"Panoptic post-processing is not implemented for Deformable DETR yet.\")\n-\n     def post_process_instance_segmentation(self):\n         raise NotImplementedError(\"Segmentation post-processing is not implemented for Deformable DETR yet.\")\n "
        },
        {
            "sha": "74c04d604ff356ee5c0d4dd55e8da37b018173ba",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -61,7 +61,6 @@\n     logging,\n )\n from .utils.chat_template_utils import render_jinja_template\n-from .utils.deprecation import deprecate_kwarg\n from .utils.type_validators import (\n     device_validator,\n     image_size_validator,\n@@ -630,6 +629,9 @@ def __call__(\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] object with processed inputs in a dict format.\n         \"\"\"\n+        if \"audios\" in kwargs and audio is None:\n+            raise ValueError(\"You passed keyword argument `audios` which is deprecated. Please use `audio` instead.\")\n+\n         if images is None and text is None and videos is None and audio is None:\n             raise ValueError(f\"You need to provide at least one input to call {self.__class__.__name__}\")\n \n@@ -1534,12 +1536,6 @@ def validate_init_kwargs(processor_config, valid_kwargs):\n \n         return unused_kwargs, valid_kwargs\n \n-    @deprecate_kwarg(\"video_fps\", version=\"4.58\", new_name=\"fps\")\n-    @deprecate_kwarg(\n-        \"video_load_backend\",\n-        version=\"4.59\",\n-        additional_message=\". This function will use `torchcodec` by default, or `torchvision` if `torchcodec` is not installed.\",\n-    )\n     def apply_chat_template(\n         self,\n         conversation: Union[list[dict[str, str]], list[list[dict[str, str]]]],\n@@ -1627,9 +1623,6 @@ def apply_chat_template(\n                 if value is not None and not isinstance(value, dict):\n                     processed_kwargs[kwarg_type][key] = value\n \n-        # pop unused and deprecated kwarg\n-        kwargs.pop(\"video_load_backend\", None)\n-\n         # Pass unprocessed custom kwargs\n         processed_kwargs[\"template_kwargs\"].update(kwargs)\n "
        },
        {
            "sha": "5436a4252722e03c718275f70b1f3a8e8413d5ce",
            "filename": "tests/fixtures/config.json",
            "status": "added",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/tests%2Ffixtures%2Fconfig.json",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/tests%2Ffixtures%2Fconfig.json",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ffixtures%2Fconfig.json?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -0,0 +1,4 @@\n+{\n+    \"model_type\": \"wav2vec2\"\n+}\n+  \n\\ No newline at end of file"
        },
        {
            "sha": "ff27cd3a8a0bbecc5b1d983c49ffc3b442348646",
            "filename": "tests/models/auto/test_processor_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -69,6 +69,7 @@\n SAMPLE_PROCESSOR_CONFIG = get_tests_dir(\"fixtures/dummy_feature_extractor_config.json\")\n SAMPLE_VOCAB_LLAMA = get_tests_dir(\"fixtures/test_sentencepiece.model\")\n SAMPLE_VOCAB = get_tests_dir(\"fixtures/vocab.json\")\n+SAMPLE_CONFIG = get_tests_dir(\"fixtures/config.json\")\n SAMPLE_PROCESSOR_CONFIG_DIR = get_tests_dir(\"fixtures\")\n \n \n@@ -109,6 +110,7 @@ def test_processor_from_local_directory_from_extractor_config(self):\n             # copy relevant files\n             copyfile(SAMPLE_PROCESSOR_CONFIG, os.path.join(tmpdirname, FEATURE_EXTRACTOR_NAME))\n             copyfile(SAMPLE_VOCAB, os.path.join(tmpdirname, \"vocab.json\"))\n+            copyfile(SAMPLE_CONFIG, os.path.join(tmpdirname, \"config.json\"))\n \n             processor = AutoProcessor.from_pretrained(tmpdirname)\n "
        },
        {
            "sha": "d852ffb6a9437c20a8864bfbb77ed7385c0332a0",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -942,7 +942,7 @@ def test_get_text_features(self):\n         model = Blip2Model(config).to(torch_device)\n         model.eval()\n         text_features = model.get_text_features(**inputs_dict)\n-        self.assertEqual(text_features[0].shape, (1, 10, config.text_config.vocab_size))\n+        self.assertEqual(text_features[0].shape, (10, config.text_config.vocab_size))\n \n     def test_get_image_features(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -957,11 +957,7 @@ def test_get_image_features(self):\n         image_features = model.get_image_features(**inputs_dict)\n         self.assertEqual(\n             image_features[0].shape,\n-            (\n-                self.model_tester.vision_model_tester.batch_size,\n-                self.model_tester.vision_model_tester.seq_length,\n-                config.vision_config.hidden_size,\n-            ),\n+            (config.vision_config.hidden_size,),\n         )\n \n     def test_get_qformer_features(self):\n@@ -977,7 +973,7 @@ def test_get_qformer_features(self):\n         qformer_features = model.get_qformer_features(**inputs_dict)\n         self.assertEqual(\n             qformer_features[0].shape,\n-            (self.model_tester.vision_model_tester.batch_size, 10, config.vision_config.hidden_size),\n+            (10, config.vision_config.hidden_size),\n         )\n \n     @unittest.skip(\"T5 backbone deepcopies the configs, and fixing it would be more involved\")"
        },
        {
            "sha": "4327f0a158785032e93c631f79f45deddfcba9b8",
            "filename": "tests/models/clap/test_modeling_clap.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -562,7 +562,7 @@ def test_integration_unfused(self):\n         processor = ClapProcessor.from_pretrained(model_id)\n \n         for padding in self.paddings:\n-            inputs = processor(audios=audio_sample[\"audio\"][\"array\"], return_tensors=\"pt\", padding=padding).to(\n+            inputs = processor(audio=audio_sample[\"audio\"][\"array\"], return_tensors=\"pt\", padding=padding).to(\n                 torch_device\n             )\n \n@@ -590,7 +590,7 @@ def test_integration_fused(self):\n \n         for padding in self.paddings:\n             inputs = processor(\n-                audios=audio_sample[\"audio\"][\"array\"], return_tensors=\"pt\", padding=padding, truncation=\"fusion\"\n+                audio=audio_sample[\"audio\"][\"array\"], return_tensors=\"pt\", padding=padding, truncation=\"fusion\"\n             ).to(torch_device)\n \n             audio_embed = model.get_audio_features(**inputs)\n@@ -616,7 +616,7 @@ def test_batched_fused(self):\n         processor = ClapProcessor.from_pretrained(model_id)\n \n         for padding in self.paddings:\n-            inputs = processor(audios=audio_samples, return_tensors=\"pt\", padding=padding, truncation=\"fusion\").to(\n+            inputs = processor(audio=audio_samples, return_tensors=\"pt\", padding=padding, truncation=\"fusion\").to(\n                 torch_device\n             )\n \n@@ -643,7 +643,7 @@ def test_batched_unfused(self):\n         processor = ClapProcessor.from_pretrained(model_id)\n \n         for padding in self.paddings:\n-            inputs = processor(audios=audio_samples, return_tensors=\"pt\", padding=padding).to(torch_device)\n+            inputs = processor(audio=audio_samples, return_tensors=\"pt\", padding=padding).to(torch_device)\n \n             audio_embed = model.get_audio_features(**inputs)\n             expected_mean = EXPECTED_MEANS_FUSED[padding]"
        },
        {
            "sha": "ce6000ed2c57e27410b2dd359c50236cb8c0ee02",
            "filename": "tests/models/clap/test_processing_clap.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fclap%2Ftest_processing_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fclap%2Ftest_processing_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclap%2Ftest_processing_clap.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -79,7 +79,7 @@ def test_feature_extractor(self):\n         raw_speech = floats_list((3, 1000))\n \n         input_feat_extract = feature_extractor(raw_speech, return_tensors=\"np\")\n-        input_processor = processor(audios=raw_speech, return_tensors=\"np\")\n+        input_processor = processor(audio=raw_speech, return_tensors=\"np\")\n \n         for key in input_feat_extract:\n             self.assertAlmostEqual(input_feat_extract[key].sum(), input_processor[key].sum(), delta=1e-2)"
        },
        {
            "sha": "c8392eff0d1db304d2c4f4396d4f2f0896a33272",
            "filename": "tests/models/conditional_detr/test_image_processing_conditional_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fconditional_detr%2Ftest_image_processing_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fconditional_detr%2Ftest_image_processing_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconditional_detr%2Ftest_image_processing_conditional_detr.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -160,11 +160,8 @@ def test_image_processor_from_dict_with_kwargs(self):\n             self.assertEqual(image_processor.size, {\"shortest_edge\": 18, \"longest_edge\": 1333})\n             self.assertEqual(image_processor.do_pad, True)\n \n-            image_processor = image_processing_class.from_dict(\n-                self.image_processor_dict, size=42, max_size=84, pad_and_return_pixel_mask=False\n-            )\n-            self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 84})\n-            self.assertEqual(image_processor.do_pad, False)\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 1333})\n \n     @slow\n     def test_call_pytorch_with_coco_detection_annotations(self):"
        },
        {
            "sha": "594bcbceb1783b249b9e46aae9fd5a464b08e14b",
            "filename": "tests/models/deformable_detr/test_image_processing_deformable_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fdeformable_detr%2Ftest_image_processing_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fdeformable_detr%2Ftest_image_processing_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeformable_detr%2Ftest_image_processing_deformable_detr.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -165,11 +165,8 @@ def test_image_processor_from_dict_with_kwargs(self):\n             self.assertEqual(image_processor.size, {\"shortest_edge\": 18, \"longest_edge\": 1333})\n             self.assertEqual(image_processor.do_pad, True)\n \n-            image_processor = image_processing_class.from_dict(\n-                self.image_processor_dict, size=42, max_size=84, pad_and_return_pixel_mask=False\n-            )\n-            self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 84})\n-            self.assertEqual(image_processor.do_pad, False)\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 1333})\n \n     @slow\n     def test_call_pytorch_with_coco_detection_annotations(self):"
        },
        {
            "sha": "3a51b05dc7e32597be85a232f0d0c11add830274",
            "filename": "tests/models/detr/test_image_processing_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fdetr%2Ftest_image_processing_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fdetr%2Ftest_image_processing_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdetr%2Ftest_image_processing_detr.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -169,11 +169,8 @@ def test_image_processor_from_dict_with_kwargs(self):\n             self.assertEqual(image_processor.size, {\"shortest_edge\": 18, \"longest_edge\": 1333})\n             self.assertEqual(image_processor.do_pad, True)\n \n-            image_processor = image_processing_class.from_dict(\n-                self.image_processor_dict, size=42, max_size=84, pad_and_return_pixel_mask=False\n-            )\n-            self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 84})\n-            self.assertEqual(image_processor.do_pad, False)\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 1333})\n \n     def test_should_raise_if_annotation_format_invalid(self):\n         image_processor_dict = self.image_processor_tester.prepare_image_processor_dict()"
        },
        {
            "sha": "0b52faa66b3c061ed54dcd695225912a1a381ab5",
            "filename": "tests/models/glm4v/test_processor_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fglm4v%2Ftest_processor_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fglm4v%2Ftest_processor_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v%2Ftest_processor_glm4v.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -215,7 +215,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n             add_generation_prompt=True,\n             tokenize=True,\n             return_dict=True,\n-            video_fps=video_fps,\n+            fps=video_fps,\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 8)"
        },
        {
            "sha": "b085cfeda7f899afa667c0a9106ac53f2fa9a31d",
            "filename": "tests/models/grounding_dino/test_image_processing_grounding_dino.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fgrounding_dino%2Ftest_image_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fgrounding_dino%2Ftest_image_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_image_processing_grounding_dino.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -179,11 +179,8 @@ def test_image_processor_from_dict_with_kwargs(self):\n             self.assertEqual(image_processor.size, {\"shortest_edge\": 18, \"longest_edge\": 1333})\n             self.assertEqual(image_processor.do_pad, True)\n \n-            image_processor = image_processing_class.from_dict(\n-                self.image_processor_dict, size=42, max_size=84, pad_and_return_pixel_mask=False\n-            )\n-            self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 84})\n-            self.assertEqual(image_processor.do_pad, False)\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 1333})\n \n     def test_post_process_object_detection(self):\n         for image_processing_class in self.image_processor_list:"
        },
        {
            "sha": "be61f940e12da5d8db7f7f0605bd67ac354d406c",
            "filename": "tests/models/maskformer/test_image_processing_maskformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fmaskformer%2Ftest_image_processing_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fmaskformer%2Ftest_image_processing_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmaskformer%2Ftest_image_processing_maskformer.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -432,30 +432,6 @@ def test_binary_mask_to_rle(self):\n         self.assertEqual(rle[0], 21)\n         self.assertEqual(rle[1], 45)\n \n-    def test_post_process_segmentation(self):\n-        for image_processing_class in self.image_processor_list:\n-            feature_extractor = image_processing_class(num_labels=self.image_processor_tester.num_classes)\n-            outputs = self.image_processor_tester.get_fake_maskformer_outputs()\n-            segmentation = feature_extractor.post_process_segmentation(outputs)\n-\n-            self.assertEqual(\n-                segmentation.shape,\n-                (\n-                    self.image_processor_tester.batch_size,\n-                    self.image_processor_tester.num_classes,\n-                    self.image_processor_tester.height,\n-                    self.image_processor_tester.width,\n-                ),\n-            )\n-\n-            target_size = (1, 4)\n-            segmentation = feature_extractor.post_process_segmentation(outputs, target_size=target_size)\n-\n-            self.assertEqual(\n-                segmentation.shape,\n-                (self.image_processor_tester.batch_size, self.image_processor_tester.num_classes, *target_size),\n-            )\n-\n     def test_post_process_semantic_segmentation(self):\n         for image_processing_class in self.image_processor_list:\n             feature_extractor = image_processing_class(num_labels=self.image_processor_tester.num_classes)"
        },
        {
            "sha": "b2ac9930a4f91f1ee30a063d38303ecfaaf9551e",
            "filename": "tests/models/seamless_m4t/test_modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -867,7 +867,7 @@ def input_audio(self):\n         sampling_rate = 16000\n         input_features = torch.rand((2, seq_len))\n \n-        return self.processor(audios=[input_features.tolist()], sampling_rate=sampling_rate, return_tensors=\"pt\").to(\n+        return self.processor(audio=[input_features.tolist()], sampling_rate=sampling_rate, return_tensors=\"pt\").to(\n             torch_device\n         )\n "
        },
        {
            "sha": "5fccf3d92a4c014d2e29174d6ea233451040df79",
            "filename": "tests/models/seamless_m4t/test_processing_seamless_m4t.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fseamless_m4t%2Ftest_processing_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fseamless_m4t%2Ftest_processing_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t%2Ftest_processing_seamless_m4t.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -86,7 +86,7 @@ def test_feature_extractor(self):\n         raw_speech = floats_list((3, 1000))\n \n         input_feat_extract = feature_extractor(raw_speech, return_tensors=\"np\")\n-        input_processor = processor(audios=raw_speech, return_tensors=\"np\")\n+        input_processor = processor(audio=raw_speech, return_tensors=\"np\")\n \n         for key in input_feat_extract:\n             self.assertAlmostEqual(input_feat_extract[key].sum(), input_processor[key].sum(), delta=1e-2)"
        },
        {
            "sha": "1a07f8b7d3c7a3af0ff58d8b9b5b2353d3efae4b",
            "filename": "tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -929,7 +929,7 @@ def input_audio(self):\n         sampling_rate = 16000\n         input_features = torch.rand((2, seq_len))\n \n-        return self.processor(audios=[input_features.tolist()], sampling_rate=sampling_rate, return_tensors=\"pt\").to(\n+        return self.processor(audio=[input_features.tolist()], sampling_rate=sampling_rate, return_tensors=\"pt\").to(\n             torch_device\n         )\n "
        },
        {
            "sha": "dfff76b98454323e70521169d503dcbd957c9dde",
            "filename": "tests/models/swin2sr/test_image_processing_swin2sr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fswin2sr%2Ftest_image_processing_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fswin2sr%2Ftest_image_processing_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswin2sr%2Ftest_image_processing_swin2sr.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -117,7 +117,6 @@ def test_image_processor_properties(self):\n             self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n             self.assertTrue(hasattr(image_processing, \"do_pad\"))\n             self.assertTrue(hasattr(image_processing, \"size_divisor\"))\n-            self.assertTrue(hasattr(image_processing, \"pad_size\"))  # deprecated but should be available\n \n     def calculate_expected_size(self, image):\n         old_height, old_width = get_image_size(image)"
        },
        {
            "sha": "2802aad7eb4dc9fd5e51b1a417110c553a4827e0",
            "filename": "tests/models/vitmatte/test_image_processing_vitmatte.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -126,8 +126,6 @@ def test_image_processor_properties(self):\n             self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n             self.assertTrue(hasattr(image_processing, \"do_pad\"))\n             self.assertTrue(hasattr(image_processing, \"size_divisor\"))\n-            # Check size_divisibility for BC, the image proccessor has to have an atribute\n-            self.assertTrue(hasattr(image_processing, \"size_divisibility\"))\n \n     def test_call_numpy(self):\n         # create random numpy tensors"
        },
        {
            "sha": "18cdb8c867bac95e3dc1df192c4339a67e040ff3",
            "filename": "tests/models/yolos/test_image_processing_yolos.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fyolos%2Ftest_image_processing_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32a58e31463e238c967207bf73772490c353551a/tests%2Fmodels%2Fyolos%2Ftest_image_processing_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fyolos%2Ftest_image_processing_yolos.py?ref=32a58e31463e238c967207bf73772490c353551a",
            "patch": "@@ -171,11 +171,8 @@ def test_image_processor_from_dict_with_kwargs(self):\n             self.assertEqual(image_processor.size, {\"shortest_edge\": 18, \"longest_edge\": 1333})\n             self.assertEqual(image_processor.do_pad, True)\n \n-            image_processor = image_processing_class.from_dict(\n-                self.image_processor_dict, size=42, max_size=84, pad_and_return_pixel_mask=False\n-            )\n-            self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 84})\n-            self.assertEqual(image_processor.do_pad, False)\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 1333})\n \n     def test_equivalence_padding(self):\n         # Initialize image_processings"
        }
    ],
    "stats": {
        "total": 2928,
        "additions": 121,
        "deletions": 2807
    }
}