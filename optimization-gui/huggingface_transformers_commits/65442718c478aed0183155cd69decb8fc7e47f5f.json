{
    "author": "yonigozlan",
    "message": "Add support for inheritance from class with different suffix in modular (#34077)\n\n* add support for different suffix in modular\r\n\r\n* add dummy example, pull new changes for modular\r\n\r\n* nide lines order change",
    "sha": "65442718c478aed0183155cd69decb8fc7e47f5f",
    "files": [
        {
            "sha": "640331ace1d57b47cef0483aa20f96569e79b644",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "added",
            "additions": 546,
            "deletions": 0,
            "changes": 546,
            "blob_url": "https://github.com/huggingface/transformers/blob/65442718c478aed0183155cd69decb8fc7e47f5f/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65442718c478aed0183155cd69decb8fc7e47f5f/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=65442718c478aed0183155cd69decb8fc7e47f5f",
            "patch": "@@ -0,0 +1,546 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from examples/modular-transformers/modular_new_task_model.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_new_task_model.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+from dataclasses import dataclass\n+from typing import ClassVar, List, Optional, Tuple, Union\n+\n+import torch\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from ...cache_utils import Cache, StaticCache\n+from ...generation import GenerationMixin\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    ModelOutput,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    is_flash_attn_2_available,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from .configuration_new_task_model import NewTaskModelConfig\n+\n+\n+if is_flash_attn_2_available():\n+    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n+\n+from ..auto import AutoModel, AutoModelForCausalLM\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+_CONFIG_FOR_DOC = \"NewTaskModelConfig\"\n+\n+\n+# Adapted from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+# But NewTaskModel has no causal mask on prefix\n+def _prepare_4d_causal_attention_mask_with_cache_position(\n+    attention_mask: torch.Tensor,\n+    sequence_length: int,\n+    target_length: int,\n+    dtype: torch.dtype,\n+    device: torch.device,\n+    min_dtype: float,\n+    cache_position: torch.Tensor,\n+    batch_size: int,\n+    is_training: bool = False,\n+    token_type_ids: torch.Tensor = None,\n+):\n+    \"\"\"\n+    Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+    `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+    Args:\n+        attention_mask (`torch.Tensor`):\n+            A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n+        sequence_length (`int`):\n+            The sequence length being processed.\n+        target_length (`int`):\n+            The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n+        dtype (`torch.dtype`):\n+            The dtype to use for the 4D attention mask.\n+        device (`torch.device`):\n+            The device to plcae the 4D attention mask on.\n+        min_dtype (`float`):\n+            The minimum value representable with the dtype `dtype`.\n+        cache_position (`torch.Tensor`):\n+            Indices depicting the position of the input sequence tokens in the sequence.\n+        batch_size (`torch.Tensor`):\n+            Batch size.\n+        is_training (`bool`):\n+            Whether the model is in training mode or in inference. The condition is checked by presence/absence of `token_type_ids/labels`\n+    \"\"\"\n+    if attention_mask is not None and attention_mask.dim() == 4:\n+        # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+        causal_mask = attention_mask\n+    else:\n+        causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)\n+        # Causal diagonal mask only if training, otherwise attend to the whole prefix. Training-specific attn for prefix is handled below\n+        if sequence_length != 1:\n+            if is_training:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            else:\n+                causal_mask[:, :sequence_length] = 0.0\n+\n+        causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+        causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+        if attention_mask is not None:\n+            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+            mask_length = attention_mask.shape[-1]\n+            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(causal_mask.device)\n+            padding_mask = padding_mask == 0\n+            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                padding_mask, min_dtype\n+            )\n+            # we are training thus we need to create a full mask on the image + prefix but causal on suffix\n+            if is_training:\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    token_type_ids[:, None, None, :].to(causal_mask.device) == 0, 0\n+                )\n+    return causal_mask\n+\n+\n+@dataclass\n+class NewTaskModelCausalLMOutputWithPast(ModelOutput):\n+    \"\"\"\n+    Base class for NewTaskModelcausal language model (or autoregressive) outputs.\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+            Language modeling loss (for next-token prediction).\n+        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.text_config.vocab_size)`):\n+            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder after projecting last hidden state.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: torch.FloatTensor = None\n+    past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+class NewTaskModelMultiModalProjector(nn.Module):\n+    def __init__(self, config: NewTaskModelConfig):\n+        super().__init__()\n+        self.linear = nn.Linear(config.vision_config.hidden_size, config.vision_config.projection_dim, bias=True)\n+\n+    def forward(self, image_features):\n+        hidden_states = self.linear(image_features)\n+\n+        return hidden_states\n+\n+\n+NEW_TASK_MODEL_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`NewTaskModelConfig`] or [`NewTaskModelVisionConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n+    NEW_TASK_MODEL_START_DOCSTRING,\n+)\n+class NewTaskModelPreTrainedModel(PreTrainedModel):\n+    config_class = NewTaskModelConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"NewTaskModelMultiModalProjector\"]\n+    _skip_keys_device_placement = \"past_key_values\"\n+    _supports_flash_attn_2 = False\n+    _supports_cache_class = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n+    _supports_sdpa = True\n+    _supports_cache_class = True\n+\n+    def _init_weights(self, module):\n+        # important: this ported version of NewTaskModelisn't meant for training from scratch - only\n+        # inference and fine-tuning\n+        std = (\n+            self.config.initializer_range\n+            if hasattr(self.config, \"initializer_range\")\n+            else self.config.text_config.initializer_range\n+        )\n+\n+        if hasattr(module, \"class_embedding\"):\n+            module.class_embedding.data.normal_(mean=0.0, std=std)\n+\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+    @property\n+    def _supports_sdpa(self):\n+        \"\"\"\n+        Retrieve language_model's attribute to check whether the model supports\n+        SDPA or not.\n+        \"\"\"\n+        return self.language_model._supports_sdpa\n+\n+\n+NEW_TASK_MODEL_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n+            The tensors corresponding to the input images. Pixel values can be obtained using\n+            [`AutoImageProcessor`]. See [`SiglipImageProcessor.__call__`] for details ([]`NewTaskModelProcessor`] uses\n+            [`SiglipImageProcessor`] for processing images).\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n+            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n+            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n+            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The NEW_TASK_MODEL model which consists of a vision backbone and a language model.\"\"\",\n+    NEW_TASK_MODEL_START_DOCSTRING,\n+)\n+class NewTaskModelForNewTask(NewTaskModelPreTrainedModel, GenerationMixin):\n+    main_input_name: ClassVar[str] = \"doc_input_ids\"  # transformers-related\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.vision_tower = AutoModel.from_config(config=config.vision_config)\n+        self.multi_modal_projector = NewTaskModelMultiModalProjector(config)\n+        self.vocab_size = config.text_config.vocab_size\n+        self._attn_implementation = config._attn_implementation\n+\n+        language_model = AutoModelForCausalLM.from_config(\n+            config=config.text_config, attn_implementation=self._attn_implementation\n+        )\n+\n+        if language_model._tied_weights_keys is not None:\n+            self._tied_weights_keys = [f\"language_model.{k}\" for k in language_model._tied_weights_keys]\n+        self.language_model = language_model\n+\n+        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n+\n+        self.embedding_dim = self.config.embedding_dim\n+        self.custom_text_proj = nn.Linear(self.config.text_config.hidden_size, self.embedding_dim)\n+\n+        if self.language_model._tied_weights_keys is not None:\n+            self._tied_weights_keys = [f\"model.language_model.{k}\" for k in self.language_model._tied_weights_keys]\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self):\n+        return self.language_model.get_output_embeddings()\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.language_model.set_output_embeddings(new_embeddings)\n+\n+    def set_decoder(self, decoder):\n+        self.language_model.set_decoder(decoder)\n+\n+    def get_decoder(self):\n+        return self.language_model.get_decoder()\n+\n+    def tie_weights(self):\n+        return self.language_model.tie_weights()\n+\n+    def _update_causal_mask(\n+        self, attention_mask, token_type_ids, inputs_embeds, past_key_values, cache_position, is_training: bool = False\n+    ):\n+        using_static_cache = isinstance(past_key_values, StaticCache)\n+        dtype = inputs_embeds.dtype\n+        min_dtype = torch.finfo(dtype).min\n+        sequence_length = inputs_embeds.shape[1]\n+        if using_static_cache:\n+            target_length = past_key_values.get_max_length()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else cache_position[0] + sequence_length + 1\n+            )\n+\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            return attention_mask\n+\n+        causal_mask = torch.full(\n+            (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+        )\n+        # Causal diagonal mask only if training, otherwise attend to the whole prefix. Training-specific attn for prefix is handled below\n+        if sequence_length != 1:\n+            if is_training:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            else:\n+                causal_mask[:, :sequence_length] = 0.0\n+\n+        causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+        causal_mask = causal_mask[None, None, :, :].expand(inputs_embeds.shape[0], 1, -1, -1)\n+        if attention_mask is not None:\n+            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+            mask_length = attention_mask.shape[-1]\n+            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(causal_mask.device)\n+            padding_mask = padding_mask == 0\n+            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                padding_mask, min_dtype\n+            )\n+            # we are training thus we need to create a full mask on the image + prefix but causal on suffix\n+            if is_training:\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    token_type_ids[:, None, None, :].to(causal_mask.device) == 0, 0\n+                )\n+        return causal_mask\n+\n+    @add_start_docstrings_to_model_forward(NEW_TASK_MODEL_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=NewTaskModelCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        num_logits_to_keep: int = 0,\n+    ) -> Union[Tuple, NewTaskModelCausalLMOutputWithPast]:\n+        r\"\"\"\n+        Args:\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.text_config.vocab_size]`.\n+\n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, NewTaskModelForNewTask\n+\n+        >>> model = NewTaskModelForNewTask.from_pretrained(\"google/NewTaskModel-test-224px-hf\")\n+        >>> processor = AutoProcessor.from_pretrained(\"google/NewTaskModel-test-224px-hf\")\n+\n+        >>> prompt = \"answer en Where is the cow standing?\"\n+        >>> url = \"https://huggingface.co/gv-hf/NewTaskModel-test-224px-hf/resolve/main/cow_beach_1.png\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(images=image, text=prompt,  return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(**inputs, max_length=30)\n+        >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"answer en Where is the cow standing?\\nbeach\"\n+        ```\n+        Returns:\n+        \"\"\"\n+        vlm_outputs = super().forward(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            token_type_ids=token_type_ids,\n+            cache_position=cache_position,\n+            inputs_embeds=inputs_embeds,\n+            labels=labels,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=True,\n+            return_dict=True,\n+            num_logits_to_keep=num_logits_to_keep,\n+        )\n+        last_hidden_states = vlm_outputs.hidden_states[-1]  # (batch_size, sequence_length, hidden_size)\n+        proj = self.custom_text_proj(last_hidden_states)  # (batch_size, sequence_length, dim)\n+\n+        # L2 normalization\n+        embeddings = proj / proj.norm(dim=-1, keepdim=True)  # (batch_size, sequence_length, dim)\n+\n+        embeddings = embeddings * attention_mask.unsqueeze(-1)  # (batch_size, sequence_length, dim)\n+\n+        return (embeddings,) + vlm_outputs\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        position_ids=None,\n+        pixel_values=None,\n+        attention_mask=None,\n+        token_type_ids=None,\n+        use_cache=True,\n+        num_logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        model_inputs = self.language_model.prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            cache_position=cache_position,\n+            use_cache=use_cache,\n+            num_logits_to_keep=num_logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n+            if model_inputs[\"inputs_embeds\"] is not None:\n+                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n+                device = model_inputs[\"inputs_embeds\"].device\n+            else:\n+                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n+                device = model_inputs[\"input_ids\"].device\n+\n+            dtype = self.get_output_embeddings().weight.dtype\n+            min_dtype = torch.finfo(dtype).min\n+\n+            model_inputs[\"attention_mask\"] = _prepare_4d_causal_attention_mask_with_cache_position(\n+                attention_mask,\n+                sequence_length=sequence_length,\n+                target_length=past_key_values.get_max_length(),\n+                dtype=dtype,\n+                device=device,\n+                min_dtype=min_dtype,\n+                cache_position=cache_position,\n+                batch_size=batch_size,\n+            )\n+\n+        model_inputs[\"token_type_ids\"] = token_type_ids\n+\n+        # position_ids in NewTaskModel are 1-indexed\n+        if model_inputs.get(\"position_ids\") is not None:\n+            model_inputs[\"position_ids\"] += 1\n+\n+        # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+        # Otherwise we need pixel values to be passed to model. NOTE: use_cache=False needs pixel_values always\n+        if cache_position[0] == 0:\n+            model_inputs[\"pixel_values\"] = pixel_values\n+\n+        return model_inputs\n+\n+    def resize_token_embeddings(\n+        self,\n+        new_num_tokens: Optional[int] = None,\n+        pad_to_multiple_of=None,\n+    ) -> nn.Embedding:\n+        model_embeds = self.language_model.resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n+\n+        # Update vocab size\n+        self.config.text_config.vocab_size = model_embeds.num_embeddings\n+        self.config.vocab_size = model_embeds.num_embeddings\n+        self.vocab_size = model_embeds.num_embeddings\n+\n+        return model_embeds"
        },
        {
            "sha": "877fba00a50ff4417a8a41f527b75258f268109f",
            "filename": "examples/modular-transformers/modular_new_task_model.py",
            "status": "added",
            "additions": 84,
            "deletions": 0,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/65442718c478aed0183155cd69decb8fc7e47f5f/examples%2Fmodular-transformers%2Fmodular_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65442718c478aed0183155cd69decb8fc7e47f5f/examples%2Fmodular-transformers%2Fmodular_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_new_task_model.py?ref=65442718c478aed0183155cd69decb8fc7e47f5f",
            "patch": "@@ -0,0 +1,84 @@\n+from typing import ClassVar, List, Optional, Union\n+\n+import torch\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from transformers.models.paligemma.modeling_paligemma import PaliGemmaForConditionalGeneration\n+\n+from ...cache_utils import Cache\n+\n+\n+class NewTaskModelForNewTask(PaliGemmaForConditionalGeneration):\n+    main_input_name: ClassVar[str] = \"doc_input_ids\"  # transformers-related\n+\n+    def __init__(self, config):\n+        super().__init__(config=config)\n+\n+        self.embedding_dim = self.config.embedding_dim\n+        self.custom_text_proj = nn.Linear(self.config.text_config.hidden_size, self.embedding_dim)\n+\n+        if self.language_model._tied_weights_keys is not None:\n+            self._tied_weights_keys = [f\"model.language_model.{k}\" for k in self.language_model._tied_weights_keys]\n+\n+        self.post_init()\n+\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        num_logits_to_keep: int = 0,\n+    ):\n+        r\"\"\"\n+        Returns:\n+        \"\"\"\n+        vlm_outputs = super().forward(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            token_type_ids=token_type_ids,\n+            cache_position=cache_position,\n+            inputs_embeds=inputs_embeds,\n+            labels=labels,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=True,\n+            return_dict=True,\n+            num_logits_to_keep=num_logits_to_keep,\n+        )\n+        last_hidden_states = vlm_outputs.hidden_states[-1]  # (batch_size, sequence_length, hidden_size)\n+        proj = self.custom_text_proj(last_hidden_states)  # (batch_size, sequence_length, dim)\n+\n+        # L2 normalization\n+        embeddings = proj / proj.norm(dim=-1, keepdim=True)  # (batch_size, sequence_length, dim)\n+\n+        embeddings = embeddings * attention_mask.unsqueeze(-1)  # (batch_size, sequence_length, dim)\n+\n+        return (embeddings,) + vlm_outputs\n+\n+    def resize_token_embeddings(\n+        self,\n+        new_num_tokens: Optional[int] = None,\n+        pad_to_multiple_of=None,\n+    ) -> nn.Embedding:\n+        model_embeds = self.language_model.resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n+\n+        # Update vocab size\n+        self.config.text_config.vocab_size = model_embeds.num_embeddings\n+        self.config.vocab_size = model_embeds.num_embeddings\n+        self.vocab_size = model_embeds.num_embeddings\n+\n+        return model_embeds"
        },
        {
            "sha": "c107a4831862319cc6ab9c41993b8f79a65e13a6",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 66,
            "deletions": 11,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/65442718c478aed0183155cd69decb8fc7e47f5f/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65442718c478aed0183155cd69decb8fc7e47f5f/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=65442718c478aed0183155cd69decb8fc7e47f5f",
            "patch": "@@ -204,7 +204,15 @@ class ReplaceNameTransformer(m.MatcherDecoratableTransformer):\n         - LLaMa -> MyNewModel       abd     MyNewModel      -> Llama\n     \"\"\"\n \n-    def __init__(self, old_name, new_name, given_old_name=None, given_new_name=None):\n+    def __init__(\n+        self,\n+        old_name,\n+        new_name,\n+        given_old_name=None,\n+        given_new_name=None,\n+        old_class_name: str = None,\n+        new_class_name: str = None,\n+    ):\n         super().__init__()\n         self.old_name = old_name\n         self.new_name = new_name\n@@ -220,6 +228,18 @@ def __init__(self, old_name, new_name, given_old_name=None, given_new_name=None)\n         }\n         if given_old_name is not None and given_new_name is not None and given_old_name not in self.patterns:\n             self.patterns[given_old_name] = given_new_name\n+        if self.old_name in CONFIG_MAPPING_NAMES:\n+            self.default_old_name = CONFIG_MAPPING_NAMES[self.old_name].replace(\"Config\", \"\")\n+            if self.default_old_name.isupper():\n+                self.default_old_name = self.default_old_name.capitalize()\n+        if new_class_name is not None and old_class_name is not None and old_class_name not in self.patterns:\n+            # In last recourse, when the suffix of the new class is not the same as the old class,\n+            # and if the old and new classes start with the default name, we keep the default class name\n+            # and replace the old suffix with the new one.\n+            # Useful when we have a class like `ColPaliForRetrieval` inheriting from `PaliGemmaForConditionalGeneration`\n+            # where a model extends another model, but is used for a different task.\n+            if old_class_name.startswith(self.default_old_name) and new_class_name.startswith(self.default_name):\n+                self.patterns[old_class_name[len(self.default_old_name) :]] = new_class_name[len(self.default_name) :]\n \n     def preserve_case_replace(self, text):\n         # Create a regex pattern to match all variations\n@@ -235,7 +255,9 @@ def replace(match):\n \n     def convert_to_camelcase(self, text):\n         # Regex pattern to match consecutive uppercase letters and lowercase the first set\n-        result = re.sub(r\"^[A-Z]+(?=[A-Z][a-z])\", lambda m: m.group(0).capitalize(), text, count=1)\n+        result = re.sub(\n+            rf\"^({self.old_name})(?=[a-z]+)\", lambda m: self.default_old_name, text, flags=re.IGNORECASE, count=1\n+        )\n         return result\n \n     @m.leave(m.Name() | m.SimpleString() | m.Comment())\n@@ -249,9 +271,24 @@ def leave_ClassDef(self, original_node, updated_node):\n         return updated_node.with_changes(name=cst.Name(self.convert_to_camelcase(updated_node.name.value)))\n \n \n-def find_classes_in_file(module: cst.Module, old_id=\"llama\", new_id=\"gemma\", given_old_name=None, given_new_name=None):\n+def find_classes_in_file(\n+    module: cst.Module,\n+    old_id=\"llama\",\n+    new_id=\"gemma\",\n+    given_old_name=None,\n+    given_new_name=None,\n+    old_class_name=None,\n+    new_class_name=None,\n+):\n     \"\"\"Helper function to rename and then parse a source file using the ClassFinder\"\"\"\n-    transformer = ReplaceNameTransformer(old_id, new_id, given_old_name, given_new_name)\n+    transformer = ReplaceNameTransformer(\n+        old_id,\n+        new_id,\n+        given_old_name=given_old_name,\n+        given_new_name=given_new_name,\n+        old_class_name=old_class_name,\n+        new_class_name=new_class_name,\n+    )\n     new_module = module.visit(transformer)\n \n     wrapper = MetadataWrapper(new_module)\n@@ -868,7 +905,7 @@ def leave_ClassDef(self, original_node, updated_node):\n                     dep: class_finder.class_start_line.get(dep, 1000)\n                     for dep in class_finder.class_dependency_mapping.get(class_name, [])\n                 }\n-            if list_dependencies == []:\n+            if len(list_dependencies) == 0:\n                 # so, maybe standard renaming did not work (the class name is different)\n                 # we try with another renaming pattern\n                 potential_given_name = get_new_part(class_name, super_class)\n@@ -884,6 +921,30 @@ def leave_ClassDef(self, original_node, updated_node):\n                     dep: class_finder.class_start_line.get(dep, 1000)\n                     for dep in class_finder.class_dependency_mapping.get(class_name, [])\n                 }\n+            if len(list_dependencies) == 0:\n+                # last recourse, if the suffix of the new class is different from the one of the super class\n+                # e.g. MyNewClassForSegmentation extends MyOldClassForObjectDetection\n+                # we try with another renaming pattern\n+                class_finder = find_classes_in_file(\n+                    self.transformers_imports[super_file_name],\n+                    model_name,\n+                    self.model_name,\n+                    self.given_old_name,\n+                    self.given_new_name,\n+                    super_class,\n+                    class_name,\n+                )\n+                visited_module[super_file_name] = class_finder\n+                list_dependencies = {\n+                    dep: class_finder.class_start_line.get(dep, 1000)\n+                    for dep in class_finder.class_dependency_mapping.get(class_name, [])\n+                }\n+            if len(list_dependencies) == 0:\n+                raise ValueError(\n+                    f\"We were unable to find dependencies for {class_name} (based on inheriting from {super_class})\"\n+                    f\"   Here are all the global dependencies that we found in you modular file: {list(class_finder.class_dependency_mapping.keys())}.\"\n+                    f\"   This usually means that the name of `{class_name}` does not match the pattern of `{super_class}`\"\n+                )\n \n             list_dependencies = sorted(list_dependencies.items(), key=lambda x: x[1], reverse=True)\n             start_insert_idx = self.global_scope_index\n@@ -917,12 +978,6 @@ def leave_ClassDef(self, original_node, updated_node):\n \n             if len(list_dependencies) > 0:\n                 updated_node = replace_call_to_super(class_finder, updated_node, class_name, all_bases)\n-            else:\n-                raise ValueError(\n-                    f\"We were unable to find dependencies for {class_name} (based on inheriting from {super_class})\"\n-                    f\"   Here are all the global dependencies that we found in you modular file: {list(class_finder.class_dependency_mapping.keys())}.\"\n-                    f\"   This usually means that the name of `{class_name}` does not match the pattern of `{super_class}`\"\n-                )\n \n         # Now, if a class was defined without parents, we look for the name\n         match_pattern = \"|\".join(TYPE_TO_FILE_TYPE.keys())"
        }
    ],
    "stats": {
        "total": 707,
        "additions": 696,
        "deletions": 11
    }
}