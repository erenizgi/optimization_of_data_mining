{
    "author": "SohamPrabhu",
    "message": "Update roc bert docs (#38835)\n\n* Moved the sources to the right\n\n* small Changes\n\n* Some Changes to moonshine\n\n* Added the install to pipline\n\n* updated the monshine model card\n\n* Update docs/source/en/model_doc/moonshine.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/moonshine.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/moonshine.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/moonshine.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Updated Documentation According to changes\n\n* Fixed the model with the commits\n\n* Changes to the roc_bert\n\n* Final Update to the branch\n\n* Adds Quantizaiton to the model\n\n* Finsihed Fixing the Roc_bert docs\n\n* Fixed Moshi\n\n* Fixed Problems\n\n* Fixed Problems\n\n* Fixed Problems\n\n* Fixed Problems\n\n* Fixed Problems\n\n* Fixed Problems\n\n* Added the install to pipline\n\n* updated the monshine model card\n\n* Update docs/source/en/model_doc/moonshine.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/moonshine.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/moonshine.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Updated Documentation According to changes\n\n* Fixed the model with the commits\n\n* Fixed the problems\n\n* Final Fix\n\n* Final Fix\n\n* Final Fix\n\n* Update roc_bert.md\n\n---------\n\nCo-authored-by: Your Name <sohamprabhu@Mac.fios-router.home>\nCo-authored-by: Your Name <sohamprabhu@Sohams-MacBook-Air.local>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "a396f4324bec03e0329bcd67a58648b95d557599",
    "files": [
        {
            "sha": "90373085a1339ea83a98c1925a5369ad80ecd45b",
            "filename": "docs/source/en/model_doc/roc_bert.md",
            "status": "modified",
            "additions": 63,
            "deletions": 24,
            "changes": 87,
            "blob_url": "https://github.com/huggingface/transformers/blob/a396f4324bec03e0329bcd67a58648b95d557599/docs%2Fsource%2Fen%2Fmodel_doc%2Froc_bert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a396f4324bec03e0329bcd67a58648b95d557599/docs%2Fsource%2Fen%2Fmodel_doc%2Froc_bert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Froc_bert.md?ref=a396f4324bec03e0329bcd67a58648b95d557599",
            "patch": "@@ -14,39 +14,78 @@ rendered properly in your Markdown viewer.\n \n -->\n \n+<div style=\"float: right;\">\n+   <div class=\"flex flex-wrap space-x-1\">\n+          <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+   </div>\n+</div>\n+\n # RoCBert\n \n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n+[RoCBert](https://aclanthology.org/2022.acl-long.65.pdf) is a pretrained Chinese [BERT](./bert) model designed against adversarial attacks like typos and synonyms. It is pretrained with a contrastive learning objective to align normal and adversarial text examples. The examples include different semantic, phonetic, and visual features of Chinese. This makes RoCBert more robust against manipulation.\n+\n+You can find all the original RoCBert checkpoints under the [weiweishi](https://huggingface.co/weiweishi) profile.\n+\n+> [!TIP]\n+> This model was contributed by [weiweishi](https://huggingface.co/weiweishi).\n+> \n+> Click on the RoCBert models in the right sidebar for more examples of how to apply RoCBert to different Chinese language tasks.\n+\n+The example below demonstrates how to predict the [MASK] token with [`Pipeline`], [`AutoModel`], and from the command line.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+import torch\n+from transformers import pipeline\n+\n+pipeline = pipeline(\n+   task=\"fill-mask\",\n+   model=\"weiweishi/roc-bert-base-zh\",\n+   torch_dtype=torch.float16,\n+   device=0\n+)\n+pipeline(\"這家餐廳的拉麵是我[MASK]過的最好的拉麵之\")\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import AutoModelForMaskedLM, AutoTokenizer\n \n-## Overview\n+tokenizer = AutoTokenizer.from_pretrained(\n+   \"weiweishi/roc-bert-base-zh\",\n+)\n+model = AutoModelForMaskedLM.from_pretrained(\n+   \"weiweishi/roc-bert-base-zh\",\n+   torch_dtype=torch.float16,\n+   device_map=\"auto\",\n+)\n+inputs = tokenizer(\"這家餐廳的拉麵是我[MASK]過的最好的拉麵之\", return_tensors=\"pt\").to(\"cuda\")\n \n-The RoCBert model was proposed in [RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining](https://aclanthology.org/2022.acl-long.65.pdf)  by HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou.\n-It's a pretrained Chinese language model that is robust under various forms of adversarial attacks.\n+with torch.no_grad():\n+   outputs = model(**inputs)\n+   predictions = outputs.logits\n \n-The abstract from the paper is the following:\n+masked_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n+predicted_token_id = predictions[0, masked_index].argmax(dim=-1)\n+predicted_token = tokenizer.decode(predicted_token_id)\n \n-*Large-scale pretrained language models have achieved SOTA results on NLP tasks. However, they have been shown\n-vulnerable to adversarial attacks especially for logographic languages like Chinese. In this work, we propose\n-ROCBERT: a pretrained Chinese Bert that is robust to various forms of adversarial attacks like word perturbation,\n-synonyms, typos, etc. It is pretrained with the contrastive learning objective which maximizes the label consistency\n-under different synthesized adversarial examples. The model takes as input multimodal information including the\n-semantic, phonetic and visual features. We show all these features are important to the model robustness since the\n-attack can be performed in all the three forms. Across 5 Chinese NLU tasks, ROCBERT outperforms strong baselines under\n-three blackbox adversarial algorithms without sacrificing the performance on clean testset. It also performs the best\n-in the toxic content detection task under human-made attacks.*\n+print(f\"The predicted token is: {predicted_token}\")\n+```\n \n-This model was contributed by [weiweishi](https://huggingface.co/weiweishi).\n+</hfoption>\n+<hfoption id=\"transformers CLI\">\n \n-## Resources\n+```bash\n+echo -e \"這家餐廳的拉麵是我[MASK]過的最好的拉麵之\" | transformers-cli run --task fill-mask --model weiweishi/roc-bert-base-zh --device 0\n+```\n \n-- [Text classification task guide](../tasks/sequence_classification)\n-- [Token classification task guide](../tasks/token_classification)\n-- [Question answering task guide](../tasks/question_answering)\n-- [Causal language modeling task guide](../tasks/language_modeling)\n-- [Masked language modeling task guide](../tasks/masked_language_modeling)\n-- [Multiple choice task guide](../tasks/multiple_choice)\n+</hfoption>\n+</hfoptions>\n \n ## RoCBertConfig\n "
        }
    ],
    "stats": {
        "total": 87,
        "additions": 63,
        "deletions": 24
    }
}