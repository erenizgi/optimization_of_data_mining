{
    "author": "htahboub",
    "message": "Minor error condition bug fix (#33781)\n\n* Error condition bug fix\r\n\r\n* Update error message\r\n\r\n* Update src/transformers/models/qwen2_vl/modeling_qwen2_vl.py\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* Making change in the rest of the repo\r\n\r\n* Formatting\r\n\r\n* Formatting with ruff\r\n\r\n---------\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
    "sha": "2bd4d5897dc73e8b172832070a6f9e567a0df017",
    "files": [
        {
            "sha": "c57d41785f6dd157b6cf1d5b88c186249df518fb",
            "filename": "examples/modular-transformers/modeling_dummy.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -882,7 +882,7 @@ def forward(\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n+                \"You must specify exactly one of input_ids or inputs_embeds\"\n             )\n \n         if self.gradient_checkpointing and self.training and use_cache:"
        },
        {
            "sha": "a1fabf5d8c511afe1ebc09df21b42232bf9fbf58",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -759,7 +759,7 @@ def forward(\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n+                \"You must specify exactly one of input_ids or inputs_embeds\"\n             )\n \n         if self.gradient_checkpointing and self.training and use_cache:"
        },
        {
            "sha": "6eb2e88fc6999de2b2fdf56abf460d6a02f21400",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -620,9 +620,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training and use_cache:\n             logger.warning_once("
        },
        {
            "sha": "2d29b2c2402f9e323beb05d692be07bd63ce45d5",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -1278,9 +1278,7 @@ def forward(\n             use_cache = False\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if pixel_values is not None and inputs_embeds is not None:\n             raise ValueError("
        },
        {
            "sha": "ecdf8192c88235ecbbbb8cf83f21218e319ee199",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -459,9 +459,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training:\n             if use_cache:"
        },
        {
            "sha": "cfce053eae58891006a7aaced4a3462afeb36edf",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -842,9 +842,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training and use_cache:\n             logger.warning_once("
        },
        {
            "sha": "df8659e18f51f8c1dab28de8b6d0378ed388d9de",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -991,9 +991,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training and use_cache:\n             logger.warning_once("
        },
        {
            "sha": "03721d15bafa49dc1a3d219d47512a3192ff41ca",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -964,9 +964,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training:\n             if use_cache:"
        },
        {
            "sha": "80ae18b907a6b2160da84f0f36505accc9bff56f",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -650,9 +650,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):  # ^ is python for xor\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embeddings(input_ids)"
        },
        {
            "sha": "a4964b40af00a398facd23a55c4ba71fc0ef1f03",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -754,9 +754,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training and use_cache:\n             logger.warning_once("
        },
        {
            "sha": "8d7655a52dc03ac33e6a5870531438e98e81966e",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -667,9 +667,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training and use_cache:\n             logger.warning_once("
        },
        {
            "sha": "9f78db02dfe6d2b836ae0ba58573cfcd414a6e31",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -768,9 +768,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training and use_cache:\n             logger.warning_once("
        },
        {
            "sha": "518de264aeb94aaad4898062dda9f3683842527c",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -605,9 +605,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training and use_cache:\n             logger.warning_once("
        },
        {
            "sha": "6fe0655a956b68e92d8dea451834d2c9c0867c70",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -674,9 +674,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training:\n             if use_cache:"
        },
        {
            "sha": "1436d469de9b7f978c0b23084ad47231d437d6ef",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -876,9 +876,7 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training:\n             if use_cache:"
        },
        {
            "sha": "0fc96af8376a445090b9605a3ff0d606cbce7f61",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -603,9 +603,7 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_in(input_ids)"
        },
        {
            "sha": "a2f37662a7e4769d5361cf12bd9fd56b62f7b374",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -746,9 +746,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training:\n             if use_cache:"
        },
        {
            "sha": "0eb27d452f08d2f256ac89e852666f083a7c56a8",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -766,9 +766,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training and use_cache:\n             logger.warning_once("
        },
        {
            "sha": "b33af0bfca395187ef72cae491db4c7604bcdee0",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -997,9 +997,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training and use_cache:\n             logger.warning_once("
        },
        {
            "sha": "81a5b8fe85fd77590b8278d2c0b0b3bf756e521b",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -1172,9 +1172,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training and use_cache:\n             logger.warning_once("
        },
        {
            "sha": "877146c5cf30c781016d36bd6c663d6ebaf3bb9c",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -1282,9 +1282,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training and use_cache:\n             logger.warning_once("
        },
        {
            "sha": "ebb0fc30dbdad84ec14b6574479bb59bd27ea9aa",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -972,9 +972,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training and use_cache:\n             logger.warning_once("
        },
        {
            "sha": "640521365f4df2fd27b6d6216f20f4666bd122d6",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -878,9 +878,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training and use_cache:\n             logger.warning_once("
        },
        {
            "sha": "0bc08f9f86864fe5195fba9503fb25665361e3e8",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -444,9 +444,7 @@ def forward(\n         )\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if pixel_values is not None and inputs_embeds is not None:\n             raise ValueError("
        },
        {
            "sha": "b9d20a47e61ec2a9924c462b73f264e32cbf9881",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -786,9 +786,7 @@ def forward(\n         )\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if pixel_values is not None and inputs_embeds is not None:\n             raise ValueError("
        },
        {
            "sha": "74d0145e604bb8bd5f3dc74f10ad1cb70cbd8e6b",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -875,9 +875,7 @@ def forward(\n         )\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if (pixel_values is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n             raise ValueError("
        },
        {
            "sha": "39c55930a8d574424b0c446a21cee09a85b5954b",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -380,9 +380,7 @@ def forward(\n         )\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if (pixel_values is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n             raise ValueError("
        },
        {
            "sha": "4443ae68fa64ed9b274bb0e30e443d4d5ccda668",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -572,9 +572,7 @@ def forward(\n         )\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if (pixel_values is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n             raise ValueError("
        },
        {
            "sha": "45ea55cc4950eb7f590e7958f5f84c14260b06b9",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -590,9 +590,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):  # ^ is python for xor\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embeddings(input_ids)"
        },
        {
            "sha": "fb4bfca7357236f1c911f7fd1dbe3c3b448e4af1",
            "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -862,9 +862,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):  # ^ is python for xor\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embeddings(input_ids)"
        },
        {
            "sha": "68f0bf88c1d9b79d5fb8ae7c7fe2c9172e9fefcf",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -749,9 +749,7 @@ def forward(\n \n         # retrieve input_ids and inputs_embeds\n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training and use_cache:\n             logger.warning_once("
        },
        {
            "sha": "08466a5567ac5d8c0a626227ec1230250feab7ee",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -959,9 +959,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training:\n             if use_cache:"
        },
        {
            "sha": "3d367d4daac5c06d99d192185f54c0f048add15e",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -1606,9 +1606,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training and use_cache:\n             logger.warning_once(\n@@ -2145,9 +2143,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if pixel_values is not None and inputs_embeds is not None:\n             raise ValueError("
        },
        {
            "sha": "51aeaa19472e1d5b7c03b005f593d5a8e2951f3c",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -771,9 +771,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training and use_cache:\n             logger.warning_once("
        },
        {
            "sha": "169689f9add3d2094a309705614d1caf03e2bec6",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -799,9 +799,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training and use_cache:\n             logger.warning_once("
        },
        {
            "sha": "c83811a2e719ff2c38a974179a11fa08ede26bf2",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -947,9 +947,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training and use_cache:\n             logger.warning_once("
        },
        {
            "sha": "560edca663d2154f20de56c28edb096018d44d8d",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -453,9 +453,7 @@ def forward(\n         ```\"\"\"\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if pixel_values is not None and inputs_embeds is not None:\n             raise ValueError("
        },
        {
            "sha": "e1398e58fd0e70e83a46738498037e8c9fcdca69",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -621,9 +621,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training:\n             if use_cache:"
        },
        {
            "sha": "1f6eaacee99739bf170fde5f0385b0d5ae3cf6b9",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -911,9 +911,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training:\n             if use_cache:"
        },
        {
            "sha": "f478fa4d9c6a87f56b953361dcb6a6ece240191d",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -938,9 +938,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training:\n             if use_cache:"
        },
        {
            "sha": "22523f6411c50a545f66b07eb2070813861b46ef",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -851,9 +851,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training:\n             if use_cache:"
        },
        {
            "sha": "9173cd7d6b08dc59ef088806a881b2db3c21a497",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -1021,9 +1021,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training:\n             if use_cache:"
        },
        {
            "sha": "9ca33395e923a7068f98ef561c2a23da3a5a1d42",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -1126,9 +1126,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training:\n             if use_cache:"
        },
        {
            "sha": "8ac9df3f6b6f8aa38bd6d749d68c3acbdab129a6",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -695,9 +695,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training and use_cache:\n             logger.warning_once("
        },
        {
            "sha": "6e337b54bfb3dee80f7a41d71845979ff6022d0c",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -896,9 +896,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training:\n             if use_cache:"
        },
        {
            "sha": "9131d454280a4a10a83da7e7052c2e83104d4558",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -824,9 +824,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training:\n             if use_cache:"
        },
        {
            "sha": "008240d0d929e66ca06939feb89a20ffabc5ee38",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -516,9 +516,7 @@ def forward(\n         )\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if (pixel_values_images is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n             raise ValueError("
        },
        {
            "sha": "9a2bb8fb0dacfebf2f812c48f16049476292e273",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bd4d5897dc73e8b172832070a6f9e567a0df017/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=2bd4d5897dc73e8b172832070a6f9e567a0df017",
            "patch": "@@ -441,9 +441,7 @@ def forward(\n         )\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if pixel_values is not None and inputs_embeds is not None:\n             raise ValueError("
        }
    ],
    "stats": {
        "total": 192,
        "additions": 49,
        "deletions": 143
    }
}