{
    "author": "Cyrilvallez",
    "message": "Raise error instead of warning when using meta device in from_pretrained (#40942)\n\n* raise instead of warning\n\n* add timm\n\n* remove",
    "sha": "eb04363a0dd52692913f2f026031db3902e2587c",
    "files": [
        {
            "sha": "12c3e7cd99efc84f16a33a6abe948bdecb57e3ac",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb04363a0dd52692913f2f026031db3902e2587c/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb04363a0dd52692913f2f026031db3902e2587c/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=eb04363a0dd52692913f2f026031db3902e2587c",
            "patch": "@@ -4910,11 +4910,10 @@ def from_pretrained(\n         if device_map is None and not is_deepspeed_zero3_enabled():\n             device_in_context = get_torch_context_manager_or_global_device()\n             if device_in_context == torch.device(\"meta\"):\n-                # TODO Cyril: raise an error instead of the warning in v4.53 (and change the test to check for raise instead of success)\n-                logger.warning(\n-                    \"We detected that you are using `from_pretrained` with a meta device context manager or `torch.set_default_device('meta')`\\n\"\n-                    \"This is an anti-pattern and will raise an Error in version v4.53\\nIf you want to initialize a model on the meta device, use \"\n-                    \"the context manager or global device with `from_config`, or `ModelClass(config)`\"\n+                raise RuntimeError(\n+                    \"You are using `from_pretrained` with a meta device context manager or `torch.set_default_device('meta')`.\\n\"\n+                    \"This is an anti-pattern as `from_pretrained` wants to load existing weights.\\nIf you want to initialize an \"\n+                    \"empty model on the meta device, use the context manager or global device with `from_config`, or `ModelClass(config)`\"\n                 )\n             device_map = device_in_context\n "
        },
        {
            "sha": "0c927b82d12b522cc46bbcc5bea7a4da7f4a31e3",
            "filename": "tests/models/perception_lm/test_modeling_perception_lm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb04363a0dd52692913f2f026031db3902e2587c/tests%2Fmodels%2Fperception_lm%2Ftest_modeling_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb04363a0dd52692913f2f026031db3902e2587c/tests%2Fmodels%2Fperception_lm%2Ftest_modeling_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fperception_lm%2Ftest_modeling_perception_lm.py?ref=eb04363a0dd52692913f2f026031db3902e2587c",
            "patch": "@@ -313,10 +313,6 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n     def test_can_be_initialized_on_meta(self):\n         pass\n \n-    @unittest.skip(\"ViT PE / TimmWrapperModel cannot be tested with meta device\")\n-    def test_can_load_with_meta_device_context_manager(self):\n-        pass\n-\n     @unittest.skip(\"Specifying both inputs_embeds and pixel_values are not supported for PerceptionLM\")\n     def test_generate_from_inputs_embeds_0_greedy(self):\n         pass"
        },
        {
            "sha": "d8fc0d53a4cdaf77fda0ca34b72c6b968ce7faee",
            "filename": "tests/models/timm_backbone/test_modeling_timm_backbone.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb04363a0dd52692913f2f026031db3902e2587c/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb04363a0dd52692913f2f026031db3902e2587c/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py?ref=eb04363a0dd52692913f2f026031db3902e2587c",
            "patch": "@@ -169,7 +169,7 @@ def test_can_load_with_global_device_set(self):\n         pass\n \n     @unittest.skip(reason=\"TimmBackbone uses its own `from_pretrained` without device_map support\")\n-    def test_can_load_with_meta_device_context_manager(self):\n+    def test_cannot_load_with_meta_device_context_manager(self):\n         pass\n \n     @unittest.skip(reason=\"model weights aren't tied in TimmBackbone.\")"
        },
        {
            "sha": "a5df6cfeb3102b18c3f835f706dd0c86a537adeb",
            "filename": "tests/models/xcodec/test_modeling_xcodec.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb04363a0dd52692913f2f026031db3902e2587c/tests%2Fmodels%2Fxcodec%2Ftest_modeling_xcodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb04363a0dd52692913f2f026031db3902e2587c/tests%2Fmodels%2Fxcodec%2Ftest_modeling_xcodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxcodec%2Ftest_modeling_xcodec.py?ref=eb04363a0dd52692913f2f026031db3902e2587c",
            "patch": "@@ -151,10 +151,6 @@ def test_gradient_checkpointing_backward_compatibility(self):\n             model = model_class(config)\n             self.assertTrue(model.is_gradient_checkpointing)\n \n-    @unittest.skip(\"XcodecModel cannot be tested with meta device\")\n-    def test_can_load_with_meta_device_context_manager(self):\n-        pass\n-\n     @unittest.skip(reason=\"We cannot configure to output a smaller model.\")\n     def test_model_is_small(self):\n         pass"
        },
        {
            "sha": "188c7517d54c05497b7d758370787be4dc344580",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 5,
            "deletions": 12,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb04363a0dd52692913f2f026031db3902e2587c/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb04363a0dd52692913f2f026031db3902e2587c/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=eb04363a0dd52692913f2f026031db3902e2587c",
            "patch": "@@ -4488,7 +4488,7 @@ def test_can_load_with_global_device_set(self):\n                 unique_devices, {device}, f\"All parameters should be on {device}, but found {unique_devices}.\"\n             )\n \n-    def test_can_load_with_meta_device_context_manager(self):\n+    def test_cannot_load_with_meta_device_context_manager(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             # Need to deepcopy here as it is modified in-place in save_pretrained (it sets sdpa for default attn, which\n@@ -4497,18 +4497,11 @@ def test_can_load_with_meta_device_context_manager(self):\n \n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n-\n                 with torch.device(\"meta\"):\n-                    new_model = model_class.from_pretrained(tmpdirname)\n-                unique_devices = {param.device for param in new_model.parameters()} | {\n-                    buffer.device for buffer in new_model.buffers()\n-                }\n-\n-            self.assertEqual(\n-                unique_devices,\n-                {torch.device(\"meta\")},\n-                f\"All parameters should be on meta device, but found {unique_devices}.\",\n-            )\n+                    with self.assertRaisesRegex(\n+                        RuntimeError, \"You are using `from_pretrained` with a meta device context manager\"\n+                    ):\n+                        _ = model_class.from_pretrained(tmpdirname)\n \n     def test_config_attn_implementation_setter(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()"
        }
    ],
    "stats": {
        "total": 36,
        "additions": 10,
        "deletions": 26
    }
}