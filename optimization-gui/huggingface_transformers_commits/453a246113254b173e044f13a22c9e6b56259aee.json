{
    "author": "ppadjinTT",
    "message": "avoiding conditional indexing in positionalencoding to avoid possibilâ€¦ (#42090)\n\navoiding conditional indexing in positionalencoding to avoid possibility of empty tensors",
    "sha": "453a246113254b173e044f13a22c9e6b56259aee",
    "files": [
        {
            "sha": "919b22ba84f58821f66d5aaeba5ad550e645847c",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/453a246113254b173e044f13a22c9e6b56259aee/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/453a246113254b173e044f13a22c9e6b56259aee/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=453a246113254b173e044f13a22c9e6b56259aee",
            "patch": "@@ -434,8 +434,8 @@ def forward(self, hidden_states):\n         pos_seq = torch.arange(0, seq_len).to(device=hidden_states.device, dtype=torch.long)\n         pos_seq = pos_seq[:, None] - pos_seq[None, :]\n \n-        pos_seq[pos_seq < -self.max_length] = -self.max_length\n-        pos_seq[pos_seq >= self.max_length] = self.max_length - 1\n+        pos_seq = torch.where(pos_seq < -self.max_length, -self.max_length, pos_seq)\n+        pos_seq = torch.where(pos_seq >= self.max_length, self.max_length - 1, pos_seq)\n         pos_seq = pos_seq + self.max_length\n \n         return self.pe_k(pos_seq)"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}