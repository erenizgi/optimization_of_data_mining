{
    "author": "njeffrie",
    "message": "Support batching for UsefulSensors Moonshine (#35922)\n\n* Add support for attention masking in moonshine.\r\n\r\nTested against Open ASR Leaderboard with batch size 256.\r\n\r\n* Update comments and ensure attention masks are passed everywhere.\r\n\r\nPerform attention mask downsampling inside of moonshine forward call.\r\n\r\n* Hide padding behind conditional. Fix encoder/decoder masking.\r\n\r\n- Correctly pipe encoder attention mask into decoder\r\n- Add correct scaling factor if one is not already provided.\r\n- Fix formatting with ruff\r\n\r\n* Add auto generated modeling_moonshine file.\r\n\r\n* Update formatting in generated model file.\r\n\r\n* Address review comments.\r\n\r\n* Fix typo.\r\n\r\n* Add `pad_head_dim_to_multiple_of` to moonshine config.\r\n\r\n* Correct args order for MooonshineConfig.\r\n\r\n* Update configuration moonshine too.\r\n\r\n* Update src/transformers/models/moonshine/modular_moonshine.py\r\n\r\n* Update src/transformers/models/moonshine/configuration_moonshine.py\r\n\r\n---------\r\n\r\nCo-authored-by: eustlb <94853470+eustlb@users.noreply.github.com>",
    "sha": "693328f2bc66b0ef20950c43a46d440cbad50265",
    "files": [
        {
            "sha": "0ea6f149e4306f304663c15bcc61774a64de7dc8",
            "filename": "src/transformers/models/moonshine/configuration_moonshine.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/693328f2bc66b0ef20950c43a46d440cbad50265/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconfiguration_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/693328f2bc66b0ef20950c43a46d440cbad50265/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconfiguration_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconfiguration_moonshine.py?ref=693328f2bc66b0ef20950c43a46d440cbad50265",
            "patch": "@@ -64,6 +64,9 @@ class MoonshineConfig(PretrainedConfig):\n             by meanpooling all the original heads within that group. For more details checkout [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `decoder_num_attention_heads`.\n+        pad_head_dim_to_multiple_of (`int`, *optional*):\n+            Pad head dimension in encoder and decoder to the next multiple of this value. Necessary for using certain\n+            optimized attention implementations.\n         encoder_hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n             The non-linear activation function (function or string) in the encoder.\n         decoder_hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n@@ -164,6 +167,7 @@ def __init__(\n         decoder_num_attention_heads=8,\n         encoder_num_key_value_heads=None,\n         decoder_num_key_value_heads=None,\n+        pad_head_dim_to_multiple_of=None,\n         encoder_hidden_act=\"gelu\",\n         decoder_hidden_act=\"silu\",\n         max_position_embeddings=512,\n@@ -196,6 +200,8 @@ def __init__(\n             decoder_num_key_value_heads = decoder_num_attention_heads\n         self.decoder_num_key_value_heads = decoder_num_key_value_heads\n \n+        self.pad_head_dim_to_multiple_of = pad_head_dim_to_multiple_of\n+\n         self.encoder_hidden_act = encoder_hidden_act\n         self.decoder_hidden_act = decoder_hidden_act\n         self.max_position_embeddings = max_position_embeddings"
        },
        {
            "sha": "96285262514b86e647e1ba4478631a00879ed761",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 85,
            "deletions": 8,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/693328f2bc66b0ef20950c43a46d440cbad50265/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/693328f2bc66b0ef20950c43a46d440cbad50265/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=693328f2bc66b0ef20950c43a46d440cbad50265",
            "patch": "@@ -18,6 +18,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import math\n from typing import Callable, Optional, Tuple, Union\n \n import numpy as np\n@@ -27,7 +28,11 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_attn_mask_utils import (\n+    AttentionMaskConverter,\n+    _prepare_4d_attention_mask,\n+    _prepare_4d_attention_mask_for_sdpa,\n+)\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -270,6 +275,23 @@ def forward(\n                 attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         is_causal = True if self.is_causal and attention_mask is None and q_len > 1 else False\n+\n+        # Pad head size dimension to next specified multiple. Q K and V always have equal head sizes.\n+        head_dim_padding = 0\n+        if self.config.pad_head_dim_to_multiple_of is not None:\n+            head_dim = query_states.shape[-1]\n+            target_multiple = self.config.pad_head_dim_to_multiple_of\n+            target_head_dim = target_multiple * ((head_dim + target_multiple - 1) // target_multiple)\n+            head_dim_padding = target_head_dim - head_dim\n+            if head_dim_padding > 0:\n+                # Ensure scaling is correct even with padding.\n+                if self.scaling is None:\n+                    self.scaling = 1.0 / math.sqrt(query_states.shape[-1])\n+\n+                query_states = torch.nn.functional.pad(query_states, (0, head_dim_padding))\n+                key_states = torch.nn.functional.pad(key_states, (0, head_dim_padding))\n+                value_states = torch.nn.functional.pad(value_states, (0, head_dim_padding))\n+\n         attn_output, attn_weights = attention_interface(\n             self,\n             query_states,\n@@ -282,6 +304,10 @@ def forward(\n             **kwargs,\n         )\n \n+        # Remove head size padding.\n+        if head_dim_padding > 0:\n+            attn_output = attn_output[:, :, :, :-head_dim_padding]\n+\n         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n         return attn_output, attn_weights\n@@ -603,9 +629,11 @@ def forward(\n                 `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n                 `input_values`, the [`AutoFeatureExtractor`] should be used for padding\n                 and conversion into a tensor of type `torch.FloatTensor`.\n-            attention_mask (`torch.Tensor`)`, *optional*):\n-                Moonshine does not support masking of the `input_values`, this argument is preserved for compatibility,\n-                but it is not used.\n+            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Mask to avoid performing attention on padding indices in `input_values`. Mask values selected in `[0, 1]`:\n+                - 1 for tokens that are **not masked**,\n+                - 0 for tokens that are **masked**.\n+                [What are attention masks?](../glossary#attention-mask)\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n                 tensors for more detail.\n@@ -632,6 +660,22 @@ def forward(\n         hidden_states = nn.functional.gelu(self.conv3(hidden_states))\n         hidden_states = hidden_states.permute(0, 2, 1)\n \n+        # attention mask downsampling\n+        if attention_mask is not None:\n+            mask_len = self._get_feat_extract_output_lengths(attention_mask.shape[-1])\n+            downsample_stride = 64 * 3 * 2  # conv strides\n+            attention_mask = attention_mask[..., ::downsample_stride][..., :mask_len]\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if (attention_mask == 0.0).any() else None\n+\n+            # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+            elif self.config._attn_implementation == \"sdpa\" and not output_attentions:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, hidden_states.dtype)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n+\n         position_ids = torch.arange(0, hidden_states.shape[1], device=hidden_states.device).unsqueeze(0)\n \n         # create position embeddings to be shared across the decoder layers\n@@ -649,7 +693,7 @@ def forward(\n                 layer_outputs = self._gradient_checkpointing_func(\n                     encoder_layer.__call__,\n                     hidden_states,\n-                    None,\n+                    attention_mask,\n                     position_ids,\n                     None,\n                     output_attentions,\n@@ -660,6 +704,7 @@ def forward(\n             else:\n                 layer_outputs = encoder_layer(\n                     hidden_states,\n+                    attention_mask=attention_mask,\n                     position_ids=position_ids,\n                     output_attentions=output_attentions,\n                     position_embeddings=position_embeddings,\n@@ -810,13 +855,19 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         \"\"\"\n         Args:\n             encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n                 Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n                 of the decoder.\n+            encoder_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Mask to avoid performing attention on padding indices in `encoder_hidden_states`. Mask values selected in `[0, 1]`:\n+                - 1 for tokens that are **not masked**,\n+                - 0 for tokens that are **masked**.\n+                [What are attention masks?](../glossary#attention-mask)\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -865,6 +916,26 @@ def forward(\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n \n+        # attention mask downsampling\n+        if encoder_attention_mask is not None:\n+            mask_len = encoder_hidden_states.shape[-2]\n+            downsample_stride = 64 * 3 * 2  # conv strides\n+            encoder_attention_mask = encoder_attention_mask[..., ::downsample_stride][..., :mask_len]\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                encoder_attention_mask = encoder_attention_mask if (encoder_attention_mask == 0.0).any() else None\n+\n+            # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+            elif self.config._attn_implementation == \"sdpa\" and not output_attentions:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask, hidden_states.dtype, hidden_states.shape[-2]\n+                )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, hidden_states.dtype, hidden_states.shape[-2]\n+                )\n+\n         for decoder_layer in self.layers:\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n@@ -886,6 +957,7 @@ def forward(\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n                     attention_mask=causal_mask,\n+                    encoder_attention_mask=encoder_attention_mask,\n                     encoder_hidden_states=encoder_hidden_states,\n                     position_ids=position_ids,\n                     past_key_value=past_key_values,\n@@ -1168,9 +1240,11 @@ def compute_num_masked_span(input_length):\n             `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n             `input_values`, the [`AutoFeatureExtractor`] should be used for padding\n             and conversion into a tensor of type `torch.FloatTensor`.\n-        attention_mask (`torch.Tensor`)`, *optional*):\n-            Moonshine does not support masking of the `input_values`, this argument is preserved for compatibility,\n-            but it is not used.\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding indices in `input_values`. Mask values selected in `[0, 1]`:\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+            [What are attention masks?](../glossary#attention-mask)\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n             Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n             it.\n@@ -1371,6 +1445,7 @@ def forward(\n         if encoder_outputs is None:\n             encoder_outputs = self.encoder(\n                 input_values,\n+                attention_mask=attention_mask,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n@@ -1387,6 +1462,7 @@ def forward(\n         decoder_outputs = self.decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,\n+            encoder_attention_mask=attention_mask,\n             encoder_hidden_states=encoder_outputs[0],\n             past_key_values=past_key_values,\n             inputs_embeds=decoder_inputs_embeds,\n@@ -1517,6 +1593,7 @@ def forward(\n \n         outputs = self.model(\n             input_values,\n+            attention_mask=attention_mask,\n             decoder_input_ids=decoder_input_ids,\n             encoder_outputs=encoder_outputs,\n             decoder_attention_mask=decoder_attention_mask,"
        },
        {
            "sha": "a78b153725d5db4615a4f064a9eeabbe8bae911d",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 90,
            "deletions": 7,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/693328f2bc66b0ef20950c43a46d440cbad50265/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/693328f2bc66b0ef20950c43a46d440cbad50265/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=693328f2bc66b0ef20950c43a46d440cbad50265",
            "patch": "@@ -12,6 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import math\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -21,6 +22,10 @@\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...configuration_utils import PretrainedConfig\n from ...generation import GenerationMixin\n+from ...modeling_attn_mask_utils import (\n+    _prepare_4d_attention_mask,\n+    _prepare_4d_attention_mask_for_sdpa,\n+)\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -91,6 +96,9 @@ class MoonshineConfig(PretrainedConfig):\n             by meanpooling all the original heads within that group. For more details checkout [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `decoder_num_attention_heads`.\n+        pad_head_dim_to_multiple_of (`int`, *optional*):\n+            Pad head dimension in encoder and decoder to the next multiple of this value. Necessary for using certain\n+            optimized attention implementations.\n         encoder_hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n             The non-linear activation function (function or string) in the encoder.\n         decoder_hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n@@ -191,6 +199,7 @@ def __init__(\n         decoder_num_attention_heads=8,\n         encoder_num_key_value_heads=None,\n         decoder_num_key_value_heads=None,\n+        pad_head_dim_to_multiple_of=None,\n         encoder_hidden_act=\"gelu\",\n         decoder_hidden_act=\"silu\",\n         max_position_embeddings=512,\n@@ -223,6 +232,8 @@ def __init__(\n             decoder_num_key_value_heads = decoder_num_attention_heads\n         self.decoder_num_key_value_heads = decoder_num_key_value_heads\n \n+        self.pad_head_dim_to_multiple_of = pad_head_dim_to_multiple_of\n+\n         self.encoder_hidden_act = encoder_hidden_act\n         self.decoder_hidden_act = decoder_hidden_act\n         self.max_position_embeddings = max_position_embeddings\n@@ -360,6 +371,23 @@ def forward(\n                 attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         is_causal = True if self.is_causal and attention_mask is None and q_len > 1 else False\n+\n+        # Pad head size dimension to next specified multiple. Q K and V always have equal head sizes.\n+        head_dim_padding = 0\n+        if self.config.pad_head_dim_to_multiple_of is not None:\n+            head_dim = query_states.shape[-1]\n+            target_multiple = self.config.pad_head_dim_to_multiple_of\n+            target_head_dim = target_multiple * ((head_dim + target_multiple - 1) // target_multiple)\n+            head_dim_padding = target_head_dim - head_dim\n+            if head_dim_padding > 0:\n+                # Ensure scaling is correct even with padding.\n+                if self.scaling is None:\n+                    self.scaling = 1.0 / math.sqrt(query_states.shape[-1])\n+\n+                query_states = torch.nn.functional.pad(query_states, (0, head_dim_padding))\n+                key_states = torch.nn.functional.pad(key_states, (0, head_dim_padding))\n+                value_states = torch.nn.functional.pad(value_states, (0, head_dim_padding))\n+\n         attn_output, attn_weights = attention_interface(\n             self,\n             query_states,\n@@ -372,6 +400,10 @@ def forward(\n             **kwargs,\n         )\n \n+        # Remove head size padding.\n+        if head_dim_padding > 0:\n+            attn_output = attn_output[:, :, :, :-head_dim_padding]\n+\n         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n         return attn_output, attn_weights\n@@ -593,9 +625,11 @@ def forward(\n                 `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n                 `input_values`, the [`AutoFeatureExtractor`] should be used for padding\n                 and conversion into a tensor of type `torch.FloatTensor`.\n-            attention_mask (`torch.Tensor`)`, *optional*):\n-                Moonshine does not support masking of the `input_values`, this argument is preserved for compatibility,\n-                but it is not used.\n+            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Mask to avoid performing attention on padding indices in `input_values`. Mask values selected in `[0, 1]`:\n+                - 1 for tokens that are **not masked**,\n+                - 0 for tokens that are **masked**.\n+                [What are attention masks?](../glossary#attention-mask)\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n                 tensors for more detail.\n@@ -622,6 +656,22 @@ def forward(\n         hidden_states = nn.functional.gelu(self.conv3(hidden_states))\n         hidden_states = hidden_states.permute(0, 2, 1)\n \n+        # attention mask downsampling\n+        if attention_mask is not None:\n+            mask_len = self._get_feat_extract_output_lengths(attention_mask.shape[-1])\n+            downsample_stride = 64 * 3 * 2  # conv strides\n+            attention_mask = attention_mask[..., ::downsample_stride][..., :mask_len]\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if (attention_mask == 0.0).any() else None\n+\n+            # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+            elif self.config._attn_implementation == \"sdpa\" and not output_attentions:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, hidden_states.dtype)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n+\n         position_ids = torch.arange(0, hidden_states.shape[1], device=hidden_states.device).unsqueeze(0)\n \n         # create position embeddings to be shared across the decoder layers\n@@ -639,7 +689,7 @@ def forward(\n                 layer_outputs = self._gradient_checkpointing_func(\n                     encoder_layer.__call__,\n                     hidden_states,\n-                    None,\n+                    attention_mask,\n                     position_ids,\n                     None,\n                     output_attentions,\n@@ -650,6 +700,7 @@ def forward(\n             else:\n                 layer_outputs = encoder_layer(\n                     hidden_states,\n+                    attention_mask=attention_mask,\n                     position_ids=position_ids,\n                     output_attentions=output_attentions,\n                     position_embeddings=position_embeddings,\n@@ -698,13 +749,19 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         \"\"\"\n         Args:\n             encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n                 Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n                 of the decoder.\n+            encoder_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Mask to avoid performing attention on padding indices in `encoder_hidden_states`. Mask values selected in `[0, 1]`:\n+                - 1 for tokens that are **not masked**,\n+                - 0 for tokens that are **masked**.\n+                [What are attention masks?](../glossary#attention-mask)\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -753,6 +810,26 @@ def forward(\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n \n+        # attention mask downsampling\n+        if encoder_attention_mask is not None:\n+            mask_len = encoder_hidden_states.shape[-2]\n+            downsample_stride = 64 * 3 * 2  # conv strides\n+            encoder_attention_mask = encoder_attention_mask[..., ::downsample_stride][..., :mask_len]\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                encoder_attention_mask = encoder_attention_mask if (encoder_attention_mask == 0.0).any() else None\n+\n+            # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+            elif self.config._attn_implementation == \"sdpa\" and not output_attentions:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask, hidden_states.dtype, hidden_states.shape[-2]\n+                )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, hidden_states.dtype, hidden_states.shape[-2]\n+                )\n+\n         for decoder_layer in self.layers:\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n@@ -774,6 +851,7 @@ def forward(\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n                     attention_mask=causal_mask,\n+                    encoder_attention_mask=encoder_attention_mask,\n                     encoder_hidden_states=encoder_hidden_states,\n                     position_ids=position_ids,\n                     past_key_value=past_key_values,\n@@ -816,9 +894,11 @@ def forward(\n             `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n             `input_values`, the [`AutoFeatureExtractor`] should be used for padding\n             and conversion into a tensor of type `torch.FloatTensor`.\n-        attention_mask (`torch.Tensor`)`, *optional*):\n-            Moonshine does not support masking of the `input_values`, this argument is preserved for compatibility,\n-            but it is not used.\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding indices in `input_values`. Mask values selected in `[0, 1]`:\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+            [What are attention masks?](../glossary#attention-mask)\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n             Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n             it.\n@@ -945,6 +1025,7 @@ def forward(\n         if encoder_outputs is None:\n             encoder_outputs = self.encoder(\n                 input_values,\n+                attention_mask=attention_mask,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n@@ -961,6 +1042,7 @@ def forward(\n         decoder_outputs = self.decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,\n+            encoder_attention_mask=attention_mask,\n             encoder_hidden_states=encoder_outputs[0],\n             past_key_values=past_key_values,\n             inputs_embeds=decoder_inputs_embeds,\n@@ -1075,6 +1157,7 @@ def forward(\n \n         outputs = self.model(\n             input_values,\n+            attention_mask=attention_mask,\n             decoder_input_ids=decoder_input_ids,\n             encoder_outputs=encoder_outputs,\n             decoder_attention_mask=decoder_attention_mask,"
        }
    ],
    "stats": {
        "total": 196,
        "additions": 181,
        "deletions": 15
    }
}