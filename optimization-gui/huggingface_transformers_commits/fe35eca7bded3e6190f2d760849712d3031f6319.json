{
    "author": "dross20",
    "message": "Update BigBirdPegasus model card (#39104)\n\n* Update \bigbird_pegasus.md\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "fe35eca7bded3e6190f2d760849712d3031f6319",
    "files": [
        {
            "sha": "bf9b417543a7aae213caa43b2139ca7e320988a5",
            "filename": "docs/source/en/model_doc/bigbird_pegasus.md",
            "status": "modified",
            "additions": 112,
            "deletions": 48,
            "changes": 160,
            "blob_url": "https://github.com/huggingface/transformers/blob/fe35eca7bded3e6190f2d760849712d3031f6319/docs%2Fsource%2Fen%2Fmodel_doc%2Fbigbird_pegasus.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/fe35eca7bded3e6190f2d760849712d3031f6319/docs%2Fsource%2Fen%2Fmodel_doc%2Fbigbird_pegasus.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbigbird_pegasus.md?ref=fe35eca7bded3e6190f2d760849712d3031f6319",
            "patch": "@@ -14,59 +14,123 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# BigBirdPegasus\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+           <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n </div>\n \n-## Overview\n-\n-The BigBird model was proposed in [Big Bird: Transformers for Longer Sequences](https://huggingface.co/papers/2007.14062) by\n-Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon,\n-Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others. BigBird, is a sparse-attention\n-based transformer which extends Transformer based models, such as BERT to much longer sequences. In addition to sparse\n-attention, BigBird also applies global attention as well as random attention to the input sequence. Theoretically, it\n-has been shown that applying sparse, global, and random attention approximates full attention, while being\n-computationally much more efficient for longer sequences. As a consequence of the capability to handle longer context,\n-BigBird has shown improved performance on various long document NLP tasks, such as question answering and\n-summarization, compared to BERT or RoBERTa.\n-\n-The abstract from the paper is the following:\n-\n-*Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP.\n-Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence\n-length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that\n-reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and\n-is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our\n-theoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS), that attend to the entire\n-sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to\n-8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context,\n-BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also\n-propose novel applications to genomics data.*\n-\n-The original code can be found [here](https://github.com/google-research/bigbird).\n-\n-## Usage tips\n-\n-- For an in-detail explanation on how BigBird's attention works, see [this blog post](https://huggingface.co/blog/big-bird).\n-- BigBird comes with 2 implementations: **original_full** & **block_sparse**. For the sequence length < 1024, using\n-  **original_full** is advised as there is no benefit in using **block_sparse** attention.\n-- The code currently uses window size of 3 blocks and 2 global blocks.\n-- Sequence length must be divisible by block size.\n-- Current implementation supports only **ITC**.\n-- Current implementation doesn't support **num_random_blocks = 0**.\n-- BigBirdPegasus uses the [PegasusTokenizer](https://github.com/huggingface/transformers/blob/main/src/transformers/models/pegasus/tokenization_pegasus.py).\n-- BigBird is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\n-  the left.\n+# BigBirdPegasus\n+\n+[BigBirdPegasus](https://huggingface.co/papers/2007.14062) is an encoder-decoder (sequence-to-sequence) transformer model for long-input summarization. It extends the [BigBird](./big_bird) architecture with an additional pretraining objective borrowed from [Pegasus](./pegasus) called gap sequence generation (GSG). Whole sentences are masked and the model has to fill in the gaps in the document. BigBirdPegasus's ability to keep track of long contexts makes it effective at summarizing lengthy inputs, surpassing the performance of base Pegasus models.\n+\n+You can find all the original BigBirdPegasus checkpoints under the [Google](https://huggingface.co/google/models?search=bigbird-pegasus) organization.\n+\n+> [!TIP]\n+> This model was contributed by [vasudevgupta](https://huggingface.co/vasudevgupta).\n+>\n+> Click on the BigBirdPegasus models in the right sidebar for more examples of how to apply BigBirdPegasus to different language tasks.\n+\n+The example below demonstrates how to summarize text with [`Pipeline`], [`AutoModel`], and from the command line.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+import torch\n+from transformers import pipeline\n+\n+pipeline = pipeline(\n+    task=\"summarization\",\n+    model=\"google/bigbird-pegasus-large-arxiv\",\n+    torch_dtype=torch.float32,\n+    device=0\n+)\n+pipeline(\"\"\"Plants are among the most remarkable and essential life forms on Earth, possessing a unique ability to produce their own food through a process known as photosynthesis. This complex biochemical process is fundamental not only to plant life but to virtually all life on the planet.\n+Through photosynthesis, plants capture energy from sunlight using a green pigment called chlorophyll, which is located in specialized cell structures called chloroplasts. In the presence of light, plants absorb carbon dioxide from the atmosphere through small pores in their leaves called stomata, and take in water from the soil through their root systems.\n+These ingredients are then transformed into glucose, a type of sugar that serves as a source of chemical energy, and oxygen, which is released as a byproduct into the atmosphere. The glucose produced during photosynthesis is not just used immediately; plants also store it as starch or convert it into other organic compounds like cellulose, which is essential for building their cellular structure.\n+This energy reserve allows them to grow, develop leaves, produce flowers, bear fruit, and carry out various physiological processes throughout their lifecycle.\"\"\")\n+```\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n+\n+tokenizer = AutoTokenizer.from_pretrained(\n+    \"google/bigbird-pegasus-large-arxiv\"\n+)\n+model = AutoModelForSeq2SeqLM.from_pretrained(\n+    \"google/bigbird-pegasus-large-arxiv\",\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"auto\",\n+)\n+\n+input_text = \"\"\"Plants are among the most remarkable and essential life forms on Earth, possessing a unique ability to produce their own food through a process known as photosynthesis. This complex biochemical process is fundamental not only to plant life but to virtually all life on the planet.\n+Through photosynthesis, plants capture energy from sunlight using a green pigment called chlorophyll, which is located in specialized cell structures called chloroplasts. In the presence of light, plants absorb carbon dioxide from the atmosphere through small pores in their leaves called stomata, and take in water from the soil through their root systems.\n+These ingredients are then transformed into glucose, a type of sugar that serves as a source of chemical energy, and oxygen, which is released as a byproduct into the atmosphere. The glucose produced during photosynthesis is not just used immediately; plants also store it as starch or convert it into other organic compounds like cellulose, which is essential for building their cellular structure.\n+This energy reserve allows them to grow, develop leaves, produce flowers, bear fruit, and carry out various physiological processes throughout their lifecycle.\"\"\"\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+\n+output = model.generate(**input_ids, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n+</hfoption>\n+<hfoption id=\"transformers-cli\">\n+\n+```bash\n+echo -e \"Plants are among the most remarkable and essential life forms on Earth, possessing a unique ability to produce their own food through a process known as photosynthesis. This complex biochemical process is fundamental not only to plant life but to virtually all life on the planet. Through photosynthesis, plants capture energy from sunlight using a green pigment called chlorophyll, which is located in specialized cell structures called chloroplasts.\" | transformers-cli run --task summarization --model google/bigbird-pegasus-large-arxiv --device 0\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n+\n+The example below uses [bitsandbytes](../quantization/bitsandbytes) to only quantize the weights to int4.\n+\n+```py\n+import torch\n+from transformers import BitsAndBytesConfig, AutoModelForSeq2SeqLM, AutoTokenizer\n+\n+quantization_config = BitsAndBytesConfig(\n+    load_in_4bit=True,\n+    bnb_4bit_compute_dtype=torch.bfloat16,\n+    bnb_4bit_quant_type=\"nf4\"\n+)\n+model = AutoModelForSeq2SeqLM.from_pretrained(\n+    \"google/bigbird-pegasus-large-arxiv\",\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"auto\",\n+    quantization_config=quantization_config\n+)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\n+    \"google/bigbird-pegasus-large-arxiv\"\n+)\n+\n+input_text = \"\"\"Plants are among the most remarkable and essential life forms on Earth, possessing a unique ability to produce their own food through a process known as photosynthesis. This complex biochemical process is fundamental not only to plant life but to virtually all life on the planet.\n+Through photosynthesis, plants capture energy from sunlight using a green pigment called chlorophyll, which is located in specialized cell structures called chloroplasts. In the presence of light, plants absorb carbon dioxide from the atmosphere through small pores in their leaves called stomata, and take in water from the soil through their root systems.\n+These ingredients are then transformed into glucose, a type of sugar that serves as a source of chemical energy, and oxygen, which is released as a byproduct into the atmosphere. The glucose produced during photosynthesis is not just used immediately; plants also store it as starch or convert it into other organic compounds like cellulose, which is essential for building their cellular structure.\n+This energy reserve allows them to grow, develop leaves, produce flowers, bear fruit, and carry out various physiological processes throughout their lifecycle.\"\"\"\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+\n+output = model.generate(**input_ids, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n+\n+## Notes\n+\n+- BigBirdPegasus also uses the [`PegasusTokenizer`].\n+- Inputs should be padded on the right because BigBird uses absolute position embeddings.\n+- BigBirdPegasus supports `original_full` and `block_sparse` attention. If the input sequence length is less than 1024, it is recommended to use `original_full` since sparse patterns don't offer much benefit for smaller inputs.\n+- The current implementation uses window size of 3 blocks and 2 global blocks, only supports the ITC-implementation, and doesn't support `num_random_blocks=0`.\n+- The sequence length must be divisible by the block size.\n \n ## Resources\n \n-- [Text classification task guide](../tasks/sequence_classification)\n-- [Question answering task guide](../tasks/question_answering)\n-- [Causal language modeling task guide](../tasks/language_modeling)\n-- [Translation task guide](../tasks/translation)\n-- [Summarization task guide](../tasks/summarization)\n+Read the [Understanding BigBird's Block Sparse Attention](https://huggingface.co/blog/big-bird) blog post for more details about how BigBird's attention works.\n \n ## BigBirdPegasusConfig\n "
        }
    ],
    "stats": {
        "total": 160,
        "additions": 112,
        "deletions": 48
    }
}