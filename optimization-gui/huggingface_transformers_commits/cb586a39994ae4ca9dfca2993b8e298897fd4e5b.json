{
    "author": "MekkCyber",
    "message": "Add require_read_token to fp8 tests (#36189)\n\nfix",
    "sha": "cb586a39994ae4ca9dfca2993b8e298897fd4e5b",
    "files": [
        {
            "sha": "f572567ed18cb541968cf063c401293be7bd9177",
            "filename": "tests/quantization/finegrained_fp8/test_fp8.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb586a39994ae4ca9dfca2993b8e298897fd4e5b/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb586a39994ae4ca9dfca2993b8e298897fd4e5b/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py?ref=cb586a39994ae4ca9dfca2993b8e298897fd4e5b",
            "patch": "@@ -20,6 +20,7 @@\n from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, FineGrainedFP8Config, OPTForCausalLM\n from transformers.testing_utils import (\n     require_accelerate,\n+    require_read_token,\n     require_torch_gpu,\n     require_torch_multi_gpu,\n     slow,\n@@ -59,6 +60,7 @@ def test_from_dict(self):\n \n @slow\n @require_accelerate\n+@require_read_token\n @require_torch_gpu\n class FP8QuantizerTest(unittest.TestCase):\n     model_name = \"meta-llama/Llama-3.2-1B\""
        }
    ],
    "stats": {
        "total": 2,
        "additions": 2,
        "deletions": 0
    }
}