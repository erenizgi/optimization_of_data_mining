{
    "author": "ArthurZucker",
    "message": "Kernels flash attn (#39474)\n\n* use partial to wrap around `transformers` utils!\n\n* try to refactor?\n\n* revert one wrong change\n\n* just a nit\n\n* push\n\n* reverter watever was wrong!\n\n* some nits\n\n* fixes when there is no attention mask\n\n* bring the licence back\n\n* some fixes\n\n* nit\n\n* style\n\n* remove prints\n\n* correct dtype\n\n* fa flags for testing\n\n* update\n\n* use paged attention if requested!\n\n* updates\n\n* a clone was needed, not sure why\n\n* automatically create cu seq lens when input is flash, this at least makes sure layers don't re-compute\n\n* simplify and improve?\n\n* flash attention is kinda broken on recent cuda version so allow the opportunity to use something else\n\n* fix!\n\n* protect kernels import\n\n* update\n\n* properly parse generation config being passed\n\n* revert and update\n\n* add two tests\n\n* some fixes\n\n* fix test FA2\n\n* takes comment into account\n\n* fixup\n\n* revert changes\n\n* revert the clone, it is only needed because the metal kernel is not doing it?\n\n* [docs] update attention implementation and cache docs (#39547)\n\n* update docs\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* applu suggestions\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* fix mps on our side for now\n\n* Update src/transformers/integrations/flash_paged.py\n\n* no qa\n\n---------\n\nCo-authored-by: Vasqu <antonprogamer@gmail.com>\nCo-authored-by: Raushan Turganbay <raushan@huggingface.co>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "efceeaf2678678553e94dce78859f87776e633a7",
    "files": [
        {
            "sha": "e462e483c24e430c3ffb5b4572d3578a245a44bc",
            "filename": "src/transformers/generation/continuous_batching.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/efceeaf2678678553e94dce78859f87776e633a7/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/efceeaf2678678553e94dce78859f87776e633a7/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py?ref=efceeaf2678678553e94dce78859f87776e633a7",
            "patch": "@@ -1119,7 +1119,8 @@ def __init__(\n         self._request_lock = threading.Lock()\n         self.model.generation_config.top_p = None\n         self.do_sample = getattr(generation_config, \"do_sample\", True)\n-        self.logit_processor = self.model._get_logits_processor(self.model.generation_config)\n+        generation_config = model.generation_config if generation_config is None else generation_config\n+        self.logit_processor = self.model._get_logits_processor(generation_config)\n         self.use_cuda_graph = getattr(generation_config, \"use_cuda_graph\", True)\n         self.profile = getattr(generation_config, \"profile\", False)\n         self.manual_eviction = manual_eviction"
        },
        {
            "sha": "3bffb5fdda91a9e50a0360b13bb1a10260547dfc",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/efceeaf2678678553e94dce78859f87776e633a7/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/efceeaf2678678553e94dce78859f87776e633a7/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=efceeaf2678678553e94dce78859f87776e633a7",
            "patch": "@@ -677,6 +677,24 @@ def prepare_inputs_for_generation(\n         if encoder_attention_mask is not None:\n             model_inputs[\"attention_mask\"] = encoder_attention_mask\n \n+        if \"flash\" in self.config._attn_implementation and self._supports_attention_backend:\n+            tensor_kws = {\"dtype\": torch.int32, \"device\": self.device}\n+            pos = model_inputs[\"position_ids\"][:, -1]\n+\n+            cu_seq_lens_k = torch.cat([torch.zeros(1, **tensor_kws), pos.cumsum(0).add(1)], 0)\n+            max_length_k = int(pos.max()) + 1\n+\n+            bs, seq_len = input_ids.size()\n+            q_len = torch.ones(bs, **tensor_kws) if seq_len == 1 else pos.to(torch.int32).add(1)\n+            cu_seq_lens_q = torch.cat([torch.zeros(1, **tensor_kws), q_len.cumsum(0)], 0)\n+            max_length_q = int(q_len.max())\n+\n+            model_inputs.update(\n+                cu_seq_lens_q=cu_seq_lens_q.to(self.device),\n+                cu_seq_lens_k=cu_seq_lens_k.to(self.device),\n+                max_length_q=max_length_q,\n+                max_length_k=max_length_k,\n+            )\n         # 7. Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n         for key, value in kwargs.items():\n             if key not in model_inputs:"
        },
        {
            "sha": "43c65b46c8059554b1531996428dfa04445302b0",
            "filename": "src/transformers/integrations/flash_attention.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/efceeaf2678678553e94dce78859f87776e633a7/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/efceeaf2678678553e94dce78859f87776e633a7/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_attention.py?ref=efceeaf2678678553e94dce78859f87776e633a7",
            "patch": "@@ -38,7 +38,6 @@ def flash_attention_forward(\n             \"FlashAttention does not support inputs with dim=0.\\n\"\n             \"Please check your input shapes or use SDPA instead.\"\n         )\n-\n     # FA2 uses non-transposed inputs\n     query = query.transpose(1, 2)\n     key = key.transpose(1, 2)\n@@ -76,6 +75,7 @@ def flash_attention_forward(\n         use_top_left_mask=_use_top_left_mask,\n         target_dtype=target_dtype,\n         attn_implementation=module.config._attn_implementation,\n+        layer_idx=module.layer_idx if hasattr(module, \"layer_idx\") else None,\n         **kwargs,\n     )\n "
        },
        {
            "sha": "236e216b3ff2853fe5b14ee801722a6608e49850",
            "filename": "src/transformers/integrations/flash_paged.py",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/efceeaf2678678553e94dce78859f87776e633a7/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/efceeaf2678678553e94dce78859f87776e633a7/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_paged.py?ref=efceeaf2678678553e94dce78859f87776e633a7",
            "patch": "@@ -5,7 +5,7 @@\n \n \n if is_flash_attn_2_available():\n-    from flash_attn import flash_attn_varlen_func\n+    from flash_attn import flash_attn_varlen_func  # noqa: F401\n \n \n def paged_attention_forward(\n@@ -20,6 +20,7 @@ def paged_attention_forward(\n     max_seqlen_q=None,\n     max_seqlen_k=None,\n     block_tables=None,\n+    implementation=None,\n     **kwargs,\n ) -> torch.Tensor:\n     r\"\"\"Perform the forward pass of attention with paged key-value cache.\n@@ -46,12 +47,14 @@ def paged_attention_forward(\n     \"\"\"\n     k, v = cache.update(k, v, module.layer_idx, cumulative_seqlens_k=cumulative_seqlens_k, **kwargs)\n \n+    if implementation is not None:\n+        flash_attn_varlen_func = implementation.flash_attn_varlen_func\n     attn_output = flash_attn_varlen_func(\n-        q.transpose(1, 2).squeeze(0),\n-        k.transpose(1, 2).squeeze(0),\n-        v.transpose(1, 2).squeeze(0),\n+        q.transpose(1, 2).squeeze(0).contiguous(),\n+        k.transpose(1, 2).squeeze(0).contiguous(),\n+        v.transpose(1, 2).squeeze(0).contiguous(),\n         cumulative_seqlens_q.to(torch.int32),\n-        cumulative_seqlens_k.to(torch.int32),\n+        cumulative_seqlens_k.to(torch.int32).clone(),\n         max_seqlen_q,\n         max_seqlen_k,\n         softmax_scale=module.scaling,"
        },
        {
            "sha": "848c2a2141135ae36c5ac5e8d49bf5390a45ade6",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 164,
            "deletions": 318,
            "changes": 482,
            "blob_url": "https://github.com/huggingface/transformers/blob/efceeaf2678678553e94dce78859f87776e633a7/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/efceeaf2678678553e94dce78859f87776e633a7/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=efceeaf2678678553e94dce78859f87776e633a7",
            "patch": "@@ -11,7 +11,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n import inspect\n import os\n import warnings\n@@ -20,6 +19,8 @@\n import torch\n import torch.nn.functional as F\n \n+from transformers.utils.import_utils import is_kernels_available\n+\n from .utils import (\n     is_flash_attn_2_available,\n     is_flash_attn_3_available,\n@@ -31,25 +32,16 @@\n \n \n logger = logging.get_logger(__name__)\n-flash_attn_func = None\n \n \n-def _index_first_axis(tensor, indices):\n-    \"\"\"\n-    A local implementation of the PyTorch indexing operation `tensor[indices]` on the first axis,\n-    after flattening the first two dimensions of the tensor. This is functionally equivalent to\n-    FA2's `index_first_axis` and replaces the need to import it.\n-    \"\"\"\n-    # The input tensor is expected to be of shape (batch, seq_len, ...). We flatten the first\n-    # two dimensions to get (total_tokens, ...) before indexing.\n-    reshaped_tensor = tensor.reshape(-1, *tensor.shape[2:])\n-    return reshaped_tensor[indices]\n+def _index_first_axis(tensor: torch.Tensor, indices: torch.Tensor) -> torch.Tensor:\n+    reshaped = tensor.contiguous().reshape(-1, *tensor.shape[2:])\n+    return reshaped[indices]\n \n \n def _fa3_unpad_input(hidden_states, attention_mask, unused_mask=None):\n     \"\"\"\n     FA3-compatible unpad_input function.\n-\n     Arguments:\n         hidden_states: (batch, seqlen, ...)\n         attention_mask: (batch, seqlen), bool / int, 1 means valid and 0 means not valid.\n@@ -80,7 +72,6 @@ def _fa3_unpad_input(hidden_states, attention_mask, unused_mask=None):\n def _fa3_pad_input(hidden_states, indices, batch, seqlen):\n     \"\"\"\n     FA3-compatible pad_input function.\n-\n     Arguments:\n         hidden_states: (total_nnz, ...), where total_nnz = number of tokens in selected in attention_mask.\n         indices: (total_nnz), the indices that represent the non-masked tokens of the original padded input sequence.\n@@ -95,109 +86,12 @@ def _fa3_pad_input(hidden_states, indices, batch, seqlen):\n     return output.view(batch, seqlen, *dim)\n \n \n-FA_VERSION = None\n-if is_flash_attn_2_available():\n-    from flash_attn import flash_attn_func as flash_attn_2_func\n-    from flash_attn import flash_attn_varlen_func as flash_attn_2_varlen_func\n-    from flash_attn.bert_padding import pad_input as pad_input_fa2\n-    from flash_attn.bert_padding import unpad_input as unpad_input_fa2\n-    from flash_attn.layers.rotary import apply_rotary_emb\n-\n-    HAS_FA2 = True\n-    FA_VERSION = 2\n-elif is_torch_npu_available():\n-    # patch functions in package `flash-attn` when using flash-attention on Ascend NPU.\n-    from .integrations.npu_flash_attention import npu_apply_rotary_emb as apply_rotary_emb  # noqa: F401\n-    from .integrations.npu_flash_attention import npu_flash_attn_func as flash_attn_2_func\n-    from .integrations.npu_flash_attention import npu_flash_attn_varlen_func as flash_attn_2_varlen_func\n-    from .integrations.npu_flash_attention import pad_input as pad_input_fa2\n-    from .integrations.npu_flash_attention import unpad_input as unpad_input_fa2\n-\n-    HAS_FA2 = True\n-    FA_VERSION = 2\n-else:\n-    flash_attn_2_func = None\n-    flash_attn_2_varlen_func = None\n-    pad_input_fa2 = None\n-    unpad_input_fa2 = None\n-    apply_rotary_emb = None\n-    HAS_FA2 = False\n-\n-if is_flash_attn_3_available():\n-    from flash_attn_interface import flash_attn_func as flash_attn_3_func\n-    from flash_attn_interface import flash_attn_varlen_func as flash_attn_3_varlen_func\n-\n-    pad_input_fa3 = _fa3_pad_input\n-    unpad_input_fa3 = _fa3_unpad_input\n-    HAS_FA3 = True\n-    FA_VERSION = 3\n-else:\n-    flash_attn_3_func = None\n-    flash_attn_3_varlen_func = None\n-    pad_input_fa3 = None\n-    unpad_input_fa3 = None\n-    HAS_FA3 = False\n-\n-\n-# Current Flash Attention implementations\n-if FA_VERSION:\n-    flash_attn_func = globals()[f\"flash_attn_{FA_VERSION}_func\"]\n-    flash_attn_varlen_func = globals()[f\"flash_attn_{FA_VERSION}_varlen_func\"]\n-    unpad_input = globals()[f\"unpad_input_fa{FA_VERSION}\"]\n-    pad_input = globals()[f\"pad_input_fa{FA_VERSION}\"]\n-\n-\n-_flash_supports_window_size = False\n-\n-\n-if flash_attn_func:\n-    _flash_supports_window_size = \"window_size\" in list(inspect.signature(flash_attn_func).parameters)\n-\n-\n-def is_flash_attn_available():\n-    \"\"\"Determine whether flash-attention can be used or not.\"\"\"\n-\n-    if is_flash_attn_3_available():\n-        return True\n-\n-    # if package `flash-attn` is available, flash-attention can be used natively.\n-    if is_flash_attn_2_available():\n-        return True\n-\n-    # flash-attention can be used on Ascend NPU without package `flash-attn`\n-    if is_torch_npu_available():\n-        return True\n-\n-    return False\n-\n-\n-def flash_attn_supports_top_left_mask():\n-    \"\"\"Determine whether flash-attention uses top-left or down-right mask\"\"\"\n-\n-    if is_flash_attn_3_available():\n-        return False\n-\n-    if is_flash_attn_2_available():\n-        # top-left mask is used in package `flash-attn` with version lower than 2.1.0\n-        return not is_flash_attn_greater_or_equal_2_10()\n-\n-    if is_torch_npu_available():\n-        # down-right mask is used on Ascend NPU by default, set env `NPU_FA2_SPARSE_MODE=2` to activate top-left mask.\n-        from .integrations.npu_flash_attention import is_npu_fa2_top_left_aligned_causal_mask\n-\n-        return is_npu_fa2_top_left_aligned_causal_mask()\n-\n-    return False\n-\n-\n def _get_unpad_data(attention_mask: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, int]:\n     \"\"\"\n     Retrieves indexing data required to repad unpadded (ragged) tensors.\n-\n     Arguments:\n         attention_mask (`torch.Tensor`):\n             Boolean or int tensor of shape (batch_size, sequence_length), 1 means valid and 0 means not valid.\n-\n     Return:\n         indices (`torch.Tensor`):\n             The indices of non-masked tokens from the flattened input sequence.\n@@ -229,10 +123,8 @@ def _upad_input(\n ):\n     \"\"\"\n     Unpads query, key, and values tensors, using a single dimension for all tokens even though they belong to different batches.\n-\n     This function is used instead of `flash_attn.bert_padding.unpad_input` in order to avoid the recomputation of the same intermediary\n     tensors for query, key, value tensors.\n-\n     Arguments:\n         query_layer (`torch.Tensor`):\n             Query state with padding. Shape: (batch_size, query_length, num_heads, head_dim).\n@@ -246,7 +138,6 @@ def _upad_input(\n             Target length.\n         unpad_input_func:\n             The function to use for unpadding the input tensors.\n-\n     Return:\n         query_layer (`torch.Tensor`):\n             Query state without padding. Shape: (total_target_length, num_heads, head_dim).\n@@ -299,14 +190,12 @@ def _upad_input(\n     )\n \n \n-def _prepare_flash_attention_from_position_ids(query, key, value, position_ids):\n+def _prepare_from_posids(query, key, value, position_ids):\n     \"\"\"\n     This function returns necessary arguments to call `flash_attn_varlen_func`.\n     All three query, key, value states will be flattened.\n     Cumulative lengths of each examples in the batch will be extracted from position_ids.\n-\n     NOTE: ideally cumulative lengths should be prepared at the data collator stage\n-\n     Arguments:\n         query (`torch.Tensor`):\n             Query state with padding. Shape: (batch_size, query_length, num_heads, head_dim).\n@@ -316,7 +205,6 @@ def _prepare_flash_attention_from_position_ids(query, key, value, position_ids):\n             Value state with padding. Shape: (batch_size, kv_seq_len, num_key_value_heads, head_dim).\n         position_ids (`torch.Tensor`):\n             Boolean or int tensor of shape (batch_size, sequence_length), 1 means valid and 0 means not valid.\n-\n     Return:\n         query (`torch.Tensor`):\n             Query state without padding. Shape: (total_target_length, num_heads, head_dim).\n@@ -331,19 +219,22 @@ def _prepare_flash_attention_from_position_ids(query, key, value, position_ids):\n         (max_seqlen_in_batch_q, max_seqlen_in_batch_k) (`tuple[int]`):\n             Maximum sequence length in batch (`max_seqlen_in_batch_q` for the target sequence i.e. query, `max_seqlen_in_batch_k` for the source sequence i.e. key/value).\n     \"\"\"\n-    query = query.view(-1, query.size(-2), query.size(-1))\n+    query = query.contiguous().view(-1, query.size(-2), query.size(-1))\n     key = key.contiguous().view(-1, key.size(-2), key.size(-1))\n     value = value.contiguous().view(-1, value.size(-2), value.size(-1))\n+    cu_seqlens_k = torch.cat(\n+        [torch.tensor([0], dtype=torch.int32, device=query.device), position_ids[:, -1].cumsum(dim=0) + 1], dim=0\n+    )\n+    max_k = torch.max(position_ids, dim=1).values.max().item() + 1\n     position_ids = position_ids.flatten()\n     indices_q = torch.arange(position_ids.size(0), device=position_ids.device, dtype=torch.int32)\n \n     cu_seq_lens = torch.cat(\n         (\n-            indices_q[position_ids == 0],\n+            torch.tensor([0], device=position_ids.device, dtype=torch.int32),\n             torch.tensor(position_ids.size(), device=position_ids.device, dtype=torch.int32),\n         )\n     )\n-\n     # NOTE: With torch compile, this will cause a graph break if you don't set\n     # `TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1` in the environment or call\n     # `torch._dynamo.config.capture_scalar_outputs = True` before doing the forward pass.\n@@ -353,61 +244,101 @@ def _prepare_flash_attention_from_position_ids(query, key, value, position_ids):\n     # We should use cu_seq_lens instead of position_ids to get the max length since position_ids is not always increasing\n     # for some models (e.g. qwen2-vl).\n     max_length = cu_seq_lens.diff().max().item()\n+    return (query, key, value, indices_q, (cu_seq_lens, cu_seqlens_k), (max_length, max_k))\n \n-    return (query, key, value, indices_q, (cu_seq_lens, cu_seq_lens), (max_length, max_length))\n \n-\n-def prepare_fa2_from_position_ids(*args, **kwargs):\n+def _prepare_flash_attention_from_position_ids(query, key, value, position_ids):\n     warnings.warn(\n-        \"The function `prepare_fa2_from_position_ids` in `transformers.modeling_flash_attention_utils` is deprecated and will be removed in a future version. Please use `_prepare_flash_attention_from_position_ids` instead.\",\n+        \"prepare_fa2_from_position_ids is deprecated, use _prepare_from_posids\",\n         FutureWarning,\n     )\n-    return _prepare_flash_attention_from_position_ids(*args, **kwargs)\n+    return _prepare_from_posids(query, key, value, position_ids)\n+\n+\n+def fa_peft_integration_check(q, k, v, target_dtype: Optional[torch.dtype] = None):\n+    if target_dtype and q.dtype == torch.float32:\n+        logger.warning_once(f\"Casting fp32 inputs back to {target_dtype} for flash-attn compatibility.\")\n+        q, k, v = q.to(target_dtype), k.to(target_dtype), v.to(target_dtype)\n+    return q, k, v\n+\n+\n+def _lazy_imports(impl: Optional[str]):\n+    # returns funcs and pad/unpad based on impl\n+    is_fa2 = is_flash_attn_2_available() or is_torch_npu_available()\n+    is_fa3 = is_flash_attn_3_available()\n+    if impl == \"flash_attention_2\" or (impl is None and is_fa2 and not is_fa3):\n+        try:\n+            from flash_attn import flash_attn_func, flash_attn_varlen_func\n+            from flash_attn.bert_padding import pad_input, unpad_input\n+\n+            return flash_attn_func, flash_attn_varlen_func, pad_input, unpad_input, False\n+\n+        except ImportError as e:\n+            if not globals().get(\"use_remote_fa2\", None):\n+                use_remote_fa2 = (\n+                    input(\n+                        \"Unable to import the official flash attention, do you want to try to use `kernels-community/flash-attn` (trust remote code) Yes or No? \"\n+                    )\n+                    .strip()\n+                    .lower()\n+                )\n+                globals()[\"use_remote_fa2\"] = use_remote_fa2 in {\"yes\", \"y\", \"1\"}\n+            if globals()[\"use_remote_fa2\"]:\n+                if not is_kernels_available():\n+                    raise ImportError(\"You need to install kernels: `pip install kernels`\")\n+                from kernels import get_kernel\n+\n+                impl = get_kernel(\"kernels-community/flash-attn\")\n+                pad_input, unpad_input = _fa3_pad_input, _fa3_unpad_input\n+                return (\n+                    getattr(impl, \"flash_attn_func\", None),\n+                    getattr(impl, \"flash_attn_varlen_func\"),\n+                    pad_input,\n+                    unpad_input,\n+                    True,\n+                )\n+\n+            else:\n+                raise ImportError(\n+                    \"Failed to import flash attention 2, please install it or use another implementation.\"\n+                ) from e\n+    if impl == \"flash_attention_3\" or (impl is None and is_fa3):\n+        from flash_attn_interface import flash_attn_func, flash_attn_varlen_func\n+\n+        pad_input, unpad_input = _fa3_pad_input, _fa3_unpad_input\n+        return flash_attn_func, flash_attn_varlen_func, pad_input, unpad_input, True\n+    else:\n+        pad_input, unpad_input = _fa3_pad_input, _fa3_unpad_input\n+        return (\n+            getattr(impl, \"flash_attn_func\", None),\n+            getattr(impl, \"flash_attn_varlen_func\"),\n+            pad_input,\n+            unpad_input,\n+            True,\n+        )\n \n \n-def fa_peft_integration_check(\n-    query: torch.Tensor,\n-    key: torch.Tensor,\n-    value: torch.Tensor,\n-    target_dtype: Optional[torch.dtype] = None,\n-):\n-    \"\"\"\n-    PEFT usually casts the layer norms in float32 for training stability reasons\n-    therefore the input hidden states gets silently casted in float32. Hence, we need\n-    cast them back in float16 / bfloat16 just to be sure everything works as expected.\n-    This might slowdown training & inference so it is recommended to not cast the LayerNorms!\n+_flash_supports_window = None\n \n-    Args:\n-        query (`torch.Tensor`):\n-            Input query states to be passed to Flash Attention API\n-        key (`torch.Tensor`):\n-            Input key states to be passed to Flash Attention API\n-        value (`torch.Tensor`):\n-            Input value states to be passed to Flash Attention API\n-        target_dtype (`torch.dtype`, *optional*):\n-            The dtype to convert the attention tensors to. Conversion can be ignored by\n-            not providing the target dtype.\n-    \"\"\"\n-    if target_dtype is None:\n-        return query, key, value\n-\n-    input_dtype = query.dtype\n-    if input_dtype == torch.float32:\n-        logger.warning_once(\n-            f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-            f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-            f\" {target_dtype}.\"\n-        )\n \n-        query = query.to(target_dtype)\n-        key = key.to(target_dtype)\n-        value = value.to(target_dtype)\n+def is_flash_attn_available():\n+    return is_flash_attn_3_available() or is_flash_attn_2_available() or is_torch_npu_available()\n+\n+\n+def flash_attn_supports_top_left_mask():\n+    if is_flash_attn_3_available():\n+        return False\n+    if is_flash_attn_2_available():\n+        return not is_flash_attn_greater_or_equal_2_10()\n+\n+    from .integrations.npu_flash_attention import is_npu_fa2_top_left_aligned_causal_mask\n \n-    return query, key, value\n+    return is_npu_fa2_top_left_aligned_causal_mask()\n \n \n-flash_241 = is_flash_attn_greater_or_equal(\"2.4.1\")\n-deterministic_g = None\n+class FlashAttentionKwargs(TypedDict, total=False):\n+    cumulative_seqlens_q: Optional[torch.LongTensor]\n+    cumulative_seqlens_k: Optional[torch.LongTensor]\n \n \n def _flash_attention_forward(\n@@ -429,185 +360,100 @@ def _flash_attention_forward(\n     max_length_q: Optional[int] = None,\n     max_length_k: Optional[int] = None,\n     target_dtype: Optional[torch.dtype] = None,\n-    attn_implementation: Optional[str] = None,\n+    implementation: Optional[str] = None,\n     **kwargs,\n ):\n-    \"\"\"\n-    Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n-    first unpad the input, then computes the attention scores and pad the final attention scores.\n-\n-    Args:\n-        query_states (`torch.Tensor`):\n-            Input query states to be passed to Flash Attention API\n-        key_states (`torch.Tensor`):\n-            Input key states to be passed to Flash Attention API\n-        value_states (`torch.Tensor`):\n-            Input value states to be passed to Flash Attention API\n-        attention_mask (`torch.Tensor`, *optional*):\n-            The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the\n-            position of padding tokens and 1 for the position of non-padding tokens.\n-        dropout (`float`):\n-            Attention dropout\n-        softmax_scale (`float`, *optional*):\n-            The scaling of QK^T before applying softmax. Default to 1 / sqrt(head_dim)\n-        use_top_left_mask (`bool`, defaults to `False`):\n-            flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference.\n-        softcap (`float`, *optional*):\n-            Softcap for the attention logits, used e.g. in gemma2.\n-        deterministic (`bool`, *optional*):\n-            Determines if the deterministic option introduced in flash_attn>=2.4.1 is enabled.\n-        attn_implementation (`str`, *optional*):\n-            The attention implementation to use. If None, will default to the one based on the environment.\n-    \"\"\"\n-    if attn_implementation is None:\n-        _flash_attn_varlen_func = flash_attn_varlen_func\n-        _flash_attn_func = flash_attn_func\n-        _pad_input = pad_input\n-        _unpad_input = unpad_input\n-        _is_fa3 = HAS_FA3\n-    elif attn_implementation == \"flash_attention_3\":\n-        _flash_attn_varlen_func = flash_attn_3_varlen_func\n-        _flash_attn_func = flash_attn_3_func\n-        _pad_input = pad_input_fa3\n-        _unpad_input = unpad_input_fa3\n-        _is_fa3 = True\n-    elif attn_implementation == \"flash_attention_2\":\n-        _flash_attn_varlen_func = flash_attn_2_varlen_func\n-        _flash_attn_func = flash_attn_2_func\n-        _pad_input = pad_input_fa2\n-        _unpad_input = unpad_input_fa2\n-        _is_fa3 = False\n-\n-    if not use_top_left_mask:\n-        causal = is_causal\n+    if not all(k in globals() for k in (\"_flash_fn\", \"_flash_varlen_fn\", \"_pad_fn\", \"_unpad_fn\", \"_is_fa3\")):\n+        flash_fn, flash_varlen_fn, pad_fn, unpad_fn, is_fa3 = _lazy_imports(implementation)\n+        globals()[\"_flash_fn\"] = flash_fn\n+        globals()[\"_flash_varlen_fn\"] = flash_varlen_fn\n+        globals()[\"_pad_fn\"] = pad_fn\n+        globals()[\"_unpad_fn\"] = unpad_fn\n+        globals()[\"_is_fa3\"] = is_fa3\n+        flash_supports_window = \"window_size\" in inspect.signature(flash_varlen_fn).parameters\n+        globals()[\"_flash_supports_window\"] = flash_supports_window\n     else:\n-        # TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1.\n-        causal = is_causal and query_length != 1\n-\n-    # Assuming 4D tensors, key_states.shape[1] is the key/value sequence length (source length).\n-    use_sliding_windows = (\n-        _flash_supports_window_size and sliding_window is not None and key_states.shape[1] > sliding_window\n+        flash_fn = globals()[\"_flash_fn\"]\n+        flash_varlen_fn = globals()[\"_flash_varlen_fn\"]\n+        pad_fn = globals()[\"_pad_fn\"]\n+        unpad_fn = globals()[\"_unpad_fn\"]\n+        is_fa3 = globals()[\"_is_fa3\"]\n+        flash_supports_window = globals()[\"_flash_supports_window\"]\n+\n+    causal = is_causal and not (use_top_left_mask and query_length == 1)\n+    use_sw = (\n+        (_flash_supports_window or flash_supports_window) and sliding_window and key_states.shape[1] > sliding_window\n     )\n-    flash_kwargs = {\"window_size\": (sliding_window, sliding_window)} if use_sliding_windows else {}\n-\n-    if _is_fa3:\n-        if dropout > 0.0:\n-            logger.warning_once(\"Flash Attention 3 does not support dropout. Setting dropout to 0.0.\")\n-    else:\n+    flash_kwargs = {\"window_size\": (sliding_window, sliding_window)} if use_sw else {}\n+    if not is_fa3:\n         flash_kwargs[\"dropout_p\"] = dropout\n-\n-    if flash_241:\n-        if deterministic is None:\n-            global deterministic_g\n-            if deterministic_g is None:\n-                deterministic_g = os.environ.get(\"FLASH_ATTENTION_DETERMINISTIC\", \"0\") == \"1\"\n-            deterministic = deterministic_g\n-        flash_kwargs[\"deterministic\"] = deterministic\n-\n+    if is_flash_attn_greater_or_equal(\"2.4.1\"):\n+        det = deterministic if deterministic is not None else os.getenv(\"FLASH_ATTENTION_DETERMINISTIC\", \"0\") == \"1\"\n+        flash_kwargs[\"deterministic\"] = det\n     if softcap is not None:\n         flash_kwargs[\"softcap\"] = softcap\n \n-    # PEFT possibly silently casts tensors to fp32, this potentially reconverts to correct dtype or is a no op\n     query_states, key_states, value_states = fa_peft_integration_check(\n         query_states, key_states, value_states, target_dtype\n     )\n-\n-    # We will use `flash_attn_varlen_func` to prevent cross-example attention and also allow padding free approach\n-    # under two cases:\n-    # Case 1. If position_ids is provided and check all examples do not contain only 1 sequence, If tensor in increasing\n-    # then we probably have one sequence, otherwise it is packed. Additionally check we are in pre-fill/training stage.\n-    # Case 2. Some models pass directly pre-computed `cu_seqlens` so we don't need to infer it from position ids. It is safe to\n-    # use `flash_attn_varlen_func` knowing we already have all necessary the kwargs. NOTE: it is user's responsibility\n-    # to take care of flattenning `position_ids` if that's needed by the model. See #39121 for more information\n-    is_fa2_with_position_ids = (\n-        position_ids is not None\n-        and query_states.shape[0] == 1\n-        and (max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all()))\n-    )\n-    is_fa2_with_varlen_kwargs = all(\n-        kwarg is not None for kwarg in (cu_seq_lens_q, cu_seq_lens_k, max_length_q, max_length_k)\n-    )\n-\n-    # Contains at least one padding token in the sequence\n+    use_mask = position_ids is not None or all([cu_seq_lens_q, cu_seq_lens_k, max_length_q, max_length_k])\n     if attention_mask is not None:\n-        batch_size = query_states.shape[0]\n-        query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = _upad_input(\n-            query_states, key_states, value_states, attention_mask, query_length, _unpad_input\n+        q, k, v, idx, (cu_q, cu_k), (mq, mk) = _upad_input(\n+            query_states, key_states, value_states, attention_mask, query_length, unpad_fn\n         )\n-        cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n-        max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n-\n-        attn_output_unpad = _flash_attn_varlen_func(\n-            query_states,\n-            key_states,\n-            value_states,\n-            cu_seqlens_q=cu_seqlens_q,\n-            cu_seqlens_k=cu_seqlens_k,\n-            max_seqlen_q=max_seqlen_in_batch_q,\n-            max_seqlen_k=max_seqlen_in_batch_k,\n+        # TODO for now this is required to work with https://huggingface.co/kernels-community/metal-flash-sdpa/blob/main/torch-ext/metal_flash_sdpa/__init__.p\n+        if \"mps\" in str(q.device):\n+            cu_k = cu_k.clone()\n+        out_unpad = flash_varlen_fn(\n+            q,\n+            k,\n+            v,\n+            cu_seqlens_q=cu_q.to(torch.int32),\n+            cu_seqlens_k=cu_k.to(torch.int32),\n+            max_seqlen_q=mq,\n+            max_seqlen_k=mk,\n             softmax_scale=softmax_scale,\n             causal=causal,\n             **flash_kwargs,\n         )\n-        attn_output = _pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n-\n-    elif is_fa2_with_varlen_kwargs or is_fa2_with_position_ids:\n-        batch_size = query_states.size(0)\n-\n+        if isinstance(out_unpad, tuple):\n+            out_unpad = out_unpad[0]\n+        out = pad_fn(out_unpad, idx, query_states.shape[0], query_length)\n+    elif use_mask:\n         if cu_seq_lens_q is None or cu_seq_lens_k is None:\n-            query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = (\n-                _prepare_flash_attention_from_position_ids(query_states, key_states, value_states, position_ids)\n+            if position_ids is None:\n+                raise ValueError(\n+                    \"Position ids should be passed if the attention mask is not passed and the cu_seq-lens are not passed.\"\n+                )\n+            q, k, v, idx, (cu_q, cu_k), (mq, mk) = _prepare_from_posids(\n+                query_states, key_states, value_states, position_ids\n             )\n-\n-            cu_seq_lens_q, cu_seq_lens_k = cu_seq_lens\n-            max_length_q, max_length_k = max_seq_lens\n-\n         else:\n-            query_states = query_states.reshape(-1, query_states.size(-2), query_states.size(-1))\n-            key_states = key_states.reshape(-1, key_states.size(-2), key_states.size(-1))\n-            value_states = value_states.reshape(-1, value_states.size(-2), value_states.size(-1))\n-\n-        attn_output = _flash_attn_varlen_func(\n-            query_states,\n-            key_states,\n-            value_states,\n-            cu_seqlens_q=cu_seq_lens_q,\n-            cu_seqlens_k=cu_seq_lens_k,\n-            max_seqlen_q=max_length_q,\n-            max_seqlen_k=max_length_k,\n+            q = query_states.reshape(-1, query_states.size(-2), query_states.size(-1))\n+            k = key_states.reshape(-1, key_states.size(-2), key_states.size(-1))\n+            v = value_states.reshape(-1, value_states.size(-2), value_states.size(-1))\n+            mq, mk = max_length_q, max_length_k\n+            cu_q, cu_k = cu_seq_lens_q, cu_seq_lens_k\n+        if \"mps\" in str(q.device):\n+            cu_k = cu_k.clone()\n+        out = flash_varlen_fn(\n+            q,\n+            k,\n+            v,\n+            cu_seqlens_q=cu_q.to(torch.int32),\n+            cu_seqlens_k=cu_k.to(torch.int32),\n+            max_seqlen_q=mq,\n+            max_seqlen_k=mk,\n             softmax_scale=softmax_scale,\n             causal=causal,\n             **flash_kwargs,\n         )\n-\n-        attn_output = attn_output.view(batch_size, -1, attn_output.size(-2), attn_output.size(-1))\n-\n+        if isinstance(out, tuple):\n+            out = out[0]\n+        out = out.view(query_states.shape[0], -1, out.size(-2), out.size(-1))\n     else:\n-        attn_output = _flash_attn_func(\n+        out = flash_fn(\n             query_states, key_states, value_states, softmax_scale=softmax_scale, causal=causal, **flash_kwargs\n         )\n \n-    if isinstance(attn_output, tuple):\n-        return attn_output[0]\n-    return attn_output\n-\n-\n-class FlashAttentionKwargs(TypedDict, total=False):\n-    \"\"\"\n-    Keyword arguments for Flash Attention with Compile.\n-\n-    Attributes:\n-        cumulative_seqlens_q (`torch.LongTensor`, *optional*)\n-            Gets cumulative sequence length for query state.\n-        cumulative_seqlens_k (`torch.LongTensor`, *optional*)\n-            Gets cumulative sequence length for key state.\n-        max_length_q (`int`, *optional*):\n-            Maximum sequence length for query state.\n-        max_length_k (`int`, *optional*):\n-            Maximum sequence length for key state.\n-    \"\"\"\n-\n-    cumulative_seqlens_q: Optional[torch.LongTensor]\n-    cumulative_seqlens_k: Optional[torch.LongTensor]\n-    max_length_q: Optional[int]\n-    max_length_k: Optional[int]\n+    return out[0] if isinstance(out, tuple) else out"
        },
        {
            "sha": "f4fd894b320dfbd957d308829686516535a2bee1",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 20,
            "deletions": 11,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/efceeaf2678678553e94dce78859f87776e633a7/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/efceeaf2678678553e94dce78859f87776e633a7/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=efceeaf2678678553e94dce78859f87776e633a7",
            "patch": "@@ -72,6 +72,7 @@\n     verify_tp_plan,\n )\n from .loss.loss_utils import LOSS_MAPPING\n+from .masking_utils import ALL_MASK_ATTENTION_FUNCTIONS\n from .pytorch_utils import (  # noqa: F401\n     Conv1D,\n     apply_chunking_to_forward,\n@@ -2785,30 +2786,38 @@ def _check_and_adjust_attn_implementation(\n             None to sdpa (to potentially eager).\n         \"\"\"\n         applicable_attn_implementation = \"sdpa\" if attn_implementation is None else attn_implementation\n-        if re.match(r\"^[^/:]+/[^/:]+:[^/:]+$\", applicable_attn_implementation):\n+        if re.match(r\"^[^/:]+/[^/:]+:?[^/:]+$\", applicable_attn_implementation):\n             if not is_kernels_available():\n                 raise ValueError(\"kernels is not installed. Please install it with `pip install kernels`.\")\n \n             # Extract repo_id and kernel_name from the string\n-            repo_id, kernel_name = applicable_attn_implementation.split(\":\")\n-            kernel_name = kernel_name.strip()\n+            if \":\" in applicable_attn_implementation:\n+                repo_id, kernel_name = attn_implementation.split(\":\")\n+                kernel_name = kernel_name.strip()\n+            else:\n+                repo_id = attn_implementation\n+                kernel_name = None\n             repo_id = repo_id.strip()\n-\n             try:\n                 kernel = get_kernel(repo_id)\n-                ALL_ATTENTION_FUNCTIONS.register(f\"kernel_{repo_id.replace('/', '_')}\", getattr(kernel, kernel_name))\n-                applicable_attn_implementation = f\"kernel_{repo_id.replace('/', '_')}\"\n+                if hasattr(kernel, \"flash_attn_varlen_func\"):\n+                    ALL_ATTENTION_FUNCTIONS._global_mapping[repo_id] = partial(\n+                        flash_attention_forward, implementation=kernel\n+                    )\n+                elif kernel_name is not None:\n+                    ALL_ATTENTION_FUNCTIONS[repo_id] = getattr(kernel, kernel_name)\n+                ALL_MASK_ATTENTION_FUNCTIONS._global_mapping[repo_id] = ALL_MASK_ATTENTION_FUNCTIONS[\n+                    \"flash_attention_2\"\n+                ]\n+                applicable_attn_implementation = repo_id\n             except FileNotFoundError as e:\n                 logger.warning_once(\n                     f\"Could not find a kernel repository '{repo_id}' compatible with your device in the hub: {e}. Using \"\n                     \"default attention implementation instead (sdpa if available, eager otherwise).\"\n                 )\n                 applicable_attn_implementation = \"sdpa\"  # Try to fallback to sdpa in this case\n-            except AttributeError:\n-                raise ValueError(\n-                    \"the kernel function name or class specified in the attn_implementation argument is not valid. Please check \"\n-                    \"the documentation for the correct format, and check that the kernel exports the class and the function correctly.\"\n-                )\n+            finally:\n+                return applicable_attn_implementation\n         if applicable_attn_implementation not in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys():\n             message = (\n                 f'Specified `attn_implementation=\"{attn_implementation}\"` is not supported. The only possible arguments are '"
        },
        {
            "sha": "0e117d71f712f913af89a79ca63a0f280d404434",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/efceeaf2678678553e94dce78859f87776e633a7/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/efceeaf2678678553e94dce78859f87776e633a7/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=efceeaf2678678553e94dce78859f87776e633a7",
            "patch": "@@ -104,6 +104,7 @@\n     is_jinja_available,\n     is_jumanpp_available,\n     is_keras_nlp_available,\n+    is_kernels_available,\n     is_levenshtein_available,\n     is_librosa_available,\n     is_liger_kernel_available,\n@@ -586,6 +587,16 @@ def require_flash_attn(test_case):\n     return unittest.skipUnless(is_flash_attn_2_available(), \"test requires Flash Attention\")(test_case)\n \n \n+def require_kernels(test_case):\n+    \"\"\"\n+    Decorator marking a test that requires Flash Attention.\n+\n+    These tests are skipped when Flash Attention isn't installed.\n+\n+    \"\"\"\n+    return unittest.skipUnless(is_kernels_available(), \"test requires Flash Attention\")(test_case)\n+\n+\n def require_flash_attn_3(test_case):\n     \"\"\"\n     Decorator marking a test that requires Flash Attention 3.\n@@ -1103,6 +1114,11 @@ def require_torch_gpu(test_case):\n     return unittest.skipUnless(torch_device == \"cuda\", \"test requires CUDA\")(test_case)\n \n \n+def require_torch_mps(test_case):\n+    \"\"\"Decorator marking a test that requires CUDA and PyTorch.\"\"\"\n+    return unittest.skipUnless(torch_device == \"mps\", \"test requires MPS\")(test_case)\n+\n+\n def require_large_cpu_ram(test_case, memory: float = 80):\n     \"\"\"Decorator marking a test that requires a CPU RAM with more than `memory` GiB of memory.\"\"\"\n     if not is_psutil_available():"
        },
        {
            "sha": "f277df1af17ec254305849353dd65dfa0b7ecc9d",
            "filename": "src/transformers/utils/auto_docstring.py",
            "status": "modified",
            "additions": 13,
            "deletions": 6,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/efceeaf2678678553e94dce78859f87776e633a7/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/efceeaf2678678553e94dce78859f87776e633a7/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fauto_docstring.py?ref=efceeaf2678678553e94dce78859f87776e633a7",
            "patch": "@@ -1142,10 +1142,14 @@ def get_placeholders_dict(placeholders: list, model_name: str) -> dict:\n     for placeholder in placeholders:\n         # Infer placeholders from the model name and the auto modules\n         if placeholder in PLACEHOLDER_TO_AUTO_MODULE:\n-            place_holder_value = getattr(\n-                getattr(auto_module, PLACEHOLDER_TO_AUTO_MODULE[placeholder][0]),\n-                PLACEHOLDER_TO_AUTO_MODULE[placeholder][1],\n-            ).get(model_name, None)\n+            try:\n+                place_holder_value = getattr(\n+                    getattr(auto_module, PLACEHOLDER_TO_AUTO_MODULE[placeholder][0]),\n+                    PLACEHOLDER_TO_AUTO_MODULE[placeholder][1],\n+                ).get(model_name, None)\n+            except ImportError:\n+                # In case a library is not installed, we don't want to fail the docstring generation\n+                place_holder_value = None\n             if place_holder_value is not None:\n                 if isinstance(place_holder_value, (list, tuple)):\n                     place_holder_value = place_holder_value[0]\n@@ -1170,8 +1174,11 @@ def format_args_docstring(docstring, model_name):\n     placeholders_dict = get_placeholders_dict(placeholders, model_name)\n     # replace the placeholders in the docstring with the values from the placeholders_dict\n     for placeholder, value in placeholders_dict.items():\n-        docstring = docstring.replace(f\"{{{placeholder}}}\", value)\n-\n+        if placeholder is not None:\n+            try:\n+                docstring = docstring.replace(f\"{{{placeholder}}}\", value)\n+            except Exception:\n+                pass\n     return docstring\n \n "
        },
        {
            "sha": "9c4c0da4ee19c21f635687341a628e2ce7eaaa9e",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 88,
            "deletions": 73,
            "changes": 161,
            "blob_url": "https://github.com/huggingface/transformers/blob/efceeaf2678678553e94dce78859f87776e633a7/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/efceeaf2678678553e94dce78859f87776e633a7/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=efceeaf2678678553e94dce78859f87776e633a7",
            "patch": "@@ -86,12 +86,14 @@\n     require_deepspeed,\n     require_flash_attn,\n     require_flash_attn_3,\n+    require_kernels,\n     require_non_hpu,\n     require_safetensors,\n     require_torch,\n     require_torch_accelerator,\n     require_torch_gpu,\n     require_torch_greater_or_equal,\n+    require_torch_mps,\n     require_torch_multi_accelerator,\n     require_torch_multi_gpu,\n     require_torch_sdpa,\n@@ -3474,94 +3476,107 @@ def flash_attn_inference_equivalence(self, attn_implementation: str, padding_sid\n                 self.skipTest(f\"{model_class.__name__} does not support {attn_implementation}\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            config.head_dim = 64  # fa2 does not always support arbitrary headim\n             model = model_class(config)\n \n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_fa = model_class.from_pretrained(\n-                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=attn_implementation\n-                )\n-                model_fa.to(torch_device)\n+            model.to(torch_device)\n+            dummy_input = inputs_dict[model.main_input_name][:1]\n+            if dummy_input.dtype in [torch.float32, torch.float16]:\n+                dummy_input = dummy_input.to(torch.bfloat16)\n \n-                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.bfloat16)\n-                model.to(torch_device)\n+            dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n \n-                dummy_input = inputs_dict[model.main_input_name][:1]\n-                if dummy_input.dtype in [torch.float32, torch.float16]:\n-                    dummy_input = dummy_input.to(torch.bfloat16)\n+            if dummy_attention_mask is not None:\n+                dummy_attention_mask = dummy_attention_mask[:1]\n+                if padding_side == \"left\":\n+                    dummy_attention_mask[:, 1:] = 1\n+                    dummy_attention_mask[:, :1] = 0\n+                else:\n+                    dummy_attention_mask[:, :-1] = 1\n+                    dummy_attention_mask[:, -1:] = 0\n+            if model.config.is_encoder_decoder:\n+                decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", dummy_input)[:1]\n \n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n+                outputs = model(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n+                model.set_attn_implementation(attn_implementation)\n+                outputs_fa = model(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n+            else:\n+                outputs = model(dummy_input, output_hidden_states=True)\n+                model.set_attn_implementation(attn_implementation)\n+                outputs_fa = model(dummy_input, output_hidden_states=True)\n \n-                if dummy_attention_mask is not None:\n-                    dummy_attention_mask = dummy_attention_mask[:1]\n-                    if padding_side == \"left\":\n-                        dummy_attention_mask[:, 1:] = 1\n-                        dummy_attention_mask[:, :1] = 0\n-                    else:\n-                        dummy_attention_mask[:, :-1] = 1\n-                        dummy_attention_mask[:, -1:] = 0\n-                if model.config.is_encoder_decoder:\n-                    decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", dummy_input)[:1]\n+            model.set_attn_implementation(\"sdpa\")\n+            logits = (\n+                outputs.hidden_states[-1] if not model.config.is_encoder_decoder else outputs.decoder_hidden_states[-1]\n+            )\n+            logits_fa = (\n+                outputs_fa.hidden_states[-1]\n+                if not model.config.is_encoder_decoder\n+                else outputs_fa.decoder_hidden_states[-1]\n+            )\n \n-                    outputs = model(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n-                    outputs_fa = model_fa(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n-                else:\n-                    outputs = model(dummy_input, output_hidden_states=True)\n-                    outputs_fa = model_fa(dummy_input, output_hidden_states=True)\n+            assert torch.allclose(logits_fa, logits, atol=4e-2, rtol=4e-2)\n \n-                logits = (\n-                    outputs.hidden_states[-1]\n-                    if not model.config.is_encoder_decoder\n-                    else outputs.decoder_hidden_states[-1]\n-                )\n-                logits_fa = (\n-                    outputs_fa.hidden_states[-1]\n-                    if not model.config.is_encoder_decoder\n-                    else outputs_fa.decoder_hidden_states[-1]\n-                )\n+            if model.config.is_encoder_decoder:\n+                other_inputs = {\n+                    \"decoder_input_ids\": decoder_input_ids,\n+                    \"decoder_attention_mask\": dummy_attention_mask,\n+                    \"output_hidden_states\": True,\n+                }\n+                if dummy_attention_mask is not None:\n+                    other_inputs[\"attention_mask\"] = dummy_attention_mask\n \n-                assert torch.allclose(logits_fa, logits, atol=4e-2, rtol=4e-2)\n+                outputs = model(dummy_input, **other_inputs)\n+                model.set_attn_implementation(attn_implementation)\n+                outputs_fa = model(dummy_input, **other_inputs)\n+            else:\n+                other_inputs = {\n+                    \"output_hidden_states\": True,\n+                }\n+                if dummy_attention_mask is not None:\n+                    other_inputs[\"attention_mask\"] = dummy_attention_mask\n \n-                if model.config.is_encoder_decoder:\n-                    other_inputs = {\n-                        \"decoder_input_ids\": decoder_input_ids,\n-                        \"decoder_attention_mask\": dummy_attention_mask,\n-                        \"output_hidden_states\": True,\n-                    }\n-                    if dummy_attention_mask is not None:\n-                        other_inputs[\"attention_mask\"] = dummy_attention_mask\n+                outputs = model(dummy_input, **other_inputs)\n+                model.set_attn_implementation(attn_implementation)\n+                outputs_fa = model(dummy_input, **other_inputs)\n \n-                    outputs = model(dummy_input, **other_inputs)\n-                    outputs_fa = model_fa(dummy_input, **other_inputs)\n-                else:\n-                    other_inputs = {\n-                        \"output_hidden_states\": True,\n-                    }\n-                    if dummy_attention_mask is not None:\n-                        other_inputs[\"attention_mask\"] = dummy_attention_mask\n+            model.set_attn_implementation(\"sdpa\")\n+            logits = (\n+                outputs.hidden_states[-1] if not model.config.is_encoder_decoder else outputs.decoder_hidden_states[-1]\n+            )\n+            logits_fa = (\n+                outputs_fa.hidden_states[-1]\n+                if not model.config.is_encoder_decoder\n+                else outputs_fa.decoder_hidden_states[-1]\n+            )\n \n-                    outputs = model(dummy_input, **other_inputs)\n-                    outputs_fa = model_fa(dummy_input, **other_inputs)\n+            if padding_side == \"left\":\n+                assert torch.allclose(logits_fa[1:], logits[1:], atol=4e-2, rtol=4e-2)\n \n-                logits = (\n-                    outputs.hidden_states[-1]\n-                    if not model.config.is_encoder_decoder\n-                    else outputs.decoder_hidden_states[-1]\n-                )\n-                logits_fa = (\n-                    outputs_fa.hidden_states[-1]\n-                    if not model.config.is_encoder_decoder\n-                    else outputs_fa.decoder_hidden_states[-1]\n-                )\n+                # check with inference + dropout\n+                model.train()\n+                model.set_attn_implementation(attn_implementation)\n+                _ = model(dummy_input, **other_inputs)\n+            else:\n+                assert torch.allclose(logits_fa[:-1], logits[:-1], atol=4e-2, rtol=4e-2)\n \n-                if padding_side == \"left\":\n-                    assert torch.allclose(logits_fa[1:], logits[1:], atol=4e-2, rtol=4e-2)\n+    @require_kernels\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    @slow\n+    @is_flaky()\n+    def test_flash_attn_kernels_inference_equivalence(self):\n+        self.flash_attn_inference_equivalence(attn_implementation=\"kernels-community/flash-attn3\", padding_side=\"left\")\n \n-                    # check with inference + dropout\n-                    model.train()\n-                    _ = model_fa(dummy_input, **other_inputs)\n-                else:\n-                    assert torch.allclose(logits_fa[:-1], logits[:-1], atol=4e-2, rtol=4e-2)\n+    @require_torch_mps\n+    @require_kernels\n+    @mark.flash_attn_test\n+    @slow\n+    @is_flaky()\n+    def test_flash_attn_kernels_mps_inference_equivalence(self):\n+        self.flash_attn_inference_equivalence(\n+            attn_implementation=\"kernels-community/metal-flash-sdpa\", padding_side=\"left\"\n+        )\n \n     @require_flash_attn\n     @require_torch_gpu"
        }
    ],
    "stats": {
        "total": 745,
        "additions": 330,
        "deletions": 415
    }
}