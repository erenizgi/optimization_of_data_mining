{
    "author": "jla524",
    "message": "Fix Qwen2 failing tests (#34819)\n\n* fix: qwen2 model ids\r\n\r\n* fix: line\r\n\r\n* fix: more format\r\n\r\n* update: reformat",
    "sha": "f4c04ba32bad116850bac3fd3caaf9161a507438",
    "files": [
        {
            "sha": "6c32a66e03626c78aa6ea8c80451c3f059cf4910",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 22,
            "deletions": 20,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/f4c04ba32bad116850bac3fd3caaf9161a507438/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f4c04ba32bad116850bac3fd3caaf9161a507438/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=f4c04ba32bad116850bac3fd3caaf9161a507438",
            "patch": "@@ -440,15 +440,15 @@ class Qwen2IntegrationTest(unittest.TestCase):\n     @slow\n     def test_model_450m_logits(self):\n         input_ids = [1, 306, 4658, 278, 6593, 310, 2834, 338]\n-        model = Qwen2ForCausalLM.from_pretrained(\"Qwen/Qwen2-450m-beta\", device_map=\"auto\")\n+        model = Qwen2ForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B\", device_map=\"auto\")\n         input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n         with torch.no_grad():\n             out = model(input_ids).logits.float().cpu()\n         # Expected mean on dim = -1\n-        EXPECTED_MEAN = torch.tensor([[-2.5548, -2.5737, -3.0600, -2.5906, -2.8478, -2.8118, -2.9325, -2.7694]])\n+        EXPECTED_MEAN = torch.tensor([[-1.9537, -1.6193, -1.4123, -1.4673, -1.8511, -1.9309, -1.9826, -2.1776]])\n         torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, atol=1e-2, rtol=1e-2)\n         # slicing logits[0, 0, 0:30]\n-        EXPECTED_SLICE = torch.tensor([-5.8781, -5.8616, -0.1052, -4.7200, -5.8781, -5.8774, -5.8773, -5.8777, -5.8781, -5.8780, -5.8781, -5.8779, -1.0787,  1.7583, -5.8779, -5.8780, -5.8783, -5.8778, -5.8776, -5.8781, -5.8784, -5.8778, -5.8778, -5.8777, -5.8779, -5.8778, -5.8776, -5.8780, -5.8779, -5.8781])  # fmt: skip\n+        EXPECTED_SLICE = torch.tensor([3.2025, 7.1265, 4.6058, 3.6423, 1.6357, 3.9265, 5.1883, 5.8760, 2.7942, 4.4823, 3.2571, 2.1063, 3.4275, 4.2028, 1.9767, 5.2115, 6.6756, 6.3999, 6.0483, 5.7378, 5.6660, 5.2298, 5.4103, 5.1248, 5.4376, 2.4570, 2.6107, 5.4039, 2.8077, 4.7777])  # fmt: skip\n         print(out[0, 0, :30])\n         torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, atol=1e-4, rtol=1e-4)\n \n@@ -458,10 +458,12 @@ def test_model_450m_logits(self):\n \n     @slow\n     def test_model_450m_generation(self):\n-        EXPECTED_TEXT_COMPLETION = \"\"\"My favourite condiment is 100% ketchup. I love it on everything. I’m not a big\"\"\"\n+        EXPECTED_TEXT_COMPLETION = (\n+            \"\"\"My favourite condiment is 100% natural, organic and vegan. I love to use it in my cooking and I\"\"\"\n+        )\n         prompt = \"My favourite condiment is \"\n-        tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-450m-beta\", use_fast=False)\n-        model = Qwen2ForCausalLM.from_pretrained(\"Qwen/Qwen2-450m-beta\", device_map=\"auto\")\n+        tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\", use_fast=False)\n+        model = Qwen2ForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B\", device_map=\"auto\")\n         input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.model.embed_tokens.weight.device)\n \n         # greedy generation outputs\n@@ -482,7 +484,7 @@ def test_model_450m_long_prompt(self):\n         # An input with 4097 tokens that is above the size of the sliding window\n         input_ids = [1] + [306, 338] * 2048\n         model = Qwen2ForCausalLM.from_pretrained(\n-            \"Qwen/Qwen2-450m-beta\",\n+            \"Qwen/Qwen2-0.5B\",\n             device_map=\"auto\",\n             load_in_4bit=True,\n             attn_implementation=\"flash_attention_2\",\n@@ -509,11 +511,7 @@ def test_model_450m_long_prompt_sdpa(self):\n         EXPECTED_OUTPUT_TOKEN_IDS = [306, 338]\n         # An input with 4097 tokens that is above the size of the sliding window\n         input_ids = [1] + [306, 338] * 2048\n-        model = Qwen2ForCausalLM.from_pretrained(\n-            \"Qwen/Qwen2-450m-beta\",\n-            device_map=\"auto\",\n-            attn_implementation=\"sdpa\",\n-        )\n+        model = Qwen2ForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B\", device_map=\"auto\", attn_implementation=\"sdpa\")\n         input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n         generated_ids = model.generate(input_ids, max_new_tokens=4, temperature=0)\n         self.assertEqual(EXPECTED_OUTPUT_TOKEN_IDS, generated_ids[0][-2:].tolist())\n@@ -530,9 +528,11 @@ def test_model_450m_long_prompt_sdpa(self):\n         backend_empty_cache(torch_device)\n         gc.collect()\n \n-        EXPECTED_TEXT_COMPLETION = \"\"\"My favourite condiment is 100% ketchup. I love it on everything. I’m not a big\"\"\"\n+        EXPECTED_TEXT_COMPLETION = (\n+            \"\"\"My favourite condiment is 100% natural, organic and vegan. I love to use it in my cooking and I\"\"\"\n+        )\n         prompt = \"My favourite condiment is \"\n-        tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-450m-beta\", use_fast=False)\n+        tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\", use_fast=False)\n \n         input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.model.embed_tokens.weight.device)\n \n@@ -544,13 +544,13 @@ def test_model_450m_long_prompt_sdpa(self):\n     @slow\n     def test_speculative_generation(self):\n         EXPECTED_TEXT_COMPLETION = (\n-            \"My favourite condiment is 100% Sriracha. I love the heat, the tang and the fact costs\"\n+            \"My favourite condiment is 100% natural honey, and I always like to use it in my recipes. I love\"\n         )\n         prompt = \"My favourite condiment is \"\n-        tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-beta\", use_fast=False)\n-        model = Qwen2ForCausalLM.from_pretrained(\"Qwen/Qwen2-450m-beta\", device_map=\"auto\", torch_dtype=torch.float16)\n+        tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B\", use_fast=False)\n+        model = Qwen2ForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B\", device_map=\"auto\", torch_dtype=torch.float16)\n         assistant_model = Qwen2ForCausalLM.from_pretrained(\n-            \"Qwen/Qwen2-450m-beta\", device_map=\"auto\", torch_dtype=torch.float16\n+            \"Qwen/Qwen2-0.5B\", device_map=\"auto\", torch_dtype=torch.float16\n         )\n         input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.model.embed_tokens.weight.device)\n \n@@ -576,10 +576,12 @@ def test_export_static_cache(self):\n             convert_and_export_with_cache,\n         )\n \n-        qwen_model = \"Qwen/Qwen2.5-0.5B\"\n+        qwen_model = \"Qwen/Qwen2-0.5B\"\n \n         tokenizer = AutoTokenizer.from_pretrained(qwen_model, pad_token=\"</s>\", padding_side=\"right\")\n-        EXPECTED_TEXT_COMPLETION = [\"My favourite condiment is 100% sugar. I have a jar of 1000 grams of sugar. I use\"]\n+        EXPECTED_TEXT_COMPLETION = [\n+            \"My favourite condiment is 100% natural, organic, gluten free, vegan, and free from preservatives. I\"\n+        ]\n         max_generation_length = tokenizer(EXPECTED_TEXT_COMPLETION, return_tensors=\"pt\", padding=True)[\n             \"input_ids\"\n         ].shape[-1]"
        }
    ],
    "stats": {
        "total": 42,
        "additions": 22,
        "deletions": 20
    }
}