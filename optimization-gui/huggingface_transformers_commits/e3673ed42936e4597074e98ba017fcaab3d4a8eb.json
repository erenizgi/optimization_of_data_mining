{
    "author": "stevhliu",
    "message": "[docs] Attention backends + continuous batching (#42329)\n\n* attention\n\n* continuous batching\n\n* fix import\n\n* feedback\n\n* link to blog post\n\n* feedback\n\n* fix import",
    "sha": "e3673ed42936e4597074e98ba017fcaab3d4a8eb",
    "files": [
        {
            "sha": "5ca0b720c1238748487aedea895185f978005d4f",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 20,
            "deletions": 23,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3673ed42936e4597074e98ba017fcaab3d4a8eb/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3673ed42936e4597074e98ba017fcaab3d4a8eb/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=e3673ed42936e4597074e98ba017fcaab3d4a8eb",
            "patch": "@@ -23,8 +23,6 @@\n       title: Legacy model contribution\n     - local: auto_docstring\n       title: Documenting a model\n-    - local: attention_interface\n-      title: Customizing attention function\n     title: Models\n   - sections:\n     - local: fast_tokenizers\n@@ -61,11 +59,29 @@\n     - local: llm_tutorial\n       title: Text generation\n     - local: generation_strategies\n-      title: Generation strategies\n+      title: Decoding methods\n     - local: generation_features\n       title: Generation features\n     - local: tasks/prompting\n       title: Prompt engineering\n+    - local: perplexity\n+      title: Perplexity of fixed-length models\n+    title: Generate API\n+  - sections:\n+    - local: attention_interface\n+      title: Attention backends\n+    - local: continuous_batching\n+      title: Continuous batching\n+    - local: kernel_doc/overview\n+      title: Kernels in transformers\n+    - local: perf_torch_compile\n+      title: torch.compile\n+    - local: perf_infer_gpu_one\n+      title: GPU\n+    - local: perf_infer_gpu_multi\n+      title: Distributed inference\n+    - local: perf_infer_cpu\n+      title: CPU\n     - local: llm_optims\n       title: Optimizing inference\n     - local: cache_explanation\n@@ -74,9 +90,7 @@\n       title: KV cache strategies\n     - local: llm_tutorial_optimization\n       title: Getting the most out of LLMs\n-    - local: perplexity\n-      title: Perplexity of fixed-length models\n-    title: LLMs\n+    title: Optimization\n   - sections:\n     - local: conversations\n       title: Chat basics\n@@ -101,24 +115,12 @@\n     - local: open_webui\n       title: Open WebUI\n     title: Serving\n-  - sections:\n-    - local: perf_torch_compile\n-      title: torch.compile\n-    - local: perf_infer_gpu_one\n-      title: GPU\n-    - local: perf_infer_gpu_multi\n-      title: Distributed inference\n-    - local: perf_infer_cpu\n-      title: CPU\n-    title: Optimization\n   - local: agents\n     title: Agents\n   - local: tools\n     title: Tools\n   - local: transformers_as_backend\n     title: Transformers as modeling backend\n-  - local: continuous_batching\n-    title: Continuous Batching\n   title: Inference\n - isExpanded: false\n   sections:\n@@ -218,11 +220,6 @@\n   - local: quantization/contribute\n     title: Contribute\n   title: Quantization\n-- isExpanded: false\n-  sections:\n-  - local: kernel_doc/overview\n-    title: Kernels in transformers\n-  title: Kernels\n - isExpanded: false\n   sections:\n   - local: serialization"
        },
        {
            "sha": "59fddbabff9f7fbcb91aaaaea9b1921f7df09d2d",
            "filename": "docs/source/en/attention_interface.md",
            "status": "modified",
            "additions": 105,
            "deletions": 90,
            "changes": 195,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3673ed42936e4597074e98ba017fcaab3d4a8eb/docs%2Fsource%2Fen%2Fattention_interface.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3673ed42936e4597074e98ba017fcaab3d4a8eb/docs%2Fsource%2Fen%2Fattention_interface.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fattention_interface.md?ref=e3673ed42936e4597074e98ba017fcaab3d4a8eb",
            "patch": "@@ -13,103 +13,145 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Attention Interface\n+# Attention backends\n \n-This page describes how to use the `AttentionInterface` in order to register custom attention functions to use with\n-supported models.\n+All attention implementations perform the same computation. Every token is compared to every other token. The difference is *how* the computation is performed. Basic attention scales poorly because it materializes the full attention matrix in memory, creating bottlenecks that slow down inference. Optimized implementations rearrange the math to reduce memory traffic for faster, more affordable inference.\n \n-## Customizing attention function\n+The [`AttentionInterface`] provides optimized attention implementations. It decouples the attention implementation from the model implementation to simplify experimentation with different functions. Add new backends easily with this consistent interface.\n \n-Most recent models can now switch from one attention function used in the Attention layer to the other, thanks to a simple mapping.\n-By default, we provide the implementation for [`sdpa`](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html),\n-[`flash_attention_2`](https://github.com/Dao-AILab/flash-attention) and [`flex_attention`](https://pytorch.org/docs/stable/nn.attention.flex_attention.html#module-torch.nn.attention.flex_attention)\n-as well as `eager`, which is a simple matrix multiplication without any optimization on top.  \n-This is the setting you can usually choose when instantiating a model:\n+| attention backend | description |\n+|---|---|\n+| `\"flash_attention_3\"` | improves FlashAttention-2 by also overlapping operations and fusing forward and backward passes more tightly |\n+| `\"flash_attention_2\"` | tiles computations into smaller blocks and uses fast on-chip memory |\n+| `\"flex_attention\"` | framework for specifying custom attention patterns (sparse, block-local, sliding window) without writing low-level kernels by hand |\n+| `\"sdpa\"` | built-in PyTorch implementation of [scaled dot product attention](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) |\n+| <code>\"paged&#124;flash_attention_2\"</code> | Paged version of FlashAttention-2 |\n+| <code>\"paged&#124;sdpa\"</code> | Paged version of SDPA |\n+| <code>\"paged&#124;eager\"</code> | Paged version of eager |\n \n-```python\n-from transformers import AutoModelForCausalLM\n+## Set an attention backend\n \n-model_id = \"meta-llama/Llama-3.2-1B\"\n+Use the `attn_implementation` argument in [`~PreTrainedModel.from_pretrained`] to instantiate a model with a specific attention function.\n+\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM\n \n-# Here, using flash attention as an example\n-model = AutoModelForCausalLM.from_pretrained(model_id, attn_implementation=\"flash_attention_2\")\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"meta-llama/Llama-3.2-1B\", attn_implementation=\"flash_attention_2\"\n+)\n ```\n \n-But what if you wanted to create your own attention function? Or simply play around with existing ones, adding\n-a few statements here and there? You can now do so with the `AttentionInterface`! Here is an example:\n+Switch between attention backends at runtime without reloading the model using [`~PreTrainedModel.set_attn_implementation`].\n \n-```python\n-from transformers import AutoModelForCausalLM, AttentionInterface\n-from transformers.integrations.sdpa_attention import sdpa_attention_forward\n-import torch\n+```py\n+model.set_attn_implementation(\"sdpa\")\n+```\n \n-model_id = \"meta-llama/Llama-3.2-1B\"\n+### Kernels\n \n-def my_new_sdpa(*args, **kwargs):\n-    print(\"I just entered the attention computation\")\n-    return sdpa_attention_forward(*args, **kwargs)\n+Download and load compiled compute kernels directly from the [Hub](https://huggingface.co/models?other=kernels) at runtime with the [Kernels](https://huggingface.co/docs/kernels/index) library. This avoids packaging issues from mismatched PyTorch or CUDA versions.\n \n-AttentionInterface.register(\"my_new_sdpa\", my_new_sdpa)\n+Kernels automatically register to [`AttentionInterface`] upon detection. You don't need to install the FlashAttention package explicitly.\n \n-model = AutoModelForCausalLM.from_pretrained(model_id, attn_implementation=\"my_new_sdpa\")\n-# Try running the forward with the new attention function\n-model(torch.ones(1, 5, dtype=int))\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM\n+\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"meta-llama/Llama-3.2-1B\", attn_implementation=\"kernels-community/flash-attn2\"\n+)\n ```\n \n-You will see it prints \"I just entered the attention computation\" as many times as there are layers in the model (with this example, 16 times).\n+### SDPA context manager\n \n-## Dynamically switching attention function\n+PyTorch's scaled dot product attention (SDPA) selects the fastest attention function for CUDA backends automatically. It defaults to the PyTorch C++ implementation for other backends.\n \n-You could dynamically change the model's attention function as well:\n+Force SDPA to use a specific implementation with the [torch.nn.attention.sdpa_kernel](https://pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html) context manager.\n \n-```python\n-# Back to use original sdpa implementation\n-model.set_attn_implementation(\"sdpa\")\n+```py\n+import torch\n+from torch.nn.attention import SDPBackend, sdpa_kernel\n+from transformers import AutoModelForCausalLM\n \n-model(torch.ones(1, 5, dtype=int))\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"meta-llama/Llama-3.2-1B\", attn_implementation=\"sdpa\"\n+)\n+\n+with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n+    outputs = model.generate(**inputs)\n ```\n \n-and it will stop printing the statements, as it now uses the `sdpa` attention.  \n-This allows to quickly change an attention function, without needing to reload the model!\n+## Backbone-specific attention\n \n-## Different attention per backbone in multimodal models\n+Multimodal models use different backbones for each modality. Optimize performance by assigning specific attention functions to each backbone. Some vision backbones perform better in fp32, for example, which FlashAttention does not support.\n \n-For multimodal models different attention functions may work better for each backbone module. For example, some vision backbones perform better in fp32, but are incompatible with FlashAttention. To continue using FlashAttention while keeping the vision encoder in fp32, create a dict and map each config to an attention implementation as shown below.\n+Map vision backbones to different attention functions with a dict while the text backbone continues to use FlashAttention. Keys in the attention implementation must match sub-config names.\n \n-```python\n+```py\n from transformers import AutoModelForImageTextToText\n \n-model_id = \"facebook/chameleon-7b\"\n-\n attention_implementation_per_backbone = {\"vision_config\": \"sdpa\", \"text_config\": \"flash_attention_2\"}\n-model = AutoModelForImageTextToText.from_pretrained(model_id, attn_implementation=attention_implementation_per_backbone)\n \n-# NOTE: keys in the attention implementation have to be the same as the sub-config names\n for key in attention_implementation_per_backbone:\n     assert key in model.config.sub_configs, f\"Invalid key in `attention_implementation`\"\n \n-# You can omit certain backbones - the default attention function (SDPA) will be used\n-# This is equivalent to the previous example\n-model = AutoModelForImageTextToText.from_pretrained(model_id, attn_implementation={\"text_config\": \"flash_attention_2\"})\n+model = AutoModelForImageTextToText.from_pretrained(\n+    \"facebook/chameleon-7b\", attn_implementation=attention_implementation_per_backbone\n+)\n+```\n+\n+Omit certain backbones from the dict to use the default attention function (SDPA).\n+\n+```py\n+model = AutoModelForImageTextToText.from_pretrained(\n+    \"facebook/chameleon-7b\", attn_implementation={\"text_config\": \"flash_attention_2\"}\n+)\n+```\n+\n+Set the same attention function for all backbones with a single string.\n \n+```py\n+model = AutoModelForImageTextToText.from_pretrained(\n+    \"facebook/chameleon-7b\", attn_implementation=\"eager\"\n+)\n+```\n \n-# Set the same attention implementation for all backbones with single string, same as in non-multimodal models\n-model = AutoModelForImageTextToText.from_pretrained(model_id, attn_implementation=\"eager\")\n+Set the attention function globally with an empty key.\n \n-# Alternatively use a dict with an empty key for global configuration\n-model = AutoModelForImageTextToText.from_pretrained(model_id, attn_implementation={\"\": \"eager\"})\n+```py\n+model = AutoModelForImageTextToText.from_pretrained(\n+    \"facebook/chameleon-7b\", attn_implementation={\"\": \"eager\"}\n+)\n ```\n \n-## What about new args needed in my custom attention function?\n+## Create a new attention function\n+\n+Customize or create new attention functions by adding them to the attention registry with [`AttentionInterface.register`]. Models use these functions through the `attn_implementation` argument.\n \n-But indeed, what if the new function requires a new arg to be properly used? It's no issue! Models supporting the\n-`AttentionInterface` propagate kwargs all the way to the Attention layers, and to the used attention function. That way,\n-you can simply pass the arg (as a kwargs, i.e. you need to qualify the name of the arg) in the model's forward, and it will be correctly used in the attention. However, custom attention functions have some limitations. In particular, it must follow the signature and return format of other attention functions, i.e.\n+This example customizes the attention function to print a statement for each layer.\n \n ```python\n+import torch\n from transformers import AutoModelForCausalLM, AttentionInterface\n from transformers.integrations.sdpa_attention import sdpa_attention_forward\n+\n+def my_new_sdpa(*args, **kwargs):\n+    print(\"I just entered the attention computation\")\n+    return sdpa_attention_forward(*args, **kwargs)\n+\n+AttentionInterface.register(\"my_new_sdpa\", my_new_sdpa)\n+\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", attn_implementation=\"my_new_sdpa\")\n+model(torch.ones(1, 5, dtype=int))\n+```\n+\n+You can also add new arguments to the attention function. Models supporting [`AttentionInterface`] propagate kwargs to attention layers and the attention function. Pass arguments as kwargs in the model's forward function. Custom attention functions must follow this signature and return format.\n+\n+```python\n import torch\n+from transformers import AutoModelForCausalLM, AttentionInterface\n+from transformers.integrations.sdpa_attention import sdpa_attention_forward\n \n def custom_attention(\n     module: torch.nn.Module,  # required arg\n@@ -127,44 +169,19 @@ def custom_attention(\n AttentionInterface.register(\"custom\", custom_attention)\n \n model = AutoModelForCausalLM.from_pretrained(model_id, attn_implementation=\"custom\")\n-# Forward pass with the new kwargs\n model(torch.ones(1, 5, dtype=int), a_new_kwargs=..., another_new_kwargs=...)\n ```\n \n-If in doubt about what args/kwargs a given model sends to the attention function, simply check that model's modeling code on [GitHub](https://github.com/huggingface/transformers/tree/main/src/transformers/models)!\n+Check a model's [modeling code](https://github.com/huggingface/transformers/tree/main/src/transformers/models) to confirm what arguments and kwargs it sends to the attention function.\n \n-## Accessing current available implementations\n+### AttentionMaskInterface\n \n-Most of the time, you will simply need to `register` a new function. If, however, you need to access an existing one,\n-and/or perform a few checks, the preferred way is to use the global `ALL_ATTENTION_FUNCTIONS`. It behaves the same way you\n-would expect from a usual Python dictionary:\n-\n-```python\n->>> from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS\n-\n->>> list(ALL_ATTENTION_FUNCTIONS.keys())\n->>> ['flash_attention_2', 'flex_attention', 'sdpa']\n-\n->>> ALL_ATTENTION_FUNCTIONS[\"sdpa\"]\n->>> <function transformers.integrations.sdpa_attention.sdpa_attention_forward>\n-\n->>> ALL_ATTENTION_FUNCTIONS.get(\"sdpa\", None)\n->>> <function transformers.integrations.sdpa_attention.sdpa_attention_forward>\n-\n-# You can also globally `register` a new function directly on it\n->>> ALL_ATTENTION_FUNCTIONS.register(\"new_func\", new_func)\n-```\n-\n-## Attention Mask Interface\n-\n-Having a new attention function may mean that you need a new format of attention mask to decide what key and value tokens\n-the query tokens should attend to. This is now possible with the `AttentionMaskInterface`! It works in the same way as\n-the `AttentionInterface`:\n+Configure which key and value tokens queries attend to with [`AttentionMaskInterface`]. Some attention functions require this configuration. Customize the attention mask function and add it to the registry with [`AttentionMaskInterface.register`].\n \n ```python\n+import torch\n from transformers import AttentionMaskInterface\n from transformers.masking_utils import sdpa_mask\n-import torch\n \n def my_new_sdpa_mask(*args, **kwargs):\n     print(\"I just entered the attention mask computation\")\n@@ -173,11 +190,9 @@ def my_new_sdpa_mask(*args, **kwargs):\n AttentionMaskInterface.register(\"my_new_sdpa_mask\", my_new_sdpa_mask)\n ```\n \n-The reason you have to register it is because we need to automatically correct your mask format based on the attention implementation (for example, flex attention uses a BlockMask format, while sdpa uses a 4D tensor).\n-By default, if you do not register an attention mask function along with your attention function, mask creation will be skipped\n-and `attention_mask=None` will be passed along to the Attention layers.\n+Registered attention masks automatically correct the mask format for the attention implementation. For example, FlexAttention uses a [BlockMask](https://docs.pytorch.org/docs/stable/nn.attention.flex_attention.html?utm_source=chatgpt.com#torch.nn.attention.flex_attention.BlockMask) format, while SDPA uses a 4D tensor. Without a registered attention mask function, mask creation is skipped and `attention_mask=None` passes to the model's attention layers.\n \n-The default signature of the attention mask functions is the following:\n+This is the default signature for an attention mask function.\n \n ```python\n def custom_attention_mask(\n@@ -191,6 +206,6 @@ def custom_attention_mask(\n ) -> Optional[torch.Tensor]:\n ```\n \n-It mostly works thanks to the `mask_function`, which is a `Callable` in the form of [torch's mask_mod functions](https://pytorch.org/blog/flexattention/), taking 4 indices as input and returning a boolean to indicate if this position should take part in the attention computation.\n+The `mask_function` argument is a `Callable` that mimics PyTorch's [mask_mod](https://pytorch.org/blog/flexattention/) functions. It takes 4 indices as input and returns a boolean. This boolean indicates if the position contributes to the attention computation.\n \n-If you cannot use the `mask_function` to create your mask for some reason, you can try to work around it by doing something similar to our [torch export workaround](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/executorch.py).\n+Use this [workaround](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/executorch.py) for torch export if `mask_function` fails to create a mask."
        },
        {
            "sha": "9c8efd234c4819a32cee87a5176d52c06b64972e",
            "filename": "docs/source/en/continuous_batching.md",
            "status": "modified",
            "additions": 77,
            "deletions": 97,
            "changes": 174,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3673ed42936e4597074e98ba017fcaab3d4a8eb/docs%2Fsource%2Fen%2Fcontinuous_batching.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3673ed42936e4597074e98ba017fcaab3d4a8eb/docs%2Fsource%2Fen%2Fcontinuous_batching.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcontinuous_batching.md?ref=e3673ed42936e4597074e98ba017fcaab3d4a8eb",
            "patch": "@@ -14,64 +14,42 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Continuous Batching\n+# Continuous batching\n \n-Continuous Batching (CB) is an advanced technique to optimize the inference of transformer models by dynamically grouping multiple requests into batches. This approach maximizes GPU utilization and throughput, specifically for workloads with many variable-length inputs.\n+Continuous batching maximizes GPU utilization. It increases throughput and reduces latency by using dynamic scheduling to rearrange the batch at each step. The system removes completed requests and adds new requests immediately to prevent GPU idling. Chunked prefill prevents expensive prefill work from stalling the batch while still allowing new requests still join.\n \n-We are particularly interested in having Continuous Batching in transformers for the following use cases:\n-- Evaluation of models on large datasets with variable-length inputs\n-- Generating outputs for multiple sequences for GRPO policies\n+Continuous batching works with [transformers serve](./serving), a server for deploying local models, and [`~ContinuousMixin.generate_batch`].\n \n-CB is what makes inference engines like vLLM or SGLang efficient. That being said, transformers does not aim to be a production-ready inference engine, but a complete framework for model development. For this reason, CB is available in `transformers serve`.\n+## generate_batch\n \n-If you are not familiar with some of the core concepts CB is built upon, we invite you to read the associated blog post: [Continuous Batching: Efficient Inference for Large Language Models](https://huggingface.co/blog/continuous-batching). _broken link for now_\n-\n-## API Reference\n-\n-## Usage Examples\n-\n-The main way to use CB in transformers is via the `generate_batch` method.\n-\n-Unlike `generate`, CB takes already tokenized inputs, known as input IDs. Each sequence of input IDs is represented as a list of integers, in python: `list[int]`. Since \n-\n-For a more detailed example, please refer to: [examples/continuous_batching](./path/to/example)\n-\n-### `generate_batch` example\n-\n-We have created a `ContinuousMixin` that is inherited by the `GenerationMixin` so that all auto regressive text models support CB.\n-\n-This adds the `generate_batch` method to all models that inherit from `GenerationMixin`.\n-\n-You can use it as follows:\n+The [`~ContinuousMixin.generate_batch`] method works with all autoregressive text models. It accepts a list of tokenized inputs and a [`GenerationConfig`] to configure generation settings.\n \n ```py\n import datasets\n import torch\n-\n from transformers import AutoModelForCausalLM, AutoTokenizer\n from transformers.generation import GenerationConfig\n \n model = AutoModelForCausalLM.from_pretrained(\n     \"Qwen/Qwen3-4B-Instruct-2507\",\n-    attn_implementation=\"spda_paged\",\n-    device_map=\"cuda\",  # if you need cuda\n+    attn_implementation=\"sdpa_paged\",\n+    device_map=\"cuda\",\n     dtype=torch.bfloat16,\n )\n-tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n+tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B-Instruct-2507\", padding_side=\"left\")\n \n-# prepare a batch of inputs\n dataset = datasets.load_dataset(\"openai/gsm8k\", \"socratic\", split=\"test\")\n dataset = dataset.select(range(args.samples))\n tokenized_datasets = dataset.map(lambda x: tokenizer(x[\"question\"]), batched=True)\n simple_batch_inputs = [item[\"input_ids\"] for item in tokenized_datasets]\n \n generation_config = GenerationConfig(\n     max_new_tokens=32,\n-    use_cuda_graph=False,  # Not supported for simple version\n+    use_cuda_graph=False,\n     eos_token_id=tokenizer.eos_token_id,\n     pad_token_id=tokenizer.pad_token_id,\n     do_sample=False,\n-    max_batch_tokens=512,  # max number of tokens in a batch, this is just a default value you should tune based on your hardware\n+    max_batch_tokens=512,\n )\n \n batch_outputs = model.generate_batch(\n@@ -84,59 +62,45 @@ for request_id, output in batch_outputs.items():\n     print(f\"Request {request_id} output: {generated_text}\")\n ```\n \n-### `ContinuousBatchingManager` example\n-\n-If you want more control w.r.t. how you want to schedule requests using CB, you can use the `ContinuousBatchingManager` class directly.\n-\n-This is what we use in `transformers serve` because requests arrive asynchronously and we can leverage the asynchronous nature of the CB process to make things more efficient.\n+## ContinuousBatchingManager\n \n-Under the hood, the `ContinuousBatchingManager` creates a background thread that receives inputs from a python `queue.Queue` which it uses to get requests to batch in each forward pass.\n+The [`ContinuousBatchingManager`] orchestrates the background thread by pulling requests from the queue and filling the GPU to capacity. Every iteration checks for finished requests and schedules new ones to join the batch. Use this manager to customize request scheduling.\n \n-Note that the manager is thread safe!\n+Call [`~ContinuousMixin.init_continuous_batching`] to initialize the manager with a [`GenerationConfig`] and [`~ContinuousBatchingManager.start`] the background thread.\n \n ```py\n-import datasets\n-import torch\n-\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n-from transformers.generation import GenerationConfig\n from transformers.generation.continuous_batching import RequestStatus\n \n-model = AutoModelForCausalLM.from_pretrained(\n-    \"Qwen/Qwen3-4B-Instruct-2507\",\n-    attn_implementation=\"spda_paged\",\n-    device_map=\"cuda\",  # if you need cuda\n-    dtype=torch.bfloat16,\n-)\n-tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n-\n-# prepare a batch of inputs\n-dataset = datasets.load_dataset(\"openai/gsm8k\", \"socratic\", split=\"test\")\n-dataset = dataset.select(range(args.samples))\n-tokenized_datasets = dataset.map(lambda x: tokenizer(x[\"question\"]), batched=True)\n-simple_batch_inputs = [item[\"input_ids\"] for item in tokenized_datasets]\n-\n-# initialize the manager, available method thanks to the `ContinuousMixin`\n manager = model.init_continuous_batching(generation_config=generation_config)\n-\n-# start the background thread\n manager.start()\n+```\n \n-# this is for demonstration purposes only, in practice this is most useful to do concurrently\n-for i, input in enumerate(simple_batch_inputs):\n-    request_id = manager.add_request(input_ids=input, request_id=f\"request_{i}\")  # if you do not specify a request_id, one will be generated for you\n+Use [`~ContinuousBatchingManager.add_request`] to asynchronously submit individual requests. Provide a specific request id or the manager wgenerates one automatically.\n \n-# Can be done in an other thread\n-for id, request in manager.get_result():\n+```py\n+for i, input_ids in enumerate(simple_batch_inputs):\n+    request_id = manager.add_request(input_ids=input_ids, request_id=f\"request_{i}\")\n+```\n+\n+Retrieve *all* results as they arrive with [`~ContinuousBatchingManager.get_result`].\n+\n+```py\n+for request_id, request in manager.get_result():\n     generated_text = tokenizer.decode(request.generated_tokens, skip_special_tokens=True)\n-    print(f\"Request {id} output: {generated_text}\")\n+    print(f\"Request {request_id} output: {generated_text}\")\n+```\n \n-# you can also get results for a specific request id\n-result = manager.get_result(request_id=\"request_5\")  # this is blocking and will wait for the result to be ready\n+Use the `request_id` of a specific request to get its results. This is a blocking operation that waits until the result is ready.\n \n-# or get results for a request that is streaming\n+```py\n+result = manager.get_result(request_id=\"request_5\")\n+```\n+\n+Stream partial results for a specific request with [`~ContinuousBatchingManager.request_id_iter`].\n+\n+```py\n manager.add_request(\n-    input_ids=input,\n+    input_ids=input_ids,\n     request_id=\"streaming_request\",\n     stream=True,\n )\n@@ -146,49 +110,65 @@ for chunk in manager.request_id_iter(request_id=\"streaming_request\"):\n     # FIXME: stop iteration in `request_id_iter` when finished instead of doing it externally\n     if chunk.status == RequestStatus.FINISHED:\n         break\n+```\n+\n+Call [`~ContinuousBatchingManager.stop`] to terminate the manager.\n \n-# stop the background thread before exiting the process\n+```py\n manager.stop()\n ```\n \n-## Supported & Unsupported Features\n+## PagedAttention\n \n-### Supported Features\n+PagedAttention breaks large key-value caches into smaller, non-contiguous fixed-size pages to avoid GPU memory fragmentation and support variable-length requests. Transformers automatically enables PagedAttention when using continuous batching.\n \n-- Dynamic scheduling of variable-length requests\n-- Chunked prefill\n-- Paged Attention Cache\n-- Sliding window attention\n-- Chat templates\n+You could explicitly enable PagedAttention when instantiating a model rather than waiting for [`~ContinuousMixin.generate_batch`] to dynamically enable it.\n \n-### Unsupported Features\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM\n+\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"Qwen/Qwen3-4B-Instruct-2507\",\n+    attn_implementation=\"paged|flash_attention_2\",\n+    device_map=\"cuda\",\n+    torch_dtype=torch.bfloat16\n+)\n+```\n \n-At the moment, the following features are not supported with CB. We plan to add support to the following:\n+## Sliding window attention\n \n-- Prefix caching\n-- Beam search\n-- tool calling\n+Sliding window attention limits the backward context of a token to save compute. Generation cost stays proportional to window size. This reduces compute per step and simplifies continuous batching.\n \n-The others are unplanned, but depending on community requests we might consider adding them:\n+Transformers models like Mistral and Gemma 2 natively support sliding window attention. Manually enable it in the model config if the architecture supports it. This helps with fine-tuning or running custom experiments.\n \n-- MTP (multi token prediction)\n-- Medusa\n+```py\n+from transformers import AutoConfig\n \n-## Performance Considerations\n+config = AutoConfig.from_pretrained(\"google/gemma-2-2b\")\n+config.sliding_window = 4096\n \n+model = AutoModelForCausalLM.from_pretrained(\n+    \"google/gemma-2-2b\",\n+    config=config,\n+    attn_implementation=\"paged|flash_attention_2\",\n+    device_map=\"cuda\",\n+    dtype=torch.bfloat16,\n+)\n+```\n \n-## Integration with Serving\n+Usage remains the same with [`~ContinuousMixin.generate_batch`].\n \n-You can use CB in `transformers serve` by passing the `--continuous-batching` flag when starting the server.\n+## How it works\n \n-## Monitoring\n+The [`ContinuousMixin`] class serves as the main interface for continuous batching through [`~ContinuousMixin.generate_batch`]. This method internally creates a [`ContinuousBatchingManager`].\n \n-We have added `opentelemetry` support to Continuous Batching to help you monitor its performance in production. To enable it, you need to install the `opentelemetry` extra when installing `transformers`:\n+[`ContinuousBatchingManager`] manages requests by creating a background thread for the generation loop and adding requests to the queue. The manager is thread-safe, allowing asynchronous request additions while the model generates.\n \n-```sh\n-# this installs `opentelemetry-api`, `opentelemetry-sdk` and `opentelemetry-exporter-otlp`\n-pip install transformers[open-telemetry]\n-```\n+The [`Scheduler`] selects requests for processing at each step based on the token budget. [`FIFOScheduler`] is the default scheduler. It prioritizes decoding requests over prefilling requests and assigns them to specific memory blocks. [`PrefillFirstScheduler`] prioritizes prefill requests instead.\n+\n+[`ContinuousBatchingManager`] runs the model forward pass for the scheduled requests. It then collects and returns the results.\n \n-This will enable traces and metrics collection in CB. You will then have to setup the backend to collect and visualize the traces and metrics.\n+## Resources\n \n+The [Continuous batching](https://huggingface.co/blog/continuous_batching) blog post explains KV caching, chunked prefill, and ragged batching with dynamic scheduling in more detail.\n\\ No newline at end of file"
        },
        {
            "sha": "b0f4ea9e1f751c72be36914f9080e0c5d0dea466",
            "filename": "docs/source/en/main_classes/text_generation.md",
            "status": "modified",
            "additions": 21,
            "deletions": 1,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3673ed42936e4597074e98ba017fcaab3d4a8eb/docs%2Fsource%2Fen%2Fmain_classes%2Ftext_generation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3673ed42936e4597074e98ba017fcaab3d4a8eb/docs%2Fsource%2Fen%2Fmain_classes%2Ftext_generation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Ftext_generation.md?ref=e3673ed42936e4597074e98ba017fcaab3d4a8eb",
            "patch": "@@ -41,4 +41,24 @@ like token streaming.\n \n [[autodoc]] GenerationMixin\n     - generate\n-    - compute_transition_scores\n\\ No newline at end of file\n+    - compute_transition_scores\n+\n+## ContinuousMixin\n+\n+[[autodoc]] generation.ContinuousMixin\n+\n+## ContinuousBatchingManager\n+\n+[[autodoc]] generation.ContinuousBatchingManager\n+\n+## Scheduler\n+\n+[[autodoc]] generation.Scheduler\n+\n+## FIFOScheduler\n+\n+[[autodoc]] generation.FIFOScheduler\n+\n+## PrefillFirstScheduler\n+\n+[[autodoc]] generation.PrefillFirstScheduler"
        },
        {
            "sha": "c3d364bd4736f66cf6bf58d6e45b61858b676a0e",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3673ed42936e4597074e98ba017fcaab3d4a8eb/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3673ed42936e4597074e98ba017fcaab3d4a8eb/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=e3673ed42936e4597074e98ba017fcaab3d4a8eb",
            "patch": "@@ -383,6 +383,8 @@\n             \"BayesianDetectorConfig\",\n             \"BayesianDetectorModel\",\n             \"ClassifierFreeGuidanceLogitsProcessor\",\n+            \"ContinuousBatchingManager\",\n+            \"ContinuousMixin\",\n             \"EncoderNoRepeatNGramLogitsProcessor\",\n             \"EncoderRepetitionPenaltyLogitsProcessor\",\n             \"EosTokenCriteria\",\n@@ -536,6 +538,8 @@\n     from .generation import BayesianDetectorModel as BayesianDetectorModel\n     from .generation import ClassifierFreeGuidanceLogitsProcessor as ClassifierFreeGuidanceLogitsProcessor\n     from .generation import CompileConfig as CompileConfig\n+    from .generation import ContinuousBatchingManager as ContinuousBatchingManager\n+    from .generation import ContinuousMixin as ContinuousMixin\n     from .generation import EncoderNoRepeatNGramLogitsProcessor as EncoderNoRepeatNGramLogitsProcessor\n     from .generation import EncoderRepetitionPenaltyLogitsProcessor as EncoderRepetitionPenaltyLogitsProcessor\n     from .generation import EosTokenCriteria as EosTokenCriteria"
        },
        {
            "sha": "a4728fe693c8b7efb3225de74ba28fe9ce0136a6",
            "filename": "src/transformers/generation/__init__.py",
            "status": "modified",
            "additions": 11,
            "deletions": 1,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3673ed42936e4597074e98ba017fcaab3d4a8eb/src%2Ftransformers%2Fgeneration%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3673ed42936e4597074e98ba017fcaab3d4a8eb/src%2Ftransformers%2Fgeneration%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2F__init__.py?ref=e3673ed42936e4597074e98ba017fcaab3d4a8eb",
            "patch": "@@ -86,7 +86,11 @@\n         \"StopStringCriteria\",\n     ]\n     _import_structure[\"continuous_batching\"] = [\n+        \"ContinuousBatchingManager\",\n         \"ContinuousMixin\",\n+        \"FIFOScheduler\",\n+        \"PrefillFirstScheduler\",\n+        \"Scheduler\",\n     ]\n     _import_structure[\"utils\"] = [\n         \"GenerationMixin\",\n@@ -127,7 +131,13 @@\n             EarlyExitCandidateGenerator,\n             PromptLookupCandidateGenerator,\n         )\n-        from .continuous_batching import ContinuousMixin\n+        from .continuous_batching import (\n+            ContinuousBatchingManager,\n+            ContinuousMixin,\n+            FIFOScheduler,\n+            PrefillFirstScheduler,\n+            Scheduler,\n+        )\n         from .logits_process import (\n             AlternatingCodebooksLogitsProcessor,\n             ClassifierFreeGuidanceLogitsProcessor,"
        },
        {
            "sha": "75bf6178e9786290df0a6a6ffe2b6fce6fd26efd",
            "filename": "src/transformers/generation/continuous_batching/__init__.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3673ed42936e4597074e98ba017fcaab3d4a8eb/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3673ed42936e4597074e98ba017fcaab3d4a8eb/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2F__init__.py?ref=e3673ed42936e4597074e98ba017fcaab3d4a8eb",
            "patch": "@@ -15,12 +15,16 @@\n from .cache import PagedAttentionCache\n from .continuous_api import ContinuousBatchingManager, ContinuousMixin\n from .requests import RequestState, RequestStatus\n+from .scheduler import FIFOScheduler, PrefillFirstScheduler, Scheduler\n \n \n __all__ = [\n     \"ContinuousBatchingManager\",\n     \"ContinuousMixin\",\n+    \"FIFOScheduler\",\n     \"PagedAttentionCache\",\n+    \"PrefillFirstScheduler\",\n     \"RequestState\",\n     \"RequestStatus\",\n+    \"Scheduler\",\n ]"
        }
    ],
    "stats": {
        "total": 454,
        "additions": 242,
        "deletions": 212
    }
}