{
    "author": "yonigozlan",
    "message": "Add SAM3 to ðŸ¤— Transformers (#42285)\n\n* add native image only sam3\n\n* remove sam3 source code from transformers\n\n* init sam3 refactor\n\n* add working refactored sam3\n\n* remove dedicated config mask fuser\n\n* Add support for batch inference and inference in any dtype\n\n* improve processing for mixed batched inputs\n\n* update convert script for updated checkpoints\n\n* Use consistent masks throughout modeling file (True=valid, False=Padding)\n\n* Cleanup modeling, add fully working tests, add examples in md doc, add post processing\n\n* update _supports_flash_attn flag\n\n* Remove sam3 support of points and masks input, simplify configs, modeling, processing and tests.\n\n* update sam2Video model to support sam3 tracker\n\n* add working sam3_video\n\n* add sam3 tracker and sam3 tracker video\n\n* Optimize sam3 video and sam3 tracker\n\n* add recondition_on_trk_masks option, clean up\n\n* add support for loading the 4 sam3 models from one checkpoint\n\n* fix modular (except edgetam video\n\n* Fixes after dynamic weight loader merge\n\n* change propagate_in_video to propagate_in_video_iterator sam3_video_model + cleanup\n\n* fix modular edgetam, don't implement batch inference for memories for now\n\n* copy weights in checkpoints instead of tying + cleanup + docs\n\n* support mask generation sam3 + use modular for Sam3ImageProcessorFast\n\n* Improve sam3_video API, add supports for inference streaming, add docs and tests for sam3_video\n\n* make fixup\n\n* remove unused attribute in tracker models\n\n* clean up docs\n\n* use correct checkpoints in docs\n\n* try to load kernels once at import, instead of at every call\n\n* improve kernel loading logic\n\n* update docs with abstract and paper link\n\n* take into account presence scores in sam3 video\n\n---------\n\nCo-authored-by: kalyanvasudev <kalyan051993@gmail.com>",
    "sha": "1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
    "files": [
        {
            "sha": "c3036b8a397374f151671f7f821e8fca3e928995",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -826,6 +826,8 @@\n         title: RT-DETRv2\n       - local: model_doc/sam2\n         title: SAM2\n+      - local: model_doc/sam3_tracker\n+        title: Sam3Tracker\n       - local: model_doc/segformer\n         title: SegFormer\n       - local: model_doc/seggpt\n@@ -952,6 +954,8 @@\n     - sections:\n       - local: model_doc/sam2_video\n         title: SAM2 Video\n+      - local: model_doc/sam3_tracker_video\n+        title: Sam3TrackerVideo\n       - local: model_doc/timesformer\n         title: TimeSformer\n       - local: model_doc/vjepa2\n@@ -1132,6 +1136,10 @@\n         title: Qwen3VL\n       - local: model_doc/qwen3_vl_moe\n         title: Qwen3VLMoe\n+      - local: model_doc/sam3\n+        title: SAM3\n+      - local: model_doc/sam3_video\n+        title: SAM3 Video\n       - local: model_doc/shieldgemma2\n         title: ShieldGemma2\n       - local: model_doc/siglip"
        },
        {
            "sha": "ecd7b9b097f2d508a82f6f679d97129e34780e5b",
            "filename": "docs/source/en/model_doc/sam3.md",
            "status": "added",
            "additions": 325,
            "deletions": 0,
            "changes": 325,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3.md?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,325 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+*This model was released on 2025-11-19 and added to Hugging Face Transformers on 2025-11-19.*\n+\n+# SAM3\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+    </div>\n+</div>\n+\n+## Overview\n+\n+SAM3 (Segment Anything Model 3) was introduced in [SAM 3: Segment Anything with Concepts](https://ai.meta.com/research/publications/sam-3-segment-anything-with-concepts/).\n+\n+SAM3 performs **Promptable Concept Segmentation (PCS)** on images. PCS takes text and/or image exemplars as input (e.g., \"yellow school bus\"), and predicts instance and semantic masks for **every single object** matching the concept.\n+\n+The abstract from the paper is the following:\n+\n+*We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., \"yellow school bus\"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.*\n+\n+This model was contributed by [yonigozlan](https://huggingface.co/yonigozlan) and [ronghanghu](https://huggingface.co/ronghanghu).\n+\n+## Usage examples with ðŸ¤— Transformers\n+\n+### Text-Only Prompts\n+\n+```python\n+>>> from transformers import Sam3Processor, Sam3Model\n+>>> import torch\n+>>> from PIL import Image\n+>>> import requests\n+\n+>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+>>> model = Sam3Model.from_pretrained(\"facebook/sam3\").to(device)\n+>>> processor = Sam3Processor.from_pretrained(\"facebook/sam3\")\n+\n+>>> # Load image\n+>>> image_url = \"http://images.cocodataset.org/val2017/000000077595.jpg\"\n+>>> image = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\")\n+\n+>>> # Segment using text prompt\n+>>> inputs = processor(images=image, text=\"ear\", return_tensors=\"pt\").to(device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs)\n+\n+>>> # Post-process results\n+>>> results = processor.post_process_instance_segmentation(\n+...     outputs,\n+...     threshold=0.5,\n+...     mask_threshold=0.5,\n+...     target_sizes=inputs.get(\"original_sizes\").tolist()\n+... )[0]\n+\n+>>> print(f\"Found {len(results['masks'])} objects\")\n+>>> # Results contain:\n+>>> # - masks: Binary masks resized to original image size\n+>>> # - boxes: Bounding boxes in absolute pixel coordinates (xyxy format)\n+>>> # - scores: Confidence scores\n+```\n+\n+### Single Bounding Box Prompt\n+\n+Segment objects using a bounding box on the visual concept:\n+\n+```python\n+>>> # Box in xyxy format: [x1, y1, x2, y2] in pixel coordinates\n+>>> # Example: laptop region\n+>>> box_xyxy = [100, 150, 500, 450]\n+>>> input_boxes = [[box_xyxy]]  # [batch, num_boxes, 4]\n+>>> input_boxes_labels = [[1]]  # 1 = positive box\n+\n+>>> inputs = processor(\n+...     images=image,\n+...     input_boxes=input_boxes,\n+...     input_boxes_labels=input_boxes_labels,\n+...     return_tensors=\"pt\"\n+... ).to(device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs)\n+\n+>>> # Post-process results\n+>>> results = processor.post_process_instance_segmentation(\n+...     outputs,\n+...     threshold=0.5,\n+...     mask_threshold=0.5,\n+...     target_sizes=inputs.get(\"original_sizes\").tolist()\n+... )[0]\n+```\n+\n+### Multiple Box Prompts (Positive and Negative)\n+\n+Use multiple boxes with positive and negative labels to refine the concept:\n+\n+```python\n+>>> # Load kitchen image\n+>>> kitchen_url = \"http://images.cocodataset.org/val2017/000000136466.jpg\"\n+>>> kitchen_image = Image.open(requests.get(kitchen_url, stream=True).raw).convert(\"RGB\")\n+\n+>>> # Define two positive boxes (e.g., dial and button on oven)\n+>>> # Boxes are in xyxy format [x1, y1, x2, y2] in pixel coordinates\n+>>> box1_xyxy = [59, 144, 76, 163]  # Dial box\n+>>> box2_xyxy = [87, 148, 104, 159]  # Button box\n+>>> input_boxes = [[box1_xyxy, box2_xyxy]]\n+>>> input_boxes_labels = [[1, 1]]  # Both positive\n+\n+>>> inputs = processor(\n+...     images=kitchen_image,\n+...     input_boxes=input_boxes,\n+...     input_boxes_labels=input_boxes_labels,\n+...     return_tensors=\"pt\"\n+... ).to(device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs)\n+\n+>>> # Post-process results\n+>>> results = processor.post_process_instance_segmentation(\n+...     outputs,\n+...     threshold=0.5,\n+...     mask_threshold=0.5,\n+...     target_sizes=inputs.get(\"original_sizes\").tolist()\n+... )[0]\n+```\n+\n+### Combined Prompts (Text + Negative Box)\n+\n+Use text prompts with negative visual prompts to refine the concept:\n+\n+```python\n+>>> # Segment \"handle\" but exclude the oven handle using a negative box\n+>>> text = \"handle\"\n+>>> # Negative box covering oven handle area (xyxy): [40, 183, 318, 204]\n+>>> oven_handle_box = [40, 183, 318, 204]\n+>>> input_boxes = [[oven_handle_box]]\n+\n+>>> inputs = processor(\n+...     images=kitchen_image,\n+...     text=text,\n+...     input_boxes=input_boxes,\n+...     input_boxes_labels=[[0]],  # 0 = negative (exclude this region)\n+...     return_tensors=\"pt\"\n+... ).to(device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs)\n+\n+>>> # Post-process results\n+>>> results = processor.post_process_instance_segmentation(\n+...     outputs,\n+...     threshold=0.5,\n+...     mask_threshold=0.5,\n+...     target_sizes=inputs.get(\"original_sizes\").tolist()\n+... )[0]\n+>>> # This will segment pot handles but exclude the oven handle\n+```\n+\n+### Batched Inference with Text Prompts\n+\n+Process multiple images with different text prompts efficiently:\n+\n+```python\n+>>> cat_url = \"http://images.cocodataset.org/val2017/000000077595.jpg\"\n+>>> kitchen_url = \"http://images.cocodataset.org/val2017/000000136466.jpg\"\n+>>> images = [\n+...     Image.open(requests.get(cat_url, stream=True).raw).convert(\"RGB\"),\n+...     Image.open(requests.get(kitchen_url, stream=True).raw).convert(\"RGB\")\n+... ]\n+\n+>>> # Different text prompt for each image\n+>>> text_prompts = [\"ear\", \"dial\"]\n+\n+>>> inputs = processor(images=images, text=text_prompts, return_tensors=\"pt\").to(device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs)\n+\n+>>> # Post-process results for both images\n+>>> results = processor.post_process_instance_segmentation(\n+...     outputs,\n+...     threshold=0.5,\n+...     mask_threshold=0.5,\n+...     target_sizes=inputs.get(\"original_sizes\").tolist()\n+... )\n+\n+>>> print(f\"Image 1: {len(results[0]['masks'])} objects found\")\n+>>> print(f\"Image 2: {len(results[1]['masks'])} objects found\")\n+```\n+\n+### Batched Mixed Prompts\n+\n+Use different prompt types for different images in the same batch:\n+\n+```python\n+>>> # Image 1: text prompt \"laptop\"\n+>>> # Image 2: visual prompt (dial box)\n+>>> box2_xyxy = [59, 144, 76, 163]\n+\n+>>> inputs = processor(\n+...     images=images,\n+...     text=[\"laptop\", None],  # Only first image has text\n+...     input_boxes=[None, [box2_xyxy]],  # Only second image has box\n+...     input_boxes_labels=[None, [1]],  # Positive box for second image\n+...     return_tensors=\"pt\"\n+... ).to(device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs)\n+\n+>>> # Post-process results for both images\n+>>> results = processor.post_process_instance_segmentation(\n+...     outputs,\n+...     threshold=0.5,\n+...     mask_threshold=0.5,\n+...     target_sizes=inputs.get(\"original_sizes\").tolist()\n+... )\n+>>> # Both images processed in single forward pass\n+```\n+\n+### Semantic Segmentation Output\n+\n+SAM3 also provides semantic segmentation alongside instance masks:\n+\n+```python\n+>>> inputs = processor(images=image, text=\"ear\", return_tensors=\"pt\").to(device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs)\n+\n+>>> # Instance segmentation masks\n+>>> instance_masks = torch.sigmoid(outputs.pred_masks)  # [batch, num_queries, H, W]\n+\n+>>> # Semantic segmentation (single channel)\n+>>> semantic_seg = outputs.semantic_seg  # [batch, 1, H, W]\n+\n+>>> print(f\"Instance masks: {instance_masks.shape}\")\n+>>> print(f\"Semantic segmentation: {semantic_seg.shape}\")\n+```\n+\n+### Prompt Label Conventions\n+\n+SAM3 uses the following label conventions:\n+\n+**For points and boxes:**\n+- `1`: Positive prompt (include this region/object)\n+- `0`: Negative prompt (exclude this region/object)\n+- `-10`: Padding value for batched inputs\n+\n+**Coordinate formats:**\n+- **Input boxes**: `[x1, y1, x2, y2]` (xyxy format) in pixel coordinates\n+- **Output boxes** (raw): `[x1, y1, x2, y2]` (xyxy format), normalized to [0, 1]\n+- **Output boxes** (post-processed): `[x1, y1, x2, y2]` (xyxy format) in absolute pixel coordinates\n+\n+## Sam3Config\n+\n+[[autodoc]] Sam3Config\n+\n+## Sam3ViTConfig\n+\n+[[autodoc]] Sam3ViTConfig\n+\n+## Sam3VisionConfig\n+\n+[[autodoc]] Sam3VisionConfig\n+\n+## Sam3GeometryEncoderConfig\n+\n+[[autodoc]] Sam3GeometryEncoderConfig\n+\n+## Sam3DETREncoderConfig\n+\n+[[autodoc]] Sam3DETREncoderConfig\n+\n+## Sam3DETRDecoderConfig\n+\n+[[autodoc]] Sam3DETRDecoderConfig\n+\n+## Sam3MaskDecoderConfig\n+\n+[[autodoc]] Sam3MaskDecoderConfig\n+\n+## Sam3Processor\n+\n+[[autodoc]] Sam3Processor\n+    - __call__\n+\n+## Sam3ImageProcessorFast\n+\n+[[autodoc]] Sam3ImageProcessorFast\n+    - preprocess\n+\n+## Sam3ViTModel\n+\n+[[autodoc]] Sam3ViTModel\n+    - forward\n+\n+## Sam3VisionModel\n+\n+[[autodoc]] Sam3VisionModel\n+    - forward\n+\n+## Sam3Model\n+\n+[[autodoc]] Sam3Model\n+    - forward\n+"
        },
        {
            "sha": "d6c488a0846e9ac2d10e695d4b237714d6c1161a",
            "filename": "docs/source/en/model_doc/sam3_tracker.md",
            "status": "added",
            "additions": 328,
            "deletions": 0,
            "changes": 328,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3_tracker.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3_tracker.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3_tracker.md?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,328 @@\n+<!--Copyright 2025 the HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n+\n+-->\n+*This model was released on 2025-11-19 and added to Hugging Face Transformers on 2025-11-19.*\n+\n+# SAM3 Tracker\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+    </div>\n+</div>\n+\n+## Overview\n+\n+SAM3 (Segment Anything Model 3) was introduced in [SAM 3: Segment Anything with Concepts](https://ai.meta.com/research/publications/sam-3-segment-anything-with-concepts/).\n+\n+Sam3Tracker performs **Promptable Visual Segmentation (PVS)** on images. PVS takes interactive visual prompts (points, boxes, masks) or text inputs to segment a **specific object instance** per prompt. This is the task that SAM 1 and SAM 2 focused on, and SAM 3 improves upon it.\n+\n+Sam3Tracker is an updated version of SAM2 (Segment Anything Model 2) that maintains the same API while providing improved performance and capabilities.\n+\n+The abstract from the paper is the following:\n+\n+*We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., \"yellow school bus\"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.*\n+\n+\n+This model was contributed by [yonigozlan](https://huggingface.co/yonigozlan) and [ronghanghu](https://huggingface.co/ronghanghu).\n+\n+## Usage example\n+\n+### Automatic Mask Generation with Pipeline\n+\n+Sam3Tracker can be used for automatic mask generation to segment all objects in an image using the `mask-generation` pipeline:\n+\n+```python\n+>>> from transformers import pipeline\n+\n+>>> generator = pipeline(\"mask-generation\", model=\"facebook/sam3\", device=0)\n+>>> image_url = \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/truck.jpg\"\n+>>> outputs = generator(image_url, points_per_batch=64)\n+\n+>>> len(outputs[\"masks\"])  # Number of masks generated\n+39\n+```\n+\n+### Basic Image Segmentation\n+\n+#### Single Point Click\n+\n+You can segment objects by providing a single point click on the object you want to segment:\n+\n+```python\n+>>> from transformers import Sam3TrackerProcessor, Sam3TrackerModel\n+from accelerate import Accelerator\n+>>> import torch\n+>>> from PIL import Image\n+>>> import requests\n+\n+>>> device = Accelerator().device\n+\n+>>> model = Sam3TrackerModel.from_pretrained(\"facebook/sam3\").to(device)\n+>>> processor = Sam3TrackerProcessor.from_pretrained(\"facebook/sam3\")\n+\n+>>> image_url = \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/truck.jpg\"\n+>>> raw_image = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\")\n+\n+>>> input_points = [[[[500, 375]]]]  # Single point click, 4 dimensions (image_dim, object_dim, point_per_object_dim, coordinates)\n+>>> input_labels = [[[1]]]  # 1 for positive click, 0 for negative click, 3 dimensions (image_dim, object_dim, point_label)\n+\n+>>> inputs = processor(images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\").to(model.device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs)\n+\n+>>> masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"])[0]\n+\n+>>> # The model outputs multiple mask predictions ranked by quality score\n+>>> print(f\"Generated {masks.shape[1]} masks with shape {masks.shape}\")\n+Generated 3 masks with shape torch.Size([1, 3, 1500, 2250])\n+```\n+\n+#### Multiple Points for Refinement\n+\n+You can provide multiple points to refine the segmentation:\n+\n+```python\n+>>> # Add both positive and negative points to refine the mask\n+>>> input_points = [[[[500, 375], [1125, 625]]]]  # Multiple points for refinement\n+>>> input_labels = [[[1, 1]]]  # Both positive clicks\n+\n+>>> inputs = processor(images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\").to(device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs)\n+\n+>>> masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"])[0]\n+```\n+\n+#### Bounding Box Input\n+\n+Sam3Tracker also supports bounding box inputs for segmentation:\n+\n+```python\n+>>> # Define bounding box as [x_min, y_min, x_max, y_max]\n+>>> input_boxes = [[[75, 275, 1725, 850]]]\n+\n+>>> inputs = processor(images=raw_image, input_boxes=input_boxes, return_tensors=\"pt\").to(device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs)\n+\n+>>> masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"])[0]\n+```\n+\n+#### Multiple Objects Segmentation\n+\n+You can segment multiple objects simultaneously:\n+\n+```python\n+>>> # Define points for two different objects\n+>>> input_points = [[[[500, 375]], [[650, 750]]]]  # Points for two objects in same image\n+>>> input_labels = [[[1], [1]]]  # Positive clicks for both objects\n+\n+>>> inputs = processor(images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\").to(model.device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs, multimask_output=False)\n+\n+>>> # Each object gets its own mask\n+>>> masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"])[0]\n+>>> print(f\"Generated masks for {masks.shape[0]} objects\")\n+Generated masks for 2 objects\n+```\n+\n+### Batch Inference\n+\n+#### Batched Images\n+\n+Process multiple images simultaneously for improved efficiency:\n+\n+```python\n+>>> from transformers import Sam3TrackerProcessor, Sam3TrackerModel\n+from accelerate import Accelerator\n+>>> import torch\n+>>> from PIL import Image\n+>>> import requests\n+\n+>>> device = Accelerator().device\n+\n+>>> model = Sam3TrackerModel.from_pretrained(\"facebook/sam3\").to(device)\n+>>> processor = Sam3TrackerProcessor.from_pretrained(\"facebook/sam3\")\n+\n+>>> # Load multiple images\n+>>> image_urls = [\n+...     \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/truck.jpg\",\n+...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/dog-sam.png\"\n+... ]\n+>>> raw_images = [Image.open(requests.get(url, stream=True).raw).convert(\"RGB\") for url in image_urls]\n+\n+>>> # Single point per image\n+>>> input_points = [[[[500, 375]]], [[[770, 200]]]]  # One point for each image\n+>>> input_labels = [[[1]], [[1]]]  # Positive clicks for both images\n+\n+>>> inputs = processor(images=raw_images, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\").to(model.device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs, multimask_output=False)\n+\n+>>> # Post-process masks for each image\n+>>> all_masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"])\n+>>> print(f\"Processed {len(all_masks)} images, each with {all_masks[0].shape[0]} objects\")\n+Processed 2 images, each with 1 objects\n+```\n+\n+#### Batched Objects per Image\n+\n+Segment multiple objects within each image using batch inference:\n+\n+```python\n+>>> # Multiple objects per image - different numbers of objects per image\n+>>> input_points = [\n+...     [[[500, 375]], [[650, 750]]],  # Truck image: 2 objects\n+...     [[[770, 200]]]  # Dog image: 1 object\n+... ]\n+>>> input_labels = [\n+...     [[1], [1]],  # Truck image: positive clicks for both objects\n+...     [[1]]  # Dog image: positive click for the object\n+... ]\n+\n+>>> inputs = processor(images=raw_images, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\").to(device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs, multimask_output=False)\n+\n+>>> all_masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"])\n+```\n+\n+#### Batched Images with Batched Objects and Multiple Points\n+\n+Handle complex batch scenarios with multiple points per object:\n+\n+```python\n+>>> # Add groceries image for more complex example\n+>>> groceries_url = \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/groceries.jpg\"\n+>>> groceries_image = Image.open(requests.get(groceries_url, stream=True).raw).convert(\"RGB\")\n+>>> raw_images = [raw_images[0], groceries_image]  # Use truck and groceries images\n+\n+>>> # Complex batching: multiple images, multiple objects, multiple points per object\n+>>> input_points = [\n+...     [[[500, 375]], [[650, 750]]],  # Truck image: 2 objects with 1 point each\n+...     [[[400, 300]], [[630, 300], [550, 300]]]  # Groceries image: obj1 has 1 point, obj2 has 2 points\n+... ]\n+>>> input_labels = [\n+...     [[1], [1]],  # Truck image: positive clicks\n+...     [[1], [1, 1]]  # Groceries image: positive clicks for refinement\n+... ]\n+\n+>>> inputs = processor(images=raw_images, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\").to(device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs, multimask_output=False)\n+\n+>>> all_masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"])\n+```\n+\n+#### Batched Bounding Boxes\n+\n+Process multiple images with bounding box inputs:\n+\n+```python\n+>>> # Multiple bounding boxes per image (using truck and groceries images)\n+>>> input_boxes = [\n+...     [[75, 275, 1725, 850], [425, 600, 700, 875], [1375, 550, 1650, 800], [1240, 675, 1400, 750]],  # Truck image: 4 boxes\n+...     [[450, 170, 520, 350], [350, 190, 450, 350], [500, 170, 580, 350], [580, 170, 640, 350]]  # Groceries image: 4 boxes\n+... ]\n+\n+>>> # Update images for this example\n+>>> raw_images = [raw_images[0], groceries_image]  # truck and groceries\n+\n+>>> inputs = processor(images=raw_images, input_boxes=input_boxes, return_tensors=\"pt\").to(device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs, multimask_output=False)\n+\n+>>> all_masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"])\n+>>> print(f\"Processed {len(input_boxes)} images with {len(input_boxes[0])} and {len(input_boxes[1])} boxes respectively\")\n+Processed 2 images with 4 and 4 boxes respectively\n+```\n+\n+### Using Previous Masks as Input\n+\n+Sam3Tracker can use masks from previous predictions as input to refine segmentation:\n+\n+```python\n+>>> # Get initial segmentation\n+>>> input_points = [[[[500, 375]]]]\n+>>> input_labels = [[[1]]]\n+>>> inputs = processor(images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\").to(device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs)\n+\n+>>> # Use the best mask as input for refinement\n+>>> mask_input = outputs.pred_masks[:, :, torch.argmax(outputs.iou_scores.squeeze())]\n+\n+>>> # Add additional points with the mask input\n+>>> new_input_points = [[[[500, 375], [450, 300]]]]\n+>>> new_input_labels = [[[1, 1]]]\n+>>> inputs = processor(\n+...     input_points=new_input_points,\n+...     input_labels=new_input_labels,\n+...     original_sizes=inputs[\"original_sizes\"],\n+...     return_tensors=\"pt\",\n+... ).to(device)\n+\n+>>> with torch.no_grad():\n+...     refined_outputs = model(\n+...         **inputs,\n+...         input_masks=mask_input,\n+...         image_embeddings=outputs.image_embeddings,\n+...         multimask_output=False,\n+...     )\n+```\n+\n+## Sam3TrackerConfig\n+\n+[[autodoc]] Sam3TrackerConfig\n+\n+## Sam3TrackerPromptEncoderConfig\n+\n+[[autodoc]] Sam3TrackerPromptEncoderConfig\n+\n+## Sam3TrackerMaskDecoderConfig\n+\n+[[autodoc]] Sam3TrackerMaskDecoderConfig\n+\n+## Sam3TrackerProcessor\n+\n+[[autodoc]] Sam3TrackerProcessor\n+    - __call__\n+    - post_process_masks\n+\n+## Sam3TrackerModel\n+\n+[[autodoc]] Sam3TrackerModel\n+    - forward\n+\n+## Sam3TrackerPreTrainedModel\n+\n+[[autodoc]] Sam3TrackerPreTrainedModel\n+    - forward"
        },
        {
            "sha": "0517d8c6a115458c3347bc844e7dd0c3f49c8039",
            "filename": "docs/source/en/model_doc/sam3_tracker_video.md",
            "status": "added",
            "additions": 311,
            "deletions": 0,
            "changes": 311,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3_tracker_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3_tracker_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3_tracker_video.md?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,311 @@\n+<!--Copyright 2025 the HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n+\n+-->\n+*This model was released on 2025-11-19 and added to Hugging Face Transformers on 2025-11-19.*\n+\n+\n+# SAM3 Tracker Video\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+    </div>\n+</div>\n+\n+## Overview\n+\n+SAM3 (Segment Anything Model 3) was introduced in [SAM 3: Segment Anything with Concepts](https://ai.meta.com/research/publications/sam-3-segment-anything-with-concepts/).\n+\n+Sam3TrackerVideo performs **Promptable Visual Segmentation (PVS)** on videos. PVS takes interactive visual prompts (points, boxes, masks) or text inputs to track a **specific object instance** per prompt across video frames.\n+\n+Sam3TrackerVideo is an updated version of SAM2 Video that maintains the same API while providing improved performance and capabilities.\n+\n+The abstract from the paper is the following:\n+\n+*We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., \"yellow school bus\"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.*\n+\n+\n+This model was contributed by [yonigozlan](https://huggingface.co/yonigozlan) and [ronghanghu](https://huggingface.co/ronghanghu).\n+\n+## Usage example\n+\n+### Video Segmentation and Tracking\n+\n+\n+#### Basic Video Tracking\n+\n+```python\n+>>> from transformers import Sam3TrackerVideoModel, Sam3TrackerVideoProcessor\n+from accelerate import Accelerator\n+>>> import torch\n+\n+>>> device = Accelerator().device\n+>>> model = Sam3TrackerVideoModel.from_pretrained(\"facebook/sam3\").to(device, dtype=torch.bfloat16)\n+>>> processor = Sam3TrackerVideoProcessor.from_pretrained(\"facebook/sam3\")\n+\n+>>> # Load video frames (example assumes you have a list of PIL Images)\n+>>> # video_frames = [Image.open(f\"frame_{i:05d}.jpg\") for i in range(num_frames)]\n+\n+>>> # For this example, we'll use the video loading utility\n+>>> from transformers.video_utils import load_video\n+>>> video_url = \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/bedroom.mp4\"\n+>>> video_frames, _ = load_video(video_url)\n+\n+>>> # Initialize video inference session\n+>>> inference_session = processor.init_video_session(\n+...     video=video_frames,\n+...     inference_device=device,\n+...     dtype=torch.bfloat16,\n+... )\n+\n+>>> # Add click on first frame to select object\n+>>> ann_frame_idx = 0\n+>>> ann_obj_id = 1\n+>>> points = [[[[210, 350]]]]\n+>>> labels = [[[1]]]\n+\n+>>> processor.add_inputs_to_inference_session(\n+...     inference_session=inference_session,\n+...     frame_idx=ann_frame_idx,\n+...     obj_ids=ann_obj_id,\n+...     input_points=points,\n+...     input_labels=labels,\n+... )\n+\n+>>> # Segment the object on the first frame (optional, you can also propagate the masks through the video directly)\n+>>> outputs = model(\n+...     inference_session=inference_session,\n+...     frame_idx=ann_frame_idx,\n+... )\n+>>> video_res_masks = processor.post_process_masks(\n+...     [outputs.pred_masks], original_sizes=[[inference_session.video_height, inference_session.video_width]], binarize=False\n+... )[0]\n+>>> print(f\"Segmentation shape: {video_res_masks.shape}\")\n+Segmentation shape: torch.Size([1, 1, 480, 854])\n+\n+>>> # Propagate through the entire video\n+>>> video_segments = {}\n+>>> for sam3_tracker_video_output in model.propagate_in_video_iterator(inference_session):\n+...     video_res_masks = processor.post_process_masks(\n+...         [sam3_tracker_video_output.pred_masks], original_sizes=[[inference_session.video_height, inference_session.video_width]], binarize=False\n+...     )[0]\n+...     video_segments[sam3_tracker_video_output.frame_idx] = video_res_masks\n+\n+>>> print(f\"Tracked object through {len(video_segments)} frames\")\n+Tracked object through 180 frames\n+```\n+\n+#### Multi-Object Video Tracking\n+\n+Track multiple objects simultaneously across video frames:\n+\n+```python\n+>>> # Reset for new tracking session\n+>>> inference_session.reset_inference_session()\n+\n+>>> # Add multiple objects on the first frame\n+>>> ann_frame_idx = 0\n+>>> obj_ids = [2, 3]\n+>>> input_points = [[[[200, 300]], [[400, 150]]]]  # Points for two objects (batched)\n+>>> input_labels = [[[1], [1]]]\n+\n+>>> processor.add_inputs_to_inference_session(\n+...     inference_session=inference_session,\n+...     frame_idx=ann_frame_idx,\n+...     obj_ids=obj_ids,\n+...     input_points=input_points,\n+...     input_labels=input_labels,\n+... )\n+\n+>>> # Get masks for both objects on first frame (optional, you can also propagate the masks through the video directly)\n+>>> outputs = model(\n+...     inference_session=inference_session,\n+...     frame_idx=ann_frame_idx,\n+... )\n+\n+>>> # Propagate both objects through video\n+>>> video_segments = {}\n+>>> for sam3_tracker_video_output in model.propagate_in_video_iterator(inference_session):\n+...     video_res_masks = processor.post_process_masks(\n+...         [sam3_tracker_video_output.pred_masks], original_sizes=[[inference_session.video_height, inference_session.video_width]], binarize=False\n+...     )[0]\n+...     video_segments[sam3_tracker_video_output.frame_idx] = {\n+...         obj_id: video_res_masks[i]\n+...         for i, obj_id in enumerate(inference_session.obj_ids)\n+...     }\n+\n+>>> print(f\"Tracked {len(inference_session.obj_ids)} objects through {len(video_segments)} frames\")\n+Tracked 2 objects through 180 frames\n+```\n+\n+#### Refining Video Segmentation\n+\n+You can add additional clicks on any frame to refine the tracking:\n+\n+```python\n+>>> # Add refinement click on a later frame\n+>>> refine_frame_idx = 50\n+>>> ann_obj_id = 2  # Refining first object\n+>>> points = [[[[220, 280]]]]  # Additional point\n+>>> labels = [[[1]]]  # Positive click\n+\n+>>> processor.add_inputs_to_inference_session(\n+...     inference_session=inference_session,\n+...     frame_idx=refine_frame_idx,\n+...     obj_ids=ann_obj_id,\n+...     input_points=points,\n+...     input_labels=labels,\n+... )\n+\n+>>> # Re-propagate with the additional information\n+>>> video_segments = {}\n+>>> for sam3_tracker_video_output in model.propagate_in_video_iterator(inference_session):\n+...     video_res_masks = processor.post_process_masks(\n+...         [sam3_tracker_video_output.pred_masks], original_sizes=[[inference_session.video_height, inference_session.video_width]], binarize=False\n+...     )[0]\n+...     video_segments[sam3_tracker_video_output.frame_idx] = video_res_masks\n+```\n+\n+### Streaming Video Inference\n+\n+For real-time applications, Sam3TrackerVideo supports processing video frames as they arrive:\n+\n+```python\n+>>> # Initialize session for streaming\n+>>> inference_session = processor.init_video_session(\n+...     inference_device=device,\n+...     dtype=torch.bfloat16,\n+... )\n+\n+>>> # Process frames one by one\n+>>> for frame_idx, frame in enumerate(video_frames[:10]):  # Process first 10 frames\n+...     inputs = processor(images=frame, device=device, return_tensors=\"pt\")\n+...\n+...     if frame_idx == 0:\n+...         # Add point input on first frame\n+...         processor.add_inputs_to_inference_session(\n+...             inference_session=inference_session,\n+...             frame_idx=0,\n+...             obj_ids=1,\n+...             input_points=[[[[210, 350], [250, 220]]]],\n+...             input_labels=[[[1, 1]]],\n+...             original_size=inputs.original_sizes[0], # need to be provided when using streaming video inference\n+...         )\n+...\n+...     # Process current frame\n+...     sam3_tracker_video_output = model(inference_session=inference_session, frame=inputs.pixel_values[0])\n+...\n+...     video_res_masks = processor.post_process_masks(\n+...         [sam3_tracker_video_output.pred_masks], original_sizes=inputs.original_sizes, binarize=False\n+...     )[0]\n+...     print(f\"Frame {frame_idx}: mask shape {video_res_masks.shape}\")\n+```\n+\n+#### Video Batch Processing for Multiple Objects\n+\n+Track multiple objects simultaneously in video by adding them all at once:\n+\n+```python\n+>>> # Initialize video session\n+>>> inference_session = processor.init_video_session(\n+...     video=video_frames,\n+...     inference_device=device,\n+...     dtype=torch.bfloat16,\n+... )\n+\n+>>> # Add multiple objects on the first frame using batch processing\n+>>> ann_frame_idx = 0\n+>>> obj_ids = [2, 3]  # Track two different objects\n+>>> input_points = [\n+...     [[[200, 300], [230, 250], [275, 175]], [[400, 150]]]\n+... ]  # Object 2: 3 points (2 positive, 1 negative); Object 3: 1 point\n+>>> input_labels = [\n+...     [[1, 1, 0], [1]]\n+... ]  # Object 2: positive, positive, negative; Object 3: positive\n+\n+>>> processor.add_inputs_to_inference_session(\n+...     inference_session=inference_session,\n+...     frame_idx=ann_frame_idx,\n+...     obj_ids=obj_ids,\n+...     input_points=input_points,\n+...     input_labels=input_labels,\n+... )\n+\n+>>> # Get masks for all objects on the first frame\n+>>> outputs = model(\n+...     inference_session=inference_session,\n+...     frame_idx=ann_frame_idx,\n+... )\n+>>> video_res_masks = processor.post_process_masks(\n+...     [outputs.pred_masks], original_sizes=[[inference_session.video_height, inference_session.video_width]], binarize=False\n+... )[0]\n+>>> print(f\"Generated masks for {video_res_masks.shape[0]} objects\")\n+Generated masks for 2 objects\n+\n+>>> # Propagate all objects through the video\n+>>> video_segments = {}\n+>>> for sam3_tracker_video_output in model.propagate_in_video_iterator(inference_session):\n+...     video_res_masks = processor.post_process_masks(\n+...         [sam3_tracker_video_output.pred_masks], original_sizes=[[inference_session.video_height, inference_session.video_width]], binarize=False\n+...     )[0]\n+...     video_segments[sam3_tracker_video_output.frame_idx] = {\n+...         obj_id: video_res_masks[i]\n+...         for i, obj_id in enumerate(inference_session.obj_ids)\n+...     }\n+\n+>>> print(f\"Tracked {len(inference_session.obj_ids)} objects through {len(video_segments)} frames\")\n+Tracked 2 objects through 180 frames\n+```\n+\n+<!-- TODO, add resources here. -->\n+<!-- ## Resources -->\n+<!-- A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with SAM3 Tracker Video. -->\n+\n+\n+## Sam3TrackerVideoConfig\n+\n+[[autodoc]] Sam3TrackerVideoConfig\n+\n+## Sam3TrackerVideoMaskDecoderConfig\n+\n+[[autodoc]] Sam3TrackerVideoMaskDecoderConfig\n+\n+## Sam3TrackerVideoPromptEncoderConfig\n+\n+[[autodoc]] Sam3TrackerVideoPromptEncoderConfig\n+\n+## Sam3TrackerVideoProcessor\n+\n+[[autodoc]] Sam3TrackerVideoProcessor\n+    - __call__\n+    - post_process_masks\n+    - init_video_session\n+    - add_inputs_to_inference_session\n+\n+## Sam3TrackerVideoInferenceSession\n+\n+[[autodoc]] Sam3TrackerVideoInferenceSession\n+\n+## Sam3TrackerVideoModel\n+\n+[[autodoc]] Sam3TrackerVideoModel\n+    - forward\n+    - propagate_in_video_iterator\n+"
        },
        {
            "sha": "4de5abafd2a348a3ae46ebc98afa7529bdbf0aee",
            "filename": "docs/source/en/model_doc/sam3_video.md",
            "status": "added",
            "additions": 183,
            "deletions": 0,
            "changes": 183,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3_video.md?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,183 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+*This model was released on 2025-11-19 and added to Hugging Face Transformers on 2025-11-19.*\n+\n+# SAM3 Video\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+    </div>\n+</div>\n+\n+## Overview\n+\n+SAM3 (Segment Anything Model 3) was introduced in [SAM 3: Segment Anything with Concepts](https://ai.meta.com/research/publications/sam-3-segment-anything-with-concepts/).\n+\n+SAM3 Video performs **Promptable Concept Segmentation (PCS)** on videos. PCS takes text as input (e.g., \"yellow school bus\"), and predicts instance and semantic masks for **every single object** matching the concept, while preserving object identities across video frames.\n+\n+The model combines a detection module (SAM3) with a tracking module (SAM2-style tracker) to enable robust object tracking across video frames using text prompts.\n+\n+The abstract from the paper is the following:\n+\n+*We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., \"yellow school bus\"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.*\n+\n+This model was contributed by [yonigozlan](https://huggingface.co/yonigozlan) and [ronghanghu](https://huggingface.co/ronghanghu).\n+\n+## Usage example\n+\n+### Video Segmentation and Tracking\n+\n+#### Pre-loaded Video Inference\n+\n+Process a video with all frames already available using text prompts:\n+\n+```python\n+>>> from transformers import Sam3VideoModel, Sam3VideoProcessor\n+>>> from accelerate import Accelerator\n+>>> import torch\n+\n+>>> device = Accelerator().device\n+>>> model = Sam3VideoModel.from_pretrained(\"facebook/sam3\").to(device, dtype=torch.bfloat16)\n+>>> processor = Sam3VideoProcessor.from_pretrained(\"facebook/sam3\")\n+\n+>>> # Load video frames\n+>>> from transformers.video_utils import load_video\n+>>> video_url = \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/bedroom.mp4\"\n+>>> video_frames, _ = load_video(video_url)\n+\n+>>> # Initialize video inference session\n+>>> inference_session = processor.init_video_session(\n+...     video=video_frames,\n+...     inference_device=device,\n+...     processing_device=\"cpu\",\n+...     video_storage_device=\"cpu\",\n+...     dtype=torch.bfloat16,\n+... )\n+\n+>>> # Add text prompt to detect and track objects\n+>>> text = \"person\"\n+>>> inference_session = processor.add_text_prompt(\n+...     inference_session=inference_session,\n+...     text=text,\n+... )\n+\n+>>> # Process all frames in the video\n+>>> outputs_per_frame = {}\n+>>> for model_outputs in model.propagate_in_video_iterator(\n+...     inference_session=inference_session, max_frame_num_to_track=50\n+... ):\n+...     processed_outputs = processor.postprocess_outputs(inference_session, model_outputs)\n+...     outputs_per_frame[model_outputs.frame_idx] = processed_outputs\n+\n+>>> print(f\"Processed {len(outputs_per_frame)} frames\")\n+Processed 51 frames\n+\n+>>> # Access results for a specific frame\n+>>> frame_0_outputs = outputs_per_frame[0]\n+>>> print(f\"Detected {len(frame_0_outputs['object_ids'])} objects\")\n+>>> print(f\"Object IDs: {frame_0_outputs['object_ids'].tolist()}\")\n+>>> print(f\"Scores: {frame_0_outputs['scores'].tolist()}\")\n+>>> print(f\"Boxes shape (XYXY format, absolute coordinates): {frame_0_outputs['boxes'].shape}\")\n+>>> print(f\"Masks shape: {frame_0_outputs['masks'].shape}\")\n+```\n+\n+#### Streaming Video Inference\n+\n+<div class=\"warning\">\n+âš ï¸ **Note on Streaming Inference Quality**: Streaming inference disables hotstart heuristics that remove unmatched and duplicate objects, as these require access to future frames to make informed decisions. This may result in more false positive detections and duplicate object tracks compared to pre-loaded video inference. For best results, use pre-loaded video inference when all frames are available.\n+</div>\n+\n+For real-time applications, SAM3 Video supports processing video frames as they arrive:\n+\n+```python\n+>>> # Initialize session for streaming\n+>>> streaming_inference_session = processor.init_video_session(\n+...     inference_device=device,\n+...     processing_device=\"cpu\",\n+...     video_storage_device=\"cpu\",\n+...     dtype=torch.bfloat16,\n+... )\n+\n+>>> # Add text prompt\n+>>> text = \"person\"\n+>>> streaming_inference_session = processor.add_text_prompt(\n+...     inference_session=streaming_inference_session,\n+...     text=text,\n+... )\n+\n+>>> # Process frames one by one (streaming mode)\n+>>> streaming_outputs_per_frame = {}\n+>>> for frame_idx, frame in enumerate(video_frames[:50]):  # Process first 50 frames\n+...     # First, process the frame using the processor\n+...     inputs = processor(images=frame, device=device, return_tensors=\"pt\")\n+...\n+...     # Process frame using streaming inference - pass the processed pixel_values\n+...     model_outputs = model(\n+...         inference_session=streaming_inference_session,\n+...         frame=inputs.pixel_values[0],  # Provide processed frame - this enables streaming mode\n+...         reverse=False,\n+...     )\n+...\n+...     # Post-process outputs with original_sizes for proper resolution handling\n+...     processed_outputs = processor.postprocess_outputs(\n+...         streaming_inference_session,\n+...         model_outputs,\n+...         original_sizes=inputs.original_sizes,  # Required for streaming inference\n+...     )\n+...     streaming_outputs_per_frame[frame_idx] = processed_outputs\n+...\n+...     if (frame_idx + 1) % 10 == 0:\n+...         print(f\"Processed {frame_idx + 1} frames...\")\n+\n+>>> print(f\"âœ“ Streaming inference complete! Processed {len(streaming_outputs_per_frame)} frames\")\n+âœ“ Streaming inference complete! Processed 50 frames\n+\n+>>> # Access results\n+>>> frame_0_outputs = streaming_outputs_per_frame[0]\n+>>> print(f\"Detected {len(frame_0_outputs['object_ids'])} objects in first frame\")\n+>>> print(f\"Boxes are in XYXY format (absolute pixel coordinates): {frame_0_outputs['boxes'].shape}\")\n+>>> print(f\"Masks are at original video resolution: {frame_0_outputs['masks'].shape}\")\n+```\n+\n+## Sam3VideoConfig\n+\n+[[autodoc]] Sam3VideoConfig\n+\n+## Sam3VideoProcessor\n+\n+[[autodoc]] Sam3VideoProcessor\n+    - __call__\n+    - postprocess_outputs\n+    - init_video_session\n+    - add_text_prompt\n+\n+## Sam3VideoInferenceSession\n+\n+[[autodoc]] Sam3VideoInferenceSession\n+\n+## Sam3VideoSegmentationOutput\n+\n+[[autodoc]] Sam3VideoSegmentationOutput\n+\n+## Sam3VideoModel\n+\n+[[autodoc]] Sam3VideoModel\n+    - forward\n+    - propagate_in_video_iterator\n+"
        },
        {
            "sha": "4e7306d0c0a50720efb27f0e0d3df0624cc8456b",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -208,6 +208,10 @@ def is_local_dist_rank_0():\n     \"qwen2_5_vl\",\n     \"videollava\",\n     \"vipllava\",\n+    \"sam3_video\",\n+    \"sam3\",\n+    \"sam3_tracker\",\n+    \"sam3_tracker_video\",\n ]\n \n "
        },
        {
            "sha": "75625aaff80fe1c9f0fc8fe8ccdbc1354ac46c7d",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -311,6 +311,8 @@\n     from .sam import *\n     from .sam2 import *\n     from .sam2_video import *\n+    from .sam3_tracker import *\n+    from .sam3_tracker_video import *\n     from .sam_hq import *\n     from .seamless_m4t import *\n     from .seamless_m4t_v2 import *"
        },
        {
            "sha": "c55980e471c7d401243e8b0791364ede5aa6127d",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 15,
            "deletions": 1,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -353,6 +353,12 @@\n         (\"sam2_hiera_det_model\", \"Sam2HieraDetConfig\"),\n         (\"sam2_video\", \"Sam2VideoConfig\"),\n         (\"sam2_vision_model\", \"Sam2VisionConfig\"),\n+        (\"sam3\", \"Sam3Config\"),\n+        (\"sam3_tracker\", \"Sam3TrackerConfig\"),\n+        (\"sam3_tracker_video\", \"Sam3TrackerVideoConfig\"),\n+        (\"sam3_video\", \"Sam3VideoConfig\"),\n+        (\"sam3_vision_model\", \"Sam3VisionConfig\"),\n+        (\"sam3_vit_model\", \"Sam3ViTConfig\"),\n         (\"sam_hq\", \"SamHQConfig\"),\n         (\"sam_hq_vision_model\", \"SamHQVisionConfig\"),\n         (\"sam_vision_model\", \"SamVisionConfig\"),\n@@ -795,6 +801,12 @@\n         (\"sam2_hiera_det_model\", \"Sam2HieraDetModel\"),\n         (\"sam2_video\", \"Sam2VideoModel\"),\n         (\"sam2_vision_model\", \"Sam2VisionModel\"),\n+        (\"sam3\", \"SAM3\"),\n+        (\"sam3_tracker\", \"Sam3Tracker\"),\n+        (\"sam3_tracker_video\", \"Sam3TrackerVideo\"),\n+        (\"sam3_video\", \"Sam3VideoModel\"),\n+        (\"sam3_vision_model\", \"Sam3VisionModel\"),\n+        (\"sam3_vit_model\", \"Sam3ViTModel\"),\n         (\"sam_hq\", \"SAM-HQ\"),\n         (\"sam_hq_vision_model\", \"SamHQVisionModel\"),\n         (\"sam_vision_model\", \"SamVisionModel\"),\n@@ -939,8 +951,10 @@\n         (\"qwen3_vl_moe_text\", \"qwen3_vl_moe\"),\n         (\"sam_vision_model\", \"sam\"),\n         (\"sam2_vision_model\", \"sam2\"),\n-        (\"edgetam_vision_model\", \"edgetam\"),\n         (\"sam2_hiera_det_model\", \"sam2\"),\n+        (\"sam3_vit_model\", \"sam3\"),\n+        (\"sam3_vision_model\", \"sam3\"),\n+        (\"edgetam_vision_model\", \"edgetam\"),\n         (\"sam_hq_vision_model\", \"sam_hq\"),\n         (\"llama4_text\", \"llama4\"),\n         (\"blip_2_qformer\", \"blip_2\"),"
        },
        {
            "sha": "5c2a30c34458b89605fa9336a1f0f951c240e16e",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -174,6 +174,8 @@\n             (\"sam\", (\"SamImageProcessor\", \"SamImageProcessorFast\")),\n             (\"sam2\", (None, \"Sam2ImageProcessorFast\")),\n             (\"sam2_video\", (None, \"Sam2ImageProcessorFast\")),\n+            (\"sam3\", (None, \"Sam3ImageProcessorFast\")),\n+            (\"sam3_video\", (None, \"Sam3ImageProcessorFast\")),\n             (\"sam_hq\", (\"SamImageProcessor\", \"SamImageProcessorFast\")),\n             (\"segformer\", (\"SegformerImageProcessor\", \"SegformerImageProcessorFast\")),\n             (\"seggpt\", (\"SegGptImageProcessor\", None)),"
        },
        {
            "sha": "22985f413341883a40ce1b5bcb52e7c881d00531",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -344,6 +344,13 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"sam2_hiera_det_model\", \"Sam2HieraDetModel\"),\n         (\"sam2_video\", \"Sam2VideoModel\"),\n         (\"sam2_vision_model\", \"Sam2VisionModel\"),\n+        (\"sam3\", \"Sam3Model\"),\n+        (\"sam3_tracker\", \"Sam3TrackerModel\"),\n+        (\"sam3_tracker\", \"Sam3TrackerModel\"),\n+        (\"sam3_tracker_video\", \"Sam3TrackerVideoModel\"),\n+        (\"sam3_video\", \"Sam3VideoModel\"),\n+        (\"sam3_vision_model\", \"Sam3VisionModel\"),\n+        (\"sam3_vit_model\", \"Sam3ViTModel\"),\n         (\"sam_hq\", \"SamHQModel\"),\n         (\"sam_hq_vision_model\", \"SamHQVisionModel\"),\n         (\"sam_vision_model\", \"SamVisionModel\"),\n@@ -1654,6 +1661,8 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"sam\", \"SamModel\"),\n         (\"sam2\", \"Sam2Model\"),\n         (\"sam2_video\", \"Sam2Model\"),\n+        (\"sam3_tracker\", \"Sam3TrackerModel\"),\n+        (\"sam3_video\", \"Sam3TrackerModel\"),\n         (\"sam_hq\", \"SamHQModel\"),\n     ]\n )"
        },
        {
            "sha": "9e6f4e66ff4db9bdfe2d30d27c68d571adab0d55",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -128,6 +128,7 @@\n         (\"qwen3_vl_moe\", \"Qwen3VLProcessor\"),\n         (\"sam\", \"SamProcessor\"),\n         (\"sam2\", \"Sam2Processor\"),\n+        (\"sam3\", \"Sam3Processor\"),\n         (\"sam_hq\", \"SamHQProcessor\"),\n         (\"seamless_m4t\", \"SeamlessM4TProcessor\"),\n         (\"sew\", \"Wav2Vec2Processor\"),"
        },
        {
            "sha": "d130230345d1713752fc87024703171a82b6bbb9",
            "filename": "src/transformers/models/auto/video_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -68,6 +68,7 @@\n             (\"qwen3_vl\", \"Qwen3VLVideoProcessor\"),\n             (\"qwen3_vl_moe\", \"Qwen3VLVideoProcessor\"),\n             (\"sam2_video\", \"Sam2VideoVideoProcessor\"),\n+            (\"sam3_video\", \"Sam3VideoVideoProcessor\"),\n             (\"smolvlm\", \"SmolVLMVideoProcessor\"),\n             (\"video_llama_3\", \"VideoLlama3VideoProcessor\"),\n             (\"video_llava\", \"VideoLlavaVideoProcessor\"),"
        },
        {
            "sha": "45b6f4fc09f52cf413cd018b0e11f4b7218d5177",
            "filename": "src/transformers/models/edgetam_video/configuration_edgetam_video.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fconfiguration_edgetam_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fconfiguration_edgetam_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fconfiguration_edgetam_video.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -187,6 +187,8 @@ class EdgeTamVideoConfig(PreTrainedConfig):\n             Whether to use multimask output for tracking.\n         max_object_pointers_in_encoder (`int`, *optional*, defaults to 16):\n             The maximum number of object pointers in the encoder.\n+        max_cond_frame_num (`int`, *optional*, defaults to -1):\n+            Maximum number of conditioning frames to use in memory attention. Set to -1 to use all conditioning frames.\n         enable_temporal_pos_encoding_for_object_pointers (`bool`, *optional*, defaults to `True`):\n             Whether to enable temporal positional encoding for object pointers.\n         memory_attention_hidden_size (`int`, *optional*, defaults to 256):\n@@ -313,6 +315,7 @@ def __init__(\n         multimask_max_pt_num=1,\n         multimask_output_for_tracking=True,\n         max_object_pointers_in_encoder=16,\n+        max_cond_frame_num=-1,\n         enable_temporal_pos_encoding_for_object_pointers=True,\n         # memory attention\n         memory_attention_hidden_size=256,\n@@ -388,6 +391,7 @@ def __init__(\n         self.multimask_max_pt_num = multimask_max_pt_num\n         self.multimask_output_for_tracking = multimask_output_for_tracking\n         self.max_object_pointers_in_encoder = max_object_pointers_in_encoder\n+        self.max_cond_frame_num = max_cond_frame_num\n         self.enable_temporal_pos_encoding_for_object_pointers = enable_temporal_pos_encoding_for_object_pointers\n \n         # memory attention"
        },
        {
            "sha": "2d8f3e133598c228ed42c16b4ef8e4c1b8483ca1",
            "filename": "src/transformers/models/edgetam_video/modeling_edgetam_video.py",
            "status": "modified",
            "additions": 69,
            "deletions": 4,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodeling_edgetam_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodeling_edgetam_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodeling_edgetam_video.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -1531,13 +1531,19 @@ class EdgeTamVideoImageSegmentationOutput(ModelOutput):\n @auto_docstring(custom_intro=\"Base class for the Sam2 model's output.\")\n class EdgeTamVideoSegmentationOutput(ModelOutput):\n     r\"\"\"\n+    object_ids (`list[int]`, *optional*):\n+        List of object IDs being tracked in the current frame.\n     pred_masks (`torch.FloatTensor` of shape `(batch_size, num_masks, height, width)`):\n         The predicted masks stored at the model's resolution.\n+    object_score_logits (`torch.FloatTensor` of shape `(batch_size,)`, *optional*):\n+        Logits for the object scores, indicating if objects are present.\n     frame_idx (`int`):\n         The frame index of the video.\n     \"\"\"\n \n+    object_ids: Optional[list[int]] = None\n     pred_masks: Optional[torch.FloatTensor] = None\n+    object_score_logits: Optional[torch.FloatTensor] = None\n     frame_idx: Optional[int] = None\n \n \n@@ -2131,6 +2137,7 @@ def forward(\n \n         num_objects = inference_session.get_obj_num()\n         pred_masks_per_obj = [None] * num_objects\n+        object_score_logits_per_obj = [None] * num_objects\n         # Note: We avoid batched inference here because per-object inputs (clicks/masks)\n         # can differ across objects.\n         for obj_idx in range(num_objects):\n@@ -2141,6 +2148,9 @@ def forward(\n             # conditioning output, reuse the cached masks instead of recomputing.\n             if (not has_new_inputs) and has_cond_output:\n                 pred_masks = inference_session.get_output(obj_idx, frame_idx, \"pred_masks\", is_conditioning_frame=True)\n+                object_score_logits = inference_session.get_output(\n+                    obj_idx, frame_idx, \"object_score_logits\", is_conditioning_frame=True\n+                )\n                 is_init_cond_frame = True\n             else:\n                 # Defaults when there are no new inputs\n@@ -2173,8 +2183,10 @@ def forward(\n                     obj_idx, frame_idx, output_value=current_out, is_conditioning_frame=is_init_cond_frame\n                 )\n                 pred_masks = current_out[\"pred_masks\"]\n+                object_score_logits = current_out[\"object_score_logits\"]\n \n             pred_masks_per_obj[obj_idx] = pred_masks\n+            object_score_logits_per_obj[obj_idx] = object_score_logits.squeeze(-1)\n             if not is_init_cond_frame:\n                 # only for tracked frames, not for initial conditioning frames\n                 inference_session.frames_tracked_per_obj[obj_idx][frame_idx] = {\"reverse\": reverse}\n@@ -2183,10 +2195,17 @@ def forward(\n         # the mask scores on GPU for output to avoid any CPU conversion in between)\n         if len(pred_masks_per_obj) > 1:\n             all_pred_masks = torch.cat(pred_masks_per_obj, dim=0)\n+            all_object_score_logits = torch.cat(object_score_logits_per_obj, dim=0)\n         else:\n             all_pred_masks = pred_masks_per_obj[0]\n+            all_object_score_logits = object_score_logits_per_obj[0]\n \n-        return EdgeTamVideoSegmentationOutput(pred_masks=all_pred_masks, frame_idx=frame_idx)\n+        return EdgeTamVideoSegmentationOutput(\n+            object_ids=inference_session.obj_ids.copy(),\n+            pred_masks=all_pred_masks,\n+            object_score_logits=all_object_score_logits,\n+            frame_idx=frame_idx,\n+        )\n \n     def get_image_features(\n         self,\n@@ -2507,6 +2526,46 @@ def _use_mask_as_output(\n             image_embeddings=high_res_features + [backbone_features],\n         )\n \n+    def _select_closest_cond_frames(self, frame_idx, cond_frame_outputs, max_cond_frame_num):\n+        \"\"\"\n+        Select up to `max_cond_frame_num` conditioning frames from `cond_frame_outputs`\n+        that are temporally closest to the current frame at `frame_idx`. Here, we take\n+        - a) the closest conditioning frame before `frame_idx` (if any);\n+        - b) the closest conditioning frame after `frame_idx` (if any);\n+        - c) any other temporally closest conditioning frames until reaching a total\n+            of `max_cond_frame_num` conditioning frames.\n+\n+        Outputs:\n+        - selected_outputs: selected items (keys & values) from `cond_frame_outputs`.\n+        - unselected_outputs: items (keys & values) not selected in `cond_frame_outputs`.\n+        \"\"\"\n+        if max_cond_frame_num == -1 or len(cond_frame_outputs) <= max_cond_frame_num:\n+            selected_outputs = cond_frame_outputs\n+            unselected_outputs = {}\n+        else:\n+            selected_outputs = {}\n+            # the closest conditioning frame before `frame_idx` (if any)\n+            idx_before = max((t for t in cond_frame_outputs if t < frame_idx), default=None)\n+            if idx_before is not None:\n+                selected_outputs[idx_before] = cond_frame_outputs[idx_before]\n+\n+            # the closest conditioning frame after `frame_idx` (if any)\n+            idx_after = min((t for t in cond_frame_outputs if t >= frame_idx), default=None)\n+            if idx_after is not None:\n+                selected_outputs[idx_after] = cond_frame_outputs[idx_after]\n+\n+            # add other temporally closest conditioning frames until reaching a total\n+            # of `max_cond_frame_num` conditioning frames.\n+            num_remain = max_cond_frame_num - len(selected_outputs)\n+            inds_remain = sorted(\n+                (t for t in cond_frame_outputs if t not in selected_outputs),\n+                key=lambda x: abs(x - frame_idx),\n+            )[:num_remain]\n+            selected_outputs.update((t, cond_frame_outputs[t]) for t in inds_remain)\n+            unselected_outputs = {t: v for t, v in cond_frame_outputs.items() if t not in selected_outputs}\n+\n+        return selected_outputs, unselected_outputs\n+\n     def _gather_memory_frame_outputs(\n         self,\n         inference_session: EdgeTamVideoInferenceSession,\n@@ -2522,12 +2581,15 @@ def _gather_memory_frame_outputs(\n         \"\"\"\n         temporal_positions_and_previous_outputs = []\n \n-        # Add conditioning frame outputs (no limit here, as is the case in the original checkpoints)\n+        # Add conditioning frame outputs (limited by max_cond_frame_num)\n         conditioning_outputs = inference_session.output_dict_per_obj[obj_idx][\"cond_frame_outputs\"]\n         if not conditioning_outputs:\n             raise ValueError(\n                 \"maskmem_features in conditioning outputs cannot be empty when not is_initial_conditioning_frame\"\n             )\n+        conditioning_outputs, unselected_conditioning_outputs = self._select_closest_cond_frames(\n+            frame_idx, conditioning_outputs, max_cond_frame_num=self.config.max_cond_frame_num\n+        )\n \n         # Store (temporal_position, output_data) tuples\n         temporal_positions_and_previous_outputs = [(0, out) for out in conditioning_outputs.values()]\n@@ -2544,7 +2606,7 @@ def _gather_memory_frame_outputs(\n \n             # check if the output is already stored without using get_output to avoid unnecessary memory transfers between CPU and GPU\n             output_data = inference_session.output_dict_per_obj[obj_idx][\"non_cond_frame_outputs\"].get(\n-                previous_frame_idx, None\n+                previous_frame_idx, unselected_conditioning_outputs.get(previous_frame_idx, None)\n             )\n \n             temporal_positions_and_previous_outputs.append((relative_temporal_offset, output_data))\n@@ -3003,6 +3065,7 @@ def propagate_in_video_iterator(\n         start_frame_idx: Optional[int] = None,\n         max_frame_num_to_track: Optional[int] = None,\n         reverse: bool = False,\n+        show_progress_bar: bool = False,\n     ) -> Iterator[EdgeTamVideoSegmentationOutput]:\n         r\"\"\"\n         inference_session (`EdgeTamVideoInferenceSession`):\n@@ -3015,6 +3078,8 @@ def propagate_in_video_iterator(\n             The maximum number of frames to track.\n         reverse (`bool`, *optional*, defaults to `False`):\n             Whether to propagate in reverse.\n+        show_progress_bar (`bool`, *optional*, defaults to `False`):\n+            Whether to show a progress bar during propagation.\n         \"\"\"\n         num_frames = inference_session.num_frames\n \n@@ -3044,7 +3109,7 @@ def propagate_in_video_iterator(\n             end_frame_idx = min(start_frame_idx + max_frame_num_to_track, num_frames - 1)\n             processing_order = range(start_frame_idx, end_frame_idx + 1)\n \n-        for frame_idx in tqdm(processing_order, desc=\"propagate in video\"):\n+        for frame_idx in tqdm(processing_order, desc=\"propagate in video\", disable=not show_progress_bar):\n             edgetam_video_output = self(inference_session, frame_idx=frame_idx, reverse=reverse)\n             yield edgetam_video_output\n "
        },
        {
            "sha": "8239ad027b8f36440fdceeec283f6539c0693b3b",
            "filename": "src/transformers/models/edgetam_video/modular_edgetam_video.py",
            "status": "modified",
            "additions": 285,
            "deletions": 1,
            "changes": 286,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodular_edgetam_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodular_edgetam_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodular_edgetam_video.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -15,10 +15,11 @@\n \n import math\n from collections.abc import Callable\n-from typing import Optional\n+from typing import Any, Optional\n \n import torch\n import torch.nn as nn\n+import torch.nn.functional as F\n import torch.utils.checkpoint\n from torch import Tensor\n \n@@ -46,6 +47,7 @@\n from ..sam2_video.modeling_sam2_video import (\n     Sam2VideoAttention,\n     Sam2VideoFeedForward,\n+    Sam2VideoImageSegmentationOutput,\n     Sam2VideoInferenceSession,\n     Sam2VideoLayerNorm,\n     Sam2VideoMemoryAttention,\n@@ -54,6 +56,7 @@\n     Sam2VideoModel,\n     Sam2VideoPositionEmbeddingSine,\n     Sam2VideoPreTrainedModel,\n+    Sam2VideoSegmentationOutput,\n     Sam2VideoTwoWayAttentionBlock,\n     Sam2VideoVisionEncoderOutput,\n     Sam2VideoVisionRotaryEmbedding,\n@@ -108,6 +111,8 @@ class EdgeTamVideoConfig(Sam2VideoConfig):\n             Whether to use multimask output for tracking.\n         max_object_pointers_in_encoder (`int`, *optional*, defaults to 16):\n             The maximum number of object pointers in the encoder.\n+        max_cond_frame_num (`int`, *optional*, defaults to -1):\n+            Maximum number of conditioning frames to use in memory attention. Set to -1 to use all conditioning frames.\n         enable_temporal_pos_encoding_for_object_pointers (`bool`, *optional*, defaults to `True`):\n             Whether to enable temporal positional encoding for object pointers.\n         memory_attention_hidden_size (`int`, *optional*, defaults to 256):\n@@ -234,6 +239,7 @@ def __init__(\n         multimask_max_pt_num=1,\n         multimask_output_for_tracking=True,\n         max_object_pointers_in_encoder=16,\n+        max_cond_frame_num=-1,\n         enable_temporal_pos_encoding_for_object_pointers=True,\n         # memory attention\n         memory_attention_hidden_size=256,\n@@ -309,6 +315,7 @@ def __init__(\n         self.multimask_max_pt_num = multimask_max_pt_num\n         self.multimask_output_for_tracking = multimask_output_for_tracking\n         self.max_object_pointers_in_encoder = max_object_pointers_in_encoder\n+        self.max_cond_frame_num = max_cond_frame_num\n         self.enable_temporal_pos_encoding_for_object_pointers = enable_temporal_pos_encoding_for_object_pointers\n \n         # memory attention\n@@ -1023,6 +1030,14 @@ def _forward_2d(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.\n         return latents_2d, positional_encoding_2d\n \n \n+class EdgeTamVideoImageSegmentationOutput(Sam2VideoImageSegmentationOutput):\n+    pass\n+\n+\n+class EdgeTamVideoSegmentationOutput(Sam2VideoSegmentationOutput):\n+    pass\n+\n+\n @auto_docstring\n class EdgeTamVideoModel(Sam2VideoModel):\n     _tied_weights_keys = {\n@@ -1235,6 +1250,275 @@ def _encode_new_memory(\n \n         return maskmem_features, maskmem_pos_enc\n \n+    def forward(\n+        self,\n+        inference_session: EdgeTamVideoInferenceSession,\n+        frame_idx: Optional[int] = None,\n+        frame: Optional[torch.Tensor] = None,\n+        reverse: bool = False,\n+    ) -> EdgeTamVideoSegmentationOutput:\n+        r\"\"\"\n+        inference_session (`EdgeTamVideoInferenceSession`):\n+            The video inference session object.\n+        frame_idx (`int`, *optional*):\n+            The index of the frame on which to run inference. No need to provide when inferring\n+            on a new streamed frame.\n+        frame (`torch.Tensor`, *optional*):\n+            The frame to process. Provide when streaming.\n+        reverse (`bool`, *optional*, defaults to `False`):\n+            Whether to propagate in reverse.\n+        \"\"\"\n+        if frame is not None:\n+            frame_idx = inference_session.add_new_frame(frame, frame_idx)\n+\n+        if frame is not None and inference_session.get_obj_num() == 0:\n+            raise ValueError(\"No objects are provided for tracking; please add inputs first.\")\n+\n+        num_objects = inference_session.get_obj_num()\n+        pred_masks_per_obj = [None] * num_objects\n+        object_score_logits_per_obj = [None] * num_objects\n+        # Note: We avoid batched inference here because per-object inputs (clicks/masks)\n+        # can differ across objects.\n+        for obj_idx in range(num_objects):\n+            obj_id = inference_session.obj_idx_to_id(obj_idx)\n+            has_new_inputs = obj_id in inference_session.obj_with_new_inputs\n+            has_cond_output = frame_idx in inference_session.output_dict_per_obj[obj_idx][\"cond_frame_outputs\"]\n+            # If this object has no new inputs and this frame already has a\n+            # conditioning output, reuse the cached masks instead of recomputing.\n+            if (not has_new_inputs) and has_cond_output:\n+                pred_masks = inference_session.get_output(obj_idx, frame_idx, \"pred_masks\", is_conditioning_frame=True)\n+                object_score_logits = inference_session.get_output(\n+                    obj_idx, frame_idx, \"object_score_logits\", is_conditioning_frame=True\n+                )\n+                is_init_cond_frame = True\n+            else:\n+                # Defaults when there are no new inputs\n+                is_init_cond_frame = False\n+                point_inputs = None\n+                mask_inputs = None\n+\n+                if has_new_inputs:\n+                    is_init_cond_frame = frame_idx not in inference_session.frames_tracked_per_obj[obj_idx]\n+                    if is_init_cond_frame:\n+                        reverse = False\n+                    point_inputs = inference_session.point_inputs_per_obj[obj_idx].get(frame_idx, None)\n+                    mask_inputs = inference_session.mask_inputs_per_obj[obj_idx].get(frame_idx, None)\n+                    if point_inputs is not None or mask_inputs is not None:\n+                        inference_session.obj_with_new_inputs.remove(obj_id)\n+\n+                current_out = self._run_single_frame_inference(\n+                    inference_session=inference_session,\n+                    obj_idx=obj_idx,\n+                    frame_idx=frame_idx,\n+                    batch_size=1,  # run on the slice of a single object\n+                    is_init_cond_frame=is_init_cond_frame,\n+                    point_inputs=point_inputs,\n+                    mask_inputs=mask_inputs,\n+                    reverse=reverse,\n+                    run_mem_encoder=True,\n+                    streaming=frame is not None,\n+                )\n+                inference_session.store_output(\n+                    obj_idx, frame_idx, output_value=current_out, is_conditioning_frame=is_init_cond_frame\n+                )\n+                pred_masks = current_out[\"pred_masks\"]\n+                object_score_logits = current_out[\"object_score_logits\"]\n+\n+            pred_masks_per_obj[obj_idx] = pred_masks\n+            object_score_logits_per_obj[obj_idx] = object_score_logits.squeeze(-1)\n+            if not is_init_cond_frame:\n+                # only for tracked frames, not for initial conditioning frames\n+                inference_session.frames_tracked_per_obj[obj_idx][frame_idx] = {\"reverse\": reverse}\n+\n+        # Resize the output mask to the original video resolution (we directly use\n+        # the mask scores on GPU for output to avoid any CPU conversion in between)\n+        if len(pred_masks_per_obj) > 1:\n+            all_pred_masks = torch.cat(pred_masks_per_obj, dim=0)\n+            all_object_score_logits = torch.cat(object_score_logits_per_obj, dim=0)\n+        else:\n+            all_pred_masks = pred_masks_per_obj[0]\n+            all_object_score_logits = object_score_logits_per_obj[0]\n+\n+        return EdgeTamVideoSegmentationOutput(\n+            object_ids=inference_session.obj_ids.copy(),\n+            pred_masks=all_pred_masks,\n+            object_score_logits=all_object_score_logits,\n+            frame_idx=frame_idx,\n+        )\n+\n+    def _use_mask_as_output(\n+        self,\n+        backbone_features: torch.Tensor,\n+        high_res_features: list[torch.Tensor],\n+        mask_inputs: torch.Tensor,\n+    ) -> EdgeTamVideoImageSegmentationOutput:\n+        \"\"\"\n+        Directly turn binary `mask_inputs` into a output mask logits without using SAM.\n+        (same input and output shapes as in forward above).\n+        \"\"\"\n+        # Use -10/+20 as logits for neg/pos pixels (very close to 0/1 in prob after sigmoid).\n+        out_scale, out_bias = 20.0, -10.0  # sigmoid(-10.0)=4.5398e-05\n+        mask_inputs_float = mask_inputs.to(backbone_features[0].dtype)\n+        high_res_masks = mask_inputs_float * out_scale + out_bias\n+        low_res_masks = F.interpolate(\n+            high_res_masks.float(),\n+            size=(high_res_masks.size(-2) // 4, high_res_masks.size(-1) // 4),\n+            align_corners=False,\n+            mode=\"bilinear\",\n+            antialias=True,  # use antialias for downsampling\n+        ).to(backbone_features[0].dtype)\n+        # a dummy IoU prediction of all 1's under mask input\n+        iou_scores = mask_inputs.new_ones(mask_inputs.size(0), 1).to(backbone_features[0].dtype)\n+        # produce an object pointer using the SAM decoder from the mask input\n+        object_pointer = self._single_frame_forward(\n+            input_masks=self.mask_downsample(mask_inputs_float.to(backbone_features[0].dtype)),\n+            image_embeddings=high_res_features + [backbone_features],\n+        ).object_pointer\n+        # In this method, we are treating mask_input as output, e.g. using it directly to create spatial mem;\n+        # Below, we follow the same design axiom to use mask_input to decide if obj appears or not instead of relying\n+        # on the object_scores from the SAM decoder.\n+        is_obj_appearing = torch.any(mask_inputs.flatten(1).float() > 0.0, dim=1)\n+        is_obj_appearing = is_obj_appearing[..., None]\n+        lambda_is_obj_appearing = is_obj_appearing.to(backbone_features[0].dtype)\n+        object_score_logits = out_scale * lambda_is_obj_appearing + out_bias\n+        object_pointer = lambda_is_obj_appearing * object_pointer\n+        object_pointer = object_pointer + (1 - lambda_is_obj_appearing) * self.no_object_pointer\n+        return EdgeTamVideoImageSegmentationOutput(\n+            iou_scores=iou_scores,\n+            pred_masks=low_res_masks,\n+            high_res_masks=high_res_masks,\n+            object_pointer=object_pointer,\n+            object_score_logits=object_score_logits,\n+            image_embeddings=high_res_features + [backbone_features],\n+        )\n+\n+    def _run_single_frame_inference(\n+        self,\n+        inference_session: EdgeTamVideoInferenceSession,\n+        frame_idx: int,\n+        obj_idx: int,\n+        batch_size: int,\n+        is_init_cond_frame: bool,\n+        point_inputs: Optional[torch.Tensor],\n+        mask_inputs: Optional[torch.Tensor],\n+        reverse: bool,\n+        run_mem_encoder: bool,\n+        prev_sam_mask_logits: Optional[torch.Tensor] = None,\n+        streaming: bool = False,\n+    ) -> dict[str, Any]:\n+        \"\"\"\n+        Perform a single tracking step for video object segmentation.\n+\n+        Args:\n+            inference_session (`EdgeTamVideoInferenceSession`):\n+                The video inference session object.\n+            frame_idx (`int`):\n+                Index of the current frame.\n+            obj_idx (`int`):\n+                Index of the current object.\n+            batch_size (`int`):\n+                Batch size of the current frame.\n+            is_init_cond_frame (`bool`):\n+                Whether this is an initial conditioning frame with user inputs.\n+            point_inputs (`dict`, *optional*):\n+                Point prompt inputs for the current frame.\n+            mask_inputs (`torch.Tensor`, *optional*):\n+                Mask prompt inputs for the current frame.\n+            reverse (`bool`, *optional*, defaults to `False`):\n+                Whether to track in reverse time order.\n+            run_mem_encoder (`bool`, *optional*, defaults to `True`):\n+                Whether to run the memory encoder on predicted masks.\n+            prev_sam_mask_logits (`torch.Tensor`, *optional*):\n+                Previously predicted SAM mask logits that can be fed with new clicks.\n+            streaming (`bool`, *optional*, defaults to `False`):\n+                Whether this is streaming inference.\n+\n+        Returns:\n+            `dict`: Dictionary containing the tracking results for the current frame, including:\n+                - pred_masks: Predicted low-resolution masks.\n+                - object_pointer: Object pointer for memory.\n+                - object_score_logits: Object score logits (inference only).\n+                - maskmem_features: Memory features for future frames.\n+                - maskmem_pos_enc: Memory positional encodings.\n+        \"\"\"\n+        # Retrieve correct image features\n+        current_vision_feats, current_vision_pos_embeds = self._prepare_vision_features(\n+            inference_session, frame_idx, batch_size\n+        )\n+        # point and mask should not appear as input simultaneously on the same frame\n+        if point_inputs is not None and mask_inputs is not None:\n+            raise ValueError(\n+                \"point_inputs and mask_inputs should not appear as input simultaneously on the same frame\"\n+            )\n+        # High-resolution feature maps for the SAM head, reshape (HW)BC => BCHW\n+        if len(current_vision_feats) > 1:\n+            high_res_features = [\n+                x.permute(1, 2, 0).view(x.size(1), x.size(2), *s)\n+                for x, s in zip(current_vision_feats[:-1], self.backbone_feature_sizes[:-1])\n+            ]\n+        else:\n+            high_res_features = None\n+        if mask_inputs is not None:\n+            # We directly output the mask input (see it as a GT mask) without using a SAM prompt encoder + mask decoder.\n+            pix_feat = current_vision_feats[-1].permute(1, 2, 0)\n+            pix_feat = pix_feat.view(-1, self.hidden_dim, *self.backbone_feature_sizes[-1])\n+            sam_outputs = self._use_mask_as_output(pix_feat, high_res_features, mask_inputs)\n+        else:\n+            # fused the visual feature with previous memory features in the memory bank\n+            pix_feat = self._prepare_memory_conditioned_features(\n+                inference_session=inference_session,\n+                frame_idx=frame_idx,\n+                obj_idx=obj_idx,\n+                is_initial_conditioning_frame=is_init_cond_frame,\n+                current_vision_features=current_vision_feats[-1],\n+                current_vision_positional_embeddings=current_vision_pos_embeds[-1],\n+                num_total_frames=inference_session.num_frames,\n+                track_in_reverse_time=reverse,\n+                streaming=streaming,\n+            )\n+            # apply SAM-style segmentation head\n+            # here we might feed previously predicted low-res SAM mask logits into the SAM mask decoder,\n+            # e.g. in demo where such logits come from earlier interaction instead of correction sampling\n+            # (in this case, any `mask_inputs` shouldn't reach here as they are sent to _use_mask_as_output instead)\n+            if prev_sam_mask_logits is not None:\n+                mask_inputs = prev_sam_mask_logits\n+            multimask_output = self._use_multimask(is_init_cond_frame, point_inputs)\n+            sam_outputs = self._single_frame_forward(\n+                pixel_values=None,  # Vision features already computed\n+                input_points=point_inputs[\"point_coords\"] if point_inputs is not None else None,\n+                input_labels=point_inputs[\"point_labels\"] if point_inputs is not None else None,\n+                input_masks=mask_inputs,\n+                image_embeddings=high_res_features + [pix_feat],\n+                multimask_output=multimask_output,\n+            )\n+\n+        # Finally run the memory encoder on the predicted mask to encode\n+        # it into a new memory feature (which will be used to condition vision features in future frames)\n+        maskmem_features = None\n+        maskmem_pos_enc = None\n+        if run_mem_encoder and self.num_maskmem > 0:\n+            maskmem_features, maskmem_pos_enc = self._encode_new_memory(\n+                current_vision_feats=current_vision_feats[-1],\n+                pred_masks_high_res=sam_outputs.high_res_masks,\n+                object_score_logits=sam_outputs.object_score_logits,\n+                is_mask_from_pts=(point_inputs is not None or mask_inputs is not None),\n+            )\n+\n+        current_out = {\n+            \"pred_masks\": sam_outputs.pred_masks,\n+            \"object_pointer\": sam_outputs.object_pointer,\n+            \"maskmem_features\": maskmem_features if maskmem_features is not None else None,\n+            \"maskmem_pos_enc\": maskmem_pos_enc,\n+        }\n+        if not self.training:\n+            current_out[\"object_score_logits\"] = sam_outputs.object_score_logits\n+\n+        return current_out\n+\n+    def _batch_encode_memories(self):\n+        raise NotImplementedError(\"Batch memory encoding is not implemented for EdgeTamVideo yet.\")\n+        # Todo, implement batch memory encoding for edgetam video\n+\n \n __all__ = [\n     \"EdgeTamVideoMaskDecoderConfig\","
        },
        {
            "sha": "68b9a55f04fd496f00eae5061261fcca54aa8afb",
            "filename": "src/transformers/models/sam2/image_processing_sam2_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam2%2Fimage_processing_sam2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam2%2Fimage_processing_sam2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fimage_processing_sam2_fast.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -463,11 +463,9 @@ def _preprocess_image_like_inputs(\n         original_sizes = [image.shape[-2:] for image in images]\n         images_kwargs = kwargs.copy()\n         pixel_values = self._preprocess(images, **images_kwargs)\n-        reshaped_input_sizes = [image.shape[-2:] for image in images]\n         data = {\n             \"pixel_values\": pixel_values,\n             \"original_sizes\": original_sizes,\n-            \"reshaped_input_sizes\": reshaped_input_sizes,\n         }\n \n         if segmentation_maps is not None:\n@@ -695,9 +693,6 @@ def post_process_for_mask_generation(self, all_masks, all_scores, all_boxes, cro\n         \"\"\"\n         return _post_process_for_mask_generation(all_masks, all_scores, all_boxes, crops_nms_thresh)\n \n-    def pad_image(self):\n-        raise NotImplementedError(\"No pad_image for SAM 2.\")\n-\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],"
        },
        {
            "sha": "0c40c989fe009f64d65bee09d5731a9a24409863",
            "filename": "src/transformers/models/sam2/modular_sam2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 11,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -103,15 +103,6 @@ class Sam2ImageProcessorFast(SamImageProcessorFast):\n     def __init__(self, **kwargs: Unpack[Sam2FastImageProcessorKwargs]):\n         BaseImageProcessorFast.__init__(self, **kwargs)\n \n-    def pad_image(self):\n-        raise NotImplementedError(\"No pad_image for SAM 2.\")\n-\n-    def _get_preprocess_shape(self):\n-        raise NotImplementedError(\"No _get_preprocess_shape for SAM 2.\")\n-\n-    def resize(self):\n-        raise NotImplementedError(\"No need to override resize for SAM 2.\")\n-\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n@@ -151,11 +142,9 @@ def _preprocess_image_like_inputs(\n         original_sizes = [image.shape[-2:] for image in images]\n         images_kwargs = kwargs.copy()\n         pixel_values = self._preprocess(images, **images_kwargs)\n-        reshaped_input_sizes = [image.shape[-2:] for image in images]\n         data = {\n             \"pixel_values\": pixel_values,\n             \"original_sizes\": original_sizes,\n-            \"reshaped_input_sizes\": reshaped_input_sizes,\n         }\n \n         if segmentation_maps is not None:\n@@ -299,6 +288,12 @@ def post_process_masks(\n \n         return output_masks\n \n+    def _get_preprocess_shape(self):\n+        raise NotImplementedError(\"No _get_preprocess_shape for SAM 2.\")\n+\n+    def resize(self):\n+        raise NotImplementedError(\"No need to override resize for SAM 2.\")\n+\n \n @dataclass\n @auto_docstring(custom_intro=\"Base class for the vision encoder's outputs.\")"
        },
        {
            "sha": "05dbe7347edd64742f6a5028b32c00dc5d777206",
            "filename": "src/transformers/models/sam2/processing_sam2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam2%2Fprocessing_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam2%2Fprocessing_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fprocessing_sam2.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -94,7 +94,6 @@ def __call__(\n             A [`BatchEncoding`] with the following fields:\n             - `pixel_values` (`torch.Tensor`): The processed image(s).\n             - `original_sizes` (`list[list[float]]`): The original sizes of the images.\n-            - `reshaped_input_sizes` (`torch.Tensor`): The reshaped input sizes of the images.\n             - `labels` (`torch.Tensor`): The processed segmentation maps (if provided).\n             - `input_points` (`torch.Tensor`): The processed points.\n             - `input_labels` (`torch.Tensor`): The processed labels."
        },
        {
            "sha": "a33af36fe2d60281eb4ab17e55fc790be9663bb5",
            "filename": "src/transformers/models/sam2_video/configuration_sam2_video.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fconfiguration_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fconfiguration_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fconfiguration_sam2_video.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -186,6 +186,8 @@ class Sam2VideoConfig(PreTrainedConfig):\n             Whether to use multimask output for tracking.\n         max_object_pointers_in_encoder (`int`, *optional*, defaults to 16):\n             The maximum number of object pointers in the encoder.\n+        max_cond_frame_num (`int`, *optional*, defaults to -1):\n+            Maximum number of conditioning frames to use in memory attention. Set to -1 to use all conditioning frames.\n         enable_temporal_pos_encoding_for_object_pointers (`bool`, *optional*, defaults to `True`):\n             Whether to enable temporal positional encoding for object pointers.\n         memory_attention_hidden_size (`int`, *optional*, defaults to 256):\n@@ -293,6 +295,7 @@ def __init__(\n         multimask_max_pt_num=1,\n         multimask_output_for_tracking=True,\n         max_object_pointers_in_encoder=16,\n+        max_cond_frame_num=-1,\n         enable_temporal_pos_encoding_for_object_pointers=True,\n         # memory attention\n         memory_attention_hidden_size=256,\n@@ -354,6 +357,7 @@ def __init__(\n         self.multimask_max_pt_num = multimask_max_pt_num\n         self.multimask_output_for_tracking = multimask_output_for_tracking\n         self.max_object_pointers_in_encoder = max_object_pointers_in_encoder\n+        self.max_cond_frame_num = max_cond_frame_num\n         # The next 4 are True for sam2.1 and False for sam2\n         self.enable_occlusion_spatial_embedding = enable_occlusion_spatial_embedding\n         self.enable_temporal_pos_encoding_for_object_pointers = enable_temporal_pos_encoding_for_object_pointers"
        },
        {
            "sha": "8ab33daf541566fc4d4cecec9b610fce7baa32c5",
            "filename": "src/transformers/models/sam2_video/modeling_sam2_video.py",
            "status": "modified",
            "additions": 183,
            "deletions": 26,
            "changes": 209,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -647,13 +647,19 @@ class Sam2VideoImageSegmentationOutput(ModelOutput):\n @auto_docstring(custom_intro=\"Base class for the Sam2 model's output.\")\n class Sam2VideoSegmentationOutput(ModelOutput):\n     r\"\"\"\n+    object_ids (`list[int]`, *optional*):\n+        List of object IDs being tracked in the current frame.\n     pred_masks (`torch.FloatTensor` of shape `(batch_size, num_masks, height, width)`):\n         The predicted masks stored at the model's resolution.\n+    object_score_logits (`torch.FloatTensor` of shape `(batch_size,)`, *optional*):\n+        Logits for the object scores, indicating if objects are present.\n     frame_idx (`int`):\n         The frame index of the video.\n     \"\"\"\n \n+    object_ids: Optional[list[int]] = None\n     pred_masks: Optional[torch.FloatTensor] = None\n+    object_score_logits: Optional[torch.FloatTensor] = None\n     frame_idx: Optional[int] = None\n \n \n@@ -1693,6 +1699,7 @@ def forward(\n         frame_idx: Optional[int] = None,\n         frame: Optional[torch.Tensor] = None,\n         reverse: bool = False,\n+        run_mem_encoder: bool = True,\n     ) -> Sam2VideoSegmentationOutput:\n         r\"\"\"\n         inference_session (`Sam2VideoInferenceSession`):\n@@ -1704,6 +1711,8 @@ def forward(\n             The frame to process. Provide when streaming.\n         reverse (`bool`, *optional*, defaults to `False`):\n             Whether to propagate in reverse.\n+        run_mem_encoder (`bool`, *optional*, defaults to `True`):\n+            Whether to run the memory encoder on predicted masks. The memory encoder is batched across all objects for efficiency.\n         \"\"\"\n         if frame is not None:\n             frame_idx = inference_session.add_new_frame(frame, frame_idx)\n@@ -1713,6 +1722,14 @@ def forward(\n \n         num_objects = inference_session.get_obj_num()\n         pred_masks_per_obj = [None] * num_objects\n+        object_score_logits_per_obj = [None] * num_objects\n+\n+        # Collect data for batched memory encoding\n+        objects_needing_memory_encoding = []\n+        high_res_masks_for_memory = []\n+        object_score_logits_for_memory = []\n+        is_mask_from_pts_per_obj = []\n+\n         # Note: We avoid batched inference here because per-object inputs (clicks/masks)\n         # can differ across objects.\n         for obj_idx in range(num_objects):\n@@ -1723,6 +1740,9 @@ def forward(\n             # conditioning output, reuse the cached masks instead of recomputing.\n             if (not has_new_inputs) and has_cond_output:\n                 pred_masks = inference_session.get_output(obj_idx, frame_idx, \"pred_masks\", is_conditioning_frame=True)\n+                object_score_logits = inference_session.get_output(\n+                    obj_idx, frame_idx, \"object_score_logits\", is_conditioning_frame=True\n+                )\n                 is_init_cond_frame = True\n             else:\n                 # Defaults when there are no new inputs\n@@ -1748,27 +1768,52 @@ def forward(\n                     point_inputs=point_inputs,\n                     mask_inputs=mask_inputs,\n                     reverse=reverse,\n-                    run_mem_encoder=True,\n                     streaming=frame is not None,\n                 )\n                 inference_session.store_output(\n                     obj_idx, frame_idx, output_value=current_out, is_conditioning_frame=is_init_cond_frame\n                 )\n                 pred_masks = current_out[\"pred_masks\"]\n+                object_score_logits = current_out[\"object_score_logits\"]\n+\n+                # Collect data for batched memory encoding\n+                if run_mem_encoder and self.num_maskmem > 0:\n+                    objects_needing_memory_encoding.append(obj_idx)\n+                    high_res_masks_for_memory.append(current_out[\"high_res_masks\"])\n+                    object_score_logits_for_memory.append(object_score_logits)\n+                    is_mask_from_pts_per_obj.append(point_inputs is not None or mask_inputs is not None)\n \n             pred_masks_per_obj[obj_idx] = pred_masks\n+            object_score_logits_per_obj[obj_idx] = object_score_logits.squeeze(-1)\n             if not is_init_cond_frame:\n                 # only for tracked frames, not for initial conditioning frames\n                 inference_session.frames_tracked_per_obj[obj_idx][frame_idx] = {\"reverse\": reverse}\n \n+        # Batch encode memories for all objects at once\n+        self._batch_encode_memories(\n+            inference_session=inference_session,\n+            frame_idx=frame_idx,\n+            objects_needing_memory_encoding=objects_needing_memory_encoding,\n+            high_res_masks_for_memory=high_res_masks_for_memory,\n+            object_score_logits_for_memory=object_score_logits_for_memory,\n+            is_mask_from_pts_per_obj=is_mask_from_pts_per_obj,\n+        )\n+\n         # Resize the output mask to the original video resolution (we directly use\n         # the mask scores on GPU for output to avoid any CPU conversion in between)\n         if len(pred_masks_per_obj) > 1:\n             all_pred_masks = torch.cat(pred_masks_per_obj, dim=0)\n+            all_object_score_logits = torch.cat(object_score_logits_per_obj, dim=0)\n         else:\n             all_pred_masks = pred_masks_per_obj[0]\n+            all_object_score_logits = object_score_logits_per_obj[0]\n \n-        return Sam2VideoSegmentationOutput(pred_masks=all_pred_masks, frame_idx=frame_idx)\n+        return Sam2VideoSegmentationOutput(\n+            object_ids=inference_session.obj_ids.copy(),\n+            pred_masks=all_pred_masks,\n+            object_score_logits=all_object_score_logits,\n+            frame_idx=frame_idx,\n+        )\n \n     def get_image_features(\n         self,\n@@ -2056,10 +2101,21 @@ def _use_mask_as_output(\n         # Use -10/+20 as logits for neg/pos pixels (very close to 0/1 in prob after sigmoid).\n         out_scale, out_bias = 20.0, -10.0  # sigmoid(-10.0)=4.5398e-05\n         mask_inputs_float = mask_inputs.to(backbone_features[0].dtype)\n+\n+        # Ensure mask is at self.image_size resolution for consistency\n+        if mask_inputs_float.shape[-2:] != (self.image_size, self.image_size):\n+            mask_inputs_float = F.interpolate(\n+                mask_inputs_float.float(),\n+                size=(self.image_size, self.image_size),\n+                align_corners=False,\n+                mode=\"bilinear\",\n+                antialias=True,\n+            ).to(mask_inputs.dtype)\n+\n         high_res_masks = mask_inputs_float * out_scale + out_bias\n         low_res_masks = F.interpolate(\n             high_res_masks.float(),\n-            size=(high_res_masks.size(-2) // 4, high_res_masks.size(-1) // 4),\n+            size=self.prompt_encoder.mask_input_size,\n             align_corners=False,\n             mode=\"bilinear\",\n             antialias=True,  # use antialias for downsampling\n@@ -2085,10 +2141,50 @@ def _use_mask_as_output(\n             pred_masks=low_res_masks,\n             high_res_masks=high_res_masks,\n             object_pointer=object_pointer,\n-            object_score_logits=object_score_logits,\n+            object_score_logits=object_score_logits.unsqueeze(-1),\n             image_embeddings=high_res_features + [backbone_features],\n         )\n \n+    def _select_closest_cond_frames(self, frame_idx, cond_frame_outputs, max_cond_frame_num):\n+        \"\"\"\n+        Select up to `max_cond_frame_num` conditioning frames from `cond_frame_outputs`\n+        that are temporally closest to the current frame at `frame_idx`. Here, we take\n+        - a) the closest conditioning frame before `frame_idx` (if any);\n+        - b) the closest conditioning frame after `frame_idx` (if any);\n+        - c) any other temporally closest conditioning frames until reaching a total\n+            of `max_cond_frame_num` conditioning frames.\n+\n+        Outputs:\n+        - selected_outputs: selected items (keys & values) from `cond_frame_outputs`.\n+        - unselected_outputs: items (keys & values) not selected in `cond_frame_outputs`.\n+        \"\"\"\n+        if max_cond_frame_num == -1 or len(cond_frame_outputs) <= max_cond_frame_num:\n+            selected_outputs = cond_frame_outputs\n+            unselected_outputs = {}\n+        else:\n+            selected_outputs = {}\n+            # the closest conditioning frame before `frame_idx` (if any)\n+            idx_before = max((t for t in cond_frame_outputs if t < frame_idx), default=None)\n+            if idx_before is not None:\n+                selected_outputs[idx_before] = cond_frame_outputs[idx_before]\n+\n+            # the closest conditioning frame after `frame_idx` (if any)\n+            idx_after = min((t for t in cond_frame_outputs if t >= frame_idx), default=None)\n+            if idx_after is not None:\n+                selected_outputs[idx_after] = cond_frame_outputs[idx_after]\n+\n+            # add other temporally closest conditioning frames until reaching a total\n+            # of `max_cond_frame_num` conditioning frames.\n+            num_remain = max_cond_frame_num - len(selected_outputs)\n+            inds_remain = sorted(\n+                (t for t in cond_frame_outputs if t not in selected_outputs),\n+                key=lambda x: abs(x - frame_idx),\n+            )[:num_remain]\n+            selected_outputs.update((t, cond_frame_outputs[t]) for t in inds_remain)\n+            unselected_outputs = {t: v for t, v in cond_frame_outputs.items() if t not in selected_outputs}\n+\n+        return selected_outputs, unselected_outputs\n+\n     def _gather_memory_frame_outputs(\n         self,\n         inference_session: Sam2VideoInferenceSession,\n@@ -2104,12 +2200,15 @@ def _gather_memory_frame_outputs(\n         \"\"\"\n         temporal_positions_and_previous_outputs = []\n \n-        # Add conditioning frame outputs (no limit here, as is the case in the original checkpoints)\n+        # Add conditioning frame outputs (limited by max_cond_frame_num)\n         conditioning_outputs = inference_session.output_dict_per_obj[obj_idx][\"cond_frame_outputs\"]\n         if not conditioning_outputs:\n             raise ValueError(\n                 \"maskmem_features in conditioning outputs cannot be empty when not is_initial_conditioning_frame\"\n             )\n+        conditioning_outputs, unselected_conditioning_outputs = self._select_closest_cond_frames(\n+            frame_idx, conditioning_outputs, max_cond_frame_num=self.config.max_cond_frame_num\n+        )\n \n         # Store (temporal_position, output_data) tuples\n         temporal_positions_and_previous_outputs = [(0, out) for out in conditioning_outputs.values()]\n@@ -2126,7 +2225,7 @@ def _gather_memory_frame_outputs(\n \n             # check if the output is already stored without using get_output to avoid unnecessary memory transfers between CPU and GPU\n             output_data = inference_session.output_dict_per_obj[obj_idx][\"non_cond_frame_outputs\"].get(\n-                previous_frame_idx, None\n+                previous_frame_idx, unselected_conditioning_outputs.get(previous_frame_idx, None)\n             )\n \n             temporal_positions_and_previous_outputs.append((relative_temporal_offset, output_data))\n@@ -2414,7 +2513,6 @@ def _run_single_frame_inference(\n         point_inputs: Optional[torch.Tensor],\n         mask_inputs: Optional[torch.Tensor],\n         reverse: bool,\n-        run_mem_encoder: bool,\n         prev_sam_mask_logits: Optional[torch.Tensor] = None,\n         streaming: bool = False,\n     ) -> dict[str, Any]:\n@@ -2438,8 +2536,6 @@ def _run_single_frame_inference(\n                 Mask prompt inputs for the current frame.\n             reverse (`bool`, *optional*, defaults to `False`):\n                 Whether to track in reverse time order.\n-            run_mem_encoder (`bool`, *optional*, defaults to `True`):\n-                Whether to run the memory encoder on predicted masks.\n             prev_sam_mask_logits (`torch.Tensor`, *optional*):\n                 Previously predicted SAM mask logits that can be fed with new clicks.\n             streaming (`bool`, *optional*, defaults to `False`):\n@@ -2449,9 +2545,8 @@ def _run_single_frame_inference(\n             `dict`: Dictionary containing the tracking results for the current frame, including:\n                 - pred_masks: Predicted low-resolution masks.\n                 - object_pointer: Object pointer for memory.\n+                - high_res_masks: High-resolution masks for batched memory encoding.\n                 - object_score_logits: Object score logits (inference only).\n-                - maskmem_features: Memory features for future frames.\n-                - maskmem_pos_enc: Memory positional encodings.\n         \"\"\"\n         # Retrieve correct image features\n         current_vision_feats, current_vision_pos_embeds = self._prepare_vision_features(\n@@ -2504,23 +2599,11 @@ def _run_single_frame_inference(\n                 multimask_output=multimask_output,\n             )\n \n-        # Finally run the memory encoder on the predicted mask to encode\n-        # it into a new memory feature (which will be used to condition vision features in future frames)\n-        maskmem_features = None\n-        maskmem_pos_enc = None\n-        if run_mem_encoder and self.num_maskmem > 0:\n-            maskmem_features, maskmem_pos_enc = self._encode_new_memory(\n-                current_vision_feats=current_vision_feats[-1],\n-                pred_masks_high_res=sam_outputs.high_res_masks,\n-                object_score_logits=sam_outputs.object_score_logits,\n-                is_mask_from_pts=(point_inputs is not None or mask_inputs is not None),\n-            )\n-\n+        # Memory encoding is now handled in batch by the caller (forward method)\n         current_out = {\n             \"pred_masks\": sam_outputs.pred_masks,\n             \"object_pointer\": sam_outputs.object_pointer,\n-            \"maskmem_features\": maskmem_features if maskmem_features is not None else None,\n-            \"maskmem_pos_enc\": maskmem_pos_enc,\n+            \"high_res_masks\": sam_outputs.high_res_masks,  # Needed for batched memory encoding\n         }\n         if not self.training:\n             current_out[\"object_score_logits\"] = sam_outputs.object_score_logits\n@@ -2538,6 +2621,20 @@ def _encode_new_memory(\n         batch_size = current_vision_feats.size(1)  # batch size on this frame\n         channels = self.hidden_dim\n         height, width = self.backbone_feature_sizes[-1]  # top-level (lowest-resolution) feature size\n+\n+        mask_input_size_h, mask_input_size_w = self.prompt_encoder.mask_input_size\n+        mask_mem_size_h = mask_input_size_h * 4\n+        mask_mem_size_w = mask_input_size_w * 4\n+        if pred_masks_high_res.shape[2:] != (mask_mem_size_h, mask_mem_size_w):\n+            # downsample the predicted high-res masks into the mask encoder input size\n+            pred_masks_high_res = F.interpolate(\n+                pred_masks_high_res.float(),\n+                size=(mask_mem_size_h, mask_mem_size_w),\n+                align_corners=False,\n+                mode=\"bilinear\",\n+                antialias=True,  # use antialias for downsampling\n+            ).to(pred_masks_high_res.dtype)\n+\n         # top-level feature, (HW)BC => BCHW\n         pix_feat = current_vision_feats.permute(1, 2, 0).view(batch_size, channels, height, width)\n         if is_mask_from_pts and not self.training:\n@@ -2568,6 +2665,63 @@ def _encode_new_memory(\n \n         return maskmem_features, maskmem_pos_enc\n \n+    def _batch_encode_memories(\n+        self,\n+        inference_session: Sam2VideoInferenceSession,\n+        frame_idx: int,\n+        objects_needing_memory_encoding: list[int],\n+        high_res_masks_for_memory: list[torch.Tensor],\n+        object_score_logits_for_memory: list[torch.Tensor],\n+        is_mask_from_pts_per_obj: list[bool],\n+    ):\n+        \"\"\"\n+        Batch encode memories for multiple objects at once.\n+\n+        Args:\n+            inference_session: The video inference session object\n+            frame_idx: Index of the current frame\n+            objects_needing_memory_encoding: List of object indices that need memory encoding\n+            high_res_masks_for_memory: List of high-resolution masks for each object\n+            object_score_logits_for_memory: List of object score logits for each object\n+            is_mask_from_pts_per_obj: List of booleans indicating if mask is from points for each object\n+        \"\"\"\n+        if not objects_needing_memory_encoding:\n+            return\n+\n+        # Get vision features once for all objects\n+        current_vision_feats, _ = self._prepare_vision_features(inference_session, frame_idx, batch_size=1)\n+\n+        # Stack all high-res masks and object scores\n+        high_res_masks_batched = torch.cat(high_res_masks_for_memory, dim=0)\n+        object_score_logits_batched = torch.cat(object_score_logits_for_memory, dim=0)\n+\n+        # Expand vision features to match batch size\n+        expanded_vision_feats = current_vision_feats[-1].expand(-1, len(objects_needing_memory_encoding), -1)\n+\n+        # Encode all memories in one batch call\n+        maskmem_features_batched, maskmem_pos_enc_batched = self._encode_new_memory(\n+            current_vision_feats=expanded_vision_feats,\n+            pred_masks_high_res=high_res_masks_batched,\n+            object_score_logits=object_score_logits_batched,\n+            is_mask_from_pts=any(is_mask_from_pts_per_obj),\n+        )\n+\n+        # Split and store encoded memories per object\n+        for i, obj_idx in enumerate(objects_needing_memory_encoding):\n+            # Extract per-object memory from batched result\n+            maskmem_features = maskmem_features_batched[:, i : i + 1]\n+            maskmem_pos_enc = maskmem_pos_enc_batched[:, i : i + 1]\n+\n+            # Update the stored output with memory features\n+            output_dict = inference_session.output_dict_per_obj[obj_idx]\n+            # Determine if this was a conditioning frame\n+            storage_key = (\n+                \"cond_frame_outputs\" if frame_idx in output_dict[\"cond_frame_outputs\"] else \"non_cond_frame_outputs\"\n+            )\n+            if frame_idx in output_dict[storage_key]:\n+                output_dict[storage_key][frame_idx][\"maskmem_features\"] = maskmem_features\n+                output_dict[storage_key][frame_idx][\"maskmem_pos_enc\"] = maskmem_pos_enc\n+\n     @torch.inference_mode()\n     @auto_docstring(\n         custom_intro=\"\"\"\n@@ -2581,6 +2735,7 @@ def propagate_in_video_iterator(\n         start_frame_idx: Optional[int] = None,\n         max_frame_num_to_track: Optional[int] = None,\n         reverse: bool = False,\n+        show_progress_bar: bool = False,\n     ) -> Iterator[Sam2VideoSegmentationOutput]:\n         r\"\"\"\n         inference_session (`Sam2VideoInferenceSession`):\n@@ -2593,6 +2748,8 @@ def propagate_in_video_iterator(\n             The maximum number of frames to track.\n         reverse (`bool`, *optional*, defaults to `False`):\n             Whether to propagate in reverse.\n+        show_progress_bar (`bool`, *optional*, defaults to `False`):\n+            Whether to show a progress bar during propagation.\n         \"\"\"\n         num_frames = inference_session.num_frames\n \n@@ -2622,7 +2779,7 @@ def propagate_in_video_iterator(\n             end_frame_idx = min(start_frame_idx + max_frame_num_to_track, num_frames - 1)\n             processing_order = range(start_frame_idx, end_frame_idx + 1)\n \n-        for frame_idx in tqdm(processing_order, desc=\"propagate in video\"):\n+        for frame_idx in tqdm(processing_order, desc=\"propagate in video\", disable=not show_progress_bar):\n             sam2_video_output = self(inference_session, frame_idx=frame_idx, reverse=reverse)\n             yield sam2_video_output\n "
        },
        {
            "sha": "88103217c2ad19ed5b5f0dbdb7a3282145a56158",
            "filename": "src/transformers/models/sam2_video/modular_sam2_video.py",
            "status": "modified",
            "additions": 187,
            "deletions": 26,
            "changes": 213,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -108,6 +108,8 @@ class Sam2VideoConfig(PreTrainedConfig):\n             Whether to use multimask output for tracking.\n         max_object_pointers_in_encoder (`int`, *optional*, defaults to 16):\n             The maximum number of object pointers in the encoder.\n+        max_cond_frame_num (`int`, *optional*, defaults to -1):\n+            Maximum number of conditioning frames to use in memory attention. Set to -1 to use all conditioning frames.\n         enable_temporal_pos_encoding_for_object_pointers (`bool`, *optional*, defaults to `True`):\n             Whether to enable temporal positional encoding for object pointers.\n         memory_attention_hidden_size (`int`, *optional*, defaults to 256):\n@@ -215,6 +217,7 @@ def __init__(\n         multimask_max_pt_num=1,\n         multimask_output_for_tracking=True,\n         max_object_pointers_in_encoder=16,\n+        max_cond_frame_num=-1,\n         enable_temporal_pos_encoding_for_object_pointers=True,\n         # memory attention\n         memory_attention_hidden_size=256,\n@@ -276,6 +279,7 @@ def __init__(\n         self.multimask_max_pt_num = multimask_max_pt_num\n         self.multimask_output_for_tracking = multimask_output_for_tracking\n         self.max_object_pointers_in_encoder = max_object_pointers_in_encoder\n+        self.max_cond_frame_num = max_cond_frame_num\n         # The next 4 are True for sam2.1 and False for sam2\n         self.enable_occlusion_spatial_embedding = enable_occlusion_spatial_embedding\n         self.enable_temporal_pos_encoding_for_object_pointers = enable_temporal_pos_encoding_for_object_pointers\n@@ -968,13 +972,19 @@ class Sam2VideoImageSegmentationOutput(Sam2ImageSegmentationOutput):\n @auto_docstring(custom_intro=\"Base class for the Sam2 model's output.\")\n class Sam2VideoSegmentationOutput(ModelOutput):\n     r\"\"\"\n+    object_ids (`list[int]`, *optional*):\n+        List of object IDs being tracked in the current frame.\n     pred_masks (`torch.FloatTensor` of shape `(batch_size, num_masks, height, width)`):\n         The predicted masks stored at the model's resolution.\n+    object_score_logits (`torch.FloatTensor` of shape `(batch_size,)`, *optional*):\n+        Logits for the object scores, indicating if objects are present.\n     frame_idx (`int`):\n         The frame index of the video.\n     \"\"\"\n \n+    object_ids: Optional[list[int]] = None\n     pred_masks: Optional[torch.FloatTensor] = None\n+    object_score_logits: Optional[torch.FloatTensor] = None\n     frame_idx: Optional[int] = None\n \n \n@@ -1754,10 +1764,21 @@ def _use_mask_as_output(\n         # Use -10/+20 as logits for neg/pos pixels (very close to 0/1 in prob after sigmoid).\n         out_scale, out_bias = 20.0, -10.0  # sigmoid(-10.0)=4.5398e-05\n         mask_inputs_float = mask_inputs.to(backbone_features[0].dtype)\n+\n+        # Ensure mask is at self.image_size resolution for consistency\n+        if mask_inputs_float.shape[-2:] != (self.image_size, self.image_size):\n+            mask_inputs_float = F.interpolate(\n+                mask_inputs_float.float(),\n+                size=(self.image_size, self.image_size),\n+                align_corners=False,\n+                mode=\"bilinear\",\n+                antialias=True,\n+            ).to(mask_inputs.dtype)\n+\n         high_res_masks = mask_inputs_float * out_scale + out_bias\n         low_res_masks = F.interpolate(\n             high_res_masks.float(),\n-            size=(high_res_masks.size(-2) // 4, high_res_masks.size(-1) // 4),\n+            size=self.prompt_encoder.mask_input_size,\n             align_corners=False,\n             mode=\"bilinear\",\n             antialias=True,  # use antialias for downsampling\n@@ -1783,10 +1804,50 @@ def _use_mask_as_output(\n             pred_masks=low_res_masks,\n             high_res_masks=high_res_masks,\n             object_pointer=object_pointer,\n-            object_score_logits=object_score_logits,\n+            object_score_logits=object_score_logits.unsqueeze(-1),\n             image_embeddings=high_res_features + [backbone_features],\n         )\n \n+    def _select_closest_cond_frames(self, frame_idx, cond_frame_outputs, max_cond_frame_num):\n+        \"\"\"\n+        Select up to `max_cond_frame_num` conditioning frames from `cond_frame_outputs`\n+        that are temporally closest to the current frame at `frame_idx`. Here, we take\n+        - a) the closest conditioning frame before `frame_idx` (if any);\n+        - b) the closest conditioning frame after `frame_idx` (if any);\n+        - c) any other temporally closest conditioning frames until reaching a total\n+            of `max_cond_frame_num` conditioning frames.\n+\n+        Outputs:\n+        - selected_outputs: selected items (keys & values) from `cond_frame_outputs`.\n+        - unselected_outputs: items (keys & values) not selected in `cond_frame_outputs`.\n+        \"\"\"\n+        if max_cond_frame_num == -1 or len(cond_frame_outputs) <= max_cond_frame_num:\n+            selected_outputs = cond_frame_outputs\n+            unselected_outputs = {}\n+        else:\n+            selected_outputs = {}\n+            # the closest conditioning frame before `frame_idx` (if any)\n+            idx_before = max((t for t in cond_frame_outputs if t < frame_idx), default=None)\n+            if idx_before is not None:\n+                selected_outputs[idx_before] = cond_frame_outputs[idx_before]\n+\n+            # the closest conditioning frame after `frame_idx` (if any)\n+            idx_after = min((t for t in cond_frame_outputs if t >= frame_idx), default=None)\n+            if idx_after is not None:\n+                selected_outputs[idx_after] = cond_frame_outputs[idx_after]\n+\n+            # add other temporally closest conditioning frames until reaching a total\n+            # of `max_cond_frame_num` conditioning frames.\n+            num_remain = max_cond_frame_num - len(selected_outputs)\n+            inds_remain = sorted(\n+                (t for t in cond_frame_outputs if t not in selected_outputs),\n+                key=lambda x: abs(x - frame_idx),\n+            )[:num_remain]\n+            selected_outputs.update((t, cond_frame_outputs[t]) for t in inds_remain)\n+            unselected_outputs = {t: v for t, v in cond_frame_outputs.items() if t not in selected_outputs}\n+\n+        return selected_outputs, unselected_outputs\n+\n     def _gather_memory_frame_outputs(\n         self,\n         inference_session: Sam2VideoInferenceSession,\n@@ -1802,12 +1863,15 @@ def _gather_memory_frame_outputs(\n         \"\"\"\n         temporal_positions_and_previous_outputs = []\n \n-        # Add conditioning frame outputs (no limit here, as is the case in the original checkpoints)\n+        # Add conditioning frame outputs (limited by max_cond_frame_num)\n         conditioning_outputs = inference_session.output_dict_per_obj[obj_idx][\"cond_frame_outputs\"]\n         if not conditioning_outputs:\n             raise ValueError(\n                 \"maskmem_features in conditioning outputs cannot be empty when not is_initial_conditioning_frame\"\n             )\n+        conditioning_outputs, unselected_conditioning_outputs = self._select_closest_cond_frames(\n+            frame_idx, conditioning_outputs, max_cond_frame_num=self.config.max_cond_frame_num\n+        )\n \n         # Store (temporal_position, output_data) tuples\n         temporal_positions_and_previous_outputs = [(0, out) for out in conditioning_outputs.values()]\n@@ -1824,7 +1888,7 @@ def _gather_memory_frame_outputs(\n \n             # check if the output is already stored without using get_output to avoid unnecessary memory transfers between CPU and GPU\n             output_data = inference_session.output_dict_per_obj[obj_idx][\"non_cond_frame_outputs\"].get(\n-                previous_frame_idx, None\n+                previous_frame_idx, unselected_conditioning_outputs.get(previous_frame_idx, None)\n             )\n \n             temporal_positions_and_previous_outputs.append((relative_temporal_offset, output_data))\n@@ -2112,7 +2176,6 @@ def _run_single_frame_inference(\n         point_inputs: Optional[torch.Tensor],\n         mask_inputs: Optional[torch.Tensor],\n         reverse: bool,\n-        run_mem_encoder: bool,\n         prev_sam_mask_logits: Optional[torch.Tensor] = None,\n         streaming: bool = False,\n     ) -> dict[str, Any]:\n@@ -2136,8 +2199,6 @@ def _run_single_frame_inference(\n                 Mask prompt inputs for the current frame.\n             reverse (`bool`, *optional*, defaults to `False`):\n                 Whether to track in reverse time order.\n-            run_mem_encoder (`bool`, *optional*, defaults to `True`):\n-                Whether to run the memory encoder on predicted masks.\n             prev_sam_mask_logits (`torch.Tensor`, *optional*):\n                 Previously predicted SAM mask logits that can be fed with new clicks.\n             streaming (`bool`, *optional*, defaults to `False`):\n@@ -2147,9 +2208,8 @@ def _run_single_frame_inference(\n             `dict`: Dictionary containing the tracking results for the current frame, including:\n                 - pred_masks: Predicted low-resolution masks.\n                 - object_pointer: Object pointer for memory.\n+                - high_res_masks: High-resolution masks for batched memory encoding.\n                 - object_score_logits: Object score logits (inference only).\n-                - maskmem_features: Memory features for future frames.\n-                - maskmem_pos_enc: Memory positional encodings.\n         \"\"\"\n         # Retrieve correct image features\n         current_vision_feats, current_vision_pos_embeds = self._prepare_vision_features(\n@@ -2202,23 +2262,11 @@ def _run_single_frame_inference(\n                 multimask_output=multimask_output,\n             )\n \n-        # Finally run the memory encoder on the predicted mask to encode\n-        # it into a new memory feature (which will be used to condition vision features in future frames)\n-        maskmem_features = None\n-        maskmem_pos_enc = None\n-        if run_mem_encoder and self.num_maskmem > 0:\n-            maskmem_features, maskmem_pos_enc = self._encode_new_memory(\n-                current_vision_feats=current_vision_feats[-1],\n-                pred_masks_high_res=sam_outputs.high_res_masks,\n-                object_score_logits=sam_outputs.object_score_logits,\n-                is_mask_from_pts=(point_inputs is not None or mask_inputs is not None),\n-            )\n-\n+        # Memory encoding is now handled in batch by the caller (forward method)\n         current_out = {\n             \"pred_masks\": sam_outputs.pred_masks,\n             \"object_pointer\": sam_outputs.object_pointer,\n-            \"maskmem_features\": maskmem_features if maskmem_features is not None else None,\n-            \"maskmem_pos_enc\": maskmem_pos_enc,\n+            \"high_res_masks\": sam_outputs.high_res_masks,  # Needed for batched memory encoding\n         }\n         if not self.training:\n             current_out[\"object_score_logits\"] = sam_outputs.object_score_logits\n@@ -2236,6 +2284,20 @@ def _encode_new_memory(\n         batch_size = current_vision_feats.size(1)  # batch size on this frame\n         channels = self.hidden_dim\n         height, width = self.backbone_feature_sizes[-1]  # top-level (lowest-resolution) feature size\n+\n+        mask_input_size_h, mask_input_size_w = self.prompt_encoder.mask_input_size\n+        mask_mem_size_h = mask_input_size_h * 4\n+        mask_mem_size_w = mask_input_size_w * 4\n+        if pred_masks_high_res.shape[2:] != (mask_mem_size_h, mask_mem_size_w):\n+            # downsample the predicted high-res masks into the mask encoder input size\n+            pred_masks_high_res = F.interpolate(\n+                pred_masks_high_res.float(),\n+                size=(mask_mem_size_h, mask_mem_size_w),\n+                align_corners=False,\n+                mode=\"bilinear\",\n+                antialias=True,  # use antialias for downsampling\n+            ).to(pred_masks_high_res.dtype)\n+\n         # top-level feature, (HW)BC => BCHW\n         pix_feat = current_vision_feats.permute(1, 2, 0).view(batch_size, channels, height, width)\n         if is_mask_from_pts and not self.training:\n@@ -2274,6 +2336,7 @@ def forward(\n         frame_idx: Optional[int] = None,\n         frame: Optional[torch.Tensor] = None,\n         reverse: bool = False,\n+        run_mem_encoder: bool = True,\n     ) -> Sam2VideoSegmentationOutput:\n         r\"\"\"\n         inference_session (`Sam2VideoInferenceSession`):\n@@ -2285,6 +2348,8 @@ def forward(\n             The frame to process. Provide when streaming.\n         reverse (`bool`, *optional*, defaults to `False`):\n             Whether to propagate in reverse.\n+        run_mem_encoder (`bool`, *optional*, defaults to `True`):\n+            Whether to run the memory encoder on predicted masks. The memory encoder is batched across all objects for efficiency.\n         \"\"\"\n         if frame is not None:\n             frame_idx = inference_session.add_new_frame(frame, frame_idx)\n@@ -2294,6 +2359,14 @@ def forward(\n \n         num_objects = inference_session.get_obj_num()\n         pred_masks_per_obj = [None] * num_objects\n+        object_score_logits_per_obj = [None] * num_objects\n+\n+        # Collect data for batched memory encoding\n+        objects_needing_memory_encoding = []\n+        high_res_masks_for_memory = []\n+        object_score_logits_for_memory = []\n+        is_mask_from_pts_per_obj = []\n+\n         # Note: We avoid batched inference here because per-object inputs (clicks/masks)\n         # can differ across objects.\n         for obj_idx in range(num_objects):\n@@ -2304,6 +2377,9 @@ def forward(\n             # conditioning output, reuse the cached masks instead of recomputing.\n             if (not has_new_inputs) and has_cond_output:\n                 pred_masks = inference_session.get_output(obj_idx, frame_idx, \"pred_masks\", is_conditioning_frame=True)\n+                object_score_logits = inference_session.get_output(\n+                    obj_idx, frame_idx, \"object_score_logits\", is_conditioning_frame=True\n+                )\n                 is_init_cond_frame = True\n             else:\n                 # Defaults when there are no new inputs\n@@ -2329,27 +2405,109 @@ def forward(\n                     point_inputs=point_inputs,\n                     mask_inputs=mask_inputs,\n                     reverse=reverse,\n-                    run_mem_encoder=True,\n                     streaming=frame is not None,\n                 )\n                 inference_session.store_output(\n                     obj_idx, frame_idx, output_value=current_out, is_conditioning_frame=is_init_cond_frame\n                 )\n                 pred_masks = current_out[\"pred_masks\"]\n+                object_score_logits = current_out[\"object_score_logits\"]\n+\n+                # Collect data for batched memory encoding\n+                if run_mem_encoder and self.num_maskmem > 0:\n+                    objects_needing_memory_encoding.append(obj_idx)\n+                    high_res_masks_for_memory.append(current_out[\"high_res_masks\"])\n+                    object_score_logits_for_memory.append(object_score_logits)\n+                    is_mask_from_pts_per_obj.append(point_inputs is not None or mask_inputs is not None)\n \n             pred_masks_per_obj[obj_idx] = pred_masks\n+            object_score_logits_per_obj[obj_idx] = object_score_logits.squeeze(-1)\n             if not is_init_cond_frame:\n                 # only for tracked frames, not for initial conditioning frames\n                 inference_session.frames_tracked_per_obj[obj_idx][frame_idx] = {\"reverse\": reverse}\n \n+        # Batch encode memories for all objects at once\n+        self._batch_encode_memories(\n+            inference_session=inference_session,\n+            frame_idx=frame_idx,\n+            objects_needing_memory_encoding=objects_needing_memory_encoding,\n+            high_res_masks_for_memory=high_res_masks_for_memory,\n+            object_score_logits_for_memory=object_score_logits_for_memory,\n+            is_mask_from_pts_per_obj=is_mask_from_pts_per_obj,\n+        )\n+\n         # Resize the output mask to the original video resolution (we directly use\n         # the mask scores on GPU for output to avoid any CPU conversion in between)\n         if len(pred_masks_per_obj) > 1:\n             all_pred_masks = torch.cat(pred_masks_per_obj, dim=0)\n+            all_object_score_logits = torch.cat(object_score_logits_per_obj, dim=0)\n         else:\n             all_pred_masks = pred_masks_per_obj[0]\n+            all_object_score_logits = object_score_logits_per_obj[0]\n+\n+        return Sam2VideoSegmentationOutput(\n+            object_ids=inference_session.obj_ids.copy(),\n+            pred_masks=all_pred_masks,\n+            object_score_logits=all_object_score_logits,\n+            frame_idx=frame_idx,\n+        )\n+\n+    def _batch_encode_memories(\n+        self,\n+        inference_session: Sam2VideoInferenceSession,\n+        frame_idx: int,\n+        objects_needing_memory_encoding: list[int],\n+        high_res_masks_for_memory: list[torch.Tensor],\n+        object_score_logits_for_memory: list[torch.Tensor],\n+        is_mask_from_pts_per_obj: list[bool],\n+    ):\n+        \"\"\"\n+        Batch encode memories for multiple objects at once.\n \n-        return Sam2VideoSegmentationOutput(pred_masks=all_pred_masks, frame_idx=frame_idx)\n+        Args:\n+            inference_session: The video inference session object\n+            frame_idx: Index of the current frame\n+            objects_needing_memory_encoding: List of object indices that need memory encoding\n+            high_res_masks_for_memory: List of high-resolution masks for each object\n+            object_score_logits_for_memory: List of object score logits for each object\n+            is_mask_from_pts_per_obj: List of booleans indicating if mask is from points for each object\n+        \"\"\"\n+        if not objects_needing_memory_encoding:\n+            return\n+\n+        # Get vision features once for all objects\n+        current_vision_feats, _ = self._prepare_vision_features(inference_session, frame_idx, batch_size=1)\n+\n+        # Stack all high-res masks and object scores\n+        high_res_masks_batched = torch.cat(high_res_masks_for_memory, dim=0)\n+        object_score_logits_batched = torch.cat(object_score_logits_for_memory, dim=0)\n+\n+        # Expand vision features to match batch size\n+        expanded_vision_feats = current_vision_feats[-1].expand(-1, len(objects_needing_memory_encoding), -1)\n+\n+        # Encode all memories in one batch call\n+        maskmem_features_batched, maskmem_pos_enc_batched = self._encode_new_memory(\n+            current_vision_feats=expanded_vision_feats,\n+            pred_masks_high_res=high_res_masks_batched,\n+            object_score_logits=object_score_logits_batched,\n+            is_mask_from_pts=any(is_mask_from_pts_per_obj),\n+        )\n+\n+        # Split and store encoded memories per object\n+        for i, obj_idx in enumerate(objects_needing_memory_encoding):\n+            # Extract per-object memory from batched result\n+            maskmem_features = maskmem_features_batched[:, i : i + 1]\n+            maskmem_pos_enc = maskmem_pos_enc_batched[:, i : i + 1]\n+\n+            # Update the stored output with memory features\n+            output_dict = inference_session.output_dict_per_obj[obj_idx]\n+            # Determine if this was a conditioning frame\n+            storage_key = (\n+                \"cond_frame_outputs\" if frame_idx in output_dict[\"cond_frame_outputs\"] else \"non_cond_frame_outputs\"\n+            )\n+            if frame_idx in output_dict[storage_key]:\n+                output_dict[storage_key][frame_idx][\"maskmem_features\"] = maskmem_features\n+                output_dict[storage_key][frame_idx][\"maskmem_pos_enc\"] = maskmem_pos_enc\n \n     @torch.inference_mode()\n     @auto_docstring(\n@@ -2364,6 +2522,7 @@ def propagate_in_video_iterator(\n         start_frame_idx: Optional[int] = None,\n         max_frame_num_to_track: Optional[int] = None,\n         reverse: bool = False,\n+        show_progress_bar: bool = False,\n     ) -> Iterator[Sam2VideoSegmentationOutput]:\n         r\"\"\"\n         inference_session (`Sam2VideoInferenceSession`):\n@@ -2376,6 +2535,8 @@ def propagate_in_video_iterator(\n             The maximum number of frames to track.\n         reverse (`bool`, *optional*, defaults to `False`):\n             Whether to propagate in reverse.\n+        show_progress_bar (`bool`, *optional*, defaults to `False`):\n+            Whether to show a progress bar during propagation.\n         \"\"\"\n         num_frames = inference_session.num_frames\n \n@@ -2405,7 +2566,7 @@ def propagate_in_video_iterator(\n             end_frame_idx = min(start_frame_idx + max_frame_num_to_track, num_frames - 1)\n             processing_order = range(start_frame_idx, end_frame_idx + 1)\n \n-        for frame_idx in tqdm(processing_order, desc=\"propagate in video\"):\n+        for frame_idx in tqdm(processing_order, desc=\"propagate in video\", disable=not show_progress_bar):\n             sam2_video_output = self(inference_session, frame_idx=frame_idx, reverse=reverse)\n             yield sam2_video_output\n "
        },
        {
            "sha": "0583e820e3bc05f3ae22af19e488669db39728ee",
            "filename": "src/transformers/models/sam2_video/processing_sam2_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -97,7 +97,6 @@ def __call__(\n             A [`BatchEncoding`] with the following fields:\n             - `pixel_values` (`torch.Tensor`): The processed image(s).\n             - `original_sizes` (`list[list[float]]`): The original sizes of the images.\n-            - `reshaped_input_sizes` (`torch.Tensor`): The reshaped input sizes of the images.\n             - `labels` (`torch.Tensor`): The processed segmentation maps (if provided).\n             - `input_points` (`torch.Tensor`): The processed points.\n             - `input_labels` (`torch.Tensor`): The processed labels."
        },
        {
            "sha": "fda7eeef4fe5b0f4835fd480dfee6c4ea6614d1f",
            "filename": "src/transformers/models/sam3/__init__.py",
            "status": "added",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3%2F__init__.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,28 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_sam3 import *\n+    from .modeling_sam3 import *\n+    from .processing_sam3 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "f65dafcf81fd5bb56f60b64a7d4e920fd685c35b",
            "filename": "src/transformers/models/sam3/configuration_sam3.py",
            "status": "added",
            "additions": 518,
            "deletions": 0,
            "changes": 518,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3%2Fconfiguration_sam3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3%2Fconfiguration_sam3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3%2Fconfiguration_sam3.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,518 @@\n+# coding=utf-8\n+# Copyright 2025 Meta AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"SAM3 model configuration\"\"\"\n+\n+from transformers import CLIPTextConfig\n+\n+from ...configuration_utils import PreTrainedConfig\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+class Sam3ViTConfig(PreTrainedConfig):\n+    r\"\"\"\n+    Configuration class for SAM3 Vision Encoder (ViT backbone).\n+\n+    Instantiating a configuration defaults will yield a similar configuration to that of SAM 3\n+    [facebook/sam3](https://huggingface.co/facebook/sam3) architecture.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 1024):\n+            Dimensionality of the encoder layers.\n+        intermediate_size (`int`, *optional*, defaults to 4736):\n+            Dimensionality of the feedforward (MLP) layers.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            Number of input image channels.\n+        image_size (`int`, *optional*, defaults to 1008):\n+            Expected input image size.\n+        patch_size (`int`, *optional*, defaults to 14):\n+            Size of image patches.\n+        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by layer normalization layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for attention probabilities.\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            Base frequency for RoPE.\n+        window_size (`int`, *optional*, defaults to 24):\n+            Window size for windowed attention.\n+        global_attn_indexes (`list[int]`, *optional*, defaults to `[7, 15, 23, 31]`):\n+            Indexes of layers with global attention.\n+        layer_scale_init_value (`float`, *optional*):\n+            Initial value for layer scale. None means no layer scale.\n+        pretrain_image_size (`int`, *optional*, defaults to 336):\n+            Pretrained model image size for position embedding initialization.\n+        hidden_dropout (`float`, *optional*, defaults to 0.0):\n+            Dropout probability for hidden states.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing weight matrices.\n+    \"\"\"\n+\n+    base_config_key = \"backbone_config\"\n+    model_type = \"sam3_vit_model\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=1024,\n+        intermediate_size=4736,\n+        num_hidden_layers=32,\n+        num_attention_heads=16,\n+        num_channels=3,\n+        image_size=1008,\n+        patch_size=14,\n+        hidden_act=\"gelu\",\n+        layer_norm_eps=1e-6,\n+        attention_dropout=0.0,\n+        rope_theta=10000.0,\n+        window_size=24,\n+        global_attn_indexes=None,\n+        layer_scale_init_value=None,\n+        pretrain_image_size=336,\n+        hidden_dropout=0.0,\n+        initializer_range=0.02,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        if global_attn_indexes is None:\n+            global_attn_indexes = [7, 15, 23, 31]\n+\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.hidden_act = hidden_act\n+        self.layer_norm_eps = layer_norm_eps\n+        self.attention_dropout = attention_dropout\n+        self.rope_theta = rope_theta\n+        self.window_size = window_size\n+        self.global_attn_indexes = global_attn_indexes\n+        self.layer_scale_init_value = layer_scale_init_value\n+        self.pretrain_image_size = pretrain_image_size\n+        self.hidden_dropout = hidden_dropout\n+        self.initializer_range = initializer_range\n+\n+\n+class Sam3VisionConfig(PreTrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Sam3VisionModel`]. It is used to instantiate a SAM\n+    vision encoder according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    defaults will yield a similar configuration to that of SAM 3\n+    [facebook/sam3](https://huggingface.co/facebook/sam3) architecture.\n+\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*):\n+            Configuration for the vision backbone. This is used to instantiate the backbone using\n+            `AutoModel.from_config`.\n+        fpn_hidden_size (`int`, *optional*, defaults to 256):\n+            The hidden dimension of the FPN.\n+        backbone_feature_sizes (`List[List[int]]`, *optional*, defaults to `[[288, 288], [144, 144], [72, 72]]`):\n+            The spatial sizes (height, width) of the feature maps from the backbone at different scales.\n+        scale_factors (`list[float]`, *optional*, defaults to `[4.0, 2.0, 1.0, 0.5]`):\n+            Scale factors for FPN multi-scale features. List of scaling factors for each FPN level.\n+        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function in the neck.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon for the layer normalization.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+\n+    \"\"\"\n+\n+    base_config_key = \"vision_config\"\n+    model_type = \"sam3_vision_model\"\n+    sub_configs = {\n+        \"backbone_config\": AutoConfig,\n+    }\n+\n+    def __init__(\n+        self,\n+        backbone_config=None,\n+        fpn_hidden_size=256,\n+        backbone_feature_sizes=None,\n+        scale_factors=None,\n+        hidden_act=\"gelu\",\n+        layer_norm_eps=1e-6,\n+        initializer_range=0.02,\n+        **kwargs,\n+    ):\n+        scale_factors = [4.0, 2.0, 1.0, 0.5] if scale_factors is None else scale_factors\n+        if backbone_feature_sizes is None:\n+            backbone_feature_sizes = [[288, 288], [144, 144], [72, 72]]\n+\n+        if isinstance(backbone_config, dict):\n+            backbone_config[\"model_type\"] = backbone_config.get(\"model_type\", \"sam3_vit_model\")\n+            backbone_config = CONFIG_MAPPING[backbone_config[\"model_type\"]](**backbone_config)\n+        elif backbone_config is None:\n+            backbone_config = CONFIG_MAPPING[\"sam3_vit_model\"]()\n+\n+        self.backbone_config = backbone_config\n+\n+        # Neck\n+        self.fpn_hidden_size = fpn_hidden_size\n+        self.scale_factors = scale_factors\n+        self.backbone_feature_sizes = backbone_feature_sizes\n+\n+        self.hidden_act = hidden_act\n+        self.layer_norm_eps = layer_norm_eps\n+        self.initializer_range = initializer_range\n+        super().__init__(**kwargs)\n+\n+\n+class Sam3GeometryEncoderConfig(PreTrainedConfig):\n+    r\"\"\"\n+    Configuration class for SAM3 Geometry Encoder.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 256):\n+            Dimensionality of the encoder layers.\n+        num_layers (`int`, *optional*, defaults to 3):\n+            Number of transformer encoder layers for processing geometry prompts.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads in the geometry encoder.\n+        intermediate_size (`int`, *optional*, defaults to 2048):\n+            Dimensionality of the feedforward layers.\n+        dropout (`float`, *optional*, defaults to 0.1):\n+            Dropout probability.\n+        hidden_act (`str`, *optional*, defaults to `\"relu\"`):\n+            Activation function in FFN.\n+        hidden_dropout (`float`, *optional*, defaults to 0.0):\n+            Dropout probability for hidden states.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            Epsilon for layer normalization.\n+        roi_size (`int`, *optional*, defaults to 7):\n+            ROI size for box pooling operations.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing weight matrices.\n+    \"\"\"\n+\n+    model_type = \"sam3_geometry_encoder\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=256,\n+        num_layers=3,\n+        num_attention_heads=8,\n+        intermediate_size=2048,\n+        dropout=0.1,\n+        hidden_act=\"relu\",\n+        hidden_dropout=0.0,\n+        layer_norm_eps=1e-6,\n+        roi_size=7,\n+        initializer_range=0.02,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.hidden_size = hidden_size\n+        self.num_layers = num_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.dropout = dropout\n+        self.hidden_act = hidden_act\n+        self.hidden_dropout = hidden_dropout\n+        self.layer_norm_eps = layer_norm_eps\n+        self.roi_size = roi_size\n+        self.initializer_range = initializer_range\n+\n+\n+class Sam3DETREncoderConfig(PreTrainedConfig):\n+    r\"\"\"\n+    Configuration class for SAM3 DETR Encoder (vision-text fusion encoder).\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 256):\n+            Dimensionality of the encoder layers.\n+        num_layers (`int`, *optional*, defaults to 6):\n+            Number of encoder layers.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads.\n+        intermediate_size (`int`, *optional*, defaults to 2048):\n+            Dimensionality of the feedforward layers.\n+        dropout (`float`, *optional*, defaults to 0.1):\n+            Dropout probability.\n+        hidden_act (`str`, *optional*, defaults to `\"relu\"`):\n+            Activation function in FFN.\n+        hidden_dropout (`float`, *optional*, defaults to 0.0):\n+            Dropout probability for hidden states.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            Epsilon for layer normalization.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing weight matrices.\n+    \"\"\"\n+\n+    model_type = \"sam3_detr_encoder\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=256,\n+        num_layers=6,\n+        num_attention_heads=8,\n+        intermediate_size=2048,\n+        dropout=0.1,\n+        hidden_act=\"relu\",\n+        hidden_dropout=0.0,\n+        layer_norm_eps=1e-6,\n+        initializer_range=0.02,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.hidden_size = hidden_size\n+        self.num_layers = num_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.dropout = dropout\n+        self.hidden_act = hidden_act\n+        self.hidden_dropout = hidden_dropout\n+        self.layer_norm_eps = layer_norm_eps\n+        self.initializer_range = initializer_range\n+\n+\n+class Sam3DETRDecoderConfig(PreTrainedConfig):\n+    r\"\"\"\n+    Configuration class for SAM3 DETR Decoder (object query decoder).\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 256):\n+            Dimensionality of the decoder layers.\n+        num_layers (`int`, *optional*, defaults to 6):\n+            Number of decoder layers.\n+        num_queries (`int`, *optional*, defaults to 200):\n+            Number of object queries.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads.\n+        intermediate_size (`int`, *optional*, defaults to 2048):\n+            Dimensionality of the feedforward layers.\n+        dropout (`float`, *optional*, defaults to 0.1):\n+            Dropout probability.\n+        hidden_act (`str`, *optional*, defaults to `\"relu\"`):\n+            Activation function in FFN.\n+        hidden_dropout (`float`, *optional*, defaults to 0.0):\n+            Dropout probability for hidden states.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            Epsilon for layer normalization.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing weight matrices.\n+    \"\"\"\n+\n+    model_type = \"sam3_detr_decoder\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=256,\n+        num_layers=6,\n+        num_queries=200,\n+        num_attention_heads=8,\n+        intermediate_size=2048,\n+        dropout=0.1,\n+        hidden_act=\"relu\",\n+        hidden_dropout=0.0,\n+        layer_norm_eps=1e-6,\n+        initializer_range=0.02,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.hidden_size = hidden_size\n+        self.num_layers = num_layers\n+        self.num_queries = num_queries\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.dropout = dropout\n+        self.hidden_act = hidden_act\n+        self.hidden_dropout = hidden_dropout\n+        self.layer_norm_eps = layer_norm_eps\n+        self.initializer_range = initializer_range\n+\n+\n+class Sam3MaskDecoderConfig(PreTrainedConfig):\n+    r\"\"\"\n+    Configuration class for SAM3 Mask Decoder (pixel-level mask prediction).\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 256):\n+            Dimensionality of the mask decoder.\n+        num_upsampling_stages (`int`, *optional*, defaults to 3):\n+            Number of upsampling stages in the pixel decoder (FPN).\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            Epsilon for layer normalization.\n+        dropout (`float`, *optional*, defaults to 0.0):\n+            Dropout probability for prompt cross-attention.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for prompt cross-attention.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing weight matrices.\n+    \"\"\"\n+\n+    model_type = \"sam3_mask_decoder\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=256,\n+        num_upsampling_stages=3,\n+        layer_norm_eps=1e-6,\n+        dropout=0.0,\n+        num_attention_heads=8,\n+        initializer_range=0.02,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.hidden_size = hidden_size\n+        self.num_upsampling_stages = num_upsampling_stages\n+        self.layer_norm_eps = layer_norm_eps\n+        self.dropout = dropout\n+        self.num_attention_heads = num_attention_heads\n+        self.initializer_range = initializer_range\n+\n+\n+class Sam3Config(PreTrainedConfig):\n+    r\"\"\"\n+    Configuration class to store the configuration of a [`Sam3Model`].\n+\n+    Instantiating a configuration defaults will yield a similar configuration to that of SAM 3\n+    [facebook/sam3](https://huggingface.co/facebook/sam3) architecture.\n+\n+    This is the main configuration class that combines all sub-configurations for the SAM3 model.\n+\n+    Args:\n+        vision_config (`dict` or `Sam3VisionConfig`, *optional*):\n+            Configuration for the vision encoder.\n+        text_config (`dict` or `Sam3TextConfig`, *optional*):\n+            Configuration for the text encoder.\n+        geometry_encoder_config (`dict` or `Sam3GeometryEncoderConfig`, *optional*):\n+            Configuration for the geometry encoder.\n+        detr_encoder_config (`dict` or `Sam3DETREncoderConfig`, *optional*):\n+            Configuration for the DETR encoder.\n+        detr_decoder_config (`dict` or `Sam3DETRDecoderConfig`, *optional*):\n+            Configuration for the DETR decoder.\n+        mask_decoder_config (`dict` or `Sam3MaskDecoderConfig`, *optional*):\n+            Configuration for the mask decoder.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing weight matrices.\n+\n+    Example:\n+    ```python\n+    >>> from transformers import Sam3Config, Sam3Model\n+\n+    >>> # Initializing a SAM3 configuration\n+    >>> configuration = Sam3Config()\n+\n+    >>> # Initializing a model from the configuration\n+    >>> model = Sam3Model(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\n+    \"\"\"\n+\n+    model_type = \"sam3\"\n+    is_composition = True\n+    sub_configs = {\n+        \"vision_config\": Sam3VisionConfig,\n+        \"text_config\": CLIPTextConfig,\n+        \"geometry_encoder_config\": Sam3GeometryEncoderConfig,\n+        \"detr_encoder_config\": Sam3DETREncoderConfig,\n+        \"detr_decoder_config\": Sam3DETRDecoderConfig,\n+        \"mask_decoder_config\": Sam3MaskDecoderConfig,\n+    }\n+\n+    def __init__(\n+        self,\n+        vision_config=None,\n+        text_config=None,\n+        geometry_encoder_config=None,\n+        detr_encoder_config=None,\n+        detr_decoder_config=None,\n+        mask_decoder_config=None,\n+        initializer_range=0.02,\n+        **kwargs,\n+    ):\n+        # Vision config\n+        if vision_config is None:\n+            vision_config = {}\n+        if isinstance(vision_config, dict):\n+            self.vision_config = Sam3VisionConfig(**vision_config)\n+        else:\n+            self.vision_config = vision_config\n+\n+        # Text config (CLIPTextModelWithProjection defaults)\n+        if text_config is None:\n+            text_config = {\n+                \"vocab_size\": 49408,\n+                \"hidden_size\": 1024,\n+                \"intermediate_size\": 4096,  # hidden_size * mlp_ratio (1024 * 4)\n+                \"projection_dim\": 512,  # CLIP's internal projection dimension\n+                \"num_hidden_layers\": 24,\n+                \"num_attention_heads\": 16,\n+                \"max_position_embeddings\": 32,\n+                \"hidden_act\": \"gelu\",\n+            }\n+        if isinstance(text_config, dict):\n+            self.text_config = CLIPTextConfig(**text_config)\n+        else:\n+            self.text_config = text_config\n+\n+        # Geometry encoder config\n+        if geometry_encoder_config is None:\n+            geometry_encoder_config = {}\n+        if isinstance(geometry_encoder_config, dict):\n+            self.geometry_encoder_config = Sam3GeometryEncoderConfig(**geometry_encoder_config)\n+        else:\n+            self.geometry_encoder_config = geometry_encoder_config\n+\n+        # DETR encoder config\n+        if detr_encoder_config is None:\n+            detr_encoder_config = {}\n+        if isinstance(detr_encoder_config, dict):\n+            self.detr_encoder_config = Sam3DETREncoderConfig(**detr_encoder_config)\n+        else:\n+            self.detr_encoder_config = detr_encoder_config\n+\n+        # DETR decoder config\n+        if detr_decoder_config is None:\n+            detr_decoder_config = {}\n+        if isinstance(detr_decoder_config, dict):\n+            self.detr_decoder_config = Sam3DETRDecoderConfig(**detr_decoder_config)\n+        else:\n+            self.detr_decoder_config = detr_decoder_config\n+\n+        # Mask decoder config\n+        if mask_decoder_config is None:\n+            mask_decoder_config = {}\n+        if isinstance(mask_decoder_config, dict):\n+            self.mask_decoder_config = Sam3MaskDecoderConfig(**mask_decoder_config)\n+        else:\n+            self.mask_decoder_config = mask_decoder_config\n+\n+        self.initializer_range = initializer_range\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\n+    \"Sam3Config\",\n+    \"Sam3ViTConfig\",\n+    \"Sam3VisionConfig\",\n+    \"Sam3GeometryEncoderConfig\",\n+    \"Sam3DETREncoderConfig\",\n+    \"Sam3DETRDecoderConfig\",\n+    \"Sam3MaskDecoderConfig\",\n+]"
        },
        {
            "sha": "30777ba77e3142523f69521c691e50c93284b9fa",
            "filename": "src/transformers/models/sam3/convert_sam3_to_hf.py",
            "status": "added",
            "additions": 475,
            "deletions": 0,
            "changes": 475,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3%2Fconvert_sam3_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3%2Fconvert_sam3_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3%2Fconvert_sam3_to_hf.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,475 @@\n+# Copyright 2025 The Meta AI Authors and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"\n+Convert SAM3 checkpoints from the original implementation to HuggingFace format.\n+\n+Original repository: https://github.com/facebookresearch/segment-anything-3\n+\"\"\"\n+\n+import argparse\n+import gc\n+import os\n+from typing import Optional\n+\n+import regex as re\n+import torch\n+\n+from transformers import CLIPTokenizerFast, Sam3Config, Sam3ImageProcessorFast, Sam3Model, Sam3Processor\n+from transformers.utils import logging\n+\n+\n+logging.set_verbosity_info()\n+logger = logging.get_logger(__name__)\n+\n+# fmt: off\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    r\"^sam3_model\\.\": r\"\",\n+    # ============================================================================\n+    # Vision Encoder - ViT Backbone\n+    # ============================================================================\n+    r\"^backbone\\.vision_backbone\\.trunk\\.\":                                 r\"vision_encoder.backbone.\",\n+    r\"^vision_encoder\\.backbone\\.pos_embed\":                                r\"vision_encoder.backbone.embeddings.position_embeddings\",\n+    r\"^vision_encoder\\.backbone\\.patch_embed\\.proj\\.\":                      r\"vision_encoder.backbone.embeddings.patch_embeddings.projection.\",\n+    r\"^vision_encoder\\.backbone\\.ln_pre\\.\":                                 r\"vision_encoder.backbone.layer_norm.\",\n+    r\"^vision_encoder\\.backbone\\.blocks\\.(\\d+)\\.norm1\\.\":                   r\"vision_encoder.backbone.layers.\\1.layer_norm1.\",\n+    r\"^vision_encoder\\.backbone\\.blocks\\.(\\d+)\\.norm2\\.\":                   r\"vision_encoder.backbone.layers.\\1.layer_norm2.\",\n+    r\"^vision_encoder\\.backbone\\.blocks\\.(\\d+)\\.attn\\.qkv\\.\":               r\"vision_encoder.backbone.layers.\\1.attention.qkv.\",\n+    r\"^vision_encoder\\.backbone\\.blocks\\.(\\d+)\\.attn\\.proj\\.\":              r\"vision_encoder.backbone.layers.\\1.attention.o_proj.\",\n+    r\"^vision_encoder\\.backbone\\.blocks\\.(\\d+)\\.attn\\.freqs_cis\":           r\"vision_encoder.backbone.layers.\\1.rotary_emb.rope_embeddings\",\n+    r\"^vision_encoder\\.backbone\\.blocks\\.(\\d+)\\.mlp\\.fc1\\.\":                r\"vision_encoder.backbone.layers.\\1.mlp.fc1.\",\n+    r\"^vision_encoder\\.backbone\\.blocks\\.(\\d+)\\.mlp\\.fc2\\.\":                r\"vision_encoder.backbone.layers.\\1.mlp.fc2.\",\n+\n+    # Vision Encoder - FPN Neck\n+    r\"^backbone\\.vision_backbone\\.neck\\.fpn\\.(\\d+)\\.\":                      r\"vision_encoder.neck.fpn_layers.\\1.\",\n+    r\"^backbone\\.vision_backbone\\.convs\\.(\\d+)\\.dconv_2x2_0\\.\":             r\"vision_encoder.neck.fpn_layers.\\1.scale_layers.0.\",\n+    r\"^backbone\\.vision_backbone\\.convs\\.(\\d+)\\.dconv_2x2_1\\.\":             r\"vision_encoder.neck.fpn_layers.\\1.scale_layers.2.\",\n+    r\"^backbone\\.vision_backbone\\.convs\\.(\\d+)\\.dconv_2x2\\.\":               r\"vision_encoder.neck.fpn_layers.\\1.scale_layers.0.\",\n+    r\"^backbone\\.vision_backbone\\.convs\\.(\\d+)\\.maxpool_2x2\\.\":             r\"vision_encoder.neck.fpn_layers.\\1.scale_layers.0.\",\n+    r\"^backbone\\.vision_backbone\\.convs\\.(\\d+)\\.conv_1x1\\.\":                r\"vision_encoder.neck.fpn_layers.\\1.proj1.\",\n+    r\"^backbone\\.vision_backbone\\.convs\\.(\\d+)\\.conv_3x3\\.\":                r\"vision_encoder.neck.fpn_layers.\\1.proj2.\",\n+\n+    # ============================================================================\n+    # Text Encoder (CLIP)\n+    # ============================================================================\n+    r\"^backbone\\.language_backbone\\.encoder\\.\":                             r\"text_encoder.\",\n+    r\"^text_encoder\\.token_embedding\\.\":                                    r\"text_encoder.text_model.embeddings.token_embedding.\",\n+    r\"^text_encoder\\.positional_embedding\":                                 r\"text_encoder.text_model.embeddings.position_embedding.weight\",\n+    r\"^text_encoder\\.ln_final\\.\":                                           r\"text_encoder.text_model.final_layer_norm.\",\n+    r\"^text_encoder\\.text_projection\":                                      r\"text_encoder.text_projection.weight\",\n+    r\"^text_encoder\\.transformer\\.resblocks\\.(\\d+)\\.attn\\.in_proj_\":       r\"text_encoder.text_model.encoder.layers.\\1.self_attn.in_proj_\",\n+    r\"^text_encoder\\.transformer\\.resblocks\\.(\\d+)\\.attn\\.out_proj\\.\":     r\"text_encoder.text_model.encoder.layers.\\1.self_attn.out_proj.\",\n+    r\"^text_encoder\\.transformer\\.resblocks\\.(\\d+)\\.ln_1\\.\":               r\"text_encoder.text_model.encoder.layers.\\1.layer_norm1.\",\n+    r\"^text_encoder\\.transformer\\.resblocks\\.(\\d+)\\.ln_2\\.\":               r\"text_encoder.text_model.encoder.layers.\\1.layer_norm2.\",\n+    r\"^text_encoder\\.transformer\\.resblocks\\.(\\d+)\\.mlp\\.c_fc\\.\":          r\"text_encoder.text_model.encoder.layers.\\1.mlp.fc1.\",\n+    r\"^text_encoder\\.transformer\\.resblocks\\.(\\d+)\\.mlp\\.c_proj\\.\":        r\"text_encoder.text_model.encoder.layers.\\1.mlp.fc2.\",\n+    r\"^backbone\\.language_backbone\\.resizer\\.\":                             r\"text_projection.\",\n+\n+    # ============================================================================\n+    # Geometry Encoder\n+    # ============================================================================\n+    r\"^geometry_encoder\\.encode\\.(\\d+)\\.cross_attn_image\\.out_proj\\.\":     r\"geometry_encoder.layers.\\1.cross_attn.o_proj.\",\n+    r\"^geometry_encoder\\.encode\\.(\\d+)\\.cross_attn_image\\.\":               r\"geometry_encoder.layers.\\1.cross_attn.\",\n+    r\"^geometry_encoder\\.encode\\.(\\d+)\\.self_attn\\.out_proj\\.\":            r\"geometry_encoder.layers.\\1.self_attn.o_proj.\",\n+    r\"^geometry_encoder\\.encode\\.(\\d+)\\.self_attn\\.\":                      r\"geometry_encoder.layers.\\1.self_attn.\",\n+    r\"^geometry_encoder\\.encode\\.(\\d+)\\.linear1\\.\":                        r\"geometry_encoder.layers.\\1.mlp.fc1.\",\n+    r\"^geometry_encoder\\.encode\\.(\\d+)\\.linear2\\.\":                        r\"geometry_encoder.layers.\\1.mlp.fc2.\",\n+    r\"^geometry_encoder\\.encode\\.(\\d+)\\.norm1\\.\":                          r\"geometry_encoder.layers.\\1.layer_norm1.\",\n+    r\"^geometry_encoder\\.encode\\.(\\d+)\\.norm2\\.\":                          r\"geometry_encoder.layers.\\1.layer_norm2.\",\n+    r\"^geometry_encoder\\.encode\\.(\\d+)\\.norm3\\.\":                          r\"geometry_encoder.layers.\\1.layer_norm3.\",\n+    r\"^geometry_encoder\\.img_pre_norm\\.\":                                   r\"geometry_encoder.vision_layer_norm.\",\n+    r\"^geometry_encoder\\.norm\\.\":                                           r\"geometry_encoder.prompt_layer_norm.\",\n+    r\"^geometry_encoder\\.encode_norm\\.\":                                    r\"geometry_encoder.output_layer_norm.\",\n+\n+    # ============================================================================\n+    # DETR Encoder\n+    # ============================================================================\n+    r\"^transformer\\.encoder\\.layers\\.(\\d+)\\.cross_attn_image\\.out_proj\\.\":  r\"detr_encoder.layers.\\1.cross_attn.o_proj.\",\n+    r\"^transformer\\.encoder\\.layers\\.(\\d+)\\.cross_attn_image\\.\":            r\"detr_encoder.layers.\\1.cross_attn.\",\n+    r\"^transformer\\.encoder\\.layers\\.(\\d+)\\.self_attn\\.out_proj\\.\":         r\"detr_encoder.layers.\\1.self_attn.o_proj.\",\n+    r\"^transformer\\.encoder\\.layers\\.(\\d+)\\.self_attn\\.\":                   r\"detr_encoder.layers.\\1.self_attn.\",\n+    r\"^transformer\\.encoder\\.layers\\.(\\d+)\\.cross_attn\\.out_proj\\.\":        r\"detr_encoder.layers.\\1.cross_attn.o_proj.\",\n+    r\"^transformer\\.encoder\\.layers\\.(\\d+)\\.cross_attn\\.\":                  r\"detr_encoder.layers.\\1.cross_attn.\",\n+    r\"^transformer\\.encoder\\.layers\\.(\\d+)\\.linear1\\.\":                     r\"detr_encoder.layers.\\1.mlp.fc1.\",\n+    r\"^transformer\\.encoder\\.layers\\.(\\d+)\\.linear2\\.\":                     r\"detr_encoder.layers.\\1.mlp.fc2.\",\n+    r\"^transformer\\.encoder\\.layers\\.(\\d+)\\.norm1\\.\":                       r\"detr_encoder.layers.\\1.layer_norm1.\",\n+    r\"^transformer\\.encoder\\.layers\\.(\\d+)\\.norm2\\.\":                       r\"detr_encoder.layers.\\1.layer_norm2.\",\n+    r\"^transformer\\.encoder\\.layers\\.(\\d+)\\.norm3\\.\":                       r\"detr_encoder.layers.\\1.layer_norm3.\",\n+\n+    # ============================================================================\n+    # DETR Decoder\n+    # ============================================================================\n+    r\"^transformer\\.decoder\\.query_embed\\.\":                                r\"detr_decoder.query_embed.\",\n+    r\"^transformer\\.decoder\\.reference_points\\.\":                           r\"detr_decoder.reference_points.\",\n+    r\"^transformer\\.decoder\\.instance_query_embed\\.\":                       r\"detr_decoder.instance_query_embed.\",\n+    r\"^transformer\\.decoder\\.instance_reference_points\\.\":                  r\"detr_decoder.instance_reference_points.\",\n+    r\"^transformer\\.decoder\\.presence_token\\.\":                             r\"detr_decoder.presence_token.\",\n+    r\"^transformer\\.decoder\\.presence_token_head\\.layers\\.0\\.\":             r\"detr_decoder.presence_head.layer1.\",\n+    r\"^transformer\\.decoder\\.presence_token_head\\.layers\\.1\\.\":             r\"detr_decoder.presence_head.layer2.\",\n+    r\"^transformer\\.decoder\\.presence_token_head\\.layers\\.2\\.\":             r\"detr_decoder.presence_head.layer3.\",\n+    r\"^transformer\\.decoder\\.presence_token_out_norm\\.\":                    r\"detr_decoder.presence_layer_norm.\",\n+    r\"^transformer\\.decoder\\.norm\\.\":                                       r\"detr_decoder.output_layer_norm.\",\n+    r\"^transformer\\.decoder\\.bbox_embed\\.layers\\.0\\.\":                      r\"detr_decoder.box_head.layer1.\",\n+    r\"^transformer\\.decoder\\.bbox_embed\\.layers\\.1\\.\":                      r\"detr_decoder.box_head.layer2.\",\n+    r\"^transformer\\.decoder\\.bbox_embed\\.layers\\.2\\.\":                      r\"detr_decoder.box_head.layer3.\",\n+    r\"^transformer\\.decoder\\.instance_bbox_embed\\.layers\\.0\\.\":             r\"detr_decoder.instance_box_head.layer1.\",\n+    r\"^transformer\\.decoder\\.instance_bbox_embed\\.layers\\.1\\.\":             r\"detr_decoder.instance_box_head.layer2.\",\n+    r\"^transformer\\.decoder\\.instance_bbox_embed\\.layers\\.2\\.\":             r\"detr_decoder.instance_box_head.layer3.\",\n+    r\"^transformer\\.decoder\\.ref_point_head\\.layers\\.0\\.\":                  r\"detr_decoder.ref_point_head.layer1.\",\n+    r\"^transformer\\.decoder\\.ref_point_head\\.layers\\.1\\.\":                  r\"detr_decoder.ref_point_head.layer2.\",\n+    r\"^transformer\\.decoder\\.boxRPB_embed_x\\.layers\\.0\\.\":                  r\"detr_decoder.box_rpb_embed_x.layer1.\",\n+    r\"^transformer\\.decoder\\.boxRPB_embed_x\\.layers\\.1\\.\":                  r\"detr_decoder.box_rpb_embed_x.layer2.\",\n+    r\"^transformer\\.decoder\\.boxRPB_embed_y\\.layers\\.0\\.\":                  r\"detr_decoder.box_rpb_embed_y.layer1.\",\n+    r\"^transformer\\.decoder\\.boxRPB_embed_y\\.layers\\.1\\.\":                  r\"detr_decoder.box_rpb_embed_y.layer2.\",\n+    r\"^transformer\\.decoder\\.layers\\.(\\d+)\\.self_attn\\.out_proj\\.\":         r\"detr_decoder.layers.\\1.self_attn.o_proj.\",\n+    r\"^transformer\\.decoder\\.layers\\.(\\d+)\\.self_attn\\.\":                   r\"detr_decoder.layers.\\1.self_attn.\",\n+    r\"^transformer\\.decoder\\.layers\\.(\\d+)\\.ca_text\\.out_proj\\.\":           r\"detr_decoder.layers.\\1.text_cross_attn.o_proj.\",\n+    r\"^transformer\\.decoder\\.layers\\.(\\d+)\\.ca_text\\.\":                     r\"detr_decoder.layers.\\1.text_cross_attn.\",\n+    r\"^transformer\\.decoder\\.layers\\.(\\d+)\\.cross_attn\\.out_proj\\.\":        r\"detr_decoder.layers.\\1.vision_cross_attn.o_proj.\",\n+    r\"^transformer\\.decoder\\.layers\\.(\\d+)\\.cross_attn\\.\":                  r\"detr_decoder.layers.\\1.vision_cross_attn.\",\n+    r\"^transformer\\.decoder\\.layers\\.(\\d+)\\.linear1\\.\":                     r\"detr_decoder.layers.\\1.mlp.fc1.\",\n+    r\"^transformer\\.decoder\\.layers\\.(\\d+)\\.linear2\\.\":                     r\"detr_decoder.layers.\\1.mlp.fc2.\",\n+    r\"^transformer\\.decoder\\.layers\\.(\\d+)\\.norm1\\.\":                       r\"detr_decoder.layers.\\1.vision_cross_attn_layer_norm.\",\n+    r\"^transformer\\.decoder\\.layers\\.(\\d+)\\.catext_norm\\.\":                 r\"detr_decoder.layers.\\1.text_cross_attn_layer_norm.\",\n+    r\"^transformer\\.decoder\\.layers\\.(\\d+)\\.norm2\\.\":                       r\"detr_decoder.layers.\\1.self_attn_layer_norm.\",\n+    r\"^transformer\\.decoder\\.layers\\.(\\d+)\\.norm3\\.\":                       r\"detr_decoder.layers.\\1.mlp_layer_norm.\",\n+\n+    # ============================================================================\n+    # Dot Product Scoring\n+    # ============================================================================\n+    r\"^dot_prod_scoring\\.prompt_mlp\\.layers\\.0\\.\":                          r\"dot_product_scoring.text_mlp.layer1.\",\n+    r\"^dot_prod_scoring\\.prompt_mlp\\.layers\\.1\\.\":                          r\"dot_product_scoring.text_mlp.layer2.\",\n+    r\"^dot_prod_scoring\\.prompt_mlp\\.out_norm\\.\":                           r\"dot_product_scoring.text_mlp_out_norm.\",\n+    r\"^dot_prod_scoring\\.prompt_proj\\.\":                                    r\"dot_product_scoring.text_proj.\",\n+    r\"^dot_prod_scoring\\.hs_proj\\.\":                                        r\"dot_product_scoring.query_proj.\",\n+\n+    # ============================================================================\n+    # Mask Decoder\n+    # ============================================================================\n+    r\"^segmentation_head\\.pixel_decoder\\.conv_layers\\.(\\d+)\\.\":             r\"mask_decoder.pixel_decoder.conv_layers.\\1.\",\n+    r\"^segmentation_head\\.pixel_decoder\\.norms\\.(\\d+)\\.\":                   r\"mask_decoder.pixel_decoder.norms.\\1.\",\n+    r\"^segmentation_head\\.mask_embed\\.layers\\.(\\d+)\\.\":                     r\"mask_decoder.mask_embedder.layers.\\1.\",\n+    r\"^segmentation_head\\.mask_predictor\\.mask_embed\\.layers\\.(\\d+)\\.\":     r\"mask_decoder.mask_embedder.layers.\\1.\",\n+    r\"^segmentation_head\\.instance_seg_head\\.\":                             r\"mask_decoder.instance_projection.\",\n+    r\"^segmentation_head\\.semantic_seg_head\\.\":                             r\"mask_decoder.semantic_projection.\",\n+    r\"^segmentation_head\\.cross_attend_prompt\\.out_proj\\.\":                 r\"mask_decoder.prompt_cross_attn.o_proj.\",\n+    r\"^segmentation_head\\.cross_attend_prompt\\.\":                           r\"mask_decoder.prompt_cross_attn.\",\n+    r\"^segmentation_head\\.cross_attn_norm\\.\":                               r\"mask_decoder.prompt_cross_attn_norm.\",\n+}\n+# fmt: on\n+\n+\n+def convert_old_keys_to_new_keys(state_dict_keys: list[str]) -> dict[str, str]:\n+    \"\"\"\n+    Convert original SAM3 checkpoint keys to HuggingFace format.\n+\n+    This function applies regex patterns to efficiently rename keys in bulk.\n+\n+    Args:\n+        state_dict_keys: List of original checkpoint keys\n+\n+    Returns:\n+        Dictionary mapping original keys to new keys\n+    \"\"\"\n+    output_dict = {}\n+    if state_dict_keys is not None:\n+        old_text = \"\\n\".join(state_dict_keys)\n+        new_text = old_text\n+\n+        # Apply all regex patterns\n+        for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING.items():\n+            new_text = re.sub(pattern, replacement, new_text, flags=re.MULTILINE)\n+\n+        output_dict = dict(zip(old_text.split(\"\\n\"), new_text.split(\"\\n\")))\n+\n+    return output_dict\n+\n+\n+def split_qkv(state_dict: dict) -> dict:\n+    \"\"\"\n+    Split combined QKV weights/biases into separate Q, K, V projections.\n+\n+    Both the vision backbone and text encoder in the original SAM3 use combined QKV projections,\n+    but the refactored model uses separate Q, K, V projections.\n+\n+    Args:\n+        state_dict: State dictionary with combined QKV weights\n+\n+    Returns:\n+        State dictionary with split Q, K, V weights\n+    \"\"\"\n+    # Handle vision backbone: .attention.qkv.* â†’ .attention.{q,k,v}_proj.*\n+    vision_keys_to_split = [key for key in state_dict.keys() if \".attention.qkv.\" in key]\n+\n+    for key in vision_keys_to_split:\n+        qkv = state_dict.pop(key)\n+        # Split into 3 equal chunks along dimension 0 (output dimension)\n+        q, k, v = torch.chunk(qkv, 3, dim=0)\n+\n+        # Create new keys for q_proj, k_proj, v_proj\n+        state_dict[key.replace(\".qkv.\", \".q_proj.\")] = q\n+        state_dict[key.replace(\".qkv.\", \".k_proj.\")] = k\n+        state_dict[key.replace(\".qkv.\", \".v_proj.\")] = v\n+\n+    # Handle all attention layers with in_proj_* (text encoder, DETR decoder cross-attention, mask decoder)\n+    # These use: .{attn_type}.in_proj_* â†’ .{attn_type}.{q,k,v}_proj.*\n+    in_proj_keys_to_split = [key for key in state_dict.keys() if \".in_proj_\" in key]\n+\n+    for key in in_proj_keys_to_split:\n+        in_proj = state_dict.pop(key)\n+        # Split into 3 equal chunks along dimension 0 (output dimension)\n+        q, k, v = torch.chunk(in_proj, 3, dim=0)\n+\n+        # Create new keys for q_proj, k_proj, v_proj\n+        # Replace \"in_proj_weight\" with \"q_proj.weight\" (or \"in_proj_bias\" with \"q_proj.bias\")\n+        if key.endswith(\"in_proj_weight\"):\n+            base_key = key.replace(\"in_proj_weight\", \"\")\n+            state_dict[base_key + \"q_proj.weight\"] = q\n+            state_dict[base_key + \"k_proj.weight\"] = k\n+            state_dict[base_key + \"v_proj.weight\"] = v\n+        elif key.endswith(\"in_proj_bias\"):\n+            base_key = key.replace(\"in_proj_bias\", \"\")\n+            state_dict[base_key + \"q_proj.bias\"] = q\n+            state_dict[base_key + \"k_proj.bias\"] = k\n+            state_dict[base_key + \"v_proj.bias\"] = v\n+\n+    return state_dict\n+\n+\n+def load_original_state_dict(checkpoint_path: str) -> dict[str, torch.Tensor]:\n+    \"\"\"Load the original SAM3 checkpoint.\"\"\"\n+    print(f\"Loading original checkpoint from {checkpoint_path}\")\n+\n+    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n+\n+    # Handle different checkpoint formats\n+    if \"model\" in checkpoint:\n+        state_dict = checkpoint[\"model\"]\n+    elif \"state_dict\" in checkpoint:\n+        state_dict = checkpoint[\"state_dict\"]\n+    else:\n+        state_dict = checkpoint\n+\n+    print(f\"Loaded {len(state_dict)} keys from checkpoint\")\n+    return state_dict\n+\n+\n+def get_sam3_config(\n+    vision_config: Optional[dict] = None,\n+    text_config: Optional[dict] = None,\n+) -> Sam3Config:\n+    \"\"\"\n+    Create SAM3 configuration.\n+\n+    Args:\n+        vision_config: Optional vision encoder configuration overrides\n+        text_config: Optional text encoder configuration overrides\n+\n+    Returns:\n+        Sam3Config instance\n+    \"\"\"\n+    config = Sam3Config()\n+\n+    # Update with any provided overrides\n+    if vision_config is not None:\n+        for key, value in vision_config.items():\n+            setattr(config.vision_config, key, value)\n+\n+    if text_config is not None:\n+        # Text config is a CLIPTextConfig\n+        for key, value in text_config.items():\n+            setattr(config.text_config, key, value)\n+\n+    return config\n+\n+\n+def convert_sam3_checkpoint(\n+    checkpoint_path: str,\n+    output_path: str,\n+    config: Optional[Sam3Config] = None,\n+    push_to_hub: bool = False,\n+    repo_id: Optional[str] = None,\n+    safe_serialization: bool = True,\n+):\n+    \"\"\"\n+    Convert SAM3 checkpoint from original format to HuggingFace format.\n+\n+    Args:\n+        checkpoint_path: Path to the original checkpoint file\n+        output_path: Path to save the converted checkpoint\n+        config: Optional Sam3Config to use (otherwise creates default)\n+        push_to_hub: Whether to push the model to the Hub\n+        repo_id: Repository ID for pushing to Hub\n+        safe_serialization: Whether to save using safetensors\n+    \"\"\"\n+    # Create output directory\n+    os.makedirs(output_path, exist_ok=True)\n+\n+    # Load configuration\n+    if config is None:\n+        config = get_sam3_config()\n+\n+    config.architectures = [\"Sam3Model\"]\n+    config.save_pretrained(output_path)\n+    print(\"Model config saved successfully\")\n+\n+    # Load and convert weights\n+    print(\"Loading original checkpoint...\")\n+    state_dict_old = load_original_state_dict(checkpoint_path)\n+\n+    print(\"Converting checkpoint keys...\")\n+    all_keys = list(state_dict_old.keys())\n+    key_mapping = convert_old_keys_to_new_keys(all_keys)\n+\n+    # Create new state dict with converted keys\n+    state_dict_new = {}\n+\n+    for old_key in all_keys:\n+        new_key = key_mapping.get(old_key, old_key)\n+        # Special handling: Strip cls token from vision backbone position embeddings\n+        if new_key == \"vision_encoder.backbone.embeddings.position_embeddings\":\n+            # Original has [1, 577, 1024] with cls token, but refactored expects [1, 576, 1024] without cls token\n+            # Strip the first position (cls token position)\n+            state_dict_new[new_key] = state_dict_old[old_key][:, 1:, :]\n+        else:\n+            state_dict_new[new_key] = state_dict_old[old_key]\n+\n+    del state_dict_old\n+    gc.collect()\n+\n+    # Split combined QKV projections into separate Q, K, V projections\n+    print(\"Splitting QKV projections...\")\n+    state_dict_new = split_qkv(state_dict_new)\n+\n+    # Transpose CLIP text projection (stored transposed in original)\n+    if \"text_encoder.text_projection.weight\" in state_dict_new:\n+        print(\"Transposing CLIP text_projection...\")\n+        state_dict_new[\"text_encoder.text_projection.weight\"] = state_dict_new[\"text_encoder.text_projection.weight\"].T\n+\n+    # Load into HF model\n+    print(\"Loading weights into Sam3Model...\")\n+    model = Sam3Model(config)\n+    missing_keys, unexpected_keys = model.load_state_dict(state_dict_new, strict=False)\n+\n+    if missing_keys:\n+        logger.warning(f\"Missing keys ({len(missing_keys)}):\")\n+        for key in missing_keys:  # Show more keys for debugging\n+            logger.warning(f\"  - {key}\")\n+\n+    if unexpected_keys:\n+        logger.warning(f\"Unexpected keys ({len(unexpected_keys)}):\")\n+        for key in unexpected_keys:  # Show more keys for debugging\n+            logger.warning(f\"  - {key}\")\n+\n+    # Note: Some missing/unexpected keys are expected:\n+    # - vision_encoder.backbone.embeddings.patch_embeddings.projection.bias: patch projection has bias=False\n+    # - geometry_encoder.mask_encoder.projection.*: this is nn.Identity() in original (no weights)\n+    # - rotary_emb.rope_embeddings: pre-computed in original, computed on-the-fly in refactored\n+    # - text_encoder.text_projection.bias: projection layer might not have bias\n+\n+    # Save model\n+    print(f\"Saving converted model to {output_path}\")\n+    model.save_pretrained(\n+        output_path,\n+        safe_serialization=safe_serialization,\n+    )\n+\n+    # Save processor\n+    print(\"Creating and saving processor...\")\n+    image_processor = Sam3ImageProcessorFast()\n+    tokenizer = CLIPTokenizerFast.from_pretrained(\"openai/clip-vit-base-patch32\", max_length=32, model_max_length=32)\n+    processor = Sam3Processor(image_processor=image_processor, tokenizer=tokenizer)\n+    processor.save_pretrained(output_path)\n+\n+    # Push to hub if requested\n+    if push_to_hub:\n+        if repo_id is None:\n+            raise ValueError(\"repo_id must be provided when push_to_hub=True\")\n+        print(f\"Pushing model to Hub: {repo_id}\")\n+        model.push_to_hub(repo_id, use_temp_dir=True)\n+        processor.push_to_hub(repo_id, use_temp_dir=True)\n+\n+    print(\"Conversion complete!\")\n+    print(f\"Model saved successfully to: {output_path}\")\n+\n+    # Cleanup\n+    del state_dict_new, model\n+    gc.collect()\n+\n+    # Verify the conversion by reloading\n+    print(\"\\nVerifying converted checkpoint can be loaded...\")\n+    try:\n+        model = Sam3Model.from_pretrained(output_path)\n+        param_count = sum(p.numel() for p in model.parameters())\n+        print(f\"âœ“ Successfully loaded model with {param_count:,} parameters\")\n+        del model\n+        gc.collect()\n+    except Exception as e:\n+        print(f\"âœ— Failed to reload model: {e}\")\n+\n+    print(\"\\n\" + \"=\" * 80)\n+    print(\"Conversion finished!\")\n+    print(\"=\" * 80)\n+    print(f\"Output directory: {output_path}\")\n+    print(\"\\nTo test the model, you can run:\")\n+    print(\">>> from transformers import Sam3Model\")\n+    print(f\">>> model = Sam3Model.from_pretrained('{output_path}')\")\n+    print(\"=\" * 80)\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser(description=\"Convert SAM3 checkpoint to HuggingFace format\")\n+    parser.add_argument(\n+        \"--checkpoint_path\",\n+        type=str,\n+        required=True,\n+        help=\"Path to the original SAM3 checkpoint file\",\n+    )\n+    parser.add_argument(\n+        \"--output_path\",\n+        type=str,\n+        required=True,\n+        help=\"Path to save the converted checkpoint\",\n+    )\n+    parser.add_argument(\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether to push the converted model to the Hugging Face Hub\",\n+    )\n+    parser.add_argument(\n+        \"--repo_id\",\n+        type=str,\n+        default=None,\n+        help=\"Repository ID for pushing to Hub (e.g., 'facebook/sam3-large')\",\n+    )\n+    parser.add_argument(\n+        \"--safe_serialization\",\n+        action=\"store_true\",\n+        default=True,\n+        help=\"Whether to save using safetensors format\",\n+    )\n+\n+    args = parser.parse_args()\n+\n+    convert_sam3_checkpoint(\n+        checkpoint_path=args.checkpoint_path,\n+        output_path=args.output_path,\n+        push_to_hub=args.push_to_hub,\n+        repo_id=args.repo_id,\n+        safe_serialization=args.safe_serialization,\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "90089a334bbb96016fa657557eb00a0c9449b60b",
            "filename": "src/transformers/models/sam3/image_processing_sam3_fast.py",
            "status": "added",
            "additions": 949,
            "deletions": 0,
            "changes": 949,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3%2Fimage_processing_sam3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3%2Fimage_processing_sam3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3%2Fimage_processing_sam3_fast.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,949 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/sam3/modular_sam3.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_sam3.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 The Meta AI Authors and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import math\n+from copy import deepcopy\n+from itertools import product\n+from typing import Any, Optional, Union\n+\n+import numpy as np\n+import torch\n+import torch.nn.functional as F\n+from torchvision.ops.boxes import batched_nms\n+\n+from ...image_processing_utils import BatchFeature, get_size_dict\n+from ...image_processing_utils_fast import BaseImageProcessorFast\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+    pil_torch_interpolation_mapping,\n+)\n+from ...processing_utils import ImagesKwargs, Unpack\n+from ...utils import TensorType, auto_docstring\n+\n+\n+class Sam3FastImageProcessorKwargs(ImagesKwargs, total=False):\n+    r\"\"\"\n+    mask_size (`dict[str, int]`, *optional*):\n+        The size `{\"height\": int, \"width\": int}` to resize the segmentation maps to.\n+    \"\"\"\n+\n+    mask_size: dict[str, int]\n+\n+\n+def _compute_stability_score(masks: \"torch.Tensor\", mask_threshold: float, stability_score_offset: int):\n+    # One mask is always contained inside the other.\n+    # Save memory by preventing unnecessary cast to torch.int64\n+    intersections = (\n+        (masks > (mask_threshold + stability_score_offset)).sum(-1, dtype=torch.int16).sum(-1, dtype=torch.int32)\n+    )\n+    unions = (masks > (mask_threshold - stability_score_offset)).sum(-1, dtype=torch.int16).sum(-1, dtype=torch.int32)\n+    stability_scores = intersections / unions\n+    return stability_scores\n+\n+\n+def _mask_to_rle(input_mask: \"torch.Tensor\"):\n+    \"\"\"\n+    Encodes masks the run-length encoding (RLE), in the format expected by pycoco tools.\n+    \"\"\"\n+    # Put in fortran order and flatten height and width\n+    batch_size, height, width = input_mask.shape\n+    input_mask = input_mask.permute(0, 2, 1).flatten(1)\n+\n+    # Compute change indices\n+    diff = input_mask[:, 1:] ^ input_mask[:, :-1]\n+    change_indices = diff.nonzero()\n+\n+    # Encode run length\n+    out = []\n+    for i in range(batch_size):\n+        cur_idxs = change_indices[change_indices[:, 0] == i, 1] + 1\n+        if len(cur_idxs) == 0:\n+            # No changes => either all 0 or all 1\n+            # If the entire mask is 0, RLE is [height*width] or if the entire mask is 1, RLE is [0, height*width].\n+            if input_mask[i, 0] == 0:\n+                out.append({\"size\": [height, width], \"counts\": [height * width]})\n+            else:\n+                out.append({\"size\": [height, width], \"counts\": [0, height * width]})\n+            continue\n+        btw_idxs = cur_idxs[1:] - cur_idxs[:-1]\n+        counts = [] if input_mask[i, 0] == 0 else [0]\n+        counts += [cur_idxs[0].item()] + btw_idxs.tolist() + [height * width - cur_idxs[-1].item()]\n+        out.append({\"size\": [height, width], \"counts\": counts})\n+    return out\n+\n+\n+def _batched_mask_to_box(masks: \"torch.Tensor\"):\n+    \"\"\"\n+    Computes the bounding boxes around the given input masks. The bounding boxes are in the XYXY format which\n+    corresponds the following required indices:\n+        - LEFT: left hand side of the bounding box\n+        - TOP: top of the bounding box\n+        - RIGHT: right of the bounding box\n+        - BOTTOM: bottom of the bounding box\n+\n+    Return [0,0,0,0] for an empty mask. For input shape channel_1 x channel_2 x ... x height x width, the output shape\n+    is channel_1 x channel_2 x ... x 4.\n+\n+    Args:\n+        - masks (`torch.Tensor` of shape `(batch, nb_mask, height, width)`)\n+    \"\"\"\n+    # torch.max below raises an error on empty inputs, just skip in this case\n+\n+    if torch.numel(masks) == 0:\n+        return torch.zeros(*masks.shape[:-2], 4, device=masks.device)\n+\n+    # Normalize shape to Cxheightxwidth\n+    shape = masks.shape\n+    height, width = shape[-2:]\n+\n+    # Get top and bottom edges\n+    in_height, _ = torch.max(masks, dim=-1)\n+    in_height_coords = in_height * torch.arange(height, device=in_height.device)[None, :]\n+    bottom_edges, _ = torch.max(in_height_coords, dim=-1)\n+    in_height_coords = in_height_coords + height * (~in_height)\n+    top_edges, _ = torch.min(in_height_coords, dim=-1)\n+\n+    # Get left and right edges\n+    in_width, _ = torch.max(masks, dim=-2)\n+    in_width_coords = in_width * torch.arange(width, device=in_width.device)[None, :]\n+    right_edges, _ = torch.max(in_width_coords, dim=-1)\n+    in_width_coords = in_width_coords + width * (~in_width)\n+    left_edges, _ = torch.min(in_width_coords, dim=-1)\n+\n+    # If the mask is empty the right edge will be to the left of the left edge.\n+    # Replace these boxes with [0, 0, 0, 0]\n+    empty_filter = (right_edges < left_edges) | (bottom_edges < top_edges)\n+    out = torch.stack([left_edges, top_edges, right_edges, bottom_edges], dim=-1)\n+    out = out * (~empty_filter).unsqueeze(-1)\n+\n+    # Return to original shape\n+    out = out.reshape(*shape[:-2], 4)\n+    return out\n+\n+\n+def _is_box_near_crop_edge(boxes, crop_box, orig_box, atol=20.0):\n+    \"\"\"Filter masks at the edge of a crop, but not at the edge of the original image.\"\"\"\n+    crop_box_torch = torch.as_tensor(crop_box, dtype=torch.float, device=boxes.device)\n+    orig_box_torch = torch.as_tensor(orig_box, dtype=torch.float, device=boxes.device)\n+\n+    left, top, _, _ = crop_box\n+    offset = torch.tensor([[left, top, left, top]], device=boxes.device)\n+    # Check if boxes has a channel dimension\n+    if len(boxes.shape) == 3:\n+        offset = offset.unsqueeze(1)\n+    boxes = (boxes + offset).float()\n+\n+    near_crop_edge = torch.isclose(boxes, crop_box_torch[None, :], atol=atol, rtol=0)\n+    near_image_edge = torch.isclose(boxes, orig_box_torch[None, :], atol=atol, rtol=0)\n+    near_crop_edge = torch.logical_and(near_crop_edge, ~near_image_edge)\n+    return torch.any(near_crop_edge, dim=1)\n+\n+\n+def _pad_masks(masks, crop_box: list[int], orig_height: int, orig_width: int):\n+    left, top, right, bottom = crop_box\n+    if left == 0 and top == 0 and right == orig_width and bottom == orig_height:\n+        return masks\n+    # Coordinate transform masks\n+    pad_x, pad_y = orig_width - (right - left), orig_height - (bottom - top)\n+    pad = (left, pad_x - left, top, pad_y - top)\n+    return torch.nn.functional.pad(masks, pad, value=0)\n+\n+\n+def _generate_crop_boxes(\n+    image,\n+    target_size: int,  # Is it tuple here?\n+    crop_n_layers: int = 0,\n+    overlap_ratio: float = 512 / 1500,\n+    points_per_crop: Optional[int] = 32,\n+    crop_n_points_downscale_factor: Optional[list[int]] = 1,\n+) -> tuple[list[list[int]], list[int]]:\n+    \"\"\"\n+    Generates a list of crop boxes of different sizes. Each layer has (2**i)**2 boxes for the ith layer.\n+\n+    Args:\n+        image (Union[`numpy.ndarray`, `PIL.Image`, `torch.Tensor`]):\n+            Image to generate crops for.\n+        target_size (`int`):\n+            Size of the smallest crop.\n+        crop_n_layers (`int`, *optional*):\n+            If `crops_n_layers>0`, mask prediction will be run again on crops of the image. Sets the number of layers\n+            to run, where each layer has 2**i_layer number of image crops.\n+        overlap_ratio (`int`, *optional*):\n+            Sets the degree to which crops overlap. In the first crop layer, crops will overlap by this fraction of the\n+            image length. Later layers with more crops scale down this overlap.\n+        points_per_crop (`int`, *optional*):\n+            Number of points to sam3ple per crop.\n+        crop_n_points_downscale_factor (`int`, *optional*):\n+            The number of points-per-side sam3pled in layer n is scaled down by crop_n_points_downscale_factor**n.\n+        input_data_format (`str` or `ChannelDimension`, *optional*):\n+            The channel dimension format of the input image. If not provided, it will be inferred.\n+    \"\"\"\n+\n+    if isinstance(image, list):\n+        raise ValueError(\"Only one image is allowed for crop generation.\")\n+    original_size = image.shape[-2:]\n+\n+    points_grid = []\n+    for i in range(crop_n_layers + 1):\n+        n_points = int(points_per_crop / (crop_n_points_downscale_factor**i))\n+        points_grid.append(_build_point_grid(n_points))\n+\n+    crop_boxes, layer_idxs = _generate_per_layer_crops(crop_n_layers, overlap_ratio, original_size)\n+\n+    cropped_images, point_grid_per_crop = _generate_crop_images(\n+        crop_boxes, image, points_grid, layer_idxs, target_size, original_size\n+    )\n+    crop_boxes = torch.tensor(crop_boxes)\n+    crop_boxes = crop_boxes.float()\n+    points_per_crop = torch.stack(point_grid_per_crop)\n+    points_per_crop = points_per_crop.unsqueeze(0).permute(0, 2, 1, 3)\n+    cropped_images = torch.stack(cropped_images)\n+\n+    input_labels = torch.ones_like(points_per_crop[:, :, :, 0], dtype=torch.int64)\n+\n+    return crop_boxes, points_per_crop, cropped_images, input_labels\n+\n+\n+def _generate_per_layer_crops(crop_n_layers, overlap_ratio, original_size):\n+    \"\"\"\n+    Generates 2 ** (layers idx + 1) crops for each crop_n_layers. Crops are in the XYWH format : The XYWH format\n+    consists of the following required indices:\n+        - X: X coordinate of the top left of the bounding box\n+        - Y: Y coordinate of the top left of the bounding box\n+        - W: width of the bounding box\n+        - H: height of the bounding box\n+    \"\"\"\n+    crop_boxes, layer_idxs = [], []\n+    im_height, im_width = original_size\n+    short_side = min(im_height, im_width)\n+\n+    # Original image\n+    crop_boxes.append([0, 0, im_width, im_height])\n+    layer_idxs.append(0)\n+    for i_layer in range(crop_n_layers):\n+        n_crops_per_side = 2 ** (i_layer + 1)\n+        overlap = int(overlap_ratio * short_side * (2 / n_crops_per_side))\n+\n+        crop_width = int(math.ceil((overlap * (n_crops_per_side - 1) + im_width) / n_crops_per_side))\n+        crop_height = int(math.ceil((overlap * (n_crops_per_side - 1) + im_height) / n_crops_per_side))\n+\n+        crop_box_x0 = [int((crop_width - overlap) * i) for i in range(n_crops_per_side)]\n+        crop_box_y0 = [int((crop_height - overlap) * i) for i in range(n_crops_per_side)]\n+\n+        for left, top in product(crop_box_x0, crop_box_y0):\n+            box = [left, top, min(left + crop_width, im_width), min(top + crop_height, im_height)]\n+            crop_boxes.append(box)\n+            layer_idxs.append(i_layer + 1)\n+\n+    return crop_boxes, layer_idxs\n+\n+\n+def _build_point_grid(n_per_side: int) -> torch.Tensor:\n+    \"\"\"Generates a 2D grid of points evenly spaced in [0,1]x[0,1].\"\"\"\n+    offset = 1 / (2 * n_per_side)\n+    points_one_side = torch.linspace(offset, 1 - offset, n_per_side)\n+    points_x = torch.tile(points_one_side[None, :], (n_per_side, 1))\n+    points_y = torch.tile(points_one_side[:, None], (1, n_per_side))\n+    points = torch.stack([points_x, points_y], dim=-1).reshape(-1, 2)\n+    return points\n+\n+\n+def _generate_crop_images(\n+    crop_boxes, image, points_grid, layer_idxs, target_size, original_size, input_data_format=None\n+):\n+    \"\"\"\n+    Takes as an input bounding boxes that are used to crop the image. Based in the crops, the corresponding points are\n+    also passed.\n+    \"\"\"\n+    cropped_images = []\n+    total_points_per_crop = []\n+    for i, crop_box in enumerate(crop_boxes):\n+        left, top, right, bottom = crop_box\n+        cropped_im = image[:, top:bottom, left:right]\n+\n+        cropped_images.append(cropped_im)\n+\n+        cropped_im_size = cropped_im.shape[-2:]\n+        points_scale = torch.tensor(cropped_im_size).flip(dims=(0,)).unsqueeze(0)\n+\n+        points = points_grid[layer_idxs[i]] * points_scale\n+        normalized_points = _normalize_coordinates(target_size, points, original_size)\n+        total_points_per_crop.append(normalized_points)\n+\n+    return cropped_images, total_points_per_crop\n+\n+\n+def _normalize_coordinates(\n+    target_size: int, coords: torch.Tensor, original_size: tuple[int, int], is_bounding_box=False\n+) -> torch.Tensor:\n+    \"\"\"\n+    Expects a numpy array of length 2 in the final dimension. Requires the original image size in (height, width)\n+    format.\n+    \"\"\"\n+    old_height, old_width = original_size\n+\n+    scale = target_size * 1.0 / max(old_height, old_width)\n+    new_height, new_width = old_height * scale, old_width * scale\n+    new_width = int(new_width + 0.5)\n+    new_height = int(new_height + 0.5)\n+\n+    coords = deepcopy(coords).float()\n+\n+    if is_bounding_box:\n+        coords = coords.reshape(-1, 2, 2)\n+\n+    coords[..., 0] = coords[..., 0] * (new_width / old_width)\n+    coords[..., 1] = coords[..., 1] * (new_height / old_height)\n+\n+    if is_bounding_box:\n+        coords = coords.reshape(-1, 4)\n+\n+    return coords\n+\n+\n+def _rle_to_mask(rle: dict[str, Any]) -> torch.Tensor:\n+    \"\"\"Compute a binary mask from an uncompressed RLE.\"\"\"\n+    height, width = rle[\"size\"]\n+    mask = torch.empty(height * width, dtype=bool)\n+    idx = 0\n+    parity = False\n+    for count in rle[\"counts\"]:\n+        mask[idx : idx + count] = parity\n+        idx += count\n+        parity = not parity\n+    mask = mask.reshape(width, height)\n+    return mask.transpose(0, 1)  # Reshape to original shape\n+\n+\n+def _post_process_for_mask_generation(rle_masks, iou_scores, mask_boxes, amg_crops_nms_thresh=0.7):\n+    \"\"\"\n+    Perform NMS (Non Maximum Suppression) on the outputs.\n+\n+    Args:\n+            rle_masks (`torch.Tensor`):\n+                binary masks in the RLE format\n+            iou_scores (`torch.Tensor` of shape (nb_masks, 1)):\n+                iou_scores predicted by the model\n+            mask_boxes (`torch.Tensor`):\n+                The bounding boxes corresponding to segmentation masks\n+            amg_crops_nms_thresh (`float`, *optional*, defaults to 0.7):\n+                NMS threshold.\n+    \"\"\"\n+    keep_by_nms = batched_nms(\n+        boxes=mask_boxes.float(),\n+        scores=iou_scores,\n+        idxs=torch.zeros(mask_boxes.shape[0]),\n+        iou_threshold=amg_crops_nms_thresh,\n+    )\n+\n+    iou_scores = iou_scores[keep_by_nms]\n+    rle_masks = [rle_masks[i] for i in keep_by_nms]\n+    mask_boxes = mask_boxes[keep_by_nms]\n+    masks = [_rle_to_mask(rle) for rle in rle_masks]\n+\n+    return masks, iou_scores, rle_masks, mask_boxes\n+\n+\n+def _scale_boxes(boxes, target_sizes):\n+    \"\"\"\n+    Scale batch of bounding boxes to the target sizes.\n+\n+    Args:\n+        boxes (`torch.Tensor` of shape `(batch_size, num_boxes, 4)`):\n+            Bounding boxes to scale. Each box is expected to be in (x1, y1, x2, y2) format.\n+        target_sizes (`list[tuple[int, int]]` or `torch.Tensor` of shape `(batch_size, 2)`):\n+            Target sizes to scale the boxes to. Each target size is expected to be in (height, width) format.\n+\n+    Returns:\n+        `torch.Tensor` of shape `(batch_size, num_boxes, 4)`: Scaled bounding boxes.\n+    \"\"\"\n+\n+    if isinstance(target_sizes, (list, tuple)):\n+        image_height = torch.tensor([i[0] for i in target_sizes])\n+        image_width = torch.tensor([i[1] for i in target_sizes])\n+    elif isinstance(target_sizes, torch.Tensor):\n+        image_height, image_width = target_sizes.unbind(1)\n+    else:\n+        raise TypeError(\"`target_sizes` must be a list, tuple or torch.Tensor\")\n+\n+    scale_factor = torch.stack([image_width, image_height, image_width, image_height], dim=1)\n+    scale_factor = scale_factor.unsqueeze(1).to(boxes.device)\n+    boxes = boxes * scale_factor\n+    return boxes\n+\n+\n+@auto_docstring\n+class Sam3ImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    size = {\"height\": 1008, \"width\": 1008}\n+    mask_size = {\"height\": 288, \"width\": 288}\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_convert_rgb = True\n+\n+    valid_kwargs = Sam3FastImageProcessorKwargs\n+\n+    # modular artefacts\n+    do_pad = None\n+    pad_size = None\n+    mask_pad_size = None\n+\n+    def __init__(self, **kwargs: Unpack[Sam3FastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    def _further_process_kwargs(\n+        self,\n+        size: Optional[SizeDict] = None,\n+        mask_size: Optional[SizeDict] = None,\n+        default_to_square: Optional[bool] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        data_format: Optional[ChannelDimension] = None,\n+        **kwargs,\n+    ) -> dict:\n+        \"\"\"\n+        Update kwargs that need further processing before being validated\n+        Can be overridden by subclasses to customize the processing of kwargs.\n+        \"\"\"\n+        if kwargs is None:\n+            kwargs = {}\n+        if size is not None:\n+            size = SizeDict(**get_size_dict(size=size, default_to_square=default_to_square))\n+        if mask_size is not None:\n+            mask_size = SizeDict(**get_size_dict(mask_size, param_name=\"mask_size\"))\n+        if isinstance(image_mean, list):\n+            image_mean = tuple(image_mean)\n+        if isinstance(image_std, list):\n+            image_std = tuple(image_std)\n+        if data_format is None:\n+            data_format = ChannelDimension.FIRST\n+\n+        kwargs[\"size\"] = size\n+        kwargs[\"mask_size\"] = mask_size\n+        kwargs[\"image_mean\"] = image_mean\n+        kwargs[\"image_std\"] = image_std\n+        kwargs[\"data_format\"] = data_format\n+\n+        # torch resize uses interpolation instead of resample\n+        # Check if resample is an int before checking if it's an instance of PILImageResampling\n+        # because if pillow < 9.1.0, resample is an int and PILImageResampling is a module.\n+        # Checking PILImageResampling will fail with error `TypeError: isinstance() arg 2 must be a type or tuple of types`.\n+        resample = kwargs.pop(\"resample\")\n+        kwargs[\"interpolation\"] = (\n+            pil_torch_interpolation_mapping[resample] if isinstance(resample, (PILImageResampling, int)) else resample\n+        )\n+\n+        return kwargs\n+\n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput] = None,\n+        **kwargs: Unpack[Sam3FastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The segmentation maps to preprocess.\n+        \"\"\"\n+        return super().preprocess(images, segmentation_maps, **kwargs)\n+\n+    def _preprocess_image_like_inputs(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput],\n+        do_convert_rgb: bool,\n+        input_data_format: ChannelDimension,\n+        device: Optional[Union[str, \"torch.device\"]] = None,\n+        **kwargs: Unpack[Sam3FastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess image-like inputs.\n+        \"\"\"\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        original_sizes = [image.shape[-2:] for image in images]\n+        images_kwargs = kwargs.copy()\n+        pixel_values = self._preprocess(images, **images_kwargs)\n+        data = {\n+            \"pixel_values\": pixel_values,\n+            \"original_sizes\": original_sizes,\n+        }\n+\n+        if segmentation_maps is not None:\n+            processed_segmentation_maps = self._prepare_image_like_inputs(\n+                images=segmentation_maps,\n+                expected_ndims=2,\n+                do_convert_rgb=False,\n+                input_data_format=ChannelDimension.FIRST,\n+            )\n+\n+            segmentation_maps_kwargs = kwargs.copy()\n+            segmentation_maps_kwargs.update(\n+                {\n+                    \"do_normalize\": False,\n+                    \"do_rescale\": False,\n+                    \"interpolation\": pil_torch_interpolation_mapping[PILImageResampling.NEAREST],\n+                    \"size\": segmentation_maps_kwargs.pop(\"mask_size\"),\n+                }\n+            )\n+            processed_segmentation_maps = self._preprocess(\n+                images=processed_segmentation_maps, **segmentation_maps_kwargs\n+            )\n+            data[\"labels\"] = processed_segmentation_maps.squeeze(1).to(torch.int64)\n+\n+        return BatchFeature(data=data, tensor_type=kwargs[\"return_tensors\"])\n+\n+    def generate_crop_boxes(\n+        self,\n+        image: \"torch.Tensor\",\n+        target_size,\n+        crop_n_layers: int = 0,\n+        overlap_ratio: float = 512 / 1500,\n+        points_per_crop: Optional[int] = 32,\n+        crop_n_points_downscale_factor: Optional[list[int]] = 1,\n+        device: Optional[\"torch.device\"] = None,\n+    ):\n+        \"\"\"\n+        Generates a list of crop boxes of different sizes. Each layer has (2**i)**2 boxes for the ith layer.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Input original image\n+            target_size (`int`):\n+                Target size of the resized image\n+            crop_n_layers (`int`, *optional*, defaults to 0):\n+                If >0, mask prediction will be run again on crops of the image. Sets the number of layers to run, where\n+                each layer has 2**i_layer number of image crops.\n+            overlap_ratio (`float`, *optional*, defaults to 512/1500):\n+                Sets the degree to which crops overlap. In the first crop layer, crops will overlap by this fraction of\n+                the image length. Later layers with more crops scale down this overlap.\n+            points_per_crop (`int`, *optional*, defaults to 32):\n+                Number of points to sam3ple from each crop.\n+            crop_n_points_downscale_factor (`list[int]`, *optional*, defaults to 1):\n+                The number of points-per-side sam3pled in layer n is scaled down by crop_n_points_downscale_factor**n.\n+            device (`torch.device`, *optional*, defaults to None):\n+                Device to use for the computation. If None, cpu will be used.\n+            input_data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format of the input image. If not provided, it will be inferred.\n+            return_tensors (`str`, *optional*, defaults to `pt`):\n+                If `pt`, returns `torch.Tensor`.\n+        \"\"\"\n+        image = self._process_image(image)\n+        crop_boxes, points_per_crop, cropped_images, input_labels = _generate_crop_boxes(\n+            image,\n+            target_size,\n+            crop_n_layers,\n+            overlap_ratio,\n+            points_per_crop,\n+            crop_n_points_downscale_factor,\n+        )\n+        if device is None:\n+            device = torch.device(\"cpu\")\n+        crop_boxes = crop_boxes.to(device)\n+        points_per_crop = points_per_crop.to(device)\n+        # cropped_images stays as torch.Tensor\n+        input_labels = input_labels.to(device)\n+\n+        return crop_boxes, points_per_crop, cropped_images, input_labels\n+\n+    def filter_masks(\n+        self,\n+        masks,\n+        iou_scores,\n+        original_size,\n+        cropped_box_image,\n+        pred_iou_thresh=0.88,\n+        stability_score_thresh=0.95,\n+        mask_threshold=0,\n+        stability_score_offset=1,\n+    ):\n+        \"\"\"\n+        Filters the predicted masks by selecting only the ones that meets several criteria. The first criterion being\n+        that the iou scores needs to be greater than `pred_iou_thresh`. The second criterion is that the stability\n+        score needs to be greater than `stability_score_thresh`. The method also converts the predicted masks to\n+        bounding boxes and pad the predicted masks if necessary.\n+\n+        Args:\n+            masks (`torch.Tensor`):\n+                Input masks.\n+            iou_scores (`torch.Tensor`):\n+                List of IoU scores.\n+            original_size (`tuple[int,int]`):\n+                Size of the original image.\n+            cropped_box_image (`torch.Tensor`):\n+                The cropped image.\n+            pred_iou_thresh (`float`, *optional*, defaults to 0.88):\n+                The threshold for the iou scores.\n+            stability_score_thresh (`float`, *optional*, defaults to 0.95):\n+                The threshold for the stability score.\n+            mask_threshold (`float`, *optional*, defaults to 0):\n+                The threshold for the predicted masks.\n+            stability_score_offset (`float`, *optional*, defaults to 1):\n+                The offset for the stability score used in the `_compute_stability_score` method.\n+\n+        \"\"\"\n+        original_height, original_width = original_size\n+        iou_scores = iou_scores.flatten(0, 1)\n+        masks = masks.flatten(0, 1)\n+\n+        if masks.shape[0] != iou_scores.shape[0]:\n+            raise ValueError(\"masks and iou_scores must have the sam3e batch size.\")\n+\n+        if masks.device != iou_scores.device:\n+            iou_scores = iou_scores.to(masks.device)\n+\n+        batch_size = masks.shape[0]\n+\n+        keep_mask = torch.ones(batch_size, dtype=torch.bool, device=masks.device)\n+\n+        if pred_iou_thresh > 0.0:\n+            keep_mask = keep_mask & (iou_scores > pred_iou_thresh)\n+\n+        # compute stability score\n+        if stability_score_thresh > 0.0:\n+            stability_scores = _compute_stability_score(masks, mask_threshold, stability_score_offset)\n+            keep_mask = keep_mask & (stability_scores > stability_score_thresh)\n+\n+        scores = iou_scores[keep_mask]\n+        masks = masks[keep_mask]\n+\n+        # binarize masks\n+        masks = masks > mask_threshold\n+        converted_boxes = _batched_mask_to_box(masks)\n+\n+        keep_mask = ~_is_box_near_crop_edge(\n+            converted_boxes, cropped_box_image, [0, 0, original_width, original_height]\n+        )\n+\n+        scores = scores[keep_mask]\n+        masks = masks[keep_mask]\n+        converted_boxes = converted_boxes[keep_mask]\n+\n+        masks = _pad_masks(masks, cropped_box_image, original_height, original_width)\n+        # conversion to rle is necessary to run non-maximum suppression\n+        masks = _mask_to_rle(masks)\n+\n+        return masks, scores, converted_boxes\n+\n+    def post_process_masks(\n+        self,\n+        masks,\n+        original_sizes,\n+        mask_threshold=0.0,\n+        binarize=True,\n+        max_hole_area=0.0,\n+        max_sprinkle_area=0.0,\n+        apply_non_overlapping_constraints=False,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Remove padding and upscale masks to the original image size.\n+\n+        Args:\n+            masks (`Union[torch.Tensor, List[torch.Tensor], np.ndarray, List[np.ndarray]]`):\n+                Batched masks from the mask_decoder in (batch_size, num_channels, height, width) format.\n+            original_sizes (`Union[torch.Tensor, List[Tuple[int,int]]]`):\n+                The original sizes of each image before it was resized to the model's expected input shape, in (height,\n+                width) format.\n+            mask_threshold (`float`, *optional*, defaults to 0.0):\n+                Threshold for binarization and post-processing operations.\n+            binarize (`bool`, *optional*, defaults to `True`):\n+                Whether to binarize the masks.\n+            max_hole_area (`float`, *optional*, defaults to 0.0):\n+                The maximum area of a hole to fill.\n+            max_sprinkle_area (`float`, *optional*, defaults to 0.0):\n+                The maximum area of a sprinkle to fill.\n+            apply_non_overlapping_constraints (`bool`, *optional*, defaults to `False`):\n+                Whether to apply non-overlapping constraints to the masks.\n+\n+        Returns:\n+            (`torch.Tensor`): Batched masks in batch_size, num_channels, height, width) format, where (height, width)\n+            is given by original_size.\n+        \"\"\"\n+        if isinstance(original_sizes, (torch.Tensor, np.ndarray)):\n+            original_sizes = original_sizes.tolist()\n+        # TODO: add connected components kernel for postprocessing\n+        output_masks = []\n+        for i, original_size in enumerate(original_sizes):\n+            if isinstance(masks[i], np.ndarray):\n+                masks[i] = torch.from_numpy(masks[i])\n+            elif not isinstance(masks[i], torch.Tensor):\n+                raise TypeError(\"Input masks should be a list of `torch.tensors` or a list of `np.ndarray`\")\n+            interpolated_mask = F.interpolate(masks[i], original_size, mode=\"bilinear\", align_corners=False)\n+            if apply_non_overlapping_constraints:\n+                interpolated_mask = self._apply_non_overlapping_constraints(interpolated_mask)\n+            if binarize:\n+                interpolated_mask = interpolated_mask > mask_threshold\n+            output_masks.append(interpolated_mask)\n+\n+        return output_masks\n+\n+    def post_process_for_mask_generation(self, all_masks, all_scores, all_boxes, crops_nms_thresh):\n+        \"\"\"\n+        Post processes mask that are generated by calling the Non Maximum Suppression algorithm on the predicted masks.\n+\n+        Args:\n+            all_masks (`torch.Tensor`):\n+                List of all predicted segmentation masks\n+            all_scores (`torch.Tensor`):\n+                List of all predicted iou scores\n+            all_boxes (`torch.Tensor`):\n+                List of all bounding boxes of the predicted masks\n+            crops_nms_thresh (`float`):\n+                Threshold for NMS (Non Maximum Suppression) algorithm.\n+        \"\"\"\n+        return _post_process_for_mask_generation(all_masks, all_scores, all_boxes, crops_nms_thresh)\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        return super()._preprocess(images, return_tensors=return_tensors, **kwargs).pixel_values\n+\n+    def _apply_non_overlapping_constraints(self, pred_masks: torch.Tensor) -> torch.Tensor:\n+        \"\"\"\n+        Apply non-overlapping constraints to the object scores in pred_masks. Here we\n+        keep only the highest scoring object at each spatial location in pred_masks.\n+        \"\"\"\n+        batch_size = pred_masks.size(0)\n+        if batch_size == 1:\n+            return pred_masks\n+\n+        device = pred_masks.device\n+        # \"max_obj_inds\": object index of the object with the highest score at each location\n+        max_obj_inds = torch.argmax(pred_masks, dim=0, keepdim=True)\n+        # \"batch_obj_inds\": object index of each object slice (along dim 0) in `pred_masks`\n+        batch_obj_inds = torch.arange(batch_size, device=device)[:, None, None, None]\n+        keep = max_obj_inds == batch_obj_inds\n+        # suppress overlapping regions' scores below -10.0 so that the foreground regions\n+        # don't overlap (here sigmoid(-10.0)=4.5398e-05)\n+        pred_masks = torch.where(keep, pred_masks, torch.clamp(pred_masks, max=-10.0))\n+        return pred_masks\n+\n+    def post_process_semantic_segmentation(\n+        self, outputs, target_sizes: Optional[list[tuple]] = None, threshold: float = 0.5\n+    ):\n+        \"\"\"\n+        Converts the output of [`Sam3Model`] into semantic segmentation maps.\n+\n+        Args:\n+            outputs ([`Sam3ImageSegmentationOutput`]):\n+                Raw outputs of the model containing semantic_seg.\n+            target_sizes (`list[tuple]` of length `batch_size`, *optional*):\n+                List of tuples corresponding to the requested final size (height, width) of each prediction. If unset,\n+                predictions will not be resized.\n+            threshold (`float`, *optional*, defaults to 0.5):\n+                Threshold for binarizing the semantic segmentation masks.\n+\n+        Returns:\n+            semantic_segmentation: `list[torch.Tensor]` of length `batch_size`, where each item is a semantic\n+            segmentation map of shape (height, width) corresponding to the target_sizes entry (if `target_sizes` is\n+            specified). Each entry is a binary mask (0 or 1).\n+        \"\"\"\n+        # Get semantic segmentation output\n+        # semantic_seg has shape (batch_size, 1, height, width)\n+        semantic_logits = outputs.semantic_seg\n+\n+        if semantic_logits is None:\n+            raise ValueError(\n+                \"Semantic segmentation output is not available in the model outputs. \"\n+                \"Make sure the model was run with semantic segmentation enabled.\"\n+            )\n+\n+        # Apply sigmoid to convert logits to probabilities\n+        semantic_probs = semantic_logits.sigmoid()\n+\n+        # Resize and binarize semantic segmentation maps\n+        if target_sizes is not None:\n+            if len(semantic_logits) != len(target_sizes):\n+                raise ValueError(\n+                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n+                )\n+\n+            semantic_segmentation = []\n+\n+            for idx in range(len(semantic_logits)):\n+                resized_probs = torch.nn.functional.interpolate(\n+                    semantic_probs[idx].unsqueeze(dim=0),\n+                    size=target_sizes[idx],\n+                    mode=\"bilinear\",\n+                    align_corners=False,\n+                )\n+                # Binarize: values > threshold become 1, otherwise 0\n+                semantic_map = (resized_probs[0, 0] > threshold).to(torch.long)\n+                semantic_segmentation.append(semantic_map)\n+        else:\n+            # Binarize without resizing\n+            semantic_segmentation = (semantic_probs[:, 0] > threshold).to(torch.long)\n+            semantic_segmentation = [semantic_segmentation[i] for i in range(semantic_segmentation.shape[0])]\n+\n+        return semantic_segmentation\n+\n+    def post_process_object_detection(\n+        self, outputs, threshold: float = 0.3, target_sizes: Optional[list[tuple]] = None\n+    ):\n+        \"\"\"\n+        Converts the raw output of [`Sam3Model`] into final bounding boxes in (top_left_x, top_left_y,\n+        bottom_right_x, bottom_right_y) format.\n+\n+        Args:\n+            outputs ([`Sam3ImageSegmentationOutput`]):\n+                Raw outputs of the model containing pred_boxes, pred_logits, and optionally presence_logits.\n+            threshold (`float`, *optional*, defaults to 0.3):\n+                Score threshold to keep object detection predictions.\n+            target_sizes (`list[tuple[int, int]]`, *optional*):\n+                List of tuples (`tuple[int, int]`) containing the target size `(height, width)` of each image in the\n+                batch. If unset, predictions will not be resized.\n+\n+        Returns:\n+            `list[dict]`: A list of dictionaries, each dictionary containing the following keys:\n+                - **scores** (`torch.Tensor`): The confidence scores for each predicted box on the image.\n+                - **boxes** (`torch.Tensor`): Image bounding boxes in (top_left_x, top_left_y, bottom_right_x,\n+                  bottom_right_y) format.\n+        \"\"\"\n+        pred_logits = outputs.pred_logits  # (batch_size, num_queries)\n+        pred_boxes = outputs.pred_boxes  # (batch_size, num_queries, 4) in xyxy format\n+        presence_logits = outputs.presence_logits  # (batch_size, 1) or None\n+\n+        batch_size = pred_logits.shape[0]\n+\n+        if target_sizes is not None and len(target_sizes) != batch_size:\n+            raise ValueError(\"Make sure that you pass in as many target sizes as images\")\n+\n+        # Compute scores: combine pred_logits with presence_logits if available\n+        batch_scores = pred_logits.sigmoid()\n+        if presence_logits is not None:\n+            presence_scores = presence_logits.sigmoid()  # (batch_size, 1)\n+            batch_scores = batch_scores * presence_scores  # Broadcast multiplication\n+\n+        # Boxes are already in xyxy format from the model\n+        batch_boxes = pred_boxes\n+\n+        # Convert from relative [0, 1] to absolute [0, height/width] coordinates\n+        if target_sizes is not None:\n+            batch_boxes = _scale_boxes(batch_boxes, target_sizes)\n+\n+        results = []\n+        for scores, boxes in zip(batch_scores, batch_boxes):\n+            keep = scores > threshold\n+            scores = scores[keep]\n+            boxes = boxes[keep]\n+            results.append({\"scores\": scores, \"boxes\": boxes})\n+\n+        return results\n+\n+    def post_process_instance_segmentation(\n+        self,\n+        outputs,\n+        threshold: float = 0.3,\n+        mask_threshold: float = 0.5,\n+        target_sizes: Optional[list[tuple]] = None,\n+    ):\n+        \"\"\"\n+        Converts the raw output of [`Sam3Model`] into instance segmentation predictions with bounding boxes and masks.\n+\n+        Args:\n+            outputs ([`Sam3ImageSegmentationOutput`]):\n+                Raw outputs of the model containing pred_boxes, pred_logits, pred_masks, and optionally\n+                presence_logits.\n+            threshold (`float`, *optional*, defaults to 0.3):\n+                Score threshold to keep instance predictions.\n+            mask_threshold (`float`, *optional*, defaults to 0.5):\n+                Threshold for binarizing the predicted masks.\n+            target_sizes (`list[tuple[int, int]]`, *optional*):\n+                List of tuples (`tuple[int, int]`) containing the target size `(height, width)` of each image in the\n+                batch. If unset, predictions will not be resized.\n+\n+        Returns:\n+            `list[dict]`: A list of dictionaries, each dictionary containing the following keys:\n+                - **scores** (`torch.Tensor`): The confidence scores for each predicted instance on the image.\n+                - **boxes** (`torch.Tensor`): Image bounding boxes in (top_left_x, top_left_y, bottom_right_x,\n+                  bottom_right_y) format.\n+                - **masks** (`torch.Tensor`): Binary segmentation masks for each instance, shape (num_instances,\n+                  height, width).\n+        \"\"\"\n+        pred_logits = outputs.pred_logits  # (batch_size, num_queries)\n+        pred_boxes = outputs.pred_boxes  # (batch_size, num_queries, 4) in xyxy format\n+        pred_masks = outputs.pred_masks  # (batch_size, num_queries, height, width)\n+        presence_logits = outputs.presence_logits  # (batch_size, 1) or None\n+\n+        batch_size = pred_logits.shape[0]\n+\n+        if target_sizes is not None and len(target_sizes) != batch_size:\n+            raise ValueError(\"Make sure that you pass in as many target sizes as images\")\n+\n+        # Compute scores: combine pred_logits with presence_logits if available\n+        batch_scores = pred_logits.sigmoid()\n+        if presence_logits is not None:\n+            presence_scores = presence_logits.sigmoid()  # (batch_size, 1)\n+            batch_scores = batch_scores * presence_scores  # Broadcast multiplication\n+\n+        # Apply sigmoid to mask logits\n+        batch_masks = pred_masks.sigmoid()\n+\n+        # Boxes are already in xyxy format from the model\n+        batch_boxes = pred_boxes\n+\n+        # Scale boxes to target sizes if provided\n+        if target_sizes is not None:\n+            batch_boxes = _scale_boxes(batch_boxes, target_sizes)\n+\n+        results = []\n+        for idx, (scores, boxes, masks) in enumerate(zip(batch_scores, batch_boxes, batch_masks)):\n+            # Filter by score threshold\n+            keep = scores > threshold\n+            scores = scores[keep]\n+            boxes = boxes[keep]\n+            masks = masks[keep]  # (num_keep, height, width)\n+\n+            # Resize masks to target size if provided\n+            if target_sizes is not None:\n+                target_size = target_sizes[idx]\n+                if len(masks) > 0:\n+                    masks = torch.nn.functional.interpolate(\n+                        masks.unsqueeze(0),  # (1, num_keep, height, width)\n+                        size=target_size,\n+                        mode=\"bilinear\",\n+                        align_corners=False,\n+                    ).squeeze(0)  # (num_keep, target_height, target_width)\n+\n+            # Binarize masks\n+            masks = (masks > mask_threshold).to(torch.long)\n+\n+            results.append({\"scores\": scores, \"boxes\": boxes, \"masks\": masks})\n+\n+        return results\n+\n+\n+__all__ = [\"Sam3ImageProcessorFast\"]"
        },
        {
            "sha": "df3406859698a3ebe798687eaac539b1bc3b0780",
            "filename": "src/transformers/models/sam3/modeling_sam3.py",
            "status": "added",
            "additions": 2396,
            "deletions": 0,
            "changes": 2396,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3%2Fmodeling_sam3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3%2Fmodeling_sam3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3%2Fmodeling_sam3.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a"
        },
        {
            "sha": "76131044296de8e07978585a94d30d87df426e23",
            "filename": "src/transformers/models/sam3/modular_sam3.py",
            "status": "added",
            "additions": 257,
            "deletions": 0,
            "changes": 257,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3%2Fmodular_sam3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3%2Fmodular_sam3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3%2Fmodular_sam3.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,257 @@\n+# coding=utf-8\n+# Copyright 2025 The Meta AI Authors and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from typing import Optional\n+\n+import torch\n+\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+)\n+from ..sam2.image_processing_sam2_fast import Sam2ImageProcessorFast\n+\n+\n+def _scale_boxes(boxes, target_sizes):\n+    \"\"\"\n+    Scale batch of bounding boxes to the target sizes.\n+\n+    Args:\n+        boxes (`torch.Tensor` of shape `(batch_size, num_boxes, 4)`):\n+            Bounding boxes to scale. Each box is expected to be in (x1, y1, x2, y2) format.\n+        target_sizes (`list[tuple[int, int]]` or `torch.Tensor` of shape `(batch_size, 2)`):\n+            Target sizes to scale the boxes to. Each target size is expected to be in (height, width) format.\n+\n+    Returns:\n+        `torch.Tensor` of shape `(batch_size, num_boxes, 4)`: Scaled bounding boxes.\n+    \"\"\"\n+\n+    if isinstance(target_sizes, (list, tuple)):\n+        image_height = torch.tensor([i[0] for i in target_sizes])\n+        image_width = torch.tensor([i[1] for i in target_sizes])\n+    elif isinstance(target_sizes, torch.Tensor):\n+        image_height, image_width = target_sizes.unbind(1)\n+    else:\n+        raise TypeError(\"`target_sizes` must be a list, tuple or torch.Tensor\")\n+\n+    scale_factor = torch.stack([image_width, image_height, image_width, image_height], dim=1)\n+    scale_factor = scale_factor.unsqueeze(1).to(boxes.device)\n+    boxes = boxes * scale_factor\n+    return boxes\n+\n+\n+class Sam3ImageProcessorFast(Sam2ImageProcessorFast):\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    size = {\"height\": 1008, \"width\": 1008}\n+    mask_size = {\"height\": 288, \"width\": 288}\n+\n+    def post_process_semantic_segmentation(\n+        self, outputs, target_sizes: Optional[list[tuple]] = None, threshold: float = 0.5\n+    ):\n+        \"\"\"\n+        Converts the output of [`Sam3Model`] into semantic segmentation maps.\n+\n+        Args:\n+            outputs ([`Sam3ImageSegmentationOutput`]):\n+                Raw outputs of the model containing semantic_seg.\n+            target_sizes (`list[tuple]` of length `batch_size`, *optional*):\n+                List of tuples corresponding to the requested final size (height, width) of each prediction. If unset,\n+                predictions will not be resized.\n+            threshold (`float`, *optional*, defaults to 0.5):\n+                Threshold for binarizing the semantic segmentation masks.\n+\n+        Returns:\n+            semantic_segmentation: `list[torch.Tensor]` of length `batch_size`, where each item is a semantic\n+            segmentation map of shape (height, width) corresponding to the target_sizes entry (if `target_sizes` is\n+            specified). Each entry is a binary mask (0 or 1).\n+        \"\"\"\n+        # Get semantic segmentation output\n+        # semantic_seg has shape (batch_size, 1, height, width)\n+        semantic_logits = outputs.semantic_seg\n+\n+        if semantic_logits is None:\n+            raise ValueError(\n+                \"Semantic segmentation output is not available in the model outputs. \"\n+                \"Make sure the model was run with semantic segmentation enabled.\"\n+            )\n+\n+        # Apply sigmoid to convert logits to probabilities\n+        semantic_probs = semantic_logits.sigmoid()\n+\n+        # Resize and binarize semantic segmentation maps\n+        if target_sizes is not None:\n+            if len(semantic_logits) != len(target_sizes):\n+                raise ValueError(\n+                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n+                )\n+\n+            semantic_segmentation = []\n+\n+            for idx in range(len(semantic_logits)):\n+                resized_probs = torch.nn.functional.interpolate(\n+                    semantic_probs[idx].unsqueeze(dim=0),\n+                    size=target_sizes[idx],\n+                    mode=\"bilinear\",\n+                    align_corners=False,\n+                )\n+                # Binarize: values > threshold become 1, otherwise 0\n+                semantic_map = (resized_probs[0, 0] > threshold).to(torch.long)\n+                semantic_segmentation.append(semantic_map)\n+        else:\n+            # Binarize without resizing\n+            semantic_segmentation = (semantic_probs[:, 0] > threshold).to(torch.long)\n+            semantic_segmentation = [semantic_segmentation[i] for i in range(semantic_segmentation.shape[0])]\n+\n+        return semantic_segmentation\n+\n+    def post_process_object_detection(\n+        self, outputs, threshold: float = 0.3, target_sizes: Optional[list[tuple]] = None\n+    ):\n+        \"\"\"\n+        Converts the raw output of [`Sam3Model`] into final bounding boxes in (top_left_x, top_left_y,\n+        bottom_right_x, bottom_right_y) format.\n+\n+        Args:\n+            outputs ([`Sam3ImageSegmentationOutput`]):\n+                Raw outputs of the model containing pred_boxes, pred_logits, and optionally presence_logits.\n+            threshold (`float`, *optional*, defaults to 0.3):\n+                Score threshold to keep object detection predictions.\n+            target_sizes (`list[tuple[int, int]]`, *optional*):\n+                List of tuples (`tuple[int, int]`) containing the target size `(height, width)` of each image in the\n+                batch. If unset, predictions will not be resized.\n+\n+        Returns:\n+            `list[dict]`: A list of dictionaries, each dictionary containing the following keys:\n+                - **scores** (`torch.Tensor`): The confidence scores for each predicted box on the image.\n+                - **boxes** (`torch.Tensor`): Image bounding boxes in (top_left_x, top_left_y, bottom_right_x,\n+                  bottom_right_y) format.\n+        \"\"\"\n+        pred_logits = outputs.pred_logits  # (batch_size, num_queries)\n+        pred_boxes = outputs.pred_boxes  # (batch_size, num_queries, 4) in xyxy format\n+        presence_logits = outputs.presence_logits  # (batch_size, 1) or None\n+\n+        batch_size = pred_logits.shape[0]\n+\n+        if target_sizes is not None and len(target_sizes) != batch_size:\n+            raise ValueError(\"Make sure that you pass in as many target sizes as images\")\n+\n+        # Compute scores: combine pred_logits with presence_logits if available\n+        batch_scores = pred_logits.sigmoid()\n+        if presence_logits is not None:\n+            presence_scores = presence_logits.sigmoid()  # (batch_size, 1)\n+            batch_scores = batch_scores * presence_scores  # Broadcast multiplication\n+\n+        # Boxes are already in xyxy format from the model\n+        batch_boxes = pred_boxes\n+\n+        # Convert from relative [0, 1] to absolute [0, height/width] coordinates\n+        if target_sizes is not None:\n+            batch_boxes = _scale_boxes(batch_boxes, target_sizes)\n+\n+        results = []\n+        for scores, boxes in zip(batch_scores, batch_boxes):\n+            keep = scores > threshold\n+            scores = scores[keep]\n+            boxes = boxes[keep]\n+            results.append({\"scores\": scores, \"boxes\": boxes})\n+\n+        return results\n+\n+    def post_process_instance_segmentation(\n+        self,\n+        outputs,\n+        threshold: float = 0.3,\n+        mask_threshold: float = 0.5,\n+        target_sizes: Optional[list[tuple]] = None,\n+    ):\n+        \"\"\"\n+        Converts the raw output of [`Sam3Model`] into instance segmentation predictions with bounding boxes and masks.\n+\n+        Args:\n+            outputs ([`Sam3ImageSegmentationOutput`]):\n+                Raw outputs of the model containing pred_boxes, pred_logits, pred_masks, and optionally\n+                presence_logits.\n+            threshold (`float`, *optional*, defaults to 0.3):\n+                Score threshold to keep instance predictions.\n+            mask_threshold (`float`, *optional*, defaults to 0.5):\n+                Threshold for binarizing the predicted masks.\n+            target_sizes (`list[tuple[int, int]]`, *optional*):\n+                List of tuples (`tuple[int, int]`) containing the target size `(height, width)` of each image in the\n+                batch. If unset, predictions will not be resized.\n+\n+        Returns:\n+            `list[dict]`: A list of dictionaries, each dictionary containing the following keys:\n+                - **scores** (`torch.Tensor`): The confidence scores for each predicted instance on the image.\n+                - **boxes** (`torch.Tensor`): Image bounding boxes in (top_left_x, top_left_y, bottom_right_x,\n+                  bottom_right_y) format.\n+                - **masks** (`torch.Tensor`): Binary segmentation masks for each instance, shape (num_instances,\n+                  height, width).\n+        \"\"\"\n+        pred_logits = outputs.pred_logits  # (batch_size, num_queries)\n+        pred_boxes = outputs.pred_boxes  # (batch_size, num_queries, 4) in xyxy format\n+        pred_masks = outputs.pred_masks  # (batch_size, num_queries, height, width)\n+        presence_logits = outputs.presence_logits  # (batch_size, 1) or None\n+\n+        batch_size = pred_logits.shape[0]\n+\n+        if target_sizes is not None and len(target_sizes) != batch_size:\n+            raise ValueError(\"Make sure that you pass in as many target sizes as images\")\n+\n+        # Compute scores: combine pred_logits with presence_logits if available\n+        batch_scores = pred_logits.sigmoid()\n+        if presence_logits is not None:\n+            presence_scores = presence_logits.sigmoid()  # (batch_size, 1)\n+            batch_scores = batch_scores * presence_scores  # Broadcast multiplication\n+\n+        # Apply sigmoid to mask logits\n+        batch_masks = pred_masks.sigmoid()\n+\n+        # Boxes are already in xyxy format from the model\n+        batch_boxes = pred_boxes\n+\n+        # Scale boxes to target sizes if provided\n+        if target_sizes is not None:\n+            batch_boxes = _scale_boxes(batch_boxes, target_sizes)\n+\n+        results = []\n+        for idx, (scores, boxes, masks) in enumerate(zip(batch_scores, batch_boxes, batch_masks)):\n+            # Filter by score threshold\n+            keep = scores > threshold\n+            scores = scores[keep]\n+            boxes = boxes[keep]\n+            masks = masks[keep]  # (num_keep, height, width)\n+\n+            # Resize masks to target size if provided\n+            if target_sizes is not None:\n+                target_size = target_sizes[idx]\n+                if len(masks) > 0:\n+                    masks = torch.nn.functional.interpolate(\n+                        masks.unsqueeze(0),  # (1, num_keep, height, width)\n+                        size=target_size,\n+                        mode=\"bilinear\",\n+                        align_corners=False,\n+                    ).squeeze(0)  # (num_keep, target_height, target_width)\n+\n+            # Binarize masks\n+            masks = (masks > mask_threshold).to(torch.long)\n+\n+            results.append({\"scores\": scores, \"boxes\": boxes, \"masks\": masks})\n+\n+        return results\n+\n+\n+__all__ = [\"Sam3ImageProcessorFast\"]"
        },
        {
            "sha": "ac236bfc2045e79e3f4d336675d426fe6de15740",
            "filename": "src/transformers/models/sam3/processing_sam3.py",
            "status": "added",
            "additions": 674,
            "deletions": 0,
            "changes": 674,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3%2Fprocessing_sam3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3%2Fprocessing_sam3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3%2Fprocessing_sam3.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,674 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"\n+Processor class for SAM3.\n+\"\"\"\n+\n+from copy import deepcopy\n+from typing import Optional, Union\n+\n+import numpy as np\n+\n+from ...image_utils import ImageInput\n+from ...processing_utils import ProcessorMixin\n+from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n+from ...utils import TensorType, is_torch_available, logging\n+from ...utils.import_utils import requires\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+def box_cxcywh_to_xyxy(x):\n+    x_c, y_c, w, h = x.unbind(-1)\n+    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n+    return torch.stack(b, dim=-1)\n+\n+\n+def box_cxcywh_to_xywh(x):\n+    x_c, y_c, w, h = x.unbind(-1)\n+    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (w), (h)]\n+    return torch.stack(b, dim=-1)\n+\n+\n+def box_xywh_to_xyxy(x):\n+    x, y, w, h = x.unbind(-1)\n+    b = [(x), (y), (x + w), (y + h)]\n+    return torch.stack(b, dim=-1)\n+\n+\n+def box_xywh_to_cxcywh(x):\n+    x, y, w, h = x.unbind(-1)\n+    b = [(x + 0.5 * w), (y + 0.5 * h), (w), (h)]\n+    return torch.stack(b, dim=-1)\n+\n+\n+def box_xyxy_to_xywh(x):\n+    x, y, X, Y = x.unbind(-1)\n+    b = [(x), (y), (X - x), (Y - y)]\n+    return torch.stack(b, dim=-1)\n+\n+\n+def box_xyxy_to_cxcywh(x):\n+    x0, y0, x1, y1 = x.unbind(-1)\n+    b = [(x0 + x1) / 2, (y0 + y1) / 2, (x1 - x0), (y1 - y0)]\n+    return torch.stack(b, dim=-1)\n+\n+\n+def box_area(boxes):\n+    \"\"\"\n+    Batched version of box area. Boxes should be in [x0, y0, x1, y1] format.\n+\n+    Inputs:\n+    - boxes: Tensor of shape (..., 4)\n+\n+    Returns:\n+    - areas: Tensor of shape (...,)\n+    \"\"\"\n+    x0, y0, x1, y1 = boxes.unbind(-1)\n+    return (x1 - x0) * (y1 - y0)\n+\n+\n+@requires(backends=(\"torch\",))\n+class Sam3Processor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a SAM3 processor which wraps a SAM3 image processor and bounding boxes processing into a\n+    single processor.\n+\n+    [`Sam2Processor`] offers all the functionalities of [`Sam2ImageProcessorFast`] and [`Sam2VideoProcessor`]. See the docstring of\n+    [`~Sam2ImageProcessorFast.__call__`] and [`~Sam2VideoProcessor.__call__`] for more information.\n+\n+    Args:\n+        image_processor (`Sam2ImageProcessorFast`):\n+            An instance of [`Sam2ImageProcessorFast`].\n+        tokenizer ([`PreTrainedTokenizer`, `PreTrainedTokenizerFast`]):\n+            An instance of [`PreTrainedTokenizer`, `PreTrainedTokenizerFast`]. The tokenizer is a required input.\n+        target_size (`int`, *optional*):\n+            The target size (target_size, target_size) to which the image will be resized.\n+        point_pad_value (`int`, *optional*, defaults to -10):\n+            The value used for padding input boxes.\n+    \"\"\"\n+\n+    def __init__(\n+        self, image_processor, tokenizer, target_size: Optional[int] = None, point_pad_value: int = -10, **kwargs\n+    ):\n+        super().__init__(image_processor, tokenizer, **kwargs)\n+        self.point_pad_value = point_pad_value\n+        self.target_size = target_size if target_size is not None else self.image_processor.size[\"height\"]\n+\n+    def __call__(\n+        self,\n+        images: Optional[ImageInput] = None,\n+        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n+        segmentation_maps: Optional[ImageInput] = None,\n+        input_boxes: Optional[Union[list[list[list[float]]], torch.Tensor]] = None,\n+        input_boxes_labels: Optional[Union[list[list[list[int]]], torch.Tensor]] = None,\n+        original_sizes: Optional[Union[list[list[float]], torch.Tensor]] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        **kwargs,\n+    ) -> BatchEncoding:\n+        r\"\"\"\n+        This method uses [`Sam3ImageProcessorFast.__call__`] method to prepare image(s) for the model. It also prepares bounding boxes for the model if they are provided.\n+\n+        Args:\n+            images (`ImageInput`, *optional*):\n+                The image(s) to process.\n+            text (`str`, `list[str]`, `list[list[str]]`, *optional*):\n+                The text to process.\n+            segmentation_maps (`ImageInput`, *optional*):\n+                The segmentation maps to process.\n+            input_boxes (`list[list[list[float]]]`, `torch.Tensor`, *optional*):\n+                The bounding boxes to process.\n+            input_boxes_labels (`list[list[int]]`, `torch.Tensor`, *optional*):\n+                The labels for the bounding boxes.\n+            original_sizes (`list[list[float]]`, `torch.Tensor`, *optional*):\n+                The original sizes of the images.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return.\n+            **kwargs:\n+                Additional keyword arguments to pass to the image processor.\n+\n+        Returns:\n+            A [`BatchEncoding`] with the following fields:\n+            - `pixel_values` (`torch.Tensor`): The processed image(s).\n+            - `original_sizes` (`list[list[float]]`): The original sizes of the images.\n+            - `labels` (`torch.Tensor`): The processed segmentation maps (if provided).\n+            - `input_boxes_labels` (`torch.Tensor`): The processed labels for the bounding boxes.\n+            - `input_boxes` (`torch.Tensor`): The processed bounding boxes.\n+        \"\"\"\n+        if images is not None:\n+            encoding_image_processor = self.image_processor(\n+                images,\n+                segmentation_maps=segmentation_maps,\n+                return_tensors=return_tensors,\n+                **kwargs,\n+            )\n+        elif original_sizes is not None:\n+            if isinstance(original_sizes, torch.Tensor):\n+                original_sizes = original_sizes.cpu().tolist()\n+            encoding_image_processor = BatchEncoding({\"original_sizes\": original_sizes}, tensor_type=return_tensors)\n+        else:\n+            raise ValueError(\"Either images or original_sizes must be provided\")\n+\n+        original_sizes = encoding_image_processor[\"original_sizes\"]\n+        # Check original_sizes is of length 1 or len(images)\n+        if images is not None and len(original_sizes) != 1 and len(original_sizes) != len(images):\n+            raise ValueError(\n+                \"original_sizes must be of length 1 or len(images). If you are passing a single image, you must pass a single original_size.\"\n+            )\n+        text = self._resolve_text_prompts(text, input_boxes)\n+\n+        encoding_image_processor.update(\n+            self.tokenizer(text, return_tensors=return_tensors, padding=\"max_length\", max_length=32)\n+        )\n+\n+        # Process input boxes if provided\n+        if input_boxes is not None:\n+            # Validate and convert inputs to standardized format\n+            processed_boxes = self._validate_single_input(\n+                input_boxes,\n+                expected_depth=3,\n+                input_name=\"boxes\",\n+                expected_format=\"[image level, box level, box coordinates]\",\n+                expected_coord_size=4,\n+            )\n+            processed_boxes_labels = self._validate_single_input(\n+                input_boxes_labels,\n+                expected_depth=2,\n+                input_name=\"labels\",\n+                expected_format=\"[image level, box level]\",\n+            )\n+\n+            # Get padding requirements for all inputs\n+            if processed_boxes is not None:\n+                boxes_max_dims = self._get_nested_dimensions(processed_boxes)[:2]\n+            if processed_boxes_labels is not None:\n+                boxes_labels_max_dims = self._get_nested_dimensions(processed_boxes_labels)[:2]\n+\n+            # Ensure boxes and labels have consistent dimensions\n+            if processed_boxes is not None and processed_boxes_labels is not None:\n+                if boxes_max_dims != boxes_labels_max_dims:\n+                    raise ValueError(\n+                        \"Input boxes and labels have inconsistent dimensions. Please ensure they have the same dimensions.\"\n+                    )\n+\n+            # Pad and normalize all inputs to final tensor format\n+            if processed_boxes is not None:\n+                padded_boxes = self._pad_nested_list(processed_boxes, boxes_max_dims + [4])\n+                final_boxes = torch.tensor(padded_boxes, dtype=torch.float32)\n+                self._normalize_tensor_coordinates(\n+                    final_boxes, original_sizes, is_bounding_box=True, preserve_padding=True\n+                )\n+                final_boxes = box_xyxy_to_cxcywh(final_boxes)\n+                encoding_image_processor.update({\"input_boxes\": final_boxes})\n+\n+            if processed_boxes_labels is not None:\n+                padded_boxes_labels = self._pad_nested_list(processed_boxes_labels, boxes_labels_max_dims)\n+                final_boxes_labels = torch.tensor(padded_boxes_labels, dtype=torch.int64)\n+                encoding_image_processor.update({\"input_boxes_labels\": final_boxes_labels})\n+\n+        return encoding_image_processor\n+\n+    def _normalize_coordinates(self, coords: \"torch.Tensor\", original_size, is_bounding_box=False) -> \"torch.Tensor\":\n+        \"\"\"\n+        Expects a numpy array of length 2 in the final dimension. Requires the original image size in (H, W) format.\n+\n+        Args:\n+            target_size (`int`):\n+                The target size of the image.\n+            coords (`torch.Tensor`):\n+                The coordinates to be normalized.\n+            original_size (`tuple`):\n+                The original size of the image.\n+            is_bounding_box (`bool`, *optional*, defaults to `False`):\n+                Whether the coordinates are bounding boxes.\n+        \"\"\"\n+        old_h, old_w = original_size\n+        coords = deepcopy(coords).float()\n+\n+        if is_bounding_box:\n+            coords = coords.reshape(-1, 2, 2)\n+        coords[..., 0] = coords[..., 0] / old_w\n+        coords[..., 1] = coords[..., 1] / old_h\n+\n+        if is_bounding_box:\n+            coords = coords.reshape(-1, 4)\n+\n+        return coords\n+\n+    def _convert_to_nested_list(self, data, expected_depth, current_depth=0):\n+        \"\"\"\n+        Recursively convert various input formats (tensors, numpy arrays, lists) to nested lists.\n+        Preserves None values within lists.\n+\n+        Args:\n+            data: Input data in any format (may be None or contain None values)\n+            expected_depth: Expected nesting depth\n+            current_depth: Current depth in recursion\n+\n+        Returns:\n+            Nested list representation of the data (or None)\n+        \"\"\"\n+        if data is None:\n+            return None\n+\n+        # Convert tensor/numpy to list if we're at a leaf level or if it's a multi-dimensional array\n+        if isinstance(data, torch.Tensor):  # PyTorch tensor\n+            if current_depth == expected_depth - 2 or len(data.shape) <= 2:  # At coordinate level or small tensor\n+                return data.numpy().tolist()\n+            else:\n+                return [self._convert_to_nested_list(item, expected_depth, current_depth + 1) for item in data]\n+        elif isinstance(data, np.ndarray):  # NumPy array\n+            if current_depth == expected_depth - 2 or len(data.shape) <= 2:  # At coordinate level or small array\n+                return data.tolist()\n+            else:\n+                return [self._convert_to_nested_list(item, expected_depth, current_depth + 1) for item in data]\n+        elif isinstance(data, list):\n+            if current_depth == expected_depth:\n+                # We've reached the expected depth, return as is\n+                return data\n+            else:\n+                # Continue recursion, preserving None values\n+                return [\n+                    self._convert_to_nested_list(item, expected_depth, current_depth + 1) if item is not None else None\n+                    for item in data\n+                ]\n+        elif isinstance(data, (int, float)):\n+            return data\n+        else:\n+            raise ValueError(f\"Unsupported data type: {type(data)}\")\n+\n+    def _resolve_text_prompts(self, text, input_boxes):\n+        \"\"\"\n+        Resolve text prompts by setting defaults based on prompt types.\n+        \"\"\"\n+        # If no text provided, infer default based on prompt type\n+        if text is None:\n+            return \"visual\" if input_boxes else None\n+\n+        if not isinstance(text, (list, tuple)):\n+            return text\n+\n+        # Validate list/tuple length matches both prompt types if provided\n+        text = list(text)  # Convert to list to allow modification\n+\n+        if input_boxes and len(text) != len(input_boxes):\n+            raise ValueError(\n+                f\"The number of text prompts must match the number of input boxes. \"\n+                f\"Got {len(text)} text prompts and {len(input_boxes)} input boxes.\"\n+            )\n+\n+        # Fill in None values with defaults based on corresponding prompt\n+        for i, text_value in enumerate(text):\n+            if text_value is None and input_boxes and input_boxes[i] is not None:\n+                text[i] = \"visual\"\n+\n+        return text\n+\n+    def _get_nested_dimensions(self, nested_list, max_dims=None):\n+        \"\"\"\n+        Get the maximum dimensions at each level of nesting, skipping None values.\n+\n+        Args:\n+            nested_list (`list`):\n+                Nested list structure (may contain None values).\n+            max_dims (`list`, *optional*):\n+                Current maximum dimensions (for recursion).\n+\n+        Returns:\n+            `list`: A list of maximum dimensions for each nesting level.\n+        \"\"\"\n+        if max_dims is None:\n+            max_dims = []\n+\n+        if not isinstance(nested_list, list):\n+            return max_dims\n+\n+        if len(max_dims) == 0:\n+            max_dims.append(len(nested_list))\n+        else:\n+            max_dims[0] = max(max_dims[0], len(nested_list))\n+\n+        if len(nested_list) > 0:\n+            for item in nested_list:\n+                # Skip None values\n+                if item is None:\n+                    continue\n+                if isinstance(item, list):\n+                    sub_dims = self._get_nested_dimensions(item)\n+                    # Merge sub_dims into max_dims\n+                    for i, dim in enumerate(sub_dims):\n+                        if i + 1 >= len(max_dims):\n+                            max_dims.append(dim)\n+                        else:\n+                            max_dims[i + 1] = max(max_dims[i + 1], dim)\n+\n+        return max_dims\n+\n+    def _pad_nested_list(self, nested_list, target_dims, current_level=0, pad_value=None):\n+        \"\"\"\n+        Recursively pad a nested list to match target dimensions. Replaces None values with padded structures.\n+\n+        Args:\n+            nested_list (`list`):\n+                Nested list to pad (may contain None values).\n+            target_dims (`list`):\n+                Target dimensions for each level.\n+            current_level (`int`, *optional*, defaults to 0):\n+                Current nesting level.\n+            pad_value (`int`, *optional*):\n+                Value to use for padding.\n+\n+        Returns:\n+            `list`: The padded nested list.\n+        \"\"\"\n+        if pad_value is None:\n+            pad_value = self.point_pad_value\n+\n+        if current_level >= len(target_dims):\n+            return nested_list\n+\n+        # Ensure we have a list\n+        if not isinstance(nested_list, list):\n+            nested_list = [nested_list]\n+\n+        # Pad current level\n+        current_size = len(nested_list)\n+        target_size = target_dims[current_level]\n+\n+        # Pad with appropriate values\n+        if current_level == len(target_dims) - 1:\n+            # At the coordinate level, pad with pad_value\n+            nested_list.extend([pad_value] * (target_size - current_size))\n+        else:\n+            # At higher levels, pad with nested structures\n+            if current_size > 0:\n+                # Create appropriately sized template\n+                if current_level < len(target_dims) - 2:\n+                    # For non-coordinate levels, create empty nested structure\n+                    template_dims = target_dims[current_level + 1 :]\n+                    template = self._create_empty_nested_structure(template_dims, pad_value)\n+                else:\n+                    # For coordinate level, create list of pad_values\n+                    template = [pad_value] * target_dims[current_level + 1]\n+\n+                nested_list.extend([deepcopy(template) for _ in range(target_size - current_size)])\n+            else:\n+                # Create from scratch\n+                template_dims = target_dims[current_level + 1 :]\n+                template = self._create_empty_nested_structure(template_dims, pad_value)\n+                nested_list.extend([deepcopy(template) for _ in range(target_size)])\n+\n+        # Recursively pad sublists, replacing None with padded structures\n+        if current_level < len(target_dims) - 1:\n+            for i in range(len(nested_list)):\n+                if nested_list[i] is None:\n+                    # Replace None with fully padded structure\n+                    template_dims = target_dims[current_level + 1 :]\n+                    nested_list[i] = self._create_empty_nested_structure(template_dims, pad_value)\n+                elif isinstance(nested_list[i], list):\n+                    nested_list[i] = self._pad_nested_list(nested_list[i], target_dims, current_level + 1, pad_value)\n+\n+        return nested_list\n+\n+    def _create_empty_nested_structure(self, dims, pad_value):\n+        \"\"\"\n+        Create an empty nested structure with given dimensions filled with pad_value.\n+\n+        Args:\n+            dims (`list`):\n+                The dimensions of the nested structure.\n+            pad_value (`int`):\n+                The value to fill the structure with.\n+        \"\"\"\n+        if len(dims) == 1:\n+            return [pad_value] * dims[0]\n+        else:\n+            return [self._create_empty_nested_structure(dims[1:], pad_value) for _ in range(dims[0])]\n+\n+    def _get_nesting_level(self, input_list):\n+        \"\"\"\n+        Get the nesting level of a list structure, skipping None values.\n+\n+        Args:\n+            input_list (`list`):\n+                The list to get the nesting level of.\n+        \"\"\"\n+        if isinstance(input_list, list):\n+            if len(input_list) == 0:\n+                return 1\n+            # Find first non-None element to determine nesting level\n+            for item in input_list:\n+                if item is not None:\n+                    return 1 + self._get_nesting_level(item)\n+            # All elements are None, treat as single level\n+            return 1\n+        elif isinstance(input_list, (np.ndarray, torch.Tensor)):\n+            # For arrays/tensors, the nesting level is the number of dimensions\n+            return len(input_list.shape)\n+        return 0\n+\n+    def _validate_single_input(\n+        self,\n+        data: Union[torch.Tensor, np.ndarray, list],\n+        expected_depth: int,\n+        input_name: str,\n+        expected_format: str,\n+        expected_coord_size: Optional[int] = None,\n+    ) -> list:\n+        \"\"\"\n+                Validate a single input by ensuring proper nesting and raising an error if the input is not valid.\n+\n+                Args:\n+                    data (`torch.Tensor`, `np.ndarray`, or `list`):\n+                        Input data to process.\n+                    expected_depth (`int`):\n+                        Expected nesting depth.\n+                    input_name (`str`):\n+                        Name of the input for error messages.\n+                    expected_format (`str`):\n+                        The expected format of the input.\n+                    expected_coord_size (`int`, *optional*):\n+                        Expected coordinate size (4 for boxes, None for labels).\n+        .\n+        \"\"\"\n+        if data is None:\n+            return None\n+\n+        # Handle tensors and numpy arrays first\n+        if isinstance(data, (torch.Tensor, np.ndarray)):\n+            # For tensors/arrays, we can directly check the number of dimensions\n+            if data.ndim != expected_depth:\n+                raise ValueError(\n+                    f\"Input {input_name} must be a tensor/array with {expected_depth} dimensions. The expected nesting format is {expected_format}. Got {data.ndim} dimensions.\"\n+                )\n+            elif expected_coord_size is not None:\n+                if data.shape[-1] != expected_coord_size:\n+                    raise ValueError(\n+                        f\"Input {input_name} must be a tensor/array with {expected_coord_size} as the last dimension, got {data.shape[-1]}.\"\n+                    )\n+            return self._convert_to_nested_list(data, expected_depth)\n+\n+        # Handle nested lists\n+        if isinstance(data, list):\n+            current_depth = self._get_nesting_level(data)\n+            if current_depth != expected_depth:\n+                raise ValueError(\n+                    f\"Input {input_name} must be a nested list with {expected_depth} levels. The expected nesting format is {expected_format}. Got {current_depth} levels.\"\n+                )\n+            return self._convert_to_nested_list(data, expected_depth)\n+\n+    def _normalize_tensor_coordinates(self, tensor, original_sizes, is_bounding_box=False, preserve_padding=False):\n+        \"\"\"\n+        Helper method to normalize coordinates in a tensor across multiple images.\n+\n+        Args:\n+            tensor (`torch.Tensor`):\n+                Input tensor with coordinates.\n+            original_sizes (`list`):\n+                Original image sizes.\n+            is_bounding_box (`bool`, *optional*, defaults to `False`):\n+                Whether coordinates are bounding boxes.\n+            preserve_padding (`bool`, *optional*, defaults to `False`):\n+                Whether to preserve padding values (for boxes).\n+        \"\"\"\n+        if preserve_padding:\n+            # For boxes: avoid normalizing pad values\n+            mask = tensor != self.point_pad_value\n+            coord_mask = mask.all(dim=-1, keepdim=True)\n+\n+        for img_idx in range(len(original_sizes)):\n+            if img_idx < tensor.shape[0]:\n+                original_size = original_sizes[img_idx] if img_idx < len(original_sizes) else original_sizes[0]\n+                normalized_coords = self._normalize_coordinates(\n+                    tensor[img_idx], original_size, is_bounding_box=is_bounding_box\n+                )\n+\n+                if preserve_padding:\n+                    # Only update non-padded values\n+                    img_mask = coord_mask[img_idx]\n+                    tensor[img_idx] = torch.where(\n+                        img_mask.expand_as(tensor[img_idx]), normalized_coords, tensor[img_idx]\n+                    )\n+                else:\n+                    tensor[img_idx] = normalized_coords\n+\n+    def post_process_semantic_segmentation(self, outputs, target_sizes=None, threshold=0.5):\n+        \"\"\"\n+        Converts the output of [`Sam3Model`] into semantic segmentation maps.\n+\n+        Args:\n+            outputs ([`Sam3ImageSegmentationOutput`]):\n+                Raw outputs of the model containing semantic_seg.\n+            target_sizes (`list[tuple]` of length `batch_size`, *optional*):\n+                List of tuples corresponding to the requested final size (height, width) of each prediction. If unset,\n+                predictions will not be resized.\n+            threshold (`float`, *optional*, defaults to 0.5):\n+                Threshold for binarizing the semantic segmentation masks.\n+\n+        Returns:\n+            semantic_segmentation: `list[torch.Tensor]` of length `batch_size`, where each item is a semantic\n+            segmentation map of shape (height, width) corresponding to the target_sizes entry (if `target_sizes` is\n+            specified). Each entry is a binary mask (0 or 1).\n+        \"\"\"\n+        return self.image_processor.post_process_semantic_segmentation(outputs, target_sizes, threshold)\n+\n+    def post_process_object_detection(self, outputs, threshold=0.3, target_sizes=None):\n+        \"\"\"\n+        Converts the raw output of [`Sam3Model`] into final bounding boxes in (top_left_x, top_left_y,\n+        bottom_right_x, bottom_right_y) format. This is a convenience wrapper around the image processor method.\n+\n+        Args:\n+            outputs ([`Sam3ImageSegmentationOutput`]):\n+                Raw outputs of the model containing pred_boxes, pred_logits, and optionally presence_logits.\n+            threshold (`float`, *optional*, defaults to 0.3):\n+                Score threshold to keep object detection predictions.\n+            target_sizes (`list[tuple[int, int]]`, *optional*):\n+                List of tuples (`tuple[int, int]`) containing the target size `(height, width)` of each image in the\n+                batch. If unset, predictions will not be resized.\n+\n+        Returns:\n+            `list[dict]`: A list of dictionaries, each dictionary containing the following keys:\n+                - **scores** (`torch.Tensor`): The confidence scores for each predicted box on the image.\n+                - **boxes** (`torch.Tensor`): Image bounding boxes in (top_left_x, top_left_y, bottom_right_x,\n+                  bottom_right_y) format.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoModel, AutoProcessor\n+        >>> from PIL import Image\n+        >>> import requests\n+\n+        >>> model = AutoModel.from_pretrained(\"facebook/sam3-base\")\n+        >>> processor = AutoProcessor.from_pretrained(\"facebook/sam3-base\")\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> inputs = processor(images=image, text=\"cat\", return_tensors=\"pt\")\n+        >>> outputs = model(**inputs)\n+\n+        >>> # Post-process to get bounding boxes\n+        >>> results = processor.post_process_object_detection(outputs, threshold=0.3, target_sizes=[image.size[::-1]])\n+        >>> boxes = results[0][\"boxes\"]\n+        >>> scores = results[0][\"scores\"]\n+        ```\n+        \"\"\"\n+        return self.image_processor.post_process_object_detection(outputs, threshold, target_sizes)\n+\n+    def post_process_instance_segmentation(\n+        self,\n+        outputs,\n+        threshold=0.3,\n+        mask_threshold=0.5,\n+        target_sizes=None,\n+    ):\n+        \"\"\"\n+        Converts the raw output of [`Sam3Model`] into instance segmentation predictions with bounding boxes and masks.\n+        This is a convenience wrapper around the image processor method.\n+\n+        Args:\n+            outputs ([`Sam3ImageSegmentationOutput`]):\n+                Raw outputs of the model containing pred_boxes, pred_logits, pred_masks, and optionally\n+                presence_logits.\n+            threshold (`float`, *optional*, defaults to 0.3):\n+                Score threshold to keep instance predictions.\n+            mask_threshold (`float`, *optional*, defaults to 0.5):\n+                Threshold for binarizing the predicted masks.\n+            target_sizes (`list[tuple[int, int]]`, *optional*):\n+                List of tuples (`tuple[int, int]`) containing the target size `(height, width)` of each image in the\n+                batch. If unset, predictions will not be resized.\n+\n+        Returns:\n+            `list[dict]`: A list of dictionaries, each dictionary containing the following keys:\n+                - **scores** (`torch.Tensor`): The confidence scores for each predicted instance on the image.\n+                - **boxes** (`torch.Tensor`): Image bounding boxes in (top_left_x, top_left_y, bottom_right_x,\n+                  bottom_right_y) format.\n+                - **masks** (`torch.Tensor`): Binary segmentation masks for each instance, shape (num_instances,\n+                  height, width).\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoModel, AutoProcessor\n+        >>> from PIL import Image\n+        >>> import requests\n+\n+        >>> model = AutoModel.from_pretrained(\"facebook/sam3-base\")\n+        >>> processor = AutoProcessor.from_pretrained(\"facebook/sam3-base\")\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> inputs = processor(images=image, text=\"cat\", return_tensors=\"pt\")\n+        >>> outputs = model(**inputs)\n+\n+        >>> # Post-process to get instance segmentation\n+        >>> results = processor.post_process_instance_segmentation(\n+        ...     outputs, threshold=0.3, target_sizes=[image.size[::-1]]\n+        ... )\n+        >>> masks = results[0][\"masks\"]\n+        >>> boxes = results[0][\"boxes\"]\n+        >>> scores = results[0][\"scores\"]\n+        ```\n+        \"\"\"\n+        return self.image_processor.post_process_instance_segmentation(\n+            outputs, threshold, mask_threshold, target_sizes\n+        )\n+\n+\n+__all__ = [\"Sam3Processor\"]"
        },
        {
            "sha": "3d063b14f0896e530d19f675b42abee32a7ed09f",
            "filename": "src/transformers/models/sam3_tracker/__init__.py",
            "status": "added",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2F__init__.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,30 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_sam3_tracker import *\n+    from .modeling_sam3_tracker import *\n+    from .processing_sam3_tracker import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "bace289bc0bf8a075c871d52c1037aba9e8e8d4c",
            "filename": "src/transformers/models/sam3_tracker/configuration_sam3_tracker.py",
            "status": "added",
            "additions": 240,
            "deletions": 0,
            "changes": 240,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fconfiguration_sam3_tracker.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fconfiguration_sam3_tracker.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fconfiguration_sam3_tracker.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,240 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/sam3_tracker/modular_sam3_tracker.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_sam3_tracker.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from ...configuration_utils import PreTrainedConfig\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+class Sam3TrackerPromptEncoderConfig(PreTrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Sam3TrackerPromptEncoder`]. The [`Sam3TrackerPromptEncoder`]\n+    module is used to encode the input 2D points and bounding boxes.\n+\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 256):\n+            Dimensionality of the hidden states.\n+        image_size (`int`, *optional*, defaults to 1008):\n+            The expected output resolution of the image.\n+        patch_size (`int`, *optional*, defaults to 14):\n+            The size (resolution) of each patch.\n+        mask_input_channels (`int`, *optional*, defaults to 16):\n+            The number of channels to be fed to the `MaskDecoder` module.\n+        num_point_embeddings (`int`, *optional*, defaults to 4):\n+            The number of point embeddings to be used.\n+        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function in the encoder and pooler.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        scale (`float`, *optional*, defaults to 1):\n+            The scale factor for the prompt encoder.\n+    \"\"\"\n+\n+    base_config_key = \"prompt_encoder_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=256,\n+        image_size=1008,\n+        patch_size=14,\n+        mask_input_channels=16,\n+        num_point_embeddings=4,\n+        hidden_act=\"gelu\",\n+        layer_norm_eps=1e-6,\n+        scale=1,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.hidden_size = hidden_size\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.mask_input_channels = mask_input_channels\n+        self.num_point_embeddings = num_point_embeddings\n+        self.hidden_act = hidden_act\n+        self.layer_norm_eps = layer_norm_eps\n+        self.scale = scale\n+\n+\n+class Sam3TrackerMaskDecoderConfig(PreTrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Sam3TrackerMaskDecoder`]. It is used to instantiate a SAM3_TRACKER\n+    memory encoder according to the specified arguments, defining the model architecture.\n+\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 256):\n+            Dimensionality of the hidden states.\n+        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function in the SAM3_TRACKER mask decoder.\n+        mlp_dim (`int`, *optional*, defaults to 2048):\n+            The dimension of the MLP in the two-way transformer.\n+        num_hidden_layers (`int`, *optional*, defaults to 2):\n+            The number of hidden layers in the two-way transformer.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            The number of attention heads in the two-way transformer.\n+        attention_downsample_rate (`int`, *optional*, defaults to 2):\n+            The downsample rate for the attention layers.\n+        num_multimask_outputs (`int`, *optional*, defaults to 3):\n+            The number of multimask outputs.\n+        iou_head_depth (`int`, *optional*, defaults to 3):\n+            The depth of the IoU head.\n+        iou_head_hidden_dim (`int`, *optional*, defaults to 256):\n+            The hidden dimension of the IoU head.\n+        dynamic_multimask_via_stability (`bool`, *optional*, defaults to `True`):\n+            Whether to use dynamic multimask via stability.\n+        dynamic_multimask_stability_delta (`float`, *optional*, defaults to 0.05):\n+            The stability delta for the dynamic multimask.\n+        dynamic_multimask_stability_thresh (`float`, *optional*, defaults to 0.98):\n+            The stability threshold for the dynamic multimask.\n+\n+    \"\"\"\n+\n+    base_config_key = \"mask_decoder_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=256,\n+        hidden_act=\"gelu\",\n+        mlp_dim=2048,\n+        num_hidden_layers=2,\n+        num_attention_heads=8,\n+        attention_downsample_rate=2,\n+        num_multimask_outputs=3,\n+        iou_head_depth=3,\n+        iou_head_hidden_dim=256,\n+        dynamic_multimask_via_stability=True,\n+        dynamic_multimask_stability_delta=0.05,\n+        dynamic_multimask_stability_thresh=0.98,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.hidden_size = hidden_size\n+        self.num_multimask_outputs = num_multimask_outputs\n+        self.hidden_act = hidden_act\n+        self.iou_head_depth = iou_head_depth\n+        self.iou_head_hidden_dim = iou_head_hidden_dim\n+        self.dynamic_multimask_via_stability = dynamic_multimask_via_stability\n+        self.dynamic_multimask_stability_delta = dynamic_multimask_stability_delta\n+        self.dynamic_multimask_stability_thresh = dynamic_multimask_stability_thresh\n+\n+        # TwoWayTransformer configuration\n+        self.num_hidden_layers = num_hidden_layers\n+        self.hidden_size = hidden_size\n+        self.num_attention_heads = num_attention_heads\n+        self.mlp_dim = mlp_dim\n+        self.attention_downsample_rate = attention_downsample_rate\n+\n+\n+class Sam3TrackerConfig(PreTrainedConfig):\n+    r\"\"\"\n+    [`Sam3TrackerConfig`] is the configuration class to store the configuration of a [`Sam3TrackerModel`]. It is used to instantiate a\n+    SAM3_TRACKER model according to the specified arguments, defining the memory attention, memory encoder, and image encoder\n+    configs. Instantiating a configuration defaults will yield a similar configuration to that of the SAM 2.1 Hiera-tiny\n+    [facebook/sam3_tracker.1-hiera-tiny](https://huggingface.co/facebook/sam3_tracker.1-hiera-tiny) architecture.\n+\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+        vision_config (Union[`dict`, `Sam3TrackerVisionConfig`], *optional*):\n+            Dictionary of configuration options used to initialize [`Sam3TrackerVisionConfig`].\n+        prompt_encoder_config (Union[`dict`, `Sam3TrackerPromptEncoderConfig`], *optional*):\n+            Dictionary of configuration options used to initialize [`Sam3TrackerPromptEncoderConfig`].\n+        mask_decoder_config (Union[`dict`, `Sam3TrackerMaskDecoderConfig`], *optional*):\n+            Dictionary of configuration options used to initialize [`Sam3TrackerMaskDecoderConfig`].\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            Standard deviation for parameter initialization.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import (\n+    ...     Sam3TrackerVisionConfig,\n+    ...     Sam3TrackerPromptEncoderConfig,\n+    ...     Sam3TrackerMaskDecoderConfig,\n+    ...     Sam3TrackerModel,\n+    ... )\n+\n+    >>> # Initializing a Sam3TrackerConfig with `\"facebook/sam3_tracker.1_hiera_tiny\"` style configuration\n+    >>> configuration = Sam3Trackerconfig()\n+\n+    >>> # Initializing a Sam3TrackerModel (with random weights) from the `\"facebook/sam3_tracker.1_hiera_tiny\"` style configuration\n+    >>> model = Sam3TrackerModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+\n+    >>> # We can also initialize a Sam3TrackerConfig from a Sam3TrackerVisionConfig, Sam3TrackerPromptEncoderConfig, and Sam3TrackerMaskDecoderConfig\n+\n+    >>> # Initializing SAM3_TRACKER vision encoder, memory attention, and memory encoder configurations\n+    >>> vision_config = Sam3TrackerVisionConfig()\n+    >>> prompt_encoder_config = Sam3TrackerPromptEncoderConfig()\n+    >>> mask_decoder_config = Sam3TrackerMaskDecoderConfig()\n+\n+    >>> config = Sam3TrackerConfig(vision_config, prompt_encoder_config, mask_decoder_config)\n+    ```\"\"\"\n+\n+    model_type = \"sam3_tracker\"\n+    sub_configs = {\n+        \"vision_config\": AutoConfig,\n+        \"prompt_encoder_config\": Sam3TrackerPromptEncoderConfig,\n+        \"mask_decoder_config\": Sam3TrackerMaskDecoderConfig,\n+    }\n+\n+    def __init__(\n+        self,\n+        vision_config=None,\n+        prompt_encoder_config=None,\n+        mask_decoder_config=None,\n+        initializer_range=0.02,\n+        **kwargs,\n+    ):\n+        vision_config = (\n+            vision_config\n+            if vision_config is not None\n+            else {\"backbone_feature_sizes\": [[288, 288], [144, 144], [72, 72]]}\n+        )\n+        prompt_encoder_config = prompt_encoder_config if prompt_encoder_config is not None else {}\n+        mask_decoder_config = mask_decoder_config if mask_decoder_config is not None else {}\n+\n+        if isinstance(vision_config, dict):\n+            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"sam3_vision_model\")\n+            vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n+        if isinstance(prompt_encoder_config, Sam3TrackerPromptEncoderConfig):\n+            prompt_encoder_config = prompt_encoder_config.to_dict()\n+        if isinstance(mask_decoder_config, Sam3TrackerMaskDecoderConfig):\n+            mask_decoder_config = mask_decoder_config.to_dict()\n+\n+        self.vision_config = vision_config\n+        self.prompt_encoder_config = Sam3TrackerPromptEncoderConfig(**prompt_encoder_config)\n+        self.mask_decoder_config = Sam3TrackerMaskDecoderConfig(**mask_decoder_config)\n+\n+        self.initializer_range = initializer_range\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"Sam3TrackerConfig\", \"Sam3TrackerPromptEncoderConfig\", \"Sam3TrackerMaskDecoderConfig\"]"
        },
        {
            "sha": "d89ea60010482f262263c46134ad0b1976e46b46",
            "filename": "src/transformers/models/sam3_tracker/modeling_sam3_tracker.py",
            "status": "added",
            "additions": 1094,
            "deletions": 0,
            "changes": 1094,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fmodeling_sam3_tracker.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fmodeling_sam3_tracker.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fmodeling_sam3_tracker.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,1094 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/sam3_tracker/modular_sam3_tracker.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_sam3_tracker.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from collections.abc import Callable\n+from dataclasses import dataclass\n+from typing import Optional, Union\n+\n+import numpy as np\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+from torch import Tensor\n+\n+from transformers.utils.generic import OutputRecorder\n+\n+from ... import initialization as init\n+from ...activations import ACT2FN\n+from ...modeling_outputs import BaseModelOutput\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import ModelOutput, auto_docstring\n+from ...utils.generic import TransformersKwargs, check_model_inputs\n+from ..auto import AutoModel\n+from .configuration_sam3_tracker import Sam3TrackerConfig, Sam3TrackerMaskDecoderConfig, Sam3TrackerPromptEncoderConfig\n+\n+\n+@dataclass\n+@auto_docstring(custom_intro=\"Base class for the Sam3Tracker model's output.\")\n+class Sam3TrackerImageSegmentationOutput(ModelOutput):\n+    r\"\"\"\n+    iou_scores (`torch.FloatTensor` of shape `(batch_size, point_batch_size, num_masks)`):\n+        The Intersection over Union (IoU) scores of the predicted masks.\n+    pred_masks (`torch.FloatTensor` of shape `(batch_size, point_batch_size, num_masks, height, width)`):\n+        The predicted low-resolution masks. This is an alias for `low_res_masks`. These masks need to be post-processed\n+        by the processor to be brought to the original image size.\n+    object_score_logits (`torch.FloatTensor` of shape `(batch_size, point_batch_size, 1)`):\n+        Logits for the object score, indicating if an object is present.\n+    image_embeddings (`tuple(torch.FloatTensor)`):\n+        The features from the FPN, which are used by the mask decoder. This is a tuple of `torch.FloatTensor` where each\n+        tensor has shape `(batch_size, channels, height, width)`.\n+    vision_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of each stage) of shape `(batch_size, height, width, hidden_size)`.\n+        Hidden-states of the vision model at the output of each stage.\n+    vision_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.\n+        Attentions weights of the vision model.\n+    mask_decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.\n+        Attentions weights of the mask decoder.\n+    \"\"\"\n+\n+    iou_scores: Optional[torch.FloatTensor] = None\n+    pred_masks: Optional[torch.FloatTensor] = None\n+    object_score_logits: Optional[torch.FloatTensor] = None\n+    image_embeddings: tuple[torch.FloatTensor, ...] = None\n+    vision_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    vision_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    mask_decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+\n+\n+class Sam3TrackerFeedForward(nn.Module):\n+    def __init__(\n+        self,\n+        input_dim: int,\n+        hidden_dim: int,\n+        output_dim: int,\n+        num_layers: int,\n+        activation: str = \"relu\",\n+        sigmoid_output: bool = False,\n+    ):\n+        super().__init__()\n+        self.num_layers = num_layers\n+        self.activation = ACT2FN[activation]\n+        self.proj_in = nn.Linear(input_dim, hidden_dim)\n+        self.proj_out = nn.Linear(hidden_dim, output_dim)\n+        self.layers = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(num_layers - 2)])\n+        self.sigmoid_output = sigmoid_output\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.proj_in(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n+        for layer in self.layers:\n+            hidden_states = self.activation(layer(hidden_states))\n+\n+        hidden_states = self.proj_out(hidden_states)\n+        if self.sigmoid_output:\n+            hidden_states = F.sigmoid(hidden_states)\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class Sam3TrackerPreTrainedModel(PreTrainedModel):\n+    config_class = Sam3TrackerConfig\n+    base_model_prefix = \"sam3_tracker\"\n+    main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n+    _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n+    _supports_attention_backend = True\n+\n+    @torch.no_grad()\n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, Sam3TrackerModel):\n+            if module.no_memory_embedding is not None:\n+                init.zeros_(module.no_memory_embedding)\n+\n+\n+class Sam3TrackerPositionalEmbedding(nn.Module):\n+    def __init__(self, config: Sam3TrackerPromptEncoderConfig):\n+        super().__init__()\n+        self.scale = config.scale\n+        positional_embedding = self.scale * torch.randn((2, config.hidden_size // 2))\n+        self.register_buffer(\"positional_embedding\", positional_embedding)\n+\n+    def forward(self, input_coords, input_shape=None):\n+        \"\"\"Positionally encode points that are normalized to [0,1].\"\"\"\n+        coordinates = input_coords.clone()\n+\n+        if input_shape is not None:\n+            coordinates[:, :, :, 0] = coordinates[:, :, :, 0] / input_shape[1]\n+            coordinates[:, :, :, 1] = coordinates[:, :, :, 1] / input_shape[0]\n+        coordinates.to(torch.float32)\n+\n+        # assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\n+        coordinates = 2 * coordinates - 1\n+        coordinates = coordinates.to(self.positional_embedding.dtype)\n+        coordinates = coordinates @ self.positional_embedding\n+        coordinates = 2 * np.pi * coordinates\n+        # outputs d_1 x ... x d_n x channel shape\n+        return torch.cat([torch.sin(coordinates), torch.cos(coordinates)], dim=-1)\n+\n+\n+class Sam3TrackerMaskEmbedding(nn.Module):\n+    def __init__(self, config: Sam3TrackerPromptEncoderConfig):\n+        super().__init__()\n+        self.mask_input_channels = config.mask_input_channels // 4\n+        self.activation = ACT2FN[config.hidden_act]\n+        self.conv1 = nn.Conv2d(1, self.mask_input_channels, kernel_size=2, stride=2)\n+        self.conv2 = nn.Conv2d(self.mask_input_channels, config.mask_input_channels, kernel_size=2, stride=2)\n+        self.conv3 = nn.Conv2d(config.mask_input_channels, config.hidden_size, kernel_size=1)\n+        self.layer_norm1 = Sam3TrackerLayerNorm(\n+            self.mask_input_channels, eps=config.layer_norm_eps, data_format=\"channels_first\"\n+        )\n+        self.layer_norm2 = Sam3TrackerLayerNorm(\n+            self.mask_input_channels * 4, eps=config.layer_norm_eps, data_format=\"channels_first\"\n+        )\n+\n+    def forward(self, masks):\n+        hidden_states = self.conv1(masks)\n+        hidden_states = self.layer_norm1(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n+\n+        hidden_states = self.conv2(hidden_states)\n+        hidden_states = self.layer_norm2(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n+        dense_embeddings = self.conv3(hidden_states)\n+        return dense_embeddings\n+\n+\n+class Sam3TrackerPromptEncoder(nn.Module):\n+    def __init__(self, config: Sam3TrackerPromptEncoderConfig):\n+        super().__init__()\n+        self.shared_embedding = Sam3TrackerPositionalEmbedding(config)\n+        self.mask_embed = Sam3TrackerMaskEmbedding(config)\n+        self.no_mask_embed = nn.Embedding(1, config.hidden_size)\n+\n+        self.image_embedding_size = (config.image_size // config.patch_size, config.image_size // config.patch_size)\n+        self.mask_input_size = (4 * config.image_size // config.patch_size, 4 * config.image_size // config.patch_size)\n+        self.input_image_size = config.image_size\n+\n+        self.point_embed = nn.Embedding(config.num_point_embeddings, config.hidden_size)\n+        self.hidden_size = config.hidden_size\n+        self.not_a_point_embed = nn.Embedding(1, config.hidden_size)\n+\n+    def _embed_points(self, points: torch.Tensor, labels: torch.Tensor, pad: bool) -> torch.Tensor:\n+        \"\"\"Embeds point prompts.\"\"\"\n+        points = points + 0.5  # Shift to center of pixel\n+        if pad:\n+            points = torch.nn.functional.pad(points, (0, 0, 0, 1), mode=\"constant\", value=0)\n+            labels = torch.nn.functional.pad(labels, (0, 1), mode=\"constant\", value=-1)\n+        input_shape = (self.input_image_size, self.input_image_size)\n+        point_embedding = self.shared_embedding(points, input_shape)\n+\n+        # torch.where and expanding the labels tensor is required by the ONNX export\n+        point_embedding = torch.where(labels[..., None] == -1, self.not_a_point_embed.weight, point_embedding)\n+\n+        # This is required for the ONNX export. The dtype, device need to be explicitly\n+        # specified as otherwise torch.onnx.export interprets as double\n+        point_embedding = torch.where(\n+            labels[..., None] != -10,\n+            point_embedding,\n+            torch.zeros_like(point_embedding),\n+        )\n+\n+        # Add point embeddings for labels >= 0\n+        point_embedding = point_embedding + self.point_embed(labels.clamp(min=0)) * (labels >= 0).unsqueeze(-1)\n+\n+        return point_embedding\n+\n+    def _embed_boxes(self, boxes: torch.Tensor) -> torch.Tensor:\n+        \"\"\"Embeds box prompts.\"\"\"\n+        boxes = boxes + 0.5  # Shift to center of pixel\n+        coords = boxes.view(*boxes.shape[:2], 2, 2)\n+        # add padding point for consistency with the original implementation\n+        coords = torch.nn.functional.pad(coords, (0, 0, 0, 1), mode=\"constant\", value=0)\n+        corner_embedding = self.shared_embedding(coords, (self.input_image_size, self.input_image_size))\n+        corner_embedding[:, :, 0, :] += self.point_embed.weight[2]\n+        corner_embedding[:, :, 1, :] += self.point_embed.weight[3]\n+        corner_embedding[:, :, 2, :] = self.not_a_point_embed.weight.expand_as(corner_embedding[:, :, 2, :])\n+        return corner_embedding\n+\n+    def forward(\n+        self,\n+        input_points: Optional[tuple[torch.Tensor, torch.Tensor]],\n+        input_labels: Optional[torch.Tensor],\n+        input_boxes: Optional[torch.Tensor],\n+        input_masks: Optional[torch.Tensor],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Embeds different types of prompts, returning both sparse and dense embeddings.\n+\n+        Args:\n+            points (`torch.Tensor`, *optional*):\n+                point coordinates and labels to embed.\n+            boxes (`torch.Tensor`, *optional*):\n+                boxes to embed\n+            masks (`torch.Tensor`, *optional*):\n+                masks to embed\n+        \"\"\"\n+        sparse_embeddings = None\n+        batch_size = 1\n+        if input_points is not None:\n+            batch_size = input_points.shape[0]\n+            if input_labels is None:\n+                raise ValueError(\"If points are provided, labels must also be provided.\")\n+            point_embeddings = self._embed_points(input_points, input_labels, pad=(input_boxes is None))\n+            sparse_embeddings = point_embeddings\n+        if input_boxes is not None:\n+            batch_size = input_boxes.shape[0]\n+            box_embeddings = self._embed_boxes(input_boxes)\n+            if sparse_embeddings is None:\n+                sparse_embeddings = box_embeddings\n+            else:\n+                sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=2)\n+        if input_masks is not None:\n+            dense_embeddings = self.mask_embed(input_masks)\n+        else:\n+            dense_embeddings = self.no_mask_embed.weight.reshape(1, -1, 1, 1).expand(\n+                batch_size, -1, self.image_embedding_size[0], self.image_embedding_size[1]\n+            )\n+\n+        return sparse_embeddings, dense_embeddings\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class Sam3TrackerAttention(nn.Module):\n+    \"\"\"\n+    SAM3_TRACKER's attention layer that allows for downscaling the size of the embedding after projection to queries, keys, and\n+    values.\n+    \"\"\"\n+\n+    def __init__(self, config, downsample_rate=None):\n+        super().__init__()\n+        downsample_rate = config.attention_downsample_rate if downsample_rate is None else downsample_rate\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.internal_dim = config.hidden_size // downsample_rate\n+        self.num_attention_heads = config.num_attention_heads\n+        self.head_dim = self.internal_dim // config.num_attention_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.is_causal = False\n+\n+        self.q_proj = nn.Linear(self.hidden_size, self.internal_dim)\n+        self.k_proj = nn.Linear(self.hidden_size, self.internal_dim)\n+        self.v_proj = nn.Linear(self.hidden_size, self.internal_dim)\n+        self.o_proj = nn.Linear(self.internal_dim, self.hidden_size)\n+\n+    def forward(\n+        self,\n+        query: torch.Tensor,\n+        key: torch.Tensor,\n+        value: torch.Tensor,\n+        attention_similarity: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        # Input projections\n+        batch_size, point_batch_size = query.shape[:2]\n+        new_shape = (batch_size * point_batch_size, -1, self.num_attention_heads, self.head_dim)\n+\n+        query = self.q_proj(query).view(*new_shape).transpose(1, 2)\n+        key = self.k_proj(key).view(*new_shape).transpose(1, 2)\n+        value = self.v_proj(value).view(*new_shape).transpose(1, 2)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query,\n+            key,\n+            value,\n+            attention_mask=attention_similarity,\n+            dropout=0.0,\n+            scaling=self.scaling,\n+            is_causal=self.is_causal,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(\n+            batch_size, point_batch_size, -1, self.num_attention_heads * self.head_dim\n+        ).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+\n+        return attn_output, attn_weights\n+\n+\n+class Sam3TrackerTwoWayAttentionBlock(nn.Module):\n+    def __init__(self, config: Sam3TrackerMaskDecoderConfig, skip_first_layer_pe: bool = False):\n+        \"\"\"\n+        A transformer block with four layers:\n+            (1) self-attention of sparse inputs (2) cross attention of sparse inputs -> dense inputs (3) mlp block on\n+            sparse inputs (4) cross attention of dense inputs -> sparse inputs\n+\n+        Arguments:\n+            config (`Sam3TrackerMaskDecoderConfig`):\n+                The configuration file used to instantiate the block\n+            attention_downsample_rate (*optionalk*, int, defaults to 2):\n+                The downsample ratio of the block used to reduce the inner dim of the attention.\n+            skip_first_layer_pe (*optional*, bool, defaults to `False`):\n+                Whether or not to skip the addition of the query_point_embedding on the first layer.\n+        \"\"\"\n+        super().__init__()\n+        self.self_attn = Sam3TrackerAttention(config, downsample_rate=1)\n+        self.layer_norm1 = nn.LayerNorm(config.hidden_size)\n+\n+        self.cross_attn_token_to_image = Sam3TrackerAttention(config)\n+        self.layer_norm2 = nn.LayerNorm(config.hidden_size)\n+\n+        self.mlp = Sam3TrackerFeedForward(\n+            config.hidden_size, config.mlp_dim, config.hidden_size, num_layers=config.num_hidden_layers\n+        )\n+        self.layer_norm3 = nn.LayerNorm(config.hidden_size)\n+\n+        self.layer_norm4 = nn.LayerNorm(config.hidden_size)\n+        self.cross_attn_image_to_token = Sam3TrackerAttention(config)\n+\n+        self.skip_first_layer_pe = skip_first_layer_pe\n+\n+    def forward(\n+        self,\n+        queries: Tensor,\n+        keys: Tensor,\n+        query_point_embedding: Tensor,\n+        key_point_embedding: Tensor,\n+        attention_similarity: Tensor,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ):\n+        # Self attention block\n+        if self.skip_first_layer_pe:\n+            queries, _ = self.self_attn(query=queries, key=queries, value=queries)\n+        else:\n+            query = queries + query_point_embedding\n+            attn_out, _ = self.self_attn(query=query, key=query, value=queries)\n+            queries = queries + attn_out\n+        queries = self.layer_norm1(queries)\n+\n+        # Cross attention block, tokens attending to image embedding\n+        query = queries + query_point_embedding\n+        key = keys + key_point_embedding\n+\n+        attn_out, _ = self.cross_attn_token_to_image(\n+            query=query, key=key, value=keys, attention_similarity=attention_similarity\n+        )\n+        queries = queries + attn_out\n+\n+        queries = self.layer_norm2(queries)\n+\n+        # MLP block\n+        mlp_out = self.mlp(queries)\n+        queries = queries + mlp_out\n+        queries = self.layer_norm3(queries)\n+\n+        # Cross attention block, image embedding attending to tokens\n+        query = queries + query_point_embedding\n+        key = keys + key_point_embedding\n+\n+        attn_out, _ = self.cross_attn_image_to_token(query=key, key=query, value=queries)\n+        keys = keys + attn_out\n+\n+        keys = self.layer_norm4(keys)\n+        return queries, keys, attn_out\n+\n+\n+class Sam3TrackerTwoWayTransformer(nn.Module):\n+    def __init__(self, config: Sam3TrackerMaskDecoderConfig):\n+        super().__init__()\n+        self.config = config\n+\n+        self.num_hidden_layers = config.num_hidden_layers\n+        self.layers = nn.ModuleList()\n+\n+        for i in range(self.num_hidden_layers):\n+            self.layers.append(Sam3TrackerTwoWayAttentionBlock(config, skip_first_layer_pe=(i == 0)))\n+\n+        self.final_attn_token_to_image = Sam3TrackerAttention(config)\n+        self.layer_norm_final_attn = nn.LayerNorm(config.hidden_size)\n+\n+    def forward(\n+        self,\n+        point_embeddings: Tensor,\n+        image_embeddings: Tensor,\n+        image_positional_embeddings: Tensor,\n+        attention_similarity: Tensor,\n+        target_embedding=None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, BaseModelOutput]:\n+        if image_embeddings is None:\n+            raise ValueError(\"You have to specify an image_embedding\")\n+\n+        image_embeddings = image_embeddings.flatten(2).permute(0, 2, 1).unsqueeze(1)\n+        image_positional_embeddings = image_positional_embeddings.flatten(2).permute(0, 2, 1).unsqueeze(1)\n+\n+        # Prepare queries\n+        queries = point_embeddings\n+        keys = image_embeddings\n+\n+        # Apply transformer blocks and final layernorm\n+        for layer in self.layers:\n+            if target_embedding is not None:\n+                queries += target_embedding\n+\n+            queries, keys, _ = layer(\n+                queries=queries,\n+                keys=keys,\n+                query_point_embedding=point_embeddings,\n+                key_point_embedding=image_positional_embeddings,\n+                attention_similarity=attention_similarity,\n+                **kwargs,\n+            )\n+        # Apply the final attention layer from the points to the image\n+        query = queries + point_embeddings\n+        key = keys + image_positional_embeddings\n+\n+        attn_out, _ = self.final_attn_token_to_image(query=query, key=key, value=keys)\n+\n+        queries = queries + attn_out\n+        queries = self.layer_norm_final_attn(queries)\n+        return queries, keys\n+\n+\n+class Sam3TrackerLayerNorm(nn.LayerNorm):\n+    r\"\"\"LayerNorm that supports two data formats: channels_last (default) or channels_first.\n+    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch_size, height,\n+    width, channels) while channels_first corresponds to inputs with shape (batch_size, channels, height, width).\n+    \"\"\"\n+\n+    def __init__(self, normalized_shape, *, eps=1e-6, data_format=\"channels_last\", **kwargs):\n+        super().__init__(normalized_shape, eps=eps, **kwargs)\n+        if data_format not in [\"channels_last\", \"channels_first\"]:\n+            raise NotImplementedError(f\"Unsupported data format: {data_format}\")\n+        self.data_format = data_format\n+\n+    def forward(self, features: torch.Tensor) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            features: Tensor of shape (batch_size, channels, height, width) OR (batch_size, height, width, channels)\n+        \"\"\"\n+        if self.data_format == \"channels_first\":\n+            features = features.permute(0, 2, 3, 1)\n+            features = super().forward(features)\n+            features = features.permute(0, 3, 1, 2)\n+        else:\n+            features = super().forward(features)\n+        return features\n+\n+\n+class Sam3TrackerMaskDecoder(nn.Module):\n+    def __init__(self, config: Sam3TrackerMaskDecoderConfig):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+\n+        self.num_multimask_outputs = config.num_multimask_outputs\n+        self.num_mask_tokens = config.num_multimask_outputs + 1\n+\n+        self.iou_token = nn.Embedding(1, self.hidden_size)\n+        self.mask_tokens = nn.Embedding(self.num_mask_tokens, self.hidden_size)\n+\n+        self.transformer = Sam3TrackerTwoWayTransformer(config)\n+\n+        # should we create a new class for this?\n+        self.upscale_conv1 = nn.ConvTranspose2d(self.hidden_size, self.hidden_size // 4, kernel_size=2, stride=2)\n+        self.upscale_conv2 = nn.ConvTranspose2d(self.hidden_size // 4, self.hidden_size // 8, kernel_size=2, stride=2)\n+        self.upscale_layer_norm = Sam3TrackerLayerNorm(self.hidden_size // 4, data_format=\"channels_first\")\n+        self.activation = nn.GELU()\n+\n+        mlps_list = []\n+        for _ in range(self.num_mask_tokens):\n+            mlps_list += [Sam3TrackerFeedForward(self.hidden_size, self.hidden_size, self.hidden_size // 8, 3)]\n+        self.output_hypernetworks_mlps = nn.ModuleList(mlps_list)\n+        self.iou_prediction_head = Sam3TrackerFeedForward(\n+            self.hidden_size,\n+            config.iou_head_hidden_dim,\n+            self.num_mask_tokens,\n+            config.iou_head_depth,\n+            sigmoid_output=True,\n+        )\n+\n+        self.conv_s0 = nn.Conv2d(config.hidden_size, config.hidden_size // 8, kernel_size=1, stride=1)\n+        self.conv_s1 = nn.Conv2d(config.hidden_size, config.hidden_size // 4, kernel_size=1, stride=1)\n+\n+        self.obj_score_token = nn.Embedding(1, self.hidden_size)\n+        self.pred_obj_score_head = Sam3TrackerFeedForward(self.hidden_size, self.hidden_size, 1, 3)\n+\n+        self.dynamic_multimask_via_stability = config.dynamic_multimask_via_stability\n+        self.dynamic_multimask_stability_delta = config.dynamic_multimask_stability_delta\n+        self.dynamic_multimask_stability_thresh = config.dynamic_multimask_stability_thresh\n+\n+    def forward(\n+        self,\n+        image_embeddings: torch.Tensor,\n+        image_positional_embeddings: torch.Tensor,\n+        sparse_prompt_embeddings: torch.Tensor,\n+        dense_prompt_embeddings: torch.Tensor,\n+        multimask_output: bool,\n+        high_resolution_features: list[torch.Tensor],\n+        attention_similarity: Optional[torch.Tensor] = None,\n+        target_embedding: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Predict masks given image and prompt embeddings.\n+\n+        Args:\n+            image_embeddings (`torch.Tensor`):\n+                The embeddings from the image encoder.\n+            image_positional_embeddings (`torch.Tensor`):\n+                Positional encoding with the shape of image_embeddings.\n+            sparse_prompt_embeddings (`torch.Tensor`):\n+                The embeddings of the points and boxes.\n+            dense_prompt_embeddings (`torch.Tensor`):\n+                The embeddings of the mask inputs.\n+            multimask_output (`bool`):\n+                Whether to return multiple masks or a single mask.\n+            high_resolution_features (`list[torch.Tensor]`, *optional*):\n+                The high-resolution features from the vision encoder.\n+            attention_similarity (`torch.Tensor`, *optional*):\n+                The attention similarity tensor.\n+            target_embedding (`torch.Tensor`, *optional*):\n+                The target embedding.\n+        \"\"\"\n+        batch_size, num_channels, height, width = image_embeddings.shape\n+        point_batch_size = sparse_prompt_embeddings.shape[1]\n+        # Concatenate output tokens\n+        output_tokens = torch.cat(\n+            [\n+                self.obj_score_token.weight,\n+                self.iou_token.weight,\n+                self.mask_tokens.weight,\n+            ],\n+            dim=0,\n+        )\n+        output_tokens = output_tokens.repeat(batch_size, point_batch_size, 1, 1)\n+\n+        if sparse_prompt_embeddings.shape[0] != 0:\n+            tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=2)\n+        else:\n+            tokens = output_tokens\n+        point_embeddings = tokens.to(self.iou_token.weight.dtype)\n+\n+        # Expand per-image data in batch direction to be per-mask\n+        image_embeddings = image_embeddings + dense_prompt_embeddings\n+        image_embeddings = image_embeddings.repeat_interleave(point_batch_size, dim=0)\n+        image_positional_embeddings = image_positional_embeddings.repeat_interleave(point_batch_size, 0)\n+        # Run the transformer\n+        point_embeddings, image_embeddings = self.transformer(\n+            point_embeddings=point_embeddings,\n+            image_embeddings=image_embeddings,\n+            image_positional_embeddings=image_positional_embeddings,\n+            attention_similarity=attention_similarity,\n+            target_embedding=target_embedding,\n+            **kwargs,\n+        )\n+        iou_token_out = point_embeddings[:, :, 1, :]\n+        mask_tokens_out = point_embeddings[:, :, 2 : (2 + self.num_mask_tokens), :]\n+\n+        # Upscale mask embeddings and predict masks using the mask tokens\n+        image_embeddings = image_embeddings.transpose(2, 3).view(\n+            batch_size * point_batch_size, num_channels, height, width\n+        )\n+\n+        feat_s0, feat_s1 = high_resolution_features\n+        feat_s0 = feat_s0.repeat_interleave(point_batch_size, dim=0)\n+        feat_s1 = feat_s1.repeat_interleave(point_batch_size, dim=0)\n+        upscaled_embedding = self.upscale_conv1(image_embeddings) + feat_s1\n+        upscaled_embedding = self.activation(self.upscale_layer_norm(upscaled_embedding))\n+        upscaled_embedding = self.activation(self.upscale_conv2(upscaled_embedding) + feat_s0)\n+\n+        hyper_in_list: list[torch.Tensor] = []\n+        for i in range(self.num_mask_tokens):\n+            current_mlp = self.output_hypernetworks_mlps[i]\n+            hyper_in_list += [current_mlp(mask_tokens_out[:, :, i, :])]\n+        hyper_in = torch.stack(hyper_in_list, dim=2)\n+\n+        _, num_channels, height, width = upscaled_embedding.shape\n+        upscaled_embedding = upscaled_embedding.view(batch_size, point_batch_size, num_channels, height * width)\n+        masks = (hyper_in @ upscaled_embedding).view(batch_size, point_batch_size, -1, height, width)\n+\n+        # Generate mask quality predictions\n+        iou_pred = self.iou_prediction_head(iou_token_out)\n+        object_score_logits = self.pred_obj_score_head(point_embeddings[:, :, 0, :])\n+\n+        # Select the correct mask or masks for output\n+        if multimask_output:\n+            mask_slice = slice(1, None)\n+            masks = masks[:, :, mask_slice, :, :]\n+            iou_pred = iou_pred[:, :, mask_slice]\n+        elif self.dynamic_multimask_via_stability and not self.training:\n+            mask_slice = slice(0, 1)\n+            masks, iou_pred = self._dynamic_multimask_via_stability(masks, iou_pred)\n+        else:\n+            mask_slice = slice(0, 1)\n+            masks = masks[:, :, mask_slice, :, :]\n+            iou_pred = iou_pred[:, :, mask_slice]\n+\n+        sam_tokens_out = mask_tokens_out[:, :, mask_slice]  # [b, 3, c] shape\n+\n+        return masks, iou_pred, sam_tokens_out, object_score_logits\n+\n+    def _get_stability_scores(self, mask_logits):\n+        \"\"\"\n+        Compute stability scores of the mask logits based on the IoU between upper and\n+        lower thresholds.\n+        \"\"\"\n+        mask_logits = mask_logits.flatten(-2)\n+        stability_delta = self.dynamic_multimask_stability_delta\n+        area_i = torch.sum(mask_logits > stability_delta, dim=-1).float()\n+        area_u = torch.sum(mask_logits > -stability_delta, dim=-1).float()\n+        stability_scores = torch.where(area_u > 0, area_i / area_u, 1.0)\n+        return stability_scores\n+\n+    def _dynamic_multimask_via_stability(self, all_mask_logits, all_iou_scores):\n+        \"\"\"\n+        When outputting a single mask, if the stability score from the current single-mask\n+        output (based on output token 0) falls below a threshold, we instead select from\n+        multi-mask outputs (based on output token 1~3) the mask with the highest predicted\n+        IoU score. This is intended to ensure a valid mask for both clicking and tracking.\n+        \"\"\"\n+        # The best mask from multimask output tokens (1~3)\n+        multimask_logits = all_mask_logits[:, :, 1:, :, :]\n+        multimask_iou_scores = all_iou_scores[:, :, 1:]\n+        best_scores_inds = torch.argmax(multimask_iou_scores, dim=-1)  # [B, P]\n+        best_scores_inds_expanded = best_scores_inds.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n+        best_scores_inds_expanded = best_scores_inds_expanded.expand(\n+            -1, -1, 1, multimask_logits.size(-2), multimask_logits.size(-1)\n+        )\n+        best_multimask_logits = torch.gather(multimask_logits, 2, best_scores_inds_expanded)  # [B, P, 1, H, W]\n+        best_multimask_iou_scores = torch.gather(multimask_iou_scores, 2, best_scores_inds.unsqueeze(-1))  # [B, P, 1]\n+\n+        # The mask from singlemask output token 0 and its stability score\n+        singlemask_logits = all_mask_logits[:, :, 0:1, :, :]\n+        singlemask_iou_scores = all_iou_scores[:, :, 0:1]\n+        stability_scores = self._get_stability_scores(singlemask_logits)\n+        is_stable = stability_scores >= self.dynamic_multimask_stability_thresh\n+\n+        # Dynamically fall back to best multimask output upon low stability scores.\n+        mask_logits_out = torch.where(\n+            is_stable[..., None, None].expand_as(singlemask_logits),\n+            singlemask_logits,\n+            best_multimask_logits,\n+        )\n+        iou_scores_out = torch.where(\n+            is_stable.expand_as(singlemask_iou_scores),\n+            singlemask_iou_scores,\n+            best_multimask_iou_scores,\n+        )\n+        return mask_logits_out, iou_scores_out\n+\n+\n+@dataclass\n+@auto_docstring(custom_intro=\"Base class for the vision encoder's outputs.\")\n+class Sam3TrackerVisionEncoderOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, height, width, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the model.\n+    fpn_hidden_states (`tuple(torch.FloatTensor)`):\n+        Tuple of `torch.FloatTensor` (one for each feature level, from high to low resolution) of shape\n+        `(batch_size, hidden_size, height, width)`. Feature maps from the Feature Pyramid Network neck.\n+    fpn_position_encoding (`tuple(torch.FloatTensor)`):\n+        Tuple of `torch.FloatTensor` (one for each feature level, from high to low resolution) of shape\n+        `(batch_size, hidden_size, height, width)`. Positional encodings corresponding to the `fpn_hidden_states`.\n+    hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+        one for the output of each stage) of shape `(batch_size, height, width, hidden_size)`. Hidden-states of the\n+        model at the output of each stage.\n+    attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n+        the self-attention heads.\n+    \"\"\"\n+\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    fpn_hidden_states: Optional[torch.FloatTensor] = None\n+    fpn_position_encoding: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Segment Anything Model 2 (SAM 2) for generating segmentation masks, given an input image and\n+    input points and labels, boxes, or masks.\n+    \"\"\"\n+)\n+class Sam3TrackerModel(Sam3TrackerPreTrainedModel):\n+    input_modalities = [\"image\", \"text\"]\n+    _can_record_outputs = {\"mask_decoder_attentions\": OutputRecorder(Sam3TrackerTwoWayAttentionBlock, index=2)}\n+    _keys_to_ignore_on_load_unexpected = [\n+        r\"^detector_model.\",\n+        r\"^memory_.*\",\n+        r\"^mask_downsample.*\",\n+        r\"^object_pointer_proj.*\",\n+        r\"^temporal_positional_encoding_projection_layer.*\",\n+        \"no_memory_positional_encoding\",\n+        \"no_object_pointer\",\n+        \"occlusion_spatial_embedding_parameter\",\n+    ]\n+    _checkpoint_conversion_mapping = {\n+        \"tracker_model.\": \"\",\n+        \"detector_model.vision_encoder.backbone.\": \"vision_encoder.backbone.\",\n+        \"tracker_neck.\": \"vision_encoder.neck.\",\n+    }\n+\n+    def __init__(self, config: Sam3TrackerConfig):\n+        # loading from a sam3_video config\n+        if hasattr(config, \"tracker_config\") and config.tracker_config is not None:\n+            if isinstance(config.tracker_config, dict):\n+                config.tracker_config = Sam3TrackerConfig(**config.tracker_config)\n+            config = config.tracker_config\n+        super().__init__(config)\n+        self.shared_image_embedding = Sam3TrackerPositionalEmbedding(config.prompt_encoder_config)\n+        self.vision_encoder = AutoModel.from_config(config.vision_config)\n+        self.prompt_encoder = Sam3TrackerPromptEncoder(config.prompt_encoder_config)\n+        # The module using it is not a PreTrainedModel subclass so we need this\n+        config.mask_decoder_config._attn_implementation = config._attn_implementation\n+        self.mask_decoder = Sam3TrackerMaskDecoder(config.mask_decoder_config)\n+\n+        self.backbone_feature_sizes = config.vision_config.backbone_feature_sizes\n+        # a single token to indicate no memory embedding from previous frames\n+        self.hidden_dim = config.vision_config.fpn_hidden_size\n+        self.no_memory_embedding = torch.nn.Parameter(torch.zeros(1, 1, self.hidden_dim))\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.vision_encoder.get_input_embeddings()\n+\n+    def get_image_wide_positional_embeddings(self) -> torch.Tensor:\n+        size = self.prompt_encoder.image_embedding_size\n+        target_device = self.shared_image_embedding.positional_embedding.device\n+        target_dtype = self.shared_image_embedding.positional_embedding.dtype\n+        grid = torch.ones(size, device=target_device, dtype=target_dtype)\n+        y_embed = grid.cumsum(dim=0) - 0.5\n+        x_embed = grid.cumsum(dim=1) - 0.5\n+        y_embed = y_embed / size[0]\n+        x_embed = x_embed / size[1]\n+\n+        positional_embedding = self.shared_image_embedding(torch.stack([x_embed, y_embed], dim=-1))\n+        return positional_embedding.permute(2, 0, 1).unsqueeze(0)  # channel x height x width\n+\n+    @torch.no_grad()\n+    def get_image_embeddings(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> list[torch.Tensor]:\n+        r\"\"\"\n+        Returns the image embeddings by passing the pixel values through the vision encoder.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+                Input pixel values\n+        \"\"\"\n+        batch_size = pixel_values.shape[0]\n+        feature_maps, _, _, _ = self.get_image_features(pixel_values, **kwargs)\n+\n+        # add no memory embedding to the last feature map\n+        feature_maps[-1] = feature_maps[-1] + self.no_memory_embedding\n+\n+        # reshape feature maps to the same shape as the backbone feature sizes\n+        image_embeddings = [\n+            feat.permute(1, 2, 0).view(batch_size, -1, *feat_size)\n+            for feat, feat_size in zip(feature_maps, self.backbone_feature_sizes)\n+        ]\n+\n+        return image_embeddings\n+\n+    @torch.no_grad()\n+    def get_prompt_embeddings(\n+        self,\n+        input_points: Optional[torch.FloatTensor] = None,\n+        input_labels: Optional[torch.LongTensor] = None,\n+        input_boxes: Optional[torch.FloatTensor] = None,\n+        input_masks: Optional[torch.LongTensor] = None,\n+    ):\n+        r\"\"\"\n+        Returns the prompt embeddings by passing the input points, labels, boxes and masks through the prompt encoder.\n+\n+        Args:\n+            input_points (`torch.FloatTensor` of shape `(batch_size, point_batch_size, num_points_per_image, 2)`):\n+                Optional input points for the prompt encoder. The padding of the point is automatically done by the\n+                processor. `point_batch_size` refers to the number of masks that we want the model to predict per\n+                point. The model will output `point_batch_size` times 3 masks in total.\n+            input_labels (`torch.LongTensor` of shape `(batch_size, point_batch_size, num_points_per_image)`):\n+                Optional input labels for the prompt encoder. The padding of the labels is automatically done by the\n+                processor, or can be fed by the user.\n+            input_boxes (`torch.FloatTensor` of shape `(batch_size, num_boxes_per_image, 4)`):\n+                Optional input boxes for the prompt encoder. The padding of the boxes is automatically done by the\n+                processor. users can also pass manually the input boxes.\n+            input_masks (`torch.LongTensor` of shape `(batch_size, image_size, image_size)`):\n+                Optional input masks for the prompt encoder.\n+        \"\"\"\n+        prompt_output = self.prompt_encoder(\n+            input_points=input_points,\n+            input_labels=input_labels,\n+            input_boxes=input_boxes,\n+            input_masks=input_masks,\n+        )\n+        return prompt_output\n+\n+    @check_model_inputs()\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        input_points: Optional[torch.FloatTensor] = None,\n+        input_labels: Optional[torch.LongTensor] = None,\n+        input_boxes: Optional[torch.FloatTensor] = None,\n+        input_masks: Optional[torch.LongTensor] = None,\n+        image_embeddings: Optional[torch.FloatTensor] = None,\n+        multimask_output: bool = True,\n+        attention_similarity: Optional[torch.FloatTensor] = None,\n+        target_embedding: Optional[torch.FloatTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Sam3TrackerImageSegmentationOutput:\n+        r\"\"\"\n+        input_points (`torch.FloatTensor` of shape `(batch_size, num_points, 2)`):\n+            Input 2D spatial points, this is used by the prompt encoder to encode the prompt. Generally yields to much\n+            better results. The points can be obtained by passing a list of list of list to the processor that will\n+            create corresponding `torch` tensors of dimension 4. The first dimension is the image batch size, the\n+            second dimension is the point batch size (i.e. how many segmentation masks do we want the model to predict\n+            per input point), the third dimension is the number of points per segmentation mask (it is possible to pass\n+            multiple points for a single mask), and the last dimension is the x (vertical) and y (horizontal)\n+            coordinates of the point. If a different number of points is passed either for each image, or for each\n+            mask, the processor will create \"PAD\" points that will correspond to the (0, 0) coordinate, and the\n+            computation of the embedding will be skipped for these points using the labels.\n+        input_labels (`torch.LongTensor` of shape `(batch_size, point_batch_size, num_points)`):\n+            Input labels for the points, this is used by the prompt encoder to encode the prompt. According to the\n+            official implementation, there are 3 types of labels\n+\n+            - `1`: the point is a point that contains the object of interest\n+            - `0`: the point is a point that does not contain the object of interest\n+            - `-1`: the point corresponds to the background\n+\n+            We added the label:\n+\n+            - `-10`: the point is a padding point, thus should be ignored by the prompt encoder\n+\n+            The padding labels should be automatically done by the processor.\n+        input_boxes (`torch.FloatTensor` of shape `(batch_size, num_boxes, 4)`):\n+            Input boxes for the points, this is used by the prompt encoder to encode the prompt. Generally yields to\n+            much better generated masks. The boxes can be obtained by passing a list of list of list to the processor,\n+            that will generate a `torch` tensor, with each dimension corresponding respectively to the image batch\n+            size, the number of boxes per image and the coordinates of the top left and bottom right point of the box.\n+            In the order (`x1`, `y1`, `x2`, `y2`):\n+\n+            - `x1`: the x coordinate of the top left point of the input box\n+            - `y1`: the y coordinate of the top left point of the input box\n+            - `x2`: the x coordinate of the bottom right point of the input box\n+            - `y2`: the y coordinate of the bottom right point of the input box\n+        input_masks (`torch.FloatTensor` of shape `(batch_size, image_size, image_size)`):\n+            SAM model also accepts segmentation masks as input. The mask will be embedded by the prompt encoder to\n+            generate a corresponding embedding, that will be fed later on to the mask decoder. These masks needs to be\n+            manually fed by the user, and they need to be of shape (`batch_size`, `image_size`, `image_size`).\n+        image_embeddings (`torch.FloatTensor` of shape `(batch_size, output_channels, window_size, window_size)`):\n+            Image embeddings, this is used by the mask decoder to generate masks and iou scores. For more memory\n+            efficient computation, users can first retrieve the image embeddings using the `get_image_embeddings`\n+            method, and then feed them to the `forward` method instead of feeding the `pixel_values`.\n+        multimask_output (`bool`, *optional*):\n+            In the original implementation and paper, the model always outputs 3 masks per image (or per point / per\n+            bounding box if relevant). However, it is possible to just output a single mask, that corresponds to the\n+            \"best\" mask, by specifying `multimask_output=False`.\n+        attention_similarity (`torch.FloatTensor`, *optional*):\n+            Attention similarity tensor, to be provided to the mask decoder for target-guided attention in case the\n+            model is used for personalization as introduced in [PerSAM](https://huggingface.co/papers/2305.03048).\n+        target_embedding (`torch.FloatTensor`, *optional*):\n+            Embedding of the target concept, to be provided to the mask decoder for target-semantic prompting in case\n+            the model is used for personalization as introduced in [PerSAM](https://huggingface.co/papers/2305.03048).\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoModel, AutoProcessor\n+\n+        >>> model = AutoModel.from_pretrained(\"danelcsb/sam3_tracker.1_hiera_tiny\")\n+        >>> processor = AutoProcessor.from_pretrained(\"danelcsb/sam3_tracker.1_hiera_tiny\")\n+\n+        >>> img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/sam-car.png\"\n+        >>> raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+        >>> input_points = [[[400, 650]]]  # 2D location of a window on the car\n+        >>> inputs = processor(images=raw_image, input_points=input_points, return_tensors=\"pt\")\n+\n+        >>> # Get segmentation mask\n+        >>> outputs = model(**inputs)\n+\n+        >>> # Postprocess masks\n+        >>> masks = processor.post_process_masks(\n+        ...     outputs.pred_masks, inputs[\"original_sizes\"], inputs[\"reshaped_input_sizes\"]\n+        ... )\n+        ```\n+        \"\"\"\n+        if not ((pixel_values is None) ^ (image_embeddings is None)):\n+            raise ValueError(\"Exactly one of pixel_values or image_embeddings must be provided.\")\n+        if input_points is not None and input_boxes is not None:\n+            if input_points.shape[1] != input_boxes.shape[1]:\n+                raise ValueError(\n+                    f\"You should provide as many bounding boxes as input points per box. Got {input_points.shape[1]} and {input_boxes.shape[1]}.\"\n+                )\n+\n+        image_positional_embeddings = self.get_image_wide_positional_embeddings()\n+        # repeat with batch size\n+        batch_size = pixel_values.shape[0] if pixel_values is not None else image_embeddings[-1].shape[0]\n+        image_positional_embeddings = image_positional_embeddings.repeat(batch_size, 1, 1, 1)\n+\n+        vision_attentions = None\n+        vision_hidden_states = None\n+\n+        if pixel_values is not None:\n+            feature_maps, _, vision_hidden_states, vision_attentions = self.get_image_features(\n+                pixel_values,\n+                **kwargs,\n+            )\n+\n+            # add no memory embedding to the last feature map\n+            feature_maps[-1] = feature_maps[-1] + self.no_memory_embedding\n+\n+            # reshape feature maps to the same shape as the backbone feature sizes\n+            image_embeddings = [\n+                feat.permute(1, 2, 0).view(batch_size, -1, *feat_size)\n+                for feat, feat_size in zip(feature_maps, self.backbone_feature_sizes)\n+            ]\n+\n+        if input_points is not None and input_labels is None:\n+            input_labels = torch.ones_like(input_points[:, :, :, 0], dtype=torch.int, device=input_points.device)\n+\n+        if input_points is None and input_boxes is None:\n+            # If no points are provide, pad with an empty point (with label -1)\n+            input_points = torch.zeros(\n+                batch_size, 1, 1, 2, dtype=image_embeddings[-1].dtype, device=image_embeddings[-1].device\n+            )\n+            input_labels = -torch.ones(batch_size, 1, 1, dtype=torch.int32, device=image_embeddings[-1].device)\n+\n+        if input_masks is not None:\n+            # If mask_inputs is provided, downsize it into low-res mask input if needed\n+            # and feed it as a dense mask prompt into the SAM mask encoder\n+            if input_masks.shape[-2:] != self.prompt_encoder.mask_input_size:\n+                input_masks = F.interpolate(\n+                    input_masks.float(),\n+                    size=self.prompt_encoder.mask_input_size,\n+                    align_corners=False,\n+                    mode=\"bilinear\",\n+                    antialias=True,  # use antialias for downsampling\n+                ).to(input_masks.dtype)\n+\n+        sparse_embeddings, dense_embeddings = self.prompt_encoder(\n+            input_points=input_points,\n+            input_labels=input_labels,\n+            input_boxes=input_boxes,\n+            input_masks=input_masks,\n+        )\n+        low_res_multimasks, iou_scores, _, object_score_logits = self.mask_decoder(\n+            image_embeddings=image_embeddings[-1],\n+            image_positional_embeddings=image_positional_embeddings,\n+            sparse_prompt_embeddings=sparse_embeddings,\n+            dense_prompt_embeddings=dense_embeddings,\n+            multimask_output=multimask_output,\n+            high_resolution_features=image_embeddings[:-1],\n+            attention_similarity=attention_similarity,\n+            target_embedding=target_embedding,\n+            **kwargs,\n+        )\n+\n+        return Sam3TrackerImageSegmentationOutput(\n+            iou_scores=iou_scores,\n+            pred_masks=low_res_multimasks,\n+            object_score_logits=object_score_logits,\n+            image_embeddings=image_embeddings,\n+            vision_hidden_states=vision_hidden_states,\n+            vision_attentions=vision_attentions,\n+        )\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[\n+        list[torch.Tensor],\n+        list[torch.Tensor],\n+        Optional[tuple[torch.FloatTensor, ...]],\n+        Optional[tuple[torch.FloatTensor, ...]],\n+    ]:\n+        r\"\"\"\n+        Extract and preprocess image features using the vision encoder.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor`):\n+                Input pixel values of shape `(batch_size, num_channels, height, width)`.\n+\n+        Returns:\n+            `tuple`: A tuple containing:\n+                - feature_maps (`list[torch.Tensor]`): List of feature maps from different levels.\n+                - feature_maps_position_embeddings (`list[torch.Tensor]`): List of positional embeddings for each feature level.\n+                - vision_hidden_states (`tuple[torch.FloatTensor]`, *optional*): Hidden states from the vision encoder.\n+                - vision_attentions (`tuple[torch.FloatTensor]`, *optional*): Attention weights from the vision encoder.\n+        \"\"\"\n+        vision_outputs: Sam3TrackerVisionEncoderOutput = self.vision_encoder(\n+            pixel_values,\n+            **kwargs,\n+        )\n+\n+        feature_maps = vision_outputs.fpn_hidden_states\n+        feature_maps_position_embeddings = vision_outputs.fpn_position_encoding\n+\n+        # precompute projected level 0 and level 1 features in SAM decoder\n+        # to avoid running it again on every SAM click\n+        feature_maps = list(feature_maps)\n+        feature_maps[0] = self.mask_decoder.conv_s0(feature_maps[0])\n+        feature_maps[1] = self.mask_decoder.conv_s1(feature_maps[1])\n+\n+        # flatten NxCxHxW to HWxNxC\n+        feature_maps = [feature_map.flatten(2).permute(2, 0, 1) for feature_map in feature_maps]\n+        feature_maps_position_embeddings = [\n+            feature_map_position_embedding.flatten(2).permute(2, 0, 1)\n+            for feature_map_position_embedding in feature_maps_position_embeddings\n+        ]\n+\n+        return feature_maps, feature_maps_position_embeddings, vision_outputs.hidden_states, vision_outputs.attentions\n+\n+\n+__all__ = [\"Sam3TrackerModel\", \"Sam3TrackerPreTrainedModel\"]"
        },
        {
            "sha": "dee826adef91765b367e30f0b733380d6f759a44",
            "filename": "src/transformers/models/sam3_tracker/modular_sam3_tracker.py",
            "status": "added",
            "additions": 227,
            "deletions": 0,
            "changes": 227,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fmodular_sam3_tracker.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fmodular_sam3_tracker.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fmodular_sam3_tracker.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,227 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import torch\n+\n+from ... import initialization as init\n+from ...configuration_utils import PreTrainedConfig\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import auto_docstring\n+from ..auto import CONFIG_MAPPING, AutoModel\n+from ..sam2.configuration_sam2 import (\n+    Sam2Config,\n+    Sam2MaskDecoderConfig,\n+    Sam2PromptEncoderConfig,\n+)\n+from ..sam2.modeling_sam2 import (\n+    Sam2Attention,\n+    Sam2FeedForward,\n+    Sam2ImageSegmentationOutput,\n+    Sam2LayerNorm,\n+    Sam2MaskDecoder,\n+    Sam2MaskEmbedding,\n+    Sam2Model,\n+    Sam2PositionalEmbedding,\n+    Sam2PreTrainedModel,\n+    Sam2PromptEncoder,\n+    Sam2TwoWayAttentionBlock,\n+    Sam2TwoWayTransformer,\n+)\n+from ..sam2.processing_sam2 import Sam2Processor\n+\n+\n+class Sam3TrackerPromptEncoderConfig(Sam2PromptEncoderConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Sam3TrackerPromptEncoder`]. The [`Sam3TrackerPromptEncoder`]\n+    module is used to encode the input 2D points and bounding boxes.\n+\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 256):\n+            Dimensionality of the hidden states.\n+        image_size (`int`, *optional*, defaults to 1008):\n+            The expected output resolution of the image.\n+        patch_size (`int`, *optional*, defaults to 14):\n+            The size (resolution) of each patch.\n+        mask_input_channels (`int`, *optional*, defaults to 16):\n+            The number of channels to be fed to the `MaskDecoder` module.\n+        num_point_embeddings (`int`, *optional*, defaults to 4):\n+            The number of point embeddings to be used.\n+        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function in the encoder and pooler.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        scale (`float`, *optional*, defaults to 1):\n+            The scale factor for the prompt encoder.\n+    \"\"\"\n+\n+    base_config_key = \"prompt_encoder_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=256,\n+        image_size=1008,\n+        patch_size=14,\n+        mask_input_channels=16,\n+        num_point_embeddings=4,\n+        hidden_act=\"gelu\",\n+        layer_norm_eps=1e-6,\n+        scale=1,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+\n+class Sam3TrackerProcessor(Sam2Processor):\n+    pass\n+\n+\n+class Sam3TrackerMaskDecoderConfig(Sam2MaskDecoderConfig):\n+    pass\n+\n+\n+class Sam3TrackerConfig(Sam2Config):\n+    def __init__(\n+        self,\n+        vision_config=None,\n+        prompt_encoder_config=None,\n+        mask_decoder_config=None,\n+        initializer_range=0.02,\n+        **kwargs,\n+    ):\n+        vision_config = (\n+            vision_config\n+            if vision_config is not None\n+            else {\"backbone_feature_sizes\": [[288, 288], [144, 144], [72, 72]]}\n+        )\n+        prompt_encoder_config = prompt_encoder_config if prompt_encoder_config is not None else {}\n+        mask_decoder_config = mask_decoder_config if mask_decoder_config is not None else {}\n+\n+        if isinstance(vision_config, dict):\n+            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"sam3_vision_model\")\n+            vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n+        if isinstance(prompt_encoder_config, Sam3TrackerPromptEncoderConfig):\n+            prompt_encoder_config = prompt_encoder_config.to_dict()\n+        if isinstance(mask_decoder_config, Sam3TrackerMaskDecoderConfig):\n+            mask_decoder_config = mask_decoder_config.to_dict()\n+\n+        self.vision_config = vision_config\n+        self.prompt_encoder_config = Sam3TrackerPromptEncoderConfig(**prompt_encoder_config)\n+        self.mask_decoder_config = Sam3TrackerMaskDecoderConfig(**mask_decoder_config)\n+\n+        self.initializer_range = initializer_range\n+        PreTrainedConfig.__init__(**kwargs)\n+\n+\n+class Sam3TrackerImageSegmentationOutput(Sam2ImageSegmentationOutput):\n+    pass\n+\n+\n+class Sam3TrackerFeedForward(Sam2FeedForward):\n+    pass\n+\n+\n+@auto_docstring\n+class Sam3TrackerPreTrainedModel(Sam2PreTrainedModel):\n+    @torch.no_grad()\n+    def _init_weights(self, module):\n+        PreTrainedModel._init_weights(module)\n+        if isinstance(module, Sam3TrackerModel):\n+            if module.no_memory_embedding is not None:\n+                init.zeros_(module.no_memory_embedding)\n+\n+\n+class Sam3TrackerPositionalEmbedding(Sam2PositionalEmbedding):\n+    pass\n+\n+\n+class Sam3TrackerMaskEmbedding(Sam2MaskEmbedding):\n+    pass\n+\n+\n+class Sam3TrackerPromptEncoder(Sam2PromptEncoder):\n+    pass\n+\n+\n+class Sam3TrackerAttention(Sam2Attention):\n+    pass\n+\n+\n+class Sam3TrackerTwoWayAttentionBlock(Sam2TwoWayAttentionBlock):\n+    pass\n+\n+\n+class Sam3TrackerTwoWayTransformer(Sam2TwoWayTransformer):\n+    pass\n+\n+\n+class Sam3TrackerLayerNorm(Sam2LayerNorm):\n+    pass\n+\n+\n+class Sam3TrackerMaskDecoder(Sam2MaskDecoder):\n+    pass\n+\n+\n+class Sam3TrackerModel(Sam2Model):\n+    _checkpoint_conversion_mapping = {\n+        \"tracker_model.\": \"\",\n+        \"detector_model.vision_encoder.backbone.\": \"vision_encoder.backbone.\",\n+        \"tracker_neck.\": \"vision_encoder.neck.\",\n+    }\n+    _keys_to_ignore_on_load_unexpected = [\n+        r\"^detector_model.\",\n+        r\"^memory_.*\",\n+        r\"^mask_downsample.*\",\n+        r\"^object_pointer_proj.*\",\n+        r\"^temporal_positional_encoding_projection_layer.*\",\n+        \"no_memory_positional_encoding\",\n+        \"no_object_pointer\",\n+        \"occlusion_spatial_embedding_parameter\",\n+    ]\n+\n+    def __init__(self, config: Sam3TrackerConfig):\n+        # loading from a sam3_video config\n+        if hasattr(config, \"tracker_config\") and config.tracker_config is not None:\n+            if isinstance(config.tracker_config, dict):\n+                config.tracker_config = Sam3TrackerConfig(**config.tracker_config)\n+            config = config.tracker_config\n+        Sam3TrackerPreTrainedModel.__init__(config)\n+        self.shared_image_embedding = Sam3TrackerPositionalEmbedding(config.prompt_encoder_config)\n+        self.vision_encoder = AutoModel.from_config(config.vision_config)\n+        self.prompt_encoder = Sam3TrackerPromptEncoder(config.prompt_encoder_config)\n+        # The module using it is not a PreTrainedModel subclass so we need this\n+        config.mask_decoder_config._attn_implementation = config._attn_implementation\n+        self.mask_decoder = Sam3TrackerMaskDecoder(config.mask_decoder_config)\n+\n+        self.backbone_feature_sizes = config.vision_config.backbone_feature_sizes\n+        # a single token to indicate no memory embedding from previous frames\n+        self.hidden_dim = config.vision_config.fpn_hidden_size\n+        self.no_memory_embedding = torch.nn.Parameter(torch.zeros(1, 1, self.hidden_dim))\n+\n+        self.post_init()\n+\n+\n+__all__ = [\n+    \"Sam3TrackerConfig\",\n+    \"Sam3TrackerPromptEncoderConfig\",\n+    \"Sam3TrackerMaskDecoderConfig\",\n+    \"Sam3TrackerProcessor\",\n+    \"Sam3TrackerModel\",\n+    \"Sam3TrackerPreTrainedModel\",\n+]"
        },
        {
            "sha": "96e123913936f7bdd6657026ca6d7870ee387f83",
            "filename": "src/transformers/models/sam3_tracker/processing_sam3_tracker.py",
            "status": "added",
            "additions": 521,
            "deletions": 0,
            "changes": 521,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fprocessing_sam3_tracker.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fprocessing_sam3_tracker.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fprocessing_sam3_tracker.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,521 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/sam3_tracker/modular_sam3_tracker.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_sam3_tracker.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from copy import deepcopy\n+from typing import Optional, Union\n+\n+import numpy as np\n+import torch\n+\n+from ...image_utils import ImageInput\n+from ...processing_utils import ProcessorMixin\n+from ...tokenization_utils_base import BatchEncoding\n+from ...utils import TensorType\n+from ...utils.import_utils import requires\n+\n+\n+@requires(backends=(\"torch\",))\n+class Sam3TrackerProcessor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a SAM3_TRACKER processor which wraps a SAM3_TRACKER image processor and an 2D points & Bounding boxes processor into a\n+    single processor.\n+\n+    [`Sam3TrackerProcessor`] offers all the functionalities of [`Sam3TrackerImageProcessorFast`] and [`Sam3TrackerVideoProcessor`]. See the docstring of\n+    [`~Sam3TrackerImageProcessorFast.__call__`] and [`~Sam3TrackerVideoProcessor.__call__`] for more information.\n+\n+    Args:\n+        image_processor (`Sam3TrackerImageProcessorFast`):\n+            An instance of [`Sam3TrackerImageProcessorFast`].\n+        target_size (`int`, *optional*):\n+            The target size (target_size, target_size) to which the image will be resized.\n+        point_pad_value (`int`, *optional*, defaults to -10):\n+            The value used for padding input points.\n+    \"\"\"\n+\n+    def __init__(self, image_processor, target_size: Optional[int] = None, point_pad_value: int = -10, **kwargs):\n+        super().__init__(image_processor, **kwargs)\n+        self.point_pad_value = point_pad_value\n+        self.target_size = target_size if target_size is not None else self.image_processor.size[\"height\"]\n+\n+    def __call__(\n+        self,\n+        images: Optional[ImageInput] = None,\n+        segmentation_maps: Optional[ImageInput] = None,\n+        input_points: Optional[Union[list[list[list[list[float]]]], torch.Tensor]] = None,\n+        input_labels: Optional[Union[list[list[list[int]]], torch.Tensor]] = None,\n+        input_boxes: Optional[Union[list[list[list[float]]], torch.Tensor]] = None,\n+        original_sizes: Optional[Union[list[list[float]], torch.Tensor]] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        **kwargs,\n+    ) -> BatchEncoding:\n+        r\"\"\"\n+        This method uses [`Sam3TrackerImageProcessorFast.__call__`] method to prepare image(s) for the model. It also prepares 2D\n+        points and bounding boxes for the model if they are provided.\n+\n+        Args:\n+            images (`ImageInput`, *optional*):\n+                The image(s) to process.\n+            segmentation_maps (`ImageInput`, *optional*):\n+                The segmentation maps to process.\n+            input_points (`list[list[list[list[float]]]]`, `torch.Tensor`, *optional*):\n+                The points to add to the frame.\n+            input_labels (`list[list[list[int]]]`, `torch.Tensor`, *optional*):\n+                The labels for the points.\n+            input_boxes (`list[list[list[float]]]`, `torch.Tensor`, *optional*):\n+                The bounding boxes to add to the frame.\n+            original_sizes (`list[list[float]]`, `torch.Tensor`, *optional*):\n+                The original sizes of the images.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return.\n+            **kwargs:\n+                Additional keyword arguments to pass to the image processor.\n+\n+        Returns:\n+            A [`BatchEncoding`] with the following fields:\n+            - `pixel_values` (`torch.Tensor`): The processed image(s).\n+            - `original_sizes` (`list[list[float]]`): The original sizes of the images.\n+            - `labels` (`torch.Tensor`): The processed segmentation maps (if provided).\n+            - `input_points` (`torch.Tensor`): The processed points.\n+            - `input_labels` (`torch.Tensor`): The processed labels.\n+            - `input_boxes` (`torch.Tensor`): The processed bounding boxes.\n+        \"\"\"\n+        if images is not None:\n+            encoding_image_processor = self.image_processor(\n+                images,\n+                segmentation_maps=segmentation_maps,\n+                return_tensors=return_tensors,\n+                **kwargs,\n+            )\n+        elif original_sizes is not None:\n+            if isinstance(original_sizes, torch.Tensor):\n+                original_sizes = original_sizes.cpu().tolist()\n+            encoding_image_processor = BatchEncoding({\"original_sizes\": original_sizes}, tensor_type=return_tensors)\n+        else:\n+            raise ValueError(\"Either images or original_sizes must be provided\")\n+\n+        # pop arguments that are not used in the forward but used nevertheless\n+        original_sizes = encoding_image_processor[\"original_sizes\"]\n+        # Check original_sizes is of length 1 or len(images)\n+        if images is not None and len(original_sizes) != 1 and len(original_sizes) != len(images):\n+            raise ValueError(\n+                \"original_sizes must be of length 1 or len(images). If you are passing a single image, you must pass a single original_size.\"\n+            )\n+\n+        # Process input points, labels, and boxes if provided\n+        if input_points is not None or input_labels is not None or input_boxes is not None:\n+            # Validate and convert inputs to standardized format\n+            processed_points = self._validate_single_input(\n+                input_points,\n+                expected_depth=4,\n+                input_name=\"points\",\n+                expected_format=\"[image level, object level, point level, point coordinates]\",\n+                expected_coord_size=2,\n+            )\n+            processed_labels = self._validate_single_input(\n+                input_labels,\n+                expected_depth=3,\n+                input_name=\"labels\",\n+                expected_format=\"[image level, object level, point level]\",\n+            )\n+            processed_boxes = self._validate_single_input(\n+                input_boxes,\n+                expected_depth=3,\n+                input_name=\"boxes\",\n+                expected_format=\"[image level, box level, box coordinates]\",\n+                expected_coord_size=4,\n+            )\n+\n+            # Get padding requirements for all inputs\n+            if processed_points is not None:\n+                points_max_dims = self._get_nested_dimensions(processed_points)[:3]\n+            if processed_labels is not None:\n+                labels_max_dims = self._get_nested_dimensions(processed_labels)[:3]\n+            if processed_boxes is not None:\n+                boxes_max_dims = self._get_nested_dimensions(processed_boxes)[:2]\n+\n+            # Ensure points and labels have consistent dimensions\n+            if processed_points is not None and processed_labels is not None:\n+                if points_max_dims != labels_max_dims:\n+                    raise ValueError(\n+                        \"Input points and labels have inconsistent dimensions. Please ensure they have the same dimensions.\"\n+                    )\n+\n+            # Check that boxes don't need padding (model limitation)\n+            if processed_boxes is not None and len(processed_boxes) >= 2:\n+                if any(len(img_boxes) < boxes_max_dims[1] for img_boxes in processed_boxes):\n+                    raise ValueError(\n+                        \"Input boxes have inconsistent dimensions that would require padding, \"\n+                        \"but boxes cannot be padded due to model limitations. \"\n+                        \"Please ensure all images have the same number of boxes.\"\n+                    )\n+\n+            # Pad and normalize all inputs to final tensor format\n+            if processed_points is not None:\n+                padded_points = self._pad_nested_list(processed_points, points_max_dims + [2])\n+                final_points = torch.tensor(padded_points, dtype=torch.float32)\n+                self._normalize_tensor_coordinates(final_points, original_sizes, preserve_padding=True)\n+                encoding_image_processor.update({\"input_points\": final_points})\n+\n+            if processed_labels is not None:\n+                padded_labels = self._pad_nested_list(processed_labels, labels_max_dims)\n+                final_labels = torch.tensor(padded_labels, dtype=torch.int64)\n+                encoding_image_processor.update({\"input_labels\": final_labels})\n+\n+            if processed_boxes is not None:\n+                final_boxes = torch.tensor(processed_boxes, dtype=torch.float32)\n+                self._normalize_tensor_coordinates(final_boxes, original_sizes, is_bounding_box=True)\n+                encoding_image_processor.update({\"input_boxes\": final_boxes})\n+\n+        return encoding_image_processor\n+\n+    def _normalize_coordinates(\n+        self, target_size: int, coords: \"torch.Tensor\", original_size, is_bounding_box=False\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Expects a numpy array of length 2 in the final dimension. Requires the original image size in (H, W) format.\n+\n+        Args:\n+            target_size (`int`):\n+                The target size of the image.\n+            coords (`torch.Tensor`):\n+                The coordinates to be normalized.\n+            original_size (`tuple`):\n+                The original size of the image.\n+            is_bounding_box (`bool`, *optional*, defaults to `False`):\n+                Whether the coordinates are bounding boxes.\n+        \"\"\"\n+        old_h, old_w = original_size\n+        new_h, new_w = target_size, target_size\n+        coords = deepcopy(coords).float()\n+\n+        if is_bounding_box:\n+            coords = coords.reshape(-1, 2, 2)\n+        coords[..., 0] = coords[..., 0] * (new_w / old_w)\n+        coords[..., 1] = coords[..., 1] * (new_h / old_h)\n+\n+        if is_bounding_box:\n+            coords = coords.reshape(-1, 4)\n+\n+        return coords\n+\n+    def _convert_to_nested_list(self, data, expected_depth, current_depth=0):\n+        \"\"\"\n+        Recursively convert various input formats (tensors, numpy arrays, lists) to nested lists.\n+\n+        Args:\n+            data: Input data in any format\n+            expected_depth: Expected nesting depth\n+            current_depth: Current depth in recursion\n+\n+        Returns:\n+            Nested list representation of the data\n+        \"\"\"\n+        if data is None:\n+            return None\n+\n+        # Convert tensor/numpy to list if we're at a leaf level or if it's a multi-dimensional array\n+        if isinstance(data, torch.Tensor):  # PyTorch tensor\n+            if current_depth == expected_depth - 2 or len(data.shape) <= 2:  # At coordinate level or small tensor\n+                return data.numpy().tolist()\n+            else:\n+                return [self._convert_to_nested_list(item, expected_depth, current_depth + 1) for item in data]\n+        elif isinstance(data, np.ndarray):  # NumPy array\n+            if current_depth == expected_depth - 2 or len(data.shape) <= 2:  # At coordinate level or small array\n+                return data.tolist()\n+            else:\n+                return [self._convert_to_nested_list(item, expected_depth, current_depth + 1) for item in data]\n+        elif isinstance(data, list):\n+            if current_depth == expected_depth:\n+                # We've reached the expected depth, return as is\n+                return data\n+            else:\n+                # Continue recursion\n+                return [self._convert_to_nested_list(item, expected_depth, current_depth + 1) for item in data]\n+        elif isinstance(data, (int, float)):\n+            return data\n+        else:\n+            raise TypeError(f\"Unsupported data type: {type(data)}\")\n+\n+    def _get_nested_dimensions(self, nested_list, max_dims=None):\n+        \"\"\"\n+        Get the maximum dimensions at each level of nesting.\n+\n+        Args:\n+            nested_list (`list`):\n+                Nested list structure.\n+            max_dims (`list`, *optional*):\n+                Current maximum dimensions (for recursion).\n+\n+        Returns:\n+            `list`: A list of maximum dimensions for each nesting level.\n+        \"\"\"\n+        if max_dims is None:\n+            max_dims = []\n+\n+        if not isinstance(nested_list, list):\n+            return max_dims\n+\n+        if len(max_dims) == 0:\n+            max_dims.append(len(nested_list))\n+        else:\n+            max_dims[0] = max(max_dims[0], len(nested_list))\n+\n+        if len(nested_list) > 0:\n+            for item in nested_list:\n+                if isinstance(item, list):\n+                    sub_dims = self._get_nested_dimensions(item)\n+                    # Merge sub_dims into max_dims\n+                    for i, dim in enumerate(sub_dims):\n+                        if i + 1 >= len(max_dims):\n+                            max_dims.append(dim)\n+                        else:\n+                            max_dims[i + 1] = max(max_dims[i + 1], dim)\n+\n+        return max_dims\n+\n+    def _pad_nested_list(self, nested_list, target_dims, current_level=0, pad_value=None):\n+        \"\"\"\n+        Recursively pad a nested list to match target dimensions.\n+\n+        Args:\n+            nested_list (`list`):\n+                Nested list to pad.\n+            target_dims (`list`):\n+                Target dimensions for each level.\n+            current_level (`int`, *optional*, defaults to 0):\n+                Current nesting level.\n+            pad_value (`int`, *optional*):\n+                Value to use for padding.\n+\n+        Returns:\n+            `list`: The padded nested list.\n+        \"\"\"\n+        if pad_value is None:\n+            pad_value = self.point_pad_value\n+\n+        if current_level >= len(target_dims):\n+            return nested_list\n+\n+        # Ensure we have a list\n+        if not isinstance(nested_list, list):\n+            nested_list = [nested_list]\n+\n+        # Pad current level\n+        current_size = len(nested_list)\n+        target_size = target_dims[current_level]\n+\n+        # Pad with appropriate values\n+        if current_level == len(target_dims) - 1:\n+            # At the coordinate level, pad with pad_value\n+            nested_list.extend([pad_value] * (target_size - current_size))\n+        else:\n+            # At higher levels, pad with nested structures\n+            if current_size > 0:\n+                # Create appropriately sized template\n+                if current_level < len(target_dims) - 2:\n+                    # For non-coordinate levels, create empty nested structure\n+                    template_dims = target_dims[current_level + 1 :]\n+                    template = self._create_empty_nested_structure(template_dims, pad_value)\n+                else:\n+                    # For coordinate level, create list of pad_values\n+                    template = [pad_value] * target_dims[current_level + 1]\n+\n+                nested_list.extend([deepcopy(template) for _ in range(target_size - current_size)])\n+            else:\n+                # Create from scratch\n+                template_dims = target_dims[current_level + 1 :]\n+                template = self._create_empty_nested_structure(template_dims, pad_value)\n+                nested_list.extend([deepcopy(template) for _ in range(target_size)])\n+\n+        # Recursively pad sublists\n+        if current_level < len(target_dims) - 1:\n+            for i in range(len(nested_list)):\n+                if isinstance(nested_list[i], list):\n+                    nested_list[i] = self._pad_nested_list(nested_list[i], target_dims, current_level + 1, pad_value)\n+\n+        return nested_list\n+\n+    def _create_empty_nested_structure(self, dims, pad_value):\n+        \"\"\"\n+        Create an empty nested structure with given dimensions filled with pad_value.\n+\n+        Args:\n+            dims (`list`):\n+                The dimensions of the nested structure.\n+            pad_value (`int`):\n+                The value to fill the structure with.\n+        \"\"\"\n+        if len(dims) == 1:\n+            return [pad_value] * dims[0]\n+        else:\n+            return [self._create_empty_nested_structure(dims[1:], pad_value) for _ in range(dims[0])]\n+\n+    def _get_nesting_level(self, input_list):\n+        \"\"\"\n+        Get the nesting level of a list structure.\n+\n+        Args:\n+            input_list (`list`):\n+                The list to get the nesting level of.\n+        \"\"\"\n+        if isinstance(input_list, list):\n+            if len(input_list) == 0:\n+                return 1\n+            return 1 + self._get_nesting_level(input_list[0])\n+        elif isinstance(input_list, (np.ndarray, torch.Tensor)):\n+            # For arrays/tensors, the nesting level is the number of dimensions\n+            return len(input_list.shape)\n+        return 0\n+\n+    def _validate_single_input(\n+        self,\n+        data: Union[torch.Tensor, np.ndarray, list],\n+        expected_depth: int,\n+        input_name: str,\n+        expected_format: str,\n+        expected_coord_size: Optional[int] = None,\n+    ) -> list:\n+        \"\"\"\n+                Validate a single input by ensuring proper nesting and raising an error if the input is not valid.\n+\n+                Args:\n+                    data (`torch.Tensor`, `np.ndarray`, or `list`):\n+                        Input data to process.\n+                    expected_depth (`int`):\n+                        Expected nesting depth.\n+                    input_name (`str`):\n+                        Name of the input for error messages.\n+                    expected_format (`str`):\n+                        The expected format of the input.\n+                    expected_coord_size (`int`, *optional*):\n+                        Expected coordinate size (2 for points, 4 for boxes, None for labels).\n+        .\n+        \"\"\"\n+        if data is None:\n+            return None\n+\n+        # Handle tensors and numpy arrays first\n+        if isinstance(data, (torch.Tensor, np.ndarray)):\n+            # For tensors/arrays, we can directly check the number of dimensions\n+            if data.ndim != expected_depth:\n+                raise ValueError(\n+                    f\"Input {input_name} must be a tensor/array with {expected_depth} dimensions. The expected nesting format is {expected_format}. Got {data.ndim} dimensions.\"\n+                )\n+            elif expected_coord_size is not None:\n+                if data.shape[-1] != expected_coord_size:\n+                    raise ValueError(\n+                        f\"Input {input_name} must be a tensor/array with {expected_coord_size} as the last dimension, got {data.shape[-1]}.\"\n+                    )\n+            return self._convert_to_nested_list(data, expected_depth)\n+\n+        # Handle nested lists\n+        if isinstance(data, list):\n+            current_depth = self._get_nesting_level(data)\n+            if current_depth != expected_depth:\n+                raise ValueError(\n+                    f\"Input {input_name} must be a nested list with {expected_depth} levels. The expected nesting format is {expected_format}. Got {current_depth} levels.\"\n+                )\n+            return self._convert_to_nested_list(data, expected_depth)\n+\n+    def _normalize_tensor_coordinates(self, tensor, original_sizes, is_bounding_box=False, preserve_padding=False):\n+        \"\"\"\n+        Helper method to normalize coordinates in a tensor across multiple images.\n+\n+        Args:\n+            tensor (`torch.Tensor`):\n+                Input tensor with coordinates.\n+            original_sizes (`list`):\n+                Original image sizes.\n+            is_bounding_box (`bool`, *optional*, defaults to `False`):\n+                Whether coordinates are bounding boxes.\n+            preserve_padding (`bool`, *optional*, defaults to `False`):\n+                Whether to preserve padding values (for points).\n+        \"\"\"\n+        if preserve_padding:\n+            # For points: avoid normalizing pad values\n+            mask = tensor != self.point_pad_value\n+            coord_mask = mask.all(dim=-1, keepdim=True)\n+\n+        for img_idx in range(len(original_sizes)):\n+            if img_idx < tensor.shape[0]:\n+                original_size = original_sizes[img_idx] if img_idx < len(original_sizes) else original_sizes[0]\n+                normalized_coords = self._normalize_coordinates(\n+                    self.target_size, tensor[img_idx], original_size, is_bounding_box=is_bounding_box\n+                )\n+\n+                if preserve_padding:\n+                    # Only update non-padded values\n+                    img_mask = coord_mask[img_idx]\n+                    tensor[img_idx] = torch.where(\n+                        img_mask.expand_as(tensor[img_idx]), normalized_coords, tensor[img_idx]\n+                    )\n+                else:\n+                    tensor[img_idx] = normalized_coords\n+\n+    def post_process_masks(\n+        self,\n+        masks,\n+        original_sizes,\n+        mask_threshold=0.0,\n+        binarize=True,\n+        max_hole_area=0.0,\n+        max_sprinkle_area=0.0,\n+        apply_non_overlapping_constraints=False,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Remove padding and upscale masks to the original image size.\n+\n+        Args:\n+            masks (`Union[List[torch.Tensor], List[np.ndarray]]`):\n+                Batched masks from the mask_decoder in (batch_size, num_channels, height, width) format.\n+            original_sizes (`Union[torch.Tensor, List[Tuple[int,int]]]`):\n+                The original sizes of each image before it was resized to the model's expected input shape, in (height,\n+                width) format.\n+            mask_threshold (`float`, *optional*, defaults to 0.0):\n+                Threshold for binarization and post-processing operations.\n+            binarize (`bool`, *optional*, defaults to `True`):\n+                Whether to binarize the masks.\n+            max_hole_area (`float`, *optional*, defaults to 0.0):\n+                The maximum area of a hole to fill.\n+            max_sprinkle_area (`float`, *optional*, defaults to 0.0):\n+                The maximum area of a sprinkle to fill.\n+            apply_non_overlapping_constraints (`bool`, *optional*, defaults to `False`):\n+                Whether to apply non-overlapping constraints to the masks.\n+\n+        Returns:\n+            (`torch.Tensor`): Batched masks in batch_size, num_channels, height, width) format, where (height, width)\n+            is given by original_size.\n+        \"\"\"\n+        return self.image_processor.post_process_masks(\n+            masks,\n+            original_sizes,\n+            mask_threshold,\n+            binarize,\n+            max_hole_area,\n+            max_sprinkle_area,\n+            apply_non_overlapping_constraints,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"Sam3TrackerProcessor\"]"
        },
        {
            "sha": "8a8da808bee439394a3f2b2d6cf1430f117a18c1",
            "filename": "src/transformers/models/sam3_tracker_video/__init__.py",
            "status": "added",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2F__init__.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,30 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_sam3_tracker_video import *\n+    from .modeling_sam3_tracker_video import *\n+    from .processing_sam3_tracker_video import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "08561f270b276b1240c293fb8970bbb5ae8fe35c",
            "filename": "src/transformers/models/sam3_tracker_video/configuration_sam3_tracker_video.py",
            "status": "added",
            "additions": 401,
            "deletions": 0,
            "changes": 401,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fconfiguration_sam3_tracker_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fconfiguration_sam3_tracker_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fconfiguration_sam3_tracker_video.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,401 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/sam3_tracker_video/modular_sam3_tracker_video.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_sam3_tracker_video.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from ...configuration_utils import PreTrainedConfig\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+class Sam3TrackerVideoPromptEncoderConfig(PreTrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Sam3TrackerVideoPromptEncoder`]. The [`Sam3TrackerVideoPromptEncoder`]\n+    module is used to encode the input 2D points and bounding boxes.\n+\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 256):\n+            Dimensionality of the hidden states.\n+        image_size (`int`, *optional*, defaults to 1008):\n+            The expected output resolution of the image.\n+        patch_size (`int`, *optional*, defaults to 14):\n+            The size (resolution) of each patch.\n+        mask_input_channels (`int`, *optional*, defaults to 16):\n+            The number of channels to be fed to the `MaskDecoder` module.\n+        num_point_embeddings (`int`, *optional*, defaults to 4):\n+            The number of point embeddings to be used.\n+        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function in the encoder and pooler.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        scale (`float`, *optional*, defaults to 1):\n+            The scale factor for the prompt encoder.\n+    \"\"\"\n+\n+    base_config_key = \"prompt_encoder_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=256,\n+        image_size=1008,\n+        patch_size=14,\n+        mask_input_channels=16,\n+        num_point_embeddings=4,\n+        hidden_act=\"gelu\",\n+        layer_norm_eps=1e-6,\n+        scale=1,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.hidden_size = hidden_size\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.mask_input_channels = mask_input_channels\n+        self.num_point_embeddings = num_point_embeddings\n+        self.hidden_act = hidden_act\n+        self.layer_norm_eps = layer_norm_eps\n+        self.scale = scale\n+\n+\n+class Sam3TrackerVideoMaskDecoderConfig(PreTrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Sam3TrackerVideoMaskDecoder`]. It is used to instantiate a SAM3_TRACKER_VIDEO\n+    memory encoder according to the specified arguments, defining the model architecture.\n+\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 256):\n+            Dimensionality of the hidden states.\n+        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function in the SAM3_TRACKER_VIDEO mask decoder.\n+        mlp_dim (`int`, *optional*, defaults to 2048):\n+            The dimension of the MLP in the two-way transformer.\n+        num_hidden_layers (`int`, *optional*, defaults to 2):\n+            The number of hidden layers in the two-way transformer.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            The number of attention heads in the two-way transformer.\n+        attention_downsample_rate (`int`, *optional*, defaults to 2):\n+            The downsample rate for the attention layers.\n+        num_multimask_outputs (`int`, *optional*, defaults to 3):\n+            The number of multimask outputs.\n+        iou_head_depth (`int`, *optional*, defaults to 3):\n+            The depth of the IoU head.\n+        iou_head_hidden_dim (`int`, *optional*, defaults to 256):\n+            The hidden dimension of the IoU head.\n+        dynamic_multimask_via_stability (`bool`, *optional*, defaults to `True`):\n+            Whether to use dynamic multimask via stability.\n+        dynamic_multimask_stability_delta (`float`, *optional*, defaults to 0.05):\n+            The stability delta for the dynamic multimask.\n+        dynamic_multimask_stability_thresh (`float`, *optional*, defaults to 0.98):\n+            The stability threshold for the dynamic multimask.\n+\n+    \"\"\"\n+\n+    base_config_key = \"mask_decoder_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=256,\n+        hidden_act=\"gelu\",\n+        mlp_dim=2048,\n+        num_hidden_layers=2,\n+        num_attention_heads=8,\n+        attention_downsample_rate=2,\n+        num_multimask_outputs=3,\n+        iou_head_depth=3,\n+        iou_head_hidden_dim=256,\n+        dynamic_multimask_via_stability=True,\n+        dynamic_multimask_stability_delta=0.05,\n+        dynamic_multimask_stability_thresh=0.98,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.hidden_size = hidden_size\n+        self.num_multimask_outputs = num_multimask_outputs\n+        self.hidden_act = hidden_act\n+        self.iou_head_depth = iou_head_depth\n+        self.iou_head_hidden_dim = iou_head_hidden_dim\n+        self.dynamic_multimask_via_stability = dynamic_multimask_via_stability\n+        self.dynamic_multimask_stability_delta = dynamic_multimask_stability_delta\n+        self.dynamic_multimask_stability_thresh = dynamic_multimask_stability_thresh\n+\n+        # TwoWayTransformer configuration\n+        self.num_hidden_layers = num_hidden_layers\n+        self.hidden_size = hidden_size\n+        self.num_attention_heads = num_attention_heads\n+        self.mlp_dim = mlp_dim\n+        self.attention_downsample_rate = attention_downsample_rate\n+\n+\n+class Sam3TrackerVideoConfig(PreTrainedConfig):\n+    r\"\"\"\n+    [`Sam3TrackerVideoConfig`] is the configuration class to store the configuration of a [`Sam3TrackerVideoModel`]. It is used to instantiate a\n+    SAM3 tracker video model according to the specified arguments, defining the memory attention, memory encoder, and image encoder\n+    configs. Instantiating a configuration defaults will yield a similar configuration to that of the SAM 3\n+    [facebook/sam3](https://huggingface.co/facebook/sam3) architecture.\n+\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+        vision_config (Union[`dict`, `Sam3TrackerVideoVisionConfig`], *optional*):\n+            Dictionary of configuration options used to initialize [`Sam3TrackerVideoVisionConfig`].\n+        prompt_encoder_config (Union[`dict`, `Sam3TrackerVideoPromptEncoderConfig`], *optional*):\n+            Dictionary of configuration options used to initialize [`Sam3TrackerVideoPromptEncoderConfig`].\n+        mask_decoder_config (Union[`dict`, `Sam3TrackerVideoMaskDecoderConfig`], *optional*):\n+            Dictionary of configuration options used to initialize [`Sam3TrackerVideoMaskDecoderConfig`].\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            Standard deviation for parameter initialization.\n+        num_maskmem (`int`, *optional*, defaults to 7):\n+            The number of memory slots for the mask memory.\n+        image_size (`int`, *optional*, defaults to 1008):\n+            The size of the input images.\n+        sigmoid_scale_for_mem_enc (`float`, *optional*, defaults to 20.0):\n+            Scale factor for the sigmoid function in the memory encoder.\n+        sigmoid_bias_for_mem_enc (`float`, *optional*, defaults to -10.0):\n+            Bias for the sigmoid function in the memory encoder.\n+        enable_occlusion_spatial_embedding (`bool`, *optional*, defaults to `True`):\n+            Whether to enable spatial embedding for occlusions.\n+        multimask_output_in_sam (`bool`, *optional*, defaults to `True`):\n+            Whether to output multiple masks from the SAM head.\n+        multimask_min_pt_num (`int`, *optional*, defaults to 0):\n+            The minimum number of points to trigger multimask output.\n+        multimask_max_pt_num (`int`, *optional*, defaults to 1):\n+            The maximum number of points to trigger multimask output.\n+        multimask_output_for_tracking (`bool`, *optional*, defaults to `True`):\n+            Whether to use multimask output for tracking.\n+        max_object_pointers_in_encoder (`int`, *optional*, defaults to 16):\n+            The maximum number of object pointers in the encoder.\n+        max_cond_frame_num (`int`, *optional*, defaults to 4):\n+            Maximum number of conditioning frames to use in memory attention.\n+        enable_temporal_pos_encoding_for_object_pointers (`bool`, *optional*, defaults to `True`):\n+            Whether to enable temporal positional encoding for object pointers.\n+        memory_attention_hidden_size (`int`, *optional*, defaults to 256):\n+            Dimensionality of the memory attention hidden states.\n+        memory_attention_num_layers (`int`, *optional*, defaults to 4):\n+            The number of layers in the memory attention module.\n+        memory_attention_num_attention_heads (`int`, *optional*, defaults to 1):\n+            Number of attention heads for each attention layer in the memory attention.\n+        memory_attention_downsample_rate (`int`, *optional*, defaults to 1):\n+            The downsample rate for the attention layers.\n+        memory_attention_feed_forward_hidden_size (`int`, *optional*, defaults to 2048):\n+            The dimension of the feedforward network in the memory attention module.\n+        memory_attention_feed_forward_hidden_act (`str`, *optional*, defaults to `\"relu\"`):\n+            The non-linear activation function in the feedforward network in the memory attention module.\n+        memory_attention_dropout (`float`, *optional*, defaults to 0.1):\n+            The dropout rate for the memory attention module.\n+        memory_attention_rope_theta (`float`, *optional*, defaults to 10000):\n+            The Rope theta parameter.\n+        memory_attention_rope_feat_sizes (`list[int]`, *optional*, defaults to `[72, 72]`):\n+            The feature sizes for the Rope positional encoding.\n+        memory_attention_rope_dropout (`float`, *optional*, defaults to 0.1):\n+                The dropout rate for the Rope positional encoding.\n+        memory_encoder_hidden_size (`int`, *optional*, defaults to 256):\n+            Dimensionality of the memory encoder hidden states.\n+        memory_encoder_output_channels (`int`, *optional*, defaults to 64):\n+            The number of output channels for the memory encoder.\n+        mask_downsampler_embed_dim (`int`, *optional*, defaults to 256):\n+            The dimension of the mask downsampler embedding.\n+        mask_downsampler_kernel_size (`int`, *optional*, defaults to 3):\n+            The kernel size for the mask downsampler.\n+        mask_downsampler_stride (`int`, *optional*, defaults to 2):\n+            The stride for the mask downsampler.\n+        mask_downsampler_padding (`int`, *optional*, defaults to 1):\n+            The padding for the mask downsampler.\n+        mask_downsampler_total_stride (`int`, *optional*, defaults to 16):\n+            The total stride for the mask downsampler.\n+        mask_downsampler_hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function in the mask downsampler.\n+        memory_fuser_num_layers (`int`, *optional*, defaults to 2):\n+            The number of layers in the memory fuser.\n+        memory_fuser_embed_dim (`int`, *optional*, defaults to 256):\n+            The dimension of the embedding layer in the memory fuser.\n+        memory_fuser_intermediate_dim (`int`, *optional*, defaults to 1024):\n+            The dimension of the intermediate layer in the memory fuser.\n+        memory_fuser_kernel_size (`int`, *optional*, defaults to 7):\n+            The kernel size for the memory fuser.\n+        memory_fuser_padding (`int`, *optional*, defaults to 3):\n+            The padding for the memory fuser.\n+        memory_fuser_layer_scale_init_value (`float`, *optional*, defaults to 1e-06):\n+            The initial value for the layer scale in the memory fuser.\n+        memory_fuser_hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function in the memory fuser.\n+        kwargs (*optional*):\n+            Dictionary of keyword arguments.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import (\n+    ...     Sam3VisionConfig,\n+    ...     Sam3TrackerVideoPromptEncoderConfig,\n+    ...     Sam3TrackerVideoMaskDecoderConfig,\n+    ...     Sam3TrackerVideoModel,\n+    ... )\n+\n+    >>> # Initializing a Sam3TrackerVideoConfig with `\"facebook/sam3\"` style configuration\n+    >>> configuration = Sam3TrackerVideoConfig()\n+\n+    >>> # Initializing a Sam3TrackerVideoModel (with random weights) from the `\"facebook/sam3\"` style configuration\n+    >>> model = Sam3TrackerVideoModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+\n+    >>> # We can also initialize a Sam3TrackerVideoConfig from a Sam3TrackerVideoVisionConfig, Sam3TrackerVideoPromptEncoderConfig, and Sam3TrackerVideoMaskDecoderConfig\n+\n+    >>> # Initializing SAM3 tracker video vision encoder, memory attention, and memory encoder configurations\n+    >>> vision_config = Sam3TrackerVideoVisionConfig()\n+    >>> prompt_encoder_config = Sam3TrackerVideoPromptEncoderConfig()\n+    >>> mask_decoder_config = Sam3TrackerVideoMaskDecoderConfig()\n+\n+    >>> config = Sam3TrackerVideoConfig(vision_config, prompt_encoder_config, mask_decoder_config)\n+    ```\"\"\"\n+\n+    model_type = \"sam3_tracker_video\"\n+    sub_configs = {\n+        \"vision_config\": AutoConfig,\n+        \"prompt_encoder_config\": Sam3TrackerVideoPromptEncoderConfig,\n+        \"mask_decoder_config\": Sam3TrackerVideoMaskDecoderConfig,\n+    }\n+\n+    def __init__(\n+        self,\n+        vision_config=None,\n+        prompt_encoder_config=None,\n+        mask_decoder_config=None,\n+        initializer_range=0.02,\n+        num_maskmem=7,\n+        image_size=1008,\n+        sigmoid_scale_for_mem_enc=20.0,\n+        sigmoid_bias_for_mem_enc=-10.0,\n+        enable_occlusion_spatial_embedding=True,\n+        multimask_output_in_sam=True,\n+        multimask_min_pt_num=0,\n+        multimask_max_pt_num=1,\n+        multimask_output_for_tracking=True,\n+        max_object_pointers_in_encoder=16,\n+        max_cond_frame_num=4,\n+        enable_temporal_pos_encoding_for_object_pointers=True,\n+        # memory attention\n+        memory_attention_hidden_size=256,\n+        memory_attention_num_layers=4,\n+        memory_attention_num_attention_heads=1,\n+        memory_attention_downsample_rate=1,\n+        memory_attention_feed_forward_hidden_size=2048,\n+        memory_attention_feed_forward_hidden_act=\"relu\",\n+        memory_attention_dropout=0.1,\n+        memory_attention_rope_theta=10000,\n+        memory_attention_rope_feat_sizes=None,\n+        memory_attention_rope_dropout=0.1,\n+        # memory encoder\n+        memory_encoder_hidden_size=256,\n+        memory_encoder_output_channels=64,\n+        mask_downsampler_embed_dim=256,\n+        mask_downsampler_kernel_size=3,\n+        mask_downsampler_stride=2,\n+        mask_downsampler_padding=1,\n+        mask_downsampler_total_stride=16,\n+        mask_downsampler_hidden_act=\"gelu\",\n+        memory_fuser_num_layers=2,\n+        memory_fuser_embed_dim=256,\n+        memory_fuser_intermediate_dim=1024,\n+        memory_fuser_kernel_size=7,\n+        memory_fuser_padding=3,\n+        memory_fuser_layer_scale_init_value=1e-6,\n+        memory_fuser_hidden_act=\"gelu\",\n+        **kwargs,\n+    ):\n+        vision_config = (\n+            vision_config\n+            if vision_config is not None\n+            else {\"backbone_feature_sizes\": [[288, 288], [144, 144], [72, 72]]}\n+        )\n+        prompt_encoder_config = prompt_encoder_config if prompt_encoder_config is not None else {}\n+        mask_decoder_config = mask_decoder_config if mask_decoder_config is not None else {}\n+        memory_attention_rope_feat_sizes = (\n+            [72, 72] if memory_attention_rope_feat_sizes is None else memory_attention_rope_feat_sizes\n+        )\n+\n+        if isinstance(vision_config, dict):\n+            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"sam3_vision_model\")\n+            vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n+        if isinstance(prompt_encoder_config, Sam3TrackerVideoPromptEncoderConfig):\n+            prompt_encoder_config = prompt_encoder_config.to_dict()\n+        if isinstance(mask_decoder_config, Sam3TrackerVideoMaskDecoderConfig):\n+            mask_decoder_config = mask_decoder_config.to_dict()\n+\n+        self.vision_config = vision_config\n+        self.prompt_encoder_config = Sam3TrackerVideoPromptEncoderConfig(**prompt_encoder_config)\n+        self.mask_decoder_config = Sam3TrackerVideoMaskDecoderConfig(**mask_decoder_config)\n+\n+        self.initializer_range = initializer_range\n+        self.num_maskmem = num_maskmem  # default 1 input frame + 6 previous frames\n+        self.image_size = image_size\n+        self.sigmoid_scale_for_mem_enc = sigmoid_scale_for_mem_enc\n+        self.sigmoid_bias_for_mem_enc = sigmoid_bias_for_mem_enc\n+        self.multimask_output_in_sam = multimask_output_in_sam\n+        self.multimask_min_pt_num = multimask_min_pt_num\n+        self.multimask_max_pt_num = multimask_max_pt_num\n+        self.multimask_output_for_tracking = multimask_output_for_tracking\n+        self.max_object_pointers_in_encoder = max_object_pointers_in_encoder\n+        self.max_cond_frame_num = max_cond_frame_num\n+        # The next 4 are True for sam2.1 and False for sam2\n+        self.enable_occlusion_spatial_embedding = enable_occlusion_spatial_embedding\n+        self.enable_temporal_pos_encoding_for_object_pointers = enable_temporal_pos_encoding_for_object_pointers\n+\n+        # memory attention\n+        self.memory_attention_hidden_size = memory_attention_hidden_size\n+        self.memory_attention_num_layers = memory_attention_num_layers\n+        self.memory_attention_num_attention_heads = memory_attention_num_attention_heads\n+        self.memory_attention_downsample_rate = memory_attention_downsample_rate\n+        self.memory_attention_feed_forward_hidden_size = memory_attention_feed_forward_hidden_size\n+        self.memory_attention_feed_forward_hidden_act = memory_attention_feed_forward_hidden_act\n+        self.memory_attention_dropout = memory_attention_dropout\n+        self.memory_attention_rope_theta = memory_attention_rope_theta\n+        self.memory_attention_rope_feat_sizes = memory_attention_rope_feat_sizes\n+        self.memory_attention_rope_dropout = memory_attention_rope_dropout\n+\n+        # memory encoder\n+        self.memory_encoder_hidden_size = memory_encoder_hidden_size\n+        self.memory_encoder_output_channels = memory_encoder_output_channels\n+        self.mask_downsampler_embed_dim = mask_downsampler_embed_dim\n+        self.mask_downsampler_kernel_size = mask_downsampler_kernel_size\n+        self.mask_downsampler_stride = mask_downsampler_stride\n+        self.mask_downsampler_padding = mask_downsampler_padding\n+        self.mask_downsampler_total_stride = mask_downsampler_total_stride\n+        self.mask_downsampler_hidden_act = mask_downsampler_hidden_act\n+        self.memory_fuser_num_layers = memory_fuser_num_layers\n+        self.memory_fuser_embed_dim = memory_fuser_embed_dim\n+        self.memory_fuser_intermediate_dim = memory_fuser_intermediate_dim\n+        self.memory_fuser_kernel_size = memory_fuser_kernel_size\n+        self.memory_fuser_padding = memory_fuser_padding\n+        self.memory_fuser_layer_scale_init_value = memory_fuser_layer_scale_init_value\n+        self.memory_fuser_hidden_act = memory_fuser_hidden_act\n+\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"Sam3TrackerVideoMaskDecoderConfig\", \"Sam3TrackerVideoPromptEncoderConfig\", \"Sam3TrackerVideoConfig\"]"
        },
        {
            "sha": "95056fee3fc1f52dc636ce3832474fd37501dc94",
            "filename": "src/transformers/models/sam3_tracker_video/modeling_sam3_tracker_video.py",
            "status": "added",
            "additions": 2806,
            "deletions": 0,
            "changes": 2806,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fmodeling_sam3_tracker_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fmodeling_sam3_tracker_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fmodeling_sam3_tracker_video.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a"
        },
        {
            "sha": "cf8fdcdf303a86227f654e0e748bc821a3999e43",
            "filename": "src/transformers/models/sam3_tracker_video/modular_sam3_tracker_video.py",
            "status": "added",
            "additions": 582,
            "deletions": 0,
            "changes": 582,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fmodular_sam3_tracker_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fmodular_sam3_tracker_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fmodular_sam3_tracker_video.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,582 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional\n+\n+import torch\n+\n+from ...configuration_utils import PreTrainedConfig\n+from ...processing_utils import Unpack\n+from ...utils.generic import TransformersKwargs\n+from ..auto import CONFIG_MAPPING, AutoConfig, AutoModel\n+from ..sam2_video.configuration_sam2_video import Sam2VideoMaskDecoderConfig, Sam2VideoPromptEncoderConfig\n+from ..sam2_video.modeling_sam2_video import (\n+    Sam2VideoAttention,\n+    Sam2VideoFeedForward,\n+    Sam2VideoImageSegmentationOutput,\n+    Sam2VideoInferenceCache,\n+    Sam2VideoInferenceSession,\n+    Sam2VideoLayerNorm,\n+    Sam2VideoMaskDecoder,\n+    Sam2VideoMaskDownSampler,\n+    Sam2VideoMaskDownSamplerLayer,\n+    Sam2VideoMaskEmbedding,\n+    Sam2VideoMemoryAttention,\n+    Sam2VideoMemoryAttentionLayer,\n+    Sam2VideoMemoryEncoder,\n+    Sam2VideoMemoryFuser,\n+    Sam2VideoMemoryFuserCXBlock,\n+    Sam2VideoModel,\n+    Sam2VideoPositionalEmbedding,\n+    Sam2VideoPositionEmbeddingSine,\n+    Sam2VideoPreTrainedModel,\n+    Sam2VideoPromptEncoder,\n+    Sam2VideoRoPEAttention,\n+    Sam2VideoSegmentationOutput,\n+    Sam2VideoTwoWayAttentionBlock,\n+    Sam2VideoTwoWayTransformer,\n+    Sam2VideoVisionEncoderOutput,\n+    Sam2VideoVisionRotaryEmbedding,\n+)\n+from ..sam2_video.processing_sam2_video import Sam2VideoProcessor\n+\n+\n+class Sam3TrackerVideoPromptEncoderConfig(Sam2VideoPromptEncoderConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Sam3TrackerVideoPromptEncoder`]. The [`Sam3TrackerVideoPromptEncoder`]\n+    module is used to encode the input 2D points and bounding boxes.\n+\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 256):\n+            Dimensionality of the hidden states.\n+        image_size (`int`, *optional*, defaults to 1008):\n+            The expected output resolution of the image.\n+        patch_size (`int`, *optional*, defaults to 14):\n+            The size (resolution) of each patch.\n+        mask_input_channels (`int`, *optional*, defaults to 16):\n+            The number of channels to be fed to the `MaskDecoder` module.\n+        num_point_embeddings (`int`, *optional*, defaults to 4):\n+            The number of point embeddings to be used.\n+        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function in the encoder and pooler.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        scale (`float`, *optional*, defaults to 1):\n+            The scale factor for the prompt encoder.\n+    \"\"\"\n+\n+    base_config_key = \"prompt_encoder_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=256,\n+        image_size=1008,\n+        patch_size=14,\n+        mask_input_channels=16,\n+        num_point_embeddings=4,\n+        hidden_act=\"gelu\",\n+        layer_norm_eps=1e-6,\n+        scale=1,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+\n+class Sam3TrackerVideoProcessor(Sam2VideoProcessor):\n+    pass\n+\n+\n+class Sam3TrackerVideoMaskDecoderConfig(Sam2VideoMaskDecoderConfig):\n+    pass\n+\n+\n+class Sam3TrackerVideoConfig(PreTrainedConfig):\n+    r\"\"\"\n+    [`Sam3TrackerVideoConfig`] is the configuration class to store the configuration of a [`Sam3TrackerVideoModel`]. It is used to instantiate a\n+    SAM3 tracker video model according to the specified arguments, defining the memory attention, memory encoder, and image encoder\n+    configs. Instantiating a configuration defaults will yield a similar configuration to that of the SAM 3\n+    [facebook/sam3](https://huggingface.co/facebook/sam3) architecture.\n+\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+        vision_config (Union[`dict`, `Sam3TrackerVideoVisionConfig`], *optional*):\n+            Dictionary of configuration options used to initialize [`Sam3TrackerVideoVisionConfig`].\n+        prompt_encoder_config (Union[`dict`, `Sam3TrackerVideoPromptEncoderConfig`], *optional*):\n+            Dictionary of configuration options used to initialize [`Sam3TrackerVideoPromptEncoderConfig`].\n+        mask_decoder_config (Union[`dict`, `Sam3TrackerVideoMaskDecoderConfig`], *optional*):\n+            Dictionary of configuration options used to initialize [`Sam3TrackerVideoMaskDecoderConfig`].\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            Standard deviation for parameter initialization.\n+        num_maskmem (`int`, *optional*, defaults to 7):\n+            The number of memory slots for the mask memory.\n+        image_size (`int`, *optional*, defaults to 1008):\n+            The size of the input images.\n+        sigmoid_scale_for_mem_enc (`float`, *optional*, defaults to 20.0):\n+            Scale factor for the sigmoid function in the memory encoder.\n+        sigmoid_bias_for_mem_enc (`float`, *optional*, defaults to -10.0):\n+            Bias for the sigmoid function in the memory encoder.\n+        enable_occlusion_spatial_embedding (`bool`, *optional*, defaults to `True`):\n+            Whether to enable spatial embedding for occlusions.\n+        multimask_output_in_sam (`bool`, *optional*, defaults to `True`):\n+            Whether to output multiple masks from the SAM head.\n+        multimask_min_pt_num (`int`, *optional*, defaults to 0):\n+            The minimum number of points to trigger multimask output.\n+        multimask_max_pt_num (`int`, *optional*, defaults to 1):\n+            The maximum number of points to trigger multimask output.\n+        multimask_output_for_tracking (`bool`, *optional*, defaults to `True`):\n+            Whether to use multimask output for tracking.\n+        max_object_pointers_in_encoder (`int`, *optional*, defaults to 16):\n+            The maximum number of object pointers in the encoder.\n+        max_cond_frame_num (`int`, *optional*, defaults to 4):\n+            Maximum number of conditioning frames to use in memory attention.\n+        enable_temporal_pos_encoding_for_object_pointers (`bool`, *optional*, defaults to `True`):\n+            Whether to enable temporal positional encoding for object pointers.\n+        memory_attention_hidden_size (`int`, *optional*, defaults to 256):\n+            Dimensionality of the memory attention hidden states.\n+        memory_attention_num_layers (`int`, *optional*, defaults to 4):\n+            The number of layers in the memory attention module.\n+        memory_attention_num_attention_heads (`int`, *optional*, defaults to 1):\n+            Number of attention heads for each attention layer in the memory attention.\n+        memory_attention_downsample_rate (`int`, *optional*, defaults to 1):\n+            The downsample rate for the attention layers.\n+        memory_attention_feed_forward_hidden_size (`int`, *optional*, defaults to 2048):\n+            The dimension of the feedforward network in the memory attention module.\n+        memory_attention_feed_forward_hidden_act (`str`, *optional*, defaults to `\"relu\"`):\n+            The non-linear activation function in the feedforward network in the memory attention module.\n+        memory_attention_dropout (`float`, *optional*, defaults to 0.1):\n+            The dropout rate for the memory attention module.\n+        memory_attention_rope_theta (`float`, *optional*, defaults to 10000):\n+            The Rope theta parameter.\n+        memory_attention_rope_feat_sizes (`list[int]`, *optional*, defaults to `[72, 72]`):\n+            The feature sizes for the Rope positional encoding.\n+        memory_attention_rope_dropout (`float`, *optional*, defaults to 0.1):\n+                The dropout rate for the Rope positional encoding.\n+        memory_encoder_hidden_size (`int`, *optional*, defaults to 256):\n+            Dimensionality of the memory encoder hidden states.\n+        memory_encoder_output_channels (`int`, *optional*, defaults to 64):\n+            The number of output channels for the memory encoder.\n+        mask_downsampler_embed_dim (`int`, *optional*, defaults to 256):\n+            The dimension of the mask downsampler embedding.\n+        mask_downsampler_kernel_size (`int`, *optional*, defaults to 3):\n+            The kernel size for the mask downsampler.\n+        mask_downsampler_stride (`int`, *optional*, defaults to 2):\n+            The stride for the mask downsampler.\n+        mask_downsampler_padding (`int`, *optional*, defaults to 1):\n+            The padding for the mask downsampler.\n+        mask_downsampler_total_stride (`int`, *optional*, defaults to 16):\n+            The total stride for the mask downsampler.\n+        mask_downsampler_hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function in the mask downsampler.\n+        memory_fuser_num_layers (`int`, *optional*, defaults to 2):\n+            The number of layers in the memory fuser.\n+        memory_fuser_embed_dim (`int`, *optional*, defaults to 256):\n+            The dimension of the embedding layer in the memory fuser.\n+        memory_fuser_intermediate_dim (`int`, *optional*, defaults to 1024):\n+            The dimension of the intermediate layer in the memory fuser.\n+        memory_fuser_kernel_size (`int`, *optional*, defaults to 7):\n+            The kernel size for the memory fuser.\n+        memory_fuser_padding (`int`, *optional*, defaults to 3):\n+            The padding for the memory fuser.\n+        memory_fuser_layer_scale_init_value (`float`, *optional*, defaults to 1e-06):\n+            The initial value for the layer scale in the memory fuser.\n+        memory_fuser_hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function in the memory fuser.\n+        kwargs (*optional*):\n+            Dictionary of keyword arguments.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import (\n+    ...     Sam3VisionConfig,\n+    ...     Sam3TrackerVideoPromptEncoderConfig,\n+    ...     Sam3TrackerVideoMaskDecoderConfig,\n+    ...     Sam3TrackerVideoModel,\n+    ... )\n+\n+    >>> # Initializing a Sam3TrackerVideoConfig with `\"facebook/sam3\"` style configuration\n+    >>> configuration = Sam3TrackerVideoConfig()\n+\n+    >>> # Initializing a Sam3TrackerVideoModel (with random weights) from the `\"facebook/sam3\"` style configuration\n+    >>> model = Sam3TrackerVideoModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+\n+    >>> # We can also initialize a Sam3TrackerVideoConfig from a Sam3TrackerVideoVisionConfig, Sam3TrackerVideoPromptEncoderConfig, and Sam3TrackerVideoMaskDecoderConfig\n+\n+    >>> # Initializing SAM3 tracker video vision encoder, memory attention, and memory encoder configurations\n+    >>> vision_config = Sam3TrackerVideoVisionConfig()\n+    >>> prompt_encoder_config = Sam3TrackerVideoPromptEncoderConfig()\n+    >>> mask_decoder_config = Sam3TrackerVideoMaskDecoderConfig()\n+\n+    >>> config = Sam3TrackerVideoConfig(vision_config, prompt_encoder_config, mask_decoder_config)\n+    ```\"\"\"\n+\n+    model_type = \"sam3_tracker_video\"\n+    sub_configs = {\n+        \"vision_config\": AutoConfig,\n+        \"prompt_encoder_config\": Sam3TrackerVideoPromptEncoderConfig,\n+        \"mask_decoder_config\": Sam3TrackerVideoMaskDecoderConfig,\n+    }\n+\n+    def __init__(\n+        self,\n+        vision_config=None,\n+        prompt_encoder_config=None,\n+        mask_decoder_config=None,\n+        initializer_range=0.02,\n+        num_maskmem=7,\n+        image_size=1008,\n+        sigmoid_scale_for_mem_enc=20.0,\n+        sigmoid_bias_for_mem_enc=-10.0,\n+        enable_occlusion_spatial_embedding=True,\n+        multimask_output_in_sam=True,\n+        multimask_min_pt_num=0,\n+        multimask_max_pt_num=1,\n+        multimask_output_for_tracking=True,\n+        max_object_pointers_in_encoder=16,\n+        max_cond_frame_num=4,\n+        enable_temporal_pos_encoding_for_object_pointers=True,\n+        # memory attention\n+        memory_attention_hidden_size=256,\n+        memory_attention_num_layers=4,\n+        memory_attention_num_attention_heads=1,\n+        memory_attention_downsample_rate=1,\n+        memory_attention_feed_forward_hidden_size=2048,\n+        memory_attention_feed_forward_hidden_act=\"relu\",\n+        memory_attention_dropout=0.1,\n+        memory_attention_rope_theta=10000,\n+        memory_attention_rope_feat_sizes=None,\n+        memory_attention_rope_dropout=0.1,\n+        # memory encoder\n+        memory_encoder_hidden_size=256,\n+        memory_encoder_output_channels=64,\n+        mask_downsampler_embed_dim=256,\n+        mask_downsampler_kernel_size=3,\n+        mask_downsampler_stride=2,\n+        mask_downsampler_padding=1,\n+        mask_downsampler_total_stride=16,\n+        mask_downsampler_hidden_act=\"gelu\",\n+        memory_fuser_num_layers=2,\n+        memory_fuser_embed_dim=256,\n+        memory_fuser_intermediate_dim=1024,\n+        memory_fuser_kernel_size=7,\n+        memory_fuser_padding=3,\n+        memory_fuser_layer_scale_init_value=1e-6,\n+        memory_fuser_hidden_act=\"gelu\",\n+        **kwargs,\n+    ):\n+        vision_config = (\n+            vision_config\n+            if vision_config is not None\n+            else {\"backbone_feature_sizes\": [[288, 288], [144, 144], [72, 72]]}\n+        )\n+        prompt_encoder_config = prompt_encoder_config if prompt_encoder_config is not None else {}\n+        mask_decoder_config = mask_decoder_config if mask_decoder_config is not None else {}\n+        memory_attention_rope_feat_sizes = (\n+            [72, 72] if memory_attention_rope_feat_sizes is None else memory_attention_rope_feat_sizes\n+        )\n+\n+        if isinstance(vision_config, dict):\n+            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"sam3_vision_model\")\n+            vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n+        if isinstance(prompt_encoder_config, Sam3TrackerVideoPromptEncoderConfig):\n+            prompt_encoder_config = prompt_encoder_config.to_dict()\n+        if isinstance(mask_decoder_config, Sam3TrackerVideoMaskDecoderConfig):\n+            mask_decoder_config = mask_decoder_config.to_dict()\n+\n+        self.vision_config = vision_config\n+        self.prompt_encoder_config = Sam3TrackerVideoPromptEncoderConfig(**prompt_encoder_config)\n+        self.mask_decoder_config = Sam3TrackerVideoMaskDecoderConfig(**mask_decoder_config)\n+\n+        self.initializer_range = initializer_range\n+        self.num_maskmem = num_maskmem  # default 1 input frame + 6 previous frames\n+        self.image_size = image_size\n+        self.sigmoid_scale_for_mem_enc = sigmoid_scale_for_mem_enc\n+        self.sigmoid_bias_for_mem_enc = sigmoid_bias_for_mem_enc\n+        self.multimask_output_in_sam = multimask_output_in_sam\n+        self.multimask_min_pt_num = multimask_min_pt_num\n+        self.multimask_max_pt_num = multimask_max_pt_num\n+        self.multimask_output_for_tracking = multimask_output_for_tracking\n+        self.max_object_pointers_in_encoder = max_object_pointers_in_encoder\n+        self.max_cond_frame_num = max_cond_frame_num\n+        # The next 4 are True for sam2.1 and False for sam2\n+        self.enable_occlusion_spatial_embedding = enable_occlusion_spatial_embedding\n+        self.enable_temporal_pos_encoding_for_object_pointers = enable_temporal_pos_encoding_for_object_pointers\n+\n+        # memory attention\n+        self.memory_attention_hidden_size = memory_attention_hidden_size\n+        self.memory_attention_num_layers = memory_attention_num_layers\n+        self.memory_attention_num_attention_heads = memory_attention_num_attention_heads\n+        self.memory_attention_downsample_rate = memory_attention_downsample_rate\n+        self.memory_attention_feed_forward_hidden_size = memory_attention_feed_forward_hidden_size\n+        self.memory_attention_feed_forward_hidden_act = memory_attention_feed_forward_hidden_act\n+        self.memory_attention_dropout = memory_attention_dropout\n+        self.memory_attention_rope_theta = memory_attention_rope_theta\n+        self.memory_attention_rope_feat_sizes = memory_attention_rope_feat_sizes\n+        self.memory_attention_rope_dropout = memory_attention_rope_dropout\n+\n+        # memory encoder\n+        self.memory_encoder_hidden_size = memory_encoder_hidden_size\n+        self.memory_encoder_output_channels = memory_encoder_output_channels\n+        self.mask_downsampler_embed_dim = mask_downsampler_embed_dim\n+        self.mask_downsampler_kernel_size = mask_downsampler_kernel_size\n+        self.mask_downsampler_stride = mask_downsampler_stride\n+        self.mask_downsampler_padding = mask_downsampler_padding\n+        self.mask_downsampler_total_stride = mask_downsampler_total_stride\n+        self.mask_downsampler_hidden_act = mask_downsampler_hidden_act\n+        self.memory_fuser_num_layers = memory_fuser_num_layers\n+        self.memory_fuser_embed_dim = memory_fuser_embed_dim\n+        self.memory_fuser_intermediate_dim = memory_fuser_intermediate_dim\n+        self.memory_fuser_kernel_size = memory_fuser_kernel_size\n+        self.memory_fuser_padding = memory_fuser_padding\n+        self.memory_fuser_layer_scale_init_value = memory_fuser_layer_scale_init_value\n+        self.memory_fuser_hidden_act = memory_fuser_hidden_act\n+\n+        super().__init__(**kwargs)\n+\n+\n+class Sam3TrackerVideoInferenceCache(Sam2VideoInferenceCache):\n+    pass\n+\n+\n+class Sam3TrackerVideoInferenceSession(Sam2VideoInferenceSession):\n+    pass\n+\n+\n+class Sam3TrackerVideoLayerNorm(Sam2VideoLayerNorm):\n+    pass\n+\n+\n+class Sam3TrackerVideoPositionEmbeddingSine(Sam2VideoPositionEmbeddingSine):\n+    pass\n+\n+\n+class Sam3TrackerVideoAttention(Sam2VideoAttention):\n+    pass\n+\n+\n+class Sam3TrackerVideoTwoWayAttentionBlock(Sam2VideoTwoWayAttentionBlock):\n+    pass\n+\n+\n+class Sam3TrackerVideoFeedForward(Sam2VideoFeedForward):\n+    pass\n+\n+\n+class Sam3TrackerVideoImageSegmentationOutput(Sam2VideoImageSegmentationOutput):\n+    pass\n+\n+\n+class Sam3TrackerVideoSegmentationOutput(Sam2VideoSegmentationOutput):\n+    pass\n+\n+\n+class Sam3TrackerVideoPreTrainedModel(Sam2VideoPreTrainedModel):\n+    pass\n+\n+\n+class Sam3TrackerVideoVisionRotaryEmbedding(Sam2VideoVisionRotaryEmbedding):\n+    pass\n+\n+\n+class Sam3TrackerVideoRoPEAttention(Sam2VideoRoPEAttention):\n+    pass\n+\n+\n+class Sam3TrackerVideoMemoryAttentionLayer(Sam2VideoMemoryAttentionLayer):\n+    pass\n+\n+\n+class Sam3TrackerVideoMemoryAttention(Sam2VideoMemoryAttention):\n+    pass\n+\n+\n+class Sam3TrackerVideoMemoryFuserCXBlock(Sam2VideoMemoryFuserCXBlock):\n+    pass\n+\n+\n+class Sam3TrackerVideoMemoryFuser(Sam2VideoMemoryFuser):\n+    pass\n+\n+\n+class Sam3TrackerVideoMaskDownSamplerLayer(Sam2VideoMaskDownSamplerLayer):\n+    pass\n+\n+\n+class Sam3TrackerVideoMaskDownSampler(Sam2VideoMaskDownSampler):\n+    pass\n+\n+\n+class Sam3TrackerVideoMemoryEncoder(Sam2VideoMemoryEncoder):\n+    pass\n+\n+\n+class Sam3TrackerVideoVisionEncoderOutput(Sam2VideoVisionEncoderOutput):\n+    pass\n+\n+\n+class Sam3TrackerVideoPositionalEmbedding(Sam2VideoPositionalEmbedding):\n+    pass\n+\n+\n+class Sam3TrackerVideoMaskEmbedding(Sam2VideoMaskEmbedding):\n+    pass\n+\n+\n+class Sam3TrackerVideoPromptEncoder(Sam2VideoPromptEncoder):\n+    pass\n+\n+\n+class Sam3TrackerVideoTwoWayTransformer(Sam2VideoTwoWayTransformer):\n+    pass\n+\n+\n+class Sam3TrackerVideoMaskDecoder(Sam2VideoMaskDecoder):\n+    pass\n+\n+\n+class Sam3TrackerVideoModel(Sam2VideoModel):\n+    _checkpoint_conversion_mapping = {\n+        \"tracker_model.\": \"\",\n+        \"detector_model.vision_encoder.backbone.\": \"vision_encoder.backbone.\",\n+        \"tracker_neck.\": \"vision_encoder.neck.\",\n+    }\n+    _keys_to_ignore_on_load_unexpected = [r\"^detector_model.\"]\n+    _tied_weights_keys = {}\n+    _keys_to_ignore_on_load_missing = []\n+\n+    def __init__(self, config: Sam3TrackerVideoConfig, remove_vision_encoder: bool = False):\n+        r\"\"\"\n+        remove_vision_encoder (`bool`, *optional*, defaults to `False`):\n+            Whether to remove the vision encoder. If True, the vision encoder will be set to None.\n+        \"\"\"\n+        # loading from a sam3_video config\n+        if hasattr(config, \"tracker_config\") and config.tracker_config is not None:\n+            tracker_config = config.tracker_config\n+            if isinstance(tracker_config, dict):\n+                tracker_config = Sam3TrackerVideoConfig(**tracker_config)\n+            config = tracker_config\n+        Sam3TrackerVideoPreTrainedModel.__init__(config)\n+        self.shared_image_embedding = Sam3TrackerVideoPositionalEmbedding(config.prompt_encoder_config)\n+        self.vision_encoder = AutoModel.from_config(config.vision_config) if not remove_vision_encoder else None\n+        self.prompt_encoder = Sam3TrackerVideoPromptEncoder(config.prompt_encoder_config)\n+        # The module using it is not a PreTrainedModel subclass so we need this\n+        config.mask_decoder_config._attn_implementation = config._attn_implementation\n+        self.mask_decoder = Sam3TrackerVideoMaskDecoder(config.mask_decoder_config)\n+\n+        self.backbone_feature_sizes = config.vision_config.backbone_feature_sizes\n+        # a single token to indicate no memory embedding from previous frames\n+        self.hidden_dim = config.vision_config.fpn_hidden_size\n+        self.no_memory_embedding = torch.nn.Parameter(torch.zeros(1, 1, self.hidden_dim))\n+        self.config = config\n+        # For video sequence inference\n+        self.image_size = config.image_size\n+        self.memory_attention = Sam3TrackerVideoMemoryAttention(config)\n+        self.memory_encoder = Sam3TrackerVideoMemoryEncoder(config)\n+        self.no_memory_positional_encoding = torch.nn.Parameter(\n+            torch.zeros(1, 1, config.vision_config.fpn_hidden_size)\n+        )\n+        self.mem_dim = config.memory_encoder_output_channels\n+        self.num_maskmem = config.num_maskmem  # Number of memories accessible\n+        # Temporal encoding of the memories\n+        self.memory_temporal_positional_encoding = torch.nn.Parameter(\n+            torch.zeros(self.num_maskmem, 1, 1, self.mem_dim)\n+        )\n+\n+        self.no_object_pointer = torch.nn.Parameter(torch.zeros(1, self.hidden_dim))\n+        # A conv layer to downsample the mask prompt to stride 4 (the same stride as\n+        # low-res SAM mask logits) and to change its scales from 0~1 to SAM logit scale,\n+        # so that it can be fed into the SAM mask decoder to generate a pointer.\n+        self.mask_downsample = torch.nn.Conv2d(1, 1, kernel_size=4, stride=4)\n+        # a feedforward layer on SAM output tokens to turn them into object pointers\n+        self.object_pointer_proj = Sam3TrackerVideoFeedForward(self.hidden_dim, self.hidden_dim, self.hidden_dim, 3)\n+\n+        if self.config.enable_temporal_pos_encoding_for_object_pointers:\n+            # a linear projection on temporal positional encoding in object pointers to\n+            # avoid potential interference with spatial positional encoding\n+            self.temporal_positional_encoding_projection_layer = torch.nn.Linear(self.hidden_dim, self.mem_dim)\n+        else:\n+            self.temporal_positional_encoding_projection_layer = torch.nn.Identity()\n+\n+        self.occlusion_spatial_embedding_parameter = None  # compatibility with Sam2\n+        if config.enable_occlusion_spatial_embedding:\n+            self.occlusion_spatial_embedding_parameter = torch.nn.Parameter(torch.zeros(1, self.mem_dim))\n+\n+        self.post_init()\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[\n+        list[torch.Tensor],\n+        list[torch.Tensor],\n+        Optional[tuple[torch.FloatTensor, ...]],\n+        Optional[tuple[torch.FloatTensor, ...]],\n+    ]:\n+        r\"\"\"\n+        Extract and preprocess image features using the vision encoder.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor`):\n+                Input pixel values of shape `(batch_size, num_channels, height, width)`.\n+\n+        Returns:\n+            `tuple`: A tuple containing:\n+                - feature_maps (`list[torch.Tensor]`): List of feature maps from different levels.\n+                - feature_maps_position_embeddings (`list[torch.Tensor]`): List of positional embeddings for each feature level.\n+                - vision_hidden_states (`tuple[torch.FloatTensor]`, *optional*): Hidden states from the vision encoder.\n+                - vision_attentions (`tuple[torch.FloatTensor]`, *optional*): Attention weights from the vision encoder.\n+        \"\"\"\n+        vision_outputs: Sam3TrackerVideoVisionEncoderOutput = self.vision_encoder(\n+            pixel_values,\n+            **kwargs,\n+        )\n+\n+        feature_maps = vision_outputs.fpn_hidden_states\n+        feature_maps_position_embeddings = vision_outputs.fpn_position_encoding\n+\n+        # precompute projected level 0 and level 1 features in SAM decoder\n+        # to avoid running it again on every SAM click\n+        feature_maps = list(feature_maps[:-1])\n+        feature_maps[0] = self.mask_decoder.conv_s0(feature_maps[0])\n+        feature_maps[1] = self.mask_decoder.conv_s1(feature_maps[1])\n+\n+        # flatten NxCxHxW to HWxNxC\n+        feature_maps = [feature_map.flatten(2).permute(2, 0, 1) for feature_map in feature_maps]\n+        feature_maps_position_embeddings = [\n+            feature_map_position_embedding.flatten(2).permute(2, 0, 1)\n+            for feature_map_position_embedding in feature_maps_position_embeddings[:-1]\n+        ]\n+\n+        return feature_maps, feature_maps_position_embeddings, vision_outputs.hidden_states, vision_outputs.attentions\n+\n+\n+__all__ = [\n+    \"Sam3TrackerVideoMaskDecoderConfig\",\n+    \"Sam3TrackerVideoPromptEncoderConfig\",\n+    \"Sam3TrackerVideoConfig\",\n+    \"Sam3TrackerVideoModel\",\n+    \"Sam3TrackerVideoInferenceSession\",\n+    \"Sam3TrackerVideoPreTrainedModel\",\n+    \"Sam3TrackerVideoProcessor\",\n+]"
        },
        {
            "sha": "7ca6b97518cd78e70f5119337d0c734af710288c",
            "filename": "src/transformers/models/sam3_tracker_video/processing_sam3_tracker_video.py",
            "status": "added",
            "additions": 814,
            "deletions": 0,
            "changes": 814,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fprocessing_sam3_tracker_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fprocessing_sam3_tracker_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fprocessing_sam3_tracker_video.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,814 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/sam3_tracker_video/modular_sam3_tracker_video.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_sam3_tracker_video.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from copy import deepcopy\n+from typing import Optional, Union\n+\n+import numpy as np\n+import torch\n+\n+from ...image_utils import ImageInput\n+from ...processing_utils import ProcessorMixin\n+from ...tokenization_utils_base import BatchEncoding\n+from ...utils import TensorType\n+from ...utils.import_utils import requires\n+from ...video_utils import VideoInput\n+from .modeling_sam3_tracker_video import Sam3TrackerVideoInferenceSession\n+\n+\n+@requires(backends=(\"torch\",))\n+class Sam3TrackerVideoProcessor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a SAM2 processor which wraps a SAM2 image processor and an 2D points & Bounding boxes processor into a\n+    single processor.\n+\n+    [`Sam3TrackerVideoProcessor`] offers all the functionalities of [`Sam2ImageProcessorFast`] and [`Sam3TrackerVideoProcessor`]. See the docstring of\n+    [`~Sam2ImageProcessorFast.__call__`] and [`~Sam3TrackerVideoProcessor.__call__`] for more information.\n+\n+    Args:\n+        image_processor (`Sam2ImageProcessorFast`):\n+            An instance of [`Sam2ImageProcessorFast`].\n+        video_processor (`Sam3TrackerVideoVideoProcessor`):\n+            An instance of [`Sam3TrackerVideoVideoProcessor`].\n+        target_size (`int`, *optional*):\n+            The target size (target_size, target_size) to which the image will be resized.\n+        point_pad_value (`int`, *optional*, defaults to -10):\n+            The value used for padding input points.\n+    \"\"\"\n+\n+    def __init__(\n+        self, image_processor, video_processor, target_size: Optional[int] = None, point_pad_value: int = -10, **kwargs\n+    ):\n+        super().__init__(image_processor, video_processor, **kwargs)\n+        self.point_pad_value = point_pad_value\n+        self.target_size = target_size if target_size is not None else self.image_processor.size[\"height\"]\n+\n+    def __call__(\n+        self,\n+        images: Optional[ImageInput] = None,\n+        segmentation_maps: Optional[ImageInput] = None,\n+        input_points: Optional[Union[list[list[list[list[float]]]], torch.Tensor]] = None,\n+        input_labels: Optional[Union[list[list[list[int]]], torch.Tensor]] = None,\n+        input_boxes: Optional[Union[list[list[list[float]]], torch.Tensor]] = None,\n+        original_sizes: Optional[Union[list[list[float]], torch.Tensor]] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        **kwargs,\n+    ) -> BatchEncoding:\n+        r\"\"\"\n+        This method uses [`Sam3TrackerVideoImageProcessorFast.__call__`] method to prepare image(s) for the model. It also prepares 2D\n+        points and bounding boxes for the model if they are provided.\n+\n+        Args:\n+            images (`ImageInput`, *optional*):\n+                The image(s) to process.\n+            segmentation_maps (`ImageInput`, *optional*):\n+                The segmentation maps to process.\n+            input_points (`list[list[list[list[float]]]]`, `torch.Tensor`, *optional*):\n+                The points to add to the frame.\n+            input_labels (`list[list[list[int]]]`, `torch.Tensor`, *optional*):\n+                The labels for the points.\n+            input_boxes (`list[list[list[float]]]`, `torch.Tensor`, *optional*):\n+                The bounding boxes to add to the frame.\n+            original_sizes (`list[list[float]]`, `torch.Tensor`, *optional*):\n+                The original sizes of the images.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return.\n+            **kwargs:\n+                Additional keyword arguments to pass to the image processor.\n+\n+        Returns:\n+            A [`BatchEncoding`] with the following fields:\n+            - `pixel_values` (`torch.Tensor`): The processed image(s).\n+            - `original_sizes` (`list[list[float]]`): The original sizes of the images.\n+            - `labels` (`torch.Tensor`): The processed segmentation maps (if provided).\n+            - `input_points` (`torch.Tensor`): The processed points.\n+            - `input_labels` (`torch.Tensor`): The processed labels.\n+            - `input_boxes` (`torch.Tensor`): The processed bounding boxes.\n+        \"\"\"\n+        if images is not None:\n+            encoding_image_processor = self.image_processor(\n+                images,\n+                segmentation_maps=segmentation_maps,\n+                return_tensors=return_tensors,\n+                **kwargs,\n+            )\n+        elif original_sizes is not None:\n+            if isinstance(original_sizes, torch.Tensor):\n+                original_sizes = original_sizes.cpu().tolist()\n+            encoding_image_processor = BatchEncoding({\"original_sizes\": original_sizes}, tensor_type=return_tensors)\n+        else:\n+            raise ValueError(\"Either images or original_sizes must be provided\")\n+\n+        # pop arguments that are not used in the forward but used nevertheless\n+        original_sizes = encoding_image_processor[\"original_sizes\"]\n+        # Check original_sizes is of length 1 or len(images)\n+        if images is not None and len(original_sizes) != 1 and len(original_sizes) != len(images):\n+            raise ValueError(\n+                \"original_sizes must be of length 1 or len(images). If you are passing a single image, you must pass a single original_size.\"\n+            )\n+\n+        # Process input points, labels, and boxes if provided\n+        if input_points is not None or input_labels is not None or input_boxes is not None:\n+            # Validate and convert inputs to standardized format\n+            processed_points = self._validate_single_input(\n+                input_points,\n+                expected_depth=4,\n+                input_name=\"points\",\n+                expected_format=\"[image level, object level, point level, point coordinates]\",\n+                expected_coord_size=2,\n+            )\n+            processed_labels = self._validate_single_input(\n+                input_labels,\n+                expected_depth=3,\n+                input_name=\"labels\",\n+                expected_format=\"[image level, object level, point level]\",\n+            )\n+            processed_boxes = self._validate_single_input(\n+                input_boxes,\n+                expected_depth=3,\n+                input_name=\"boxes\",\n+                expected_format=\"[image level, box level, box coordinates]\",\n+                expected_coord_size=4,\n+            )\n+\n+            # Get padding requirements for all inputs\n+            if processed_points is not None:\n+                points_max_dims = self._get_nested_dimensions(processed_points)[:3]\n+            if processed_labels is not None:\n+                labels_max_dims = self._get_nested_dimensions(processed_labels)[:3]\n+            if processed_boxes is not None:\n+                boxes_max_dims = self._get_nested_dimensions(processed_boxes)[:2]\n+\n+            # Ensure points and labels have consistent dimensions\n+            if processed_points is not None and processed_labels is not None:\n+                if points_max_dims != labels_max_dims:\n+                    raise ValueError(\n+                        \"Input points and labels have inconsistent dimensions. Please ensure they have the same dimensions.\"\n+                    )\n+\n+            # Check that boxes don't need padding (model limitation)\n+            if processed_boxes is not None and len(processed_boxes) >= 2:\n+                if any(len(img_boxes) < boxes_max_dims[1] for img_boxes in processed_boxes):\n+                    raise ValueError(\n+                        \"Input boxes have inconsistent dimensions that would require padding, \"\n+                        \"but boxes cannot be padded due to model limitations. \"\n+                        \"Please ensure all images have the same number of boxes.\"\n+                    )\n+\n+            # Pad and normalize all inputs to final tensor format\n+            if processed_points is not None:\n+                padded_points = self._pad_nested_list(processed_points, points_max_dims + [2])\n+                final_points = torch.tensor(padded_points, dtype=torch.float32)\n+                self._normalize_tensor_coordinates(final_points, original_sizes, preserve_padding=True)\n+                encoding_image_processor.update({\"input_points\": final_points})\n+\n+            if processed_labels is not None:\n+                padded_labels = self._pad_nested_list(processed_labels, labels_max_dims)\n+                final_labels = torch.tensor(padded_labels, dtype=torch.int64)\n+                encoding_image_processor.update({\"input_labels\": final_labels})\n+\n+            if processed_boxes is not None:\n+                final_boxes = torch.tensor(processed_boxes, dtype=torch.float32)\n+                self._normalize_tensor_coordinates(final_boxes, original_sizes, is_bounding_box=True)\n+                encoding_image_processor.update({\"input_boxes\": final_boxes})\n+\n+        return encoding_image_processor\n+\n+    def _normalize_coordinates(\n+        self, target_size: int, coords: \"torch.Tensor\", original_size, is_bounding_box=False\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Expects a numpy array of length 2 in the final dimension. Requires the original image size in (H, W) format.\n+\n+        Args:\n+            target_size (`int`):\n+                The target size of the image.\n+            coords (`torch.Tensor`):\n+                The coordinates to be normalized.\n+            original_size (`tuple`):\n+                The original size of the image.\n+            is_bounding_box (`bool`, *optional*, defaults to `False`):\n+                Whether the coordinates are bounding boxes.\n+        \"\"\"\n+        old_h, old_w = original_size\n+        new_h, new_w = target_size, target_size\n+        coords = deepcopy(coords).float()\n+\n+        if is_bounding_box:\n+            coords = coords.reshape(-1, 2, 2)\n+        coords[..., 0] = coords[..., 0] * (new_w / old_w)\n+        coords[..., 1] = coords[..., 1] * (new_h / old_h)\n+\n+        if is_bounding_box:\n+            coords = coords.reshape(-1, 4)\n+\n+        return coords\n+\n+    def _convert_to_nested_list(self, data, expected_depth, current_depth=0):\n+        \"\"\"\n+        Recursively convert various input formats (tensors, numpy arrays, lists) to nested lists.\n+\n+        Args:\n+            data: Input data in any format\n+            expected_depth: Expected nesting depth\n+            current_depth: Current depth in recursion\n+\n+        Returns:\n+            Nested list representation of the data\n+        \"\"\"\n+        if data is None:\n+            return None\n+\n+        # Convert tensor/numpy to list if we're at a leaf level or if it's a multi-dimensional array\n+        if isinstance(data, torch.Tensor):  # PyTorch tensor\n+            if current_depth == expected_depth - 2 or len(data.shape) <= 2:  # At coordinate level or small tensor\n+                return data.numpy().tolist()\n+            else:\n+                return [self._convert_to_nested_list(item, expected_depth, current_depth + 1) for item in data]\n+        elif isinstance(data, np.ndarray):  # NumPy array\n+            if current_depth == expected_depth - 2 or len(data.shape) <= 2:  # At coordinate level or small array\n+                return data.tolist()\n+            else:\n+                return [self._convert_to_nested_list(item, expected_depth, current_depth + 1) for item in data]\n+        elif isinstance(data, list):\n+            if current_depth == expected_depth:\n+                # We've reached the expected depth, return as is\n+                return data\n+            else:\n+                # Continue recursion\n+                return [self._convert_to_nested_list(item, expected_depth, current_depth + 1) for item in data]\n+        elif isinstance(data, (int, float)):\n+            return data\n+        else:\n+            raise TypeError(f\"Unsupported data type: {type(data)}\")\n+\n+    def _get_nested_dimensions(self, nested_list, max_dims=None):\n+        \"\"\"\n+        Get the maximum dimensions at each level of nesting.\n+\n+        Args:\n+            nested_list (`list`):\n+                Nested list structure.\n+            max_dims (`list`, *optional*):\n+                Current maximum dimensions (for recursion).\n+\n+        Returns:\n+            `list`: A list of maximum dimensions for each nesting level.\n+        \"\"\"\n+        if max_dims is None:\n+            max_dims = []\n+\n+        if not isinstance(nested_list, list):\n+            return max_dims\n+\n+        if len(max_dims) == 0:\n+            max_dims.append(len(nested_list))\n+        else:\n+            max_dims[0] = max(max_dims[0], len(nested_list))\n+\n+        if len(nested_list) > 0:\n+            for item in nested_list:\n+                if isinstance(item, list):\n+                    sub_dims = self._get_nested_dimensions(item)\n+                    # Merge sub_dims into max_dims\n+                    for i, dim in enumerate(sub_dims):\n+                        if i + 1 >= len(max_dims):\n+                            max_dims.append(dim)\n+                        else:\n+                            max_dims[i + 1] = max(max_dims[i + 1], dim)\n+\n+        return max_dims\n+\n+    def _pad_nested_list(self, nested_list, target_dims, current_level=0, pad_value=None):\n+        \"\"\"\n+        Recursively pad a nested list to match target dimensions.\n+\n+        Args:\n+            nested_list (`list`):\n+                Nested list to pad.\n+            target_dims (`list`):\n+                Target dimensions for each level.\n+            current_level (`int`, *optional*, defaults to 0):\n+                Current nesting level.\n+            pad_value (`int`, *optional*):\n+                Value to use for padding.\n+\n+        Returns:\n+            `list`: The padded nested list.\n+        \"\"\"\n+        if pad_value is None:\n+            pad_value = self.point_pad_value\n+\n+        if current_level >= len(target_dims):\n+            return nested_list\n+\n+        # Ensure we have a list\n+        if not isinstance(nested_list, list):\n+            nested_list = [nested_list]\n+\n+        # Pad current level\n+        current_size = len(nested_list)\n+        target_size = target_dims[current_level]\n+\n+        # Pad with appropriate values\n+        if current_level == len(target_dims) - 1:\n+            # At the coordinate level, pad with pad_value\n+            nested_list.extend([pad_value] * (target_size - current_size))\n+        else:\n+            # At higher levels, pad with nested structures\n+            if current_size > 0:\n+                # Create appropriately sized template\n+                if current_level < len(target_dims) - 2:\n+                    # For non-coordinate levels, create empty nested structure\n+                    template_dims = target_dims[current_level + 1 :]\n+                    template = self._create_empty_nested_structure(template_dims, pad_value)\n+                else:\n+                    # For coordinate level, create list of pad_values\n+                    template = [pad_value] * target_dims[current_level + 1]\n+\n+                nested_list.extend([deepcopy(template) for _ in range(target_size - current_size)])\n+            else:\n+                # Create from scratch\n+                template_dims = target_dims[current_level + 1 :]\n+                template = self._create_empty_nested_structure(template_dims, pad_value)\n+                nested_list.extend([deepcopy(template) for _ in range(target_size)])\n+\n+        # Recursively pad sublists\n+        if current_level < len(target_dims) - 1:\n+            for i in range(len(nested_list)):\n+                if isinstance(nested_list[i], list):\n+                    nested_list[i] = self._pad_nested_list(nested_list[i], target_dims, current_level + 1, pad_value)\n+\n+        return nested_list\n+\n+    def _create_empty_nested_structure(self, dims, pad_value):\n+        \"\"\"\n+        Create an empty nested structure with given dimensions filled with pad_value.\n+\n+        Args:\n+            dims (`list`):\n+                The dimensions of the nested structure.\n+            pad_value (`int`):\n+                The value to fill the structure with.\n+        \"\"\"\n+        if len(dims) == 1:\n+            return [pad_value] * dims[0]\n+        else:\n+            return [self._create_empty_nested_structure(dims[1:], pad_value) for _ in range(dims[0])]\n+\n+    def _get_nesting_level(self, input_list):\n+        \"\"\"\n+        Get the nesting level of a list structure.\n+\n+        Args:\n+            input_list (`list`):\n+                The list to get the nesting level of.\n+        \"\"\"\n+        if isinstance(input_list, list):\n+            if len(input_list) == 0:\n+                return 1\n+            return 1 + self._get_nesting_level(input_list[0])\n+        elif isinstance(input_list, (np.ndarray, torch.Tensor)):\n+            # For arrays/tensors, the nesting level is the number of dimensions\n+            return len(input_list.shape)\n+        return 0\n+\n+    def _validate_single_input(\n+        self,\n+        data: Union[torch.Tensor, np.ndarray, list],\n+        expected_depth: int,\n+        input_name: str,\n+        expected_format: str,\n+        expected_coord_size: Optional[int] = None,\n+    ) -> list:\n+        \"\"\"\n+                Validate a single input by ensuring proper nesting and raising an error if the input is not valid.\n+\n+                Args:\n+                    data (`torch.Tensor`, `np.ndarray`, or `list`):\n+                        Input data to process.\n+                    expected_depth (`int`):\n+                        Expected nesting depth.\n+                    input_name (`str`):\n+                        Name of the input for error messages.\n+                    expected_format (`str`):\n+                        The expected format of the input.\n+                    expected_coord_size (`int`, *optional*):\n+                        Expected coordinate size (2 for points, 4 for boxes, None for labels).\n+        .\n+        \"\"\"\n+        if data is None:\n+            return None\n+\n+        # Handle tensors and numpy arrays first\n+        if isinstance(data, (torch.Tensor, np.ndarray)):\n+            # For tensors/arrays, we can directly check the number of dimensions\n+            if data.ndim != expected_depth:\n+                raise ValueError(\n+                    f\"Input {input_name} must be a tensor/array with {expected_depth} dimensions. The expected nesting format is {expected_format}. Got {data.ndim} dimensions.\"\n+                )\n+            elif expected_coord_size is not None:\n+                if data.shape[-1] != expected_coord_size:\n+                    raise ValueError(\n+                        f\"Input {input_name} must be a tensor/array with {expected_coord_size} as the last dimension, got {data.shape[-1]}.\"\n+                    )\n+            return self._convert_to_nested_list(data, expected_depth)\n+\n+        # Handle nested lists\n+        if isinstance(data, list):\n+            current_depth = self._get_nesting_level(data)\n+            if current_depth != expected_depth:\n+                raise ValueError(\n+                    f\"Input {input_name} must be a nested list with {expected_depth} levels. The expected nesting format is {expected_format}. Got {current_depth} levels.\"\n+                )\n+            return self._convert_to_nested_list(data, expected_depth)\n+\n+    def _normalize_tensor_coordinates(self, tensor, original_sizes, is_bounding_box=False, preserve_padding=False):\n+        \"\"\"\n+        Helper method to normalize coordinates in a tensor across multiple images.\n+\n+        Args:\n+            tensor (`torch.Tensor`):\n+                Input tensor with coordinates.\n+            original_sizes (`list`):\n+                Original image sizes.\n+            is_bounding_box (`bool`, *optional*, defaults to `False`):\n+                Whether coordinates are bounding boxes.\n+            preserve_padding (`bool`, *optional*, defaults to `False`):\n+                Whether to preserve padding values (for points).\n+        \"\"\"\n+        if preserve_padding:\n+            # For points: avoid normalizing pad values\n+            mask = tensor != self.point_pad_value\n+            coord_mask = mask.all(dim=-1, keepdim=True)\n+\n+        for img_idx in range(len(original_sizes)):\n+            if img_idx < tensor.shape[0]:\n+                original_size = original_sizes[img_idx] if img_idx < len(original_sizes) else original_sizes[0]\n+                normalized_coords = self._normalize_coordinates(\n+                    self.target_size, tensor[img_idx], original_size, is_bounding_box=is_bounding_box\n+                )\n+\n+                if preserve_padding:\n+                    # Only update non-padded values\n+                    img_mask = coord_mask[img_idx]\n+                    tensor[img_idx] = torch.where(\n+                        img_mask.expand_as(tensor[img_idx]), normalized_coords, tensor[img_idx]\n+                    )\n+                else:\n+                    tensor[img_idx] = normalized_coords\n+\n+    def post_process_masks(\n+        self,\n+        masks,\n+        original_sizes,\n+        mask_threshold=0.0,\n+        binarize=True,\n+        max_hole_area=0.0,\n+        max_sprinkle_area=0.0,\n+        apply_non_overlapping_constraints=False,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Remove padding and upscale masks to the original image size.\n+\n+        Args:\n+            masks (`Union[List[torch.Tensor], List[np.ndarray]]`):\n+                Batched masks from the mask_decoder in (batch_size, num_channels, height, width) format.\n+            original_sizes (`Union[torch.Tensor, List[Tuple[int,int]]]`):\n+                The original sizes of each image before it was resized to the model's expected input shape, in (height,\n+                width) format.\n+            mask_threshold (`float`, *optional*, defaults to 0.0):\n+                Threshold for binarization and post-processing operations.\n+            binarize (`bool`, *optional*, defaults to `True`):\n+                Whether to binarize the masks.\n+            max_hole_area (`float`, *optional*, defaults to 0.0):\n+                The maximum area of a hole to fill.\n+            max_sprinkle_area (`float`, *optional*, defaults to 0.0):\n+                The maximum area of a sprinkle to fill.\n+            apply_non_overlapping_constraints (`bool`, *optional*, defaults to `False`):\n+                Whether to apply non-overlapping constraints to the masks.\n+\n+        Returns:\n+            (`torch.Tensor`): Batched masks in batch_size, num_channels, height, width) format, where (height, width)\n+            is given by original_size.\n+        \"\"\"\n+        return self.image_processor.post_process_masks(\n+            masks,\n+            original_sizes,\n+            mask_threshold,\n+            binarize,\n+            max_hole_area,\n+            max_sprinkle_area,\n+            apply_non_overlapping_constraints,\n+            **kwargs,\n+        )\n+\n+    def init_video_session(\n+        self,\n+        video: Optional[VideoInput] = None,\n+        inference_device: Union[str, \"torch.device\"] = \"cpu\",\n+        inference_state_device: Optional[Union[str, \"torch.device\"]] = None,\n+        processing_device: Optional[Union[str, \"torch.device\"]] = None,\n+        video_storage_device: Optional[Union[str, \"torch.device\"]] = None,\n+        max_vision_features_cache_size: int = 1,\n+        dtype: torch.dtype = torch.float32,\n+    ):\n+        \"\"\"\n+        Initializes a video session for inference.\n+        If a video is provided (async inference), the video will be processed and stored on the `video_storage_device`.\n+\n+        Args:\n+            video (`VideoInput`, *optional*):\n+                The video to process. No need to provide when streaming.\n+            inference_device (`str` or `torch.device`, *optional*, defaults to \"cpu\"):\n+                The device to use for inference.\n+            inference_state_device (`str` or `torch.device`, *optional*):\n+                The device to store the inference state on.\n+            processing_device (`str` or `torch.device`, *optional*):\n+                The device to use for video processing.\n+            video_storage_device (`str` or `torch.device`, *optional*):\n+                The device to store the processed video frames on.\n+            max_vision_features_cache_size (`int`, *optional*, defaults to 1):\n+                The maximum number of vision features to cache.\n+            dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):\n+                The torch dtype to use for the whole session.\n+        \"\"\"\n+        video_storage_device = video_storage_device if video_storage_device is not None else inference_device\n+        inference_state_device = inference_state_device if inference_state_device is not None else inference_device\n+        processing_device = processing_device if processing_device is not None else inference_device\n+        pixel_values_video = None\n+        video_height = None\n+        video_width = None\n+        if video is not None:\n+            processed_video = self.video_processor(videos=video, device=processing_device, return_tensors=\"pt\")\n+            pixel_values_video = processed_video.pixel_values_videos[0]\n+            video_height = processed_video.original_sizes[0][0]\n+            video_width = processed_video.original_sizes[0][1]\n+        inference_session = Sam3TrackerVideoInferenceSession(\n+            video=pixel_values_video,\n+            video_height=video_height,\n+            video_width=video_width,\n+            inference_device=inference_device,\n+            video_storage_device=video_storage_device,\n+            inference_state_device=inference_state_device,\n+            dtype=dtype,\n+            max_vision_features_cache_size=max_vision_features_cache_size,\n+        )\n+        return inference_session\n+\n+    def add_inputs_to_inference_session(\n+        self,\n+        inference_session: Sam3TrackerVideoInferenceSession,\n+        frame_idx: int,\n+        obj_ids: Union[list[int], int],\n+        input_points: Optional[Union[list[list[list[list[float]]]], torch.Tensor]] = None,\n+        input_labels: Optional[Union[list[list[list[int]]], torch.Tensor]] = None,\n+        input_boxes: Optional[Union[list[list[list[float]]], torch.Tensor]] = None,\n+        input_masks: Optional[Union[np.ndarray, torch.Tensor, list[np.ndarray], list[torch.Tensor]]] = None,\n+        original_size: Optional[tuple[int, int]] = None,\n+        clear_old_inputs: bool = True,\n+    ) -> Sam3TrackerVideoInferenceSession:\n+        \"\"\"\n+        Process new points, boxes, or masks for a video frame and add them to the inference session.\n+\n+        Args:\n+            inference_session (`Sam3TrackerVideoInferenceSession`):\n+                The inference session for the video.\n+            frame_idx (`int`):\n+                The index of the frame to process.\n+            obj_ids (`list[int]` or `int`):\n+                The object ID(s) to associate with the points or box.\n+                These can be any integers and can be reused later on to specify an object.\n+            input_points (`list[list[list[list[float]]]]`, `torch.Tensor`, *optional*):\n+                The points to add to the frame.\n+            input_labels (`list[list[list[int]]]`, `torch.Tensor`, *optional*):\n+                The labels for the points.\n+            input_boxes (`list[list[list[float]]]`, `torch.Tensor`, *optional*):\n+                The bounding boxes to add to the frame.\n+            input_masks (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, or `list[torch.Tensor]`, *optional*):\n+                The mask(s) to add to the frame.\n+            original_size (`tuple[int, int]`, *optional*):\n+                The original size of the video. Provide when streaming.\n+            clear_old_inputs (`bool`, *optional*, defaults to `True`):\n+                Whether to clear old inputs for the object.\n+        \"\"\"\n+\n+        if isinstance(obj_ids, int):\n+            obj_ids = [obj_ids]\n+\n+        # Validate inputs\n+        if (input_points is not None) != (input_labels is not None):\n+            raise ValueError(\"points and labels must be provided together\")\n+        if input_points is None and input_boxes is None and input_masks is None:\n+            raise ValueError(\"at least one of points, boxes, or masks must be provided as input\")\n+        if input_masks is not None and (input_points is not None or input_boxes is not None):\n+            raise ValueError(\"masks cannot be provided together with points or boxes\")\n+\n+        if input_masks is not None:\n+            return self.process_new_mask_for_video_frame(inference_session, frame_idx, obj_ids, input_masks)\n+        else:\n+            return self.process_new_points_or_boxes_for_video_frame(\n+                inference_session,\n+                frame_idx,\n+                obj_ids,\n+                input_points,\n+                input_labels,\n+                input_boxes,\n+                original_size,\n+                clear_old_inputs,\n+            )\n+\n+    def process_new_points_or_boxes_for_video_frame(\n+        self,\n+        inference_session: Sam3TrackerVideoInferenceSession,\n+        frame_idx: int,\n+        obj_ids: list[int],\n+        input_points: Optional[Union[list[list[list[list[float]]]], torch.Tensor]] = None,\n+        input_labels: Optional[Union[list[list[list[int]]], torch.Tensor]] = None,\n+        input_boxes: Optional[Union[list[list[list[float]]], torch.Tensor]] = None,\n+        original_size: Optional[tuple[int, int]] = None,\n+        clear_old_inputs: bool = True,\n+    ) -> Sam3TrackerVideoInferenceSession:\n+        \"\"\"\n+        Process new points or boxes for a video frame and add them to the inference session.\n+\n+        Args:\n+            inference_session (`Sam3TrackerVideoInferenceSession`):\n+                The inference session for the video.\n+            frame_idx (`int`):\n+                The index of the frame to process.\n+            obj_ids (`list[int]`):\n+                The object ID(s) to associate with the points or box.\n+                These can be any integers and can be reused later on to specify an object.\n+            input_points (`list[list[list[list[float]]]]`, `torch.Tensor`, *optional*):\n+                The points to add to the frame.\n+            input_labels (`list[list[list[int]]]`, `torch.Tensor`, *optional*):\n+                The labels for the points.\n+            input_boxes (`list[list[list[float]]]`, `torch.Tensor`, *optional*):\n+                The bounding boxes to add to the frame.\n+            original_size (`tuple[int, int]`, *optional*):\n+                The original size of the video. Provide when streaming.\n+            clear_old_inputs (`bool`, *optional*, defaults to `True`):\n+                Whether to clear old inputs for the object.\n+        \"\"\"\n+        if original_size is not None:\n+            inference_session.video_height = original_size[0]\n+            inference_session.video_width = original_size[1]\n+        elif inference_session.video_height is None or inference_session.video_width is None:\n+            raise ValueError(\"original_size must be provided when adding points or boxes on a first streamed frame\")\n+\n+        original_sizes = [[inference_session.video_height, inference_session.video_width]]\n+\n+        encoded_inputs = self(\n+            input_points=input_points,\n+            input_labels=input_labels,\n+            input_boxes=input_boxes,\n+            original_sizes=original_sizes,\n+            return_tensors=\"pt\",\n+        )\n+        input_points = encoded_inputs.get(\"input_points\", None)\n+        input_labels = encoded_inputs.get(\"input_labels\", None)\n+        input_boxes = encoded_inputs.get(\"input_boxes\", None)\n+\n+        if input_points is not None:\n+            if input_points.shape[1] != len(obj_ids):\n+                raise ValueError(\n+                    f\"Number of object ids ({len(obj_ids)}) does not match number of points ({input_points.shape[1]})\"\n+                )\n+        else:\n+            input_points = torch.zeros(1, len(obj_ids), 0, 2, dtype=torch.float32)\n+        if input_labels is not None:\n+            if input_labels.shape[1] != len(obj_ids):\n+                raise ValueError(\n+                    f\"Number of object ids ({len(obj_ids)}) does not match number of labels ({input_labels.shape[1]})\"\n+                )\n+        else:\n+            input_labels = torch.zeros(1, len(obj_ids), 0, dtype=torch.int32)\n+        if input_boxes is not None:\n+            if input_boxes.shape[1] != len(obj_ids):\n+                raise ValueError(\n+                    f\"Number of object ids ({len(obj_ids)}) does not match number of boxes ({input_boxes.shape[1]})\"\n+                )\n+\n+        if input_boxes is not None:\n+            if not clear_old_inputs:\n+                raise ValueError(\n+                    \"cannot add box without clearing old points, since \"\n+                    \"box prompt must be provided before any point prompt \"\n+                    \"(please use clear_old_points=True instead)\"\n+                )\n+            box_coords = input_boxes.reshape(1, -1, 2, 2)\n+            box_labels = torch.tensor([2, 3], dtype=torch.int32).repeat(1, box_coords.shape[1], 1)\n+            input_points = torch.cat([box_coords, input_points], dim=2)\n+            input_labels = torch.cat([box_labels, input_labels], dim=2)\n+\n+        for obj_id, idx in zip(obj_ids, range(len(obj_ids))):\n+            obj_idx = inference_session.obj_id_to_idx(obj_id)\n+            input_points_for_obj = input_points[:, idx, :, :].unsqueeze(1)\n+            input_labels_for_obj = input_labels[:, idx, :].unsqueeze(1)\n+            # Handle existing points\n+            if not clear_old_inputs:\n+                existing_points = inference_session.point_inputs_per_obj[obj_idx].get(frame_idx, None)\n+                if existing_points is not None:\n+                    # Concatenate with existing points\n+                    input_points_for_obj = torch.cat(\n+                        [existing_points[\"point_coords\"].to(input_points_for_obj.device), input_points_for_obj], dim=2\n+                    )\n+                    input_labels_for_obj = torch.cat(\n+                        [existing_points[\"point_labels\"].to(input_labels_for_obj.device), input_labels_for_obj], dim=2\n+                    )\n+            point_inputs = {\n+                \"point_coords\": input_points_for_obj,\n+                \"point_labels\": input_labels_for_obj,\n+            }\n+\n+            inference_session.add_point_inputs(obj_idx, frame_idx, point_inputs)\n+            inference_session.remove_mask_inputs(obj_idx, frame_idx)  # Clear any mask inputs\n+\n+        inference_session.obj_with_new_inputs = obj_ids\n+\n+    def process_new_mask_for_video_frame(\n+        self,\n+        inference_session: Sam3TrackerVideoInferenceSession,\n+        frame_idx: int,\n+        obj_ids: list[int],\n+        input_masks: Union[np.ndarray, torch.Tensor, list[np.ndarray], list[torch.Tensor]],\n+    ):\n+        \"\"\"\n+        Add new mask to a frame and add them to the inference session.\n+\n+        Args:\n+            inference_session (`Sam3TrackerVideoInferenceSession`):\n+                The inference session for the video.\n+            frame_idx (`int`):\n+                The index of the frame to process.\n+            obj_ids (`list[int]`):\n+                The object ID(s) to associate with the mask.\n+                These can be any integers and can be reused later on to specify an object.\n+            input_masks (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, or `list[torch.Tensor]`):\n+                The mask(s) to add to the frame.\n+        \"\"\"\n+        if not isinstance(input_masks, list):\n+            input_masks = [input_masks]\n+        if len(input_masks) != len(obj_ids):\n+            raise ValueError(\n+                f\"Number of object ids ({len(obj_ids)}) does not match number of masks ({len(input_masks)})\"\n+            )\n+\n+        for obj_id, mask in zip(obj_ids, input_masks):\n+            obj_idx = inference_session.obj_id_to_idx(obj_id)\n+\n+            device = inference_session.inference_device\n+\n+            # Process mask\n+            if not isinstance(mask, torch.Tensor):\n+                mask = torch.tensor(mask, dtype=torch.bool)\n+            nb_dim = mask.dim()\n+            if nb_dim > 4 or nb_dim < 2:\n+                raise ValueError(f\"Mask has an unsupported number of dimensions: {nb_dim}\")\n+            for i in range(4 - nb_dim):\n+                mask = mask.unsqueeze(0)\n+\n+            mask_H, mask_W = mask.shape[-2:]\n+            mask_inputs_orig = mask.to(device)\n+            mask_inputs_orig = mask_inputs_orig.float().to(device)\n+\n+            # Resize mask if needed\n+            if mask_H != self.target_size or mask_W != self.target_size:\n+                mask_inputs = torch.nn.functional.interpolate(\n+                    mask_inputs_orig,\n+                    size=(self.target_size, self.target_size),\n+                    align_corners=False,\n+                    mode=\"bilinear\",\n+                    antialias=True,\n+                )\n+                mask_inputs = (mask_inputs >= 0.5).float()\n+            else:\n+                mask_inputs = mask_inputs_orig\n+\n+            inference_session.add_mask_inputs(obj_idx, frame_idx, mask_inputs)\n+            inference_session.remove_point_inputs(obj_idx, frame_idx)  # Clear any point inputs\n+\n+        inference_session.obj_with_new_inputs = obj_ids\n+\n+\n+__all__ = [\"Sam3TrackerVideoProcessor\"]"
        },
        {
            "sha": "97943d102b6ed94e328e4ff49162894fec0a38f5",
            "filename": "src/transformers/models/sam3_video/__init__.py",
            "status": "added",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_video%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_video%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_video%2F__init__.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,28 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_sam3_video import *\n+    from .modeling_sam3_video import *\n+    from .processing_sam3_video import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "f9aa6fb7f1ede6a847caed20fb2802acbbe9320d",
            "filename": "src/transformers/models/sam3_video/configuration_sam3_video.py",
            "status": "added",
            "additions": 229,
            "deletions": 0,
            "changes": 229,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fconfiguration_sam3_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fconfiguration_sam3_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fconfiguration_sam3_video.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,229 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"SAM3 Video model configuration\"\"\"\n+\n+from ...configuration_utils import PreTrainedConfig\n+from ...utils import logging\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Sam3VideoConfig(PreTrainedConfig):\n+    r\"\"\"\n+    Configuration class for [`Sam3VideoModel`]. This combines configurations for the detector (Sam3) and tracker\n+    (Sam2Video) components, along with detection-tracking fusion hyperparameters.\n+\n+    Instantiating a configuration defaults will yield a similar configuration to that of SAM 3\n+    [facebook/sam3](https://huggingface.co/facebook/sam3) architecture.\n+\n+    This model integrates detection and tracking with various fusion heuristics including NMS, association,\n+    hotstart, reconditioning, and occlusion handling.\n+\n+    Args:\n+        detector_config (`dict` or `Sam3Config`, *optional*):\n+            Configuration for the Sam3 detector model. If not provided, default Sam3Config will be used.\n+        tracker_config (`dict` or `Sam2VideoConfig`, *optional*):\n+            Configuration for the Sam2Video tracker model. If not provided, default Sam2VideoConfig will be used.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing weight matrices.\n+        low_res_mask_size (`int`, *optional*, defaults to 288):\n+            Size (height and width) of the low-resolution mask outputs from the tracker before upsampling to video resolution.\n+        score_threshold_detection (`float`, *optional*, defaults to 0.5):\n+            Probability threshold for detection outputs - only keep detections above this threshold.\n+        det_nms_thresh (`float`, *optional*, defaults to 0.1):\n+            IoU threshold for detection NMS (Non-Maximum Suppression).\n+        assoc_iou_thresh (`float`, *optional*, defaults to 0.1):\n+            IoU threshold for detection-to-track matching. A detection is considered \"matched\" to a tracklet if\n+            it overlaps with the tracklet above this threshold. Often a loose threshold like 0.1.\n+        trk_assoc_iou_thresh (`float`, *optional*, defaults to 0.5):\n+            IoU threshold for detection-to-track matching, used to determine whether a masklet is \"unmatched\"\n+            by any detections. Often a stricter threshold like 0.5.\n+        new_det_thresh (`float`, *optional*, defaults to 0.7):\n+            Probability threshold for a detection to be added as a new object.\n+        recondition_on_trk_masks (`bool`, *optional*, defaults to `True`):\n+            Whether to use tracked masks (True) or detection masks (False) for reconditioning. Use True when tracked\n+            masks are higher quality and detector serves as validation signal to strengthen memory and prevent drift.\n+        hotstart_delay (`int`, *optional*, defaults to 15):\n+            Number of frames to buffer outputs during hotstart. We hold off the outputs for `hotstart_delay`\n+            frames and remove tracklets based on hotstart heuristics.\n+        hotstart_unmatch_thresh (`int`, *optional*, defaults to 8):\n+            Number of unmatched frames required to remove a tracklet during hotstart period.\n+        hotstart_dup_thresh (`int`, *optional*, defaults to 8):\n+            Number of overlapping frames required to remove a duplicate tracklet during hotstart period.\n+        suppress_unmatched_only_within_hotstart (`bool`, *optional*, defaults to `True`):\n+            Whether to suppress masks only within hotstart period. If False, we can suppress masks even if\n+            they start before hotstart period.\n+        init_trk_keep_alive (`int`, *optional*, defaults to 30):\n+            Initial keep-alive counter for new tracks.\n+        max_trk_keep_alive (`int`, *optional*, defaults to 30):\n+            Maximum keep-alive counter value. Tracks with matched detections get their counter increased up to this value.\n+        min_trk_keep_alive (`int`, *optional*, defaults to -1):\n+            Minimum keep-alive counter value. Tracks with unmatched detections get their counter decreased to this value.\n+        suppress_overlapping_based_on_recent_occlusion_threshold (`float`, *optional*, defaults to 0.7):\n+            Threshold for suppressing overlapping objects based on recent occlusion. Overlapping masks with\n+            IoU above this threshold are suppressed based on which was most recently occluded.\n+        decrease_trk_keep_alive_for_empty_masklets (`bool`, *optional*, defaults to `False`):\n+            Whether to decrease keep-alive counter for masklets with zero area in SAM2 prediction.\n+        fill_hole_area (`int`, *optional*, defaults to 16):\n+            Minimum area (in pixels) for filling holes in masks and removing small sprinkles.\n+        max_num_objects (`int`, *optional*, defaults to 10000):\n+            Maximum number of objects to track. Default 10000 effectively turns off this limit.\n+        recondition_every_nth_frame (`int`, *optional*, defaults to 16):\n+            Frequency of mask reconditioning (in frames). Set to 0 to disable reconditioning.\n+        high_conf_thresh (`float`, *optional*, defaults to 0.8):\n+            High confidence threshold for reconditioning. Only detections above this threshold can recondition tracklets.\n+        high_iou_thresh (`float`, *optional*, defaults to 0.8):\n+            High IoU threshold for reconditioning. Only detections with IoU above this threshold can recondition tracklets.\n+\n+    Example:\n+    ```python\n+    >>> from transformers import Sam3VideoConfig, Sam3VideoModel\n+\n+    >>> # Initializing a SAM3 Video configuration with default detector and tracker\n+    >>> configuration = Sam3VideoConfig()\n+\n+    >>> # Initializing a model from the configuration\n+    >>> model = Sam3VideoModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    >>> detector_config = configuration.detector_config\n+    >>> tracker_config = configuration.tracker_config\n+    ```\n+    \"\"\"\n+\n+    model_type = \"sam3_video\"\n+    is_composition = True\n+    sub_configs = {\n+        \"detector_config\": AutoConfig,\n+        \"tracker_config\": AutoConfig,\n+    }\n+\n+    def __init__(\n+        self,\n+        detector_config=None,\n+        tracker_config=None,\n+        initializer_range=0.02,\n+        low_res_mask_size=288,\n+        # Detection-tracking fusion hyperparameters\n+        score_threshold_detection=0.5,\n+        det_nms_thresh=0.1,\n+        assoc_iou_thresh=0.1,\n+        trk_assoc_iou_thresh=0.5,\n+        new_det_thresh=0.7,\n+        recondition_on_trk_masks=True,\n+        # Hotstart parameters\n+        hotstart_delay=15,\n+        hotstart_unmatch_thresh=8,\n+        hotstart_dup_thresh=8,\n+        suppress_unmatched_only_within_hotstart=True,\n+        # Keep-alive parameters\n+        init_trk_keep_alive=30,\n+        max_trk_keep_alive=30,\n+        min_trk_keep_alive=-1,\n+        # Occlusion and overlap handling\n+        suppress_overlapping_based_on_recent_occlusion_threshold=0.7,\n+        decrease_trk_keep_alive_for_empty_masklets=False,\n+        # Mask post-processing\n+        fill_hole_area=16,\n+        # Object tracking limits\n+        max_num_objects=10000,\n+        # Reconditioning parameters\n+        recondition_every_nth_frame=16,\n+        high_conf_thresh=0.8,\n+        high_iou_thresh=0.8,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        # Initialize detector config (Sam3)\n+        if detector_config is None:\n+            detector_config = {}\n+            logger.info(\"detector_config is None. Initializing the Sam3Config with default values.\")\n+        if isinstance(detector_config, dict):\n+            detector_config[\"model_type\"] = detector_config.get(\"model_type\", \"sam3\")\n+            self.detector_config = CONFIG_MAPPING[detector_config[\"model_type\"]](**detector_config)\n+        elif isinstance(detector_config, PreTrainedConfig):\n+            self.detector_config = detector_config\n+        else:\n+            raise ValueError(f\"detector_config must be a dict or Sam3Config, got {type(detector_config)}\")\n+\n+        # Initialize tracker config (Sam2Video)\n+        if tracker_config is None:\n+            tracker_config = {}\n+            logger.info(\"tracker_config is None. Initializing the Sam3TrackerVideoConfig with default values.\")\n+        if isinstance(tracker_config, dict):\n+            tracker_config[\"model_type\"] = tracker_config.get(\"model_type\", \"sam3_tracker_video\")\n+            self.tracker_config = CONFIG_MAPPING[tracker_config[\"model_type\"]](**tracker_config)\n+        elif isinstance(tracker_config, PreTrainedConfig):\n+            self.tracker_config = tracker_config\n+        else:\n+            raise ValueError(f\"tracker_config must be a dict or Sam3TrackerVideoConfig, got {type(tracker_config)}\")\n+\n+        # Model initialization\n+        self.initializer_range = initializer_range\n+\n+        self.low_res_mask_size = low_res_mask_size\n+\n+        # Detection-tracking fusion hyperparameters\n+        self.score_threshold_detection = score_threshold_detection\n+        self.det_nms_thresh = det_nms_thresh\n+        self.assoc_iou_thresh = assoc_iou_thresh\n+        self.trk_assoc_iou_thresh = trk_assoc_iou_thresh\n+        self.new_det_thresh = new_det_thresh\n+\n+        self.recondition_on_trk_masks = recondition_on_trk_masks\n+\n+        # Hotstart parameters\n+        if hotstart_delay > 0:\n+            if hotstart_unmatch_thresh > hotstart_delay:\n+                raise ValueError(\n+                    f\"hotstart_unmatch_thresh ({hotstart_unmatch_thresh}) must be <= hotstart_delay ({hotstart_delay})\"\n+                )\n+            if hotstart_dup_thresh > hotstart_delay:\n+                raise ValueError(\n+                    f\"hotstart_dup_thresh ({hotstart_dup_thresh}) must be <= hotstart_delay ({hotstart_delay})\"\n+                )\n+        self.hotstart_delay = hotstart_delay\n+        self.hotstart_unmatch_thresh = hotstart_unmatch_thresh\n+        self.hotstart_dup_thresh = hotstart_dup_thresh\n+        self.suppress_unmatched_only_within_hotstart = suppress_unmatched_only_within_hotstart\n+\n+        # Keep-alive parameters\n+        self.init_trk_keep_alive = init_trk_keep_alive\n+        self.max_trk_keep_alive = max_trk_keep_alive\n+        self.min_trk_keep_alive = min_trk_keep_alive\n+\n+        # Occlusion and overlap handling\n+        self.suppress_overlapping_based_on_recent_occlusion_threshold = (\n+            suppress_overlapping_based_on_recent_occlusion_threshold\n+        )\n+        self.decrease_trk_keep_alive_for_empty_masklets = decrease_trk_keep_alive_for_empty_masklets\n+\n+        # Mask post-processing\n+        self.fill_hole_area = fill_hole_area\n+\n+        # Object tracking limits\n+        self.max_num_objects = max_num_objects\n+\n+        # Reconditioning parameters\n+        self.recondition_every_nth_frame = recondition_every_nth_frame\n+        self.high_conf_thresh = high_conf_thresh\n+        self.high_iou_thresh = high_iou_thresh\n+\n+\n+__all__ = [\"Sam3VideoConfig\"]"
        },
        {
            "sha": "49ae6d04ec34341023b688c19986a6093fa47c71",
            "filename": "src/transformers/models/sam3_video/convert_sam3_video_to_hf.py",
            "status": "added",
            "additions": 792,
            "deletions": 0,
            "changes": 792,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fconvert_sam3_video_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fconvert_sam3_video_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fconvert_sam3_video_to_hf.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,792 @@\n+# Copyright 2025 The Meta AI Authors and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"\n+Convert SAM3 checkpoints from the original implementation to HuggingFace format.\n+\n+\"\"\"\n+\n+import argparse\n+import gc\n+import os\n+from typing import Optional\n+\n+import regex as re\n+import torch\n+\n+from transformers import CLIPTokenizerFast\n+from transformers.models.sam2_video.video_processing_sam2_video import Sam2VideoVideoProcessor\n+from transformers.models.sam3.image_processing_sam3_fast import Sam3ImageProcessorFast\n+from transformers.models.sam3.modeling_sam3 import Sam3Model\n+from transformers.models.sam3_tracker.modeling_sam3_tracker import Sam3TrackerModel\n+from transformers.models.sam3_tracker_video.modeling_sam3_tracker_video import Sam3TrackerVideoModel\n+from transformers.models.sam3_video.configuration_sam3_video import Sam3VideoConfig\n+from transformers.models.sam3_video.modeling_sam3_video import Sam3VideoModel\n+from transformers.models.sam3_video.processing_sam3_video import Sam3VideoProcessor\n+from transformers.utils import logging\n+\n+\n+logging.set_verbosity_info()\n+logger = logging.get_logger(__name__)\n+\n+# fmt: off\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    r\"^sam3_model\\.\": r\"detector_model.\",\n+    r\"^sam2_predictor\\.model\\.\": r\"tracker_model.\",\n+\n+    # ============================================================================\n+    # Vision Encoder - ViT Backbone\n+    # ============================================================================\n+    r\"backbone\\.vision_backbone\\.trunk\\.\":                                 r\"vision_encoder.backbone.\",\n+    r\"vision_encoder\\.backbone\\.pos_embed\":                                r\"vision_encoder.backbone.embeddings.position_embeddings\",\n+    r\"vision_encoder\\.backbone\\.patch_embed\\.proj\\.\":                      r\"vision_encoder.backbone.embeddings.patch_embeddings.projection.\",\n+    r\"vision_encoder\\.backbone\\.ln_pre\\.\":                                 r\"vision_encoder.backbone.layer_norm.\",\n+    r\"vision_encoder\\.backbone\\.blocks\\.(\\d+)\\.norm1\\.\":                   r\"vision_encoder.backbone.layers.\\1.layer_norm1.\",\n+    r\"vision_encoder\\.backbone\\.blocks\\.(\\d+)\\.norm2\\.\":                   r\"vision_encoder.backbone.layers.\\1.layer_norm2.\",\n+    r\"vision_encoder\\.backbone\\.blocks\\.(\\d+)\\.attn\\.qkv\\.\":               r\"vision_encoder.backbone.layers.\\1.attention.qkv.\",\n+    r\"vision_encoder\\.backbone\\.blocks\\.(\\d+)\\.attn\\.proj\\.\":              r\"vision_encoder.backbone.layers.\\1.attention.o_proj.\",\n+    r\"vision_encoder\\.backbone\\.blocks\\.(\\d+)\\.attn\\.freqs_cis\":           r\"vision_encoder.backbone.layers.\\1.rotary_emb.rope_embeddings\",\n+    r\"vision_encoder\\.backbone\\.blocks\\.(\\d+)\\.mlp\\.fc1\\.\":                r\"vision_encoder.backbone.layers.\\1.mlp.fc1.\",\n+    r\"vision_encoder\\.backbone\\.blocks\\.(\\d+)\\.mlp\\.fc2\\.\":                r\"vision_encoder.backbone.layers.\\1.mlp.fc2.\",\n+\n+    # ision Encoder - FPN Neck\n+    r\"backbone\\.vision_backbone\\.neck\\.fpn\\.(\\d+)\\.\":                      r\"vision_encoder.neck.fpn_layers.\\1.\",\n+    r\"backbone\\.vision_backbone\\.convs\\.(\\d+)\\.dconv_2x2_0\\.\":             r\"vision_encoder.neck.fpn_layers.\\1.scale_layers.0.\",\n+    r\"backbone\\.vision_backbone\\.convs\\.(\\d+)\\.dconv_2x2_1\\.\":             r\"vision_encoder.neck.fpn_layers.\\1.scale_layers.2.\",\n+    r\"backbone\\.vision_backbone\\.convs\\.(\\d+)\\.dconv_2x2\\.\":               r\"vision_encoder.neck.fpn_layers.\\1.scale_layers.0.\",\n+    r\"backbone\\.vision_backbone\\.convs\\.(\\d+)\\.maxpool_2x2\\.\":             r\"vision_encoder.neck.fpn_layers.\\1.scale_layers.0.\",\n+    r\"backbone\\.vision_backbone\\.convs\\.(\\d+)\\.conv_1x1\\.\":                r\"vision_encoder.neck.fpn_layers.\\1.proj1.\",\n+    r\"backbone\\.vision_backbone\\.convs\\.(\\d+)\\.conv_3x3\\.\":                r\"vision_encoder.neck.fpn_layers.\\1.proj2.\",\n+\n+    # ision Encoder - Tracker FPN Neck\n+    r\"backbone\\.vision_backbone\\.sam2_convs\\.(\\d+)\\.dconv_2x2_0\\.\":             r\"vision_encoder.tracker_neck.fpn_layers.\\1.scale_layers.0.\",\n+    r\"backbone\\.vision_backbone\\.sam2_convs\\.(\\d+)\\.dconv_2x2_1\\.\":             r\"vision_encoder.tracker_neck.fpn_layers.\\1.scale_layers.2.\",\n+    r\"backbone\\.vision_backbone\\.sam2_convs\\.(\\d+)\\.dconv_2x2\\.\":               r\"vision_encoder.tracker_neck.fpn_layers.\\1.scale_layers.0.\",\n+    r\"backbone\\.vision_backbone\\.sam2_convs\\.(\\d+)\\.maxpool_2x2\\.\":             r\"vision_encoder.tracker_neck.fpn_layers.\\1.scale_layers.0.\",\n+    r\"backbone\\.vision_backbone\\.sam2_convs\\.(\\d+)\\.conv_1x1\\.\":                r\"vision_encoder.tracker_neck.fpn_layers.\\1.proj1.\",\n+    r\"backbone\\.vision_backbone\\.sam2_convs\\.(\\d+)\\.conv_3x3\\.\":                r\"vision_encoder.tracker_neck.fpn_layers.\\1.proj2.\",\n+\n+    # ===========================================================================\n+    # ext Encoder (CLIP)\n+    # ===========================================================================\n+    r\"backbone\\.language_backbone\\.encoder\\.\":                             r\"text_encoder.\",\n+    r\"text_encoder\\.token_embedding\\.\":                                    r\"text_encoder.text_model.embeddings.token_embedding.\",\n+    r\"text_encoder\\.positional_embedding\":                                 r\"text_encoder.text_model.embeddings.position_embedding.weight\",\n+    r\"text_encoder\\.ln_final\\.\":                                           r\"text_encoder.text_model.final_layer_norm.\",\n+    r\"text_encoder\\.text_projection\":                                      r\"text_encoder.text_projection.weight\",\n+    r\"text_encoder\\.transformer\\.resblocks\\.(\\d+)\\.attn\\.in_proj_\":       r\"text_encoder.text_model.encoder.layers.\\1.self_attn.in_proj_\",\n+    r\"text_encoder\\.transformer\\.resblocks\\.(\\d+)\\.attn\\.out_proj\\.\":     r\"text_encoder.text_model.encoder.layers.\\1.self_attn.out_proj.\",\n+    r\"text_encoder\\.transformer\\.resblocks\\.(\\d+)\\.ln_1\\.\":               r\"text_encoder.text_model.encoder.layers.\\1.layer_norm1.\",\n+    r\"text_encoder\\.transformer\\.resblocks\\.(\\d+)\\.ln_2\\.\":               r\"text_encoder.text_model.encoder.layers.\\1.layer_norm2.\",\n+    r\"text_encoder\\.transformer\\.resblocks\\.(\\d+)\\.mlp\\.c_fc\\.\":          r\"text_encoder.text_model.encoder.layers.\\1.mlp.fc1.\",\n+    r\"text_encoder\\.transformer\\.resblocks\\.(\\d+)\\.mlp\\.c_proj\\.\":        r\"text_encoder.text_model.encoder.layers.\\1.mlp.fc2.\",\n+    r\"backbone\\.language_backbone\\.resizer\\.\":                             r\"text_projection.\",\n+\n+    # ===========================================================================\n+    # eometry Encoder\n+    # ===========================================================================\n+    r\"geometry_encoder\\.encode\\.(\\d+)\\.cross_attn_image\\.out_proj\\.\":     r\"geometry_encoder.layers.\\1.cross_attn.o_proj.\",\n+    r\"geometry_encoder\\.encode\\.(\\d+)\\.cross_attn_image\\.\":               r\"geometry_encoder.layers.\\1.cross_attn.\",\n+    r\"geometry_encoder\\.encode\\.(\\d+)\\.self_attn\\.out_proj\\.\":            r\"geometry_encoder.layers.\\1.self_attn.o_proj.\",\n+    r\"geometry_encoder\\.encode\\.(\\d+)\\.self_attn\\.\":                      r\"geometry_encoder.layers.\\1.self_attn.\",\n+    r\"geometry_encoder\\.encode\\.(\\d+)\\.linear1\\.\":                        r\"geometry_encoder.layers.\\1.mlp.fc1.\",\n+    r\"geometry_encoder\\.encode\\.(\\d+)\\.linear2\\.\":                        r\"geometry_encoder.layers.\\1.mlp.fc2.\",\n+    r\"geometry_encoder\\.encode\\.(\\d+)\\.norm1\\.\":                          r\"geometry_encoder.layers.\\1.layer_norm1.\",\n+    r\"geometry_encoder\\.encode\\.(\\d+)\\.norm2\\.\":                          r\"geometry_encoder.layers.\\1.layer_norm2.\",\n+    r\"geometry_encoder\\.encode\\.(\\d+)\\.norm3\\.\":                          r\"geometry_encoder.layers.\\1.layer_norm3.\",\n+    r\"geometry_encoder\\.img_pre_norm\\.\":                                   r\"geometry_encoder.vision_layer_norm.\",\n+    r\"geometry_encoder\\.norm\\.\":                                           r\"geometry_encoder.prompt_layer_norm.\",\n+    r\"geometry_encoder\\.encode_norm\\.\":                                    r\"geometry_encoder.output_layer_norm.\",\n+\n+    # ===========================================================================\n+    # ETR Encoder\n+    # ===========================================================================\n+    r\"detector_model.transformer\\.encoder\\.layers\\.(\\d+)\\.cross_attn_image\\.out_proj\\.\":  r\"detector_model.detr_encoder.layers.\\1.cross_attn.o_proj.\",\n+    r\"detector_model.transformer\\.encoder\\.layers\\.(\\d+)\\.cross_attn_image\\.\":            r\"detector_model.detr_encoder.layers.\\1.cross_attn.\",\n+    r\"detector_model.transformer\\.encoder\\.layers\\.(\\d+)\\.self_attn\\.out_proj\\.\":         r\"detector_model.detr_encoder.layers.\\1.self_attn.o_proj.\",\n+    r\"detector_model.transformer\\.encoder\\.layers\\.(\\d+)\\.self_attn\\.\":                   r\"detector_model.detr_encoder.layers.\\1.self_attn.\",\n+    r\"detector_model.transformer\\.encoder\\.layers\\.(\\d+)\\.cross_attn\\.out_proj\\.\":        r\"detector_model.detr_encoder.layers.\\1.cross_attn.o_proj.\",\n+    r\"detector_model.transformer\\.encoder\\.layers\\.(\\d+)\\.cross_attn\\.\":                  r\"detector_model.detr_encoder.layers.\\1.cross_attn.\",\n+    r\"detector_model.transformer\\.encoder\\.layers\\.(\\d+)\\.linear1\\.\":                     r\"detector_model.detr_encoder.layers.\\1.mlp.fc1.\",\n+    r\"detector_model.transformer\\.encoder\\.layers\\.(\\d+)\\.linear2\\.\":                     r\"detector_model.detr_encoder.layers.\\1.mlp.fc2.\",\n+    r\"detector_model.transformer\\.encoder\\.layers\\.(\\d+)\\.norm1\\.\":                       r\"detector_model.detr_encoder.layers.\\1.layer_norm1.\",\n+    r\"detector_model.transformer\\.encoder\\.layers\\.(\\d+)\\.norm2\\.\":                       r\"detector_model.detr_encoder.layers.\\1.layer_norm2.\",\n+    r\"detector_model.transformer\\.encoder\\.layers\\.(\\d+)\\.norm3\\.\":                       r\"detector_model.detr_encoder.layers.\\1.layer_norm3.\",\n+\n+    # ===========================================================================\n+    # ETR Decoder\n+    # ===========================================================================\n+    r\"transformer\\.decoder\\.query_embed\\.\":                                r\"detr_decoder.query_embed.\",\n+    r\"transformer\\.decoder\\.reference_points\\.\":                           r\"detr_decoder.reference_points.\",\n+    r\"transformer\\.decoder\\.instance_query_embed\\.\":                       r\"detr_decoder.instance_query_embed.\",\n+    r\"transformer\\.decoder\\.instance_reference_points\\.\":                  r\"detr_decoder.instance_reference_points.\",\n+    r\"transformer\\.decoder\\.presence_token\\.\":                             r\"detr_decoder.presence_token.\",\n+    r\"transformer\\.decoder\\.presence_token_head\\.layers\\.0\\.\":             r\"detr_decoder.presence_head.layer1.\",\n+    r\"transformer\\.decoder\\.presence_token_head\\.layers\\.1\\.\":             r\"detr_decoder.presence_head.layer2.\",\n+    r\"transformer\\.decoder\\.presence_token_head\\.layers\\.2\\.\":             r\"detr_decoder.presence_head.layer3.\",\n+    r\"transformer\\.decoder\\.presence_token_out_norm\\.\":                    r\"detr_decoder.presence_layer_norm.\",\n+    r\"transformer\\.decoder\\.norm\\.\":                                       r\"detr_decoder.output_layer_norm.\",\n+    r\"transformer\\.decoder\\.bbox_embed\\.layers\\.0\\.\":                      r\"detr_decoder.box_head.layer1.\",\n+    r\"transformer\\.decoder\\.bbox_embed\\.layers\\.1\\.\":                      r\"detr_decoder.box_head.layer2.\",\n+    r\"transformer\\.decoder\\.bbox_embed\\.layers\\.2\\.\":                      r\"detr_decoder.box_head.layer3.\",\n+    r\"transformer\\.decoder\\.instance_bbox_embed\\.layers\\.0\\.\":             r\"detr_decoder.instance_box_head.layer1.\",\n+    r\"transformer\\.decoder\\.instance_bbox_embed\\.layers\\.1\\.\":             r\"detr_decoder.instance_box_head.layer2.\",\n+    r\"transformer\\.decoder\\.instance_bbox_embed\\.layers\\.2\\.\":             r\"detr_decoder.instance_box_head.layer3.\",\n+    r\"transformer\\.decoder\\.ref_point_head\\.layers\\.0\\.\":                  r\"detr_decoder.ref_point_head.layer1.\",\n+    r\"transformer\\.decoder\\.ref_point_head\\.layers\\.1\\.\":                  r\"detr_decoder.ref_point_head.layer2.\",\n+    r\"transformer\\.decoder\\.boxRPB_embed_x\\.layers\\.0\\.\":                  r\"detr_decoder.box_rpb_embed_x.layer1.\",\n+    r\"transformer\\.decoder\\.boxRPB_embed_x\\.layers\\.1\\.\":                  r\"detr_decoder.box_rpb_embed_x.layer2.\",\n+    r\"transformer\\.decoder\\.boxRPB_embed_y\\.layers\\.0\\.\":                  r\"detr_decoder.box_rpb_embed_y.layer1.\",\n+    r\"transformer\\.decoder\\.boxRPB_embed_y\\.layers\\.1\\.\":                  r\"detr_decoder.box_rpb_embed_y.layer2.\",\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.self_attn\\.out_proj\\.\":         r\"detr_decoder.layers.\\1.self_attn.o_proj.\",\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.self_attn\\.\":                   r\"detr_decoder.layers.\\1.self_attn.\",\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.ca_text\\.out_proj\\.\":           r\"detr_decoder.layers.\\1.text_cross_attn.o_proj.\",\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.ca_text\\.\":                     r\"detr_decoder.layers.\\1.text_cross_attn.\",\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.cross_attn\\.out_proj\\.\":        r\"detr_decoder.layers.\\1.vision_cross_attn.o_proj.\",\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.cross_attn\\.\":                  r\"detr_decoder.layers.\\1.vision_cross_attn.\",\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.linear1\\.\":                     r\"detr_decoder.layers.\\1.mlp.fc1.\",\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.linear2\\.\":                     r\"detr_decoder.layers.\\1.mlp.fc2.\",\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.norm1\\.\":                       r\"detr_decoder.layers.\\1.vision_cross_attn_layer_norm.\",\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.catext_norm\\.\":                 r\"detr_decoder.layers.\\1.text_cross_attn_layer_norm.\",\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.norm2\\.\":                       r\"detr_decoder.layers.\\1.self_attn_layer_norm.\",\n+    r\"transformer\\.decoder\\.layers\\.(\\d+)\\.norm3\\.\":                       r\"detr_decoder.layers.\\1.mlp_layer_norm.\",\n+\n+    # ===========================================================================\n+    # ot Product Scoring\n+    # ===========================================================================\n+    r\"dot_prod_scoring\\.prompt_mlp\\.layers\\.0\\.\":                          r\"dot_product_scoring.text_mlp.layer1.\",\n+    r\"dot_prod_scoring\\.prompt_mlp\\.layers\\.1\\.\":                          r\"dot_product_scoring.text_mlp.layer2.\",\n+    r\"dot_prod_scoring\\.prompt_mlp\\.out_norm\\.\":                           r\"dot_product_scoring.text_mlp_out_norm.\",\n+    r\"dot_prod_scoring\\.prompt_proj\\.\":                                    r\"dot_product_scoring.text_proj.\",\n+    r\"dot_prod_scoring\\.hs_proj\\.\":                                        r\"dot_product_scoring.query_proj.\",\n+\n+    # ===========================================================================\n+    # ask Decoder\n+    # ===========================================================================\n+    r\"segmentation_head\\.pixel_decoder\\.conv_layers\\.(\\d+)\\.\":             r\"mask_decoder.pixel_decoder.conv_layers.\\1.\",\n+    r\"segmentation_head\\.pixel_decoder\\.norms\\.(\\d+)\\.\":                   r\"mask_decoder.pixel_decoder.norms.\\1.\",\n+    r\"segmentation_head\\.mask_embed\\.layers\\.(\\d+)\\.\":                     r\"mask_decoder.mask_embedder.layers.\\1.\",\n+    r\"segmentation_head\\.mask_predictor\\.mask_embed\\.layers\\.(\\d+)\\.\":     r\"mask_decoder.mask_embedder.layers.\\1.\",\n+    r\"segmentation_head\\.instance_seg_head\\.\":                             r\"mask_decoder.instance_projection.\",\n+    r\"segmentation_head\\.semantic_seg_head\\.\":                             r\"mask_decoder.semantic_projection.\",\n+    r\"segmentation_head\\.cross_attend_prompt\\.out_proj\\.\":                 r\"mask_decoder.prompt_cross_attn.o_proj.\",\n+    r\"segmentation_head\\.cross_attend_prompt\\.\":                           r\"mask_decoder.prompt_cross_attn.\",\n+    r\"segmentation_head\\.cross_attn_norm\\.\":                               r\"mask_decoder.prompt_cross_attn_norm.\",\n+\n+    r\"^detector_model\\.vision_encoder\\.tracker_neck\\.\": r\"tracker_neck.\",\n+\n+}\n+# fmt: on\n+\n+KEYS_TO_MODIFY_MAPPING = {\n+    \"iou_prediction_head.layers.0\": \"iou_prediction_head.proj_in\",\n+    \"iou_prediction_head.layers.1\": \"iou_prediction_head.layers.0\",\n+    \"iou_prediction_head.layers.2\": \"iou_prediction_head.proj_out\",\n+    \"mask_decoder.output_upscaling.0\": \"mask_decoder.upscale_conv1\",\n+    \"mask_decoder.output_upscaling.1\": \"mask_decoder.upscale_layer_norm\",\n+    \"mask_decoder.output_upscaling.3\": \"mask_decoder.upscale_conv2\",\n+    \"mask_downscaling.0\": \"mask_embed.conv1\",\n+    \"mask_downscaling.1\": \"mask_embed.layer_norm1\",\n+    \"mask_downscaling.3\": \"mask_embed.conv2\",\n+    \"mask_downscaling.4\": \"mask_embed.layer_norm2\",\n+    \"mask_downscaling.6\": \"mask_embed.conv3\",\n+    \"dwconv\": \"depthwise_conv\",\n+    \"pwconv\": \"pointwise_conv\",\n+    \"fuser\": \"memory_fuser\",\n+    \"point_embeddings\": \"point_embed\",\n+    \"pe_layer.positional_encoding_gaussian_matrix\": \"shared_embedding.positional_embedding\",\n+    \"obj_ptr_tpos_proj\": \"temporal_positional_encoding_projection_layer\",\n+    \"no_obj_embed_spatial\": \"occlusion_spatial_embedding_parameter\",\n+    \"sam_prompt_encoder\": \"prompt_encoder\",\n+    \"sam_mask_decoder\": \"mask_decoder\",\n+    \"maskmem_tpos_enc\": \"memory_temporal_positional_encoding\",\n+    \"gamma\": \"scale\",\n+    \"image_encoder.neck\": \"vision_encoder.neck\",\n+    \"image_encoder\": \"vision_encoder.backbone\",\n+    \"neck.0\": \"neck.conv1\",\n+    \"neck.1\": \"neck.layer_norm1\",\n+    \"neck.2\": \"neck.conv2\",\n+    \"neck.3\": \"neck.layer_norm2\",\n+    \"pix_feat_proj\": \"feature_projection\",\n+    \"patch_embed.proj\": \"patch_embed.projection\",\n+    \"no_mem_embed\": \"no_memory_embedding\",\n+    \"no_mem_pos_enc\": \"no_memory_positional_encoding\",\n+    \"obj_ptr\": \"object_pointer\",\n+    \".norm\": \".layer_norm\",\n+    \"trunk.\": \"\",\n+    \"out_proj\": \"o_proj\",\n+}\n+\n+\n+def adapt_internal_ckpt(ov_sd):\n+    # Replace values instead of keys, and remove any isinstance checks\n+    sam2_sd = {k: v.replace(\"backbone.vision_backbone.trunk\", \"image_encoder.trunk\") for k, v in ov_sd.items()}\n+    sam2_sd = {k: v.replace(\"backbone.vision_backbone.convs\", \"image_encoder.neck.convs\") for k, v in sam2_sd.items()}\n+    # rename components to be consitent with paper and public release\n+    sam2_sd = {k: v.replace(\"transformer.encoder\", \"memory_attention\") for k, v in sam2_sd.items()}\n+    sam2_sd = {k: v.replace(\"maskmem_backbone\", \"memory_encoder\") for k, v in sam2_sd.items()}\n+    sam2_sd = {\n+        k: v.replace(\n+            \"memory_encoder.mask_downsampler.encoder.0.\",\n+            \"memory_encoder.mask_downsampler.layers.0.conv.\",\n+        )\n+        for k, v in sam2_sd.items()\n+    }\n+    sam2_sd = {\n+        k: v.replace(\n+            \"memory_encoder.mask_downsampler.encoder.1.\",\n+            \"memory_encoder.mask_downsampler.layers.0.layer_norm.\",\n+        )\n+        for k, v in sam2_sd.items()\n+    }\n+    sam2_sd = {\n+        k: v.replace(\n+            \"memory_encoder.mask_downsampler.encoder.3.\",\n+            \"memory_encoder.mask_downsampler.layers.1.conv.\",\n+        )\n+        for k, v in sam2_sd.items()\n+    }\n+    sam2_sd = {\n+        k: v.replace(\n+            \"memory_encoder.mask_downsampler.encoder.4.\",\n+            \"memory_encoder.mask_downsampler.layers.1.layer_norm.\",\n+        )\n+        for k, v in sam2_sd.items()\n+    }\n+    sam2_sd = {\n+        k: v.replace(\n+            \"memory_encoder.mask_downsampler.encoder.6.\",\n+            \"memory_encoder.mask_downsampler.layers.2.conv.\",\n+        )\n+        for k, v in sam2_sd.items()\n+    }\n+    sam2_sd = {\n+        k: v.replace(\n+            \"memory_encoder.mask_downsampler.encoder.7.\",\n+            \"memory_encoder.mask_downsampler.layers.2.layer_norm.\",\n+        )\n+        for k, v in sam2_sd.items()\n+    }\n+    sam2_sd = {\n+        k: v.replace(\n+            \"memory_encoder.mask_downsampler.encoder.9.\",\n+            \"memory_encoder.mask_downsampler.layers.3.conv.\",\n+        )\n+        for k, v in sam2_sd.items()\n+    }\n+    sam2_sd = {\n+        k: v.replace(\n+            \"memory_encoder.mask_downsampler.encoder.10.\",\n+            \"memory_encoder.mask_downsampler.layers.3.layer_norm.\",\n+        )\n+        for k, v in sam2_sd.items()\n+    }\n+    sam2_sd = {\n+        k: v.replace(\n+            \"memory_encoder.mask_downsampler.encoder.12.\",\n+            \"memory_encoder.mask_downsampler.final_conv.\",\n+        )\n+        for k, v in sam2_sd.items()\n+    }\n+    sam2_sd = {\n+        k: v.replace(\n+            \"memory_encoder.o_proj.\",\n+            \"memory_encoder.projection.\",\n+        )\n+        for k, v in sam2_sd.items()\n+    }\n+    # MLPBLock to MLP\n+    sam2_sd = {\n+        k: v.replace(\"mask_decoder.transformer.layers.0.mlp.lin1\", \"mask_decoder.transformer.layers.0.mlp.layers.0\")\n+        for k, v in sam2_sd.items()\n+    }\n+    sam2_sd = {\n+        k: v.replace(\"mask_decoder.transformer.layers.0.mlp.lin2\", \"mask_decoder.transformer.layers.0.mlp.layers.1\")\n+        for k, v in sam2_sd.items()\n+    }\n+    sam2_sd = {\n+        k: v.replace(\"mask_decoder.transformer.layers.1.mlp.lin1\", \"mask_decoder.transformer.layers.1.mlp.layers.0\")\n+        for k, v in sam2_sd.items()\n+    }\n+    sam2_sd = {\n+        k: v.replace(\"mask_decoder.transformer.layers.1.mlp.lin2\", \"mask_decoder.transformer.layers.1.mlp.layers.1\")\n+        for k, v in sam2_sd.items()\n+    }\n+    sam2_sd = {\n+        k: v.replace(\n+            \"mask_decoder.transformer.layers.0.mlp.layers.0.\",\n+            \"mask_decoder.transformer.layers.0.mlp.proj_in.\",\n+        )\n+        for k, v in sam2_sd.items()\n+    }\n+    sam2_sd = {\n+        k: v.replace(\n+            \"mask_decoder.transformer.layers.0.mlp.layers.1.\",\n+            \"mask_decoder.transformer.layers.0.mlp.proj_out.\",\n+        )\n+        for k, v in sam2_sd.items()\n+    }\n+    sam2_sd = {\n+        k: v.replace(\n+            \"mask_decoder.transformer.layers.1.mlp.layers.0.\",\n+            \"mask_decoder.transformer.layers.1.mlp.proj_in.\",\n+        )\n+        for k, v in sam2_sd.items()\n+    }\n+    sam2_sd = {\n+        k: v.replace(\n+            \"mask_decoder.transformer.layers.1.mlp.layers.1.\",\n+            \"mask_decoder.transformer.layers.1.mlp.proj_out.\",\n+        )\n+        for k, v in sam2_sd.items()\n+    }\n+    # FFN to MLP\n+    # sam2_sd = {k: v.replace(\".fc1\", \".layers.0\") for k, v in sam2_sd.items()}\n+    # sam2_sd = {k: v.replace(\".fc2\", \".layers.1\") for k, v in sam2_sd.items()}\n+    return sam2_sd\n+\n+\n+def replace_keys(key_mapping: dict):\n+    output_hypernetworks_mlps_pattern = r\".*.output_hypernetworks_mlps.(\\d+).layers.(\\d+).*\"\n+    output_mask_decoder_mlps_pattern = r\"tracker_model.mask_decoder.transformer.layers.(\\d+).mlp.layers.(\\d+).*\"\n+    output_mask_decoder_score_head_pattern = r\"tracker_model.mask_decoder.pred_obj_score_head.layers.(\\d+).*\"\n+    output_vision_encoder_mlps_pattern = r\"vision_encoder.backbone.blocks.(\\d+).mlp.layers.(\\d+).*\"\n+    output_vision_encoder_neck_pattern = r\"tracker_model.vision_encoder.neck.convs.(\\d+).conv\"\n+    output_memory_encoder_projection_pattern = r\"tracker_model.memory_encoder.o_proj.*\"\n+    output_object_pointer_proj_pattern = r\"tracker_model.object_pointer_proj.layers.(\\d+).*\"\n+    output_memory_encoder_mask_downsampler_pattern = r\"tracker_model.memory_encoder.mask_downsampler.encoder.(\\d+).*\"\n+    key_mapping_copy = key_mapping.copy()\n+    for value, key in key_mapping_copy.items():\n+        if not value.startswith(\"sam2_predictor.\"):\n+            continue\n+        for key_to_modify, new_key in KEYS_TO_MODIFY_MAPPING.items():\n+            if key_to_modify in key:\n+                key = key.replace(key_to_modify, new_key)\n+\n+        # vision_encoder.blocks.0.mlp.layers.1.weight -> vision_encoder.blocks.0.mlp.proj_out.weight\n+        if re.match(output_vision_encoder_mlps_pattern, key):\n+            layer_nb = int(re.match(output_vision_encoder_mlps_pattern, key).group(2))\n+            if layer_nb == 0:\n+                key = key.replace(\"layers.0\", \"proj_in\")\n+            elif layer_nb == 1:\n+                key = key.replace(\"layers.1\", \"proj_out\")\n+\n+        # mask_decoder.transformer.layers.0.mlp.layers.1.weight -> mask_decoder.transformer.layers.1.mlp.proj_out.weight\n+        if re.match(output_mask_decoder_mlps_pattern, key):\n+            layer_nb = int(re.match(output_mask_decoder_mlps_pattern, key).group(2))\n+            if layer_nb == 0:\n+                key = key.replace(\"mlp.layers.0\", \"mlp.proj_in\")\n+            elif layer_nb == 1:\n+                key = key.replace(\"mlp.layers.1\", \"mlp.proj_out\")\n+\n+        # mask_decoder.pred_obj_score_head.layers.1.weight -> mask_decoder.pred_obj_score_head.proj_in.weight\n+        if re.match(output_mask_decoder_score_head_pattern, key):\n+            layer_nb = int(re.match(output_mask_decoder_score_head_pattern, key).group(1))\n+            if layer_nb == 0:\n+                key = key.replace(\"layers.0\", \"proj_in\")\n+            elif layer_nb == 1:\n+                key = key.replace(\"layers.1\", \"layers.0\")\n+            elif layer_nb == 2:\n+                key = key.replace(\"layers.2\", \"proj_out\")\n+\n+        if re.match(output_hypernetworks_mlps_pattern, key):\n+            layer_nb = int(re.match(output_hypernetworks_mlps_pattern, key).group(2))\n+            if layer_nb == 0:\n+                key = key.replace(\"layers.0\", \"proj_in\")\n+            elif layer_nb == 1:\n+                key = key.replace(\"layers.1\", \"layers.0\")\n+            elif layer_nb == 2:\n+                key = key.replace(\"layers.2\", \"proj_out\")\n+\n+        # vision_encoder.neck.convs.1.conv.bias -> vision_encoder.neck.convs.1.bias\n+        if re.match(output_vision_encoder_neck_pattern, key):\n+            key = key.replace(\".conv.\", \".\")\n+\n+        # memory_encoder.o_proj.weight -> memory_encoder.projection.weight\n+        if re.match(output_memory_encoder_projection_pattern, key):\n+            key = key.replace(\".o_proj.\", \".projection.\")\n+\n+        if re.match(output_object_pointer_proj_pattern, key):\n+            layer_nb = int(re.match(output_object_pointer_proj_pattern, key).group(1))\n+            if layer_nb == 0:\n+                key = key.replace(\"layers.0\", \"proj_in\")\n+            elif layer_nb == 1:\n+                key = key.replace(\"layers.1\", \"layers.0\")\n+            elif layer_nb == 2:\n+                key = key.replace(\"layers.2\", \"proj_out\")\n+\n+        if re.match(output_memory_encoder_mask_downsampler_pattern, key):\n+            layer_nb = int(re.match(output_memory_encoder_mask_downsampler_pattern, key).group(1))\n+            if layer_nb == 12:\n+                key = key.replace(f\"encoder.{layer_nb}\", \"final_conv\")\n+            elif layer_nb % 3 == 0:\n+                key = key.replace(f\"encoder.{layer_nb}\", f\"layers.{layer_nb // 3}.conv\")\n+            elif layer_nb % 3 == 1:\n+                key = key.replace(f\"encoder.{layer_nb}\", f\"layers.{layer_nb // 3}.layer_norm\")\n+        key_mapping[value] = key\n+\n+    return key_mapping\n+\n+\n+def convert_old_keys_to_new_keys(state_dict_keys: list[str]) -> dict[str, str]:\n+    \"\"\"\n+    Convert original SAM3 checkpoint keys to HuggingFace format.\n+\n+    This function applies regex patterns to efficiently rename keys in bulk.\n+\n+    Args:\n+        state_dict_keys: List of original checkpoint keys\n+\n+    Returns:\n+        Dictionary mapping original keys to new keys\n+    \"\"\"\n+    output_dict = {}\n+    if state_dict_keys is not None:\n+        old_text = \"\\n\".join(state_dict_keys)\n+        new_text = old_text\n+\n+        # Apply all regex patterns\n+        for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING.items():\n+            new_text = re.sub(pattern, replacement, new_text, flags=re.MULTILINE)\n+\n+        output_dict = dict(zip(old_text.split(\"\\n\"), new_text.split(\"\\n\")))\n+\n+    output_dict = replace_keys(output_dict)\n+    output_dict = adapt_internal_ckpt(output_dict)\n+\n+    return output_dict\n+\n+\n+def split_qkv(state_dict: dict) -> dict:\n+    \"\"\"\n+    Split combined QKV weights/biases into separate Q, K, V projections.\n+\n+    Both the vision backbone and text encoder in the original SAM3 use combined QKV projections,\n+    but the refactored model uses separate Q, K, V projections.\n+\n+    Args:\n+        state_dict: State dictionary with combined QKV weights\n+\n+    Returns:\n+        State dictionary with split Q, K, V weights\n+    \"\"\"\n+    # Handle vision backbone: .attention.qkv.* â†’ .attention.{q,k,v}_proj.*\n+    vision_keys_to_split = [key for key in state_dict.keys() if \".attention.qkv.\" in key]\n+\n+    for key in vision_keys_to_split:\n+        qkv = state_dict.pop(key)\n+        # Split into 3 equal chunks along dimension 0 (output dimension)\n+        q, k, v = torch.chunk(qkv, 3, dim=0)\n+\n+        # Create new keys for q_proj, k_proj, v_proj\n+        state_dict[key.replace(\".qkv.\", \".q_proj.\")] = q\n+        state_dict[key.replace(\".qkv.\", \".k_proj.\")] = k\n+        state_dict[key.replace(\".qkv.\", \".v_proj.\")] = v\n+\n+    # Handle all attention layers with in_proj_* (text encoder, DETR decoder cross-attention, mask decoder)\n+    # These use: .{attn_type}.in_proj_* â†’ .{attn_type}.{q,k,v}_proj.*\n+    in_proj_keys_to_split = [key for key in state_dict.keys() if \".in_proj_\" in key]\n+\n+    for key in in_proj_keys_to_split:\n+        in_proj = state_dict.pop(key)\n+        # Split into 3 equal chunks along dimension 0 (output dimension)\n+        q, k, v = torch.chunk(in_proj, 3, dim=0)\n+\n+        # Create new keys for q_proj, k_proj, v_proj\n+        # Replace \"in_proj_weight\" with \"q_proj.weight\" (or \"in_proj_bias\" with \"q_proj.bias\")\n+        if key.endswith(\"in_proj_weight\"):\n+            base_key = key.replace(\"in_proj_weight\", \"\")\n+            state_dict[base_key + \"q_proj.weight\"] = q\n+            state_dict[base_key + \"k_proj.weight\"] = k\n+            state_dict[base_key + \"v_proj.weight\"] = v\n+        elif key.endswith(\"in_proj_bias\"):\n+            base_key = key.replace(\"in_proj_bias\", \"\")\n+            state_dict[base_key + \"q_proj.bias\"] = q\n+            state_dict[base_key + \"k_proj.bias\"] = k\n+            state_dict[base_key + \"v_proj.bias\"] = v\n+\n+    return state_dict\n+\n+\n+def load_original_state_dict(checkpoint_path: str) -> dict[str, torch.Tensor]:\n+    \"\"\"Load the original SAM3 checkpoint.\"\"\"\n+    print(f\"Loading original checkpoint from {checkpoint_path}\")\n+\n+    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n+\n+    # Handle different checkpoint formats\n+    if \"model\" in checkpoint:\n+        state_dict = checkpoint[\"model\"]\n+    elif \"state_dict\" in checkpoint:\n+        state_dict = checkpoint[\"state_dict\"]\n+    else:\n+        state_dict = checkpoint\n+\n+    print(f\"Loaded {len(state_dict)} keys from checkpoint\")\n+    return state_dict\n+\n+\n+def get_sam3_video_config(\n+    vision_config: Optional[dict] = None,\n+    text_config: Optional[dict] = None,\n+) -> Sam3VideoConfig:\n+    \"\"\"\n+    Create SAM3 configuration.\n+\n+    Args:\n+        vision_config: Optional vision encoder configuration overrides\n+        text_config: Optional text encoder configuration overrides\n+\n+    Returns:\n+        Sam3Config instance\n+    \"\"\"\n+    config = Sam3VideoConfig()\n+\n+    # Update with any provided overrides\n+    if vision_config is not None:\n+        for key, value in vision_config.items():\n+            setattr(config.vision_config, key, value)\n+\n+    if text_config is not None:\n+        # Text config is a CLIPTextConfig\n+        for key, value in text_config.items():\n+            setattr(config.text_config, key, value)\n+\n+    return config\n+\n+\n+def convert_sam3_checkpoint(\n+    checkpoint_path: str,\n+    output_path: str,\n+    config: Optional[Sam3VideoConfig] = None,\n+    push_to_hub: bool = False,\n+    repo_id: Optional[str] = None,\n+    safe_serialization: bool = True,\n+):\n+    \"\"\"\n+    Convert SAM3 checkpoint from original format to HuggingFace format.\n+\n+    Args:\n+        checkpoint_path: Path to the original checkpoint file\n+        output_path: Path to save the converted checkpoint\n+        config: Optional Sam3VideoConfig to use (otherwise creates default)\n+        push_to_hub: Whether to push the model to the Hub\n+        repo_id: Repository ID for pushing to Hub\n+        safe_serialization: Whether to save using safetensors\n+    \"\"\"\n+    # Create output directory\n+    os.makedirs(output_path, exist_ok=True)\n+\n+    # Load configuration\n+    if config is None:\n+        config = get_sam3_video_config()\n+\n+    config.architectures = [\"Sam3VideoModel\"]\n+    config.save_pretrained(output_path)\n+    print(\"Model config saved successfully\")\n+\n+    # Load and convert weights\n+    print(\"Loading original checkpoint...\")\n+    state_dict_old = load_original_state_dict(checkpoint_path)\n+\n+    print(\"Converting checkpoint keys...\")\n+    all_keys = list(state_dict_old.keys())\n+    key_mapping = convert_old_keys_to_new_keys(all_keys)\n+\n+    # Create new state dict with converted keys\n+    state_dict_new = {}\n+\n+    for old_key in all_keys:\n+        new_key = key_mapping.get(old_key, old_key)\n+        # Special handling: Strip cls token from vision backbone position embeddings\n+        if new_key == \"detector_model.vision_encoder.backbone.embeddings.position_embeddings\":\n+            # Original has [1, 577, 1024] with cls token, but refactored expects [1, 576, 1024] without cls token\n+            # Strip the first position (cls token position)\n+            state_dict_new[new_key] = state_dict_old[old_key][:, 1:, :]\n+        else:\n+            state_dict_new[new_key] = state_dict_old[old_key]\n+\n+    state_dict_new[\"tracker_model.shared_image_embedding.positional_embedding\"] = state_dict_new[\n+        \"tracker_model.prompt_encoder.shared_embedding.positional_embedding\"\n+    ]\n+    state_dict_new[\"tracker_model.prompt_encoder.point_embed.weight\"] = torch.cat(\n+        [state_dict_new.pop(f\"tracker_model.prompt_encoder.point_embed.{i}.weight\") for i in range(4)],\n+        dim=0,\n+    )\n+    del state_dict_old\n+    gc.collect()\n+\n+    # Split combined QKV projections into separate Q, K, V projections\n+    print(\"Splitting QKV projections...\")\n+    state_dict_new = split_qkv(state_dict_new)\n+\n+    # Transpose CLIP text projection (stored transposed in original)\n+    if \"detector_model.text_encoder.text_projection.weight\" in state_dict_new:\n+        print(\"Transposing CLIP text_projection...\")\n+        state_dict_new[\"detector_model.text_encoder.text_projection.weight\"] = state_dict_new[\n+            \"detector_model.text_encoder.text_projection.weight\"\n+        ].T\n+\n+    # Load into HF models\n+    print(\"Loading weights into Sam3VideoModel...\")\n+    model = Sam3VideoModel(config)\n+    missing_keys, unexpected_keys = model.load_state_dict(state_dict_new, strict=False)\n+\n+    if missing_keys:\n+        logger.warning(f\"Missing keys ({len(missing_keys)}):\")\n+        for key in missing_keys:  # Show more keys for debugging\n+            logger.warning(f\"  - {key}\")\n+\n+    if unexpected_keys:\n+        logger.warning(f\"Unexpected keys ({len(unexpected_keys)}):\")\n+        for key in unexpected_keys:  # Show more keys for debugging\n+            logger.warning(f\"  - {key}\")\n+\n+    # Note: Some missing/unexpected keys are expected:\n+    # - vision_encoder.backbone.embeddings.patch_embeddings.projection.bias: patch projection has bias=False\n+    # - geometry_encoder.mask_encoder.projection.*: this is nn.Identity() in original (no weights)\n+    # - rotary_emb.rope_embeddings: pre-computed in original, computed on-the-fly in refactored\n+    # - text_encoder.text_projection.bias: projection layer might not have bias\n+\n+    # Save model\n+    print(f\"Saving converted model to {output_path}\")\n+    model.save_pretrained(\n+        output_path,\n+        safe_serialization=safe_serialization,\n+    )\n+\n+    # Save processor\n+    print(\"Creating and saving processor...\")\n+    image_processor = Sam3ImageProcessorFast()\n+    video_processor = Sam2VideoVideoProcessor(\n+        image_mean=[0.5, 0.5, 0.5], image_std=[0.5, 0.5, 0.5], size={\"height\": 1008, \"width\": 1008}\n+    )\n+    tokenizer = CLIPTokenizerFast.from_pretrained(\"openai/clip-vit-base-patch32\", max_length=32, model_max_length=32)\n+    processor = Sam3VideoProcessor(\n+        image_processor=image_processor, video_processor=video_processor, tokenizer=tokenizer\n+    )\n+    processor.save_pretrained(output_path)\n+\n+    # Push to hub if requested\n+    if push_to_hub:\n+        if repo_id is None:\n+            raise ValueError(\"repo_id must be provided when push_to_hub=True\")\n+        print(f\"Pushing model to Hub: {repo_id}\")\n+        model.push_to_hub(repo_id, use_temp_dir=True, private=True)\n+        processor.push_to_hub(repo_id, use_temp_dir=True, private=True)\n+\n+    print(\"Conversion complete!\")\n+    print(f\"Model saved successfully to: {output_path}\")\n+\n+    # Cleanup\n+    del state_dict_new, model\n+    gc.collect()\n+\n+    # Verify the conversion by reloading\n+    print(\"Loading saved weights into Sam3TrackerVideoModel...\")\n+    try:\n+        model = Sam3TrackerVideoModel.from_pretrained(output_path)\n+        param_count = sum(p.numel() for p in model.parameters())\n+        print(f\"âœ“ Successfully loaded model with {param_count:,} parameters\")\n+        del model\n+        gc.collect()\n+    except Exception as e:\n+        print(f\"âœ— Failed to reload model: {e}\")\n+        raise e\n+    print(\"Loading saved weights into Sam3TrackerModel...\")\n+    try:\n+        model = Sam3TrackerModel.from_pretrained(output_path)\n+        param_count = sum(p.numel() for p in model.parameters())\n+        print(f\"âœ“ Successfully loaded model with {param_count:,} parameters\")\n+        del model\n+        gc.collect()\n+    except Exception as e:\n+        print(f\"âœ— Failed to reload model: {e}\")\n+        raise e\n+    print(\"Loading saved weights into Sam3Model...\")\n+    try:\n+        model = Sam3Model.from_pretrained(output_path)\n+        param_count = sum(p.numel() for p in model.parameters())\n+        print(f\"âœ“ Successfully loaded model with {param_count:,} parameters\")\n+        del model\n+        gc.collect()\n+    except Exception as e:\n+        print(f\"âœ— Failed to reload model: {e}\")\n+        raise e\n+\n+    print(\"\\nVerifying converted checkpoint can be loaded...\")\n+    try:\n+        model = Sam3VideoModel.from_pretrained(output_path)\n+        param_count = sum(p.numel() for p in model.parameters())\n+        print(f\"âœ“ Successfully loaded model with {param_count:,} parameters\")\n+        del model\n+        gc.collect()\n+    except Exception as e:\n+        print(f\"âœ— Failed to reload model: {e}\")\n+\n+    print(\"\\n\" + \"=\" * 80)\n+    print(\"Conversion finished!\")\n+    print(\"=\" * 80)\n+    print(f\"Output directory: {output_path}\")\n+    print(\"\\nTo test the model, you can run:\")\n+    print(\">>> from transformers import Sam3Model\")\n+    print(f\">>> model = Sam3Model.from_pretrained('{output_path}')\")\n+    print(\"=\" * 80)\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser(description=\"Convert SAM3 checkpoint to HuggingFace format\")\n+    parser.add_argument(\n+        \"--checkpoint_path\",\n+        type=str,\n+        required=True,\n+        help=\"Path to the original SAM3 checkpoint file\",\n+    )\n+    parser.add_argument(\n+        \"--output_path\",\n+        type=str,\n+        required=True,\n+        help=\"Path to save the converted checkpoint\",\n+    )\n+    parser.add_argument(\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether to push the converted model to the Hugging Face Hub\",\n+    )\n+    parser.add_argument(\n+        \"--repo_id\",\n+        type=str,\n+        default=None,\n+        help=\"Repository ID for pushing to Hub (e.g., 'facebook/sam3-large')\",\n+    )\n+    parser.add_argument(\n+        \"--safe_serialization\",\n+        action=\"store_true\",\n+        default=True,\n+        help=\"Whether to save using safetensors format\",\n+    )\n+\n+    args = parser.parse_args()\n+\n+    convert_sam3_checkpoint(\n+        checkpoint_path=args.checkpoint_path,\n+        output_path=args.output_path,\n+        push_to_hub=args.push_to_hub,\n+        repo_id=args.repo_id,\n+        safe_serialization=args.safe_serialization,\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "f0a25518f44b77c4097d3930d3fdc39fc017b5f1",
            "filename": "src/transformers/models/sam3_video/modeling_sam3_video.py",
            "status": "added",
            "additions": 1815,
            "deletions": 0,
            "changes": 1815,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fmodeling_sam3_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fmodeling_sam3_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fmodeling_sam3_video.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,1815 @@\n+# coding=utf-8\n+# Copyright 2025 The Meta AI Authors and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from collections import OrderedDict, defaultdict\n+from copy import deepcopy\n+from dataclasses import dataclass\n+from typing import Any, Optional, Union\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import Tensor\n+from tqdm.auto import tqdm\n+\n+from transformers.models.sam3.modeling_sam3 import Sam3VisionNeck\n+\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import ModelOutput, auto_docstring, is_kernels_available, logging\n+from ..auto import AutoModel\n+from .configuration_sam3_video import Sam3VideoConfig\n+\n+\n+if is_kernels_available():\n+    from kernels import get_kernel\n+\n+logger = logging.get_logger(__name__)\n+\n+cv_utils_kernel = None  # None = not attempted, False = failed, kernel object = success\n+\n+\n+def _load_cv_utils_kernel_once():\n+    \"\"\"Load cv_utils_kernel once on first use.\"\"\"\n+    global cv_utils_kernel\n+    if cv_utils_kernel is not None:\n+        return  # Already attempted loading (successfully or not)\n+\n+    if not is_kernels_available():\n+        logger.warning_once(\n+            \"kernels library is not installed. NMS post-processing, hole filling, and sprinkle removal will be skipped. \"\n+            \"Install it with `pip install kernels` for better mask quality.\"\n+        )\n+        cv_utils_kernel = False\n+        return\n+\n+    try:\n+        cv_utils_kernel = get_kernel(\"kernels-community/cv_utils\")\n+    except Exception as e:\n+        logger.warning_once(\n+            f\"Failed to load cv_utils kernel (your torch/cuda setup may not be supported): {e}. \"\n+            \"NMS post-processing, hole filling, and sprinkle removal will be skipped.\"\n+        )\n+        cv_utils_kernel = False\n+\n+\n+class Sam3VideoInferenceCache:\n+    \"\"\"Cache for vision features and model constants.\"\"\"\n+\n+    def __init__(\n+        self,\n+        inference_device: Union[torch.device, str] = \"cpu\",\n+        inference_state_device: Union[torch.device, str] = \"cpu\",\n+        max_vision_features_cache_size: int = 1,\n+    ):\n+        self.inference_device = inference_device\n+        self.inference_state_device = inference_state_device\n+        self.max_vision_features_cache_size = max_vision_features_cache_size\n+\n+        self._vision_features = {}\n+\n+    def cache_vision_features(self, frame_idx: int, features: dict):\n+        \"\"\"Cache vision features with automatic device management.\"\"\"\n+        cached = {}\n+        if len(self._vision_features) >= self.max_vision_features_cache_size:\n+            # remove the oldest frame\n+            self._vision_features.pop(min(self._vision_features.keys()))\n+\n+        for key, value in features.items():\n+            if isinstance(value, torch.Tensor):\n+                cached[key] = value.to(self.inference_state_device, non_blocking=True)\n+            elif isinstance(value, (list, tuple)) and value and isinstance(value[0], torch.Tensor):\n+                cached[key] = [v.to(self.inference_state_device, non_blocking=True) for v in value]\n+            else:\n+                cached[key] = value\n+        self._vision_features[frame_idx] = cached\n+\n+    def get_vision_features(self, frame_idx: int) -> Optional[dict]:\n+        \"\"\"Get cached vision features, automatically moved to inference device.\"\"\"\n+        if frame_idx not in self._vision_features:\n+            return None\n+\n+        cached = self._vision_features[frame_idx]\n+        moved = {}\n+        for key, value in cached.items():\n+            if isinstance(value, torch.Tensor):\n+                moved[key] = value.to(self.inference_device, non_blocking=True)\n+            elif isinstance(value, (list, tuple)) and value and isinstance(value[0], torch.Tensor):\n+                moved[key] = [v.to(self.inference_device, non_blocking=True) for v in value]\n+            else:\n+                moved[key] = value\n+        return moved\n+\n+    def clear_all(self):\n+        \"\"\"Clear all cached data.\"\"\"\n+        self._vision_features.clear()\n+\n+\n+class Sam3VideoInferenceSession:\n+    r\"\"\"\n+    Manages video inference session parameters, state and cache.\n+\n+    Args:\n+        video (`torch.FloatTensor`, *optional*):\n+            The video to process. No need to provide when streaming.\n+        video_height (`int`, *optional*):\n+            The height of the video.\n+        video_width (`int`, *optional*):\n+            The width of the video.\n+        inference_device (`torch.device`, *optional*, defaults to `\"cpu\"`):\n+            The device to use for inference.\n+        inference_state_device (`torch.device`, *optional*, defaults to `\"cpu\"`):\n+            The device to store the inference state on.\n+        video_storage_device (`torch.device`, *optional*, defaults to `\"cpu\"`):\n+            The device to store the video on.\n+        dtype (`torch.dtype`, *optional*, defaults to `\"float32\"`):\n+            The dtype to use for the video.\n+        max_vision_features_cache_size (`int`, *optional*, defaults to 1):\n+            The maximum number of vision features to cache.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        video: Optional[torch.FloatTensor] = None,\n+        video_height: Optional[int] = None,\n+        video_width: Optional[int] = None,\n+        inference_device: Union[torch.device, str] = \"cpu\",\n+        inference_state_device: Union[torch.device, str] = \"cpu\",\n+        video_storage_device: Union[torch.device, str] = \"cpu\",\n+        dtype: Union[torch.dtype, str] = \"float32\",\n+        max_vision_features_cache_size: int = 1,\n+    ):\n+        # store as a dictionary to avoid double memory allocation with torch.cat when adding new frames\n+        self.processed_frames = (\n+            dict(enumerate(video.to(video_storage_device, dtype=dtype))) if video is not None else None\n+        )\n+        self.video_height = video_height\n+        self.video_width = video_width\n+\n+        self.inference_device = inference_device\n+        self.inference_state_device = inference_state_device\n+        self.video_storage_device = video_storage_device\n+        self.dtype = dtype\n+        self.max_vision_features_cache_size = max_vision_features_cache_size\n+\n+        # Cache for computed features\n+        self.cache = Sam3VideoInferenceCache(\n+            inference_device=self.inference_device,\n+            inference_state_device=self.inference_state_device,\n+            max_vision_features_cache_size=self.max_vision_features_cache_size,\n+        )\n+\n+        # Persistent object tracking state\n+        self._obj_id_to_idx = OrderedDict()\n+        self._obj_idx_to_id = OrderedDict()\n+        self.obj_ids = []\n+\n+        self.mask_inputs_per_obj = {}\n+        self.point_inputs_per_obj = {}\n+\n+        # Persistent model outputs/history\n+        self.output_dict_per_obj = {}\n+        self.frames_tracked_per_obj = {}\n+\n+        # Session state flags\n+        self.has_new_text_input = False\n+\n+        # Detection-specific state\n+        self.text_input_ids = None  # Cached text input ids for the video\n+        self.text_embeddings = None  # Cached text embeddings for the video\n+        self.text_attention_mask = None  # Cached text attention mask for the video\n+\n+        # Tracking metadata for detection-tracking fusion\n+        self.obj_id_to_score = {}  # Detection scores per object\n+        self.obj_id_to_tracker_score_frame_wise = defaultdict(dict)  # Frame-wise tracker scores\n+        self.obj_id_to_last_occluded = {}  # Last occlusion frame per object\n+        self.max_obj_id = -1  # Maximum object ID assigned so far (-1 means no object has been assigned yet)\n+\n+        # Hotstart metadata\n+        self.obj_first_frame_idx = {}  # First frame index per object\n+        self.unmatched_frame_inds = defaultdict(list)  # Unmatched frame indices per object\n+        self.overlap_pair_to_frame_inds = defaultdict(list)  # Overlap tracking for duplicate detection\n+        self.trk_keep_alive = {}  # Keep-alive counters per object\n+        self.removed_obj_ids = set()  # Set of removed object IDs\n+        self.suppressed_obj_ids = defaultdict(set)  # Suppressed object IDs per frame\n+        self.hotstart_removed_obj_ids = set()  # Set of removed object IDs during hotstart\n+\n+        # Output buffering for hotstart delay\n+        self.output_buffer = []\n+\n+    @property\n+    def num_frames(self) -> Optional[int]:\n+        \"\"\"Number of frames in the video.\"\"\"\n+        return len(self.processed_frames) if self.processed_frames is not None else None\n+\n+    # Object management\n+    def obj_id_to_idx(self, obj_id: int) -> int:\n+        \"\"\"Map object ID to index, creating new entry if needed.\"\"\"\n+        if obj_id not in self._obj_id_to_idx:\n+            obj_idx = len(self._obj_id_to_idx)\n+            self._obj_id_to_idx[obj_id] = obj_idx\n+            self._obj_idx_to_id[obj_idx] = obj_id\n+            self.obj_ids.append(obj_id)\n+\n+            self.mask_inputs_per_obj[obj_idx] = {}\n+            self.point_inputs_per_obj[obj_idx] = {}\n+            self.output_dict_per_obj[obj_idx] = {\n+                \"cond_frame_outputs\": {},\n+                \"non_cond_frame_outputs\": {},\n+            }\n+            self.frames_tracked_per_obj[obj_idx] = {}\n+        return self._obj_id_to_idx[obj_id]\n+\n+    # Video Inference specific functions\n+    def obj_idx_to_id(self, obj_idx: int) -> int:\n+        \"\"\"Map model-side object index to client-side object id.\"\"\"\n+        return self._obj_idx_to_id[obj_idx]\n+\n+    def get_obj_num(self) -> int:\n+        \"\"\"Get the total number of unique object ids received so far in this session.\"\"\"\n+        return len(self._obj_idx_to_id)\n+\n+    def add_mask_inputs(self, obj_idx: int, frame_idx: int, inputs: torch.Tensor):\n+        \"\"\"Add mask inputs with automatic device placement.\"\"\"\n+        self.mask_inputs_per_obj[obj_idx][frame_idx] = inputs.to(\n+            self.inference_device, dtype=self.dtype, non_blocking=True\n+        )\n+\n+    def remove_mask_inputs(self, obj_idx: int, frame_idx: int):\n+        \"\"\"Remove mask inputs.\"\"\"\n+        self.mask_inputs_per_obj[obj_idx].pop(frame_idx, None)\n+\n+    def remove_object(self, obj_id: int, strict: bool = False):\n+        \"\"\"\n+        Remove an object from the inference session. This would remove the object from\n+        all frames in the video.\n+\n+        Args:\n+            obj_id (`int`): The object ID to remove.\n+        \"\"\"\n+        old_obj_idx_to_rm = self._obj_id_to_idx.get(obj_id, None)\n+        # Check whether this object_id to remove actually exists and possibly raise an error.\n+        if old_obj_idx_to_rm is None:\n+            if not strict:\n+                return\n+            raise RuntimeError(\n+                f\"Cannot remove object id {obj_id} as it doesn't exist. All existing object ids: {self.obj_ids}.\"\n+            )\n+\n+        # If this is the only remaining object id, we simply reset the state.\n+        if len(self._obj_id_to_idx) == 1:\n+            self.reset_inference_session()\n+            return\n+\n+        # Step 1: Update the object id mapping (note that it must be done after Step 0,\n+        # since Step 0 still requires the old object id mappings in inference_state)\n+        old_obj_ids = self.obj_ids\n+        old_obj_inds = list(range(len(old_obj_ids)))\n+        remain_old_obj_inds = old_obj_inds.copy()\n+        remain_old_obj_inds.remove(old_obj_idx_to_rm)\n+        new_obj_ids = [old_obj_ids[old_idx] for old_idx in remain_old_obj_inds]\n+        new_obj_inds = list(range(len(new_obj_ids)))\n+        # build new mappings\n+        old_idx_to_new_idx = dict(zip(remain_old_obj_inds, new_obj_inds))\n+        self._obj_id_to_idx = dict(zip(new_obj_ids, new_obj_inds))\n+        self._obj_idx_to_id = dict(zip(new_obj_inds, new_obj_ids))\n+        self.obj_ids = new_obj_ids\n+\n+        # Step 2: For per-object tensor storage, we shift their obj_idx in the dict keys.\n+        def _map_keys(container):\n+            new_kvs = []\n+            for k in old_obj_inds:\n+                v = container.pop(k)\n+                if k in old_idx_to_new_idx:\n+                    new_kvs.append((old_idx_to_new_idx[k], v))\n+            container.update(new_kvs)\n+\n+        _map_keys(self.point_inputs_per_obj)\n+        _map_keys(self.mask_inputs_per_obj)\n+        _map_keys(self.output_dict_per_obj)\n+        _map_keys(self.frames_tracked_per_obj)\n+\n+    # Output management with smart device placement\n+    def store_output(\n+        self,\n+        obj_idx: int,\n+        frame_idx: int,\n+        output_key: Optional[str] = None,\n+        output_value: Optional[Union[torch.Tensor, dict]] = None,\n+        is_conditioning_frame: bool = True,\n+    ):\n+        \"\"\"\n+        Store output with smart device management.\n+        If output_key is None, the output is stored as a dictionary.\n+\n+        Args:\n+            obj_idx (int): The index of the object.\n+            frame_idx (int): The index of the frame.\n+            output_key (Optional[str]): The key of the output. If None, the output is stored as a dictionary.\n+            output_value (Optional[Union[torch.Tensor, dict]]): The value of the output.\n+            is_conditioning_frame (bool): Whether the output is for a conditioning frame.\n+        \"\"\"\n+        storage_key = \"cond_frame_outputs\" if is_conditioning_frame else \"non_cond_frame_outputs\"\n+\n+        if output_key is None and isinstance(output_value, dict):\n+            self.output_dict_per_obj[obj_idx][storage_key][frame_idx] = {}\n+            for key, value in output_value.items():\n+                self.store_output(obj_idx, frame_idx, key, value, is_conditioning_frame)\n+            return\n+\n+        # Device placement: small tensors stay on inference device, large ones go to inference state device\n+        if output_key in [\"object_pointer\", \"object_score_logits\"]:  # Small tensors\n+            self.output_dict_per_obj[obj_idx][storage_key][frame_idx][output_key] = output_value\n+        elif isinstance(output_value, torch.Tensor):  # Large tensors like masks, features\n+            self.output_dict_per_obj[obj_idx][storage_key][frame_idx][output_key] = output_value.to(\n+                self.inference_state_device, non_blocking=True\n+            )\n+        else:\n+            self.output_dict_per_obj[obj_idx][storage_key][frame_idx][output_key] = output_value\n+\n+    def get_output(\n+        self,\n+        obj_idx: int,\n+        frame_idx: int,\n+        output_key: str,\n+        is_conditioning_frame: bool = True,\n+    ):\n+        \"\"\"\n+        Get output with smart device management.\n+\n+        Args:\n+            obj_idx (int): The index of the object.\n+            frame_idx (int): The index of the frame.\n+            output_key (str): The key of the output.\n+            is_conditioning_frame (bool): Whether the output is for a conditioning frame.\n+        \"\"\"\n+        storage_key = \"cond_frame_outputs\" if is_conditioning_frame else \"non_cond_frame_outputs\"\n+        out = self.output_dict_per_obj[obj_idx][storage_key].get(frame_idx, None)\n+        # move to inference device if needed\n+        if out is None:\n+            return None\n+        value = out[output_key]\n+        if isinstance(value, torch.Tensor):\n+            value = value.to(self.inference_device, non_blocking=True)\n+        return value\n+\n+    # Video frame management\n+    def add_new_frame(self, pixel_values: torch.Tensor, frame_idx: Optional[int] = None) -> int:\n+        \"\"\"Add new frame with automatic device placement.\"\"\"\n+        pixel_values = pixel_values.to(self.video_storage_device, dtype=self.dtype, non_blocking=True)\n+        if pixel_values.dim() == 4:\n+            pixel_values = pixel_values.squeeze(0)\n+\n+        if frame_idx is None:\n+            frame_idx = len(self.processed_frames) if self.processed_frames is not None else 0\n+\n+        if self.processed_frames is None:\n+            self.processed_frames = {frame_idx: pixel_values}\n+        else:\n+            self.processed_frames[frame_idx] = pixel_values\n+\n+        return frame_idx\n+\n+    def get_frame(self, frame_idx: int) -> torch.Tensor:\n+        \"\"\"Get frame from video.\"\"\"\n+        return self.processed_frames[frame_idx].to(self.inference_device, non_blocking=True)\n+\n+    def reset_tracking_data(self):\n+        \"\"\"Reset tracking data but keep cache.\"\"\"\n+        self._obj_id_to_idx.clear()\n+        self._obj_idx_to_id.clear()\n+        self.obj_ids.clear()\n+        self.output_dict_per_obj.clear()\n+        self.frames_tracked_per_obj.clear()\n+        # Note: cache and video data are preserved\n+\n+    def reset_inference_session(self):\n+        \"\"\"Reset tracking data and cache.\"\"\"\n+        self._obj_id_to_idx.clear()\n+        self._obj_idx_to_id.clear()\n+        self.obj_ids.clear()\n+        self.output_dict_per_obj.clear()\n+        self.frames_tracked_per_obj.clear()\n+        self.cache.clear_all()\n+\n+    def reset_state(self):\n+        \"\"\"Reset the inference session state.\"\"\"\n+        self._obj_id_to_idx = OrderedDict()\n+        self._obj_idx_to_id = OrderedDict()\n+        self.obj_ids = []\n+        self.output_dict_per_obj = {}\n+        self.frames_tracked_per_obj = {}\n+\n+        # Reset detection-tracking fusion state\n+        self.text_embeddings = None\n+        self.obj_id_to_score = {}\n+        self.obj_id_to_tracker_score_frame_wise = defaultdict(dict)\n+        self.obj_id_to_last_occluded = {}\n+        self.max_obj_id = 0\n+        self.obj_first_frame_idx = {}\n+        self.unmatched_frame_inds = defaultdict(list)\n+        self.overlap_pair_to_frame_inds = defaultdict(list)\n+        self.trk_keep_alive = {}\n+        self.removed_obj_ids = set()\n+        self.suppressed_obj_ids = defaultdict(set)\n+        self.output_buffer = []\n+\n+        # Clear cache\n+        self.cache.clear_all()\n+\n+\n+@dataclass\n+@auto_docstring(custom_intro=\"Base class for the Sam3Video model's output.\")\n+class Sam3VideoSegmentationOutput(ModelOutput):\n+    r\"\"\"\n+    object_ids (`list[int]`, *optional*):\n+        List of object IDs being tracked in the current frame.\n+    obj_id_to_mask (`dict[int, torch.FloatTensor]`, *optional*):\n+        Dictionary mapping object IDs to their predicted low-resolution masks.\n+        Each mask has shape `(1, H_low, W_low)`.\n+    obj_id_to_score (`dict[int, float]`, *optional*):\n+        Dictionary mapping object IDs to their detection scores.\n+    obj_id_to_tracker_score (`dict[int, float]`, *optional*):\n+        Dictionary mapping object IDs to their tracker scores for the current frame.\n+    removed_obj_ids (`set[int]`, *optional*):\n+        Set of object IDs that have been removed (e.g., via hotstart heuristics).\n+    suppressed_obj_ids (`set[int]`, *optional*):\n+        Set of object IDs that have been suppressed in the current frame.\n+    frame_idx (`int`, *optional*):\n+        The frame index of the video.\n+    \"\"\"\n+\n+    object_ids: Optional[list[int]] = None\n+    obj_id_to_mask: Optional[dict[int, torch.FloatTensor]] = None\n+    obj_id_to_score: Optional[dict[int, float]] = None\n+    obj_id_to_tracker_score: Optional[dict[int, float]] = None\n+    removed_obj_ids: Optional[set[int]] = None\n+    suppressed_obj_ids: Optional[set[int]] = None\n+    frame_idx: Optional[int] = None\n+\n+\n+class Sam3VideoPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = Sam3VideoConfig\n+    base_model_prefix = \"sam3_video\"\n+    main_input_name = \"pixel_values\"\n+    input_modalities = [\"video\", \"text\"]\n+    _supports_sdpa = True\n+    _supports_flash_attn = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+\n+\n+@auto_docstring\n+class Sam3VideoModel(Sam3VideoPreTrainedModel):\n+    all_tied_weights_keys = {}\n+\n+    def __init__(self, config: Sam3VideoConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.detector_model = AutoModel.from_config(config.detector_config)\n+        self.tracker_model = AutoModel.from_config(config.tracker_config, remove_vision_encoder=True)\n+        self.low_res_mask_size = config.low_res_mask_size\n+        self.score_threshold_detection = config.score_threshold_detection\n+        self.det_nms_thresh = config.det_nms_thresh\n+        self.assoc_iou_thresh = config.assoc_iou_thresh\n+        self.trk_assoc_iou_thresh = config.trk_assoc_iou_thresh\n+        self.new_det_thresh = config.new_det_thresh\n+        self.recondition_on_trk_masks = config.recondition_on_trk_masks\n+        # hotstart parameters\n+        self.hotstart_delay = config.hotstart_delay\n+        self.hotstart_unmatch_thresh = config.hotstart_unmatch_thresh\n+        self.hotstart_dup_thresh = config.hotstart_dup_thresh\n+        self.suppress_unmatched_only_within_hotstart = config.suppress_unmatched_only_within_hotstart\n+        self.init_trk_keep_alive = config.init_trk_keep_alive\n+        self.max_trk_keep_alive = config.max_trk_keep_alive\n+        self.min_trk_keep_alive = config.min_trk_keep_alive\n+        self.suppress_overlapping_based_on_recent_occlusion_threshold = (\n+            config.suppress_overlapping_based_on_recent_occlusion_threshold\n+        )\n+        self.decrease_trk_keep_alive_for_empty_masklets = config.decrease_trk_keep_alive_for_empty_masklets\n+        self.fill_hole_area = config.fill_hole_area\n+        self.eval()\n+\n+        # the maximum object number\n+        self.max_num_objects = config.max_num_objects\n+        self.recondition_every_nth_frame = config.recondition_every_nth_frame\n+        self.high_conf_thresh = config.high_conf_thresh\n+        self.high_iou_thresh = config.high_iou_thresh\n+\n+        self.tracker_neck = Sam3VisionNeck(config.detector_config.vision_config)\n+\n+    def get_vision_features_for_tracker(self, vision_embeds: torch.Tensor):\n+        hidden_states = vision_embeds.last_hidden_state\n+        batch_size = hidden_states.shape[0]\n+        height, width = self.tracker_model.prompt_encoder.image_embedding_size\n+        hidden_states_spatial = hidden_states.view(batch_size, height, width, -1).permute(0, 3, 1, 2)\n+\n+        fpn_hidden_states, fpn_position_encoding = self.tracker_neck(hidden_states_spatial)\n+\n+        # precompute projected level 0 and level 1 features in SAM decoder\n+        # to avoid running it again on every SAM click\n+        feature_maps = list(fpn_hidden_states[:-1])\n+        feature_maps[0] = self.tracker_model.mask_decoder.conv_s0(feature_maps[0])\n+        feature_maps[1] = self.tracker_model.mask_decoder.conv_s1(feature_maps[1])\n+\n+        # flatten NxCxHxW to HWxNxC\n+        feature_maps = [feature_map.flatten(2).permute(2, 0, 1) for feature_map in feature_maps]\n+        feature_maps_position_embeddings = [\n+            feature_map_position_embedding.flatten(2).permute(2, 0, 1)\n+            for feature_map_position_embedding in fpn_position_encoding[:-1]\n+        ]\n+        return feature_maps, feature_maps_position_embeddings\n+\n+    def run_detection(\n+        self,\n+        inference_session: Sam3VideoInferenceSession,\n+        vision_embeds: torch.Tensor,\n+    ):\n+        if inference_session.has_new_text_input:\n+            text_embeds = self.detector_model.get_text_features(\n+                input_ids=inference_session.text_input_ids,\n+                attention_mask=inference_session.text_attention_mask,\n+            )\n+            inference_session.text_embeddings = text_embeds\n+            inference_session.has_new_text_input = False\n+        else:\n+            text_embeds = inference_session.text_embeddings\n+        detector_outputs = self.detector_model(\n+            vision_embeds=vision_embeds,\n+            text_embeds=text_embeds,\n+            attention_mask=inference_session.text_attention_mask,\n+        )\n+\n+        pred_logits = detector_outputs.pred_logits\n+        presence_logits = detector_outputs.presence_logits\n+\n+        pred_probs = pred_logits.sigmoid()\n+        presence_scores = presence_logits.sigmoid()\n+        pred_probs = pred_probs * presence_scores\n+\n+        run_nms = self.det_nms_thresh > 0.0\n+        if run_nms:\n+            keep = nms_masks(\n+                pred_probs=pred_probs[0],\n+                pred_masks=detector_outputs.pred_masks[0],\n+                prob_threshold=self.score_threshold_detection,\n+                iou_threshold=self.det_nms_thresh,\n+            )\n+            # set suppressed detections' logits to a very low value\n+            detector_outputs.pred_logits[0] -= 1e4 * (~keep).float()\n+            # Recompute pred_probs after NMS suppression\n+            pred_probs = pred_logits.sigmoid()\n+            pred_probs = pred_probs * presence_scores\n+\n+        pred_boxes_xyxy = detector_outputs.pred_boxes\n+        pred_masks = detector_outputs.pred_masks\n+        # get the positive detection outputs above threshold\n+        pos_pred_idx = torch.where(pred_probs > self.score_threshold_detection)\n+        det_out = {\n+            \"bbox\": pred_boxes_xyxy[pos_pred_idx[0], pos_pred_idx[1]],\n+            \"mask\": pred_masks[pos_pred_idx[0], pos_pred_idx[1]],\n+            \"scores\": pred_probs[pos_pred_idx[0], pos_pred_idx[1]],\n+        }\n+\n+        return det_out\n+\n+    def run_tracker_propagation(\n+        self,\n+        inference_session: Sam3VideoInferenceSession,\n+        frame_idx: int,\n+        reverse: bool,\n+    ):\n+        low_res_masks_list = []\n+        obj_scores_list = []\n+        if len(inference_session.obj_ids) > 0:\n+            # propagate one frame\n+            out = self.tracker_model(\n+                inference_session=inference_session,\n+                frame_idx=frame_idx,\n+                reverse=reverse,\n+                run_mem_encoder=False,\n+            )\n+            out_low_res_masks = out.pred_masks\n+            out_obj_scores = out.object_score_logits\n+\n+            # only 1 frames should be propagated\n+            low_res_masks_list.append(out_low_res_masks.squeeze(1))\n+            obj_scores_list.append(out_obj_scores.squeeze(1))\n+\n+        # concatenate the output masklets from all local inference states\n+        H_mask = W_mask = self.low_res_mask_size\n+        if len(low_res_masks_list) > 0:\n+            low_res_masks = torch.cat(low_res_masks_list, dim=0)\n+            obj_scores = torch.cat(obj_scores_list, dim=0)\n+\n+            # Apply hole filling to the masks\n+            low_res_masks = fill_holes_in_mask_scores(\n+                low_res_masks.unsqueeze(1),\n+                max_area=self.fill_hole_area,\n+                fill_holes=True,\n+                remove_sprinkles=True,\n+            )\n+            low_res_masks = low_res_masks.squeeze(1)\n+        else:\n+            low_res_masks = torch.zeros(0, H_mask, W_mask, device=self.device)\n+            obj_scores = torch.zeros(0, device=self.device)\n+\n+        return low_res_masks, obj_scores\n+\n+    def _associate_det_trk(\n+        self,\n+        det_masks: Tensor,\n+        det_scores: Tensor,\n+        trk_masks: Tensor,\n+        trk_obj_ids: list[int],\n+    ):\n+        \"\"\"\n+        Match detections on the current frame with the existing masklets.\n+\n+        Args:\n+          - det_masks: (N, H, W) tensor of predicted masks\n+          - det_scores: (N,) tensor of detection scores\n+          - trk_masks: (M, H, W) tensor of track masks\n+          - trk_obj_ids: (M,) list of object IDs corresponding to trk_masks\n+\n+        Returns:\n+          - new_det_out_inds: list of new object indices among in FA detection outputs\n+          - unmatched_trk_obj_ids: list of existing masklet object IDs that are not matched\n+            to any detections on this frame (for unmatched, we only count masklets with >0 area)\n+          - det_to_matched_trk_obj_ids: dict[int, list[int]]: mapping from FA detection indices\n+            to the list of matched tracklet object IDs\n+          - empty_trk_obj_ids: list of existing masklet object IDs with zero area in SAM2 prediction\n+        \"\"\"\n+        iou_threshold = self.assoc_iou_thresh\n+        iou_threshold_trk = self.trk_assoc_iou_thresh\n+        new_det_thresh = self.new_det_thresh\n+\n+        trk_obj_ids_tensor = (\n+            torch.tensor(trk_obj_ids, dtype=torch.long, device=det_masks.device)\n+            if trk_obj_ids\n+            else torch.empty(0, dtype=torch.long, device=det_masks.device)\n+        )\n+        if trk_masks.size(0) == 0:\n+            # all detections are new\n+            new_det_out_inds = list(range(det_masks.size(0)))\n+            unmatched_trk_obj_ids = []\n+            empty_trk_obj_ids = []\n+            det_to_matched_trk_obj_ids = {}\n+            trk_id_to_max_iou_high_conf_det = {}\n+            return (\n+                new_det_out_inds,\n+                unmatched_trk_obj_ids,\n+                det_to_matched_trk_obj_ids,\n+                trk_id_to_max_iou_high_conf_det,\n+                empty_trk_obj_ids,\n+            )\n+        elif det_masks.size(0) == 0:\n+            # all previous tracklets are unmatched if they have a non-zero area\n+            new_det_out_inds = []\n+            trk_is_nonempty = (trk_masks > 0).any(dim=(1, 2))  # (M,) tensor\n+            # Use tensor boolean indexing - elegant and avoids intermediate conversions\n+            unmatched_trk_obj_ids = trk_obj_ids_tensor[trk_is_nonempty].tolist()\n+            empty_trk_obj_ids = trk_obj_ids_tensor[~trk_is_nonempty].tolist()\n+            det_to_matched_trk_obj_ids = {}\n+            trk_id_to_max_iou_high_conf_det = {}\n+            return (\n+                new_det_out_inds,\n+                unmatched_trk_obj_ids,\n+                det_to_matched_trk_obj_ids,\n+                trk_id_to_max_iou_high_conf_det,\n+                empty_trk_obj_ids,\n+            )\n+\n+        det_masks_binary = det_masks > 0\n+        trk_masks_binary = trk_masks > 0\n+        ious = mask_iou(det_masks_binary, trk_masks_binary)  # (N, M) tensor\n+\n+        # trk_is_matched: for each track, True if matched to any detection above threshold\n+        trk_is_matched = (ious >= iou_threshold_trk).any(dim=0)  # (M,)\n+        # Non-empty tracks not matched by Hungarian assignment above threshold are unmatched\n+        trk_is_nonempty = trk_masks_binary.any(dim=(1, 2))  # (M,)\n+        trk_is_unmatched = trk_is_nonempty & ~trk_is_matched  # (M,)\n+        # Use tensor boolean indexing directly - no intermediate conversions\n+        unmatched_trk_obj_ids = trk_obj_ids_tensor[trk_is_unmatched].tolist()\n+        empty_trk_obj_ids = trk_obj_ids_tensor[~trk_is_nonempty].tolist()\n+\n+        # For detections: allow many tracks to match to the same detection (many-to-one)\n+        # So, a detection is 'new' if it does not match any track above threshold\n+        det_matches_any_trk = (ious >= iou_threshold).any(dim=1)  # (N,)\n+        is_new_det = (det_scores >= new_det_thresh) & ~det_matches_any_trk  # (N,)\n+        new_det_out_inds = torch.where(is_new_det)[0].tolist()\n+\n+        # Build detection-to-track mappings using tensor operations\n+        det_to_matched_trk_obj_ids = {}\n+        trk_id_to_max_iou_high_conf_det = {}  # trk id --> exactly one detection idx\n+        det_to_max_iou_trk_idx = ious.argmax(dim=1)  # (N,)\n+        det_is_high_conf = (det_scores >= self.high_conf_thresh) & ~is_new_det  # (N,)\n+        det_max_iou = ious.max(dim=1)[0]  # (N,)\n+        det_is_high_iou = det_max_iou >= self.high_iou_thresh  # (N,)\n+        det_is_high_conf_and_iou = det_is_high_conf & det_is_high_iou  # (N,)\n+        high_conf_and_iou_mask = det_is_high_conf_and_iou  # Keep as tensor\n+\n+        for det_idx in range(det_masks.size(0)):\n+            # Find which tracks match this detection using tensor boolean indexing\n+            matched_trk_mask = ious[det_idx] >= iou_threshold  # (M,)\n+            det_to_matched_trk_obj_ids[det_idx] = trk_obj_ids_tensor[matched_trk_mask].tolist()\n+\n+            if high_conf_and_iou_mask[det_idx].item():\n+                trk_idx = det_to_max_iou_trk_idx[det_idx].item()\n+                trk_obj_id = trk_obj_ids_tensor[trk_idx].item()\n+                trk_id_to_max_iou_high_conf_det[trk_obj_id] = det_idx\n+\n+        return (\n+            new_det_out_inds,\n+            unmatched_trk_obj_ids,\n+            det_to_matched_trk_obj_ids,\n+            trk_id_to_max_iou_high_conf_det,\n+            empty_trk_obj_ids,\n+        )\n+\n+    def _process_hotstart(\n+        self,\n+        inference_session: Sam3VideoInferenceSession,\n+        frame_idx: int,\n+        reverse: bool,\n+        det_to_matched_trk_obj_ids: dict[int, list[int]],\n+        new_det_obj_ids: list[int],\n+        empty_trk_obj_ids: list[int],\n+        unmatched_trk_obj_ids: list[int],\n+        extra_metadata: dict[str, Any],\n+        streaming: bool = False,\n+    ):\n+        \"\"\"\n+        Handle hotstart heuristics to remove unmatched or duplicated objects.\n+\n+        In streaming mode, hotstart removal logic is disabled since we don't have\n+        future frames to make informed decisions about object removal.\n+        \"\"\"\n+        # obj_id --> first frame index where the object was detected\n+        obj_first_frame_idx = extra_metadata[\"obj_first_frame_idx\"]\n+        # obj_id --> [mismatched frame indices]\n+        unmatched_frame_inds = extra_metadata[\"unmatched_frame_inds\"]\n+        trk_keep_alive = extra_metadata[\"trk_keep_alive\"]\n+        # (first_appear_obj_id, obj_id) --> [overlap frame indices]\n+        overlap_pair_to_frame_inds = extra_metadata[\"overlap_pair_to_frame_inds\"]\n+        # removed_obj_ids: object IDs that are suppressed via hot-start\n+        removed_obj_ids = extra_metadata[\"removed_obj_ids\"]\n+        suppressed_obj_ids = extra_metadata[\"suppressed_obj_ids\"][frame_idx]\n+\n+        obj_ids_newly_removed = set()  # object IDs to be newly removed on this frame\n+        hotstart_diff = frame_idx - self.hotstart_delay if not reverse else frame_idx + self.hotstart_delay\n+\n+        # Step 1: log the frame index where each object ID first appears\n+        for obj_id in new_det_obj_ids:\n+            if obj_id not in obj_first_frame_idx:\n+                obj_first_frame_idx[obj_id] = frame_idx\n+            trk_keep_alive[int(obj_id)] = self.init_trk_keep_alive\n+\n+        matched_trks = set()\n+        # We use the det-->tracks list to check for matched objects. Otherwise, we need to compute areas to decide whether they're occluded\n+        for matched_trks_per_det in det_to_matched_trk_obj_ids.values():\n+            matched_trks.update({int(obj_id) for obj_id in matched_trks_per_det})\n+        for obj_id in matched_trks:\n+            # NOTE: To minimize number of configurable params, we use the hotstart_unmatch_thresh to set the max value of trk_keep_alive\n+            trk_keep_alive[int(obj_id)] = min(self.max_trk_keep_alive, trk_keep_alive[int(obj_id)] + 1)\n+        for obj_id in unmatched_trk_obj_ids:\n+            unmatched_frame_inds[obj_id].append(frame_idx)\n+            # NOTE: To minimize number of configurable params, we use the hotstart_unmatch_thresh to set the min value of trk_keep_alive\n+            # The max keep alive is 2x the min, means the model prefers to keep the prediction rather than suppress it if it was matched long enough.\n+            trk_keep_alive[int(obj_id)] = max(self.min_trk_keep_alive, trk_keep_alive[int(obj_id)] - 1)\n+        if self.decrease_trk_keep_alive_for_empty_masklets:\n+            for obj_id in empty_trk_obj_ids:\n+                # NOTE: To minimize number of configurable params, we use the hotstart_unmatch_thresh to set the min value of trk_keep_alive\n+                trk_keep_alive[int(obj_id)] = max(self.min_trk_keep_alive, trk_keep_alive[int(obj_id)] - 1)\n+\n+        # Step 2: removed tracks that has not matched with detections for `hotstart_unmatch_thresh` frames with hotstart period\n+        # a) add unmatched frame indices for each existing object ID\n+        # note that `unmatched_trk_obj_ids` contains those frames where the SAM2 output mask\n+        # doesn't match any FA detection; it excludes those frames where SAM2 gives an empty mask\n+        # b) remove a masklet if it first appears after `hotstart_diff` and is unmatched for more\n+        # than `self.hotstart_unmatch_thresh` frames\n+        # NOTE: In streaming mode, we skip hotstart removal logic since we don't have future frames\n+        if not streaming:\n+            for obj_id, frame_indices in unmatched_frame_inds.items():\n+                if obj_id in removed_obj_ids or obj_id in obj_ids_newly_removed:\n+                    continue  # skip if the object is already removed\n+                if len(frame_indices) >= self.hotstart_unmatch_thresh:\n+                    is_within_hotstart = (obj_first_frame_idx[obj_id] > hotstart_diff and not reverse) or (\n+                        obj_first_frame_idx[obj_id] < hotstart_diff and reverse\n+                    )\n+                    if is_within_hotstart:\n+                        obj_ids_newly_removed.add(obj_id)\n+                        logger.info(\n+                            f\"Removing object {obj_id} at frame {frame_idx} \"\n+                            f\"since it is unmatched for frames: {frame_indices}\"\n+                        )\n+                if (\n+                    trk_keep_alive[obj_id] <= 0  # Object has not been matched for too long\n+                    and not self.suppress_unmatched_only_within_hotstart\n+                    and obj_id not in removed_obj_ids\n+                    and obj_id not in obj_ids_newly_removed\n+                ):\n+                    logger.debug(f\"Suppressing object {obj_id} at frame {frame_idx}, due to being unmatched\")\n+                    suppressed_obj_ids.add(obj_id)\n+\n+        # Step 3: removed tracks that overlaps with another track for `hotstart_dup_thresh` frames\n+        # a) find overlaps tracks -- we consider overlap if they match to the same detection\n+        # NOTE: In streaming mode, we still track overlaps for metadata but skip removal logic\n+        for matched_trk_obj_ids in det_to_matched_trk_obj_ids.values():\n+            if len(matched_trk_obj_ids) < 2:\n+                continue  # only count detections that are matched to multiple (>=2) masklets\n+            # if there are multiple matched track ids, we need to find the one that appeared first;\n+            # these later appearing ids may be removed since they may be considered as duplicates\n+            first_appear_obj_id = (\n+                min(matched_trk_obj_ids, key=lambda x: obj_first_frame_idx[x])\n+                if not reverse\n+                else max(matched_trk_obj_ids, key=lambda x: obj_first_frame_idx[x])\n+            )\n+            for obj_id in matched_trk_obj_ids:\n+                if obj_id != first_appear_obj_id:\n+                    key = (first_appear_obj_id, obj_id)\n+                    overlap_pair_to_frame_inds[key].append(frame_idx)\n+\n+        # b) remove a masklet if it first appears after `hotstart_diff` and it overlaps with another\n+        # masklet (that appears earlier) for more than `self.hotstart_dup_thresh` frames\n+        # NOTE: In streaming mode, we skip hotstart removal logic since we don't have future frames\n+        if not streaming:\n+            for (first_obj_id, obj_id), frame_indices in overlap_pair_to_frame_inds.items():\n+                if obj_id in removed_obj_ids or obj_id in obj_ids_newly_removed:\n+                    continue  # skip if the object is already removed\n+                if (obj_first_frame_idx[obj_id] > hotstart_diff and not reverse) or (\n+                    obj_first_frame_idx[obj_id] < hotstart_diff and reverse\n+                ):\n+                    if len(frame_indices) >= self.hotstart_dup_thresh:\n+                        obj_ids_newly_removed.add(obj_id)\n+                        logger.info(\n+                            f\"Removing object {obj_id} at frame {frame_idx} \"\n+                            f\"since it overlaps with another track {first_obj_id} at frames: {frame_indices}\"\n+                        )\n+\n+        removed_obj_ids.update(obj_ids_newly_removed)\n+        return obj_ids_newly_removed, extra_metadata\n+\n+    def run_memory_encoder(\n+        self,\n+        inference_session: Sam3VideoInferenceSession,\n+        frame_idx: int,\n+        high_res_masks: torch.Tensor,\n+        object_score_logits: torch.Tensor,\n+    ):\n+        \"\"\"\n+        Run the memory encoder on `high_res_masks`. This is usually after applying\n+        non-overlapping constraints to object scores. Since their scores changed, their\n+        memory also need to be computed again with the memory encoder.\n+        \"\"\"\n+        # Retrieve correct image features\n+        cached_features = inference_session.cache.get_vision_features(frame_idx)\n+        current_vision_feats = cached_features[\"vision_feats\"]\n+        maskmem_features, maskmem_pos_enc = self.tracker_model._encode_new_memory(\n+            current_vision_feats=current_vision_feats[-1],\n+            pred_masks_high_res=high_res_masks,\n+            object_score_logits=object_score_logits,\n+            is_mask_from_pts=False,\n+        )\n+        return maskmem_features, maskmem_pos_enc\n+\n+    def _prepare_recondition_masks(\n+        self,\n+        inference_session: Sam3VideoInferenceSession,\n+        frame_idx: int,\n+        det_out: dict[str, Tensor],\n+        trk_masks: Tensor,\n+        trk_id_to_max_iou_high_conf_det: dict[int, int],\n+        tracker_obj_scores_global: Tensor,\n+    ) -> dict[int, Tensor]:\n+        \"\"\"\n+        Prepare high-resolution masks for reconditioned objects.\n+        Returns a dict of obj_idx -> high_res_mask for objects that should be reconditioned.\n+\n+        When recondition_on_trk_masks=True, uses detector as validation signal to strengthen tracker memory.\n+        When False, uses detector to correct tracker drift by replacing with detection masks.\n+        \"\"\"\n+        reconditioned_masks = {}\n+        reconditioned_obj_ids = set()\n+\n+        for trk_obj_id, det_idx in trk_id_to_max_iou_high_conf_det.items():\n+            obj_idx = inference_session.obj_id_to_idx(trk_obj_id)\n+            obj_score = tracker_obj_scores_global[obj_idx]\n+            if obj_score <= self.high_conf_thresh:\n+                continue\n+\n+            if self.recondition_on_trk_masks:\n+                # Validation mode: detector confirms tracker quality, strengthen memory with tracked mask\n+                new_mask = trk_masks[obj_idx : obj_idx + 1].unsqueeze(1)\n+                reconditioned_masks[obj_idx] = new_mask\n+                reconditioned_obj_ids.add(trk_obj_id)\n+            else:\n+                # Correction mode: detector corrects drift, replace tracker mask with detection mask\n+                new_mask = det_out[\"mask\"][det_idx : det_idx + 1].unsqueeze(1)\n+                reconditioned_masks[obj_idx] = new_mask >= 0.5\n+                reconditioned_obj_ids.add(trk_obj_id)\n+\n+        return reconditioned_masks, reconditioned_obj_ids\n+\n+    def _get_objects_to_suppress_based_on_most_recently_occluded(\n+        self,\n+        binary_low_res_masks: Tensor,\n+        last_occluded: list[int],\n+        obj_ids: list[int],\n+        reverse: bool = False,\n+    ):\n+        # Suppress overlapping masks for objects that were most recently occluded\n+        to_suppress = torch.zeros(\n+            binary_low_res_masks.size(0),\n+            device=binary_low_res_masks.device,\n+            dtype=torch.bool,\n+        )\n+        if len(obj_ids) <= 1:\n+            return to_suppress\n+\n+        iou = mask_iou(binary_low_res_masks, binary_low_res_masks)  # [N,N]\n+\n+        # Create masks for upper triangular matrix (i < j) and IoU threshold\n+        mask_iou_thresh = iou >= self.suppress_overlapping_based_on_recent_occlusion_threshold\n+        overlapping_pairs = torch.triu(mask_iou_thresh, diagonal=1)  # [N,N]\n+\n+        last_occ_expanded_i = last_occluded.unsqueeze(1)  # (N, 1)\n+        last_occ_expanded_j = last_occluded.unsqueeze(0)  # (1, N)\n+        # Suppress most recently occluded\n+        cmp_op = torch.gt if not reverse else torch.lt\n+        suppress_i_mask = (\n+            overlapping_pairs\n+            & cmp_op(last_occ_expanded_i, last_occ_expanded_j)  # (last_occ_expanded_i > last_occ_expanded_j)\n+            & (last_occ_expanded_j > -1)  # j can suppress i only if i was previously occluded\n+        )\n+        suppress_j_mask = (\n+            overlapping_pairs\n+            & cmp_op(last_occ_expanded_j, last_occ_expanded_i)\n+            & (last_occ_expanded_i > -1)  # i can suppress j only if j was previously occluded\n+        )\n+        # Apply suppression\n+        to_suppress = suppress_i_mask.any(dim=1) | suppress_j_mask.any(dim=0)\n+\n+        return to_suppress\n+\n+    def _suppress_overlapping_based_on_recent_occlusion(\n+        self,\n+        inference_session: Sam3VideoInferenceSession,\n+        frame_idx: int,\n+        tracker_low_res_masks_global: Tensor,\n+        tracker_metadata_new: dict[str, Any],\n+        obj_ids_newly_removed: set[int],\n+        reverse: bool = False,\n+    ):\n+        \"\"\"\n+        Suppress overlapping masks based on the most recent occlusion information. If an object is removed by hotstart, we always suppress it if it overlaps with any other object.\n+        Args:\n+            frame_idx (int): The current frame index.\n+            tracker_low_res_masks_global (Tensor): The low-resolution masks for the current frame.\n+            tracker_metadata_prev (Dict[str, Any]): The metadata from the previous frame.\n+            tracker_metadata_new (Dict[str, Any]): The metadata for the current frame.\n+            obj_ids_newly_removed (Set[int]): The object IDs that have been removed.\n+        Return:\n+            Tensor: The updated low-resolution masks with some objects suppressed.\n+        \"\"\"\n+        obj_ids_global = inference_session.obj_ids\n+        binary_tracker_low_res_masks_global = tracker_low_res_masks_global > 0\n+        batch_size = tracker_low_res_masks_global.size(0)\n+        if batch_size > 0:\n+            NEVER_OCCLUDED = -1\n+            ALWAYS_OCCLUDED = 100000  # This value should be larger than any possible frame index, indicates that the object was removed by hotstart logic\n+            last_occluded_prev = torch.cat(\n+                [\n+                    inference_session.obj_id_to_last_occluded.get(\n+                        obj_id,\n+                        torch.full(\n+                            (1,),\n+                            fill_value=(NEVER_OCCLUDED if obj_id not in obj_ids_newly_removed else ALWAYS_OCCLUDED),\n+                            device=binary_tracker_low_res_masks_global.device,\n+                            dtype=torch.long,\n+                        ),\n+                    )\n+                    for obj_id in obj_ids_global\n+                ],\n+                dim=0,\n+            )\n+            to_suppress = self._get_objects_to_suppress_based_on_most_recently_occluded(\n+                binary_tracker_low_res_masks_global,\n+                last_occluded_prev,\n+                obj_ids_global,\n+                reverse,\n+            )\n+\n+            # Update metadata with occlusion information\n+            is_obj_occluded = ~(binary_tracker_low_res_masks_global.any(dim=(-1, -2)))\n+            is_obj_occluded_or_suppressed = is_obj_occluded | to_suppress\n+            last_occluded_new = last_occluded_prev.clone()\n+            last_occluded_new[is_obj_occluded_or_suppressed] = frame_idx\n+            # Slice out the last occluded frame for each object\n+            tracker_metadata_new[\"obj_id_to_last_occluded\"] = {\n+                obj_id: last_occluded_new[obj_idx : obj_idx + 1] for obj_idx, obj_id in enumerate(obj_ids_global)\n+            }\n+\n+            # Zero out suppressed masks before memory encoding\n+            NO_OBJ_LOGIT = -10\n+            tracker_low_res_masks_global[to_suppress] = NO_OBJ_LOGIT\n+\n+        return tracker_low_res_masks_global\n+\n+    def _apply_non_overlapping_constraints(self, pred_masks):\n+        \"\"\"\n+        Apply non-overlapping constraints to the object scores in pred_masks. Here we\n+        keep only the highest scoring object at each spatial location in pred_masks.\n+        \"\"\"\n+        batch_size = pred_masks.size(0)\n+        if batch_size == 1:\n+            return pred_masks\n+\n+        device = pred_masks.device\n+        # \"max_obj_inds\": object index of the object with the highest score at each location\n+        max_obj_inds = torch.argmax(pred_masks, dim=0, keepdim=True)\n+        # \"batch_obj_inds\": object index of each object slice (along dim 0) in `pred_masks`\n+        batch_obj_inds = torch.arange(batch_size, device=device)[:, None, None, None]\n+        keep = max_obj_inds == batch_obj_inds\n+        # suppress overlapping regions' scores below -10.0 so that the foreground regions\n+        # don't overlap (here sigmoid(-10.0)=4.5398e-05)\n+        pred_masks = torch.where(keep, pred_masks, torch.clamp(pred_masks, max=-10.0))\n+        return pred_masks\n+\n+    def _suppress_shrinked_masks(self, pred_masks, new_pred_masks, shrink_threshold=0.3):\n+        area_before = (pred_masks > 0).sum(dim=(-1, -2))\n+        area_after = (new_pred_masks > 0).sum(dim=(-1, -2))\n+        area_before = torch.clamp(area_before, min=1.0)\n+        area_ratio = area_after / area_before\n+        keep = area_ratio >= shrink_threshold\n+        keep_mask = keep[..., None, None].expand_as(pred_masks)\n+        pred_masks_after = torch.where(keep_mask, pred_masks, torch.clamp(pred_masks, max=-10.0))\n+        return pred_masks_after\n+\n+    def _suppress_object_pw_area_shrinkage(self, pred_masks):\n+        \"\"\"\n+        This function suppresses masks that shrink in area after applying pixelwise non-overlapping constriants.\n+        Note that the final output can still be overlapping.\n+        \"\"\"\n+        # Apply pixel-wise non-overlapping constraint based on mask scores\n+        pixel_level_non_overlapping_masks = self._apply_non_overlapping_constraints(pred_masks)\n+        # Fully suppress masks with high shrinkage (probably noisy) based on the pixel wise non-overlapping constraints\n+        # NOTE: The output of this function can be a no op if none of the masks shrinked by a large factor.\n+        pred_masks = self._suppress_shrinked_masks(pred_masks, pixel_level_non_overlapping_masks)\n+        return pred_masks\n+\n+    def _tracker_update_memories(\n+        self,\n+        inference_session: Sam3VideoInferenceSession,\n+        frame_idx: int,\n+        low_res_masks: Tensor,\n+        reconditioned_masks: Optional[dict[int, Tensor]] = None,\n+    ):\n+        \"\"\"\n+        Run Sam3Tracker memory encoder, enforcing non-overlapping constraints globally.\n+        Now with batched memory encoding for better performance.\n+\n+        Args:\n+            inference_session: The inference session state\n+            frame_idx: Current frame index\n+            low_res_masks: Low-resolution tracker masks for all objects\n+            reconditioned_masks: Optional dict of obj_idx -> high_res_mask for objects that\n+                                should use detection masks instead of tracker masks\n+        \"\"\"\n+        if len(inference_session.obj_ids) == 0:\n+            return\n+\n+        if reconditioned_masks is None:\n+            reconditioned_masks = {}\n+        # Interpolate tracker masks to high resolution\n+        high_res_masks = low_res_masks.unsqueeze(1)\n+\n+        # Override with detection masks for reconditioned objects\n+        for obj_idx, recond_mask in reconditioned_masks.items():\n+            high_res_masks[obj_idx] = recond_mask.float()\n+            # Mark as conditioning frame for reconditioned objects\n+            output_dict = inference_session.output_dict_per_obj[obj_idx]\n+            if frame_idx in output_dict[\"non_cond_frame_outputs\"]:\n+                current_out = output_dict[\"non_cond_frame_outputs\"].pop(frame_idx)\n+                output_dict[\"cond_frame_outputs\"][frame_idx] = current_out\n+\n+        # Apply non-overlapping constraints before memory encoding\n+        high_res_masks = self._suppress_object_pw_area_shrinkage(high_res_masks)\n+        # Use mask areas as a proxy for object scores\n+        object_score_logits = torch.where((high_res_masks > 0).any(dim=(-1, -2)), 10.0, -10.0)\n+\n+        # Run memory encoder in batch for all objects at once\n+        num_objects = len(inference_session.obj_ids)\n+        object_score_logits_batched = object_score_logits.unsqueeze(-1)  # Shape: (num_objects, 1)\n+\n+        # Encode memories for all objects in one batch call\n+        maskmem_features_batched, maskmem_pos_enc_batched = self.run_memory_encoder(\n+            inference_session,\n+            frame_idx,\n+            high_res_masks,  # Shape: (num_objects, 1, H, W)\n+            object_score_logits_batched,  # Shape: (num_objects, 1)\n+        )\n+\n+        # Split and store encoded memories per object\n+        for obj_idx in range(num_objects):\n+            output_dict = inference_session.output_dict_per_obj[obj_idx]\n+            # Extract per-object memory from batched result\n+            maskmem_features = maskmem_features_batched[:, obj_idx : obj_idx + 1]\n+            maskmem_pos_enc = maskmem_pos_enc_batched[:, obj_idx : obj_idx + 1]\n+\n+            for storage_key in [\"cond_frame_outputs\", \"non_cond_frame_outputs\"]:\n+                if frame_idx not in output_dict[storage_key]:\n+                    continue\n+                current_out = output_dict[storage_key][frame_idx]\n+                current_out[\"maskmem_features\"] = maskmem_features\n+                current_out[\"maskmem_pos_enc\"] = maskmem_pos_enc\n+\n+    def run_tracker_update_planning_phase(\n+        self,\n+        inference_session: Sam3VideoInferenceSession,\n+        frame_idx: int,\n+        reverse: bool,\n+        det_out: dict[str, Tensor],\n+        tracker_low_res_masks_global: Tensor,\n+        tracker_obj_scores_global: Tensor,\n+        streaming: bool = False,\n+    ):\n+        # initialize new metadata from previous metadata (its values will be updated later)\n+        tracker_metadata_new = {\n+            \"obj_ids\": deepcopy(inference_session.obj_ids),\n+            \"obj_id_to_score\": deepcopy(inference_session.obj_id_to_score),\n+            \"obj_id_to_tracker_score_frame_wise\": deepcopy(inference_session.obj_id_to_tracker_score_frame_wise),\n+            \"obj_id_to_last_occluded\": {},  # will be filled later\n+            \"max_obj_id\": deepcopy(inference_session.max_obj_id),\n+        }\n+\n+        # Initialize reconditioned_obj_ids early to avoid UnboundLocalError\n+        reconditioned_obj_ids = set()\n+\n+        # Step 1: make the update plan and resolve heuristics\n+        det_mask_preds: Tensor = det_out[\"mask\"]  # low-res mask logits\n+        det_scores: Tensor = det_out[\"scores\"].float()  # Keep as tensor!\n+        # a) match FA and SAM2 masks and find new objects\n+        (\n+            new_det_out_inds,\n+            unmatched_trk_obj_ids,\n+            det_to_matched_trk_obj_ids,\n+            trk_id_to_max_iou_high_conf_det,\n+            empty_trk_obj_ids,\n+        ) = self._associate_det_trk(\n+            det_masks=det_mask_preds,\n+            det_scores=det_scores,\n+            trk_masks=tracker_low_res_masks_global,\n+            trk_obj_ids=inference_session.obj_ids,\n+        )\n+\n+        # check whether we've hit the maximum number of objects we can track (and if so, drop some detections)\n+        prev_obj_num = len(inference_session.obj_ids)\n+        new_det_num = len(new_det_out_inds)\n+        num_obj_dropped_due_to_limit = 0\n+        if prev_obj_num + new_det_num > self.max_num_objects:\n+            logger.warning(f\"hitting {self.max_num_objects=} with {new_det_num=} and {prev_obj_num=}\")\n+            new_det_num_to_keep = self.max_num_objects - prev_obj_num\n+            num_obj_dropped_due_to_limit = new_det_num - new_det_num_to_keep\n+            # Keep top scoring detections\n+            new_det_inds_tensor = torch.tensor(new_det_out_inds, dtype=torch.long, device=det_scores.device)\n+            scores_for_new_dets = det_scores[new_det_inds_tensor]\n+            _, top_inds = torch.topk(scores_for_new_dets, k=new_det_num_to_keep, largest=True)\n+            new_det_out_inds = [new_det_out_inds[i] for i in top_inds]\n+            new_det_num = len(new_det_out_inds)\n+\n+        # assign object IDs to new detections\n+        new_det_start_obj_id = inference_session.max_obj_id + 1\n+        new_det_obj_ids = list(range(new_det_start_obj_id, new_det_start_obj_id + new_det_num))\n+\n+        # b) handle hotstart heuristics to remove objects\n+        extra_metadata_new = deepcopy(\n+            {\n+                \"obj_first_frame_idx\": inference_session.obj_first_frame_idx,\n+                \"unmatched_frame_inds\": inference_session.unmatched_frame_inds,\n+                \"trk_keep_alive\": inference_session.trk_keep_alive,\n+                \"overlap_pair_to_frame_inds\": inference_session.overlap_pair_to_frame_inds,\n+                \"removed_obj_ids\": inference_session.removed_obj_ids,\n+                \"suppressed_obj_ids\": inference_session.suppressed_obj_ids,\n+            }\n+        )\n+\n+        obj_ids_newly_removed, extra_metadata_new = self._process_hotstart(\n+            inference_session=inference_session,\n+            frame_idx=frame_idx,\n+            reverse=reverse,\n+            det_to_matched_trk_obj_ids=det_to_matched_trk_obj_ids,\n+            new_det_obj_ids=new_det_obj_ids,\n+            empty_trk_obj_ids=empty_trk_obj_ids,\n+            unmatched_trk_obj_ids=unmatched_trk_obj_ids,\n+            extra_metadata=extra_metadata_new,\n+            streaming=streaming,\n+        )\n+        tracker_metadata_new[\"extra_metadata\"] = extra_metadata_new\n+\n+        # Step 3 (optional): prepare reconditioned masks based on high-confidence detections\n+        reconditioned_masks = {}\n+        reconditioned_obj_ids = set()\n+        should_recondition_periodic = (\n+            self.recondition_every_nth_frame > 0\n+            and frame_idx % self.recondition_every_nth_frame == 0\n+            and len(trk_id_to_max_iou_high_conf_det) > 0\n+        )\n+\n+        if should_recondition_periodic:\n+            reconditioned_masks, reconditioned_obj_ids = self._prepare_recondition_masks(\n+                inference_session=inference_session,\n+                frame_idx=frame_idx,\n+                det_out=det_out,\n+                trk_masks=tracker_low_res_masks_global,\n+                trk_id_to_max_iou_high_conf_det=trk_id_to_max_iou_high_conf_det,\n+                tracker_obj_scores_global=tracker_obj_scores_global,\n+            )\n+\n+        tracker_update_plan = {\n+            \"new_det_out_inds\": new_det_out_inds,  # List[int]\n+            \"new_det_obj_ids\": new_det_obj_ids,  # List[int]\n+            \"unmatched_trk_obj_ids\": unmatched_trk_obj_ids,  # List[int]\n+            \"det_to_matched_trk_obj_ids\": det_to_matched_trk_obj_ids,  # dict\n+            \"obj_ids_newly_removed\": obj_ids_newly_removed,  # set\n+            \"num_obj_dropped_due_to_limit\": num_obj_dropped_due_to_limit,  # int\n+            \"trk_id_to_max_iou_high_conf_det\": trk_id_to_max_iou_high_conf_det,  # dict\n+            \"reconditioned_obj_ids\": reconditioned_obj_ids,  # set\n+        }\n+\n+        # Step 4: Run SAM2 memory encoder on the current frame's prediction masks\n+        # This uses tracker masks for most objects, but detection masks for reconditioned objects\n+        batch_size = tracker_low_res_masks_global.size(0)\n+        if batch_size > 0:\n+            if self.suppress_overlapping_based_on_recent_occlusion_threshold > 0.0:\n+                # NOTE: tracker_low_res_masks_global is updated in-place then returned\n+                tracker_low_res_masks_global = self._suppress_overlapping_based_on_recent_occlusion(\n+                    inference_session=inference_session,\n+                    frame_idx=frame_idx,\n+                    tracker_low_res_masks_global=tracker_low_res_masks_global,\n+                    tracker_metadata_new=tracker_metadata_new,\n+                    obj_ids_newly_removed=obj_ids_newly_removed,\n+                    reverse=reverse,\n+                )\n+\n+            # Unified memory encoding: uses detection masks for reconditioned objects\n+            self._tracker_update_memories(\n+                inference_session=inference_session,\n+                frame_idx=frame_idx,\n+                low_res_masks=tracker_low_res_masks_global,\n+                reconditioned_masks=reconditioned_masks,\n+            )\n+\n+        # Step 5: update the SAM2 metadata based on the update plan\n+        updated_obj_ids = tracker_metadata_new[\"obj_ids\"]\n+        if len(new_det_obj_ids) > 0:\n+            updated_obj_ids = updated_obj_ids + new_det_obj_ids\n+        if len(obj_ids_newly_removed) > 0:\n+            updated_obj_ids = [obj_id for obj_id in updated_obj_ids if obj_id not in obj_ids_newly_removed]\n+        tracker_metadata_new[\"obj_ids\"] = updated_obj_ids\n+\n+        # update object scores and the maximum object ID assigned so far\n+        if len(new_det_obj_ids) > 0:\n+            # Index tensor with list of indices and convert to list\n+            new_det_scores = det_scores[\n+                torch.tensor(new_det_out_inds, dtype=torch.long, device=det_scores.device)\n+            ].tolist()\n+            tracker_metadata_new[\"obj_id_to_score\"].update(zip(new_det_obj_ids, new_det_scores))\n+            # tracker scores are not available for new objects, use det score instead.\n+            tracker_metadata_new[\"obj_id_to_tracker_score_frame_wise\"][frame_idx].update(\n+                zip(new_det_obj_ids, new_det_scores)\n+            )\n+            tracker_metadata_new[\"max_obj_id\"] = max(\n+                tracker_metadata_new[\"max_obj_id\"],\n+                max(new_det_obj_ids),\n+            )\n+        # for removed objects, we set their scores to a very low value (-1e4) but still\n+        # keep them in \"obj_id_to_score\" (it's easier to handle outputs this way)\n+        for obj_id in obj_ids_newly_removed:\n+            tracker_metadata_new[\"obj_id_to_score\"][obj_id] = -1e4\n+            tracker_metadata_new[\"obj_id_to_tracker_score_frame_wise\"][frame_idx][obj_id] = -1e4\n+            tracker_metadata_new[\"obj_id_to_last_occluded\"].pop(obj_id, None)\n+\n+        return tracker_update_plan, tracker_metadata_new\n+\n+    def _tracker_add_new_objects(\n+        self,\n+        inference_session: Sam3VideoInferenceSession,\n+        frame_idx: int,\n+        new_obj_ids: list[int],\n+        new_obj_masks: Tensor,\n+        reverse: bool = False,\n+    ):\n+        \"\"\"Add a new object to SAM2 inference states.\"\"\"\n+        new_obj_masks = new_obj_masks >= 0.5\n+        for obj_id, mask in zip(new_obj_ids, new_obj_masks):\n+            obj_idx = inference_session.obj_id_to_idx(obj_id)\n+            inference_session.add_mask_inputs(obj_idx, frame_idx, mask.unsqueeze(0).unsqueeze(0))\n+\n+        inference_session.obj_with_new_inputs = list(new_obj_ids)\n+\n+        self.tracker_model(\n+            inference_session=inference_session,\n+            frame_idx=frame_idx,\n+            reverse=reverse,\n+            run_mem_encoder=True,\n+        )\n+\n+    def run_tracker_update_execution_phase(\n+        self,\n+        inference_session: Sam3VideoInferenceSession,\n+        frame_idx: int,\n+        det_out: dict[str, Tensor],\n+        tracker_update_plan: dict,\n+        reverse: bool = False,\n+    ):\n+        # initialize tracking scores with detection scores\n+        new_det_out_inds: list[int] = tracker_update_plan[\"new_det_out_inds\"]\n+        new_det_obj_ids: list[int] = tracker_update_plan[\"new_det_obj_ids\"]\n+        obj_ids_newly_removed: set[int] = tracker_update_plan[\"obj_ids_newly_removed\"]\n+\n+        # Step 1: add new objects from FA detection to SAM2 inference states\n+        if len(new_det_out_inds) > 0:\n+            new_det_out_inds_t = torch.tensor(new_det_out_inds, dtype=torch.long)\n+            new_det_masks: Tensor = det_out[\"mask\"][new_det_out_inds_t]\n+            # initialize SAM2 with new object masks\n+            self._tracker_add_new_objects(\n+                inference_session=inference_session,\n+                frame_idx=frame_idx,\n+                new_obj_ids=new_det_obj_ids,\n+                new_obj_masks=new_det_masks,\n+                reverse=reverse,\n+            )\n+\n+        # Step 2: remove from SAM2 inference states those objects removed by heuristics\n+        for obj_id in obj_ids_newly_removed:\n+            inference_session.remove_object(obj_id, strict=False)  # implement remove_object in inference_session?\n+\n+    def build_outputs(\n+        self,\n+        inference_session: Sam3VideoInferenceSession,\n+        det_out: dict[str, Tensor],\n+        tracker_low_res_masks_global: Tensor,\n+        tracker_update_plan: dict,\n+        reconditioned_obj_ids: Optional[set] = None,\n+    ):\n+        \"\"\"\n+        Build output dictionary with low-resolution masks.\n+        Interpolation to video resolution is handled by the processor.\n+\n+        Returns:\n+            obj_id_to_mask: dict mapping obj_id to low-res mask tensor (1, H_low, W_low)\n+        \"\"\"\n+        new_det_out_inds: list[int] = tracker_update_plan[\"new_det_out_inds\"]\n+        new_det_obj_ids: list[int] = tracker_update_plan[\"new_det_obj_ids\"]\n+        obj_id_to_mask = {}  # obj_id --> low-res mask tensor\n+\n+        # Part 1: masks from tracker propagation (existing objects)\n+        existing_masklet_obj_ids = inference_session.obj_ids\n+        for obj_id, mask in zip(existing_masklet_obj_ids, tracker_low_res_masks_global):\n+            obj_id_to_mask[int(obj_id)] = mask.unsqueeze(0)  # (1, H_low, W_low)\n+\n+        # Part 2: masks from new detections\n+        if len(new_det_out_inds) > 0:\n+            new_det_out_inds_t = torch.tensor(new_det_out_inds, dtype=torch.long, device=det_out[\"mask\"].device)\n+            new_det_low_res_masks = det_out[\"mask\"][new_det_out_inds_t]\n+            # Apply hole filling to new detection masks\n+            new_det_low_res_masks = fill_holes_in_mask_scores(\n+                new_det_low_res_masks.unsqueeze(1),\n+                max_area=self.fill_hole_area,\n+                fill_holes=True,\n+                remove_sprinkles=True,\n+            ).squeeze(1)\n+\n+            for obj_id, mask in zip(new_det_obj_ids, new_det_low_res_masks):\n+                obj_id_to_mask[int(obj_id)] = mask.unsqueeze(0)  # (1, H_low, W_low)\n+\n+        # Part 3: Override masks for reconditioned objects using detection masks\n+        if reconditioned_obj_ids is not None and len(reconditioned_obj_ids) > 0:\n+            trk_id_to_max_iou_high_conf_det = tracker_update_plan.get(\"trk_id_to_max_iou_high_conf_det\", {})\n+\n+            for obj_id in reconditioned_obj_ids:\n+                det_idx = trk_id_to_max_iou_high_conf_det.get(obj_id)\n+                if det_idx is not None:\n+                    det_mask = det_out[\"mask\"][det_idx].unsqueeze(0)  # (1, H_low, W_low)\n+                    obj_id_to_mask[int(obj_id)] = det_mask\n+\n+        return obj_id_to_mask\n+\n+    def _det_track_one_frame(\n+        self,\n+        inference_session: Sam3VideoInferenceSession,\n+        frame_idx: int,\n+        reverse: bool,\n+        streaming: bool = False,\n+    ):\n+        \"\"\"\n+        This function handles one-step inference for the DenseTracking model.\n+\n+        - `inference_session` contains all the information needed for inference, including the input video frames, text prompts, and any other relevant metadata\n+        - The function processes detection and tracking for a single frame\n+        - `streaming` indicates whether this is streaming inference mode (frames provided one at a time)\n+        \"\"\"\n+\n+        pixel_values = inference_session.get_frame(frame_idx).unsqueeze(0)\n+        vision_embeds = self.detector_model.get_vision_features(pixel_values=pixel_values)\n+\n+        # Step 1: run detection\n+        # It returns a \"det_out\" dict for `frame_idx`\n+        # into `feature_cache`.\n+        det_out = self.run_detection(\n+            inference_session=inference_session,\n+            vision_embeds=vision_embeds,\n+        )\n+\n+        # share the vision encoder outputs from the detector to the tracker\n+        vision_feats, vision_pos_embeds = self.get_vision_features_for_tracker(\n+            vision_embeds=vision_embeds,\n+        )\n+        inference_session.cache.cache_vision_features(\n+            frame_idx, {\"vision_feats\": vision_feats, \"vision_pos_embeds\": vision_pos_embeds}\n+        )\n+\n+        # Step 2: propagate SAM2 states to get the SAM2 prediction masks.\n+        # The returned `tracker_low_res_masks_global` contains the masklet predictions.\n+        # Note that this step only runs the SAM2 propagation step, but doesn't encode new memory for the predicted masks;\n+        # we defer memory encoding to `run_tracker_update_execution_phase` after resolving all heuristics.\n+        tracker_low_res_masks_global, tracker_obj_scores_global = self.run_tracker_propagation(\n+            inference_session=inference_session, frame_idx=frame_idx, reverse=reverse\n+        )\n+\n+        # Step 3: based on detection outputs and the propagated SAM2 prediction masks, we make plans\n+        # for SAM2 masklet updates (i.e. which objects to add and remove, etc).\n+        # We also run SAM2 memory encoder in this step to resolve non-overlapping constraints.\n+        # **This step should involve all the heuristics needed for any updates.**\n+        # This step also generates the new masklet metadata `tracker_metadata_new` (based on its previous version).\n+        tracker_update_plan, tracker_metadata_new = self.run_tracker_update_planning_phase(\n+            inference_session=inference_session,\n+            frame_idx=frame_idx,\n+            reverse=reverse,\n+            det_out=det_out,\n+            tracker_low_res_masks_global=tracker_low_res_masks_global,\n+            tracker_obj_scores_global=tracker_obj_scores_global,\n+            streaming=streaming,\n+        )\n+\n+        # Step 4: based on `tracker_update_plan`, execute the update w.r.t. the tracker states\n+        self.run_tracker_update_execution_phase(\n+            inference_session=inference_session,\n+            frame_idx=frame_idx,\n+            reverse=reverse,\n+            det_out=det_out,\n+            tracker_update_plan=tracker_update_plan,\n+        )\n+\n+        # Step 5: finally, build the outputs for this frame\n+        reconditioned_obj_ids = tracker_update_plan[\"reconditioned_obj_ids\"]\n+        obj_id_to_mask = self.build_outputs(\n+            inference_session=inference_session,\n+            det_out=det_out,\n+            tracker_low_res_masks_global=tracker_low_res_masks_global,\n+            tracker_update_plan=tracker_update_plan,\n+            reconditioned_obj_ids=reconditioned_obj_ids,\n+        )\n+        obj_id_to_score = tracker_metadata_new[\"obj_id_to_score\"]\n+        # add tracker scores to metadata, it should be fired for frames except the first frame\n+        if tracker_obj_scores_global.shape[0] > 0:\n+            # Convert tracker_obj_scores_global to sigmoid scores before updating\n+            tracker_obj_scores_global = tracker_obj_scores_global.sigmoid().tolist()\n+            tracker_obj_ids = inference_session.obj_ids\n+            tracker_metadata_new[\"obj_id_to_tracker_score_frame_wise\"][frame_idx].update(\n+                dict(zip(tracker_obj_ids, tracker_obj_scores_global))\n+            )\n+\n+        return (\n+            obj_id_to_mask,  # a dict: obj_id --> output mask\n+            obj_id_to_score,  # a dict: obj_id --> output score (prob)\n+            tracker_metadata_new,\n+            tracker_obj_scores_global,  # a dict: obj_id --> tracker frame-level scores\n+        )\n+\n+    @torch.inference_mode()\n+    @auto_docstring(custom_intro=\"Propagate the objects through a streamed video frame.\")\n+    def forward(\n+        self,\n+        inference_session: Sam3VideoInferenceSession,\n+        frame_idx: Optional[int] = None,\n+        frame: Optional[torch.Tensor] = None,\n+        reverse: bool = False,\n+    ):\n+        r\"\"\"\n+        inference_session (`Sam3VideoInferenceSession`):\n+            The video inference session object.\n+        frame_idx (`int`, *optional*):\n+            The index of the frame on which to run inference. No need to provide when inferring\n+            on a new streamed frame.\n+        frame (`torch.Tensor`, *optional*):\n+            The frame to process. Provide when streaming.\n+        reverse (`bool`, *optional*, defaults to `False`):\n+            Whether to propagate in reverse.\n+        \"\"\"\n+        if frame is not None:\n+            frame_idx = inference_session.add_new_frame(frame, frame_idx)\n+\n+        if frame_idx is None:\n+            raise ValueError(\"frame_idx must be provided when frame is not provided for streaming.\")\n+\n+        (\n+            obj_id_to_mask,\n+            obj_id_to_score,\n+            tracker_metadata_new,\n+            _,\n+        ) = self._det_track_one_frame(\n+            inference_session=inference_session,\n+            frame_idx=frame_idx,\n+            reverse=reverse,\n+            streaming=frame is not None,\n+        )\n+        # use a dummy string in \"previous_stages_out\" to indicate this frame has outputs\n+        # inference_session.previous_stages_out[frame_idx] = \"_THIS_FRAME_HAS_OUTPUTS_\"\n+\n+        extra_metadata = tracker_metadata_new[\"extra_metadata\"]\n+        removed_obj_ids = extra_metadata[\"removed_obj_ids\"]\n+\n+        # Update inference session state\n+        inference_session.obj_id_to_score = obj_id_to_score\n+        inference_session.obj_id_to_tracker_score_frame_wise = tracker_metadata_new[\n+            \"obj_id_to_tracker_score_frame_wise\"\n+        ]\n+        inference_session.obj_id_to_last_occluded = tracker_metadata_new[\"obj_id_to_last_occluded\"]\n+        inference_session.max_obj_id = tracker_metadata_new[\"max_obj_id\"]\n+        inference_session.obj_ids = list(tracker_metadata_new[\"obj_ids\"])\n+\n+        inference_session.obj_first_frame_idx = extra_metadata[\"obj_first_frame_idx\"]\n+        inference_session.unmatched_frame_inds = extra_metadata[\"unmatched_frame_inds\"]\n+        inference_session.trk_keep_alive = extra_metadata[\"trk_keep_alive\"]\n+        inference_session.overlap_pair_to_frame_inds = extra_metadata[\"overlap_pair_to_frame_inds\"]\n+        inference_session.removed_obj_ids = removed_obj_ids\n+        inference_session.suppressed_obj_ids[frame_idx] = extra_metadata[\"suppressed_obj_ids\"][frame_idx]\n+\n+        return Sam3VideoSegmentationOutput(\n+            object_ids=list(tracker_metadata_new[\"obj_ids\"]),\n+            obj_id_to_mask=obj_id_to_mask,\n+            obj_id_to_score=obj_id_to_score,\n+            obj_id_to_tracker_score=tracker_metadata_new[\"obj_id_to_tracker_score_frame_wise\"][frame_idx],\n+            removed_obj_ids=removed_obj_ids,\n+            suppressed_obj_ids=extra_metadata[\"suppressed_obj_ids\"][frame_idx],\n+            frame_idx=frame_idx,\n+        )\n+\n+    def _get_processing_order(\n+        self,\n+        inference_session: Sam3VideoInferenceSession,\n+        start_frame_idx: int,\n+        max_frame_num_to_track: Optional[int] = None,\n+        reverse: bool = False,\n+    ):\n+        num_frames = inference_session.num_frames\n+        if max_frame_num_to_track is None:\n+            # default: track all the frames in the video\n+            max_frame_num_to_track = num_frames\n+        if reverse:\n+            end_frame_idx = start_frame_idx - max_frame_num_to_track\n+            end_frame_idx = max(end_frame_idx, 0)\n+            processing_order = range(start_frame_idx - 1, end_frame_idx - 1, -1)\n+        else:\n+            end_frame_idx = start_frame_idx + max_frame_num_to_track\n+            end_frame_idx = min(end_frame_idx, num_frames - 1)\n+            processing_order = range(start_frame_idx, end_frame_idx + 1)\n+        return processing_order, end_frame_idx\n+\n+    @torch.inference_mode()\n+    def propagate_in_video_iterator(\n+        self,\n+        inference_session: Sam3VideoInferenceSession,\n+        start_frame_idx=0,\n+        max_frame_num_to_track=None,\n+        reverse=False,\n+    ):\n+        \"\"\"\n+        Propagate the prompts to get grounding results for the entire video. This method\n+        is a generator and yields inference outputs for all frames in the range specified\n+        by `start_frame_idx`, `max_frame_num_to_track`, and `reverse`.\n+\n+        Yields:\n+            `Sam3VideoSegmentationOutput`: The segmentation output for each frame.\n+        \"\"\"\n+        processing_order, end_frame_idx = self._get_processing_order(\n+            inference_session,\n+            start_frame_idx,\n+            max_frame_num_to_track,\n+            reverse=reverse,\n+        )\n+\n+        hotstart_buffer = []\n+        for frame_idx in tqdm(processing_order):\n+            out = self(inference_session=inference_session, frame_idx=frame_idx, reverse=reverse)\n+\n+            if self.hotstart_delay > 0:\n+                # accumulate the outputs for the first `hotstart_delay` frames\n+                hotstart_buffer.append(out)\n+                # update the object IDs removed by hotstart so that we don't output them\n+                inference_session.hotstart_removed_obj_ids.update(out.removed_obj_ids)\n+\n+                if frame_idx == end_frame_idx:\n+                    # we reached the end of propagation -- yield all frames in the buffer\n+                    yield_list = hotstart_buffer\n+                    hotstart_buffer = []\n+                elif len(hotstart_buffer) >= self.hotstart_delay:\n+                    # we have enough frames -- yield and remove the first (oldest) frame from the buffer\n+                    yield_list = hotstart_buffer[:1]\n+                    hotstart_buffer = hotstart_buffer[1:]\n+                else:\n+                    # not enough frames yet -- skip yielding\n+                    yield_list = []\n+            else:\n+                yield_list = [out]  # output the current frame\n+\n+            for yield_out in yield_list:\n+                yield yield_out\n+\n+\n+@torch.jit.script\n+def fast_diag_box_iou(boxes1, boxes2):\n+    box1_xy = boxes1[:, 2:]\n+    box1_XY = boxes1[:, :2]\n+    box2_xy = boxes2[:, 2:]\n+    box2_XY = boxes2[:, :2]\n+    area1 = (box1_xy - box1_XY).prod(-1)\n+    area2 = (box2_xy - box2_XY).prod(-1)\n+\n+    lt = torch.max(box1_XY, box2_XY)\n+    rb = torch.min(box1_xy, box2_xy)\n+\n+    inter = (rb - lt).clamp(min=0).prod(-1)\n+    union = area1 + area2 - inter\n+    iou = inter / union\n+    return iou\n+\n+\n+def mask_iou(pred_masks: torch.Tensor, gt_masks: torch.Tensor) -> torch.Tensor:\n+    \"\"\"\n+    Compute the IoU (Intersection over Union) between predicted masks and ground truth masks.\n+    Args:\n+      - pred_masks: (N, H, W) bool Tensor, containing binary predicted segmentation masks\n+      - gt_masks: (M, H, W) bool Tensor, containing binary ground truth segmentation masks\n+    Returns:\n+      - ious: (N, M) float Tensor, containing IoUs for each pair of predicted and ground truth masks\n+    \"\"\"\n+    N, H, W = pred_masks.shape\n+    M, _, _ = gt_masks.shape\n+\n+    # Flatten masks: (N, 1, H*W) and (1, M, H*W)\n+    pred_flat = pred_masks.view(N, 1, H * W)\n+    gt_flat = gt_masks.view(1, M, H * W)\n+\n+    # Compute intersection and union: (N, M)\n+    intersection = (pred_flat & gt_flat).sum(dim=2).float()\n+    union = (pred_flat | gt_flat).sum(dim=2).float()\n+    ious = intersection / union.clamp(min=1)\n+    return ious  # shape: (N, M)\n+\n+\n+def nms_masks(\n+    pred_probs: torch.Tensor,\n+    pred_masks: torch.Tensor,\n+    prob_threshold: float,\n+    iou_threshold: float,\n+) -> torch.Tensor:\n+    \"\"\"\n+    Args:\n+      - pred_probs: (num_det,) float Tensor, containing the score (probability) of each detection\n+      - pred_masks: (num_det, H_mask, W_mask) float Tensor, containing the binary segmentation mask of each detection\n+      - prob_threshold: float, score threshold to prefilter detections (NMS is performed on detections above threshold)\n+      - iou_threshold: float, mask IoU threshold for NMS\n+\n+    Returns:\n+     - keep: (num_det,) bool Tensor, indicating whether each detection is kept after score thresholding + NMS\n+    \"\"\"\n+    # prefilter the detections with prob_threshold (\"valid\" are those above prob_threshold)\n+    is_valid = pred_probs > prob_threshold  # (num_det,)\n+    probs = pred_probs[is_valid]  # (num_valid,)\n+    masks_binary = pred_masks[is_valid] > 0  # (num_valid, H_mask, W_mask)\n+    if probs.numel() == 0:\n+        return is_valid  # no valid detection, return empty keep mask\n+\n+    ious = mask_iou(masks_binary, masks_binary)  # (num_valid, num_valid)\n+\n+    # Try to use kernels for NMS, fallback to keeping all valid detections if unavailable\n+    _load_cv_utils_kernel_once()\n+    if not cv_utils_kernel:\n+        return is_valid  # Fallback: keep all valid detections without NMS\n+\n+    try:\n+        kept_inds = cv_utils_kernel.generic_nms(ious, probs, iou_threshold, use_iou_matrix=True)\n+    except Exception as e:\n+        logger.warning_once(f\"Failed to run NMS using kernels library: {e}. NMS post-processing will be skipped.\")\n+        return is_valid  # Fallback: keep all valid detections without NMS\n+\n+    # valid_inds are the indices among `probs` of valid detections before NMS (or -1 for invalid)\n+    valid_inds = torch.where(is_valid, is_valid.cumsum(dim=0) - 1, -1)  # (num_det,)\n+    keep = torch.isin(valid_inds, kept_inds)  # (num_det,)\n+    return keep\n+\n+\n+def fill_holes_in_mask_scores(mask, max_area, fill_holes=True, remove_sprinkles=True):\n+    \"\"\"\n+    A post processor to fill small holes in mask scores with area under `max_area`.\n+    Holes are those small connected components in either background or foreground.\n+\n+    Note that it relies on the \"cc_torch\" package to find connected components fast. You can\n+    install it via the following command (`TORCH_CUDA_ARCH_LIST=8.0` is for A100 GPUs):\n+    ```\n+    pip uninstall -y cc_torch; TORCH_CUDA_ARCH_LIST=8.0 9.0 pip install git+https://github.com/ronghanghu/cc_torch\n+    ```\n+    Otherwise, it will fallback to a slightly slower triton implementation, or skimage if the tensor is on cpu\n+    \"\"\"\n+\n+    if max_area <= 0:\n+        return mask  # nothing to fill in this case\n+\n+    if fill_holes:\n+        # We remove small connected components in background by changing them to foreground\n+        # with a small positive mask score (0.1).\n+        mask_bg = mask <= 0\n+        bg_area_thresh = max_area\n+        _, areas_bg = _get_connected_components_with_padding(mask_bg)\n+        small_components_bg = mask_bg & (areas_bg <= bg_area_thresh)\n+        mask = torch.where(small_components_bg, 0.1, mask)\n+\n+    if remove_sprinkles:\n+        # We remove small connected components in foreground by changing them to background\n+        # with a small negative mask score (-0.1). Here we only remove connected components\n+        # whose areas are under both `max_area` and half of the entire mask's area. This\n+        # removes sprinkles while avoids filtering out tiny objects that we want to track.\n+        mask_fg = mask > 0\n+        fg_area_thresh = torch.sum(mask_fg, dim=(2, 3), keepdim=True, dtype=torch.int32)\n+        fg_area_thresh.floor_divide_(2).clamp_(max=max_area)\n+        _, areas_fg = _get_connected_components_with_padding(mask_fg)\n+        small_components_fg = mask_fg & (areas_fg <= fg_area_thresh)\n+        mask = torch.where(small_components_fg, -0.1, mask)\n+    return mask\n+\n+\n+def _get_connected_components_with_padding(mask):\n+    \"\"\"Get connected components from masks (possibly padding them to an even size).\"\"\"\n+    mask = mask.to(torch.uint8)\n+    _, _, H, W = mask.shape\n+\n+    # Try to use kernels for connected components, fallback if unavailable\n+    _load_cv_utils_kernel_once()\n+    if not cv_utils_kernel:\n+        # Fallback: return dummy labels and counts that won't trigger filtering\n+        labels = torch.zeros_like(mask, dtype=torch.int32)\n+        counts = torch.full_like(mask, fill_value=mask.shape[2] * mask.shape[3] + 1, dtype=torch.int32)\n+        return labels, counts\n+\n+    # make sure both height and width are even (to be compatible with cc_torch)\n+    pad_h = H % 2\n+    pad_w = W % 2\n+\n+    try:\n+        if pad_h == 0 and pad_w == 0:\n+            labels, counts = cv_utils_kernel.cc_2d(mask.contiguous(), get_counts=True)\n+        else:\n+            # pad the mask to make its height and width even\n+            # padding format is (padding_left,padding_right,padding_top,padding_bottom)\n+            mask_pad = F.pad(mask, (0, pad_w, 0, pad_h), mode=\"constant\", value=0)\n+            labels, counts = cv_utils_kernel.cc_2d(mask_pad.contiguous(), get_counts=True)\n+            labels = labels[:, :, :H, :W]\n+            counts = counts[:, :, :H, :W]\n+    except Exception as e:\n+        logger.warning_once(\n+            f\"Failed to compute connected components using kernels library: {e}. \"\n+            \"Hole filling and sprinkle removal will be skipped.\"\n+        )\n+        # Fallback: return dummy labels and counts that won't trigger filtering\n+        labels = torch.zeros_like(mask, dtype=torch.int32)\n+        counts = torch.full_like(mask, fill_value=H * W + 1, dtype=torch.int32)\n+        return labels, counts\n+\n+    return labels, counts\n+\n+\n+__all__ = [\n+    \"Sam3VideoModel\",\n+    \"Sam3VideoPreTrainedModel\",\n+    \"Sam3VideoInferenceSession\",\n+    \"Sam3VideoSegmentationOutput\",\n+]"
        },
        {
            "sha": "ed013273d1b8da4cf7a0dacede31ccf40b2e9c39",
            "filename": "src/transformers/models/sam3_video/processing_sam3_video.py",
            "status": "added",
            "additions": 324,
            "deletions": 0,
            "changes": 324,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fprocessing_sam3_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fprocessing_sam3_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fprocessing_sam3_video.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,324 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Optional, Union\n+\n+import torch\n+from torchvision.ops import masks_to_boxes\n+\n+from ...image_utils import ImageInput\n+from ...processing_utils import ProcessorMixin\n+from ...tokenization_utils_base import BatchEncoding\n+from ...utils import TensorType\n+from ...utils.import_utils import requires\n+from ...video_utils import VideoInput\n+from .modeling_sam3_video import Sam3VideoInferenceSession\n+\n+\n+@requires(backends=(\"torch\",))\n+class Sam3VideoProcessor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a SAM3 processor which wraps a SAM3 image processor and an 2D points & Bounding boxes processor into a\n+    single processor.\n+\n+    [`Sam3Processor`] offers all the functionalities of [`Sam3ImageProcessor`] and [`Sam3VideoProcessor`]. See the docstring of\n+    [`~Sam3ImageProcessor.__call__`] and [`~Sam3VideoProcessor.__call__`] for more information.\n+\n+    Args:\n+        image_processor (`Sam2ImageProcessorFast`):\n+            An instance of [`Sam2ImageProcessorFast`].\n+        video_processor (`Sam2VideoVideoProcessor`):\n+            An instance of [`Sam2VideoVideoProcessor`].\n+        tokenizer ([`PreTrainedTokenizer`, `PreTrainedTokenizerFast`]):\n+            An instance of [`PreTrainedTokenizer`, `PreTrainedTokenizerFast`]. The tokenizer is a required input.\n+        target_size (`int`, *optional*):\n+            The target size (target_size, target_size) to which the image will be resized.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        image_processor,\n+        video_processor,\n+        tokenizer,\n+        target_size: Optional[int] = None,\n+        **kwargs,\n+    ):\n+        super().__init__(image_processor, video_processor, tokenizer, **kwargs)\n+        self.target_size = target_size if target_size is not None else self.image_processor.size[\"height\"]\n+\n+    def __call__(\n+        self,\n+        images: Optional[ImageInput] = None,\n+        segmentation_maps: Optional[ImageInput] = None,\n+        original_sizes: Optional[Union[list[list[float]], torch.Tensor]] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        **kwargs,\n+    ) -> BatchEncoding:\n+        r\"\"\"\n+        This method uses [`Sam3VideoImageProcessorFast.__call__`] method to prepare image(s) for the model.\n+\n+        Args:\n+            images (`ImageInput`, *optional*):\n+                The image(s) to process.\n+            segmentation_maps (`ImageInput`, *optional*):\n+                The segmentation maps to process (optional, for image processor).\n+            original_sizes (`list[list[float]]`, `torch.Tensor`, *optional*):\n+                The original sizes of the images. Only used when images is not provided.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return.\n+            **kwargs:\n+                Additional keyword arguments to pass to the image processor.\n+\n+        Returns:\n+            A [`BatchEncoding`] with the following fields:\n+            - `pixel_values` (`torch.Tensor`): The processed image(s).\n+            - `original_sizes` (`list[list[float]]`): The original sizes of the images.\n+            - `labels` (`torch.Tensor`, *optional*): The processed segmentation maps (if provided).\n+        \"\"\"\n+        if images is not None:\n+            encoding_image_processor = self.image_processor(\n+                images,\n+                segmentation_maps=segmentation_maps,\n+                return_tensors=return_tensors,\n+                **kwargs,\n+            )\n+        elif original_sizes is not None:\n+            if isinstance(original_sizes, torch.Tensor):\n+                original_sizes = original_sizes.cpu().tolist()\n+            encoding_image_processor = BatchEncoding({\"original_sizes\": original_sizes}, tensor_type=return_tensors)\n+        else:\n+            raise ValueError(\"Either images or original_sizes must be provided\")\n+\n+        original_sizes = encoding_image_processor[\"original_sizes\"]\n+        # Check original_sizes is of length 1 or len(images)\n+        if images is not None and len(original_sizes) != 1 and len(original_sizes) != len(images):\n+            raise ValueError(\n+                \"original_sizes must be of length 1 or len(images). If you are passing a single image, you must pass a single original_size.\"\n+            )\n+\n+        return encoding_image_processor\n+\n+    def add_text_prompt(self, inference_session, text):\n+        \"\"\"\n+        Add text prompt to the inference session.\n+        \"\"\"\n+        encoded_text = self.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", max_length=32).to(\n+            inference_session.inference_device\n+        )\n+        inference_session.text_attention_mask = encoded_text.attention_mask\n+        inference_session.text_input_ids = encoded_text.input_ids\n+        inference_session.has_new_text_input = True\n+        return inference_session\n+\n+    def init_video_session(\n+        self,\n+        video: Optional[VideoInput] = None,\n+        inference_device: Union[str, \"torch.device\"] = \"cpu\",\n+        inference_state_device: Optional[Union[str, \"torch.device\"]] = None,\n+        processing_device: Optional[Union[str, \"torch.device\"]] = None,\n+        video_storage_device: Optional[Union[str, \"torch.device\"]] = None,\n+        max_vision_features_cache_size: int = 1,\n+        dtype: torch.dtype = torch.float32,\n+    ):\n+        \"\"\"\n+        Initializes a video session for inference.\n+        If a video is provided (async inference), the video will be processed and stored on the `video_storage_device`.\n+\n+        Args:\n+            video (`VideoInput`, *optional*):\n+                The video to process. No need to provide when streaming.\n+            inference_device (`str` or `torch.device`, *optional*, defaults to \"cpu\"):\n+                The device to use for inference.\n+            inference_state_device (`str` or `torch.device`, *optional*):\n+                The device to store the inference state on.\n+            processing_device (`str` or `torch.device`, *optional*):\n+                The device to use for video processing.\n+            video_storage_device (`str` or `torch.device`, *optional*):\n+                The device to store the processed video frames on.\n+            max_vision_features_cache_size (`int`, *optional*, defaults to 1):\n+                The maximum number of vision features to cache.\n+            dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):\n+                The torch dtype to use for the whole session.\n+        \"\"\"\n+        video_storage_device = video_storage_device if video_storage_device is not None else inference_device\n+        inference_state_device = inference_state_device if inference_state_device is not None else inference_device\n+        processing_device = processing_device if processing_device is not None else inference_device\n+        pixel_values_video = None\n+        video_height = None\n+        video_width = None\n+        if video is not None:\n+            processed_video = self.video_processor(videos=video, device=processing_device, return_tensors=\"pt\")\n+            pixel_values_video = processed_video.pixel_values_videos[0]\n+            video_height = processed_video.original_sizes[0][0]\n+            video_width = processed_video.original_sizes[0][1]\n+        inference_session = Sam3VideoInferenceSession(\n+            video=pixel_values_video,\n+            video_height=video_height,\n+            video_width=video_width,\n+            inference_device=inference_device,\n+            video_storage_device=video_storage_device,\n+            inference_state_device=inference_state_device,\n+            dtype=dtype,\n+            max_vision_features_cache_size=max_vision_features_cache_size,\n+        )\n+        return inference_session\n+\n+    def _apply_non_overlapping_constraints(self, pred_masks):\n+        \"\"\"\n+        Apply non-overlapping constraints to the object scores in pred_masks. Here we\n+        keep only the highest scoring object at each spatial location in pred_masks.\n+        \"\"\"\n+        batch_size = pred_masks.size(0)\n+        if batch_size == 1:\n+            return pred_masks\n+\n+        device = pred_masks.device\n+        # \"max_obj_inds\": object index of the object with the highest score at each location\n+        max_obj_inds = torch.argmax(pred_masks, dim=0, keepdim=True)\n+        # \"batch_obj_inds\": object index of each object slice (along dim 0) in `pred_masks`\n+        batch_obj_inds = torch.arange(batch_size, device=device)[:, None, None, None]\n+        keep = max_obj_inds == batch_obj_inds\n+        # suppress overlapping regions' scores below -10.0 so that the foreground regions\n+        # don't overlap (here sigmoid(-10.0)=4.5398e-05)\n+        pred_masks = torch.where(keep, pred_masks, torch.clamp(pred_masks, max=-10.0))\n+        return pred_masks\n+\n+    def _apply_object_wise_non_overlapping_constraints(self, pred_masks, obj_scores, background_value=-10.0):\n+        \"\"\"\n+        Applies non-overlapping constraints object wise (i.e. only one object can claim the overlapping region)\n+        \"\"\"\n+        pred_masks_single_score = torch.where(pred_masks > 0, obj_scores[..., None, None], background_value)\n+        # Apply pixel-wise non-overlapping constraint based on mask scores\n+        pixel_level_non_overlapping_masks = self._apply_non_overlapping_constraints(pred_masks_single_score)\n+        # Replace object scores with pixel scores. Note, that now only one object can claim the overlapping region\n+        pred_masks = torch.where(\n+            pixel_level_non_overlapping_masks > 0,\n+            pred_masks,\n+            torch.clamp(pred_masks, max=background_value),\n+        )\n+        return pred_masks\n+\n+    def postprocess_outputs(\n+        self,\n+        inference_session,\n+        model_outputs,\n+        original_sizes: Optional[Union[list[list[float]], torch.Tensor]] = None,\n+    ):\n+        \"\"\"\n+        Post-process model outputs to get final masks, boxes, and scores.\n+\n+        Args:\n+            inference_session (`Sam3VideoInferenceSession`):\n+                The inference session object.\n+            model_outputs (`Sam3VideoSegmentationOutput`):\n+                The raw model output from `Sam3VideoModel.forward()`.\n+            original_sizes (`list[list[float]]` or `torch.Tensor`, *optional*):\n+                Optional original frame sizes [height, width]. Required for streaming inference\n+                when video_height/video_width are not set in the session.\n+\n+        Returns:\n+            `dict`: A dictionary containing the following keys:\n+                - **object_ids** (`torch.Tensor` of shape `(num_objects,)`): Object IDs for each detected object.\n+                - **scores** (`torch.Tensor` of shape `(num_objects,)`): Detection scores for each object.\n+                - **boxes** (`torch.Tensor` of shape `(num_objects, 4)`): Bounding boxes in XYXY format\n+                  (top_left_x, top_left_y, bottom_right_x, bottom_right_y).\n+                - **masks** (`torch.Tensor` of shape `(num_objects, height, width)`): Binary segmentation masks\n+                  for each object at the original video resolution.\n+        \"\"\"\n+        obj_id_to_mask = model_outputs[\"obj_id_to_mask\"]  # low res masks (1, H_low, W_low)\n+        curr_obj_ids = sorted(obj_id_to_mask.keys())\n+\n+        # Get video dimensions - use original_sizes for streaming inference if session doesn't have them\n+        if inference_session.video_height is not None and inference_session.video_width is not None:\n+            H_video, W_video = inference_session.video_height, inference_session.video_width\n+        elif original_sizes is not None:\n+            if isinstance(original_sizes, torch.Tensor):\n+                original_sizes = original_sizes.cpu().tolist()\n+            # original_sizes is a list of [height, width] pairs, take the first one\n+            if isinstance(original_sizes[0], list):\n+                H_video, W_video = int(original_sizes[0][0]), int(original_sizes[0][1])\n+            else:\n+                H_video, W_video = int(original_sizes[0]), int(original_sizes[1])\n+        else:\n+            raise ValueError(\n+                \"Either inference_session.video_height/video_width must be set, \"\n+                \"or original_sizes must be provided for streaming inference.\"\n+            )\n+        if len(curr_obj_ids) == 0:\n+            out_obj_ids = torch.zeros(0, dtype=torch.int64)\n+            out_probs = torch.zeros(0, dtype=torch.float32)\n+            out_binary_masks = torch.zeros(0, H_video, W_video, dtype=torch.bool)\n+            out_boxes_xyxy = torch.zeros(0, 4, dtype=torch.float32)\n+        else:\n+            out_obj_ids = torch.tensor(curr_obj_ids, dtype=torch.int64)\n+            out_probs = torch.tensor([model_outputs[\"obj_id_to_score\"][obj_id] for obj_id in curr_obj_ids])\n+            out_tracker_probs = torch.tensor(\n+                [model_outputs[\"obj_id_to_tracker_score\"].get(obj_id, 0.0) for obj_id in curr_obj_ids]\n+            )\n+\n+            # Interpolate low-res masks to video resolution\n+            low_res_masks = torch.cat([obj_id_to_mask[obj_id] for obj_id in curr_obj_ids], dim=0)  # (N, H_low, W_low)\n+            # Add channel dimension for interpolation: (N, H, W) -> (N, 1, H, W)\n+            out_binary_masks = torch.nn.functional.interpolate(\n+                low_res_masks.unsqueeze(1),\n+                size=(H_video, W_video),\n+                mode=\"bilinear\",\n+                align_corners=False,\n+            ).squeeze(1)  # (N, H_video, W_video)\n+            out_binary_masks = out_binary_masks > 0\n+\n+            assert out_binary_masks.dtype == torch.bool\n+            keep = out_binary_masks.any(dim=(1, 2)).cpu()  # remove masks with 0 areas\n+            # hide outputs for those object IDs in `obj_ids_to_hide`\n+            obj_ids_to_hide = []\n+            if model_outputs[\"suppressed_obj_ids\"] is not None:\n+                obj_ids_to_hide.extend(list(model_outputs[\"suppressed_obj_ids\"]))\n+            if len(inference_session.hotstart_removed_obj_ids) > 0:\n+                obj_ids_to_hide.extend(list(inference_session.hotstart_removed_obj_ids))\n+            if len(obj_ids_to_hide) > 0:\n+                obj_ids_to_hide_t = torch.tensor(obj_ids_to_hide, dtype=torch.int64)\n+                keep &= ~torch.isin(out_obj_ids, obj_ids_to_hide_t)\n+\n+            # slice those valid entries from the original outputs\n+            keep_idx = torch.nonzero(keep, as_tuple=True)[0]\n+            keep_idx_gpu = keep_idx.pin_memory().to(device=out_binary_masks.device, non_blocking=True)\n+\n+            out_obj_ids = torch.index_select(out_obj_ids, 0, keep_idx)\n+            out_probs = torch.index_select(out_probs, 0, keep_idx)\n+            out_tracker_probs = torch.index_select(out_tracker_probs, 0, keep_idx)\n+            out_binary_masks = torch.index_select(out_binary_masks, 0, keep_idx_gpu)\n+\n+            out_boxes_xyxy = masks_to_boxes(out_binary_masks)\n+\n+        # apply non-overlapping constraints on the existing masklets\n+        if out_binary_masks.shape[0] > 1:\n+            assert len(out_binary_masks) == len(out_tracker_probs)\n+            out_binary_masks = (\n+                self._apply_object_wise_non_overlapping_constraints(\n+                    out_binary_masks.unsqueeze(1),\n+                    out_tracker_probs.unsqueeze(1).to(out_binary_masks.device),\n+                    background_value=0,\n+                ).squeeze(1)\n+            ) > 0\n+\n+        outputs = {\n+            \"object_ids\": out_obj_ids,\n+            \"scores\": out_probs,\n+            \"boxes\": out_boxes_xyxy,\n+            \"masks\": out_binary_masks,\n+        }\n+        return outputs\n+\n+\n+__all__ = [\"Sam3VideoProcessor\"]"
        },
        {
            "sha": "920ce9d0a39cecc451b329c5a241cccaa4f5b5e7",
            "filename": "src/transformers/pipelines/mask_generation.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fpipelines%2Fmask_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Fpipelines%2Fmask_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fmask_generation.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -253,7 +253,8 @@ def _forward(\n         input_boxes = model_inputs.pop(\"input_boxes\")\n         is_last = model_inputs.pop(\"is_last\")\n         original_sizes = model_inputs.pop(\"original_sizes\").tolist()\n-        reshaped_input_sizes = model_inputs.pop(\"reshaped_input_sizes\").tolist()\n+        reshaped_input_sizes = model_inputs.pop(\"reshaped_input_sizes\", None)\n+        reshaped_input_sizes = reshaped_input_sizes.tolist() if reshaped_input_sizes is not None else None\n \n         model_outputs = self.model(**model_inputs)\n "
        },
        {
            "sha": "8775150ece2244c7920ede0ba05af86938fcda4d",
            "filename": "src/transformers/utils/type_validators.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Futils%2Ftype_validators.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/src%2Ftransformers%2Futils%2Ftype_validators.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Ftype_validators.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -57,6 +57,7 @@ def device_validator(value: str | int | None = None):\n     if value is None:\n         pass\n     elif is_torch_available() and isinstance(value, torch.device):\n+        # Convert torch.device to string for validation\n         device_str = str(value)\n         if device_str.split(\":\")[0] not in possible_names:\n             raise ValueError("
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/sam3/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/tests%2Fmodels%2Fsam3%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/tests%2Fmodels%2Fsam3%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam3%2F__init__.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a"
        },
        {
            "sha": "0da9ca1b67972073e28c55fce933912df9ba9ecf",
            "filename": "tests/models/sam3/test_modeling_sam3.py",
            "status": "added",
            "additions": 1449,
            "deletions": 0,
            "changes": 1449,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/tests%2Fmodels%2Fsam3%2Ftest_modeling_sam3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/tests%2Fmodels%2Fsam3%2Ftest_modeling_sam3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam3%2Ftest_modeling_sam3.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,1449 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch SAM3 model.\"\"\"\n+\n+import gc\n+import tempfile\n+import unittest\n+\n+import requests\n+\n+from transformers.testing_utils import (\n+    backend_empty_cache,\n+    require_torch,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+    from transformers.models.sam3.configuration_sam3 import (\n+        Sam3Config,\n+        Sam3DETRDecoderConfig,\n+        Sam3DETREncoderConfig,\n+        Sam3GeometryEncoderConfig,\n+        Sam3MaskDecoderConfig,\n+        Sam3VisionConfig,\n+        Sam3ViTConfig,\n+    )\n+    from transformers.models.sam3.modeling_sam3 import Sam3Model, Sam3VisionModel\n+    from transformers.models.sam3.processing_sam3 import Sam3Processor\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+\n+class Sam3VisionModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        hidden_size=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        intermediate_size=64,\n+        num_channels=3,\n+        image_size=224,\n+        patch_size=14,\n+        window_size=8,\n+        global_attn_indexes=None,\n+        fpn_hidden_size=32,\n+        scale_factors=None,\n+        batch_size=2,\n+        is_training=False,\n+    ):\n+        if global_attn_indexes is None:\n+            global_attn_indexes = [0, 1]\n+        if scale_factors is None:\n+            scale_factors = [4.0, 2.0, 1.0, 0.5]\n+\n+        self.parent = parent\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.window_size = window_size\n+        self.global_attn_indexes = global_attn_indexes\n+        self.fpn_hidden_size = fpn_hidden_size\n+        self.scale_factors = scale_factors\n+        self.batch_size = batch_size\n+        self.is_training = is_training\n+\n+    def get_config(self):\n+        backbone_config = Sam3ViTConfig(\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            num_channels=self.num_channels,\n+            image_size=self.image_size,\n+            patch_size=self.patch_size,\n+            window_size=self.window_size,\n+            global_attn_indexes=self.global_attn_indexes,\n+        )\n+        return Sam3VisionConfig(\n+            backbone_config=backbone_config,\n+            fpn_hidden_size=self.fpn_hidden_size,\n+            scale_factors=self.scale_factors,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+        config = self.get_config()\n+\n+        return config, pixel_values\n+\n+    def create_and_check_model(self, config, pixel_values):\n+        model = Sam3VisionModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(pixel_values)\n+\n+        # Check FPN outputs\n+        self.parent.assertEqual(len(result.fpn_hidden_states), len(self.scale_factors))\n+        self.parent.assertEqual(len(result.fpn_position_encoding), len(self.scale_factors))\n+\n+        # Check last hidden state shape\n+        expected_seq_len = (self.image_size // self.patch_size) ** 2\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, expected_seq_len, self.hidden_size))\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class Sam3VisionModelTest(ModelTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Tests for SAM3 Vision Model (ViT backbone + FPN neck).\n+    \"\"\"\n+\n+    all_model_classes = (Sam3VisionModel,) if is_torch_available() else ()\n+\n+    test_resize_embeddings = False\n+    test_torch_exportable = False\n+\n+    def setUp(self):\n+        self.model_tester = Sam3VisionModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=Sam3VisionConfig, has_text_modality=False)\n+\n+    def test_config(self):\n+        self.config_tester.create_and_test_config_to_json_string()\n+        self.config_tester.create_and_test_config_to_json_file()\n+        self.config_tester.create_and_test_config_from_and_save_pretrained()\n+        self.config_tester.create_and_test_config_with_num_labels()\n+        self.config_tester.check_config_can_be_init_without_params()\n+        self.config_tester.check_config_arguments_init()\n+\n+    @unittest.skip(reason=\"SAM3's vision encoder does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    def test_model_get_set_embeddings(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            self.assertIsInstance(model.get_input_embeddings(), (nn.Module))\n+            x = model.get_output_embeddings()\n+            self.assertTrue(x is None or isinstance(x, nn.Linear))\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_attention_outputs(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+        # Force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n+\n+            # Check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+            config.backbone_config.output_attentions = True\n+\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n+\n+            # For windowed attention, check the attention shape\n+            # Attention shape: (batch_size, num_heads, seq_len, seq_len) for global attention\n+            # or windowed shape for local attention\n+            self.assertIsNotNone(attentions[0])\n+\n+    def test_hidden_states_output(self):\n+        def check_hidden_states_output(inputs_dict, config, model_class):\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            # SAM3VisionModel doesn't return hidden_states in the same way as SAM2\n+            # It returns last_hidden_state, fpn_hidden_states, and fpn_position_encoding\n+            self.assertIsNotNone(outputs.last_hidden_state)\n+            self.assertIsNotNone(outputs.fpn_hidden_states)\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+    def test_batching_equivalence(self, atol=5e-4, rtol=5e-4):\n+        super().test_batching_equivalence(atol=atol, rtol=rtol)\n+\n+    @unittest.skip(reason=\"SAM3 model can't be compiled dynamic yet\")\n+    def test_sdpa_can_compile_dynamic(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SAM3VisionModel has FPN channel mismatch with flex attention\")\n+    def test_flex_attention_with_grads(self):\n+        pass\n+\n+\n+class Sam3ModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        num_channels=3,\n+        image_size=224,  # Keep reasonable size: 224 = 16 * 14\n+        hidden_size=32,\n+        patch_size=14,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        intermediate_size=64,\n+        window_size=8,  # 224/14 = 16 patches, 16/2 = 8 per window\n+        global_attn_indexes=None,\n+        fpn_hidden_size=32,\n+        scale_factors=None,\n+        geometry_encoder_hidden_size=32,\n+        geometry_encoder_num_layers=1,  # Reduced from 2 to 1\n+        detr_encoder_hidden_size=32,\n+        detr_encoder_num_layers=1,  # Reduced from 2 to 1\n+        detr_decoder_hidden_size=32,\n+        detr_decoder_num_layers=1,  # Reduced from 2 to 1\n+        detr_decoder_num_queries=5,  # Reduced from 10 to 5\n+        mask_decoder_hidden_size=32,\n+        batch_size=2,\n+        is_training=False,\n+    ):\n+        if global_attn_indexes is None:\n+            global_attn_indexes = [0, 1]\n+        if scale_factors is None:\n+            scale_factors = [2.0, 1.0]  # Just 2 scales to reduce params\n+\n+        self.parent = parent\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.hidden_size = hidden_size\n+        self.patch_size = patch_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.window_size = window_size\n+        self.global_attn_indexes = global_attn_indexes\n+        self.fpn_hidden_size = fpn_hidden_size\n+        self.scale_factors = scale_factors\n+        self.batch_size = batch_size\n+        self.is_training = is_training\n+\n+        # Geometry encoder\n+        self.geometry_encoder_hidden_size = geometry_encoder_hidden_size\n+        self.geometry_encoder_num_layers = geometry_encoder_num_layers\n+\n+        # DETR encoder/decoder\n+        self.detr_encoder_hidden_size = detr_encoder_hidden_size\n+        self.detr_encoder_num_layers = detr_encoder_num_layers\n+        self.detr_decoder_hidden_size = detr_decoder_hidden_size\n+        self.detr_decoder_num_layers = detr_decoder_num_layers\n+        self.detr_decoder_num_queries = detr_decoder_num_queries\n+\n+        # Mask decoder\n+        self.mask_decoder_hidden_size = mask_decoder_hidden_size\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+        # Simple text input (will be processed by text encoder)\n+        input_ids = torch.randint(0, 1000, (self.batch_size, 16), device=torch_device)\n+        attention_mask = torch.ones_like(input_ids)\n+\n+        config = self.get_config()\n+\n+        return config, pixel_values, input_ids, attention_mask\n+\n+    def get_config(self):\n+        backbone_config = Sam3ViTConfig(\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            num_channels=self.num_channels,\n+            image_size=self.image_size,\n+            patch_size=self.patch_size,\n+            window_size=self.window_size,\n+            global_attn_indexes=self.global_attn_indexes,\n+        )\n+\n+        vision_config = Sam3VisionConfig(\n+            backbone_config=backbone_config,\n+            fpn_hidden_size=self.fpn_hidden_size,\n+            scale_factors=self.scale_factors,\n+        )\n+\n+        # Small text config for testing (instead of default full CLIP model)\n+        text_config = {\n+            \"vocab_size\": 1000,  # Keep at 1000 for stability\n+            \"hidden_size\": 32,\n+            \"intermediate_size\": 64,\n+            \"projection_dim\": 32,\n+            \"num_hidden_layers\": 1,\n+            \"num_attention_heads\": 4,\n+            \"max_position_embeddings\": 32,  # Keep at 32 for stability\n+            \"hidden_act\": \"gelu\",\n+        }\n+\n+        geometry_encoder_config = Sam3GeometryEncoderConfig(\n+            hidden_size=self.geometry_encoder_hidden_size,\n+            num_layers=self.geometry_encoder_num_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            mask_fuser_hidden_size=self.geometry_encoder_hidden_size,  # Match hidden_size to reduce params\n+            mask_fuser_num_layers=1,  # Reduce from default 2 to 1\n+        )\n+\n+        detr_encoder_config = Sam3DETREncoderConfig(\n+            hidden_size=self.detr_encoder_hidden_size,\n+            num_layers=self.detr_encoder_num_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+        )\n+\n+        detr_decoder_config = Sam3DETRDecoderConfig(\n+            hidden_size=self.detr_decoder_hidden_size,\n+            num_layers=self.detr_decoder_num_layers,\n+            num_queries=self.detr_decoder_num_queries,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+        )\n+\n+        mask_decoder_config = Sam3MaskDecoderConfig(\n+            hidden_size=self.mask_decoder_hidden_size,\n+            num_upsampling_stages=2,  # Reduced from 3 to 2\n+        )\n+\n+        return Sam3Config(\n+            vision_config=vision_config,\n+            text_config=text_config,\n+            geometry_encoder_config=geometry_encoder_config,\n+            detr_encoder_config=detr_encoder_config,\n+            detr_decoder_config=detr_decoder_config,\n+            mask_decoder_config=mask_decoder_config,\n+        )\n+\n+    def create_and_check_model(self, config, pixel_values, input_ids, attention_mask):\n+        model = Sam3Model(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(pixel_values=pixel_values, input_ids=input_ids, attention_mask=attention_mask)\n+\n+        # Check output shapes\n+        self.parent.assertIsNotNone(result.pred_masks)\n+        self.parent.assertIsNotNone(result.pred_boxes)\n+        self.parent.assertIsNotNone(result.pred_logits)\n+\n+        # Masks should be [batch_size, num_queries, H, W]\n+        self.parent.assertEqual(result.pred_masks.shape[0], self.batch_size)\n+        self.parent.assertEqual(result.pred_masks.shape[1], self.detr_decoder_num_queries)\n+\n+        # Boxes should be [batch_size, num_queries, 4]\n+        self.parent.assertEqual(result.pred_boxes.shape, (self.batch_size, self.detr_decoder_num_queries, 4))\n+\n+        # Logits should be [batch_size, num_queries]\n+        self.parent.assertEqual(result.pred_logits.shape, (self.batch_size, self.detr_decoder_num_queries))\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values, input_ids, attention_mask = config_and_inputs\n+        inputs_dict = {\n+            \"pixel_values\": pixel_values,\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class Sam3ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Tests for SAM3 full model.\n+    \"\"\"\n+\n+    all_model_classes = (Sam3Model,) if is_torch_available() else ()\n+    pipeline_model_mapping = {\"mask-generation\": Sam3Model} if is_torch_available() else {}\n+\n+    test_resize_embeddings = False\n+    test_torch_exportable = False\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = Sam3ModelTester(self)\n+        common_properties = [\"initializer_range\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=Sam3Config, has_text_modality=False, common_properties=common_properties\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    @unittest.skip(reason=\"SAM3 does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    def test_model_get_set_embeddings(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            # Vision encoder has input embeddings\n+            self.assertIsInstance(model.vision_encoder.get_input_embeddings(), (nn.Module))\n+            x = model.get_output_embeddings()\n+            self.assertTrue(x is None or isinstance(x, nn.Linear))\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_batching_equivalence(self, atol=5e-4, rtol=5e-4):\n+        super().test_batching_equivalence(atol=atol, rtol=rtol)\n+\n+    # Override as SAM3Model has component-specific attention outputs\n+    def test_attention_outputs(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            # Check that we have the component-specific attention outputs\n+            # Note: Some may be empty tuples if attentions aren't collected for that component\n+            self.assertIsNotNone(outputs.vision_attentions)\n+            self.assertIsNotNone(outputs.detr_encoder_attentions)\n+            self.assertIsNotNone(outputs.detr_decoder_attentions)\n+            self.assertIsNotNone(outputs.mask_decoder_attentions)\n+\n+            # Check vision attentions (from ViT backbone) - should be properly collected\n+            if outputs.vision_attentions:\n+                vision_attentions = outputs.vision_attentions\n+                self.assertEqual(len(vision_attentions), self.model_tester.num_hidden_layers)\n+\n+            # Check that at least vision attentions are present (others may require different collection mechanism)\n+            self.assertTrue(\n+                len(outputs.vision_attentions) > 0,\n+                \"At least vision attentions should be collected when output_attentions=True\",\n+            )\n+\n+            # check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+            for k in config.sub_configs:\n+                if getattr(config, k) is not None:\n+                    getattr(config, k).output_attentions = True\n+\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            # Verify again with config-based setting\n+            self.assertIsNotNone(outputs.vision_attentions)\n+            self.assertIsNotNone(outputs.detr_encoder_attentions)\n+            self.assertIsNotNone(outputs.detr_decoder_attentions)\n+            self.assertIsNotNone(outputs.mask_decoder_attentions)\n+\n+    # Override as SAM3Model has component-specific attention/hidden state outputs\n+    def test_retain_grad_hidden_states_attentions(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        for k in config.sub_configs:\n+            if getattr(config, k) is not None:\n+                getattr(config, k).output_hidden_states = True\n+                getattr(config, k).output_attentions = True\n+\n+        config.output_hidden_states = True\n+        config.output_attentions = True\n+        config._attn_implementation = \"eager\"\n+\n+        # Use first model class\n+        model_class = self.all_model_classes[0]\n+        model = model_class._from_config(config, attn_implementation=\"eager\")\n+        model.to(torch_device)\n+\n+        inputs = self._prepare_for_class(inputs_dict, model_class)\n+        outputs = model(**inputs)\n+\n+        output = outputs[0]\n+\n+        # SAM3 has component-specific hidden states and attentions\n+        # Check vision hidden states and attentions\n+        if outputs.vision_hidden_states is not None and len(outputs.vision_hidden_states) > 0:\n+            vision_hidden_states = outputs.vision_hidden_states[0]\n+            vision_hidden_states.retain_grad()\n+\n+        if outputs.vision_attentions is not None and len(outputs.vision_attentions) > 0:\n+            vision_attentions = outputs.vision_attentions[0]\n+            vision_attentions.retain_grad()\n+\n+        # Check DETR encoder hidden states and attentions\n+        if outputs.encoder_hidden_states is not None and len(outputs.encoder_hidden_states) > 0:\n+            encoder_hidden_states = outputs.encoder_hidden_states[0]\n+            encoder_hidden_states.retain_grad()\n+\n+        if outputs.detr_encoder_attentions is not None and len(outputs.detr_encoder_attentions) > 0:\n+            detr_encoder_attentions = outputs.detr_encoder_attentions[0]\n+            detr_encoder_attentions.retain_grad()\n+\n+        # Check DETR decoder hidden states and attentions\n+        if outputs.decoder_hidden_states is not None and len(outputs.decoder_hidden_states) > 0:\n+            decoder_hidden_states = outputs.decoder_hidden_states[0]\n+            decoder_hidden_states.retain_grad()\n+\n+        if outputs.detr_decoder_attentions is not None and len(outputs.detr_decoder_attentions) > 0:\n+            detr_decoder_attentions = outputs.detr_decoder_attentions[0]\n+            detr_decoder_attentions.retain_grad()\n+\n+        # Check mask decoder attentions\n+        if outputs.mask_decoder_attentions is not None and len(outputs.mask_decoder_attentions) > 0:\n+            mask_decoder_attentions = outputs.mask_decoder_attentions[0]\n+            mask_decoder_attentions.retain_grad()\n+\n+        output.flatten()[0].backward(retain_graph=True)\n+\n+        # Check gradients are not None\n+        if outputs.vision_hidden_states is not None and len(outputs.vision_hidden_states) > 0:\n+            self.assertIsNotNone(vision_hidden_states.grad)\n+\n+        if outputs.vision_attentions is not None and len(outputs.vision_attentions) > 0:\n+            self.assertIsNotNone(vision_attentions.grad)\n+\n+        if outputs.encoder_hidden_states is not None and len(outputs.encoder_hidden_states) > 0:\n+            self.assertIsNotNone(encoder_hidden_states.grad)\n+\n+        if outputs.detr_encoder_attentions is not None and len(outputs.detr_encoder_attentions) > 0:\n+            self.assertIsNotNone(detr_encoder_attentions.grad)\n+\n+        if outputs.decoder_hidden_states is not None and len(outputs.decoder_hidden_states) > 0:\n+            self.assertIsNotNone(decoder_hidden_states.grad)\n+\n+        if outputs.detr_decoder_attentions is not None and len(outputs.detr_decoder_attentions) > 0:\n+            self.assertIsNotNone(detr_decoder_attentions.grad)\n+\n+        if outputs.mask_decoder_attentions is not None and len(outputs.mask_decoder_attentions) > 0:\n+            self.assertIsNotNone(mask_decoder_attentions.grad)\n+\n+    def test_hidden_states_output(self):\n+        \"\"\"Test that SAM3 properly outputs component-specific hidden states.\"\"\"\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            # Enable hidden states output\n+            config.output_hidden_states = True\n+            for k in config.sub_configs:\n+                if getattr(config, k) is not None:\n+                    getattr(config, k).output_hidden_states = True\n+\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            # SAM3 has component-specific hidden states\n+            # Check vision hidden states\n+            if outputs.vision_hidden_states is not None:\n+                vision_hidden_states = outputs.vision_hidden_states\n+                self.assertIsInstance(vision_hidden_states, (list, tuple))\n+                # Vision encoder outputs hidden states from each layer\n+                expected_num_vision_layers = self.model_tester.num_hidden_layers + 1  # +1 for embeddings\n+                self.assertEqual(len(vision_hidden_states), expected_num_vision_layers)\n+\n+            # Check DETR encoder hidden states (stored as encoder_hidden_states)\n+            if outputs.encoder_hidden_states is not None:\n+                encoder_hidden_states = outputs.encoder_hidden_states\n+                self.assertIsInstance(encoder_hidden_states, (list, tuple))\n+\n+            # Check DETR decoder hidden states (stored as decoder_hidden_states)\n+            if outputs.decoder_hidden_states is not None:\n+                decoder_hidden_states = outputs.decoder_hidden_states\n+                self.assertIsInstance(decoder_hidden_states, (list, tuple))\n+\n+    @unittest.skip(reason=\"SAM3VisionModel has FPN channel mismatch with flex attention\")\n+    def test_flex_attention_with_grads(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"Sam3Model creates attention masks from features (with gradients), \"\n+        \"which is incompatible with flash attention's expectation of binary masks\"\n+    )\n+    def test_flash_attn_2_inference_equivalence(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"Sam3Model creates attention masks from features (with gradients), \"\n+        \"which is incompatible with flash attention's expectation of binary masks\"\n+    )\n+    def test_flash_attn_2_inference_equivalence_right_padding(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"Sam3Model creates attention masks from features (with gradients), \"\n+        \"which is incompatible with flash attention's expectation of binary masks\"\n+    )\n+    def test_flash_attn_3_inference_equivalence(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"Sam3Model creates attention masks from features (with gradients), \"\n+        \"which is incompatible with flash attention's expectation of binary masks\"\n+    )\n+    def test_flash_attn_3_inference_equivalence_right_padding(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"Sam3Model creates attention masks from features (with gradients), \"\n+        \"which is incompatible with flash attention's expectation of binary masks\"\n+    )\n+    def test_flash_attn_kernels_inference_equivalence(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"Sam3Model creates attention masks from features (with gradients), \"\n+        \"which is incompatible with flash attention's expectation of binary masks\"\n+    )\n+    def test_flash_attn_kernels_mps_inference_equivalence(self):\n+        pass\n+\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        \"\"\"\n+        Tests if composite models dispatch correctly on SDPA/eager when requested.\n+        SAM3 has multiple sub-models: vision_encoder, text_encoder, geometry_encoder,\n+        detr_encoder, detr_decoder, mask_decoder.\n+        \"\"\"\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        if not self._is_composite:\n+            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_sdpa = model_class.from_pretrained(tmpdirname, attn_implementation=\"sdpa\")\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                # Get all sub-models that support attention\n+                vision_encoder_sdpa = getattr(model_sdpa, \"vision_encoder\")\n+                text_encoder_sdpa = getattr(model_sdpa, \"text_encoder\", None)\n+                detr_encoder_sdpa = getattr(model_sdpa, \"detr_encoder\", None)\n+                detr_decoder_sdpa = getattr(model_sdpa, \"detr_decoder\", None)\n+                mask_decoder_sdpa = getattr(model_sdpa, \"mask_decoder\", None)\n+\n+                # Check that sub-models dispatch to SDPA if they support it\n+                self.assertTrue(vision_encoder_sdpa.config._attn_implementation == \"sdpa\")\n+                if text_encoder_sdpa is not None and hasattr(text_encoder_sdpa, \"_supports_sdpa\"):\n+                    # Text encoder from CLIP should support SDPA\n+                    self.assertTrue(text_encoder_sdpa.config._attn_implementation == \"sdpa\")\n+                if detr_encoder_sdpa is not None:\n+                    self.assertTrue(detr_encoder_sdpa.config._attn_implementation == \"sdpa\")\n+                if detr_decoder_sdpa is not None:\n+                    self.assertTrue(detr_decoder_sdpa.config._attn_implementation == \"sdpa\")\n+                if mask_decoder_sdpa is not None:\n+                    self.assertTrue(mask_decoder_sdpa.config._attn_implementation == \"sdpa\")\n+\n+                # Now test with eager\n+                model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n+                model_eager = model_eager.eval().to(torch_device)\n+\n+                self.assertTrue(getattr(model_eager, \"vision_encoder\").config._attn_implementation == \"eager\")\n+                if hasattr(model_eager, \"text_encoder\"):\n+                    self.assertTrue(model_eager.text_encoder.config._attn_implementation == \"eager\")\n+                if hasattr(model_eager, \"detr_encoder\"):\n+                    self.assertTrue(model_eager.detr_encoder.config._attn_implementation == \"eager\")\n+                if hasattr(model_eager, \"detr_decoder\"):\n+                    self.assertTrue(model_eager.detr_decoder.config._attn_implementation == \"eager\")\n+                if hasattr(model_eager, \"mask_decoder\"):\n+                    self.assertTrue(model_eager.mask_decoder.config._attn_implementation == \"eager\")\n+\n+                # Verify no SDPA layers in eager model\n+                for name, submodule in model_eager.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if (\n+                        class_name.endswith(\"Attention\")\n+                        and getattr(submodule, \"config\", None)\n+                        and submodule.config._attn_implementation == \"sdpa\"\n+                    ):\n+                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+    def test_forward_with_text_embeds(self):\n+        \"\"\"Test that text_embeds parameter works correctly.\"\"\"\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            # First get text embeddings\n+            with torch.no_grad():\n+                text_embeds = model.get_text_features(\n+                    input_ids=inputs_dict[\"input_ids\"], attention_mask=inputs_dict[\"attention_mask\"]\n+                )\n+\n+            # Forward with text_embeds (remove input_ids)\n+            inputs_with_embeds = {\n+                \"pixel_values\": inputs_dict[\"pixel_values\"],\n+                \"text_embeds\": text_embeds,\n+            }\n+\n+            with torch.no_grad():\n+                outputs_with_embeds = model(**inputs_with_embeds)\n+\n+            # Forward with input_ids\n+            with torch.no_grad():\n+                outputs_with_ids = model(**inputs_dict)\n+\n+            # Outputs should be very close\n+            self.assertTrue(torch.allclose(outputs_with_embeds.pred_logits, outputs_with_ids.pred_logits, atol=1e-5))\n+            self.assertTrue(torch.allclose(outputs_with_embeds.pred_boxes, outputs_with_ids.pred_boxes, atol=1e-5))\n+\n+    def test_forward_with_both_input_ids_and_text_embeds_raises_error(self):\n+        \"\"\"Test that passing both input_ids and text_embeds raises an error.\"\"\"\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            # Get text embeddings\n+            with torch.no_grad():\n+                text_embeds = model.get_text_features(\n+                    input_ids=inputs_dict[\"input_ids\"], attention_mask=inputs_dict[\"attention_mask\"]\n+                )\n+\n+            # Try to pass both (should raise error)\n+            inputs_with_both = {\n+                \"pixel_values\": inputs_dict[\"pixel_values\"],\n+                \"input_ids\": inputs_dict[\"input_ids\"],\n+                \"text_embeds\": text_embeds,\n+            }\n+\n+            with self.assertRaises(ValueError):\n+                model(**inputs_with_both)\n+\n+    def test_forward_with_vision_embeds(self):\n+        \"\"\"Test that vision_embeds parameter works correctly.\"\"\"\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            # First get vision embeddings\n+            with torch.no_grad():\n+                vision_embeds = model.get_vision_features(pixel_values=inputs_dict[\"pixel_values\"])\n+\n+            # Forward with vision_embeds (remove pixel_values)\n+            inputs_with_embeds = {\n+                \"vision_embeds\": vision_embeds,\n+                \"input_ids\": inputs_dict[\"input_ids\"],\n+                \"attention_mask\": inputs_dict[\"attention_mask\"],\n+            }\n+\n+            with torch.no_grad():\n+                outputs_with_embeds = model(**inputs_with_embeds)\n+\n+            # Forward with pixel_values\n+            with torch.no_grad():\n+                outputs_with_pixels = model(**inputs_dict)\n+\n+            # Outputs should be very close\n+            self.assertTrue(\n+                torch.allclose(outputs_with_embeds.pred_logits, outputs_with_pixels.pred_logits, atol=1e-5)\n+            )\n+            self.assertTrue(torch.allclose(outputs_with_embeds.pred_boxes, outputs_with_pixels.pred_boxes, atol=1e-5))\n+\n+    def test_forward_with_both_pixel_values_and_vision_embeds_raises_error(self):\n+        \"\"\"Test that passing both pixel_values and vision_embeds raises an error.\"\"\"\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            # Get vision embeddings\n+            with torch.no_grad():\n+                vision_embeds = model.get_vision_features(pixel_values=inputs_dict[\"pixel_values\"])\n+\n+            # Try to pass both (should raise error)\n+            inputs_with_both = {\n+                \"pixel_values\": inputs_dict[\"pixel_values\"],\n+                \"vision_embeds\": vision_embeds,\n+                \"input_ids\": inputs_dict[\"input_ids\"],\n+                \"attention_mask\": inputs_dict[\"attention_mask\"],\n+            }\n+\n+            with self.assertRaises(ValueError):\n+                model(**inputs_with_both)\n+\n+    @unittest.skip(reason=\"SAM3 model can't be compiled dynamic yet\")\n+    def test_sdpa_can_compile_dynamic(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"SAM3 uses CLIP text encoder which has two attention masks: `causal_attention_mask` and `attention_mask`.\"\n+    )\n+    def test_sdpa_can_dispatch_on_flash(self):\n+        pass\n+\n+    def test_model_outputs_equivalence(self):\n+        \"\"\"\n+        Test that tuple and dict outputs are equivalent.\n+        SAM3 returns complex outputs with component-specific fields, so we need to ensure proper conversion.\n+        \"\"\"\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        def set_nan_tensor_to_zero(t):\n+            t[t != t] = 0\n+            return t\n+\n+        def check_equivalence(model, tuple_inputs, dict_inputs, additional_kwargs={}):\n+            with torch.no_grad():\n+                tuple_output = model(**tuple_inputs, return_dict=False, **additional_kwargs)\n+                dict_output = model(**dict_inputs, return_dict=True, **additional_kwargs).to_tuple()\n+\n+                def recursive_check(tuple_object, dict_object):\n+                    if isinstance(tuple_object, (list, tuple)):\n+                        for tuple_iterable_value, dict_iterable_value in zip(tuple_object, dict_object):\n+                            recursive_check(tuple_iterable_value, dict_iterable_value)\n+                    elif isinstance(tuple_object, dict):\n+                        for tuple_iterable_value, dict_iterable_value in zip(\n+                            tuple_object.values(), dict_object.values()\n+                        ):\n+                            recursive_check(tuple_iterable_value, dict_iterable_value)\n+                    elif tuple_object is None:\n+                        return\n+                    # model might return non-tensors objects (e.g. Cache class)\n+                    elif isinstance(tuple_object, torch.Tensor):\n+                        self.assertTrue(\n+                            torch.allclose(\n+                                set_nan_tensor_to_zero(tuple_object), set_nan_tensor_to_zero(dict_object), atol=1e-5\n+                            ),\n+                            msg=(\n+                                \"Tuple and dict output are not equal. Difference:\"\n+                                f\" {torch.max(torch.abs(tuple_object - dict_object))}. Tuple has `nan`:\"\n+                                f\" {torch.isnan(tuple_object).any()} and `inf`: {torch.isinf(tuple_object)}. Dict has\"\n+                                f\" `nan`: {torch.isnan(dict_object).any()} and `inf`: {torch.isinf(dict_object)}.\"\n+                            ),\n+                        )\n+\n+                recursive_check(tuple_output, dict_output)\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            tuple_inputs = self._prepare_for_class(inputs_dict, model_class)\n+            dict_inputs = self._prepare_for_class(inputs_dict, model_class)\n+            check_equivalence(model, tuple_inputs, dict_inputs)\n+\n+            # Test with output_hidden_states\n+            tuple_inputs = self._prepare_for_class(inputs_dict, model_class)\n+            dict_inputs = self._prepare_for_class(inputs_dict, model_class)\n+            check_equivalence(model, tuple_inputs, dict_inputs, {\"output_hidden_states\": True})\n+\n+            # Test with output_attentions if supported\n+            if self.has_attentions:\n+                tuple_inputs = self._prepare_for_class(inputs_dict, model_class)\n+                dict_inputs = self._prepare_for_class(inputs_dict, model_class)\n+                check_equivalence(model, tuple_inputs, dict_inputs, {\"output_attentions\": True})\n+\n+                tuple_inputs = self._prepare_for_class(inputs_dict, model_class)\n+                dict_inputs = self._prepare_for_class(inputs_dict, model_class)\n+                check_equivalence(\n+                    model, tuple_inputs, dict_inputs, {\"output_hidden_states\": True, \"output_attentions\": True}\n+                )\n+\n+    def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n+        \"\"\"Override to ensure input_ids and attention_mask are always present for Sam3Model.\"\"\"\n+        inputs_dict = super()._prepare_for_class(inputs_dict, model_class, return_labels=return_labels)\n+\n+        # Sam3Model always requires input_ids and attention_mask for text encoding\n+        if model_class == Sam3Model:\n+            if \"input_ids\" not in inputs_dict or inputs_dict.get(\"input_ids\") is None:\n+                # Create dummy input_ids if not present\n+                # Get batch_size from pixel_values or vision_embeds\n+                if \"pixel_values\" in inputs_dict and inputs_dict.get(\"pixel_values\") is not None:\n+                    batch_size = inputs_dict[\"pixel_values\"].shape[0]\n+                elif \"vision_embeds\" in inputs_dict and inputs_dict.get(\"vision_embeds\") is not None:\n+                    vision_embeds = inputs_dict[\"vision_embeds\"]\n+                    if vision_embeds.fpn_hidden_states is not None and len(vision_embeds.fpn_hidden_states) > 0:\n+                        batch_size = vision_embeds.fpn_hidden_states[0].shape[0]\n+                    elif vision_embeds.last_hidden_state is not None:\n+                        batch_size = vision_embeds.last_hidden_state.shape[0]\n+                    else:\n+                        batch_size = 2\n+                else:\n+                    batch_size = 2\n+                config = self.model_tester.get_config()\n+                # text_config might be a dict or a config object\n+                if isinstance(config.text_config, dict):\n+                    vocab_size = config.text_config.get(\"vocab_size\", 1000)\n+                else:\n+                    vocab_size = getattr(config.text_config, \"vocab_size\", 1000)\n+                inputs_dict[\"input_ids\"] = torch.randint(0, vocab_size, (batch_size, 16), device=torch_device)\n+            if \"attention_mask\" not in inputs_dict or inputs_dict.get(\"attention_mask\") is None:\n+                inputs_dict[\"attention_mask\"] = torch.ones_like(inputs_dict[\"input_ids\"])\n+\n+        return inputs_dict\n+\n+\n+def prepare_coco_cat_image():\n+    \"\"\"Prepare COCO cat and laptop image (from batched inference notebook).\"\"\"\n+    img_url = \"http://images.cocodataset.org/val2017/000000077595.jpg\"\n+    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+    return raw_image\n+\n+\n+def prepare_coco_kitchen_image():\n+    \"\"\"Prepare COCO kitchen scene image (from batched inference notebook).\"\"\"\n+    img_url = \"http://images.cocodataset.org/val2017/000000136466.jpg\"\n+    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+    return raw_image\n+\n+\n+@slow\n+class Sam3ModelIntegrationTest(unittest.TestCase):\n+    \"\"\"Integration tests for SAM3 model with real pretrained weights.\"\"\"\n+\n+    def setUp(self):\n+        super().setUp()\n+        model_name = \"../sam3-hf-v4-video-full\"\n+        self.model = Sam3Model.from_pretrained(model_name).to(torch.float32)\n+        self.processor = Sam3Processor.from_pretrained(model_name)\n+        self.model.to(torch_device)\n+        self.model.eval()\n+\n+    def tearDown(self):\n+        super().tearDown()\n+        gc.collect()\n+        backend_empty_cache(torch_device)\n+\n+    def test_inference_text_prompt_only(self):\n+        \"\"\"Test inference with text prompt only (from multiway_prompting notebook).\"\"\"\n+        # Example from notebook: \"short hair\" text prompt\n+        raw_image = prepare_coco_cat_image()\n+        text = \"ear\"\n+\n+        inputs = self.processor(images=raw_image, text=text, return_tensors=\"pt\").to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = self.model(**inputs)\n+\n+        # Check exact output shapes\n+        self.assertEqual(outputs.pred_masks.shape, (1, 200, 288, 288))\n+        self.assertEqual(outputs.pred_boxes.shape, (1, 200, 4))\n+        self.assertEqual(outputs.pred_logits.shape, (1, 200))\n+\n+        # Check that predictions have reasonable scores (after sigmoid)\n+        scores = torch.sigmoid(outputs.pred_logits)\n+        self.assertTrue((scores >= 0).all() and (scores <= 1).all())\n+\n+        # Check exact values\n+        sorted_indices = torch.argsort(scores.squeeze(), descending=True)\n+        top_scores = scores.squeeze()[sorted_indices[:3]]\n+        top_logits = outputs.pred_logits.squeeze()[sorted_indices[:3]]\n+        top_idx = sorted_indices[0].item()\n+\n+        torch.testing.assert_close(\n+            top_scores, torch.tensor([0.9381, 0.9214, 0.0910]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+        torch.testing.assert_close(\n+            top_logits, torch.tensor([2.7182, 2.4618, -2.3020]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+        torch.testing.assert_close(\n+            outputs.pred_boxes[0, top_idx],\n+            torch.tensor([0.4704, 0.2014, 0.5615, 0.3770]).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+        torch.testing.assert_close(\n+            outputs.pred_masks[0, top_idx, :3, :3],\n+            torch.tensor(\n+                [[-2.1815, -6.2767, -7.0687], [-5.7988, -10.2704, -10.9379], [-8.5194, -10.7892, -9.9152]]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+        # Test post-processing\n+        results = self.processor.post_process_instance_segmentation(\n+            outputs, threshold=0.5, mask_threshold=0.5, target_sizes=inputs.get(\"original_sizes\").tolist()\n+        )\n+        self.assertEqual(len(results), 1)\n+        result = results[0]\n+\n+        # Check that we have detections\n+        self.assertGreater(len(result[\"masks\"]), 0)\n+        self.assertGreater(len(result[\"boxes\"]), 0)\n+        self.assertGreater(len(result[\"scores\"]), 0)\n+\n+        # Check exact values for top detection\n+        top_pp_score = result[\"scores\"][0]\n+        top_pp_box = result[\"boxes\"][0]\n+\n+        torch.testing.assert_close(top_pp_score, torch.tensor(0.9210).to(torch_device), atol=1e-4, rtol=1e-4)\n+        torch.testing.assert_close(\n+            top_pp_box, torch.tensor([402.1755, 90.1420, 459.6165, 156.3702]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+\n+    def test_inference_single_box_prompt(self):\n+        \"\"\"Test inference with a single bounding box prompt (from batched_inference notebook).\"\"\"\n+        raw_image = prepare_coco_cat_image()\n+        # Example from notebook: laptop region in image 1\n+        # Box in xyxy format: [100, 150, 500, 450]\n+        box_xyxy = [100, 150, 500, 450]\n+        input_boxes = [[box_xyxy]]\n+\n+        inputs = self.processor(\n+            images=raw_image,\n+            input_boxes=input_boxes,\n+            input_boxes_labels=[[1]],  # Positive box\n+            return_tensors=\"pt\",\n+        ).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = self.model(**inputs)\n+\n+        # Check exact output shapes\n+        self.assertEqual(outputs.pred_masks.shape, (1, 200, 288, 288))\n+        self.assertEqual(outputs.pred_boxes.shape, (1, 200, 4))\n+        self.assertEqual(outputs.pred_logits.shape, (1, 200))\n+\n+        # Check exact values\n+        scores = torch.sigmoid(outputs.pred_logits)\n+        sorted_indices = torch.argsort(scores.squeeze(), descending=True)\n+        top_scores = scores.squeeze()[sorted_indices[:3]]\n+        top_logits = outputs.pred_logits.squeeze()[sorted_indices[:3]]\n+        top_idx = sorted_indices[0].item()\n+\n+        torch.testing.assert_close(\n+            top_scores, torch.tensor([0.9308, 0.1617, 0.1336]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+        torch.testing.assert_close(\n+            top_logits, torch.tensor([2.5988, -1.6460, -1.8699]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+        torch.testing.assert_close(\n+            outputs.pred_boxes[0, top_idx],\n+            torch.tensor([0.1631, 0.4140, 0.7510, 0.9931]).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+        torch.testing.assert_close(\n+            outputs.pred_masks[0, top_idx, :3, :3],\n+            torch.tensor([[-1.8726, -3.5063, -3.7716], [-3.1987, -5.3820, -5.6782], [-3.8850, -5.4164, -5.8604]]).to(\n+                torch_device\n+            ),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+        # Test post-processing\n+        results = self.processor.post_process_instance_segmentation(\n+            outputs, threshold=0.5, mask_threshold=0.5, target_sizes=inputs.get(\"original_sizes\").tolist()\n+        )\n+        self.assertEqual(len(results), 1)\n+        result = results[0]\n+\n+        # Check that we have detections\n+        self.assertGreater(len(result[\"masks\"]), 0)\n+\n+        # Check exact values for top detection\n+        top_pp_score = result[\"scores\"][0]\n+        top_pp_box = result[\"boxes\"][0]\n+\n+        torch.testing.assert_close(top_pp_score, torch.tensor(0.9307).to(torch_device), atol=1e-4, rtol=1e-4)\n+        torch.testing.assert_close(\n+            top_pp_box, torch.tensor([104.3945, 175.9433, 480.6293, 422.0826]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+\n+    def test_inference_multi_box_prompt(self):\n+        \"\"\"Test inference with multiple box prompts with positive and negative labels (from batched_inference notebook).\"\"\"\n+        raw_image = prepare_coco_kitchen_image()\n+        # Example from notebook: multiple positive boxes (dial + button)\n+        # Dial box (xyxy): [59, 144, 76, 163]\n+        # Button box (xyxy): [87, 148, 104, 159]\n+        box1_xyxy = [59, 144, 76, 163]\n+        box2_xyxy = [87, 148, 104, 159]\n+\n+        input_boxes = [[box1_xyxy, box2_xyxy]]\n+        input_boxes_labels = [[1, 1]]  # Both positive\n+\n+        inputs = self.processor(\n+            images=raw_image, input_boxes=input_boxes, input_boxes_labels=input_boxes_labels, return_tensors=\"pt\"\n+        ).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = self.model(**inputs)\n+\n+        # Check exact output shapes\n+        self.assertEqual(outputs.pred_masks.shape, (1, 200, 288, 288))\n+        self.assertEqual(outputs.pred_boxes.shape, (1, 200, 4))\n+        self.assertEqual(outputs.pred_logits.shape, (1, 200))\n+\n+        # Check exact values\n+        scores = torch.sigmoid(outputs.pred_logits)\n+        sorted_indices = torch.argsort(scores.squeeze(), descending=True)\n+        top_scores = scores.squeeze()[sorted_indices[:3]]\n+        top_logits = outputs.pred_logits.squeeze()[sorted_indices[:3]]\n+        top_idx = sorted_indices[0].item()\n+\n+        torch.testing.assert_close(\n+            top_scores, torch.tensor([0.9611, 0.9379, 0.8348]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+        torch.testing.assert_close(\n+            top_logits, torch.tensor([3.2071, 2.7154, 1.6198]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+        torch.testing.assert_close(\n+            outputs.pred_boxes[0, top_idx],\n+            torch.tensor([0.1757, 0.2888, 0.2296, 0.3259]).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+        torch.testing.assert_close(\n+            outputs.pred_masks[0, top_idx, :3, :3],\n+            torch.tensor(\n+                [[-8.6138, -14.5615, -17.9965], [-13.6695, -20.4994, -25.6705], [-14.9681, -23.0616, -17.0045]]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+        # Test post-processing\n+        results = self.processor.post_process_instance_segmentation(\n+            outputs, threshold=0.5, mask_threshold=0.5, target_sizes=inputs.get(\"original_sizes\").tolist()\n+        )\n+        self.assertEqual(len(results), 1)\n+        result = results[0]\n+\n+        # Check that we have detections\n+        self.assertGreater(len(result[\"masks\"]), 0)\n+\n+        # Check exact values for top detection\n+        top_pp_score = result[\"scores\"][0]\n+        top_pp_box = result[\"boxes\"][0]\n+\n+        torch.testing.assert_close(top_pp_score, torch.tensor(0.9379).to(torch_device), atol=1e-4, rtol=1e-4)\n+        torch.testing.assert_close(\n+            top_pp_box, torch.tensor([86.8687, 147.5269, 104.4475, 159.6138]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+\n+    def test_inference_combined_prompts(self):\n+        \"\"\"Test inference with combined text and geometry prompts (text + negative box from batched_inference notebook).\"\"\"\n+        raw_image = prepare_coco_kitchen_image()\n+        # Example from notebook: text \"handle\" + negative box to exclude oven handle\n+        text = \"handle\"\n+        # Negative box covering the oven handle area (xyxy): [40, 183, 318, 204]\n+        oven_handle_box = [40, 183, 318, 204]\n+\n+        input_boxes = [[oven_handle_box]]\n+\n+        inputs = self.processor(\n+            images=raw_image,\n+            text=text,\n+            input_boxes=input_boxes,\n+            input_boxes_labels=[[0]],  # 0 = negative\n+            return_tensors=\"pt\",\n+        ).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = self.model(**inputs)\n+\n+        # Check exact output shapes\n+        self.assertEqual(outputs.pred_masks.shape, (1, 200, 288, 288))\n+        self.assertEqual(outputs.pred_boxes.shape, (1, 200, 4))\n+        self.assertEqual(outputs.pred_logits.shape, (1, 200))\n+\n+    def test_inference_batched_images(self):\n+        \"\"\"Test batched inference with multiple images (from batched_inference notebook).\"\"\"\n+        # Example from notebook: batch of 2 images with different text prompts\n+        raw_image1 = prepare_coco_cat_image()\n+        raw_image2 = prepare_coco_kitchen_image()\n+\n+        # Batch of 2 images with different text prompts: \"ear\" for cat, \"dial\" for kitchen\n+        inputs = self.processor(images=[raw_image1, raw_image2], text=[\"ear\", \"dial\"], return_tensors=\"pt\").to(\n+            torch_device\n+        )\n+\n+        with torch.no_grad():\n+            outputs = self.model(**inputs)\n+\n+        # Check exact output shapes\n+        self.assertEqual(outputs.pred_masks.shape, (2, 200, 288, 288))\n+        self.assertEqual(outputs.pred_boxes.shape, (2, 200, 4))\n+        self.assertEqual(outputs.pred_logits.shape, (2, 200))\n+\n+        # Check scores are reasonable\n+        scores = torch.sigmoid(outputs.pred_logits)\n+        self.assertTrue((scores >= 0).all() and (scores <= 1).all())\n+\n+        # Check exact values\n+        sorted_indices_0 = torch.argsort(scores[0], descending=True)\n+        sorted_indices_1 = torch.argsort(scores[1], descending=True)\n+        top_scores_0 = scores[0][sorted_indices_0[:3]]\n+        top_scores_1 = scores[1][sorted_indices_1[:3]]\n+        top_logits_0 = outputs.pred_logits[0][sorted_indices_0[:3]]\n+        top_logits_1 = outputs.pred_logits[1][sorted_indices_1[:3]]\n+        top_idx_0 = sorted_indices_0[0].item()\n+        top_idx_1 = sorted_indices_1[0].item()\n+\n+        torch.testing.assert_close(\n+            top_scores_0, torch.tensor([0.9381, 0.9214, 0.0910]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+        torch.testing.assert_close(\n+            top_scores_1, torch.tensor([0.8863, 0.8849, 0.8841]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+        torch.testing.assert_close(\n+            top_logits_0, torch.tensor([2.7182, 2.4618, -2.3020]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+        torch.testing.assert_close(\n+            top_logits_1, torch.tensor([2.0534, 2.0395, 2.0320]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+        torch.testing.assert_close(\n+            outputs.pred_boxes[0, top_idx_0],\n+            torch.tensor([0.4704, 0.2014, 0.5615, 0.3770]).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+        torch.testing.assert_close(\n+            outputs.pred_boxes[1, top_idx_1],\n+            torch.tensor([0.6162, 0.2769, 0.6838, 0.3238]).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+        torch.testing.assert_close(\n+            outputs.pred_masks[0, top_idx_0, :3, :3],\n+            torch.tensor(\n+                [[-2.1815, -6.2767, -7.0687], [-5.7988, -10.2704, -10.9379], [-8.5194, -10.7892, -9.9152]]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+        torch.testing.assert_close(\n+            outputs.pred_masks[1, top_idx_1, :3, :3],\n+            torch.tensor(\n+                [[-7.4371, -13.5898, -13.6496], [-11.8669, -20.6416, -23.0941], [-12.8623, -20.3439, -16.6497]]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+        # Test post-processing\n+        results = self.processor.post_process_instance_segmentation(\n+            outputs, threshold=0.3, mask_threshold=0.5, target_sizes=inputs.get(\"original_sizes\").tolist()\n+        )\n+        self.assertEqual(len(results), 2)\n+\n+        # Check that both have detections\n+        self.assertGreater(len(results[0][\"masks\"]), 0)\n+        self.assertGreater(len(results[1][\"masks\"]), 0)\n+\n+        # Check exact values for top detection in each image\n+        top_pp_score_0 = results[0][\"scores\"][0]\n+        top_pp_box_0 = results[0][\"boxes\"][0]\n+        top_pp_score_1 = results[1][\"scores\"][0]\n+        top_pp_box_1 = results[1][\"boxes\"][0]\n+\n+        torch.testing.assert_close(top_pp_score_0, torch.tensor(0.9210).to(torch_device), atol=1e-4, rtol=1e-4)\n+        torch.testing.assert_close(\n+            top_pp_box_0, torch.tensor([402.1755, 90.1421, 459.6165, 156.3701]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+        torch.testing.assert_close(top_pp_score_1, torch.tensor(0.6641).to(torch_device), atol=1e-4, rtol=1e-4)\n+        torch.testing.assert_close(\n+            top_pp_box_1, torch.tensor([110.6279, 271.1848, 137.3600, 301.3683]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+\n+    def test_inference_batched_mixed_prompts(self):\n+        \"\"\"Test batched inference with mixed prompt types (from batched_inference notebook).\"\"\"\n+        # Example from notebook: Image 1 with text \"laptop\", Image 2 with visual prompt (dial)\n+        raw_image1 = prepare_coco_cat_image()\n+        raw_image2 = prepare_coco_kitchen_image()\n+\n+        # Box for dial in image 2 (xyxy): [59, 144, 76, 163]\n+        box2_xyxy = [59, 144, 76, 163]\n+\n+        inputs = self.processor(\n+            images=[raw_image1, raw_image2],\n+            text=[\"laptop\", None],  # Only first image has text\n+            input_boxes=[None, [box2_xyxy]],  # Only second image has box\n+            input_boxes_labels=[None, [1]],\n+            return_tensors=\"pt\",\n+        ).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = self.model(**inputs)\n+\n+        # Check exact output shapes\n+        self.assertEqual(outputs.pred_masks.shape, (2, 200, 288, 288))\n+        self.assertEqual(outputs.pred_boxes.shape, (2, 200, 4))\n+        self.assertEqual(outputs.pred_logits.shape, (2, 200))\n+\n+        # Check exact values\n+        scores = torch.sigmoid(outputs.pred_logits)\n+        sorted_indices_0 = torch.argsort(scores[0], descending=True)\n+        sorted_indices_1 = torch.argsort(scores[1], descending=True)\n+        top_scores_0 = scores[0][sorted_indices_0[:3]]\n+        top_scores_1 = scores[1][sorted_indices_1[:3]]\n+        top_logits_0 = outputs.pred_logits[0][sorted_indices_0[:3]]\n+        top_logits_1 = outputs.pred_logits[1][sorted_indices_1[:3]]\n+        top_idx_0 = sorted_indices_0[0].item()\n+        top_idx_1 = sorted_indices_1[0].item()\n+\n+        torch.testing.assert_close(\n+            top_scores_0, torch.tensor([0.9756, 0.1352, 0.0701]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+        torch.testing.assert_close(\n+            top_scores_1, torch.tensor([0.9683, 0.8310, 0.8222]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+        torch.testing.assert_close(\n+            top_logits_0, torch.tensor([3.6865, -1.8555, -2.5854]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+        torch.testing.assert_close(\n+            top_logits_1, torch.tensor([3.4183, 1.5929, 1.5315]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+        torch.testing.assert_close(\n+            outputs.pred_boxes[0, top_idx_0],\n+            torch.tensor([-0.0013, 0.0016, 0.4521, 0.9964]).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+        torch.testing.assert_close(\n+            outputs.pred_boxes[1, top_idx_1],\n+            torch.tensor([0.1774, 0.2876, 0.2296, 0.3261]).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+        torch.testing.assert_close(\n+            outputs.pred_masks[0, top_idx_0, :3, :3],\n+            torch.tensor([[0.0520, 0.3121, 0.4103], [0.6820, 1.0069, 1.0949], [0.8418, 1.0318, 1.0365]]).to(\n+                torch_device\n+            ),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+        torch.testing.assert_close(\n+            outputs.pred_masks[1, top_idx_1, :3, :3],\n+            torch.tensor(\n+                [[-8.7447, -14.3499, -17.5662], [-13.6804, -20.3728, -25.5098], [-15.2996, -22.9116, -17.6658]]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+        # Test post-processing\n+        results = self.processor.post_process_instance_segmentation(\n+            outputs, threshold=0.3, mask_threshold=0.5, target_sizes=inputs.get(\"original_sizes\").tolist()\n+        )\n+        self.assertEqual(len(results), 2)\n+\n+        # Check that both have detections\n+        self.assertGreater(len(results[0][\"masks\"]), 0)\n+        self.assertGreater(len(results[1][\"masks\"]), 0)\n+\n+        # Check exact values for top detection in each image\n+        top_pp_score_0 = results[0][\"scores\"][0]\n+        top_pp_box_0 = results[0][\"boxes\"][0]\n+        top_pp_score_1 = results[1][\"scores\"][0]\n+        top_pp_box_1 = results[1][\"boxes\"][0]\n+\n+        torch.testing.assert_close(top_pp_score_0, torch.tensor(0.9655).to(torch_device), atol=1e-4, rtol=1e-4)\n+        torch.testing.assert_close(\n+            top_pp_box_0, torch.tensor([-0.8481, 0.6668, 289.3758, 423.4723]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+        torch.testing.assert_close(top_pp_score_1, torch.tensor(0.8222).to(torch_device), atol=1e-4, rtol=1e-4)\n+        torch.testing.assert_close(\n+            top_pp_box_1, torch.tensor([168.9376, 137.3257, 191.7281, 161.3243]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+\n+    # TODO add exact values\n+    def test_semantic_segmentation_output(self):\n+        \"\"\"Test that semantic segmentation output is produced.\"\"\"\n+        raw_image = prepare_coco_cat_image()\n+        inputs = self.processor(images=raw_image, text=\"ear\", return_tensors=\"pt\").to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = self.model(**inputs)\n+\n+        # Check exact semantic segmentation output shape\n+        self.assertEqual(outputs.semantic_seg.shape, (1, 1, 288, 288))\n+        # Check that semantic seg has same spatial size as pred_masks\n+        self.assertEqual(outputs.semantic_seg.shape[-2:], outputs.pred_masks.shape[-2:])"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/sam3_tracker/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/tests%2Fmodels%2Fsam3_tracker%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/tests%2Fmodels%2Fsam3_tracker%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam3_tracker%2F__init__.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a"
        },
        {
            "sha": "0c02f2613f5cb67e2b41cde114d60d6d7c493c89",
            "filename": "tests/models/sam3_tracker/test_modeling_sam3_tracker.py",
            "status": "added",
            "additions": 823,
            "deletions": 0,
            "changes": 823,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/tests%2Fmodels%2Fsam3_tracker%2Ftest_modeling_sam3_tracker.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/tests%2Fmodels%2Fsam3_tracker%2Ftest_modeling_sam3_tracker.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam3_tracker%2Ftest_modeling_sam3_tracker.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,823 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch SAM2 model.\"\"\"\n+\n+import gc\n+import tempfile\n+import unittest\n+\n+import requests\n+\n+from transformers import (\n+    Sam3TrackerConfig,\n+    Sam3TrackerMaskDecoderConfig,\n+    Sam3TrackerPromptEncoderConfig,\n+    pipeline,\n+)\n+from transformers.testing_utils import (\n+    backend_empty_cache,\n+    require_torch,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import is_torch_available, is_vision_available\n+from transformers.video_utils import load_video\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+    from transformers import Sam3TrackerModel, Sam3TrackerProcessor, Sam3VisionConfig, Sam3ViTConfig\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+\n+class Sam3TrackerPromptEncoderTester:\n+    def __init__(\n+        self,\n+        hidden_size=32,\n+        input_image_size=128,\n+        patch_size=16,\n+        mask_input_channels=8,\n+        num_point_embeddings=4,\n+        hidden_act=\"gelu\",\n+    ):\n+        self.hidden_size = hidden_size\n+        self.input_image_size = input_image_size\n+        self.patch_size = patch_size\n+        self.mask_input_channels = mask_input_channels\n+        self.num_point_embeddings = num_point_embeddings\n+        self.hidden_act = hidden_act\n+\n+    def get_config(self):\n+        return Sam3TrackerPromptEncoderConfig(\n+            image_size=self.input_image_size,\n+            patch_size=self.patch_size,\n+            mask_input_channels=self.mask_input_channels,\n+            hidden_size=self.hidden_size,\n+            num_point_embeddings=self.num_point_embeddings,\n+            hidden_act=self.hidden_act,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        dummy_points = floats_tensor([self.batch_size, 3, 2])\n+        config = self.get_config()\n+\n+        return config, dummy_points\n+\n+\n+class Sam3TrackerMaskDecoderTester:\n+    def __init__(\n+        self,\n+        hidden_size=32,\n+        hidden_act=\"relu\",\n+        mlp_dim=64,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        attention_downsample_rate=2,\n+        num_multimask_outputs=3,\n+        iou_head_depth=3,\n+        iou_head_hidden_dim=32,\n+    ):\n+        self.hidden_size = hidden_size\n+        self.hidden_act = hidden_act\n+        self.mlp_dim = mlp_dim\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.attention_downsample_rate = attention_downsample_rate\n+        self.num_multimask_outputs = num_multimask_outputs\n+        self.iou_head_depth = iou_head_depth\n+        self.iou_head_hidden_dim = iou_head_hidden_dim\n+\n+    def get_config(self):\n+        return Sam3TrackerMaskDecoderConfig(\n+            hidden_size=self.hidden_size,\n+            hidden_act=self.hidden_act,\n+            mlp_dim=self.mlp_dim,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            attention_downsample_rate=self.attention_downsample_rate,\n+            num_multimask_outputs=self.num_multimask_outputs,\n+            iou_head_depth=self.iou_head_depth,\n+            iou_head_hidden_dim=self.iou_head_hidden_dim,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        config = self.get_config()\n+\n+        dummy_inputs = {\n+            \"image_embedding\": floats_tensor([self.batch_size, self.hidden_size]),\n+        }\n+\n+        return config, dummy_inputs\n+\n+\n+class Sam3TrackerModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        num_channels=3,\n+        image_size=224,  # Keep reasonable size: 224 = 16 * 14\n+        hidden_size=32,\n+        patch_size=14,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        intermediate_size=64,\n+        window_size=8,  # 224/14 = 16 patches, 16/2 = 8 per window\n+        global_attn_indexes=None,\n+        fpn_hidden_size=32,\n+        scale_factors=None,\n+        backbone_feature_sizes=[[32, 32], [16, 16], [8, 8]],\n+        memory_encoder_hidden_size=32,\n+        batch_size=2,\n+        is_training=False,\n+    ):\n+        if global_attn_indexes is None:\n+            global_attn_indexes = [0, 1]\n+        if scale_factors is None:\n+            scale_factors = [2.0, 1.0, 0.5]  # 3 scales to match backbone_feature_sizes\n+\n+        self.parent = parent\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.hidden_size = hidden_size\n+        self.patch_size = patch_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.window_size = window_size\n+        self.global_attn_indexes = global_attn_indexes\n+        self.fpn_hidden_size = fpn_hidden_size\n+        self.scale_factors = scale_factors\n+        self.backbone_feature_sizes = backbone_feature_sizes\n+        self.batch_size = batch_size\n+        self.is_training = is_training\n+        self.memory_encoder_hidden_size = memory_encoder_hidden_size\n+        self.prompt_encoder_tester = Sam3TrackerPromptEncoderTester()\n+        self.mask_decoder_tester = Sam3TrackerMaskDecoderTester()\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+        config = self.get_config()\n+\n+        return config, pixel_values\n+\n+    def get_config(self):\n+        backbone_config = Sam3ViTConfig(\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            num_channels=self.num_channels,\n+            image_size=self.image_size,\n+            patch_size=self.patch_size,\n+            window_size=self.window_size,\n+            global_attn_indexes=self.global_attn_indexes,\n+        )\n+\n+        vision_config = Sam3VisionConfig(\n+            backbone_config=backbone_config,\n+            fpn_hidden_size=self.fpn_hidden_size,\n+            scale_factors=self.scale_factors,\n+            backbone_feature_sizes=self.backbone_feature_sizes,\n+        )\n+\n+        prompt_encoder_config = self.prompt_encoder_tester.get_config()\n+\n+        mask_decoder_config = self.mask_decoder_tester.get_config()\n+\n+        return Sam3TrackerConfig(\n+            vision_config=vision_config,\n+            prompt_encoder_config=prompt_encoder_config,\n+            mask_decoder_config=mask_decoder_config,\n+            memory_attention_hidden_size=self.hidden_size,\n+            memory_encoder_hidden_size=self.memory_encoder_hidden_size,\n+            image_size=self.image_size,\n+            mask_downsampler_embed_dim=32,\n+            memory_fuser_embed_dim=32,\n+            memory_attention_num_layers=1,\n+            memory_attention_feed_forward_hidden_size=32,\n+        )\n+\n+    def create_and_check_model(self, config, pixel_values):\n+        model = Sam3TrackerModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(pixel_values)\n+        self.parent.assertEqual(result.iou_scores.shape, (self.batch_size, 1, 3))\n+        self.parent.assertEqual(result.pred_masks.shape[:3], (self.batch_size, 1, 3))\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class Sam3TrackerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Here we also overwrite some of the tests of test_modeling_common.py, as SAM's vision encoder does not use input_ids, inputs_embeds,\n+    attention_mask and seq_length.\n+    \"\"\"\n+\n+    all_model_classes = (Sam3TrackerModel,) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\"feature-extraction\": Sam3TrackerModel, \"mask-generation\": Sam3TrackerModel} if is_torch_available() else {}\n+    )\n+\n+    test_resize_embeddings = False\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = Sam3TrackerModelTester(self)\n+        common_properties = [\"initializer_range\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=Sam3TrackerConfig, has_text_modality=False, common_properties=common_properties\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    @unittest.skip(reason=\"SAM's vision encoder does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    def test_model_get_set_embeddings(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            self.assertIsInstance(model.get_input_embeddings(), (nn.Module))\n+            x = model.get_output_embeddings()\n+            self.assertTrue(x is None or isinstance(x, nn.Linear))\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    # Overriding as Sam3TrackerModel returns vision_attentions\n+    def test_attention_outputs(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.vision_attentions\n+            expected_num_attentions = self.model_tester.num_hidden_layers\n+            self.assertEqual(len(attentions), expected_num_attentions)\n+\n+            # check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.mask_decoder_config.output_attentions = True\n+            config.vision_config.output_attentions = True\n+            config.output_attentions = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.vision_attentions\n+            self.assertEqual(len(attentions), expected_num_attentions)\n+\n+            # Check attention is always last and order is fine\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.vision_attentions\n+            self.assertEqual(len(attentions), expected_num_attentions)\n+\n+    # Override as Sam3TrackerModel has different sub-modules\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        \"\"\"\n+        Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n+        This tests only by looking at layer names, as usually SDPA layers are called \"SDPAAttention\".\n+        In contrast to the above test, this one checks if the \"config._attn_implementation\" is a dict after the model\n+        is loaded, because we manually replicate requested attn implementation on each sub-config when loading.\n+        See https://github.com/huggingface/transformers/pull/32238 for more info\n+\n+        The test tries to cover most general cases of composite models, VLMs with vision and text configs. Any model\n+        that has a different set of sub-configs has to overwrite this test.\n+        \"\"\"\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        if not self._is_composite:\n+            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_sdpa = model_class.from_pretrained(tmpdirname, attn_implementation=\"sdpa\")\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                vision_encoder_sdpa = getattr(model_sdpa, \"vision_encoder\")\n+                mask_decoder_sdpa = getattr(model_sdpa, \"mask_decoder\")\n+\n+                # `None` as it is the requested one which will be assigned to each sub-config\n+                # Sub-model will dispatch to SDPA if it can (checked below that `SDPA` layers are present)\n+                self.assertTrue(mask_decoder_sdpa.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(vision_encoder_sdpa.config._attn_implementation == \"sdpa\")\n+\n+                model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n+                model_eager = model_eager.eval().to(torch_device)\n+                self.assertTrue(getattr(model_eager, \"mask_decoder\").config._attn_implementation == \"eager\")\n+                self.assertTrue(getattr(model_eager, \"vision_encoder\").config._attn_implementation == \"eager\")\n+\n+                for name, submodule in model_eager.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if (\n+                        class_name.endswith(\"Attention\")\n+                        and getattr(submodule, \"config\", None)\n+                        and submodule.config._attn_implementation == \"sdpa\"\n+                    ):\n+                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+    # Override as Sam3TrackerModel doesn't have hidden states\n+    @unittest.skip(reason=\"skip for now (head_size should be a multiple of 8)\")\n+    def flash_attn_inference_equivalence(\n+        self, attn_implementation: str, padding_side: str, atol: float = 4e-2, rtol: float = 4e-2\n+    ):\n+        r\"\"\"\n+        Tests the equivalence between the eager and flash attention implementations.\n+        This test is only for inference and runs with `dtype=torch.bfloat16`.\n+        \"\"\"\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        for model_class in self.all_model_classes:\n+            if (attn_implementation == \"flash_attention_2\" and not model_class._supports_flash_attn_2) or (\n+                attn_implementation == \"flash_attention_3\" and not model_class._supports_flash_attn_3\n+            ):\n+                self.skipTest(f\"{model_class.__name__} does not support {attn_implementation}\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_fa = model_class.from_pretrained(\n+                    tmpdirname, dtype=torch.bfloat16, attn_implementation=attn_implementation\n+                )\n+                model_fa.to(torch_device)\n+\n+                model = model_class.from_pretrained(tmpdirname, dtype=torch.bfloat16)\n+                model.to(torch_device)\n+\n+                dummy_input = inputs_dict[model.main_input_name][:1]\n+                if dummy_input.dtype in [torch.float32, torch.float16]:\n+                    dummy_input = dummy_input.to(torch.bfloat16)\n+\n+                dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n+\n+                if dummy_attention_mask is not None:\n+                    dummy_attention_mask = dummy_attention_mask[:1]\n+                    if padding_side == \"left\":\n+                        dummy_attention_mask[:, 1:] = 1\n+                        dummy_attention_mask[:, :1] = 0\n+                    else:\n+                        dummy_attention_mask[:, :-1] = 1\n+                        dummy_attention_mask[:, -1:] = 0\n+                if model.config.is_encoder_decoder:\n+                    decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", dummy_input)[:1]\n+\n+                    outputs = model(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n+                    outputs_fa = model_fa(dummy_input, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n+                else:\n+                    outputs = model(dummy_input, output_hidden_states=True)\n+                    outputs_fa = model_fa(dummy_input, output_hidden_states=True)\n+\n+                logits = outputs.vision_hidden_states[-1]\n+                logits_fa = outputs_fa.vision_hidden_states[-1]\n+\n+                assert torch.allclose(logits_fa, logits, atol=atol, rtol=rtol)\n+\n+                if model.config.is_encoder_decoder:\n+                    other_inputs = {\n+                        \"decoder_input_ids\": decoder_input_ids,\n+                        \"decoder_attention_mask\": dummy_attention_mask,\n+                        \"output_hidden_states\": True,\n+                    }\n+                    if dummy_attention_mask is not None:\n+                        other_inputs[\"attention_mask\"] = dummy_attention_mask\n+\n+                    outputs = model(dummy_input, **other_inputs)\n+                    outputs_fa = model_fa(dummy_input, **other_inputs)\n+                else:\n+                    other_inputs = {\n+                        \"output_hidden_states\": True,\n+                    }\n+                    if dummy_attention_mask is not None:\n+                        other_inputs[\"attention_mask\"] = dummy_attention_mask\n+\n+                    outputs = model(dummy_input, **other_inputs)\n+                    outputs_fa = model_fa(dummy_input, **other_inputs)\n+\n+                logits = outputs.vision_hidden_states[-1]\n+                logits_fa = outputs_fa.vision_hidden_states[-1]\n+\n+                if padding_side == \"left\":\n+                    assert torch.allclose(logits_fa[1:], logits[1:], atol=atol, rtol=rtol)\n+\n+                    # check with inference + dropout\n+                    model.train()\n+                    _ = model_fa(dummy_input, **other_inputs)\n+                else:\n+                    assert torch.allclose(logits_fa[:-1], logits[:-1], atol=atol, rtol=rtol)\n+\n+    # Override as difference slightly higher than the threshold\n+    def test_batching_equivalence(self, atol=5e-4, rtol=5e-4):\n+        super().test_batching_equivalence(atol=atol, rtol=rtol)\n+\n+    @unittest.skip(reason=\"Sam3TrackerModel does not support training\")\n+    def test_retain_grad_hidden_states_attentions(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Hidden_states is tested in sub modules tests\")\n+    def test_hidden_states_output(self):\n+        pass\n+\n+    @slow\n+    def test_model_from_pretrained(self):\n+        model_name = \"facebook/sam2.1-hiera-tiny\"\n+        model = Sam3TrackerModel.from_pretrained(model_name)\n+        self.assertIsNotNone(model)\n+\n+    def test_sdpa_can_compile_dynamic(self):\n+        self.skipTest(reason=\"SAM2 model can't be compiled dynamic yet\")\n+\n+\n+def prepare_image():\n+    img_url = \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/truck.jpg\"\n+    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+    return raw_image\n+\n+\n+def prepare_groceries_image():\n+    img_url = \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/groceries.jpg\"\n+    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+    return raw_image\n+\n+\n+def prepare_dog_img():\n+    img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/dog-sam.png\"\n+    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+    return raw_image\n+\n+\n+def prepare_video():\n+    video_url = \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/bedroom.mp4\"\n+    raw_video, _ = load_video(video_url)\n+    return raw_video\n+\n+\n+@slow\n+class Sam3TrackerModelIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        super().setUp()\n+        checkpoint_path = \"../sam3-hf-v4-video-full\"\n+        self.model = Sam3TrackerModel.from_pretrained(checkpoint_path).to(torch.float32)\n+        self.processor = Sam3TrackerProcessor.from_pretrained(checkpoint_path)\n+        self.model.to(torch_device)\n+        self.model.eval()\n+\n+    def tearDown(self):\n+        super().tearDown()\n+        gc.collect()\n+        backend_empty_cache(torch_device)\n+\n+    def test_inference_mask_generation_one_point_multimask(self):\n+        raw_image = prepare_image()\n+        input_points = [[[[500, 375]]]]\n+        input_labels = [[[1]]]\n+\n+        inputs = self.processor(\n+            images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\"\n+        ).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = self.model(**inputs)\n+        self.assertEqual(outputs.iou_scores.shape, (1, 1, 3))\n+        self.assertEqual(outputs.pred_masks.shape, (1, 1, 3, 288, 288))\n+        sorted_indices = torch.argsort(outputs.iou_scores.squeeze(), descending=True)\n+        scores = outputs.iou_scores.squeeze()[sorted_indices]\n+        masks_logits = outputs.pred_masks.squeeze()[sorted_indices][0, :3, :3]\n+        torch.testing.assert_close(\n+            scores,\n+            torch.tensor([0.9106, 0.5326, 0.0379]).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+        torch.testing.assert_close(\n+            masks_logits,\n+            torch.tensor(\n+                [\n+                    [-18.9093, -31.1757, -23.6851],\n+                    [-20.3388, -31.0213, -29.8815],\n+                    [-20.7554, -29.4530, -30.1776],\n+                ]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+    def test_inference_mask_generation_one_point_no_multimask(self):\n+        raw_image = prepare_image()\n+        input_points = [[[[500, 375]]]]\n+        input_labels = [[[1]]]\n+\n+        inputs = self.processor(\n+            images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\"\n+        ).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = self.model(**inputs, multimask_output=False)\n+        self.assertEqual(outputs.iou_scores.shape, (1, 1, 1))\n+        self.assertEqual(outputs.pred_masks.shape, (1, 1, 1, 288, 288))\n+        scores = outputs.iou_scores.squeeze((0, 1))\n+        masks_logits = outputs.pred_masks.squeeze((0, 1))[0, :3, :3]\n+        torch.testing.assert_close(scores, torch.tensor([0.9474]).to(torch_device), atol=1e-4, rtol=1e-4)\n+        torch.testing.assert_close(\n+            masks_logits,\n+            torch.tensor(\n+                [\n+                    [-8.1500, -12.3282, -9.6828],\n+                    [-9.0512, -11.6470, -11.6363],\n+                    [-9.2391, -11.9863, -12.4858],\n+                ]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+    def test_inference_mask_generation_batched_images_multi_points(self):\n+        raw_image1 = prepare_image()\n+        raw_image2 = prepare_dog_img()\n+        input_points = [[[[500, 375]]], [[[770, 200], [730, 120]]]]\n+        input_labels = [[[1]], [[1, 0]]]\n+\n+        inputs = self.processor(\n+            images=[raw_image1, raw_image2], input_points=input_points, input_labels=input_labels, return_tensors=\"pt\"\n+        ).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = self.model(**inputs)\n+        self.assertEqual(outputs.iou_scores.shape, (2, 1, 3))\n+        self.assertEqual(outputs.pred_masks.shape, (2, 1, 3, 288, 288))\n+\n+        sorted_indices = torch.argsort(outputs.iou_scores[0].squeeze(), descending=True)\n+        scores1 = outputs.iou_scores[0].squeeze()[sorted_indices]\n+        masks_logits1 = outputs.pred_masks[0].squeeze()[sorted_indices][0, :3, :3]\n+        sorted_indices = torch.argsort(outputs.iou_scores[1].squeeze(), descending=True)\n+        scores2 = outputs.iou_scores[1].squeeze()[sorted_indices]\n+        masks_logits2 = outputs.pred_masks[1].squeeze()[sorted_indices][0, :3, :3]\n+        torch.testing.assert_close(\n+            scores1,\n+            torch.tensor([0.8837, 0.5837, 0.0372]).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+        torch.testing.assert_close(\n+            masks_logits1,\n+            torch.tensor(\n+                [\n+                    [-19.4976, -32.4384, -24.2687],\n+                    [-20.9939, -32.2782, -31.2067],\n+                    [-21.2991, -30.3071, -31.1489],\n+                ]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+        torch.testing.assert_close(\n+            scores2,\n+            torch.tensor([0.7675, 0.7505, 0.5348]).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+        torch.testing.assert_close(\n+            masks_logits2,\n+            torch.tensor(\n+                [\n+                    [-10.3051, -9.9056, -10.5699],\n+                    [-8.8009, -11.1684, -10.7158],\n+                    [-9.6653, -10.9755, -10.3231],\n+                ]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+    def test_inference_mask_generation_batched_images_batched_points_multi_points(self):\n+        raw_image1 = prepare_image()\n+        raw_image2 = prepare_groceries_image()\n+        input_points = [[[[500, 375]], [[650, 750]]], [[[400, 300]], [[630, 300], [550, 300]]]]\n+        input_labels = [[[1], [1]], [[1], [1, 1]]]\n+        inputs = self.processor(\n+            images=[raw_image1, raw_image2], input_points=input_points, input_labels=input_labels, return_tensors=\"pt\"\n+        ).to(torch_device)\n+        with torch.no_grad():\n+            outputs = self.model(**inputs, multimask_output=False)\n+        self.assertEqual(outputs.iou_scores.shape, (2, 2, 1))\n+        self.assertEqual(outputs.pred_masks.shape, (2, 2, 1, 288, 288))\n+        torch.testing.assert_close(\n+            outputs.iou_scores,\n+            torch.tensor([[[0.9370], [0.9425]], [[0.9734], [0.9262]]]).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+        torch.testing.assert_close(\n+            outputs.pred_masks[:, :, :, :2, :2],\n+            torch.tensor(\n+                [\n+                    [\n+                        [[[-7.6936, -11.7077], [-8.6289, -11.0604]]],\n+                        [[[-6.2675, -9.9616], [-6.5427, -9.0548]]],\n+                    ],\n+                    [\n+                        [[[-10.3143, -13.0117], [-10.2967, -12.3099]]],\n+                        [[[-9.1198, -10.1437], [-8.2902, -10.6460]]],\n+                    ],\n+                ]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+    def test_inference_batched_images_batched_boxes(self):\n+        raw_image1 = prepare_image()\n+        raw_image2 = prepare_groceries_image()\n+        input_boxes = [\n+            [[75, 275, 1725, 850], [425, 600, 700, 875], [1375, 550, 1650, 800], [1240, 675, 1400, 750]],\n+            [[450, 170, 520, 350], [350, 190, 450, 350], [500, 170, 580, 350], [580, 170, 640, 350]],\n+        ]\n+        inputs = self.processor(images=[raw_image1, raw_image2], input_boxes=input_boxes, return_tensors=\"pt\").to(\n+            torch_device\n+        )\n+        with torch.no_grad():\n+            outputs = self.model(**inputs, multimask_output=False)\n+        self.assertEqual(outputs.iou_scores.shape, (2, 4, 1))\n+        self.assertEqual(outputs.pred_masks.shape, (2, 4, 1, 288, 288))\n+        torch.testing.assert_close(\n+            outputs.iou_scores,\n+            torch.tensor(\n+                [\n+                    [[0.9862], [0.9666], [0.9588], [0.9331]],\n+                    [[0.9757], [0.9838], [0.9785], [0.9755]],\n+                ]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+        torch.testing.assert_close(\n+            outputs.pred_masks[:, :, :, :2, :2],\n+            torch.tensor(\n+                [\n+                    [\n+                        [[[-12.5972, -19.5327], [-12.4126, -18.3935]]],\n+                        [[[-20.2715, -31.6163], [-22.3341, -27.6888]]],\n+                        [[[-20.9112, -31.4296], [-22.9174, -26.5892]]],\n+                        [[[-23.6995, -37.8614], [-26.3752, -31.1497]]],\n+                    ],\n+                    [\n+                        [[[-21.7436, -29.5702], [-24.3507, -25.5635]]],\n+                        [[[-28.0691, -38.6044], [-31.3014, -33.8172]]],\n+                        [[[-25.3085, -33.9384], [-27.7918, -30.1258]]],\n+                        [[[-26.7339, -36.4405], [-28.8027, -31.8549]]],\n+                    ],\n+                ]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+    def test_inference_mask_generation_from_existing_points_and_mask(self):\n+        raw_image = prepare_image()\n+        input_points = [[[[500, 375]]]]\n+        input_labels = [[[1]]]\n+        original_inputs = self.processor(\n+            images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\"\n+        ).to(torch_device)\n+        with torch.no_grad():\n+            outputs = self.model(**original_inputs)\n+\n+        # best mask to use as input for new points\n+        mask_input = outputs.pred_masks[:, :, torch.argmax(outputs.iou_scores)]\n+\n+        new_input_points = [[[[500, 375], [1125, 625]]]]\n+        new_input_labels = [[[1, 1]]]\n+        inputs = self.processor(\n+            input_points=new_input_points,\n+            input_labels=new_input_labels,\n+            original_sizes=original_inputs[\"original_sizes\"],\n+            return_tensors=\"pt\",\n+        ).to(torch_device)\n+        with torch.no_grad():\n+            outputs = self.model(\n+                **inputs,\n+                input_masks=mask_input,\n+                image_embeddings=outputs.image_embeddings,\n+                multimask_output=False,\n+            )\n+\n+        self.assertEqual(outputs.iou_scores.shape, (1, 1, 1))\n+        self.assertEqual(outputs.pred_masks.shape, (1, 1, 1, 288, 288))\n+        torch.testing.assert_close(\n+            outputs.iou_scores, torch.tensor([[[0.9809]]]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+        torch.testing.assert_close(\n+            outputs.pred_masks[:, :, 0, :3, :3],\n+            torch.tensor(\n+                [\n+                    [\n+                        [\n+                            [-5.3111, -7.4920, -5.5444],\n+                            [-4.7685, -6.3513, -6.2969],\n+                            [-4.8471, -5.1722, -6.5492],\n+                        ]\n+                    ]\n+                ]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+        # with negative point\n+        new_input_points = [[[[500, 375], [1125, 625]]]]\n+        new_input_labels = [[[1, 0]]]\n+        inputs = self.processor(\n+            input_points=new_input_points,\n+            input_labels=new_input_labels,\n+            original_sizes=original_inputs[\"original_sizes\"],\n+            return_tensors=\"pt\",\n+        ).to(torch_device)\n+        with torch.no_grad():\n+            outputs = self.model(\n+                **inputs,\n+                input_masks=mask_input,\n+                image_embeddings=outputs.image_embeddings,\n+                multimask_output=False,\n+            )\n+        self.assertEqual(outputs.iou_scores.shape, (1, 1, 1))\n+        self.assertEqual(outputs.pred_masks.shape, (1, 1, 1, 288, 288))\n+        torch.testing.assert_close(\n+            outputs.iou_scores, torch.tensor([[[0.9625]]]).to(torch_device), atol=1e-4, rtol=1e-4\n+        )\n+        torch.testing.assert_close(\n+            outputs.pred_masks[:, :, 0, :3, :3],\n+            torch.tensor(\n+                [\n+                    [\n+                        [\n+                            [-13.4726, -19.9250, -16.3620],\n+                            [-13.5886, -18.7266, -17.6766],\n+                            [-14.6962, -19.3814, -19.9888],\n+                        ]\n+                    ]\n+                ]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+    def test_dummy_pipeline_generation(self):\n+        generator = pipeline(\"mask-generation\", model=\"../sam3-hf-v4-video-full\", device=torch_device)\n+        raw_image = prepare_image()\n+\n+        _ = generator(raw_image, points_per_batch=64)"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/sam3_tracker_video/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/tests%2Fmodels%2Fsam3_tracker_video%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/tests%2Fmodels%2Fsam3_tracker_video%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam3_tracker_video%2F__init__.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a"
        },
        {
            "sha": "e76d53da77db820b8c12f71bffa2073ff0864f3e",
            "filename": "tests/models/sam3_tracker_video/test_modeling_sam3_tracker_video.py",
            "status": "added",
            "additions": 543,
            "deletions": 0,
            "changes": 543,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/tests%2Fmodels%2Fsam3_tracker_video%2Ftest_modeling_sam3_tracker_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/tests%2Fmodels%2Fsam3_tracker_video%2Ftest_modeling_sam3_tracker_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam3_tracker_video%2Ftest_modeling_sam3_tracker_video.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,543 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch SAM2 model.\"\"\"\n+\n+import gc\n+import unittest\n+\n+import requests\n+\n+from transformers.testing_utils import (\n+    backend_empty_cache,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import is_torch_available, is_vision_available\n+from transformers.video_utils import load_video\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import Sam3TrackerVideoModel, Sam3TrackerVideoProcessor\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+\n+def prepare_image():\n+    img_url = \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/truck.jpg\"\n+    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+    return raw_image\n+\n+\n+def prepare_groceries_image():\n+    img_url = \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/groceries.jpg\"\n+    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+    return raw_image\n+\n+\n+def prepare_dog_img():\n+    img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/dog-sam.png\"\n+    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+    return raw_image\n+\n+\n+def prepare_video():\n+    video_url = \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/bedroom.mp4\"\n+    raw_video, _ = load_video(video_url)\n+    return raw_video\n+\n+\n+@slow\n+class Sam3TrackerVideoModelIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        super().setUp()\n+        self.video_model = Sam3TrackerVideoModel.from_pretrained(\"../sam3-hf-v4-video-full\").to(torch.float32)\n+        self.processor = Sam3TrackerVideoProcessor.from_pretrained(\"../sam3-hf-v4-video-full\")\n+        self.video_model.to(torch_device)\n+        self.video_model.eval()\n+\n+    def tearDown(self):\n+        super().tearDown()\n+        # clean-up as much as possible GPU memory occupied by PyTorch\n+        gc.collect()\n+        backend_empty_cache(torch_device)\n+\n+    def test_inference_mask_generation_video_one_point(self):\n+        raw_video = prepare_video()\n+        inference_session = self.processor.init_video_session(video=raw_video, inference_device=torch_device)\n+        ann_frame_idx = 0  # the frame index we interact with\n+        ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n+\n+        self.processor.add_inputs_to_inference_session(\n+            inference_session=inference_session,\n+            frame_idx=ann_frame_idx,\n+            obj_ids=ann_obj_id,\n+            input_points=[[[[210, 350]]]],\n+            input_labels=[[[1]]],\n+        )\n+        outputs = self.video_model(inference_session=inference_session, frame_idx=ann_frame_idx)\n+        low_res_masks = outputs.pred_masks\n+        self.assertEqual(low_res_masks.shape, (1, 1, 288, 288))\n+        video_res_masks = self.processor.post_process_masks([low_res_masks], [raw_video.shape[-3:-1]], binarize=False)[\n+            0\n+        ]\n+        self.assertEqual(video_res_masks.shape, (1, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        torch.testing.assert_close(\n+            video_res_masks[0, 0, :3, :3],\n+            torch.tensor(\n+                [[-13.5762, -13.5762, -13.7167], [-13.0870, -13.0870, -13.5405], [-12.2173, -12.2173, -13.2273]]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+        # test propagate in video frames\n+        frames = []\n+        for sam2_video_output in self.video_model.propagate_in_video_iterator(\n+            inference_session=inference_session,\n+            max_frame_num_to_track=2,\n+        ):\n+            video_res_masks = self.processor.post_process_masks(\n+                [sam2_video_output.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+            )[0]\n+            frames.append(video_res_masks)\n+        frames = torch.stack(frames, dim=0)\n+        self.assertEqual(frames.shape, (3, 1, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        torch.testing.assert_close(\n+            frames[:3, :, :, :2, :2],\n+            torch.tensor(\n+                [\n+                    [[[[-13.5762, -13.5762], [-13.0870, -13.0870]]]],\n+                    [[[[-19.1203, -19.1203], [-19.5488, -19.5488]]]],\n+                    [[[[-19.9951, -19.9951], [-20.5353, -20.5353]]]],\n+                ],\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+    def test_inference_mask_generation_video_one_point_propagate_in_video_directly(self):\n+        raw_video = prepare_video()\n+        inference_session = self.processor.init_video_session(video=raw_video, inference_device=torch_device)\n+        ann_frame_idx = 0  # the frame index we interact with\n+        ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n+\n+        self.processor.add_inputs_to_inference_session(\n+            inference_session=inference_session,\n+            frame_idx=ann_frame_idx,\n+            obj_ids=ann_obj_id,\n+            input_points=[[[[210, 350]]]],\n+            input_labels=[[[1]]],\n+        )\n+        # test propagate in video frames\n+        frames = []\n+        for sam2_video_output in self.video_model.propagate_in_video_iterator(\n+            inference_session=inference_session,\n+            start_frame_idx=ann_frame_idx,\n+            max_frame_num_to_track=2,\n+        ):\n+            video_res_masks = self.processor.post_process_masks(\n+                [sam2_video_output.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+            )[0]\n+            frames.append(video_res_masks)\n+        frames = torch.stack(frames, dim=0)\n+        self.assertEqual(frames.shape, (3, 1, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        torch.testing.assert_close(\n+            frames[:3, :, :, :2, :2],\n+            torch.tensor(\n+                [\n+                    [[[[-13.5762, -13.5762], [-13.0870, -13.0870]]]],\n+                    [[[[-19.1203, -19.1203], [-19.5488, -19.5488]]]],\n+                    [[[[-19.9951, -19.9951], [-20.5353, -20.5353]]]],\n+                ]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+    def test_inference_mask_generation_video_multi_points(self):\n+        raw_video = prepare_video()\n+        inference_session = self.processor.init_video_session(video=raw_video, inference_device=torch_device)\n+        ann_frame_idx = 0  # the frame index we interact with\n+        ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n+\n+        self.processor.add_inputs_to_inference_session(\n+            inference_session=inference_session,\n+            frame_idx=ann_frame_idx,\n+            obj_ids=ann_obj_id,\n+            input_points=[[[[210, 350], [250, 220]]]],\n+            input_labels=[[[1, 1]]],\n+        )\n+        outputs = self.video_model(inference_session=inference_session, frame_idx=ann_frame_idx)\n+        low_res_masks = outputs.pred_masks\n+        video_res_masks = self.processor.post_process_masks(\n+            [outputs.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+        )[0]\n+        self.assertEqual(low_res_masks.shape, (1, 1, 288, 288))\n+        self.assertEqual(video_res_masks.shape, (1, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        torch.testing.assert_close(\n+            video_res_masks[0, 0, :3, :3],\n+            torch.tensor(\n+                [[-11.9889, -11.9889, -12.2238], [-11.6383, -11.6383, -12.0873], [-11.0150, -11.0150, -11.8446]]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+        # test propagate in video frames\n+        frames = []\n+        for sam2_video_output in self.video_model.propagate_in_video_iterator(\n+            inference_session=inference_session,\n+            start_frame_idx=ann_frame_idx,\n+            max_frame_num_to_track=2,\n+        ):\n+            video_res_masks = self.processor.post_process_masks(\n+                [sam2_video_output.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+            )[0]\n+            frames.append(video_res_masks)\n+        frames = torch.stack(frames, dim=0)\n+        self.assertEqual(frames.shape, (3, 1, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        # higher tolerance due to errors propagating from frame to frame\n+        torch.testing.assert_close(\n+            frames[:3, :, :, :2, :2],\n+            torch.tensor(\n+                [\n+                    [[[[-11.9889, -11.9889], [-11.6383, -11.6383]]]],\n+                    [[[[-20.4502, -20.4502], [-20.6929, -20.6929]]]],\n+                    [[[[-22.0344, -22.0344], [-22.4522, -22.4522]]]],\n+                ]\n+            ).to(torch_device),\n+            atol=1e-2,\n+            rtol=1e-2,\n+        )\n+\n+    def test_inference_mask_generation_video_one_bb(self):\n+        raw_video = prepare_video()\n+        inference_session = self.processor.init_video_session(video=raw_video, inference_device=torch_device)\n+        ann_frame_idx = 0  # the frame index we interact with\n+        ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n+\n+        self.processor.add_inputs_to_inference_session(\n+            inference_session=inference_session,\n+            frame_idx=ann_frame_idx,\n+            obj_ids=ann_obj_id,\n+            input_boxes=[[[300, 0, 500, 400]]],\n+        )\n+        outputs = self.video_model(inference_session=inference_session, frame_idx=ann_frame_idx)\n+        low_res_masks = outputs.pred_masks\n+        video_res_masks = self.processor.post_process_masks(\n+            [outputs.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+        )[0]\n+        self.assertEqual(low_res_masks.shape, (1, 1, 288, 288))\n+        self.assertEqual(video_res_masks.shape, (1, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        torch.testing.assert_close(\n+            video_res_masks[0, 0, :3, :3],\n+            torch.tensor(\n+                [[-17.2589, -17.2589, -17.5130], [-17.2777, -17.2777, -17.9154], [-17.3111, -17.3111, -18.6309]]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+        # test propagate in video frames\n+        frames = []\n+        for sam2_video_output in self.video_model.propagate_in_video_iterator(\n+            inference_session=inference_session,\n+            start_frame_idx=ann_frame_idx,\n+            max_frame_num_to_track=2,\n+        ):\n+            video_res_masks = self.processor.post_process_masks(\n+                [sam2_video_output.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+            )[0]\n+            frames.append(video_res_masks)\n+        frames = torch.stack(frames, dim=0)\n+        self.assertEqual(frames.shape, (3, 1, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        # higher tolerance due to errors propagating from frame to frame\n+        torch.testing.assert_close(\n+            frames[:3, :, :, :2, :2],\n+            torch.tensor(\n+                [\n+                    [[[[-17.2589, -17.2589], [-17.2777, -17.2777]]]],\n+                    [[[[-17.8107, -17.8107], [-18.1581, -18.1581]]]],\n+                    [[[[-17.9432, -17.9432], [-18.4637, -18.4637]]]],\n+                ]\n+            ).to(torch_device),\n+            atol=1e-2,\n+            rtol=1e-2,\n+        )\n+\n+    def test_inference_mask_generation_video_one_point_one_bb(self):\n+        raw_video = prepare_video()\n+        inference_session = self.processor.init_video_session(video=raw_video, inference_device=torch_device)\n+        ann_frame_idx = 0  # the frame index we interact with\n+        ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n+\n+        self.processor.add_inputs_to_inference_session(\n+            inference_session=inference_session,\n+            frame_idx=ann_frame_idx,\n+            obj_ids=ann_obj_id,\n+            input_boxes=[[[300, 0, 500, 400]]],\n+            input_points=[[[[460, 60]]]],\n+            input_labels=[[[1]]],\n+        )\n+        outputs = self.video_model(inference_session=inference_session, frame_idx=ann_frame_idx)\n+        low_res_masks = outputs.pred_masks\n+        video_res_masks = self.processor.post_process_masks(\n+            [outputs.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+        )[0]\n+        self.assertEqual(low_res_masks.shape, (1, 1, 288, 288))\n+        self.assertEqual(video_res_masks.shape, (1, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        torch.testing.assert_close(\n+            video_res_masks[0, 0, :3, :3],\n+            torch.tensor(\n+                [[-14.0206, -14.0206, -14.1225], [-14.0568, -14.0568, -14.4570], [-14.1212, -14.1212, -15.0516]]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+        # test propagate in video frames\n+        frames = []\n+        for sam2_video_output in self.video_model.propagate_in_video_iterator(\n+            inference_session=inference_session,\n+            start_frame_idx=ann_frame_idx,\n+            max_frame_num_to_track=2,\n+        ):\n+            video_res_masks = self.processor.post_process_masks(\n+                [sam2_video_output.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+            )[0]\n+            frames.append(video_res_masks)\n+        frames = torch.stack(frames, dim=0)\n+        self.assertEqual(frames.shape, (3, 1, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        # higher tolerance due to errors propagating from frame to frame\n+        torch.testing.assert_close(\n+            frames[:3, :, :, :2, :2],\n+            torch.tensor(\n+                [\n+                    [[[[-14.0206, -14.0206], [-14.0568, -14.0568]]]],\n+                    [[[[-16.8155, -16.8155], [-17.2954, -17.2954]]]],\n+                    [[[[-16.2909, -16.2909], [-16.8887, -16.8887]]]],\n+                ]\n+            ).to(torch_device),\n+            atol=1e-2,\n+            rtol=1e-2,\n+        )\n+\n+    def test_inference_mask_generation_video_multi_objects_multi_points(self):\n+        raw_video = prepare_video()\n+        inference_session = self.processor.init_video_session(video=raw_video, inference_device=torch_device)\n+        ann_frame_idx = 0  # the frame index we interact with\n+        ann_obj_ids = [2, 3]  # give a unique id to each object we interact with (it can be any integers)\n+\n+        self.processor.add_inputs_to_inference_session(\n+            inference_session=inference_session,\n+            frame_idx=ann_frame_idx,\n+            obj_ids=ann_obj_ids,\n+            input_points=[[[[200, 300], [230, 250], [275, 175]], [[400, 150]]]],\n+            input_labels=[[[1, 1, 0], [1]]],\n+        )\n+        outputs = self.video_model(inference_session=inference_session, frame_idx=ann_frame_idx)\n+        low_res_masks = outputs.pred_masks\n+        video_res_masks = self.processor.post_process_masks(\n+            [outputs.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+        )[0]\n+        self.assertEqual(low_res_masks.shape, (2, 1, 288, 288))\n+        self.assertEqual(video_res_masks.shape, (2, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        torch.testing.assert_close(\n+            video_res_masks[:, 0, :2, :2],  # first object\n+            torch.tensor(\n+                [[[-12.8567, -12.8567], [-13.0618, -13.0618]], [[-12.1054, -12.1054], [-11.6056, -11.6056]]]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+        # test propagate in video frames\n+        frames = []\n+        for sam2_video_output in self.video_model.propagate_in_video_iterator(\n+            inference_session=inference_session,\n+            start_frame_idx=ann_frame_idx,\n+            max_frame_num_to_track=2,\n+        ):\n+            video_res_masks = self.processor.post_process_masks(\n+                [sam2_video_output.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+            )[0]\n+            frames.append(video_res_masks)\n+        frames = torch.stack(frames, dim=0)\n+        self.assertEqual(frames.shape, (3, 2, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        torch.testing.assert_close(\n+            frames[:3, :, :, :2, :2],\n+            torch.tensor(\n+                [\n+                    [[[[-12.8567, -12.8567], [-13.0618, -13.0618]]], [[[-12.1054, -12.1054], [-11.6056, -11.6056]]]],\n+                    [[[[-22.5194, -22.5194], [-22.7973, -22.7973]]], [[[-20.6199, -20.6199], [-21.0607, -21.0607]]]],\n+                    [[[[-25.0871, -25.0871], [-25.6355, -25.6355]]], [[[-19.9508, -19.9508], [-20.4212, -20.4212]]]],\n+                ]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+    def test_inference_mask_generation_video_batched_bb(self):\n+        raw_video = prepare_video()\n+        inference_session = self.processor.init_video_session(video=raw_video, inference_device=torch_device)\n+        ann_frame_idx = 0  # the frame index we interact with\n+        ann_obj_ids = [2, 3]  # give a unique id to each object we interact with (it can be any integers)\n+\n+        self.processor.add_inputs_to_inference_session(\n+            inference_session=inference_session,\n+            frame_idx=ann_frame_idx,\n+            obj_ids=ann_obj_ids,\n+            input_boxes=[[[300, 0, 500, 400], [400, 0, 600, 400]]],\n+        )\n+\n+        frames = []\n+        for sam2_video_output in self.video_model.propagate_in_video_iterator(\n+            inference_session=inference_session,\n+            start_frame_idx=ann_frame_idx,\n+            max_frame_num_to_track=2,\n+        ):\n+            video_res_masks = self.processor.post_process_masks(\n+                [sam2_video_output.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+            )[0]\n+            frames.append(video_res_masks)\n+        frames = torch.stack(frames, dim=0)\n+        self.assertEqual(frames.shape, (3, 2, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        torch.testing.assert_close(\n+            frames[:3, :, :, :2, :2],\n+            torch.tensor(\n+                [\n+                    [[[[-17.2589, -17.2589], [-17.2777, -17.2777]]], [[[-8.5523, -8.5523], [-8.5103, -8.5103]]]],\n+                    [[[[-17.8107, -17.8107], [-18.1581, -18.1581]]], [[[-9.1150, -9.1150], [-9.2327, -9.2327]]]],\n+                    [[[[-17.9432, -17.9432], [-18.4637, -18.4637]]], [[[-10.9026, -10.9026], [-11.0184, -11.0184]]]],\n+                ]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+    def test_inference_propagate_video_from_mask_input(self):\n+        raw_video = prepare_video()\n+        inference_session = self.processor.init_video_session(video=raw_video, inference_device=torch_device)\n+        ann_frame_idx = 0  # the frame index we interact with\n+        ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n+\n+        # get input_mask\n+        self.processor.add_inputs_to_inference_session(\n+            inference_session=inference_session,\n+            frame_idx=ann_frame_idx,\n+            obj_ids=ann_obj_id,\n+            input_points=[[[[210, 350], [250, 220]]]],\n+            input_labels=[[[1, 1]]],\n+        )\n+        sam2_video_output = self.video_model(inference_session=inference_session, frame_idx=ann_frame_idx)\n+\n+        # set mask as input\n+        self.processor.add_inputs_to_inference_session(\n+            inference_session=inference_session,\n+            frame_idx=ann_frame_idx,\n+            obj_ids=ann_obj_id,\n+            input_masks=self.processor.post_process_masks(\n+                [sam2_video_output.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+            )[0],\n+        )\n+        sam2_video_output = self.video_model(inference_session=inference_session, frame_idx=ann_frame_idx)\n+        low_res_masks = sam2_video_output.pred_masks\n+        self.assertEqual(low_res_masks.shape, (1, 1, 288, 288))\n+        video_res_masks = self.processor.post_process_masks(\n+            [sam2_video_output.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+        )[0]\n+        self.assertEqual(video_res_masks.shape, (1, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        torch.testing.assert_close(\n+            video_res_masks[0, 0, :3, :3],\n+            torch.tensor(\n+                [[-10.0000, -10.0000, -10.0000], [-10.0000, -10.0000, -10.0000], [-10.0000, -10.0000, -10.0000]]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+        # test propagate in video frames\n+        frames = []\n+        for sam2_video_output in self.video_model.propagate_in_video_iterator(\n+            inference_session=inference_session,\n+            start_frame_idx=ann_frame_idx,\n+            max_frame_num_to_track=2,\n+        ):\n+            video_res_masks = self.processor.post_process_masks(\n+                [sam2_video_output.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+            )[0]\n+            frames.append(video_res_masks)\n+        frames = torch.stack(frames, dim=0)\n+        self.assertEqual(frames.shape, (3, 1, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        torch.testing.assert_close(\n+            frames[:3, :, :, :2, :2],\n+            torch.tensor(\n+                [\n+                    [[[[-10.0000, -10.0000], [-10.0000, -10.0000]]]],\n+                    [[[[-21.3700, -21.3700], [-21.7191, -21.7191]]]],\n+                    [[[[-22.2242, -22.2242], [-22.7148, -22.7148]]]],\n+                ],\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n+    def test_inference_propagate_on_streamed_video(self):\n+        raw_video = prepare_video()\n+\n+        inference_session = self.processor.init_video_session(inference_device=torch_device)\n+        video_res_masks = []\n+        max_frame_num_to_track = 3\n+        for frame_idx, frame in enumerate(raw_video):\n+            if frame_idx >= max_frame_num_to_track:\n+                break\n+            inputs = self.processor(images=frame, device=torch_device, return_tensors=\"pt\")\n+            if frame_idx == 0:\n+                self.processor.add_inputs_to_inference_session(\n+                    inference_session,\n+                    frame_idx=0,\n+                    obj_ids=1,\n+                    input_points=[[[[210, 350], [250, 220]]]],\n+                    input_labels=[[[1, 1]]],\n+                    original_size=inputs.original_sizes[0],\n+                )\n+            sam2_video_output = self.video_model(inference_session=inference_session, frame=inputs.pixel_values[0])\n+            video_res_masks.append(\n+                self.processor.post_process_masks(\n+                    [sam2_video_output.pred_masks], inputs.original_sizes, binarize=False\n+                )[0]\n+            )\n+\n+        video_res_masks = torch.stack(video_res_masks, dim=0)\n+        self.assertEqual(\n+            video_res_masks.shape, (max_frame_num_to_track, 1, 1, raw_video.shape[-3], raw_video.shape[-2])\n+        )\n+        # higher tolerance due to errors propagating from frame to frame\n+        torch.testing.assert_close(\n+            video_res_masks[:3, :, :, :2, :2],\n+            torch.tensor(\n+                [\n+                    [[[[-11.9889, -11.9889], [-11.6383, -11.6383]]]],\n+                    [[[[-20.4502, -20.4502], [-20.6929, -20.6929]]]],\n+                    [[[[-22.0344, -22.0344], [-22.4522, -22.4522]]]],\n+                ]\n+            ).to(torch_device),\n+            atol=1e-2,\n+            rtol=1e-2,\n+        )"
        },
        {
            "sha": "0466ba607232b37b02b034b76ea900b1073c4134",
            "filename": "tests/models/sam3_video/__init__.py",
            "status": "added",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/tests%2Fmodels%2Fsam3_video%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/tests%2Fmodels%2Fsam3_video%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam3_video%2F__init__.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,15 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+"
        },
        {
            "sha": "64074a51518a95bd05edf9a895e04e328f56db27",
            "filename": "tests/models/sam3_video/test_modeling_sam3_video.py",
            "status": "added",
            "additions": 475,
            "deletions": 0,
            "changes": 475,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/tests%2Fmodels%2Fsam3_video%2Ftest_modeling_sam3_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/tests%2Fmodels%2Fsam3_video%2Ftest_modeling_sam3_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam3_video%2Ftest_modeling_sam3_video.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -0,0 +1,475 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch SAM3 Video model.\"\"\"\n+\n+import gc\n+import unittest\n+\n+from transformers.testing_utils import (\n+    backend_empty_cache,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import is_torch_available\n+from transformers.video_utils import load_video\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import Sam3VideoModel, Sam3VideoProcessor\n+\n+\n+def prepare_video():\n+    video_url = \"https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/bedroom.mp4\"\n+    raw_video, _ = load_video(video_url)\n+    return raw_video\n+\n+\n+@slow\n+class Sam3VideoModelIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        super().setUp()\n+        checkpoint_path = \"../sam3-hf-v4-video-full\"\n+        self.video_model = Sam3VideoModel.from_pretrained(checkpoint_path).to(torch.float32)\n+        self.processor = Sam3VideoProcessor.from_pretrained(checkpoint_path)\n+        self.video_model.to(torch_device)\n+        self.video_model.eval()\n+\n+    def tearDown(self):\n+        super().tearDown()\n+        # clean-up as much as possible GPU memory occupied by PyTorch\n+        gc.collect()\n+        backend_empty_cache(torch_device)\n+\n+    def test_inference_video_propagate_with_text_prompt(self):\n+        raw_video = prepare_video()\n+        inference_session = self.processor.init_video_session(\n+            video=raw_video,\n+            inference_device=torch_device,\n+            processing_device=\"cpu\",\n+            video_storage_device=\"cpu\",\n+        )\n+\n+        # Add text prompt\n+        text = \"person\"\n+        inference_session = self.processor.add_text_prompt(\n+            inference_session=inference_session,\n+            text=text,\n+        )\n+\n+        # Propagate through video frames\n+        outputs_per_frame = {}\n+        model_outputs_per_frame = {}\n+        for model_outputs in self.video_model.propagate_in_video_iterator(\n+            inference_session=inference_session,\n+            max_frame_num_to_track=3,\n+        ):\n+            processed_outputs = self.processor.postprocess_outputs(inference_session, model_outputs)\n+            outputs_per_frame[model_outputs.frame_idx] = processed_outputs\n+            model_outputs_per_frame[model_outputs.frame_idx] = model_outputs\n+\n+        # Check we processed the expected number of frames\n+        self.assertGreaterEqual(len(outputs_per_frame), 1)\n+        self.assertLessEqual(len(outputs_per_frame), 4)  # frame 0 + up to 3 more\n+\n+        # Check output structure for each frame\n+        for processed_outputs in outputs_per_frame.values():\n+            self.assertIn(\"object_ids\", processed_outputs)\n+            self.assertIn(\"scores\", processed_outputs)\n+            self.assertIn(\"boxes\", processed_outputs)\n+            self.assertIn(\"masks\", processed_outputs)\n+\n+            num_objects = len(processed_outputs[\"object_ids\"])\n+            if num_objects > 0:\n+                self.assertEqual(processed_outputs[\"scores\"].shape, (num_objects,))\n+                self.assertEqual(processed_outputs[\"boxes\"].shape, (num_objects, 4))\n+                self.assertEqual(\n+                    processed_outputs[\"masks\"].shape, (num_objects, raw_video.shape[-3], raw_video.shape[-2])\n+                )\n+                # Check boxes are in XYXY format (absolute coordinates)\n+                boxes = processed_outputs[\"boxes\"]\n+                self.assertTrue(torch.all(boxes[:, 2] >= boxes[:, 0]))  # x2 >= x1\n+                self.assertTrue(torch.all(boxes[:, 3] >= boxes[:, 1]))  # y2 >= y1\n+\n+        # Check numeric values for first frame\n+        if len(outputs_per_frame) > 0:\n+            first_frame_idx = min(outputs_per_frame.keys())\n+            first_outputs = outputs_per_frame[first_frame_idx]\n+            num_objects = len(first_outputs[\"object_ids\"])\n+            if num_objects > 0:\n+                # Move outputs to CPU for comparison (postprocess_outputs may return CPU tensors)\n+                object_ids = (\n+                    first_outputs[\"object_ids\"].cpu()\n+                    if isinstance(first_outputs[\"object_ids\"], torch.Tensor)\n+                    else torch.tensor(first_outputs[\"object_ids\"])\n+                )\n+                scores = (\n+                    first_outputs[\"scores\"].cpu()\n+                    if isinstance(first_outputs[\"scores\"], torch.Tensor)\n+                    else torch.tensor(first_outputs[\"scores\"])\n+                )\n+                boxes = (\n+                    first_outputs[\"boxes\"].cpu()\n+                    if isinstance(first_outputs[\"boxes\"], torch.Tensor)\n+                    else torch.tensor(first_outputs[\"boxes\"])\n+                )\n+                masks = (\n+                    first_outputs[\"masks\"].cpu()\n+                    if isinstance(first_outputs[\"masks\"], torch.Tensor)\n+                    else torch.tensor(first_outputs[\"masks\"])\n+                )\n+\n+                torch.testing.assert_close(\n+                    object_ids,\n+                    torch.tensor([0, 1], dtype=torch.int64),\n+                )\n+                torch.testing.assert_close(\n+                    scores,\n+                    torch.tensor([0.968647837638855, 0.9736108779907227], dtype=torch.float32),\n+                    atol=1e-4,\n+                    rtol=1e-4,\n+                )\n+                torch.testing.assert_close(\n+                    boxes[0],\n+                    torch.tensor([146.0, 135.0, 291.0, 404.0], dtype=torch.float32),\n+                    atol=1e-4,\n+                    rtol=1e-4,\n+                )\n+                torch.testing.assert_close(\n+                    masks[0, :3, :3].float(),\n+                    torch.tensor([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], dtype=torch.float32),\n+                    atol=1e-4,\n+                    rtol=1e-4,\n+                )\n+\n+        # Check raw model_outputs mask values for first frame\n+        if len(model_outputs_per_frame) > 0:\n+            first_frame_idx = min(model_outputs_per_frame.keys())\n+            first_model_outputs = model_outputs_per_frame[first_frame_idx]\n+            num_objects = len(first_model_outputs.object_ids)\n+            if num_objects > 0:\n+                # Check raw mask from model_outputs (low-resolution, before post-processing)\n+                first_obj_id = first_model_outputs.object_ids[0]\n+                raw_mask = first_model_outputs.obj_id_to_mask[first_obj_id].cpu()\n+                torch.testing.assert_close(\n+                    raw_mask[:1, :3, :3].float(),\n+                    torch.tensor(\n+                        [\n+                            [\n+                                [-2.952317476272583, -5.94632625579834, -7.991223335266113],\n+                                [-6.916913986206055, -10.058566093444824, -11.114638328552246],\n+                                [-8.195585250854492, -9.787644386291504, -10.39273452758789],\n+                            ]\n+                        ],\n+                        dtype=torch.float32,\n+                    ),\n+                    atol=5e-3,  # Higher tolerance for raw logits\n+                    rtol=5e-3,\n+                )\n+\n+        # Check numeric values for last frame (to verify propagation consistency)\n+        if len(outputs_per_frame) > 1:\n+            last_frame_idx = max(outputs_per_frame.keys())\n+            last_outputs = outputs_per_frame[last_frame_idx]\n+            num_objects = len(last_outputs[\"object_ids\"])\n+            if num_objects > 0:\n+                # Move outputs to CPU for comparison\n+                object_ids = (\n+                    last_outputs[\"object_ids\"].cpu()\n+                    if isinstance(last_outputs[\"object_ids\"], torch.Tensor)\n+                    else torch.tensor(last_outputs[\"object_ids\"])\n+                )\n+                scores = (\n+                    last_outputs[\"scores\"].cpu()\n+                    if isinstance(last_outputs[\"scores\"], torch.Tensor)\n+                    else torch.tensor(last_outputs[\"scores\"])\n+                )\n+                boxes = (\n+                    last_outputs[\"boxes\"].cpu()\n+                    if isinstance(last_outputs[\"boxes\"], torch.Tensor)\n+                    else torch.tensor(last_outputs[\"boxes\"])\n+                )\n+                masks = (\n+                    last_outputs[\"masks\"].cpu()\n+                    if isinstance(last_outputs[\"masks\"], torch.Tensor)\n+                    else torch.tensor(last_outputs[\"masks\"])\n+                )\n+\n+                torch.testing.assert_close(\n+                    object_ids,\n+                    torch.tensor([0, 1], dtype=torch.int64),\n+                )\n+                torch.testing.assert_close(\n+                    scores,\n+                    torch.tensor([0.968647837638855, 0.9736108779907227], dtype=torch.float32),\n+                    atol=1e-4,\n+                    rtol=1e-4,\n+                )\n+                torch.testing.assert_close(\n+                    boxes[0],\n+                    torch.tensor([157.0, 116.0, 295.0, 382.0], dtype=torch.float32),\n+                    atol=1e-4,\n+                    rtol=1e-4,\n+                )\n+                torch.testing.assert_close(\n+                    masks[0, :3, :3].float(),\n+                    torch.tensor([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], dtype=torch.float32),\n+                    atol=1e-4,\n+                    rtol=1e-4,\n+                )\n+\n+        # Check raw model_outputs mask values for last frame\n+        if len(model_outputs_per_frame) > 1:\n+            last_frame_idx = max(model_outputs_per_frame.keys())\n+            last_model_outputs = model_outputs_per_frame[last_frame_idx]\n+            num_objects = len(last_model_outputs.object_ids)\n+            if num_objects > 0:\n+                # Check raw mask from model_outputs (low-resolution, before post-processing)\n+                first_obj_id = last_model_outputs.object_ids[0]\n+                raw_mask = last_model_outputs.obj_id_to_mask[first_obj_id].cpu()\n+                torch.testing.assert_close(\n+                    raw_mask[:1, :3, :3].float(),\n+                    torch.tensor(\n+                        [\n+                            [\n+                                [-23.023313522338867, -27.02887535095215, -22.29985237121582],\n+                                [-24.373233795166016, -31.428438186645508, -24.268810272216797],\n+                                [-24.550016403198242, -32.607383728027344, -26.500947952270508],\n+                            ]\n+                        ],\n+                        dtype=torch.float32,\n+                    ),\n+                    atol=5e-3,  # Higher tolerance for raw logits\n+                    rtol=5e-3,\n+                )\n+\n+    def test_inference_video_streaming_with_text_prompt(self):\n+        raw_video = prepare_video()\n+\n+        # Initialize session for streaming (no video provided)\n+        inference_session = self.processor.init_video_session(\n+            inference_device=torch_device,\n+            processing_device=\"cpu\",\n+            video_storage_device=\"cpu\",\n+        )\n+\n+        # Add text prompt\n+        text = \"person\"\n+        inference_session = self.processor.add_text_prompt(\n+            inference_session=inference_session,\n+            text=text,\n+        )\n+\n+        # Process frames one by one (streaming mode)\n+        outputs_per_frame = {}\n+        model_outputs_per_frame = {}\n+        max_frame_num_to_track = 3\n+        for frame_idx, frame in enumerate(raw_video):\n+            if frame_idx >= max_frame_num_to_track:\n+                break\n+\n+            # Process frame using processor\n+            inputs = self.processor(images=frame, device=torch_device, return_tensors=\"pt\")\n+\n+            # Process frame using streaming inference\n+            model_outputs = self.video_model(\n+                inference_session=inference_session,\n+                frame=inputs.pixel_values[0],  # Provide processed frame - this enables streaming mode\n+                reverse=False,\n+            )\n+\n+            # Post-process outputs with original_sizes for proper resolution handling\n+            processed_outputs = self.processor.postprocess_outputs(\n+                inference_session,\n+                model_outputs,\n+                original_sizes=inputs.original_sizes,  # Required for streaming inference\n+            )\n+            outputs_per_frame[frame_idx] = processed_outputs\n+            model_outputs_per_frame[frame_idx] = model_outputs\n+\n+        # Check we processed the expected number of frames\n+        self.assertEqual(len(outputs_per_frame), max_frame_num_to_track)\n+\n+        # Check output structure for each frame\n+        for frame_idx, processed_outputs in outputs_per_frame.items():\n+            self.assertIn(\"object_ids\", processed_outputs)\n+            self.assertIn(\"scores\", processed_outputs)\n+            self.assertIn(\"boxes\", processed_outputs)\n+            self.assertIn(\"masks\", processed_outputs)\n+\n+            num_objects = len(processed_outputs[\"object_ids\"])\n+            if num_objects > 0:\n+                self.assertEqual(processed_outputs[\"scores\"].shape, (num_objects,))\n+                self.assertEqual(processed_outputs[\"boxes\"].shape, (num_objects, 4))\n+                # For streaming, masks should be at original frame resolution\n+                H_orig, W_orig = raw_video[frame_idx].shape[0], raw_video[frame_idx].shape[1]\n+                self.assertEqual(processed_outputs[\"masks\"].shape, (num_objects, H_orig, W_orig))\n+                # Check boxes are in XYXY format (absolute coordinates)\n+                boxes = processed_outputs[\"boxes\"]\n+                self.assertTrue(torch.all(boxes[:, 2] >= boxes[:, 0]))  # x2 >= x1\n+                self.assertTrue(torch.all(boxes[:, 3] >= boxes[:, 1]))  # y2 >= y1\n+\n+        # Check numeric values for first frame\n+        if len(outputs_per_frame) > 0:\n+            first_frame_idx = min(outputs_per_frame.keys())\n+            first_outputs = outputs_per_frame[first_frame_idx]\n+            num_objects = len(first_outputs[\"object_ids\"])\n+            if num_objects > 0:\n+                # Move outputs to CPU for comparison (postprocess_outputs may return CPU tensors)\n+                object_ids = (\n+                    first_outputs[\"object_ids\"].cpu()\n+                    if isinstance(first_outputs[\"object_ids\"], torch.Tensor)\n+                    else torch.tensor(first_outputs[\"object_ids\"])\n+                )\n+                scores = (\n+                    first_outputs[\"scores\"].cpu()\n+                    if isinstance(first_outputs[\"scores\"], torch.Tensor)\n+                    else torch.tensor(first_outputs[\"scores\"])\n+                )\n+                boxes = (\n+                    first_outputs[\"boxes\"].cpu()\n+                    if isinstance(first_outputs[\"boxes\"], torch.Tensor)\n+                    else torch.tensor(first_outputs[\"boxes\"])\n+                )\n+                masks = (\n+                    first_outputs[\"masks\"].cpu()\n+                    if isinstance(first_outputs[\"masks\"], torch.Tensor)\n+                    else torch.tensor(first_outputs[\"masks\"])\n+                )\n+\n+                torch.testing.assert_close(\n+                    object_ids,\n+                    torch.tensor([0, 1], dtype=torch.int64),\n+                )\n+                torch.testing.assert_close(\n+                    scores,\n+                    torch.tensor([0.9683944582939148, 0.9740181565284729], dtype=torch.float32),\n+                    atol=1e-4,\n+                    rtol=1e-4,\n+                )\n+                torch.testing.assert_close(\n+                    boxes[0],\n+                    torch.tensor([146.0, 135.0, 291.0, 404.0], dtype=torch.float32),\n+                    atol=1e-4,\n+                    rtol=1e-4,\n+                )\n+                torch.testing.assert_close(\n+                    masks[0, :3, :3].float(),\n+                    torch.tensor([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], dtype=torch.float32),\n+                    atol=1e-4,\n+                    rtol=1e-4,\n+                )\n+\n+        # Check raw model_outputs mask values for first frame\n+        if len(model_outputs_per_frame) > 0:\n+            first_frame_idx = min(model_outputs_per_frame.keys())\n+            first_model_outputs = model_outputs_per_frame[first_frame_idx]\n+            num_objects = len(first_model_outputs.object_ids)\n+            if num_objects > 0:\n+                # Check raw mask from model_outputs (low-resolution, before post-processing)\n+                first_obj_id = first_model_outputs.object_ids[0]\n+                raw_mask = first_model_outputs.obj_id_to_mask[first_obj_id].cpu()\n+                torch.testing.assert_close(\n+                    raw_mask[:1, :3, :3].float(),\n+                    torch.tensor(\n+                        [\n+                            [\n+                                [-2.987567901611328, -5.944897651672363, -7.973854064941406],\n+                                [-7.017378330230713, -10.088018417358398, -11.089308738708496],\n+                                [-8.274458885192871, -9.851463317871094, -10.428947448730469],\n+                            ]\n+                        ],\n+                        dtype=torch.float32,\n+                    ),\n+                    atol=5e-3,  # Higher tolerance for raw logits\n+                    rtol=5e-3,\n+                )\n+\n+        # Check numeric values for last frame (to verify propagation consistency)\n+        if len(outputs_per_frame) > 1:\n+            last_frame_idx = max(outputs_per_frame.keys())\n+            last_outputs = outputs_per_frame[last_frame_idx]\n+            num_objects = len(last_outputs[\"object_ids\"])\n+            if num_objects > 0:\n+                # Move outputs to CPU for comparison\n+                object_ids = (\n+                    last_outputs[\"object_ids\"].cpu()\n+                    if isinstance(last_outputs[\"object_ids\"], torch.Tensor)\n+                    else torch.tensor(last_outputs[\"object_ids\"])\n+                )\n+                scores = (\n+                    last_outputs[\"scores\"].cpu()\n+                    if isinstance(last_outputs[\"scores\"], torch.Tensor)\n+                    else torch.tensor(last_outputs[\"scores\"])\n+                )\n+                boxes = (\n+                    last_outputs[\"boxes\"].cpu()\n+                    if isinstance(last_outputs[\"boxes\"], torch.Tensor)\n+                    else torch.tensor(last_outputs[\"boxes\"])\n+                )\n+                masks = (\n+                    last_outputs[\"masks\"].cpu()\n+                    if isinstance(last_outputs[\"masks\"], torch.Tensor)\n+                    else torch.tensor(last_outputs[\"masks\"])\n+                )\n+\n+                torch.testing.assert_close(\n+                    object_ids,\n+                    torch.tensor([0, 1], dtype=torch.int64),\n+                )\n+                torch.testing.assert_close(\n+                    scores,\n+                    torch.tensor([0.9683944582939148, 0.9740181565284729], dtype=torch.float32),\n+                    atol=1e-4,\n+                    rtol=1e-4,\n+                )\n+                torch.testing.assert_close(\n+                    boxes[0],\n+                    torch.tensor([154.0, 117.0, 294.0, 395.0], dtype=torch.float32),\n+                    atol=1e-4,\n+                    rtol=1e-4,\n+                )\n+                torch.testing.assert_close(\n+                    masks[0, :3, :3].float(),\n+                    torch.tensor([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], dtype=torch.float32),\n+                    atol=1e-4,\n+                    rtol=1e-4,\n+                )\n+\n+        # Check raw model_outputs mask values for last frame\n+        if len(model_outputs_per_frame) > 1:\n+            last_frame_idx = max(model_outputs_per_frame.keys())\n+            last_model_outputs = model_outputs_per_frame[last_frame_idx]\n+            num_objects = len(last_model_outputs.object_ids)\n+            if num_objects > 0:\n+                # Check raw mask from model_outputs (low-resolution, before post-processing)\n+                first_obj_id = last_model_outputs.object_ids[0]\n+                raw_mask = last_model_outputs.obj_id_to_mask[first_obj_id].cpu()\n+                torch.testing.assert_close(\n+                    raw_mask[:1, :3, :3].float(),\n+                    torch.tensor(\n+                        [\n+                            [\n+                                [-23.935535430908203, -27.967025756835938, -23.519914627075195],\n+                                [-25.742399215698242, -32.65046310424805, -24.71213150024414],\n+                                [-25.263212203979492, -33.807132720947266, -27.463823318481445],\n+                            ]\n+                        ],\n+                        dtype=torch.float32,\n+                    ),\n+                    atol=5e-3,  # Higher tolerance for raw logits\n+                    rtol=5e-3,\n+                )"
        },
        {
            "sha": "1864e928b75285468d5047149f3b5cb2bd67b66d",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -124,6 +124,8 @@\n     \"AutoformerConfig\": [\"num_static_real_features\", \"num_time_features\"],\n     # used internally to calculate `mlp_dim`\n     \"SamVisionConfig\": [\"mlp_ratio\"],\n+    # used by sam3 video, kept here for consistency with sam2\n+    \"Sam3VisionConfig\": [\"backbone_feature_sizes\"],\n     # used internally to calculate `mlp_dim`\n     \"SamHQVisionConfig\": [\"mlp_ratio\"],\n     # For (head) training, but so far not implemented"
        },
        {
            "sha": "58ff56484f2760d775024f09d182129c40584471",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fba72361e8e0e865d569f7cd15e5aa50b41ac9a/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=1fba72361e8e0e865d569f7cd15e5aa50b41ac9a",
            "patch": "@@ -142,7 +142,10 @@\n         \"BarkCausalModel\",  # Building part of bigger (tested) model.\n         \"BarkModel\",  # Does not have a forward signature - generation tested with integration tests.\n         \"Sam2HieraDetModel\",  # Building part of bigger (tested) model.\n+        \"Sam3TrackerVideoModel\",  # Partly tested in Sam3TrackerModel, not regular model.\n         \"Sam2VideoModel\",  # Partly tested in Sam2Model, not regular model.\n+        \"Sam3ViTModel\",  # Building part of bigger (tested) model.\n+        \"Sam3VideoModel\",  # Partly tested in Sam3Model, not regular model.\n         \"EdgeTamVisionModel\",  # Building part of bigger (tested) model.\n         \"EdgeTamVideoModel\",  # Partly tested in EdgeTamModel, not regular model.\n         \"SeamlessM4TTextToUnitModel\",  # Building part of bigger (tested) model.\n@@ -212,6 +215,8 @@\n     \"models/shieldgemma2/test_modeling_shieldgemma2.py\",\n     \"models/llama4/test_modeling_llama4.py\",\n     \"models/sam2_video/test_modeling_sam2_video.py\",\n+    \"models/sam3_tracker_video/test_modeling_sam3_tracker_video.py\",\n+    \"models/sam3_video/test_modeling_sam3_video.py\",\n     \"models/edgetam_video/test_modeling_edgetam_video.py\",\n ]\n "
        }
    ],
    "stats": {
        "total": 20549,
        "additions": 20472,
        "deletions": 77
    }
}