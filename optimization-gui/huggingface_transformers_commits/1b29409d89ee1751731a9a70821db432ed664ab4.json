{
    "author": "arkhamHack",
    "message": "feat: updated model card for qwen_2.5_vl (#37099)\n\n* feat: updated model card for qwen_2.5_vl\n\n* applied suggested change 1\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* applied suggested change 2\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* applied suggested change 3\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* fix: made requested changes for quantization and notes\n\n* suggeested model card change 4\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* updated model card wiht suggested change 5\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* updated model card wiht suggested change 6\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* updated model card wiht suggested change 7\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* feat: applied requested changes\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "1b29409d89ee1751731a9a70821db432ed664ab4",
    "files": [
        {
            "sha": "2d38fe82e614ed64df918330b1334a2103fded90",
            "filename": "docs/source/en/model_doc/qwen2_5_vl.md",
            "status": "modified",
            "additions": 167,
            "deletions": 205,
            "changes": 372,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b29409d89ee1751731a9a70821db432ed664ab4/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b29409d89ee1751731a9a70821db432ed664ab4/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md?ref=1b29409d89ee1751731a9a70821db432ed664ab4",
            "patch": "@@ -14,258 +14,220 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Qwen2.5-VL\n-\n-<div class=\"flex flex-wrap space-x-1\">\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n-<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">    </div>\n </div>\n \n-## Overview\n-\n-The [Qwen2.5-VL](https://qwenlm.github.io/blog/qwen2_5-vl/) model is an update to [Qwen2-VL](https://arxiv.org/abs/2409.12191) from Qwen team, Alibaba Group. \n+# Qwen2.5-VL\n \n-The abstract from this update is the following:\n+[Qwen2.5-VL](https://huggingface.co/papers/2502.13923) is a multimodal vision-language model, available in 3B, 7B, and 72B parameters, pretrained on 4.1T tokens. The model introduces window attention in the ViT encoder to accelerate training and inference, dynamic FPS sampling on the spatial and temporal dimensions for better video understanding across different sampling rates, and an upgraded MRoPE (multi-resolutional rotary positional encoding) mechanism to better capture and learn temporal dynamics.\n \n-*Qwen2.5-VL marks a major step forward from Qwen2-VL, built upon the latest Qwen2.5 LLM. We've accelerated training and testing through the strategic implementation of window attention within the ViT. The ViT architecture itself has been refined with SwiGLU and RMSNorm, aligning it more closely with the LLM's structure. A key innovation is the expansion of native dynamic resolution to encompass the temporal dimension, in addition to spatial aspects. Furthermore, we've upgraded MRoPE, incorporating absolute time alignment on the time axis to allow the model to effectively capture temporal dynamics, regardless of frame rate, leading to superior video understanding.*\n \n-## Usage example\n+You can find all the original Qwen2.5-VL checkpoints under the [Qwen2.5-VL](https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5) collection.\n \n-### Single Media inference\n+> [!TIP]\n+> Click on the Qwen2.5-VL models in the right sidebar for more examples of how to apply Qwen2.5-VL to different vision and language tasks.\n \n-The model can accept both images and videos as input. Here's an example code for inference.\n+The example below demonstrates how to generate text based on an image with [`Pipeline`] or the [`AutoModel`] class.\n \n-```python\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n \n+```py\n import torch\n-from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n+from transformers import pipeline\n+pipe = pipeline(\n+    task=\"image-text-to-text\",\n+    model=\"Qwen/Qwen2.5-VL-7B-Instruct\",\n+    device=0,\n+    torch_dtype=torch.bfloat16\n+)\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\n+                \"type\": \"image\",\n+                \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+            },\n+            { \"type\": \"text\", \"text\": \"Describe this image.\"},\n+        ]\n+    }\n+]\n+pipe(text=messages,max_new_tokens=20, return_full_text=False)\n \n-# Load the model in half-precision on the available device(s)\n-model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", device_map=\"auto\")\n-processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n+```\n+</hfoption>\n+\n+<hfoption id=\"AutoModel\">\n \n+```py\n+import torch\n+from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n \n-conversation = [\n+model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n+    \"Qwen/Qwen2.5-VL-7B-Instruct\",\n+    torch_dtype=torch.float16,\n+    device_map=\"auto\",\n+    attn_implementation=\"sdpa\"\n+)\n+processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n+messages = [\n     {\n         \"role\":\"user\",\n         \"content\":[\n             {\n                 \"type\":\"image\",\n-                \"url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n+                \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n             },\n             {\n                 \"type\":\"text\",\n                 \"text\":\"Describe this image.\"\n             }\n         ]\n     }\n-]\n-\n-inputs = processor.apply_chat_template(\n-    conversation,\n-    add_generation_prompt=True,\n-    tokenize=True,\n-    return_dict=True,\n-    return_tensors=\"pt\"\n-).to(model.device)\n-\n \n-# Inference: Generation of the output\n-output_ids = model.generate(**inputs, max_new_tokens=128)\n-generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n-output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n-print(output_text)\n-\n-# Video\n-conversation = [\n-    {\n-        \"role\": \"user\",\n-        \"content\": [\n-            {\"type\": \"video\", \"path\": \"/path/to/video.mp4\"},\n-            {\"type\": \"text\", \"text\": \"What happened in the video?\"},\n-        ],\n-    }\n ]\n \n inputs = processor.apply_chat_template(\n-    conversation,\n-    video_fps=1,\n+    messages,\n     add_generation_prompt=True,\n     tokenize=True,\n     return_dict=True,\n     return_tensors=\"pt\"\n-).to(model.device)\n+).to(\"cuda\")\n \n-# Inference: Generation of the output\n-output_ids = model.generate(**inputs, max_new_tokens=128)\n-generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n-output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n-print(output_text)\n-```\n-\n-### Batch Mixed Media Inference\n-\n-The model can batch inputs composed of mixed samples of various types such as images, videos, and text. Here is an example.\n-\n-```python\n-# Conversation for the first image\n-conversation1 = [\n-    {\n-        \"role\": \"user\",\n-        \"content\": [\n-            {\"type\": \"image\", \"path\": \"/path/to/image1.jpg\"},\n-            {\"type\": \"text\", \"text\": \"Describe this image.\"}\n-        ]\n-    }\n-]\n-\n-# Conversation with two images\n-conversation2 = [\n-    {\n-        \"role\": \"user\",\n-        \"content\": [\n-            {\"type\": \"image\", \"path\": \"/path/to/image2.jpg\"},\n-            {\"type\": \"image\", \"path\": \"/path/to/image3.jpg\"},\n-            {\"type\": \"text\", \"text\": \"What is written in the pictures?\"}\n-        ]\n-    }\n-]\n-\n-# Conversation with pure text\n-conversation3 = [\n-    {\n-        \"role\": \"user\",\n-        \"content\": \"who are you?\"\n-    }\n+generated_ids = model.generate(**inputs, max_new_tokens=128)\n+generated_ids_trimmed = [\n+            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n ]\n-\n-\n-# Conversation with mixed midia\n-conversation4 = [\n-    {\n-        \"role\": \"user\",\n-        \"content\": [\n-            {\"type\": \"image\", \"path\": \"/path/to/image3.jpg\"},\n-            {\"type\": \"image\", \"path\": \"/path/to/image4.jpg\"},\n-            {\"type\": \"video\", \"path\": \"/path/to/video.jpg\"},\n-            {\"type\": \"text\", \"text\": \"What are the common elements in these medias?\"},\n-        ],\n-    }\n-]\n-\n-conversations = [conversation1, conversation2, conversation3, conversation4]\n-# Preparation for batch inference\n-ipnuts = processor.apply_chat_template(\n-    conversations,\n-    video_fps=1,\n-    add_generation_prompt=True,\n-    tokenize=True,\n-    return_dict=True,\n-    return_tensors=\"pt\"\n-).to(model.device)\n-\n-\n-# Batch Inference\n-output_ids = model.generate(**inputs, max_new_tokens=128)\n-generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n-output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n+output_text = processor.batch_decode(\n+       generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n+)\n print(output_text)\n ```\n+</hfoption>\n+</hfoptions>\n \n-### Usage Tips\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n \n-#### Image Resolution trade-off\n-\n-The model supports a wide range of resolution inputs. By default, it uses the native resolution for input, but higher resolutions can enhance performance at the cost of more computation. Users can set the minimum and maximum number of pixels to achieve an optimal configuration for their needs.\n-\n-```python\n-min_pixels = 224*224\n-max_pixels = 2048*2048\n-processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n-```\n-\n-In case of limited GPU RAM, one can reduce the resolution as follows:\n-\n-```python\n-min_pixels = 256*28*28\n-max_pixels = 1024*28*28 \n-processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n-```\n-This ensures each image gets encoded using a number between 256-1024 tokens. The 28 comes from the fact that the model uses a patch size of 14 and a temporal patch size of 2 (14 x 2 = 28).\n-\n-#### Multiple Image Inputs\n-\n-By default, images and video content are directly included in the conversation. When handling multiple images, it's helpful to add labels to the images and videos for better reference. Users can control this behavior with the following settings:\n+The example below uses [torchao](../quantization/torchao) to only quantize the weights to int4.\n \n ```python\n-conversation = [\n-    {\n-        \"role\": \"user\",\n-        \"content\": [\n-            {\"type\": \"image\"}, \n-            {\"type\": \"text\", \"text\": \"Hello, how are you?\"}\n-        ]\n-    },\n-    {\n-        \"role\": \"assistant\",\n-        \"content\": \"I'm doing well, thank you for asking. How can I assist you today?\"\n-    },\n-    {\n-        \"role\": \"user\",\n-        \"content\": [\n-            {\"type\": \"text\", \"text\": \"Can you describe these images and video?\"}, \n-            {\"type\": \"image\"}, \n-            {\"type\": \"image\"}, \n-            {\"type\": \"video\"}, \n-            {\"type\": \"text\", \"text\": \"These are from my vacation.\"}\n-        ]\n-    },\n-    {\n-        \"role\": \"assistant\",\n-        \"content\": \"I'd be happy to describe the images and video for you. Could you please provide more context about your vacation?\"\n-    },\n-    {\n-        \"role\": \"user\",\n-        \"content\": \"It was a trip to the mountains. Can you see the details in the images and video?\"\n-    }\n-]\n-\n-# default:\n-prompt_without_id = processor.apply_chat_template(conversation, add_generation_prompt=True)\n-# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Hello, how are you?<|im_end|>\\n<|im_start|>assistant\\nI'm doing well, thank you for asking. How can I assist you today?<|im_end|>\\n<|im_start|>user\\nCan you describe these images and video?<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|video_pad|><|vision_end|>These are from my vacation.<|im_end|>\\n<|im_start|>assistant\\nI'd be happy to describe the images and video for you. Could you please provide more context about your vacation?<|im_end|>\\n<|im_start|>user\\nIt was a trip to the mountains. Can you see the details in the images and video?<|im_end|>\\n<|im_start|>assistant\\n'\n-\n-\n-# add ids\n-prompt_with_id = processor.apply_chat_template(conversation, add_generation_prompt=True, add_vision_id=True)\n-# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nPicture 1: <|vision_start|><|image_pad|><|vision_end|>Hello, how are you?<|im_end|>\\n<|im_start|>assistant\\nI'm doing well, thank you for asking. How can I assist you today?<|im_end|>\\n<|im_start|>user\\nCan you describe these images and video?Picture 2: <|vision_start|><|image_pad|><|vision_end|>Picture 3: <|vision_start|><|image_pad|><|vision_end|>Video 1: <|vision_start|><|video_pad|><|vision_end|>These are from my vacation.<|im_end|>\\n<|im_start|>assistant\\nI'd be happy to describe the images and video for you. Could you please provide more context about your vacation?<|im_end|>\\n<|im_start|>user\\nIt was a trip to the mountains. Can you see the details in the images and video?<|im_end|>\\n<|im_start|>assistant\\n'\n-\n-```\n-\n-#### Flash-Attention 2 to speed up generation\n-\n-First, make sure to install the latest version of Flash Attention 2:\n-\n-```bash\n-pip install -U flash-attn --no-build-isolation\n-```\n-\n-Also, you should have hardware that is compatible with FlashAttention 2. Read more about it in the official documentation of the [flash attention repository](https://github.com/Dao-AILab/flash-attention). FlashAttention-2 can only be used when a model is loaded in `torch.float16` or `torch.bfloat16`.\n-\n-To load and run a model using FlashAttention-2, add `attn_implementation=\"flash_attention_2\"` when loading the model:\n-\n-```python\n-from transformers import Qwen2_5_VLForConditionalGeneration\n+import torch\n+from transformers import TorchAoConfig, Gemma3ForConditionalGeneration, AutoProcessor\n \n+quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n-    \"Qwen/Qwen2.5-VL-7B-Instruct\", \n-    torch_dtype=torch.bfloat16, \n-    attn_implementation=\"flash_attention_2\",\n+    \"Qwen/Qwen2.5-VL-7B-Instruct\",\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"auto\",\n+    quantization_config=quantization_config\n )\n-```\n-\n-\n \n+```\n+### Notes\n+\n+- Use Qwen2.5-VL for video inputs by setting `\"type\": \"video\"` as shown below.\n+    ```python\n+    conversation = [\n+        {\n+            \"role\": \"user\",\n+            \"content\": [\n+                {\"type\": \"video\", \"path\": \"/path/to/video.mp4\"},\n+                {\"type\": \"text\", \"text\": \"What happened in the video?\"},\n+            ],\n+        }\n+    ]\n+    \n+    inputs = processor.apply_chat_template(\n+        conversation,\n+        video_fps=1,\n+        add_generation_prompt=True,\n+        tokenize=True,\n+        return_dict=True,\n+        return_tensors=\"pt\"\n+    ).to(model.device)\n+    \n+    # Inference: Generation of the output\n+    output_ids = model.generate(**inputs, max_new_tokens=128)\n+    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n+    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n+    print(output_text)\n+    ```\n+- Use Qwen2.5-VL for a mixed batch of inputs (images, videos, text). Add labels when handling multiple images or videos for better reference\n+ as show below.\n+    ```python\n+    import torch\n+    from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n+    \n+    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n+        \"Qwen/Qwen2.5-VL-7B-Instruct\",\n+        torch_dtype=torch.float16,\n+        device_map=\"auto\",\n+        attn_implementation=\"sdpa\"\n+    )\n+    processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n+    conversation = [\n+        {\n+            \"role\": \"user\",\n+            \"content\": [\n+                {\"type\": \"image\"}, \n+                {\"type\": \"text\", \"text\": \"Hello, how are you?\"}\n+            ]\n+        },\n+        {\n+            \"role\": \"assistant\",\n+            \"content\": \"I'm doing well, thank you for asking. How can I assist you today?\"\n+        },\n+        {\n+            \"role\": \"user\",\n+            \"content\": [\n+                {\"type\": \"text\", \"text\": \"Can you describe these images and video?\"}, \n+                {\"type\": \"image\"}, \n+                {\"type\": \"image\"}, \n+                {\"type\": \"video\"}, \n+                {\"type\": \"text\", \"text\": \"These are from my vacation.\"}\n+            ]\n+        },\n+        {\n+            \"role\": \"assistant\",\n+            \"content\": \"I'd be happy to describe the images and video for you. Could you please provide more context about your vacation?\"\n+        },\n+        {\n+            \"role\": \"user\",\n+            \"content\": \"It was a trip to the mountains. Can you see the details in the images and video?\"\n+        }\n+    ]\n+    \n+    # default:\n+    prompt_without_id = processor.apply_chat_template(conversation, add_generation_prompt=True)\n+    # Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Hello, how are you?<|im_end|>\\n<|im_start|>assistant\\nI'm doing well, thank you for asking. How can I assist you today?<|im_end|>\\n<|im_start|>user\\nCan you describe these images and video?<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|video_pad|><|vision_end|>These are from my vacation.<|im_end|>\\n<|im_start|>assistant\\nI'd be happy to describe the images and video for you. Could you please provide more context about your vacation?<|im_end|>\\n<|im_start|>user\\nIt was a trip to the mountains. Can you see the details in the images and video?<|im_end|>\\n<|im_start|>assistant\\n'\n+    \n+    \n+    # add ids\n+    prompt_with_id = processor.apply_chat_template(conversation, add_generation_prompt=True, add_vision_id=True)\n+    # Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nPicture 1: <|vision_start|><|image_pad|><|vision_end|>Hello, how are you?<|im_end|>\\n<|im_start|>assistant\\nI'm doing well, thank you for asking. How can I assist you today?<|im_end|>\\n<|im_start|>user\\nCan you describe these images and video?Picture 2: <|vision_start|><|image_pad|><|vision_end|>Picture 3: <|vision_start|><|image_pad|><|vision_end|>Video 1: <|vision_start|><|video_pad|><|vision_end|>These are from my vacation.<|im_end|>\\n<|im_start|>assistant\\nI'd be happy to describe the images and video for you. Could you please provide more context about your vacation?<|im_end|>\\n<|im_start|>user\\nIt was a trip to the mountains. Can you see the details in the images and video?<|im_end|>\\n<|im_start|>assistant\\n'\n+    ```\n+\n+- Use the `min_pixels` and `max_pixels` parameters in [`AutoProcessor`] to set the resolution.\n+\n+    ```python\n+    min_pixels = 224*224\n+    max_pixels = 2048*2048\n+    processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n+    ```\n+    \n+    Higher resolution can require more compute whereas reducing the resolution can save memory as follows:\n+    \n+    ```python\n+    min_pixels = 256*28*28\n+    max_pixels = 1024*28*28 \n+    processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n+    ```\n ## Qwen2_5_VLConfig\n \n [[autodoc]] Qwen2_5_VLConfig"
        }
    ],
    "stats": {
        "total": 372,
        "additions": 167,
        "deletions": 205
    }
}