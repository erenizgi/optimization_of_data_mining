{
    "author": "Wanguy",
    "message": "Fix the bug that `Trainer` cannot correctly call `torch_jit_model_eval` (#35722)\n\nFix the bug that the accelerator.autocast does not pass parameters correctly when calling torch_jit_model_eval (#35706)",
    "sha": "8b78d9d6e797bc842b4c1364105b7741371a729c",
    "files": [
        {
            "sha": "9263486ddddfac74b05e92373cba386bc2811609",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b78d9d6e797bc842b4c1364105b7741371a729c/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b78d9d6e797bc842b4c1364105b7741371a729c/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=8b78d9d6e797bc842b4c1364105b7741371a729c",
            "patch": "@@ -230,6 +230,7 @@\n     from accelerate import __version__ as accelerate_version\n     from accelerate.state import AcceleratorState\n     from accelerate.utils import (\n+        AutocastKwargs,\n         DistributedDataParallelKwargs,\n         DistributedType,\n         load_fsdp_model,\n@@ -1832,7 +1833,8 @@ def torch_jit_model_eval(self, model, dataloader, training=False):\n                 # remove mixed precision hooks from the model\n                 if original_forward:\n                     jit_model.forward = original_forward\n-                with self.accelerator.autocast(cache_enabled=False), torch.no_grad():\n+                autocast_handler = AutocastKwargs(cache_enabled=False)\n+                with self.accelerator.autocast(autocast_handler=autocast_handler), torch.no_grad():\n                     if version.parse(version.parse(torch.__version__).base_version) >= version.parse(\"2.0.0\"):\n                         if isinstance(example_batch, dict):\n                             jit_model = torch.jit.trace(jit_model, example_kwarg_inputs=example_batch, strict=False)"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 3,
        "deletions": 1
    }
}