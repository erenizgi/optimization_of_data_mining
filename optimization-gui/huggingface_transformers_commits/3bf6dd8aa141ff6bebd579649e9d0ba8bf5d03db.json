{
    "author": "Sai-Suraj-27",
    "message": "fix: Fixed CodeGenTokenizationTest::test_truncation failing test (#32850)\n\n* Fixed failing CodeGenTokenizationTest::test_truncation.\r\n\r\n* [run_slow] Codegen\r\n\r\n* [run_slow] codegen",
    "sha": "3bf6dd8aa141ff6bebd579649e9d0ba8bf5d03db",
    "files": [
        {
            "sha": "184c75216290b8e905345f838d3f26896fb33958",
            "filename": "tests/models/codegen/test_tokenization_codegen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bf6dd8aa141ff6bebd579649e9d0ba8bf5d03db/tests%2Fmodels%2Fcodegen%2Ftest_tokenization_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bf6dd8aa141ff6bebd579649e9d0ba8bf5d03db/tests%2Fmodels%2Fcodegen%2Ftest_tokenization_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcodegen%2Ftest_tokenization_codegen.py?ref=3bf6dd8aa141ff6bebd579649e9d0ba8bf5d03db",
            "patch": "@@ -254,12 +254,12 @@ def test_truncation(self):\n         tokenizer = CodeGenTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")\n \n         text = \"\\nif len_a > len_b:\\n    result = a\\nelse:\\n    result = b\\n\\n\\n\\n#\"\n-        expected_trucated_text = \"\\nif len_a > len_b:      result = a\\nelse:      result = b\"\n+        expected_truncated_text = \"\\nif len_a > len_b:\\n      result = a\\nelse:\\n      result = b\"\n \n         input_ids = tokenizer.encode(text)\n         truncation_pattern = [\"^#\", re.escape(\"<|endoftext|>\"), \"^'''\", '^\"\"\"', \"\\n\\n\\n\"]\n         decoded_text = tokenizer.decode(input_ids, truncate_before_pattern=truncation_pattern)\n-        self.assertEqual(decoded_text, expected_trucated_text)\n+        self.assertEqual(decoded_text, expected_truncated_text)\n         # TODO @ArthurZ outputs of the fast tokenizer are different in this case, un-related to the PR\n \n     # tokenizer has no padding token"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}