{
    "author": "ArthurZucker",
    "message": "Refactor-tokenization-more (#42563)\n\n* On commit to bind them all\n\n* nits\n\n* smnall update\n\n* elif\n\n* super small nit\n\n* BPE!\n\n* fix\n\n* up up up\n\n* fix?\n\n* one typo\n\n* per model updates\n\n* more model specific updates\n\n* more per model updates\n\n* more model specific updates\n\n* simplify default merges\n\n* fiuxp\n\n* update\n\n* update\n\n* style\n\n* fix\n\n* fix colpali\n\n* nits\n\n* simpler regex + big shitty bird\n\n* fixup and fix\n\n* fix codellama\n\n* up\n\n* fix pop on none\n\n* fix parkeet\n\n* fix llama\n\n* big fixup\n\n* fix markul lm\n\n* update common\n\n* fix mbart\n\n* fix seamlessm4T\n\n* fix comment\n\n* torch tests\n\n* nnits and revert UNK idx change\n\n* oh only one deberta\n\n* torch tests\n\n* add convert from spm per model!\n\n* fix last 2 for pegasus\n\n* fix torch tests\n\n* fixes\n\n* fix tests\n\n* check versioned files\n\n* fix processor auto test\n\n* fix custom tok clip\n\n* try this fix\n\n* modeling rag\n\n* fix rag\n\n* roformer the Tokenizers way\n\n* up\n\n* updatge\n\n* fix unk\n\n* update\n\n* fix roberta\n\n* if there is no mapped class and no tokenizer.json its fucked -> just have the mapped class ready!\n\n* fix the rest\n\n* fix copies\n\n* fix doc and copies\n\n* fix mbart50\n\n* fix deberta_v2 test\n\n* fix and simplify whisper :)\n\n* fix big bird default was worng\n\n* fix final\n\n* fixup\n\n* small nit\n\n* a weird way to fix fuyu?\n\n* default xlm roberta to fix kosmo behaviour!\n\n* remove small errors\n\n* last fix?\n\n* fix pixtral\n\n* style\n\n* fix\n\n* quality ta radce\n\n* fix?\n\n* remove something\n\n* remov one code that shouldd not have been there!\n\n* fix ?\n\n* fixup\n\n* update\n\n* fix for custom code\n\n* add a custom model path to make sure custom stuff is registe\n\n* fix trust remote code\n\n* exceeded\n\n* don't\n\n* ouppsy for cohere\n\n* why is this one also affected\n\n* fixup\n\n* fixup\n\n* nits\n\n* fix idefics3 tests\n\n* okay read the processor\n\n* fix the layout.... models\n\n* nits\n\n* codellama needs the bos passed\n\n* fix dpr\n\n* fix?\n\n* fixup\n\n* distilbert defaults\n\n* fix\n\n* clvp update to PythonTokenizer\n\n* bloom\n\n* style\n\n* layoutxlm\n\n* style\n\n* olmo\n\n* only pop when we don't convert from tokenizer.json\n\n* fixup\n\n* hub issue\n\n* id\n\n* fix\n\n---------\n\nCo-authored-by: itazap <ita.zaporozhets@huggingface.co>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-164-45.ec2.internal>",
    "sha": "73a13f86f6d208882d59d1200609986c5a5f49a7",
    "files": [
        {
            "sha": "60eebc798fee7055008466b117064cf828c5f3a8",
            "filename": "MIGRATION_GUIDE_V5.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/MIGRATION_GUIDE_V5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/MIGRATION_GUIDE_V5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/MIGRATION_GUIDE_V5.md?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -105,7 +105,7 @@ class Llama5Tokenizer(TokenizersBackend):\n             self._vocab = vocab\n \n         if merges is not None:\n-            self._merges = merges\n+            self._merges = merges or []\n         else:\n             self._merges = generate_merges(filtered_vocab)\n \n@@ -339,7 +339,7 @@ _We aim for this to be fixed and released in a following release candidate in th\n ### Remote code incompatibility\n \n A lot of paths were removed and reworked; paths like `transformers.tokenization_utils` and `transformers.tokenization_utils_fast`, which no longer exist.\n-We'll be working on backwards compatibility for these before version 5 is fully released.\n+These now redirect to `transformers.tokenization_utils_sentencepiece` and `transformers.tokenization_utils_tokenizers` respectively; please update imports accordingly.\n \n _We aim for this to be fixed and released in a following release candidate in the week that follows RC0._\n \n@@ -621,4 +621,4 @@ Linked PR: https://github.com/huggingface/transformers/pull/42391.\n - related to 1., it is not possible to set proxies from your script. To handle proxies, you must set the `HTTP_PROXY` / `HTTPS_PROXY` environment variables\n - `hf_transfer` and therefore `HF_HUB_ENABLE_HF_TRANSFER` have been completed dropped in favor of `hf_xet`. This should be transparent for most users. Please let us know if you notice any downside!\n \n-`typer-slim` has been added as required dependency, used to implement both `hf` and `transformers` CLIs.\n\\ No newline at end of file\n+`typer-slim` has been added as required dependency, used to implement both `hf` and `transformers` CLIs."
        },
        {
            "sha": "2afe87fd38c30599d106b00c72e73f2c512b6577",
            "filename": "docs/source/en/model_doc/roformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/docs%2Fsource%2Fen%2Fmodel_doc%2Froformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/docs%2Fsource%2Fen%2Fmodel_doc%2Froformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Froformer.md?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -99,8 +99,7 @@ echo -e \"水在零度时会[MASK]\" | transformers run --task fill-mask --model j\n \n ## RoFormerTokenizerFast\n \n-[[autodoc]] RoFormerTokenizerFast\n-    - build_inputs_with_special_tokens\n+`RoFormerTokenizerFast` is an alias for [`RoFormerTokenizer`].\n \n ## RoFormerModel\n "
        },
        {
            "sha": "97e9331f67795fa84124922cf4589adae57fbeda",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 25,
            "deletions": 2,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -20,6 +20,9 @@\n \n __version__ = \"5.0.0.dev0\"\n \n+import importlib\n+import sys\n+import types\n from pathlib import Path\n from typing import TYPE_CHECKING\n \n@@ -174,6 +177,8 @@\n     \"quantizers\": [],\n     \"testing_utils\": [],\n     \"tokenization_python\": [\"PreTrainedTokenizer\", \"PythonBackend\"],\n+    \"tokenization_utils\": [],\n+    \"tokenization_utils_fast\": [],\n     \"tokenization_utils_sentencepiece\": [\"SentencePieceBackend\"],\n     \"tokenization_utils_base\": [\n         \"AddedToken\",\n@@ -768,8 +773,6 @@\n     from .utils.quantization_config import VptqConfig as VptqConfig\n     from .video_processing_utils import BaseVideoProcessor as BaseVideoProcessor\n else:\n-    import sys\n-\n     _import_structure = {k: set(v) for k, v in _import_structure.items()}\n \n     import_structure = define_import_structure(Path(__file__).parent / \"models\", prefix=\"models\")\n@@ -783,6 +786,26 @@\n         extra_objects={\"__version__\": __version__},\n     )\n \n+    def _create_tokenization_alias(alias: str, target: str) -> None:\n+        \"\"\"\n+        Lazily redirect legacy tokenization module paths to their replacements without importing heavy deps.\n+        \"\"\"\n+\n+        module = types.ModuleType(alias)\n+        module.__doc__ = f\"Alias module for backward compatibility with `{target}`.\"\n+\n+        def _get_target():\n+            return importlib.import_module(target, __name__)\n+\n+        module.__getattr__ = lambda name: getattr(_get_target(), name)\n+        module.__dir__ = lambda: dir(_get_target())\n+\n+        sys.modules[alias] = module\n+        setattr(sys.modules[__name__], alias.rsplit(\".\", 1)[-1], module)\n+\n+    _create_tokenization_alias(f\"{__name__}.tokenization_utils_fast\", \".tokenization_utils_tokenizers\")\n+    _create_tokenization_alias(f\"{__name__}.tokenization_utils\", \".tokenization_utils_sentencepiece\")\n+\n \n if not is_torch_available():\n     logger.warning_advice("
        },
        {
            "sha": "106b201dd62fb73fbf8520bf30afb8a8873fbb26",
            "filename": "src/transformers/convert_slow_tokenizer.py",
            "status": "modified",
            "additions": 224,
            "deletions": 9,
            "changes": 233,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_slow_tokenizer.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -19,6 +19,7 @@\n \"\"\"\n \n import warnings\n+from collections.abc import Collection\n from functools import lru_cache\n from typing import Optional\n \n@@ -33,6 +34,64 @@\n \n logger = logging.get_logger(__name__)\n \n+MBART_LANGUAGES = [\n+    \"ar_AR\",\n+    \"cs_CZ\",\n+    \"de_DE\",\n+    \"en_XX\",\n+    \"es_XX\",\n+    \"et_EE\",\n+    \"fi_FI\",\n+    \"fr_XX\",\n+    \"gu_IN\",\n+    \"hi_IN\",\n+    \"it_IT\",\n+    \"ja_XX\",\n+    \"kk_KZ\",\n+    \"ko_KR\",\n+    \"lt_LT\",\n+    \"lv_LV\",\n+    \"my_MM\",\n+    \"ne_NP\",\n+    \"nl_XX\",\n+    \"ro_RO\",\n+    \"ru_RU\",\n+    \"si_LK\",\n+    \"tr_TR\",\n+    \"vi_VN\",\n+    \"zh_CN\",\n+]\n+\n+MBART50_LANGUAGES = MBART_LANGUAGES + [\n+    \"af_ZA\",\n+    \"az_AZ\",\n+    \"bn_IN\",\n+    \"fa_IR\",\n+    \"he_IL\",\n+    \"hr_HR\",\n+    \"id_ID\",\n+    \"ka_GE\",\n+    \"km_KH\",\n+    \"mk_MK\",\n+    \"ml_IN\",\n+    \"mn_MN\",\n+    \"mr_IN\",\n+    \"pl_PL\",\n+    \"ps_AF\",\n+    \"pt_XX\",\n+    \"sv_SE\",\n+    \"sw_KE\",\n+    \"ta_IN\",\n+    \"te_IN\",\n+    \"th_TH\",\n+    \"tl_XX\",\n+    \"uk_UA\",\n+    \"ur_PK\",\n+    \"xh_ZA\",\n+    \"gl_ES\",\n+    \"sl_SI\",\n+]\n+\n \n def import_protobuf(error_message=\"\"):\n     if is_sentencepiece_available():\n@@ -61,15 +120,20 @@ def _get_prepend_scheme(add_prefix_space: bool, original_tokenizer) -> str:\n     return prepend_scheme\n \n \n-def generate_merges(vocab, vocab_scores):\n+def generate_merges(vocab, vocab_scores, skip_tokens: Optional[Collection[str]] = None):\n+    skip_tokens = set(skip_tokens) if skip_tokens is not None else set()\n     reverse = vocab_scores is not None\n     vocab_scores = dict(vocab_scores) if reverse else vocab\n \n     merges = []\n     for merge, piece_score in vocab_scores.items():\n+        if merge in skip_tokens:\n+            continue\n         local = []\n         for index in range(1, len(merge)):\n             piece_l, piece_r = merge[:index], merge[index:]\n+            if piece_l in skip_tokens or piece_r in skip_tokens:\n+                continue\n             if piece_l in vocab and piece_r in vocab:\n                 local.append((piece_l, piece_r, piece_score))\n         local = sorted(local, key=lambda x: (vocab[x[0]], vocab[x[1]]))\n@@ -87,22 +151,49 @@ class SentencePieceExtractor:\n \n     def __init__(self, model: str):\n         requires_backends(self, \"sentencepiece\")\n-        from sentencepiece import SentencePieceProcessor\n+        requires_backends(self, \"protobuf\")\n+\n+        # from .utils import sentencepiece_model_pb2 as model_pb2\n+        model_pb2 = import_protobuf()\n \n-        self.sp = SentencePieceProcessor()\n-        self.sp.Load(model)\n+        m = model_pb2.ModelProto()\n+        with open(model, \"rb\") as f:\n+            m.ParseFromString(f.read())\n+        self.proto = m\n \n-    def extract(self, vocab_scores=None) -> tuple[dict[str, int], list[tuple]]:\n+    def extract(self, model_type, **kwargs) -> tuple[dict[str, int], list[tuple]]:\n         \"\"\"\n         By default will return vocab and merges with respect to their order, by sending `vocab_scores` we're going to\n         order the merges with respect to the piece scores instead.\n         \"\"\"\n-        sp = self.sp\n-        vocab = {sp.id_to_piece(index): index for index in range(sp.GetPieceSize())}\n+        self.proto.trainer_spec.unk_id\n+        if model_type is None:\n+            from tokenizers.models import BPE, Unigram\n \n-        merges = generate_merges(vocab, vocab_scores)\n+            model_type = Unigram if self.proto.trainer_spec.model_type == 2 else BPE\n+        vocab = [(piece.piece, piece.score) for piece in self.proto.pieces]\n \n-        return vocab, merges\n+        if model_type.__name__ != \"BPE\":\n+            kwargs[\"unk_id\"] = self.proto.trainer_spec.unk_id\n+            kwargs[\"vocab\"] = vocab\n+        else:\n+            from .tokenization_utils_base import generate_merges\n+\n+            vocab = {word: i for i, (word, score) in enumerate(vocab)}\n+            merges = generate_merges(vocab)\n+            kwargs[\"vocab\"] = vocab\n+            kwargs[\"merges\"] = merges\n+\n+        # control tokens are special\n+        # user defined symbols are not\n+        # both user and control tokens are AddedTokens\n+        # Add user defined symbols (type == 4) from sentencepiece (https://github.com/google/sentencepiece/blob/6225e08edb2577757163b3f5dbba4c0b670ef445/src/sentencepiece_model.proto#L299C29-L299C33)\n+        spm_added_tokens = [(id, p.piece, p.type == 3) for id, p in enumerate(self.proto.pieces) if p.type in [3, 4]]\n+        kwargs[\"additional_special_tokens\"] = [\n+            AddedToken(token, normalized=False, special=special)\n+            for id, token, special in sorted(spm_added_tokens, key=lambda x: x[0])\n+        ]\n+        return kwargs\n \n \n class GemmaSentencePieceExtractor(SentencePieceExtractor):\n@@ -549,6 +640,16 @@ class SpmConverter(Converter):\n     SpmExtractor = SentencePieceExtractor\n     special_tokens = {}\n \n+    @classmethod\n+    def convert_from_spm(cls, vocab=None, **kwargs):\n+        \"\"\"\n+        Hook used when converting directly from a SentencePiece model without a slow tokenizer instance.\n+        By default, return kwargs unchanged.\n+        \"\"\"\n+        if vocab is not None:\n+            kwargs[\"vocab\"] = vocab\n+        return kwargs\n+\n     def __init__(self, *args):\n         requires_backends(self, \"protobuf\")\n \n@@ -754,6 +855,25 @@ def post_processor(self):\n             ],\n         )\n \n+    @classmethod\n+    def convert_from_spm(cls, vocab=None, **kwargs):\n+        pad_token = str(kwargs.get(\"pad_token\", \"<pad>\"))\n+        unk_token = str(kwargs.get(\"unk_token\", \"<unk>\"))\n+        mask_token = str(kwargs.get(\"mask_token\", \"<mask>\"))\n+\n+        vocab_list = [\n+            (\"<s>NOTUSED\", 0.0),\n+            (pad_token, 0.0),\n+            (\"</s>NOTUSED\", 0.0),\n+            (unk_token, 0.0),\n+            (\"<unk>NOTUSED\", -100.0),\n+        ]\n+        if vocab is not None:\n+            vocab_list.extend(list(vocab)[1:])\n+        vocab_list.append((mask_token, 0.0))\n+        kwargs[\"vocab\"] = vocab_list\n+        return kwargs\n+\n \n class DebertaV2Converter(SpmConverter):\n     def pre_tokenizer(self, replacement, add_prefix_space):\n@@ -840,6 +960,27 @@ def post_processor(self):\n             ],\n         )\n \n+    @classmethod\n+    def convert_from_spm(cls, vocab=None, **kwargs):\n+        bos_token = str(kwargs.get(\"bos_token\", \"<s>\"))\n+        pad_token = str(kwargs.get(\"pad_token\", \"<pad>\"))\n+        eos_token = str(kwargs.get(\"eos_token\", \"</s>\"))\n+        unk_token = str(kwargs.get(\"unk_token\", \"<unk>\"))\n+        mask_token = str(kwargs.get(\"mask_token\", \"<mask>\"))\n+\n+        vocab_list = [\n+            (bos_token, 0.0),\n+            (pad_token, 0.0),\n+            (eos_token, 0.0),\n+            (unk_token, 0.0),\n+        ]\n+        if vocab is not None:\n+            vocab_list.extend(list(vocab)[3:])\n+        vocab_list.extend((lang_code, 0.0) for lang_code in MBART_LANGUAGES)\n+        vocab_list.append((mask_token, 0.0))\n+        kwargs[\"vocab\"] = vocab_list\n+        return kwargs\n+\n \n class MBart50Converter(SpmConverter):\n     def vocab(self, proto):\n@@ -867,6 +1008,27 @@ def post_processor(self):\n             ],\n         )\n \n+    @classmethod\n+    def convert_from_spm(cls, vocab=None, **kwargs):\n+        cls_token = str(kwargs.get(\"cls_token\", \"<s>\"))\n+        pad_token = str(kwargs.get(\"pad_token\", \"<pad>\"))\n+        eos_token = str(kwargs.get(\"eos_token\", \"</s>\"))\n+        unk_token = str(kwargs.get(\"unk_token\", \"<unk>\"))\n+        mask_token = str(kwargs.get(\"mask_token\", \"<mask>\"))\n+\n+        vocab_list = [\n+            (cls_token, 0.0),\n+            (pad_token, 0.0),\n+            (eos_token, 0.0),\n+            (unk_token, 0.0),\n+        ]\n+        if vocab is not None:\n+            vocab_list.extend(list(vocab)[3:])\n+        vocab_list.extend((lang_code, 0.0) for lang_code in MBART50_LANGUAGES)\n+        vocab_list.append((mask_token, 0.0))\n+        kwargs[\"vocab\"] = vocab_list\n+        return kwargs\n+\n \n class NllbConverter(SpmConverter):\n     def vocab(self, proto):\n@@ -892,6 +1054,28 @@ def post_processor(self):\n             ],\n         )\n \n+    @classmethod\n+    def convert_from_spm(cls, vocab=None, **kwargs):\n+        bos_token = str(kwargs.get(\"bos_token\", \"<s>\"))\n+        pad_token = str(kwargs.get(\"pad_token\", \"<pad>\"))\n+        eos_token = str(kwargs.get(\"eos_token\", \"</s>\"))\n+        unk_token = str(kwargs.get(\"unk_token\", \"<unk>\"))\n+\n+        reordered_vocab = {\n+            bos_token: 0,\n+            pad_token: 1,\n+            eos_token: 2,\n+            unk_token: 3,\n+        }\n+        if vocab is not None:\n+            tokens = vocab.keys() if isinstance(vocab, dict) else [tok for tok, _ in vocab]\n+            for token in tokens:\n+                if token in reordered_vocab:\n+                    continue\n+                reordered_vocab[token] = len(reordered_vocab)\n+        kwargs[\"vocab\"] = reordered_vocab\n+        return kwargs\n+\n \n class SeamlessM4TConverter(SpmConverter):\n     def vocab(self, proto):\n@@ -944,6 +1128,26 @@ def post_processor(self):\n             ],\n         )\n \n+    @classmethod\n+    def convert_from_spm(cls, vocab=None, **kwargs):\n+        bos_token = str(kwargs.get(\"bos_token\", \"<s>\"))\n+        pad_token = str(kwargs.get(\"pad_token\", \"<pad>\"))\n+        eos_token = str(kwargs.get(\"eos_token\", \"</s>\"))\n+        unk_token = str(kwargs.get(\"unk_token\", \"<unk>\"))\n+        mask_token = str(kwargs.get(\"mask_token\", \"<mask>\"))\n+\n+        vocab_list = [\n+            (bos_token, 0.0),\n+            (pad_token, 0.0),\n+            (eos_token, 0.0),\n+            (unk_token, 0.0),\n+        ]\n+        if vocab is not None:\n+            vocab_list.extend(list(vocab)[3:])\n+        vocab_list.append((mask_token, 0.0))\n+        kwargs[\"vocab\"] = vocab_list\n+        return kwargs\n+\n \n class XLNetConverter(SpmConverter):\n     def vocab(self, proto):\n@@ -1078,6 +1282,17 @@ def post_processor(self):\n             ],\n         )\n \n+    @classmethod\n+    def convert_from_spm(cls, vocab=None, **kwargs):\n+        extra_ids = kwargs.get(\"extra_ids\", 100)\n+        extra_tokens = [f\"<extra_id_{i}>\" for i in range(extra_ids - 1, -1, -1)]\n+        vocab_list = list(vocab) if vocab is not None else []\n+        vocab_list.extend((token, 0.0) for token in extra_tokens)\n+\n+        kwargs.setdefault(\"additional_special_tokens\", extra_tokens)\n+        kwargs[\"vocab\"] = vocab_list\n+        return kwargs\n+\n \n class UdopConverter(SpmConverter):\n     def post_processor(self):"
        },
        {
            "sha": "749995324bb48694368f34aab8c0d689e7bdfdee",
            "filename": "src/transformers/models/albert/tokenization_albert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 12,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Tokenization classes for ALBERT model.\"\"\"\n \n-from typing import Optional\n+from typing import Optional, Union\n \n from tokenizers import Regex, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import Unigram\n@@ -73,19 +73,20 @@ class AlbertTokenizer(TokenizersBackend):\n             other word.\n         trim_offsets (`bool`, *optional*, defaults to `True`):\n             Whether the post processing step should trim offsets to avoid including whitespaces.\n-        vocab (`dict`, *optional*):\n-            Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n+        vocab (`str` or `list[tuple[str, float]]`, *optional*):\n+            Custom vocabulary with `(token, score)` tuples. If not provided, vocabulary is loaded from `vocab_file`.\n         vocab_file (`str`, *optional*):\n             [SentencePiece](https://github.com/google/sentencepiece) file (generally has a .model extension) that\n             contains the vocabulary necessary to instantiate a tokenizer.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n+    model = Unigram\n \n     def __init__(\n         self,\n+        vocab: Optional[Union[str, list[tuple[str, float]]]] = None,\n         do_lower_case: bool = True,\n         keep_accents: bool = False,\n         bos_token: str = \"[CLS]\",\n@@ -97,19 +98,15 @@ def __init__(\n         mask_token: str = \"[MASK]\",\n         add_prefix_space: bool = True,\n         trim_offsets: bool = True,\n-        vocab: Optional[dict] = None,\n-        vocab_file: Optional[str] = None,\n         **kwargs,\n     ):\n-        self.vocab_file = vocab_file\n         self.add_prefix_space = add_prefix_space\n         self.trim_offsets = trim_offsets\n-\n         self.do_lower_case = do_lower_case\n         self.keep_accents = keep_accents\n \n         if vocab is not None:\n-            self._vocab_scores = [(token, 0.0) for token in vocab.keys()] if isinstance(vocab, dict) else list(vocab)\n+            self._vocab_scores = vocab\n         else:\n             self._vocab_scores = [\n                 (str(pad_token), 0.0),\n@@ -163,10 +160,7 @@ def __init__(\n             ],\n         )\n \n-        tokenizer_object = self._tokenizer\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             do_lower_case=self.do_lower_case,\n             keep_accents=self.keep_accents,\n             bos_token=bos_token,"
        },
        {
            "sha": "1eaf5bad9202b30395173842c1381f9be1c8bcdf",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 73,
            "deletions": 472,
            "changes": 545,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"Auto Tokenizer class.\"\"\"\n \n import importlib\n-import inspect\n import json\n import os\n from collections import OrderedDict\n@@ -26,16 +25,15 @@\n from ...configuration_utils import PreTrainedConfig\n from ...dynamic_module_utils import get_class_from_dynamic_module, resolve_trust_remote_code\n from ...modeling_gguf_pytorch_utils import load_gguf_checkpoint\n-from ...tokenization_python import PreTrainedTokenizer, PythonBackend\n-from ...tokenization_utils_base import TOKENIZER_CONFIG_FILE, find_sentencepiece_model_file, load_vocab_and_merges\n+from ...tokenization_utils_base import TOKENIZER_CONFIG_FILE\n from ...utils import (\n     extract_commit_hash,\n     is_g2p_en_available,\n     is_sentencepiece_available,\n     is_tokenizers_available,\n     logging,\n )\n-from ...utils.hub import cached_file, has_file\n+from ...utils.hub import cached_file\n from ..encoder_decoder import EncoderDecoderConfig\n from .auto_factory import _LazyAutoMapping\n from .configuration_auto import (\n@@ -68,8 +66,8 @@\n         (\"aimv2\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None),\n         (\"albert\", \"AlbertTokenizer\" if is_tokenizers_available() else None),\n         (\"align\", \"BertTokenizer\" if is_tokenizers_available() else None),\n-        (\"arcee\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"aria\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"arcee\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n+        (\"aria\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"aya_vision\", \"CohereTokenizer\" if is_tokenizers_available() else None),\n         (\"bark\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"bart\", \"RobertaTokenizer\" if is_tokenizers_available() else None),\n@@ -82,19 +80,19 @@\n         (\"big_bird\", \"BigBirdTokenizer\" if is_tokenizers_available() else None),\n         (\"bigbird_pegasus\", \"PegasusTokenizer\" if is_tokenizers_available() else None),\n         (\"biogpt\", \"BioGptTokenizer\"),\n-        (\"bitnet\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"bitnet\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"blenderbot\", \"BlenderbotTokenizer\" if is_tokenizers_available() else None),\n         (\"blenderbot-small\", \"BlenderbotSmallTokenizer\"),\n         (\"blip\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"blip-2\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\"bloom\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n-        (\"blt\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"blt\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"bridgetower\", \"RobertaTokenizer\"),\n         (\"bros\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"byt5\", \"ByT5Tokenizer\"),\n         (\"camembert\", \"CamembertTokenizer\" if is_tokenizers_available() else None),\n         (\"canine\", \"CanineTokenizer\"),\n-        (\"chameleon\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"chameleon\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"chinese_clip\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"clap\", \"RobertaTokenizer\"),\n         (\"clip\", \"CLIPTokenizer\" if is_tokenizers_available() else None),\n@@ -104,34 +102,34 @@\n         (\"codegen\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\"cohere\", \"CohereTokenizer\" if is_tokenizers_available() else None),\n         (\"cohere2\", \"CohereTokenizer\" if is_tokenizers_available() else None),\n-        (\"colpali\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"colpali\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"colqwen2\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n         (\"convbert\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"cpm\", \"CpmTokenizer\" if is_tokenizers_available() else None),\n         (\"cpmant\", \"CpmAntTokenizer\"),\n-        (\"csm\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"csm\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"ctrl\", \"CTRLTokenizer\"),\n         (\"data2vec-audio\", \"Wav2Vec2CTCTokenizer\"),\n         (\"data2vec-text\", \"RobertaTokenizer\"),\n         (\"dbrx\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\"deberta\", \"DebertaTokenizer\" if is_tokenizers_available() else None),\n         (\"deberta-v2\", \"DebertaV2Tokenizer\" if is_tokenizers_available() else None),\n-        (\"deepseek_v2\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"deepseek_v3\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"deepseek_vl\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"deepseek_vl_hybrid\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"deepseek_v2\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n+        (\"deepseek_v3\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n+        (\"deepseek_vl\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n+        (\"deepseek_vl_hybrid\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"dia\", \"DiaTokenizer\"),\n-        (\"diffllama\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"diffllama\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"distilbert\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"dpr\", \"DPRQuestionEncoderTokenizerFast\" if is_tokenizers_available() else None),\n         (\"electra\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"emu3\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\"ernie\", \"BertTokenizer\" if is_tokenizers_available() else None),\n-        (\"ernie4_5\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"ernie4_5_moe\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"ernie4_5\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n+        (\"ernie4_5_moe\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"esm\", \"EsmTokenizer\"),\n         (\"exaone4\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n-        (\"falcon\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"falcon\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"falcon_mamba\", \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None),\n         (\"fastspeech2_conformer\", \"FastSpeech2ConformerTokenizer\" if is_g2p_en_available() else None),\n         (\"flaubert\", \"FlaubertTokenizer\"),\n@@ -141,62 +139,63 @@\n         (\"fnet\", \"FNetTokenizerFast\" if is_tokenizers_available() else None),\n         (\"fsmt\", \"FSMTTokenizer\"),\n         (\"funnel\", \"FunnelTokenizer\" if is_tokenizers_available() else None),\n+        (\"fuyu\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"gemma\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n         (\"gemma2\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n         (\"gemma3\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n         (\"gemma3_text\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n         (\"gemma3n\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n         (\"gemma3n_text\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n         (\"git\", \"BertTokenizer\" if is_tokenizers_available() else None),\n-        (\"glm\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"glm4\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"glm4_moe\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"glm4v\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"glm4v_moe\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"got_ocr2\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"glm\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n+        (\"glm4\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n+        (\"glm4_moe\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n+        (\"glm4v\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n+        (\"glm4v_moe\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n+        (\"got_ocr2\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"gpt-sw3\", \"GPTSw3Tokenizer\" if is_sentencepiece_available() else None),\n         (\"gpt2\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\"gpt_bigcode\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\"gpt_neo\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\"gpt_neox\", \"GPTNeoXTokenizer\" if is_tokenizers_available() else None),\n         (\"gpt_neox_japanese\", \"GPTNeoXJapaneseTokenizer\"),\n-        (\"gpt_oss\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"gpt_oss\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"gptj\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\"granite\", \"GPT2Tokenizer\"),\n         (\"granitemoe\", \"GPT2Tokenizer\"),\n         (\"granitemoehybrid\", \"GPT2Tokenizer\"),\n         (\"granitemoeshared\", \"GPT2Tokenizer\"),\n         (\"grounding-dino\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"groupvit\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"helium\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"helium\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"herbert\", \"HerbertTokenizer\" if is_tokenizers_available() else None),\n         (\"hubert\", \"Wav2Vec2CTCTokenizer\"),\n         (\"ibert\", \"RobertaTokenizer\"),\n-        (\"idefics\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"idefics2\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"idefics3\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"idefics\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n+        (\"idefics2\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n+        (\"idefics3\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"instructblip\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\"instructblipvideo\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\"internvl\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n-        (\"jamba\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"janus\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"jetmoe\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"jamba\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n+        (\"janus\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n+        (\"jetmoe\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"kosmos-2\", \"XLMRobertaTokenizer\" if is_tokenizers_available() else None),\n-        (\"kosmos-2.5\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"kosmos-2.5\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"layoutlm\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"layoutlmv2\", \"LayoutLMv2Tokenizer\" if is_tokenizers_available() else None),\n         (\"layoutlmv3\", \"LayoutLMv3Tokenizer\" if is_tokenizers_available() else None),\n         (\"layoutxlm\", \"LayoutXLMTokenizer\" if is_tokenizers_available() else None),\n         (\"led\", \"LEDTokenizer\" if is_tokenizers_available() else None),\n-        (\"lfm2_vl\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"lfm2_vl\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"lilt\", \"RobertaTokenizer\" if is_tokenizers_available() else None),\n         (\"llama\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n-        (\"llama4\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"llama4_text\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"llava\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"llava_next\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"llava_next_video\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"llava_onevision\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"llama4\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n+        (\"llama4_text\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n+        (\"llava\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n+        (\"llava_next\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n+        (\"llava_next_video\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n+        (\"llava_onevision\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"longformer\", \"RobertaTokenizer\" if is_tokenizers_available() else None),\n         (\"longt5\", \"T5Tokenizer\" if is_tokenizers_available() else None),\n         (\"luke\", \"LukeTokenizer\"),\n@@ -218,37 +217,37 @@\n                 \"MistralCommonBackend\"\n                 if is_mistral_common_available()\n                 else (\"LlamaTokenizer\" if is_sentencepiece_available() else None),\n-                \"LlamaTokenizerFast\" if is_tokenizers_available() and not is_mistral_common_available() else None,\n+                \"LlamaTokenizer\" if is_tokenizers_available() and not is_mistral_common_available() else None,\n             ),\n         ),\n         (\n             \"mistral\",\n             \"MistralCommonBackend\"\n             if is_mistral_common_available()\n-            else (\"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+            else (\"LlamaTokenizer\" if is_tokenizers_available() else None),\n         ),\n         (\n             \"mistral3\",\n             (\n                 \"MistralCommonBackend\"\n                 if is_mistral_common_available()\n                 else (\"LlamaTokenizer\" if is_sentencepiece_available() else None),\n-                \"LlamaTokenizerFast\" if is_tokenizers_available() and not is_mistral_common_available() else None,\n+                \"LlamaTokenizer\" if is_tokenizers_available() and not is_mistral_common_available() else None,\n             ),\n         ),\n         (\n             \"mixtral\",\n             \"MistralCommonBackend\"\n             if is_mistral_common_available()\n-            else (\"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+            else (\"LlamaTokenizer\" if is_tokenizers_available() else None),\n         ),\n-        (\"mllama\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"mllama\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"mluke\", \"MLukeTokenizer\" if is_sentencepiece_available() else None),\n         (\"mm-grounding-dino\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"mobilebert\", \"MobileBertTokenizer\" if is_tokenizers_available() else None),\n-        (\"modernbert\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"moonshine\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"moshi\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"modernbert\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n+        (\"moonshine\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n+        (\"moshi\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"mpnet\", \"MPNetTokenizer\" if is_tokenizers_available() else None),\n         (\"mpt\", \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None),\n         (\"mra\", \"RobertaTokenizer\"),\n@@ -257,7 +256,7 @@\n         (\"musicgen_melody\", \"T5Tokenizer\" if is_tokenizers_available() else None),\n         (\"mvp\", \"MvpTokenizer\" if is_tokenizers_available() else None),\n         (\"myt5\", \"MyT5Tokenizer\"),\n-        (\"nemotron\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"nemotron\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"nezha\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"nllb\", \"NllbTokenizer\" if is_tokenizers_available() else None),\n         (\"nllb-moe\", \"NllbTokenizer\" if is_tokenizers_available() else None),\n@@ -274,21 +273,21 @@\n         (\"ovis2\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n         (\"owlv2\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None),\n         (\"owlvit\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"paligemma\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"paligemma\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"pegasus\", \"PegasusTokenizer\" if is_tokenizers_available() else None),\n         (\"pegasus_x\", \"PegasusTokenizer\" if is_tokenizers_available() else None),\n         (\"perceiver\", \"PerceiverTokenizer\"),\n-        (\"persimmon\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"persimmon\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"phi\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n-        (\"phi3\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"phimoe\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"phi3\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n+        (\"phimoe\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"phobert\", \"PhobertTokenizer\"),\n         (\"pix2struct\", \"T5Tokenizer\" if is_tokenizers_available() else None),\n         (\n             \"pixtral\",\n             \"MistralCommonBackend\"\n             if is_mistral_common_available()\n-            else (\"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+            else (\"TokenizersBackend\" if is_tokenizers_available() else None),\n         ),\n         (\"plbart\", \"PLBartTokenizer\" if is_tokenizers_available() else None),\n         (\"prophetnet\", \"ProphetNetTokenizer\"),\n@@ -314,14 +313,14 @@\n         (\"roberta\", \"RobertaTokenizer\"),\n         (\"roberta-prelayernorm\", \"RobertaTokenizer\"),\n         (\"roc_bert\", \"RoCBertTokenizer\"),\n-        (\"roformer\", \"RoFormerTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"roformer\", \"RoFormerTokenizer\" if is_tokenizers_available() else None),\n         (\"rwkv\", \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None),\n         (\"seamless_m4t\", \"SeamlessM4TTokenizer\" if is_tokenizers_available() else None),\n         (\"seamless_m4t_v2\", \"SeamlessM4TTokenizer\" if is_tokenizers_available() else None),\n         (\"shieldgemma2\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n         (\"siglip\", \"SiglipTokenizer\" if is_sentencepiece_available() else None),\n         (\"siglip2\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"smollm3\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"smollm3\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"speech_to_text\", \"Speech2TextTokenizer\" if is_sentencepiece_available() else None),\n         (\"speecht5\", \"SpeechT5Tokenizer\" if is_sentencepiece_available() else None),\n         (\"splinter\", \"SplinterTokenizer\"),\n@@ -336,16 +335,16 @@\n         (\"tvp\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"udop\", \"UdopTokenizer\" if is_tokenizers_available() else None),\n         (\"umt5\", \"T5Tokenizer\" if is_tokenizers_available() else None),\n-        (\"video_llava\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"video_llava\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"vilt\", \"BertTokenizer\" if is_tokenizers_available() else None),\n-        (\"vipllava\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"vipllava\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"visual_bert\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"vits\", \"VitsTokenizer\"),\n         (\n             \"voxtral\",\n             \"MistralCommonBackend\"\n             if is_mistral_common_available()\n-            else (\"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+            else (\"LlamaTokenizer\" if is_tokenizers_available() else None),\n         ),\n         (\"wav2vec2\", \"Wav2Vec2CTCTokenizer\"),\n         (\"wav2vec2-bert\", \"Wav2Vec2CTCTokenizer\"),\n@@ -361,8 +360,8 @@\n         (\"xlstm\", \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None),\n         (\"xmod\", \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None),\n         (\"yoso\", \"AlbertTokenizer\" if is_tokenizers_available() else None),\n-        (\"zamba\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"zamba2\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"zamba\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n+        (\"zamba2\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n     ]\n )\n \n@@ -389,13 +388,17 @@ def load_merges(merges_file):\n \n \n def tokenizer_class_from_name(class_name: str) -> Union[type[Any], None]:\n+    # Bloom tokenizer classes were removed but should map to the fast backend for BC\n+    if class_name in {\"BloomTokenizer\", \"BloomTokenizerFast\"}:\n+        return TokenizersBackend\n+\n     if class_name in REGISTERED_FAST_ALIASES:\n         return REGISTERED_FAST_ALIASES[class_name]\n \n     if class_name in REGISTERED_TOKENIZER_CLASSES:\n         return REGISTERED_TOKENIZER_CLASSES[class_name]\n \n-    if class_name == \"PreTrainedTokenizerFast\":\n+    if class_name == \"TokenizersBackend\":\n         return TokenizersBackend\n \n     # V5: TOKENIZER_MAPPING_NAMES now maps to single strings, not tuples\n@@ -404,7 +407,7 @@ def tokenizer_class_from_name(class_name: str) -> Union[type[Any], None]:\n             module_name = model_type_to_module_name(module_name)\n             if (\n                 module_name in [\"mistral\", \"mistral3\", \"mixtral\", \"ministral\", \"ministral3\", \"pixtral\", \"voxtral\"]\n-                and class_name == \"MistralCommonTokenizer\"\n+                and class_name == \"MistralCommonBackend\"\n             ):\n                 module = importlib.import_module(\".tokenization_mistral_common\", \"transformers\")\n             else:\n@@ -428,402 +431,6 @@ def tokenizer_class_from_name(class_name: str) -> Union[type[Any], None]:\n     return None\n \n \n-def _find_sentencepiece_model_file(pretrained_model_name_or_path, **kwargs):\n-    # Delegate to shared helper to avoid duplication\n-    return find_sentencepiece_model_file(pretrained_model_name_or_path, **kwargs)\n-\n-\n-def _load_tokenizers_backend(tokenizer_class, pretrained_model_name_or_path, inputs, kwargs):\n-    \"\"\"\n-    Load a tokenizer using only the tokenizers backend (no SentencePiece fallback).\n-\n-    This function attempts to load with the following priority:\n-    1. If tokenizer.json exists, load directly\n-    2. If any .model file (SPM) exists, try extracting vocab and merges\n-    3. If vocab.json and merges.txt exist, load with those\n-    4. If vocab.txt exists (WordPiece models), load with that\n-\n-    Args:\n-        tokenizer_class: The tokenizer class to instantiate\n-        pretrained_model_name_or_path: Path or model id\n-        inputs: Additional positional arguments for tokenizer init\n-        kwargs: Additional keyword arguments\n-\n-    Returns:\n-        An instantiated tokenizer object\n-\n-    Raises:\n-        ValueError: If tokenizer could not be loaded with tokenizers backend\n-    \"\"\"\n-    files_loaded = []\n-\n-    # Try tokenizer.json first\n-    try:\n-        tokenizer_json_exists = has_file(\n-            pretrained_model_name_or_path,\n-            \"tokenizer.json\",\n-            revision=kwargs.get(\"revision\"),\n-            token=kwargs.get(\"token\"),\n-            cache_dir=kwargs.get(\"cache_dir\"),\n-            local_files_only=kwargs.get(\"local_files_only\", False),\n-        )\n-    except Exception:\n-        tokenizer_json_exists = False\n-\n-    if tokenizer_json_exists:\n-        files_loaded.append(\"tokenizer.json\")\n-        kwargs[\"backend\"] = \"tokenizers\"\n-        kwargs[\"files_loaded\"] = files_loaded\n-        # Some old models have uploaded a tokenizer.json but haven't updated tokenizer_config.json to point to the correct tokenizer class\n-        tokenizer_class = (\n-            TokenizersBackend\n-            if tokenizer_class.__name__ in (\"PythonBackend\", \"PreTrainedTokenizer\")\n-            else tokenizer_class\n-        )\n-        return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n-\n-    # Try tekken.json (Mistral format)\n-    try:\n-        if has_file(\n-            pretrained_model_name_or_path,\n-            \"tekken.json\",\n-            revision=kwargs.get(\"revision\"),\n-            token=kwargs.get(\"token\"),\n-            cache_dir=kwargs.get(\"cache_dir\"),\n-            local_files_only=kwargs.get(\"local_files_only\", False),\n-        ):\n-            from ...integrations.mistral import convert_tekken_tokenizer\n-\n-            tekken_file = cached_file(\n-                pretrained_model_name_or_path,\n-                \"tekken.json\",\n-                **{\n-                    k: v\n-                    for k, v in kwargs.items()\n-                    if k\n-                    in [\"cache_dir\", \"force_download\", \"proxies\", \"token\", \"revision\", \"local_files_only\", \"subfolder\"]\n-                },\n-            )\n-            if tekken_file is not None:\n-                files_loaded.append(\"tekken.json\")\n-                kwargs[\"backend\"] = \"tokenizers\"\n-                kwargs[\"files_loaded\"] = files_loaded\n-                return convert_tekken_tokenizer(tekken_file)\n-    except (ImportError, Exception):\n-        pass\n-\n-    # Try extracting from SentencePiece model\n-    spm_file = _find_sentencepiece_model_file(pretrained_model_name_or_path, **kwargs)\n-    if spm_file is not None:\n-        try:\n-            resolved_spm = cached_file(\n-                pretrained_model_name_or_path,\n-                spm_file,\n-                cache_dir=kwargs.get(\"cache_dir\"),\n-                force_download=kwargs.get(\"force_download\", False),\n-                proxies=kwargs.get(\"proxies\"),\n-                token=kwargs.get(\"token\"),\n-                revision=kwargs.get(\"revision\"),\n-                local_files_only=kwargs.get(\"local_files_only\", False),\n-                subfolder=kwargs.get(\"subfolder\", \"\"),\n-            )\n-        except Exception:\n-            resolved_spm = None\n-\n-        if resolved_spm is not None:\n-            try:\n-                from ...tokenization_utils_sentencepiece import SentencePieceExtractor\n-\n-                fast_sig = inspect.signature(getattr(tokenizer_class, \"__init__\", tokenizer_class))\n-                if \"vocab\" in fast_sig.parameters:\n-                    try:\n-                        vocab_ids, vocab_scores, merges = SentencePieceExtractor(resolved_spm).extract()\n-                        files_loaded.append(spm_file)\n-                        kwargs[\"backend\"] = \"tokenizers\"\n-                        kwargs[\"files_loaded\"] = files_loaded\n-                        # If tokenizer needs both vocab and merges (BPE models)\n-                        if \"merges\" in fast_sig.parameters:\n-                            return tokenizer_class.from_pretrained(\n-                                pretrained_model_name_or_path, *inputs, vocab=vocab_scores, merges=merges, **kwargs\n-                            )\n-                        # If tokenizer only needs vocab (Unigram models like NLLB, SeamlessM4T)\n-                        else:\n-                            return tokenizer_class.from_pretrained(\n-                                pretrained_model_name_or_path, *inputs, vocab=vocab_scores, **kwargs\n-                            )\n-                    except Exception:\n-                        pass\n-            except ImportError as e:\n-                if \"sentencepiece\" in str(e).lower() or \"SentencePiece\" in str(e):\n-                    raise ImportError(\n-                        f\"This checkpoint only contains a SentencePiece model file ({spm_file}), but the `sentencepiece` library is not installed. \"\n-                        f\"Please install sentencepiece to load this tokenizer: `pip install sentencepiece`\"\n-                    ) from e\n-                raise\n-            except Exception:\n-                pass\n-\n-    vocab, merges, loaded = load_vocab_and_merges(pretrained_model_name_or_path, **kwargs)\n-    if vocab is not None:\n-        files_loaded.extend(loaded)\n-        if issubclass(tokenizer_class, PreTrainedTokenizer):\n-            kwargs[\"backend\"] = \"python\"\n-        else:\n-            kwargs[\"backend\"] = \"tokenizers\"\n-        kwargs[\"files_loaded\"] = files_loaded\n-        if merges is not None:\n-            return tokenizer_class.from_pretrained(\n-                pretrained_model_name_or_path, *inputs, vocab=vocab, merges=merges, **kwargs\n-            )\n-        else:\n-            return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, vocab=vocab, **kwargs)\n-\n-    # Try vocab.txt (WordPiece models like SplinterTokenizer)\n-    try:\n-        resolved_vocab_txt = cached_file(\n-            pretrained_model_name_or_path,\n-            \"vocab.txt\",\n-            cache_dir=kwargs.get(\"cache_dir\"),\n-            force_download=kwargs.get(\"force_download\", False),\n-            proxies=kwargs.get(\"proxies\"),\n-            token=kwargs.get(\"token\"),\n-            revision=kwargs.get(\"revision\"),\n-            local_files_only=kwargs.get(\"local_files_only\", False),\n-            subfolder=kwargs.get(\"subfolder\", \"\"),\n-        )\n-    except Exception:\n-        resolved_vocab_txt = None\n-\n-    if resolved_vocab_txt is not None:\n-        try:\n-            fast_sig = inspect.signature(getattr(tokenizer_class, \"__init__\", tokenizer_class))\n-            if \"vocab\" in fast_sig.parameters:\n-                # Load vocab.txt: each line is a token, line number is the ID\n-                vocab = OrderedDict()\n-                with open(resolved_vocab_txt, \"r\", encoding=\"utf-8\") as reader:\n-                    tokens = reader.readlines()\n-                for index, token in enumerate(tokens):\n-                    token = token.rstrip(\"\\n\")\n-                    vocab[token] = index\n-                files_loaded.append(\"vocab.txt\")\n-                kwargs[\"backend\"] = \"tokenizers\"\n-                kwargs[\"files_loaded\"] = files_loaded\n-                return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, vocab=vocab, **kwargs)\n-        except Exception:\n-            pass\n-\n-    # If all methods failed, raise an error\n-    raise ValueError(\n-        f\"Could not load tokenizer from {pretrained_model_name_or_path} using tokenizers backend. \"\n-        \"No tokenizer.json, tekken.json, vocab.json+merges.txt, vocab.txt, or compatible SentencePiece model found.\"\n-    )\n-\n-\n-def _try_load_tokenizer_with_fallbacks(tokenizer_class, pretrained_model_name_or_path, inputs, kwargs):\n-    \"\"\"\n-    Try to load a tokenizer with backend selection.\n-\n-    This function routes to the appropriate backend based on the 'backend' parameter:\n-    - \"tokenizers\" (default): Uses HuggingFace tokenizers library backend\n-    - \"sentencepiece\": Uses SentencePiece backend\n-\n-    For the tokenizers backend, attempts to load with the following priority:\n-    1. If tokenizer.json exists, load directly\n-    2. If any .model file (SPM) exists, try extracting vocab and merges\n-    3. If vocab.json and merges.txt exist, load with those\n-    4. Fallback to SentencePieceBackend if available\n-\n-    Args:\n-        tokenizer_class: The tokenizer class to instantiate (can be None)\n-        pretrained_model_name_or_path: Path or model id\n-        inputs: Additional positional arguments for tokenizer init\n-        kwargs: Additional keyword arguments (may include 'backend' parameter, defaults to \"tokenizers\")\n-\n-    Returns:\n-        An instantiated tokenizer object\n-\n-    Raises:\n-        ValueError: If no tokenizer could be loaded\n-    \"\"\"\n-    # Extract the backend parameter - default to \"tokenizers\" to prioritize tokenizers backend\n-    backend = kwargs.pop(\"backend\", \"tokenizers\")\n-\n-    # Validate backend parameter\n-    if backend not in [\"sentencepiece\", \"tokenizers\"]:\n-        logger.warning(\n-            f\"Invalid backend '{backend}' specified. Valid options are 'tokenizers' or 'sentencepiece'. \"\n-            \"Defaulting to 'tokenizers' backend.\"\n-        )\n-        backend = \"tokenizers\"\n-\n-    # Route to SentencePiece backend if requested\n-    if backend == \"sentencepiece\":\n-        if SentencePieceBackend is None:\n-            raise ValueError(\n-                \"SentencePiece backend was requested but sentencepiece is not installed. \"\n-                \"Please install it with: pip install sentencepiece\"\n-            )\n-        logger.info(\"Loading tokenizer with SentencePiece backend\")\n-        # Track files loaded for SentencePiece backend\n-        spm_file = _find_sentencepiece_model_file(pretrained_model_name_or_path, **kwargs)\n-        files_loaded = [spm_file] if spm_file else []\n-        kwargs[\"backend\"] = \"sentencepiece\"\n-        kwargs[\"files_loaded\"] = files_loaded\n-        # Resolve the SPM file path and pass it as vocab_file\n-        if spm_file is not None:\n-            resolved_vocab_file = cached_file(\n-                pretrained_model_name_or_path,\n-                spm_file,\n-                cache_dir=kwargs.get(\"cache_dir\"),\n-                force_download=kwargs.get(\"force_download\", False),\n-                proxies=kwargs.get(\"proxies\"),\n-                token=kwargs.get(\"token\"),\n-                revision=kwargs.get(\"revision\"),\n-                local_files_only=kwargs.get(\"local_files_only\", False),\n-                subfolder=kwargs.get(\"subfolder\", \"\"),\n-            )\n-            kwargs[\"vocab_file\"] = resolved_vocab_file\n-        if isinstance(tokenizer_class, type) and issubclass(tokenizer_class, SentencePieceBackend):\n-            logger.info(\"Loading tokenizer with SentencePiece backend using tokenizer class\")\n-            return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n-        return SentencePieceBackend.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n-\n-    # Route to tokenizers backend (default)\n-    if backend == \"tokenizers\":\n-        if tokenizer_class is not None:\n-            # Check if tokenizer_class inherits from PreTrainedTokenizer (but not from TokenizersBackend/SentencePieceBackend)\n-            # These are edge cases with custom logic (e.g., BioGptTokenizer with Moses tokenization)\n-            from ...tokenization_python import PreTrainedTokenizer\n-\n-            # Build list of backend classes to check against\n-            backend_classes = [TokenizersBackend] if TokenizersBackend else []\n-            if SentencePieceBackend:\n-                backend_classes.append(SentencePieceBackend)\n-\n-            # Check if it's a custom PreTrainedTokenizer (not a backend class)\n-            is_custom_pre_trained = (\n-                isinstance(tokenizer_class, type)\n-                and issubclass(tokenizer_class, PreTrainedTokenizer)\n-                and not any(issubclass(tokenizer_class, bc) for bc in backend_classes)\n-                and tokenizer_class.__name__ not in (\"PythonBackend\", \"PreTrainedTokenizer\")\n-            )\n-\n-            # Check if it's a completely custom tokenizer (not PreTrainedTokenizer, not backend class)\n-            # e.g., MistralCommonBackend which has its own from_pretrained logic\n-            inherits_from_backend = isinstance(tokenizer_class, type) and any(\n-                bc and issubclass(tokenizer_class, bc) for bc in backend_classes\n-            )\n-            is_completely_custom = (\n-                isinstance(tokenizer_class, type)\n-                and not issubclass(tokenizer_class, PythonBackend)\n-                and not inherits_from_backend\n-            )\n-\n-            if is_custom_pre_trained:\n-                logger.info(\"Loading tokenizer with custom PreTrainedTokenizer backend (edge case)\")\n-                # Track the backend type for custom tokenizers\n-                kwargs[\"backend\"] = \"custom\"\n-                kwargs[\"files_loaded\"] = []  # Custom tokenizers may load various files\n-                return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n-\n-            if is_completely_custom:\n-                # For completely custom tokenizers (like MistralCommonBackend), try calling from_pretrained directly\n-                logger.info(\"Loading tokenizer with custom tokenizer class (non-PreTrainedTokenizer)\")\n-                # Filter out AutoTokenizer-specific kwargs that custom tokenizers don't accept\n-                custom_kwargs = {k: v for k, v in kwargs.items() if k not in [\"backend\", \"files_loaded\"]}\n-                custom_kwargs[\"_from_auto\"] = True  # Signal that this is called from AutoTokenizer\n-                return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **custom_kwargs)\n-\n-            if TokenizersBackend is None:\n-                raise ValueError(\n-                    \"Tokenizers backend is the default but tokenizers library is not installed. \"\n-                    \"Please install it with: pip install tokenizers\"\n-                )\n-            logger.info(\"Loading tokenizer with tokenizers backend\")\n-            try:\n-                return _load_tokenizers_backend(tokenizer_class, pretrained_model_name_or_path, inputs, kwargs)\n-            except ValueError as e:\n-                # If tokenizers backend fails, try falling back to SentencePiece backend if available\n-                spm_file = _find_sentencepiece_model_file(pretrained_model_name_or_path, **kwargs)\n-                if spm_file is not None and SentencePieceBackend is not None:\n-                    logger.info(\n-                        f\"Tokenizers backend failed: {e}. \"\n-                        f\"Falling back to SentencePieceBackend since {spm_file} file was found.\"\n-                    )\n-                    files_loaded = [spm_file]\n-                    kwargs[\"backend\"] = \"sentencepiece\"\n-                    kwargs[\"files_loaded\"] = files_loaded\n-                    # Resolve the SPM file path and pass it as vocab_file\n-                    resolved_vocab_file = cached_file(\n-                        pretrained_model_name_or_path,\n-                        spm_file,\n-                        cache_dir=kwargs.get(\"cache_dir\"),\n-                        force_download=kwargs.get(\"force_download\", False),\n-                        proxies=kwargs.get(\"proxies\"),\n-                        token=kwargs.get(\"token\"),\n-                        revision=kwargs.get(\"revision\"),\n-                        local_files_only=kwargs.get(\"local_files_only\", False),\n-                        subfolder=kwargs.get(\"subfolder\", \"\"),\n-                    )\n-                    kwargs[\"vocab_file\"] = resolved_vocab_file\n-                    if tokenizer_class is not None and issubclass(tokenizer_class, SentencePieceBackend):\n-                        logger.info(\n-                            \"Falling back to SentencePiece backend using tokenizer class that inherits from SentencePieceBackend.\"\n-                        )\n-                        return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n-                    return SentencePieceBackend.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n-                # If no fallback available, try calling tokenizer class directly as last resort\n-                if hasattr(tokenizer_class, \"from_pretrained\"):\n-                    logger.info(\n-                        f\"Tokenizers backend failed: {e}. Trying to load tokenizer directly from tokenizer class.\"\n-                    )\n-                    # Filter out AutoTokenizer-specific kwargs that custom tokenizers don't accept\n-                    custom_kwargs = {k: v for k, v in kwargs.items() if k not in [\"backend\", \"files_loaded\"]}\n-                    custom_kwargs[\"_from_auto\"] = True  # Signal that this is called from AutoTokenizer\n-                    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **custom_kwargs)\n-                # Re-raise if no fallback options available\n-                raise\n-\n-        # If no tokenizer class but tokenizers backend requested, fall back to SentencePiece if available\n-        spm_file = _find_sentencepiece_model_file(pretrained_model_name_or_path, **kwargs)\n-        if spm_file is not None and SentencePieceBackend is not None:\n-            logger.info(\n-                f\"Tokenizers backend was requested but no tokenizer class found. \"\n-                f\"Falling back to SentencePieceBackend since {spm_file} file was found.\"\n-            )\n-            files_loaded = [spm_file]\n-            kwargs[\"backend\"] = \"sentencepiece\"\n-            kwargs[\"files_loaded\"] = files_loaded\n-            # Resolve the SPM file path and pass it as vocab_file\n-            resolved_vocab_file = cached_file(\n-                pretrained_model_name_or_path,\n-                spm_file,\n-                cache_dir=kwargs.get(\"cache_dir\"),\n-                force_download=kwargs.get(\"force_download\", False),\n-                proxies=kwargs.get(\"proxies\"),\n-                token=kwargs.get(\"token\"),\n-                revision=kwargs.get(\"revision\"),\n-                local_files_only=kwargs.get(\"local_files_only\", False),\n-                subfolder=kwargs.get(\"subfolder\", \"\"),\n-            )\n-            kwargs[\"vocab_file\"] = resolved_vocab_file\n-            if (\n-                tokenizer_class is not None\n-                and SentencePieceBackend is not None\n-                and issubclass(tokenizer_class, SentencePieceBackend)\n-            ):\n-                logger.info(\n-                    \"Falling back to SentencePiece backend using tokenizer class that inherits from SentencePieceBackend.\"\n-                )\n-                return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n-            return SentencePieceBackend.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n-\n-        raise ValueError(\n-            f\"Could not load tokenizer from {pretrained_model_name_or_path}. \"\n-            \"No tokenizer class could be determined and no SentencePiece model found.\"\n-        )\n-\n-\n def get_tokenizer_config(\n     pretrained_model_name_or_path: Union[str, os.PathLike[str]],\n     cache_dir: Optional[Union[str, os.PathLike[str]]] = None,\n@@ -1084,7 +691,7 @@ def from_pretrained(\n \n         if (\n             config_tokenizer_class is not None\n-            and config_tokenizer_class != \"PreTrainedTokenizerFast\"\n+            and config_tokenizer_class != \"TokenizersBackend\"\n             and \"Fast\" in config_tokenizer_class\n         ):\n             config_tokenizer_class = config_tokenizer_class[:-4]\n@@ -1125,10 +732,12 @@ def from_pretrained(\n                 tokenizer_class = tokenizer_class_from_name(tokenizer_class_candidate)\n                 if tokenizer_class is None and not tokenizer_class_candidate.endswith(\"Fast\"):\n                     tokenizer_class = tokenizer_class_from_name(tokenizer_class_candidate + \"Fast\")\n+                if tokenizer_class.__name__ == \"PythonBackend\":  # unless you inherit from it?\n+                    tokenizer_class = TokenizersBackend\n             else:\n                 tokenizer_class = fast_tokenizer_class\n \n-            return _try_load_tokenizer_with_fallbacks(tokenizer_class, pretrained_model_name_or_path, inputs, kwargs)\n+            return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n \n         # Otherwise we have to be creative.\n         # if model is an encoder decoder, the encoder tokenizer class is used by default\n@@ -1144,17 +753,9 @@ def from_pretrained(\n \n         model_type = config_class_to_model_type(type(config).__name__)\n         if model_type is not None:\n-            tokenizer_class = TOKENIZER_MAPPING[type(config)]\n-\n+            tokenizer_class = TOKENIZER_MAPPING.get(type(config), TokenizersBackend)\n             if tokenizer_class is not None:\n-                return _try_load_tokenizer_with_fallbacks(\n-                    tokenizer_class, pretrained_model_name_or_path, inputs, kwargs\n-                )\n-            else:\n-                raise ValueError(\n-                    \"This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \"\n-                    \"in order to use this tokenizer.\"\n-                )\n+                return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n \n         raise ValueError(\n             f\"Unrecognized configuration class {config.__class__} to build an AutoTokenizer.\\n\""
        },
        {
            "sha": "b6e92391841144fb62e296797e1b8c3234ce763b",
            "filename": "src/transformers/models/barthez/tokenization_barthez.py",
            "status": "modified",
            "additions": 5,
            "deletions": 10,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -14,6 +14,8 @@\n # limitations under the License\n \"\"\"Tokenization classes for the BARThez model.\"\"\"\n \n+from typing import Optional, Union\n+\n from tokenizers import Regex, Tokenizer, decoders, normalizers, pre_tokenizers\n from tokenizers.models import Unigram\n \n@@ -77,7 +79,7 @@ class BarthezTokenizer(TokenizersBackend):\n         vocab_file (`str`, *optional*):\n             [SentencePiece](https://github.com/google/sentencepiece) file (generally has a *.spm* extension) that\n             contains the vocabulary necessary to instantiate a tokenizer.\n-        vocab (`dict`, *optional*):\n+        vocab (`str`, `dict` or `list`, *optional*):\n             Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n         add_prefix_space (`bool`, *optional*, defaults to `True`):\n             Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n@@ -90,22 +92,20 @@ class BarthezTokenizer(TokenizersBackend):\n \n     def __init__(\n         self,\n+        vocab: Optional[Union[str, dict, list]] = None,\n         bos_token=\"<s>\",\n         eos_token=\"</s>\",\n         sep_token=\"</s>\",\n         cls_token=\"<s>\",\n         unk_token=\"<unk>\",\n         pad_token=\"<pad>\",\n         mask_token=\"<mask>\",\n-        vocab_file=None,\n-        vocab=None,\n         add_prefix_space=True,\n         **kwargs,\n     ):\n         # Mask token behave like a normal word, i.e. include the space before it\n         mask_token = AddedToken(mask_token, lstrip=True, rstrip=False) if isinstance(mask_token, str) else mask_token\n         self.add_prefix_space = add_prefix_space\n-        self.vocab_file = vocab_file\n \n         if vocab is not None:\n             self._vocab = vocab\n@@ -122,10 +122,7 @@ def __init__(\n \n         self._tokenizer.normalizer = normalizers.Sequence(\n             [\n-                normalizers.Replace(\"\\n\", \" \"),\n-                normalizers.Replace(\"\\r\", \" \"),\n-                normalizers.Replace(\"\\t\", \" \"),\n-                normalizers.Replace(Regex(r\" {2,}\"), \" \"),\n+                normalizers.Replace(Regex(r\"\\s{2,}|[\\n\\r\\t]\"), \" \"),\n                 normalizers.NFC(),\n                 normalizers.Strip(left=False, right=True),\n             ]\n@@ -134,9 +131,7 @@ def __init__(\n         self._tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(replacement=\"▁\", prepend_scheme=prepend_scheme)\n         self._tokenizer.decoder = decoders.Metaspace(replacement=\"▁\", prepend_scheme=prepend_scheme)\n \n-        tokenizer_object = self._tokenizer\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             bos_token=bos_token,\n             eos_token=eos_token,\n             unk_token=unk_token,"
        },
        {
            "sha": "361776b72a329dd6af2876374f0e7d005451403b",
            "filename": "src/transformers/models/bert/tokenization_bert.py",
            "status": "modified",
            "additions": 8,
            "deletions": 21,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"Tokenization classes for Bert.\"\"\"\n \n import collections\n-from typing import Optional\n+from typing import Optional, Union\n \n from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import WordPiece\n@@ -48,8 +48,8 @@ class BertTokenizer(TokenizersBackend):\n     this superclass for more information regarding those methods.\n \n     Args:\n-        vocab_file (`str`, *optional*):\n-            File containing the vocabulary.\n+        vocab (`str` or `dict[str, int]`, *optional*):\n+            Custom vocabulary dictionary. If not provided, vocabulary is loaded from `vocab_file`.\n         do_lower_case (`bool`, *optional*, defaults to `False`):\n             Whether or not to lowercase the input when tokenizing.\n         unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n@@ -72,17 +72,15 @@ class BertTokenizer(TokenizersBackend):\n         strip_accents (`bool`, *optional*):\n             Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n             value for `lowercase` (as in the original BERT).\n-        vocab (`dict`, *optional*):\n-            Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n+    model = WordPiece\n \n     def __init__(\n         self,\n-        vocab_file: Optional[str] = None,\n+        vocab: Optional[Union[str, dict[str, int]]] = None,\n         do_lower_case: bool = False,\n         unk_token: str = \"[UNK]\",\n         sep_token: str = \"[SEP]\",\n@@ -91,28 +89,21 @@ def __init__(\n         mask_token: str = \"[MASK]\",\n         tokenize_chinese_chars: bool = True,\n         strip_accents: Optional[bool] = None,\n-        vocab: Optional[dict] = None,\n         **kwargs,\n     ):\n         self.do_lower_case = do_lower_case\n         self.tokenize_chinese_chars = tokenize_chinese_chars\n         self.strip_accents = strip_accents\n-\n-        if vocab is not None:\n-            self._vocab = (\n-                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n-            )\n-        else:\n-            self._vocab = {\n+        if vocab is None:\n+            vocab = {\n                 str(pad_token): 0,\n                 str(unk_token): 1,\n                 str(cls_token): 2,\n                 str(sep_token): 3,\n                 str(mask_token): 4,\n             }\n-\n+        self._vocab = vocab\n         self._tokenizer = Tokenizer(WordPiece(self._vocab, unk_token=str(unk_token)))\n-\n         self._tokenizer.normalizer = normalizers.BertNormalizer(\n             clean_text=True,\n             handle_chinese_chars=tokenize_chinese_chars,\n@@ -121,11 +112,7 @@ def __init__(\n         )\n         self._tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n         self._tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n-\n-        tokenizer_object = self._tokenizer\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             do_lower_case=do_lower_case,\n             unk_token=unk_token,\n             sep_token=sep_token,"
        },
        {
            "sha": "7bc55a9fe162f3bcd76664e4100de24297d78924",
            "filename": "src/transformers/models/big_bird/tokenization_big_bird.py",
            "status": "modified",
            "additions": 18,
            "deletions": 42,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -14,6 +14,8 @@\n # limitations under the License.\n \"\"\"Tokenization classes for Big Bird model.\"\"\"\n \n+from typing import Optional, Union\n+\n from tokenizers import Regex, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import Unigram\n \n@@ -37,7 +39,7 @@ class BigBirdTokenizer(TokenizersBackend):\n     this superclass for more information regarding those methods\n \n     Args:\n-        vocab (`dict`, *optional*):\n+        vocab (`str`, `dict` or `list`, *optional*):\n             Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n         unk_token (`str`, *optional*, defaults to `\"<unk>\"`):\n             The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n@@ -80,10 +82,11 @@ class BigBirdTokenizer(TokenizersBackend):\n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n     prefix_tokens: list[int] = []\n+    model = Unigram\n \n     def __init__(\n         self,\n-        vocab=None,\n+        vocab: Optional[Union[str, dict, list]] = None,\n         unk_token=\"<unk>\",\n         bos_token=\"<s>\",\n         eos_token=\"</s>\",\n@@ -92,8 +95,6 @@ def __init__(\n         mask_token=\"[MASK]\",\n         cls_token=\"[CLS]\",\n         add_prefix_space=True,\n-        vocab_file=None,\n-        tokenizer_file=None,\n         **kwargs,\n     ):\n         bos_token = AddedToken(bos_token, lstrip=False, rstrip=False) if isinstance(bos_token, str) else bos_token\n@@ -105,47 +106,18 @@ def __init__(\n         mask_token = AddedToken(mask_token, lstrip=True, rstrip=False) if isinstance(mask_token, str) else mask_token\n \n         self.add_prefix_space = add_prefix_space\n-        self.vocab_file = vocab_file\n \n         # Convert vocab to list of (token, score) tuples\n         if vocab is None:\n-            vocab_scores = [(str(pad_token), 0.0), (str(eos_token), 0.0), (str(bos_token), 0.0)]\n-        elif isinstance(vocab, dict):\n-            vocab_scores = [(str(token), float(score)) for token, score in vocab.items()]\n-        elif isinstance(vocab, list) and len(vocab) > 0:\n-            if isinstance(vocab[0], (tuple, list)):\n-                vocab_scores = [(str(token), float(score)) for token, score in vocab]\n-            else:\n-                vocab_scores = [(str(token), 0.0) for token in vocab]\n-        else:\n-            vocab_scores = [(str(pad_token), 0.0), (str(eos_token), 0.0), (str(bos_token), 0.0)]\n-\n-        # Find unk_id in vocab\n-        unk_token_content = str(unk_token)\n-        unk_id = next((idx for idx, (token, _) in enumerate(vocab_scores) if token == unk_token_content), None)\n-        if unk_id is None:\n-            unk_id = min(len(vocab_scores), 100)\n-            if len(vocab_scores) > 100:\n-                vocab_scores.insert(100, (unk_token_content, 0.0))\n-            else:\n-                vocab_scores.append((unk_token_content, 0.0))\n-\n-        # Ensure cls_token and sep_token are in vocab\n-        cls_token_str = str(cls_token)\n-        sep_token_str = str(sep_token)\n-        cls_token_id = next((idx for idx, (token, _) in enumerate(vocab_scores) if token == cls_token_str), None)\n-        sep_token_id = next((idx for idx, (token, _) in enumerate(vocab_scores) if token == sep_token_str), None)\n+            vocab = [(str(pad_token), 0.0), (str(eos_token), 0.0), (str(bos_token), 0.0), (str(unk_token), 0.0)]\n+            unk_id = 3\n+        elif isinstance(vocab, list):\n+            # vocab.insert(100, (str(unk_token), 0.0))  # Ensure unk_token is in vocab at index 100\n+            unk_id = vocab.index((str(unk_token), 0.0)) if (str(unk_token), 0.0) in vocab else 100\n \n-        if cls_token_id is None:\n-            cls_token_id = len(vocab_scores)\n-            vocab_scores.append((cls_token_str, 0.0))\n-        if sep_token_id is None:\n-            sep_token_id = len(vocab_scores)\n-            vocab_scores.append((sep_token_str, 0.0))\n-\n-        self._tokenizer = Tokenizer(Unigram(vocab_scores, unk_id=unk_id, byte_fallback=False))\n+        self._tokenizer = Tokenizer(Unigram(vocab, unk_id=unk_id, byte_fallback=False))\n         self._tokenizer.normalizer = normalizers.Sequence(\n-            [normalizers.Strip(left=False, right=True), normalizers.Replace(Regex(r\" {2,}\"), SPIECE_UNDERLINE)]\n+            [normalizers.Strip(left=False, right=False), normalizers.Replace(Regex(r\" {2,}\"), SPIECE_UNDERLINE)]\n         )\n \n         prepend_scheme = \"always\" if add_prefix_space else \"never\"\n@@ -155,18 +127,22 @@ def __init__(\n         self._tokenizer.decoder = decoders.Metaspace(replacement=\"▁\", prepend_scheme=prepend_scheme, split=True)\n \n         super().__init__(\n-            tokenizer_object=self._tokenizer,\n             bos_token=bos_token,\n             eos_token=eos_token,\n             unk_token=unk_token,\n             pad_token=pad_token,\n             mask_token=mask_token,\n             cls_token=cls_token,\n             sep_token=sep_token,\n+            add_prefix_space=add_prefix_space,\n             **kwargs,\n         )\n \n-        self.init_kwargs[\"add_prefix_space\"] = add_prefix_space\n+        # Ensure cls_token and sep_token are in vocab\n+        cls_token_str = str(cls_token)\n+        sep_token_str = str(sep_token)\n+        cls_token_id = self.cls_token_id\n+        sep_token_id = self.sep_token_id\n \n         self._tokenizer.post_processor = processors.TemplateProcessing(\n             single=f\"{cls_token_str}:0 $A:0 {sep_token_str}:0\","
        },
        {
            "sha": "88b2ad7c0d3eba7fd730c401bfb7a71917717094",
            "filename": "src/transformers/models/blenderbot/tokenization_blenderbot.py",
            "status": "modified",
            "additions": 12,
            "deletions": 16,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -102,14 +102,15 @@ class BlenderbotTokenizer(TokenizersBackend):\n         add_prefix_space (`bool`, *optional*, defaults to `True`):\n             Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n             other word. (Blenderbot tokenizer detect beginning of words by the preceding space).\n-        vocab (`dict`, *optional*):\n-            Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n-        merges (`list`, *optional*):\n-            Custom merges list. If not provided, merges are loaded from merges_file.\n+        vocab (`str` or `dict[str, int]`, *optional*):\n+            Custom vocabulary dictionary. If not provided, vocabulary is loaded from `vocab_file`.\n+        merges (`str` or `list[str]`, *optional*):\n+            Custom merges list. If not provided, merges are loaded from `merges_file`.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n+    model = BPE\n \n     def __init__(\n         self,\n@@ -132,22 +133,20 @@ def __init__(\n             else mask_token\n         )\n \n-        if vocab is not None and merges is not None:\n-            self._vocab = (\n-                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n-            )\n-            self._merges = merges\n-        else:\n-            # Initialize with minimal vocab\n-            self._vocab = {\n+        # Initialize vocab and merges; when not provided fall back to minimal vocab\n+        self._vocab = (\n+            vocab\n+            if vocab is not None\n+            else {\n                 str(bos_token): 0,\n                 str(pad_token): 1,\n                 str(eos_token): 2,\n                 str(unk_token): 3,\n                 str(mask_token): 4,\n             }\n-            self._merges = []\n+        )\n \n+        self._merges = merges or []\n         self._tokenizer = Tokenizer(\n             BPE(\n                 vocab=self._vocab,\n@@ -168,10 +167,7 @@ def __init__(\n             trim_offsets=True,\n         )\n \n-        tokenizer_object = self._tokenizer\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             bos_token=bos_token,\n             eos_token=eos_token,\n             sep_token=sep_token,"
        },
        {
            "sha": "a9ca0d33b6f1290ed8ec3196c33cedce685717e6",
            "filename": "src/transformers/models/camembert/tokenization_camembert.py",
            "status": "modified",
            "additions": 8,
            "deletions": 12,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -14,6 +14,8 @@\n # limitations under the License\n \"\"\"Tokenization classes for Camembert model.\"\"\"\n \n+from typing import Optional, Union\n+\n from tokenizers import Regex, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import Unigram\n \n@@ -83,7 +85,7 @@ class CamembertTokenizer(TokenizersBackend):\n         vocab_file (`str`, *optional*):\n             [SentencePiece](https://github.com/google/sentencepiece) file (generally has a *.spm* extension) that\n             contains the vocabulary necessary to instantiate a tokenizer.\n-        vocab (`dict`, *optional*):\n+        vocab (`str`, `dict` or `list`, *optional*):\n             Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n     \"\"\"\n \n@@ -103,7 +105,7 @@ def __init__(\n         additional_special_tokens=None,\n         add_prefix_space=True,\n         vocab_file=None,\n-        vocab=None,\n+        vocab: Optional[Union[str, dict, list]] = None,\n         **kwargs,\n     ):\n         self.vocab_file = vocab_file\n@@ -114,9 +116,9 @@ def __init__(\n         if additional_special_tokens is None:\n             additional_special_tokens = [\"<s>NOTUSED\", \"</s>NOTUSED\", \"<unk>NOTUSED\"]\n \n-        if vocab is not None and isinstance(vocab, list):\n-            self._vocab = list(vocab)\n-            unk_index = next(i for i, (tok, _) in enumerate(self._vocab) if tok == str(unk_token))\n+        if vocab is not None:\n+            self._vocab = vocab\n+            unk_index = next((i for i, (tok, _) in enumerate(self._vocab) if tok == str(unk_token)), 0)\n             self._tokenizer = Tokenizer(Unigram(self._vocab, unk_id=unk_index, byte_fallback=False))\n         else:\n             self._vocab = [\n@@ -131,22 +133,16 @@ def __init__(\n \n         self._tokenizer.normalizer = normalizers.Sequence(\n             [\n-                normalizers.Replace(\"\\n\", \" \"),\n-                normalizers.Replace(\"\\r\", \" \"),\n-                normalizers.Replace(\"\\t\", \" \"),\n+                normalizers.Replace(Regex(r\"\\s{2,}|[\\n\\r\\t]\"), \" \"),\n                 normalizers.Strip(left=False, right=True),\n-                normalizers.Replace(Regex(\" {2,}\"), \"▁\"),\n             ]\n         )\n \n         prepend_scheme = \"always\" if add_prefix_space else \"never\"\n         self._tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(replacement=\"▁\", prepend_scheme=prepend_scheme)\n         self._tokenizer.decoder = decoders.Metaspace(replacement=\"▁\", prepend_scheme=prepend_scheme)\n \n-        tokenizer_object = self._tokenizer\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             bos_token=bos_token,\n             eos_token=eos_token,\n             sep_token=sep_token,"
        },
        {
            "sha": "4f7030312431a8d1467db7429d707c5811d8150a",
            "filename": "src/transformers/models/clip/tokenization_clip.py",
            "status": "modified",
            "additions": 22,
            "deletions": 44,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Tokenization classes for CLIP.\"\"\"\n \n-from typing import Optional\n+from typing import Optional, Union\n \n from tokenizers import Regex, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import BPE\n@@ -37,6 +37,10 @@ class CLIPTokenizer(TokenizersBackend):\n     refer to this superclass for more information regarding those methods.\n \n     Args:\n+        vocab (`str`, `dict` or `list`, *optional*):\n+            Vocabulary dict to use for the tokenizer.\n+        merges (`str` or `list`, *optional*):\n+            Merges list to use for the BPE tokenizer.\n         unk_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n             The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n             token instead.\n@@ -46,53 +50,38 @@ class CLIPTokenizer(TokenizersBackend):\n             The end of sequence token.\n         pad_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n             The token used for padding, for example when batching sequences of different lengths.\n-        vocab (`dict`, *optional*):\n-            Vocabulary dict to use for the tokenizer.\n-        merges (`list`, *optional*):\n-            Merges list to use for the BPE tokenizer.\n-        vocab_file (`str`, *optional*):\n-            Path to the vocabulary file.\n-        merges_file (`str`, *optional*):\n-            Path to the merges file.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n+    model = BPE\n \n     def __init__(\n         self,\n+        vocab: Optional[Union[str, dict[str, int]]] = None,\n+        merges: Optional[Union[str, list[str]]] = None,\n         unk_token: str = \"<|endoftext|>\",\n         bos_token: str = \"<|startoftext|>\",\n         eos_token: str = \"<|endoftext|>\",\n         pad_token: str = \"<|endoftext|>\",\n-        vocab: Optional[dict] = None,\n-        merges: Optional[list] = None,\n-        vocab_file: Optional[str] = None,\n-        merges_file: Optional[str] = None,\n         **kwargs,\n     ):\n-        self.vocab_file = vocab_file\n-        self.merges_file = merges_file\n-\n-        if vocab is not None:\n-            _vocab = {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n-        else:\n-            _vocab = {\n+        _vocab = (\n+            vocab\n+            if vocab is not None\n+            else {\n                 str(bos_token): 0,\n                 str(eos_token): 1,\n                 str(pad_token): 2,\n             }\n+        )\n \n-        if merges is not None:\n-            _merges = [tuple(merge) if isinstance(merge, list) else merge for merge in merges]\n-        else:\n-            _merges = []\n+        self._merges = merges or []\n \n         self._tokenizer = Tokenizer(\n             BPE(\n                 vocab=_vocab,\n-                merges=_merges,\n+                merges=self._merges,\n                 dropout=None,\n                 continuing_subword_prefix=\"\",\n                 end_of_word_suffix=\"</w>\",\n@@ -120,35 +109,24 @@ def __init__(\n \n         self._tokenizer.decoder = decoders.ByteLevel()\n \n-        bos_token_id = _vocab.get(str(bos_token), 0)\n-        eos_token_id = _vocab.get(str(eos_token), 1)\n-\n-        self._tokenizer.post_processor = processors.RobertaProcessing(\n-            sep=(str(eos_token), eos_token_id),\n-            cls=(str(bos_token), bos_token_id),\n-            add_prefix_space=False,\n-            trim_offsets=False,\n-        )\n-\n-        tokenizer_object = self._tokenizer\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             unk_token=unk_token,\n             bos_token=bos_token,\n             eos_token=eos_token,\n             pad_token=pad_token,\n             **kwargs,\n         )\n \n-        if hasattr(self, \"_post_init\"):\n-            self._post_init()\n+        self._tokenizer.post_processor = processors.RobertaProcessing(\n+            sep=(str(eos_token), self.eos_token_id),\n+            cls=(str(bos_token), self.bos_token_id),\n+            add_prefix_space=False,\n+            trim_offsets=False,\n+        )\n \n-    def _post_init(self):\n-        super()._post_init()\n+        # Very ugly hack to enable padding to have a correct decoding see https://github.com/huggingface/tokenizers/issues/872\n         self._wrap_decode_method_backend_tokenizer()\n \n-    # Very ugly hack to enable padding to have a correct decoding see https://github.com/huggingface/tokenizers/issues/872\n     def _wrap_decode_method_backend_tokenizer(self):\n         orig_decode_method = self.backend_tokenizer.decode\n "
        },
        {
            "sha": "0e2a5d7737f5e830b52162ab01ef1ba5bc3b2cbf",
            "filename": "src/transformers/models/clvp/tokenization_clvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 63,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fclvp%2Ftokenization_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fclvp%2Ftokenization_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Ftokenization_clvp.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -15,9 +15,7 @@\n \"\"\"Tokenization class for CLVP.\"\"\"\n \n import json\n-import os\n from functools import lru_cache\n-from typing import Optional\n \n import regex as re\n \n@@ -123,10 +121,6 @@ class ClvpTokenizer(PreTrainedTokenizer):\n         add_prefix_space (`bool`, *optional*, defaults to `False`):\n             Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n             other word. (CLVP tokenizer detect beginning of words by the preceding space).\n-        add_bos_token (`bool`, *optional*, defaults to `False`):\n-            Whether to add `bos_token` in front of the sequence when add_special_tokens=True.\n-        add_eos_token (`bool`, *optional*, defaults to `False`):\n-            Whether to add `eos_token` in end of the sequence when add_special_tokens=True.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n@@ -145,29 +139,14 @@ def __init__(\n         eos_token=\"[STOP]\",\n         pad_token=\"[STOP]\",\n         add_prefix_space=False,\n-        add_bos_token=False,\n-        add_eos_token=False,\n         **kwargs,\n     ):\n         bos_token = AddedToken(bos_token, special=True) if isinstance(bos_token, str) else bos_token\n         eos_token = AddedToken(eos_token, special=True) if isinstance(eos_token, str) else eos_token\n         unk_token = AddedToken(unk_token, special=True) if isinstance(unk_token, str) else unk_token\n         pad_token = AddedToken(pad_token, special=True) if isinstance(pad_token, str) else pad_token\n \n-        self.add_bos_token = add_bos_token\n-        self.add_eos_token = add_eos_token\n         self._normalizer = None\n-\n-        # Set special_tokens_pattern based on add_bos_token and add_eos_token flags\n-        if add_bos_token and add_eos_token:\n-            kwargs[\"special_tokens_pattern\"] = \"bos_eos\"\n-        elif add_bos_token:\n-            kwargs[\"special_tokens_pattern\"] = \"bos\"\n-        elif add_eos_token:\n-            kwargs[\"special_tokens_pattern\"] = \"eos\"\n-        else:\n-            kwargs[\"special_tokens_pattern\"] = \"none\"\n-\n         with open(vocab_file, encoding=\"utf-8\") as vocab_handle:\n             self.encoder = json.load(vocab_handle)\n         self.decoder = {v: k for k, v in self.encoder.items()}\n@@ -191,8 +170,7 @@ def __init__(\n             eos_token=eos_token,\n             pad_token=pad_token,\n             add_prefix_space=add_prefix_space,\n-            add_bos_token=add_bos_token,\n-            add_eos_token=add_eos_token,\n+            special_tokens_pattern=\"none\",\n             **kwargs,\n         )\n \n@@ -251,17 +229,6 @@ def bpe(self, token):\n         self.cache[token] = word\n         return word\n \n-    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n-        bos_token_id = [self.bos_token_id] if self.add_bos_token else []\n-        eos_token_id = [self.eos_token_id] if self.add_eos_token else []\n-\n-        output = bos_token_id + token_ids_0 + eos_token_id\n-\n-        if token_ids_1 is not None:\n-            output = output + bos_token_id + token_ids_1 + eos_token_id\n-\n-        return output\n-\n     def _tokenize(self, text):\n         \"\"\"Tokenize a string.\"\"\"\n         bpe_tokens = []\n@@ -303,34 +270,5 @@ def clean_up_tokenization(self, text):\n         text = text.replace(self.unk_token, \"\").replace(\"   \", \" \").replace(\"  \", \" \")\n         return text\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-        merge_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"merges_file\"]\n-        )\n-\n-        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n-            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + \"\\n\")\n-\n-        index = 0\n-        with open(merge_file, \"w\", encoding=\"utf-8\") as writer:\n-            writer.write(\"#version: 0.2\\n\")\n-            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):\n-                if index != token_index:\n-                    logger.warning(\n-                        f\"Saving vocabulary to {merge_file}: BPE merge indices are not consecutive.\"\n-                        \" Please check that the tokenizer is not corrupted!\"\n-                    )\n-                    index = token_index\n-                writer.write(\" \".join(bpe_tokens) + \"\\n\")\n-                index += 1\n-\n-        return vocab_file, merge_file\n-\n \n __all__ = [\"ClvpTokenizer\"]"
        },
        {
            "sha": "af08f3e0f172f4bf62aa91a0bb99dbff62c4ab0f",
            "filename": "src/transformers/models/code_llama/tokenization_code_llama.py",
            "status": "modified",
            "additions": 20,
            "deletions": 43,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -14,10 +14,11 @@\n # limitations under the License.\n \n \n-from tokenizers import AddedToken, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n+from typing import Optional, Union\n+\n+from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import BPE\n \n-from ...tokenization_utils_base import _get_prepend_scheme, generate_merges\n from ...tokenization_utils_tokenizers import TokenizersBackend\n from ...utils import logging\n \n@@ -97,9 +98,9 @@ class CodeLlamaTokenizer(TokenizersBackend):\n         add_prefix_space (`bool`, *optional*):\n             Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n             other word.\n-        vocab (`dict`, *optional*):\n+        vocab (`str`, `dict` or `list`, *optional*):\n             Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n-        merges (`list`, *optional*):\n+        merges (`str` or `list`, *optional*):\n             Custom merges list. If not provided, merges are loaded from merges_file.\n         vocab_file (`str`, *optional*):\n             [SentencePiece](https://github.com/google/sentencepiece) file (generally has a .model extension) that\n@@ -109,9 +110,12 @@ class CodeLlamaTokenizer(TokenizersBackend):\n     vocab_files_names = VOCAB_FILES_NAMES\n     padding_side = \"left\"\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n+    model = BPE\n \n     def __init__(\n         self,\n+        vocab: Optional[Union[str, dict[str, int]]] = None,\n+        merges: Optional[Union[str, list[str]]] = None,\n         clean_up_tokenization_spaces=False,\n         unk_token=\"<unk>\",\n         bos_token=\"<s>\",\n@@ -122,37 +126,28 @@ def __init__(\n         eot_token=\"▁<EOT>\",\n         fill_token=\"<FILL_ME>\",\n         additional_special_tokens=None,\n-        add_bos_token=True,\n-        add_eos_token=False,\n-        use_default_system_prompt=False,\n-        add_prefix_space=None,\n-        vocab=None,\n-        merges=None,\n-        vocab_file=None,\n+        use_default_system_prompt: bool = False,\n+        add_prefix_space: Optional[bool] = True,\n+        add_bos_token: bool = True,\n         **kwargs,\n     ):\n         self.add_prefix_space = add_prefix_space if add_prefix_space is not None else True\n         self.use_default_system_prompt = use_default_system_prompt\n-\n         additional_special_tokens = additional_special_tokens or []\n         for token in [prefix_token, middle_token, suffix_token, eot_token, fill_token]:\n             additional_special_tokens += [token] if token is not None else []\n \n-        if vocab is not None:\n-            self._vocab = (\n-                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n-            )\n-        else:\n-            self._vocab = {\n+        self._vocab = (\n+            vocab\n+            if vocab is not None\n+            else {\n                 str(unk_token): 0,\n                 str(bos_token): 1,\n                 str(eos_token): 2,\n             }\n+        )\n \n-        filtered_vocab = {\n-            t: i for t, i in self._vocab.items() if t not in {str(eos_token), str(bos_token), str(unk_token)}\n-        }\n-        self._merges = merges if merges is not None else generate_merges(filtered_vocab)\n+        self._merges = merges or []\n         self._tokenizer = Tokenizer(\n             BPE(\n                 vocab=self._vocab,\n@@ -163,55 +158,37 @@ def __init__(\n                 unk_token=str(unk_token),\n             )\n         )\n+        prepend_scheme = \"first\" if self.add_prefix_space else \"none\"\n         self._tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(\n-            replacement=\"▁\", prepend_scheme=_get_prepend_scheme(self.add_prefix_space, self), split=False\n+            replacement=\"▁\", prepend_scheme=prepend_scheme, split=False\n         )\n \n         self._tokenizer.decoder = decoders.Sequence(\n             [decoders.Replace(\"▁\", \" \"), decoders.ByteFallback(), decoders.Fuse(), decoders.Strip(content=\" \", left=1)]\n         )\n \n         super().__init__(\n-            tokenizer_object=self._tokenizer,\n             clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n             unk_token=unk_token,\n             bos_token=bos_token,\n             eos_token=eos_token,\n-            add_bos_token=add_bos_token,\n-            add_eos_token=add_eos_token,\n             use_default_system_prompt=use_default_system_prompt,\n             add_prefix_space=add_prefix_space,\n             prefix_token=prefix_token,\n             middle_token=middle_token,\n             suffix_token=suffix_token,\n             eot_token=eot_token,\n             fill_token=fill_token,\n+            add_bos_token=add_bos_token,\n             additional_special_tokens=additional_special_tokens,\n             **kwargs,\n         )\n-\n-        self._add_bos_token = add_bos_token\n-        self._add_eos_token = add_eos_token\n-        self.vocab_file = vocab_file\n-\n         self._prefix_token = prefix_token\n         self._middle_token = middle_token\n         self._suffix_token = suffix_token\n         self._eot_token = eot_token\n         self.fill_token = fill_token\n \n-        self._post_init()\n-\n-    def _post_init(self):\n-        self._tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(replacement=\"▁\", prepend_scheme=\"first\", split=False)\n-        self._tokenizer.normalizer = None\n-\n-        # This matches LlamaTokenizer's behavior and is needed when loading from vocab/merges\n-        self.add_tokens([AddedToken(token, special=True) for token in self.all_special_tokens])\n-\n-        self.update_post_processor()\n-        super()._post_init()\n-\n     @property\n     def prefix_token(self):\n         return self._prefix_token"
        },
        {
            "sha": "9545a9663bac35cfa4ce94e46f85cda2bfe02d75",
            "filename": "src/transformers/models/codegen/tokenization_codegen.py",
            "status": "modified",
            "additions": 14,
            "deletions": 43,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -67,6 +67,10 @@ class CodeGenTokenizer(TokenizersBackend):\n     refer to this superclass for more information regarding those methods.\n \n     Args:\n+        vocab (`str` or `dict[str, int]`, *optional*):\n+            Custom vocabulary dictionary. If not provided, vocabulary is loaded from `vocab_file`.\n+        merges (`str` or `list[str]`, *optional*):\n+            Custom merges list. If not provided, merges are loaded from `merges_file`.\n         unk_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n             The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n             token instead.\n@@ -79,31 +83,24 @@ class CodeGenTokenizer(TokenizersBackend):\n         add_prefix_space (`bool`, *optional*, defaults to `False`):\n             Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n             other word. (CodeGen tokenizer detect beginning of words by the preceding space).\n-        add_bos_token (`bool`, *optional*, defaults to `False`):\n-            Whether or not to add an initial beginning of sentence token to the input.\n         return_token_type_ids (`bool`, *optional*, defaults to `False`):\n             Whether to return token type IDs.\n-        vocab (`dict`, *optional*):\n-            Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n-        merges (`list`, *optional*):\n-            Custom merges list. If not provided, merges are loaded from merges_file.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n+    model = BPE\n \n     def __init__(\n         self,\n-        unk_token=\"<|endoftext|>\",\n-        bos_token=\"<|endoftext|>\",\n-        eos_token=\"<|endoftext|>\",\n+        vocab: Optional[Union[str, dict[str, int]]] = None,\n+        merges: Optional[Union[str, list[str]]] = None,\n+        unk_token: str = \"<|endoftext|>\",\n+        bos_token: str = \"<|endoftext|>\",\n+        eos_token: str = \"<|endoftext|>\",\n         pad_token=None,\n-        add_prefix_space=False,\n-        add_bos_token=False,\n-        return_token_type_ids=False,\n-        vocab: Optional[dict] = None,\n-        merges: Optional[list] = None,\n+        add_prefix_space: bool = False,\n+        return_token_type_ids: bool = False,\n         **kwargs,\n     ):\n         self.return_token_type_ids = return_token_type_ids\n@@ -112,17 +109,8 @@ def __init__(\n \n         self.add_prefix_space = add_prefix_space\n \n-        if vocab is not None:\n-            self._vocab = (\n-                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n-            )\n-        else:\n-            self._vocab = {}\n-\n-        if merges is not None:\n-            self._merges = merges\n-        else:\n-            self._merges = []\n+        self._vocab = vocab if vocab is not None else {}\n+        self._merges = merges or []\n \n         self._tokenizer = Tokenizer(\n             BPE(\n@@ -141,33 +129,16 @@ def __init__(\n             add_prefix_space=True, use_regex=True, trim_offsets=False\n         )\n \n-        tokenizer_object = self._tokenizer\n-\n-        # Set these before calling super().__init__() so the base class _post_init() can use them\n-        self._add_bos_token = add_bos_token\n-        self._add_eos_token = False\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             unk_token=unk_token,\n             bos_token=bos_token,\n             eos_token=eos_token,\n             pad_token=pad_token,\n             add_prefix_space=add_prefix_space,\n-            add_bos_token=add_bos_token,\n             return_token_type_ids=return_token_type_ids,\n             **kwargs,\n         )\n \n-        self._post_init()\n-\n-    def _post_init(self):\n-        self._tokenizer.post_processor = processors.ByteLevel(\n-            add_prefix_space=True, use_regex=True, trim_offsets=False\n-        )\n-        # Ensure base class post-init runs to register special/extra tokens, etc.\n-        super()._post_init()\n-\n     def decode(\n         self,\n         token_ids: Union[int, list[int], np.ndarray, \"torch.Tensor\"],"
        },
        {
            "sha": "29bf8701cda9e045dafa2055ee57f943d4bff183",
            "filename": "src/transformers/models/cohere/tokenization_cohere.py",
            "status": "modified",
            "additions": 12,
            "deletions": 42,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -100,21 +100,23 @@ class CohereTokenizer(TokenizersBackend):\n             Whether or not the default system prompt for Cohere tokenizer should be used.\n         add_prefix_space (`bool`, *optional*, defaults to `False`):\n             Whether or not the tokenizer should automatically add a prefix space\n-        vocab (`dict`, *optional*):\n+        vocab (`str`, `dict` or `list`, *optional*):\n             Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n-        merges (`list`, *optional*):\n-            Custom merges list. If not provided, merges are loaded from merges_file.\n+        merges (`str` or `list[str]`, *optional*):\n+            Custom merges list. If not provided, merges are loaded from `merges_file`.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n     padding_side = \"left\"\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n+    model = BPE\n     # No `max_model_input_sizes`\n \n     def __init__(\n         self,\n+        vocab: Optional[Union[str, dict[str, int]]] = None,\n+        merges: Optional[Union[str, list[str]]] = None,\n         errors: str = \"replace\",\n         unk_token: str = \"<UNK>\",\n         bos_token: str = \"<BOS_TOKEN>\",\n@@ -123,40 +125,29 @@ def __init__(\n         cls_token: str = \"<CLS>\",\n         sep_token: str = \"<SEP>\",\n         mask_token: str = \"<MASK_TOKEN>\",\n-        add_bos_token: bool = True,\n-        add_eos_token: bool = False,\n         use_default_system_prompt: bool = False,\n         add_prefix_space: bool = False,\n-        vocab: Optional[dict] = None,\n-        merges: Optional[list] = None,\n         **kwargs,\n     ):\n-        self._add_bos_token = add_bos_token\n-        self._add_eos_token = add_eos_token\n         self.use_default_system_prompt = use_default_system_prompt\n         self.add_prefix_space = add_prefix_space\n         self.grounded_generation_template = kwargs.pop(\"grounded_generation_template\", None)\n         self.tool_use_template = kwargs.pop(\"tool_use_template\", None)\n \n-        if vocab is not None:\n-            self._vocab = (\n-                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n-            )\n-        else:\n-            self._vocab = {\n+        self._vocab = (\n+            vocab\n+            if vocab is not None\n+            else {\n                 str(pad_token): 0,\n                 str(unk_token): 1,\n                 str(cls_token): 2,\n                 str(sep_token): 3,\n                 str(mask_token): 4,\n                 str(bos_token): 5,\n             }\n+        )\n \n-        if merges is not None:\n-            self._merges = merges\n-        else:\n-            self._merges = []\n-\n+        self._merges = merges or []\n         self._tokenizer = Tokenizer(\n             BPE(\n                 vocab=self._vocab,\n@@ -177,10 +168,7 @@ def __init__(\n         )\n         self._tokenizer.decoder = decoders.ByteLevel(add_prefix_space=add_prefix_space, trim_offsets=True)\n \n-        tokenizer_object = self._tokenizer\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             errors=errors,\n             unk_token=unk_token,\n             bos_token=bos_token,\n@@ -189,31 +177,13 @@ def __init__(\n             cls_token=cls_token,\n             sep_token=sep_token,\n             mask_token=mask_token,\n-            add_bos_token=add_bos_token,\n-            add_eos_token=add_eos_token,\n             use_default_system_prompt=use_default_system_prompt,\n             add_prefix_space=add_prefix_space,\n             **kwargs,\n         )\n \n         self._post_init()\n \n-    def _post_init(self):\n-        \"\"\"Post-initialization to ensure add_prefix_space is applied correctly.\"\"\"\n-        # Re-apply add_prefix_space setting to pre_tokenizer and decoder\n-        # This is needed because when loading from pretrained, the tokenizer.json\n-        # has these settings baked in and we need to override them\n-        self._tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n-            [\n-                pre_tokenizers.Digits(individual_digits=True),\n-                pre_tokenizers.ByteLevel(add_prefix_space=self.add_prefix_space, trim_offsets=True),\n-            ]\n-        )\n-        self._tokenizer.decoder = decoders.ByteLevel(add_prefix_space=self.add_prefix_space, trim_offsets=True)\n-\n-        # Call parent to handle AddedToken properties\n-        super()._post_init()\n-\n     def apply_tool_use_template(\n         self,\n         conversation: list[dict[str, str]],"
        },
        {
            "sha": "4736a3ba4939843ed03f89ad7e9beea2913a081a",
            "filename": "src/transformers/models/deberta/tokenization_deberta.py",
            "status": "modified",
            "additions": 11,
            "deletions": 20,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fdeberta%2Ftokenization_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fdeberta%2Ftokenization_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Ftokenization_deberta.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -14,6 +14,8 @@\n # limitations under the License.\n \"\"\"Fast Tokenization class for model DeBERTa.\"\"\"\n \n+from typing import Optional, Union\n+\n from tokenizers import AddedToken, Tokenizer, decoders, pre_tokenizers, processors\n from tokenizers.models import BPE\n \n@@ -93,12 +95,12 @@ class DebertaTokenizer(TokenizersBackend):\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n+    model = BPE\n \n     def __init__(\n         self,\n-        vocab_file=None,\n-        vocab=None,\n-        merges=None,\n+        vocab: Optional[Union[str, dict[str, int]]] = None,\n+        merges: Optional[Union[str, list[str]]] = None,\n         errors=\"replace\",\n         bos_token=\"[CLS]\",\n         eos_token=\"[SEP]\",\n@@ -110,26 +112,21 @@ def __init__(\n         add_prefix_space=False,\n         **kwargs,\n     ):\n-        self.vocab_file = vocab_file\n         self.add_prefix_space = add_prefix_space\n \n-        if vocab is not None:\n-            self._vocab = (\n-                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n-            )\n-        else:\n-            self._vocab = {\n+        self._vocab = (\n+            vocab\n+            if vocab is not None\n+            else {\n                 str(unk_token): 0,\n                 str(cls_token): 1,\n                 str(sep_token): 2,\n                 str(pad_token): 3,\n                 str(mask_token): 4,\n             }\n+        )\n \n-        if merges is not None and isinstance(merges, list) and len(merges) > 0:\n-            self._merges = [tuple(m) if isinstance(m, list) else m for m in merges]\n-        else:\n-            self._merges = []\n+        self._merges = merges or []\n \n         self._tokenizer = Tokenizer(\n             BPE(\n@@ -148,10 +145,7 @@ def __init__(\n         self._tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=add_prefix_space)\n         self._tokenizer.decoder = decoders.ByteLevel()\n \n-        tokenizer_object = self._tokenizer\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             errors=errors,\n             bos_token=bos_token,\n             eos_token=eos_token,\n@@ -163,7 +157,6 @@ def __init__(\n             add_prefix_space=add_prefix_space,\n             **kwargs,\n         )\n-\n         self._tokenizer.post_processor = processors.TemplateProcessing(\n             single=f\"{self.cls_token} $A {self.sep_token}\",\n             pair=f\"{self.cls_token} $A {self.sep_token} {self.sep_token} $B {self.sep_token}\",\n@@ -173,8 +166,6 @@ def __init__(\n             ],\n         )\n \n-        self._post_init()\n-\n     @property\n     def mask_token(self) -> str:\n         \"\"\""
        },
        {
            "sha": "b3d54d27fb6656a98e47aa96c37fae4d442de61b",
            "filename": "src/transformers/models/deberta_v2/tokenization_deberta_v2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 28,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -14,6 +14,8 @@\n # limitations under the License.\n \"\"\"Tokenization class for model DeBERTa-v2.\"\"\"\n \n+from typing import Optional, Union\n+\n from tokenizers import Regex, Tokenizer, decoders, normalizers, pre_tokenizers\n from tokenizers.models import Unigram\n \n@@ -26,13 +28,6 @@\n VOCAB_FILES_NAMES = {\"vocab_file\": \"spm.model\", \"tokenizer_file\": \"tokenizer.json\"}\n \n \n-def _get_prepend_scheme(add_prefix_space: bool) -> str:\n-    if add_prefix_space:\n-        return \"always\"\n-    else:\n-        return \"first\"\n-\n-\n class DebertaV2Tokenizer(TokenizersBackend):\n     \"\"\"\n     Construct a DeBERTa-v2 tokenizer (backed by HuggingFace's *tokenizers* library). Based on Unigram tokenization.\n@@ -43,7 +38,7 @@ class DebertaV2Tokenizer(TokenizersBackend):\n     Args:\n         vocab_file (`str`, *optional*):\n             Path to the vocabulary file (SentencePiece model file). Not used directly but kept for compatibility.\n-        vocab (`list`, *optional*):\n+        vocab (`str`, `dict` or `list`, *optional*):\n             List of tuples (piece, score) for the vocabulary.\n         precompiled_charsmap (`bytes`, *optional*):\n             Precompiled character map for normalization.\n@@ -79,11 +74,11 @@ class DebertaV2Tokenizer(TokenizersBackend):\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n+    model = Unigram\n \n     def __init__(\n         self,\n-        vocab_file=None,\n-        vocab=None,\n+        vocab: Optional[Union[str, dict, list]] = None,\n         do_lower_case=False,\n         split_by_punct=False,\n         bos_token=\"[CLS]\",\n@@ -94,16 +89,15 @@ def __init__(\n         cls_token=\"[CLS]\",\n         mask_token=\"[MASK]\",\n         add_prefix_space=True,\n-        unk_id=3,\n+        unk_id=1,\n         **kwargs,\n     ):\n-        self.vocab_file = vocab_file\n         self.do_lower_case = do_lower_case\n         self.split_by_punct = split_by_punct\n         self.add_prefix_space = add_prefix_space\n \n         if vocab is None:\n-            self._vocab = [\n+            vocab = [\n                 (str(pad_token), 0.0),\n                 (str(unk_token), 0.0),\n                 (str(bos_token), 0.0),\n@@ -112,12 +106,11 @@ def __init__(\n                 (str(cls_token), 0.0),\n                 (str(mask_token), 0.0),\n             ]\n+            unk_id = 1\n+        elif isinstance(vocab, list):\n+            unk_id = vocab.index((str(unk_token), 0.0)) if (str(unk_token), 0.0) in vocab else unk_id\n \n-        else:\n-            self._vocab = [tuple(item) if not isinstance(item, tuple) else item for item in vocab]\n-            computed_unk_id = {piece: i for i, (piece, _score) in enumerate(self._vocab)}\n-            unk_id = computed_unk_id.get(str(unk_token))\n-\n+        self._vocab = vocab\n         self._tokenizer = Tokenizer(\n             Unigram(\n                 self._vocab,\n@@ -132,10 +125,7 @@ def __init__(\n \n         list_normalizers.extend(\n             [\n-                normalizers.Replace(\"\\n\", \" \"),\n-                normalizers.Replace(\"\\r\", \" \"),\n-                normalizers.Replace(\"\\t\", \" \"),\n-                normalizers.Replace(Regex(r\" {2,}\"), \" \"),\n+                normalizers.Replace(Regex(r\"\\s{2,}|[\\n\\r\\t]\"), \" \"),\n                 normalizers.NFC(),\n                 normalizers.Strip(left=False, right=True),\n             ]\n@@ -146,17 +136,12 @@ def __init__(\n         if split_by_punct:\n             list_pretokenizers.append(pre_tokenizers.Punctuation(behavior=\"isolated\"))\n \n-        prepend_scheme = _get_prepend_scheme(add_prefix_space)\n+        prepend_scheme = \"always\" if add_prefix_space else \"first\"\n         list_pretokenizers.append(pre_tokenizers.Metaspace(replacement=\"▁\", prepend_scheme=prepend_scheme))\n \n         self._tokenizer.pre_tokenizer = pre_tokenizers.Sequence(list_pretokenizers)\n-\n         self._tokenizer.decoder = decoders.Metaspace(replacement=\"▁\", prepend_scheme=prepend_scheme)\n-\n-        tokenizer_object = self._tokenizer\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             bos_token=bos_token,\n             eos_token=eos_token,\n             unk_token=unk_token,"
        },
        {
            "sha": "219aced5934bd4d71aa95cd1489770cd9abda930",
            "filename": "src/transformers/models/distilbert/tokenization_distilbert.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -23,6 +23,19 @@\n class DistilBertTokenizer(BertTokenizer):\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n \n+    def __init__(self, *args, do_lower_case: bool = True, **kwargs):\n+        \"\"\"\n+        Construct a DistilBERT tokenizer (backed by HuggingFace's tokenizers library). Based on WordPiece.\n+\n+        This tokenizer inherits from [`BertTokenizer`] which contains most of the main methods. Users should refer to\n+        this superclass for more information regarding those methods.\n+\n+        Args:\n+            do_lower_case (`bool`, *optional*, defaults to `True`):\n+                Whether or not to lowercase the input when tokenizing.\n+        \"\"\"\n+        super().__init__(*args, do_lower_case=do_lower_case, **kwargs)\n+\n \n # DistilBertTokenizerFast is an alias for DistilBertTokenizer (since BertTokenizer is already a fast tokenizer)\n DistilBertTokenizerFast = DistilBertTokenizer"
        },
        {
            "sha": "5934e9fe151a2715516f5096f15cc143f682d6f9",
            "filename": "src/transformers/models/dpr/tokenization_dpr.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fdpr%2Ftokenization_dpr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fdpr%2Ftokenization_dpr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2Ftokenization_dpr.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -39,6 +39,10 @@ class DPRContextEncoderTokenizer(BertTokenizer):\n \n     vocab_files_names = VOCAB_FILES_NAMES\n \n+    def __init__(self, *args, do_lower_case=False, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.do_lower_case = do_lower_case\n+\n \n class DPRQuestionEncoderTokenizer(BertTokenizer):\n     r\"\"\"\n@@ -52,6 +56,10 @@ class DPRQuestionEncoderTokenizer(BertTokenizer):\n \n     vocab_files_names = VOCAB_FILES_NAMES\n \n+    def __init__(self, *args, do_lower_case=False, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.do_lower_case = do_lower_case\n+\n \n DPRSpanPrediction = collections.namedtuple(\n     \"DPRSpanPrediction\", [\"span_score\", \"relevance_score\", \"doc_id\", \"start_index\", \"end_index\", \"text\"]\n@@ -316,5 +324,9 @@ class DPRReaderTokenizer(CustomDPRReaderTokenizerMixin, BertTokenizer):\n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n \n+    def __init__(self, *args, do_lower_case=False, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.do_lower_case = do_lower_case\n+\n \n __all__ = [\"DPRContextEncoderTokenizer\", \"DPRQuestionEncoderTokenizer\", \"DPRReaderOutput\", \"DPRReaderTokenizer\"]"
        },
        {
            "sha": "58decf09ba32afdf690bc6074859882f6f722a42",
            "filename": "src/transformers/models/funnel/tokenization_funnel.py",
            "status": "modified",
            "additions": 17,
            "deletions": 24,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Ffunnel%2Ftokenization_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Ffunnel%2Ftokenization_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Ftokenization_funnel.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Tokenization class for Funnel Transformer.\"\"\"\n \n-from typing import Optional\n+from typing import Optional, Union\n \n from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import WordPiece\n@@ -83,16 +83,17 @@ class FunnelTokenizer(TokenizersBackend):\n             value for `lowercase` (as in the original BERT).\n         wordpieces_prefix (`str`, *optional*, defaults to `\"##\"`):\n             The prefix for subwords.\n-        vocab (`dict`, *optional*):\n+        vocab (`str` or `dict[str, int]`, *optional*):\n             Custom vocabulary dictionary.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n-    slow_tokenizer_class = None\n+    model = WordPiece\n     cls_token_type_id: int = 2\n \n     def __init__(\n         self,\n+        vocab: Optional[Union[str, dict[str, int]]] = None,\n         do_lower_case: bool = True,\n         unk_token: str = \"<unk>\",\n         sep_token: str = \"<sep>\",\n@@ -105,23 +106,18 @@ def __init__(\n         tokenize_chinese_chars: bool = True,\n         strip_accents: Optional[bool] = None,\n         wordpieces_prefix: str = \"##\",\n-        vocab: Optional[dict] = None,\n-        vocab_file: Optional[str] = None,\n         **kwargs,\n     ):\n-        self.vocab_file = vocab_file\n         self.do_lower_case = do_lower_case\n         self.tokenize_chinese_chars = tokenize_chinese_chars\n         self.strip_accents = strip_accents\n         self.clean_text = clean_text\n         self.wordpieces_prefix = wordpieces_prefix\n \n-        if vocab is not None:\n-            self._vocab = (\n-                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n-            )\n-        else:\n-            self._vocab = {\n+        self._vocab = (\n+            vocab\n+            if vocab is not None\n+            else {\n                 str(pad_token): 0,\n                 str(unk_token): 1,\n                 str(cls_token): 2,\n@@ -130,6 +126,7 @@ def __init__(\n                 str(bos_token): 5,\n                 str(eos_token): 6,\n             }\n+        )\n \n         self._tokenizer = Tokenizer(WordPiece(self._vocab, unk_token=str(unk_token)))\n \n@@ -142,19 +139,7 @@ def __init__(\n         self._tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n         self._tokenizer.decoder = decoders.WordPiece(prefix=wordpieces_prefix)\n \n-        self._tokenizer.post_processor = processors.TemplateProcessing(\n-            single=f\"{cls_token}:2 $A:0 {sep_token}:0\",  # token_type_id is 2 for Funnel transformer\n-            pair=f\"{cls_token}:2 $A:0 {sep_token}:0 $B:1 {sep_token}:1\",\n-            special_tokens=[\n-                (str(cls_token), self._vocab.get(str(cls_token), 2)),\n-                (str(sep_token), self._vocab.get(str(sep_token), 3)),\n-            ],\n-        )\n-\n-        tokenizer_object = self._tokenizer\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             do_lower_case=do_lower_case,\n             unk_token=unk_token,\n             sep_token=sep_token,\n@@ -169,6 +154,14 @@ def __init__(\n             wordpieces_prefix=wordpieces_prefix,\n             **kwargs,\n         )\n+        self._tokenizer.post_processor = processors.TemplateProcessing(\n+            single=f\"{cls_token}:2 $A:0 {sep_token}:0\",  # token_type_id is 2 for Funnel transformer\n+            pair=f\"{cls_token}:2 $A:0 {sep_token}:0 $B:1 {sep_token}:1\",\n+            special_tokens=[\n+                (str(cls_token), self.cls_token_id),\n+                (str(sep_token), self.sep_token_id),\n+            ],\n+        )\n \n \n __all__ = [\"FunnelTokenizer\"]"
        },
        {
            "sha": "c6bcc0c35d4f7d4ff5747a5b1d956412ddc382b8",
            "filename": "src/transformers/models/fuyu/processing_fuyu.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -337,13 +337,13 @@ class FuyuProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs a Fuyu processor which wraps a Fuyu image processor and a Llama tokenizer into a single processor.\n \n-    [`FuyuProcessor`] offers all the functionalities of [`FuyuImageProcessor`] and [`LlamaTokenizerFast`]. See the\n+    [`FuyuProcessor`] offers all the functionalities of [`FuyuImageProcessor`] and [`TokenizersBackend`]. See the\n     [`~FuyuProcessor.__call__`] and [`~FuyuProcessor.decode`] for more information.\n \n     Args:\n         image_processor ([`FuyuImageProcessor`]):\n             The image processor is a required input.\n-        tokenizer ([`LlamaTokenizerFast`]):\n+        tokenizer ([`TokenizersBackend`]):\n             The tokenizer is a required input.\n     \"\"\"\n \n@@ -486,7 +486,7 @@ def __call__(\n     ) -> \"FuyuBatchFeature\":\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to\n+        and `kwargs` arguments to TokenizersBackend's [`~TokenizersBackend.__call__`] if `text` is not `None` to\n         encode the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         FuyuImageProcessor's [`~FuyuImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information."
        },
        {
            "sha": "530bd4f825ee53e69887df42b93ebe9471c69170",
            "filename": "src/transformers/models/gemma/tokenization_gemma.py",
            "status": "modified",
            "additions": 10,
            "deletions": 27,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -12,12 +12,11 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Optional\n+from typing import Optional, Union\n \n from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers\n from tokenizers.models import BPE\n \n-from ...tokenization_utils_base import generate_merges\n from ...tokenization_utils_tokenizers import TokenizersBackend\n from ...utils import logging\n \n@@ -30,7 +29,7 @@ class GemmaTokenizer(TokenizersBackend):\n     \"\"\"\n     Construct a fast Gemma tokenizer (backed by HuggingFace's tokenizers library).\n \n-    This tokenizer uses a Unigram model with ByteFallback, no prefix space, and a normalizer that replaces\n+    This tokenizer uses a BPE model with byte fallback, no prefix space, and a normalizer that replaces\n     spaces with \"▁\".\n \n     Args:\n@@ -50,48 +49,37 @@ class GemmaTokenizer(TokenizersBackend):\n             Whether or not to add a `bos_token` at the start of sequences.\n         add_eos_token (`bool`, optional, defaults to False):\n             Whether or not to add an `eos_token` at the end of sequences.\n-        vocab (`dict`, optional):\n+        vocab (`str` or `dict[str, int]`, optional):\n             Custom vocabulary dict. If not provided, a minimal vocabulary is created using the special tokens.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n-    slow_tokenizer_class = None\n     padding_side = \"left\"\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n+    model = BPE\n \n     def __init__(\n         self,\n+        vocab: Optional[Union[str, dict[str, int]]] = None,\n+        merges: Optional[Union[str, list[str]]] = None,\n         unk_token: str = \"<unk>\",\n         bos_token: str = \"<bos>\",\n         eos_token: str = \"<eos>\",\n         pad_token: str = \"<pad>\",\n         mask_token: str = \"<mask>\",\n-        add_bos_token: bool = True,\n-        add_eos_token: bool = False,\n-        vocab: Optional[dict] = None,\n-        merges: Optional[list[tuple[str, str]]] = None,\n         **kwargs,\n     ):\n-        self._add_bos_token = add_bos_token\n-        self._add_eos_token = add_eos_token\n-\n-        special_tokens = {str(pad_token), str(eos_token), str(bos_token), str(unk_token)}\n-\n-        if vocab is not None:\n-            self._vocab = (\n-                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n-            )\n-        else:\n-            self._vocab = {\n+        if vocab is None:\n+            vocab = {\n                 str(pad_token): 0,\n                 str(eos_token): 1,\n                 str(bos_token): 2,\n                 str(unk_token): 3,\n                 str(mask_token): 4,\n             }\n+        self._vocab = vocab\n+        self._merges = merges or []\n \n-        filtered_vocab = {t: i for t, i in self._vocab.items() if t not in special_tokens}\n-        self._merges = merges if merges is not None else generate_merges(filtered_vocab)\n         self._tokenizer = Tokenizer(\n             BPE(\n                 vocab=self._vocab,\n@@ -108,17 +96,12 @@ def __init__(\n         )\n         self._tokenizer.normalizer = normalizers.Replace(\" \", \"▁\")\n         self._tokenizer.pre_tokenizer = pre_tokenizers.Split(\" \", \"merged_with_previous\")\n-        tokenizer_object = self._tokenizer\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             unk_token=unk_token,\n             bos_token=bos_token,\n             eos_token=eos_token,\n             pad_token=pad_token,\n             mask_token=mask_token,\n-            add_bos_token=add_bos_token,\n-            add_eos_token=add_eos_token,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "914d02874b02d5f7cf9fbfe7addd702b23056186",
            "filename": "src/transformers/models/gpt2/tokenization_gpt2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 44,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -14,12 +14,12 @@\n # limitations under the License.\n \"\"\"Tokenization classes for OpenAI GPT.\"\"\"\n \n-from typing import Optional\n+from typing import Optional, Union\n \n from tokenizers import Tokenizer, decoders, pre_tokenizers\n from tokenizers.models import BPE\n \n-from ...tokenization_utils_tokenizers import TokenizersBackend\n+from ...tokenization_utils_tokenizers import AddedToken, TokenizersBackend\n from ...utils import logging\n \n \n@@ -84,45 +84,31 @@ class GPT2Tokenizer(TokenizersBackend):\n         add_bos_token (`bool`, *optional*, defaults to `False`):\n             Whether or not to add an initial beginning of sentence token to the input. This allows to treat the leading\n             word just as any other word.\n-        vocab (`dict`, *optional*):\n-            Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n-        merges (`list`, *optional*):\n-            Custom merges list. If not provided, merges are loaded from merges_file.\n+        vocab (`str` or `dict[str, int]`, *optional*):\n+            Custom vocabulary dictionary. If not provided, vocabulary is loaded from `vocab_file`.\n+        merges (`str` or `list[str]`, *optional*):\n+            Custom merges list. If not provided, merges are loaded from `merges_file`.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n+    model = BPE\n \n     def __init__(\n         self,\n-        errors=\"replace\",\n-        unk_token=\"<|endoftext|>\",\n-        bos_token=\"<|endoftext|>\",\n-        eos_token=\"<|endoftext|>\",\n-        pad_token=None,\n+        vocab: Optional[Union[str, dict[str, int]]] = None,\n+        merges: Optional[Union[str, list[str]]] = None,\n+        errors: str = \"replace\",\n+        unk_token: Union[AddedToken, str] = \"<|endoftext|>\",\n+        bos_token: Union[AddedToken, str] = \"<|endoftext|>\",\n+        eos_token: Union[AddedToken, str] = \"<|endoftext|>\",\n+        pad_token: Optional[Union[AddedToken, str]] = None,\n         add_prefix_space=False,\n-        add_bos_token=False,\n-        vocab: Optional[dict] = None,\n-        merges: Optional[list] = None,\n         **kwargs,\n     ):\n-        #  self.add_bos_token = add_bos_token\n-\n         self.add_prefix_space = add_prefix_space\n-\n-        if vocab is not None:\n-            self._vocab = (\n-                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n-            )\n-        else:\n-            self._vocab = {}\n-\n-        if merges is not None:\n-            self._merges = [tuple(merge) if isinstance(merge, list) else merge for merge in merges]\n-        else:\n-            self._merges = []\n-\n+        self._vocab = vocab if vocab is not None else {}\n+        self._merges = merges or []\n         self._tokenizer = Tokenizer(\n             BPE(\n                 vocab=self._vocab,\n@@ -133,31 +119,17 @@ def __init__(\n                 fuse_unk=False,\n             )\n         )\n-\n         self._tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=add_prefix_space)\n         self._tokenizer.decoder = decoders.ByteLevel()\n-\n-        tokenizer_object = self._tokenizer\n-\n-        # Set these before calling super().__init__() so the base class _post_init() can use them\n-        self._add_bos_token = add_bos_token\n-        self._add_eos_token = False\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             errors=errors,\n             unk_token=unk_token,\n             bos_token=bos_token,\n             eos_token=eos_token,\n             pad_token=pad_token,\n             add_prefix_space=add_prefix_space,\n-            add_bos_token=add_bos_token,\n             **kwargs,\n         )\n \n-        # Call _post_init for tokenizers created directly (not from_pretrained)\n-        # For from_pretrained, this will be called again after loading the tokenizer from file\n-        self._post_init()\n-\n \n __all__ = [\"GPT2Tokenizer\"]"
        },
        {
            "sha": "1101aaf6e9dd9157f4b87980cf939cb80cc50879",
            "filename": "src/transformers/models/gpt_neox/tokenization_gpt_neox.py",
            "status": "modified",
            "additions": 10,
            "deletions": 49,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Ftokenization_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Ftokenization_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Ftokenization_gpt_neox.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Tokenization classes for GPTNeoX.\"\"\"\n \n-from typing import Optional\n+from typing import Optional, Union\n \n from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers\n from tokenizers.models import BPE\n@@ -87,51 +87,34 @@ class GPTNeoXTokenizer(TokenizersBackend):\n             Whether or not to add an `eos_token` at the end of sequences.\n         trim_offsets (`bool`, *optional*, defaults to `True`):\n             Whether or not the post-processing step should trim offsets to avoid including whitespaces.\n-        vocab (`dict`, *optional*):\n-            Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n-        merges (`list`, *optional*):\n-            Custom merges list. If not provided, merges are loaded from merges_file.\n+        vocab (`str` or `dict[str, int]`, *optional*):\n+            Custom vocabulary dictionary. If not provided, vocabulary is loaded from `vocab_file`.\n+        merges (`str` or `list[str]`, *optional*):\n+            Custom merges list. If not provided, merges are loaded from `merges_file`.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n+    model = BPE\n \n     def __init__(\n         self,\n+        vocab: Optional[Union[str, dict[str, int]]] = None,\n+        merges: Optional[Union[str, list[str]]] = None,\n         errors: str = \"replace\",\n         unk_token: str = \"<|endoftext|>\",\n         bos_token: str = \"<|endoftext|>\",\n         eos_token: str = \"<|endoftext|>\",\n         pad_token: str = \"<|padding|>\",\n-        add_bos_token: bool = False,\n-        add_eos_token: bool = False,\n         add_prefix_space: bool = False,\n         trim_offsets: bool = True,\n-        vocab: Optional[dict] = None,\n-        merges: Optional[list] = None,\n         **kwargs,\n     ):\n-        self._add_bos_token = add_bos_token\n-        self._add_eos_token = add_eos_token\n         self.add_prefix_space = add_prefix_space\n         self.trim_offsets = trim_offsets\n \n-        if vocab is not None:\n-            self._vocab = (\n-                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n-            )\n-        else:\n-            self._vocab = {\n-                str(unk_token): 0,\n-                str(pad_token): 1,\n-            }\n-\n-        if merges is not None:\n-            self._merges = merges\n-        else:\n-            self._merges = []\n-\n+        self._vocab = vocab if vocab is not None else {str(unk_token): 0, str(pad_token): 1}\n+        self._merges = merges or []\n         self._tokenizer = Tokenizer(\n             BPE(\n                 vocab=self._vocab,\n@@ -149,38 +132,16 @@ def __init__(\n         )\n         self._tokenizer.decoder = decoders.ByteLevel(add_prefix_space=False, trim_offsets=True)\n \n-        tokenizer_object = self._tokenizer\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             errors=errors,\n             unk_token=unk_token,\n             bos_token=bos_token,\n             eos_token=eos_token,\n             pad_token=pad_token,\n-            add_bos_token=add_bos_token,\n-            add_eos_token=add_eos_token,\n             add_prefix_space=add_prefix_space,\n             trim_offsets=trim_offsets,\n             **kwargs,\n         )\n \n-        self.update_post_processor()\n-\n-    def _post_init(self):\n-        \"\"\"Post-initialization to ensure tokenizer settings are applied correctly.\"\"\"\n-        # Re-apply settings to ensure they're correct after loading from pretrained\n-        self._tokenizer.normalizer = normalizers.NFC()\n-        self._tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(\n-            add_prefix_space=self.add_prefix_space, trim_offsets=self.trim_offsets\n-        )\n-        self._tokenizer.decoder = decoders.ByteLevel(add_prefix_space=False, trim_offsets=True)\n-\n-        # Call parent to handle AddedToken properties\n-        super()._post_init()\n-\n-        # Update post processor with current bos/eos settings\n-        self.update_post_processor()\n-\n \n __all__ = [\"GPTNeoXTokenizer\"]"
        },
        {
            "sha": "781413dc20754afd0d88b3bca24d470376a8b274",
            "filename": "src/transformers/models/herbert/tokenization_herbert.py",
            "status": "modified",
            "additions": 9,
            "deletions": 25,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fherbert%2Ftokenization_herbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fherbert%2Ftokenization_herbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fherbert%2Ftokenization_herbert.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -13,7 +13,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Optional\n+from typing import Optional, Union\n \n from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import BPE\n@@ -54,19 +54,20 @@ class HerbertTokenizer(TokenizersBackend):\n             The mask token.\n         sep_token (`str`, *optional*, defaults to `\"</s>\"`):\n             The separator token.\n-        vocab (`dict`, *optional*):\n+        vocab (`str`, `dict` or `list`, *optional*):\n             Custom vocabulary dictionary.\n-        merges (`list`, *optional*):\n+        merges (`str` or `list[str]`, *optional*):\n             Custom merges list.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n-    slow_tokenizer_class = None\n+    model_input_names = [\"input_ids\", \"attention_mask\"]\n+    model = BPE\n \n     def __init__(\n         self,\n-        vocab: Optional[dict] = None,\n-        merges: Optional[list] = None,\n+        vocab: Optional[Union[str, dict[str, int]]] = None,\n+        merges: Optional[Union[str, list[str]]] = None,\n         cls_token: str = \"<s>\",\n         unk_token: str = \"<unk>\",\n         pad_token: str = \"<pad>\",\n@@ -76,19 +77,8 @@ def __init__(\n         merges_file: Optional[str] = None,\n         **kwargs,\n     ):\n-        if vocab is not None:\n-            self._vocab = (\n-                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n-            )\n-        else:\n-            self._vocab = {}\n-\n-        if merges is not None:\n-            # Convert lists to tuples if necessary (happens when loading from JSON)\n-            self._merges = [tuple(merge) if isinstance(merge, list) else merge for merge in merges]\n-        else:\n-            self._merges = []\n-\n+        self._vocab = vocab if vocab is not None else {str(unk_token): 0}\n+        self._merges = merges or []\n         self._tokenizer = Tokenizer(\n             BPE(\n                 vocab=self._vocab,\n@@ -105,13 +95,7 @@ def __init__(\n         self._tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n         self._tokenizer.decoder = decoders.BPEDecoder(suffix=\"</w>\")\n \n-        tokenizer_object = self._tokenizer\n-\n-        self.vocab_file = vocab_file\n-        self.merges_file = merges_file\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             cls_token=cls_token,\n             unk_token=unk_token,\n             pad_token=pad_token,"
        },
        {
            "sha": "6534b8f0f7f9047e5f58bae1ed04f2965e6e7ee4",
            "filename": "src/transformers/models/lasr/tokenization_lasr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Flasr%2Ftokenization_lasr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Flasr%2Ftokenization_lasr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flasr%2Ftokenization_lasr.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -65,13 +65,13 @@ class LasrTokenizer(TokenizersBackend):\n             calling get_sentinel_tokens method and token ids can be by calling get_sentinel_token_ids method\n         additional_special_tokens (`list[str]`, *optional*):\n             Additional special tokens used by the tokenizer.\n-        vocab (`dict`, *optional*):\n+        vocab (`str`, `dict` or `list`, *optional*):\n             Custom vocabulary dict. If not provided, a minimal vocabulary is created using the special tokens.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n+    model = Unigram\n \n     def __init__(\n         self,\n@@ -84,7 +84,6 @@ def __init__(\n         vocab_file=None,\n         **kwargs,\n     ):\n-        self.vocab_file = vocab_file\n         self._extra_ids = extra_ids\n \n         # Handle extra_ids and additional_special_tokens\n@@ -133,10 +132,7 @@ def __init__(\n \n         self._tokenizer.decoder = decoders.Metaspace(replacement=\"▁\", prepend_scheme=\"always\", split=True)\n \n-        tokenizer_object = self._tokenizer\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             eos_token=eos_token,\n             unk_token=unk_token,\n             pad_token=pad_token,"
        },
        {
            "sha": "9351c1b2d08edf6df3b14c5b5f7614e28bb43156",
            "filename": "src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 53,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -159,22 +159,11 @@ class LayoutLMv2Tokenizer(TokenizersBackend):\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n-    slow_tokenizer_class = None\n-\n-    @staticmethod\n-    def _load_vocab_from_file(vocab_file):\n-        \"\"\"Load vocab from a BERT-style vocab file (one token per line).\"\"\"\n-        vocab = {}\n-        with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n-            for index, line in enumerate(reader):\n-                token = line.rstrip(\"\\n\")\n-                vocab[token] = index\n-        return vocab\n+    model = models.WordPiece\n \n     def __init__(\n         self,\n-        vocab=None,\n-        vocab_file=None,\n+        vocab: Optional[Union[str, dict[str, int]]] = None,\n         do_lower_case=True,\n         unk_token=\"[UNK]\",\n         sep_token=\"[SEP]\",\n@@ -190,60 +179,30 @@ def __init__(\n         strip_accents=None,\n         **kwargs,\n     ):\n-        self.vocab_file = vocab_file\n         self.do_lower_case = do_lower_case\n \n-        # Build vocab for WordPiece\n         if vocab is not None:\n-            if isinstance(vocab, dict):\n-                _vocab = vocab\n-            else:\n-                raise ValueError(\"vocab must be a dict mapping tokens to ids\")\n-        elif vocab_file is not None:\n-            # Load vocab from file (BERT format: one token per line)\n-            _vocab = self._load_vocab_from_file(vocab_file)\n+            self._vocab = vocab\n         else:\n-            # Initialize with at least the special tokens for WordPiece\n-            _vocab = {\n+            self._vocab = {\n                 str(pad_token): 0,\n                 str(unk_token): 1,\n                 str(cls_token): 2,\n                 str(sep_token): 3,\n                 str(mask_token): 4,\n             }\n \n-        # Initialize WordPiece tokenizer\n-        self._tokenizer = Tokenizer(models.WordPiece(vocab=_vocab, unk_token=str(unk_token)))\n-\n-        # Set normalizer\n+        self._tokenizer = Tokenizer(models.WordPiece(vocab=self._vocab, unk_token=str(unk_token)))\n         self._tokenizer.normalizer = normalizers.BertNormalizer(\n             clean_text=True,\n             handle_chinese_chars=tokenize_chinese_chars,\n             strip_accents=strip_accents,\n             lowercase=do_lower_case,\n         )\n \n-        # Set pre_tokenizer\n         self._tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n-\n-        # Set decoder\n         self._tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n-\n-        # Set post_processor (will be set after super().__init__ when we have token IDs)\n-        # Temporarily set to None, will be configured after parent init\n-        self._tokenizer.post_processor = None\n-\n-        tokenizer_object = self._tokenizer\n-\n-        # additional properties\n-        self.cls_token_box = cls_token_box\n-        self.sep_token_box = sep_token_box\n-        self.pad_token_box = pad_token_box\n-        self.pad_token_label = pad_token_label\n-        self.only_label_first_subword = only_label_first_subword\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             do_lower_case=do_lower_case,\n             unk_token=unk_token,\n             sep_token=sep_token,\n@@ -260,6 +219,11 @@ def __init__(\n             **kwargs,\n         )\n \n+        self.cls_token_box = cls_token_box\n+        self.sep_token_box = sep_token_box\n+        self.pad_token_box = pad_token_box\n+        self.pad_token_label = pad_token_label\n+\n         # Now set post_processor with actual token IDs\n         cls = str(self.cls_token)\n         sep = str(self.sep_token)\n@@ -275,13 +239,6 @@ def __init__(\n             ],\n         )\n \n-        # additional properties\n-        self.cls_token_box = cls_token_box\n-        self.sep_token_box = sep_token_box\n-        self.pad_token_box = pad_token_box\n-        self.pad_token_label = pad_token_label\n-        self.only_label_first_subword = only_label_first_subword\n-\n     @add_end_docstrings(LAYOUTLMV2_ENCODE_KWARGS_DOCSTRING, LAYOUTLMV2_ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)\n     def __call__(\n         self,"
        },
        {
            "sha": "ea73f27ee66fb26445a908515a59cca0c863bb8d",
            "filename": "src/transformers/models/layoutlmv3/tokenization_layoutlmv3.py",
            "status": "modified",
            "additions": 12,
            "deletions": 61,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Tokenization class for LayoutLMv3. Same as LayoutLMv2, but RoBERTa-like BPE tokenization instead of WordPiece.\"\"\"\n \n-import json\n from typing import Optional, Union\n \n from tokenizers import Tokenizer, decoders, models, pre_tokenizers, processors\n@@ -159,15 +158,16 @@ class LayoutLMv3Tokenizer(TokenizersBackend):\n             CrossEntropyLoss.\n         only_label_first_subword (`bool`, *optional*, defaults to `True`):\n             Whether or not to only label the first subword, in case word labels are provided.\n-        vocab (`dict`, *optional*):\n-            Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file when using from_pretrained.\n-        merges (`list`, *optional*):\n-            Custom merges list. If not provided, merges are loaded from merges_file when using from_pretrained.\n+        vocab (`str` or `dict[str, int]`, *optional*):\n+            Custom vocabulary dictionary. If not provided, vocabulary is loaded from `vocab_file` when using\n+            `from_pretrained`.\n+        merges (`str` or `list[str]`, *optional*):\n+            Custom merges list. If not provided, merges are loaded from `merges_file` when using `from_pretrained`.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\", \"bbox\"]\n-    slow_tokenizer_class = None\n+    model = models.BPE\n \n     def __init__(\n         self,\n@@ -185,69 +185,26 @@ def __init__(\n         pad_token_box=[0, 0, 0, 0],\n         pad_token_label=-100,\n         only_label_first_subword=True,\n-        vocab: Optional[dict] = None,\n-        merges: Optional[list] = None,\n-        vocab_file: Optional[str] = None,\n-        merges_file: Optional[str] = None,\n+        vocab: Optional[Union[str, dict[str, int]]] = None,\n+        merges: Optional[Union[str, list[str]]] = None,\n         **kwargs,\n     ):\n         self.add_prefix_space = add_prefix_space\n-\n-        # Build vocab and merges for BPE\n-        # Priority: 1) vocab/merges dicts/lists, 2) vocab_file/merges_file paths, 3) empty\n-        if vocab is not None:\n-            _vocab = vocab\n-        elif vocab_file is not None:\n-            with open(vocab_file, encoding=\"utf-8\") as f:\n-                _vocab = json.load(f)\n-        else:\n-            _vocab = {}\n-\n-        if merges is not None:\n-            _merges = merges\n-        elif merges_file is not None:\n-            _merges = []\n-            with open(merges_file, encoding=\"utf-8\") as f:\n-                for line in f:\n-                    line = line.strip()\n-                    if line and not line.startswith(\"#\"):\n-                        _merges.append(tuple(line.split()))\n-        else:\n-            _merges = []\n-\n-        # Initialize BPE tokenizer\n+        self._vocab = vocab or {}\n+        self._merges = merges or []\n         self._tokenizer = Tokenizer(\n             models.BPE(\n-                vocab=_vocab,\n-                merges=_merges,\n+                vocab=self._vocab,\n+                merges=self._merges,\n                 dropout=None,\n                 continuing_subword_prefix=\"\",\n                 end_of_word_suffix=\"\",\n                 fuse_unk=False,\n             )\n         )\n-\n-        # Set pre_tokenizer (ByteLevel)\n         self._tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=add_prefix_space)\n-\n-        # Set decoder\n         self._tokenizer.decoder = decoders.ByteLevel()\n-\n-        # Set post_processor (will be set after super().__init__ when we have token IDs)\n-        # Temporarily set to None, will be configured after parent init\n-        self._tokenizer.post_processor = None\n-\n-        tokenizer_object = self._tokenizer\n-\n-        # additional properties\n-        self.cls_token_box = cls_token_box\n-        self.sep_token_box = sep_token_box\n-        self.pad_token_box = pad_token_box\n-        self.pad_token_label = pad_token_label\n-        self.only_label_first_subword = only_label_first_subword\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             errors=errors,\n             bos_token=bos_token,\n             eos_token=eos_token,\n@@ -277,18 +234,12 @@ def __init__(\n             add_prefix_space=add_prefix_space,\n             trim_offsets=True,\n         )\n-\n-        # additional properties\n         self.cls_token_box = cls_token_box\n         self.sep_token_box = sep_token_box\n         self.pad_token_box = pad_token_box\n         self.pad_token_label = pad_token_label\n         self.only_label_first_subword = only_label_first_subword\n \n-        # Call _post_init for tokenizers created directly (not from_pretrained)\n-        # For from_pretrained, this will be called again after loading the tokenizer from file\n-        self._post_init()\n-\n     @add_end_docstrings(LAYOUTLMV3_ENCODE_KWARGS_DOCSTRING, LAYOUTLMV3_ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)\n     def __call__(\n         self,"
        },
        {
            "sha": "24e4a57cf8ed0c2c7cb2c7aec664a083b149aaa4",
            "filename": "src/transformers/models/layoutxlm/tokenization_layoutxlm.py",
            "status": "modified",
            "additions": 13,
            "deletions": 38,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -150,8 +150,8 @@ class LayoutXLMTokenizer(TokenizersBackend):\n     refer to this superclass for more information regarding those methods.\n \n     Args:\n-        vocab (`list[tuple[str, float]]`, *optional*):\n-            Vocabulary for the tokenizer as a list of (token, score) tuples.\n+        vocab (`str`, `dict` or `list`, *optional*):\n+            Vocabulary for the tokenizer as a path, a dictionary or a list of `(token, score)` tuples.\n         bos_token (`str`, *optional*, defaults to `\"<s>\"`):\n             The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n \n@@ -206,12 +206,11 @@ class LayoutXLMTokenizer(TokenizersBackend):\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n+    model = Unigram\n \n     def __init__(\n         self,\n-        vocab_file=None,\n-        vocab=None,\n+        vocab: Optional[Union[str, list]] = None,\n         bos_token=\"<s>\",\n         eos_token=\"</s>\",\n         sep_token=\"</s>\",\n@@ -229,17 +228,10 @@ def __init__(\n     ):\n         # Mask token behave like a normal word, i.e. include the space before it\n         mask_token = AddedToken(mask_token, lstrip=True, rstrip=False) if isinstance(mask_token, str) else mask_token\n-\n         self.add_prefix_space = add_prefix_space\n \n-        # Build vocab from list of tuples if provided, else use default\n-        # Handle both list of tuples (when creating) and dict (when loading)\n         if vocab is not None:\n-            if isinstance(vocab, dict):\n-                # Convert dict to list of tuples\n-                self._vocab = [(token, score) for token, score in vocab.items()]\n-            else:\n-                self._vocab = vocab\n+            self._vocab = vocab\n         else:\n             self._vocab = [\n                 (\"<s>\", 0.0),\n@@ -250,57 +242,33 @@ def __init__(\n             if mask_token not in [v[0] for v in self._vocab]:\n                 self._vocab.append((str(mask_token), 0.0))\n \n-        # Create the Unigram tokenizer\n         self._tokenizer = Tokenizer(Unigram(self._vocab, unk_id=3, byte_fallback=False))\n-\n-        # Set up normalizer (strip right, replace multiple spaces)\n         self._tokenizer.normalizer = normalizers.Sequence(\n             [\n                 normalizers.Strip(left=False, right=True),\n                 normalizers.Replace(Regex(\" {2,}\"), \"▁\"),\n             ]\n         )\n \n-        # Set up pre_tokenizer (Metaspace)\n         prepend_scheme = _get_prepend_scheme(add_prefix_space, self)\n         self._tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(replacement=\"▁\", prepend_scheme=prepend_scheme)\n \n-        # Set up decoder\n         self._tokenizer.decoder = decoders.Metaspace(replacement=\"▁\", prepend_scheme=prepend_scheme)\n \n-        # Set up post_processor for XLM-RoBERTa style\n-        # Get token IDs\n-        cls_token_id = self._get_token_id(str(cls_token))\n-        sep_token_id = self._get_token_id(str(sep_token))\n-\n-        self._tokenizer.post_processor = processors.TemplateProcessing(\n-            single=\"<s> $A </s>\",\n-            pair=\"<s> $A </s> </s> $B </s>\",\n-            special_tokens=[\n-                (\"<s>\", cls_token_id),\n-                (\"</s>\", sep_token_id),\n-            ],\n-        )\n-\n-        tokenizer_object = self._tokenizer\n-\n-        # additional properties\n         self.cls_token_box = cls_token_box\n         self.sep_token_box = sep_token_box\n         self.pad_token_box = pad_token_box\n         self.pad_token_label = pad_token_label\n         self.only_label_first_subword = only_label_first_subword\n \n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             bos_token=bos_token,\n             eos_token=eos_token,\n             sep_token=sep_token,\n             cls_token=cls_token,\n             unk_token=unk_token,\n             pad_token=pad_token,\n             mask_token=mask_token,\n-            vocab_file=vocab_file,\n             vocab=vocab,\n             add_prefix_space=add_prefix_space,\n             cls_token_box=cls_token_box,\n@@ -311,7 +279,14 @@ def __init__(\n             **kwargs,\n         )\n \n-        self.vocab_file = vocab_file\n+        self._tokenizer.post_processor = processors.TemplateProcessing(\n+            single=f\"{str(self.cls_token)}:0 $A:0 {str(self.sep_token)}:0\",\n+            pair=f\"{str(self.cls_token)}:0 $A:0 {str(self.sep_token)}:0 {str(self.sep_token)}:0 $B:0 {str(self.sep_token)}:0\",\n+            special_tokens=[\n+                (str(self.cls_token), self.cls_token_id),\n+                (str(self.sep_token), self.sep_token_id),\n+            ],\n+        )\n \n     def _get_token_id(self, token: str) -> int:\n         \"\"\"Helper to get token ID from vocab.\"\"\""
        },
        {
            "sha": "ea6a3535ccf83e65ba05a332a75e583f72fa3b56",
            "filename": "src/transformers/models/llama/tokenization_llama.py",
            "status": "modified",
            "additions": 15,
            "deletions": 43,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -12,11 +12,12 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+from typing import Optional, Union\n \n-from tokenizers import AddedToken, Tokenizer, decoders, pre_tokenizers\n+from tokenizers import Tokenizer, decoders, pre_tokenizers\n from tokenizers.models import BPE\n \n-from ...tokenization_utils_base import _get_prepend_scheme, generate_merges\n+from ...tokenization_utils_base import _get_prepend_scheme\n from ...tokenization_utils_tokenizers import TokenizersBackend\n from ...utils import logging\n \n@@ -61,6 +62,10 @@ class LlamaTokenizer(TokenizersBackend):\n     refer to this superclass for more information regarding those methods.\n \n     Args:\n+        vocab (`str`, `dict` or `list`, *optional*):\n+            Path to the vocabulary file, a dictionary or a list of tokens.\n+        merges (`str` or `list`, *optional*):\n+            Path to the merges file or a list of merges.\n         clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n             Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n             extra spaces.\n@@ -84,42 +89,32 @@ class LlamaTokenizer(TokenizersBackend):\n     vocab_files_names = VOCAB_FILES_NAMES\n     padding_side = \"left\"\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n+    model = BPE\n \n     def __init__(\n         self,\n+        vocab: Optional[Union[str, dict, list]] = None,\n+        merges: Optional[Union[str, list]] = None,\n         clean_up_tokenization_spaces=False,\n         unk_token=\"<unk>\",\n         bos_token=\"<s>\",\n         eos_token=\"</s>\",\n-        add_bos_token=True,\n-        add_eos_token=False,\n         use_default_system_prompt=False,\n         legacy=False,\n         add_prefix_space=None,\n-        vocab=None,\n-        merges=None,\n         **kwargs,\n     ):\n         self.add_prefix_space = add_prefix_space if add_prefix_space is not None else True\n-\n-        if vocab is not None:\n-            self._vocab = (\n-                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n-            )\n-        else:\n+        self.legacy = legacy\n+        self._vocab = vocab\n+        if vocab is None:\n             self._vocab = {\n                 str(unk_token): 0,\n                 str(bos_token): 1,\n                 str(eos_token): 2,\n             }\n \n-        special_tokens = {str(eos_token), str(bos_token), str(unk_token)}\n-\n-        filtered_vocab = {t: i for t, i in self._vocab.items() if t not in special_tokens}\n-        if merges is not None:\n-            self._merges = [tuple(merge) if isinstance(merge, list) else merge for merge in merges]\n-        else:\n-            self._merges = generate_merges(filtered_vocab)\n+        self._merges = merges or []\n         self._tokenizer = Tokenizer(\n             BPE(vocab=self._vocab, merges=self._merges, fuse_unk=True, byte_fallback=True, dropout=None)\n         )\n@@ -138,40 +133,17 @@ def __init__(\n             sequence += [decoders.Strip(content=\" \", left=1)]\n \n         self._tokenizer.decoder = decoders.Sequence(sequence)\n-        tokenizer_object = self._tokenizer\n-\n+        self.use_default_system_prompt = use_default_system_prompt\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n             unk_token=unk_token,\n             bos_token=bos_token,\n             eos_token=eos_token,\n-            add_bos_token=add_bos_token,\n-            add_eos_token=add_eos_token,\n             use_default_system_prompt=use_default_system_prompt,\n             add_prefix_space=add_prefix_space,\n             **kwargs,\n         )\n \n-        self._add_bos_token = add_bos_token\n-        self._add_eos_token = add_eos_token\n-        self.use_default_system_prompt = use_default_system_prompt\n-\n-        self._post_init()\n-\n-    def _post_init(self):\n-        \"\"\"Post-initialization setup that needs to run after _tokenizer is set.\"\"\"\n-        # Only set pre_tokenizer/normalizer for Llama-3 style tokenizers (use Sequence)\n-        pre_tok = self._tokenizer.pre_tokenizer\n-        if pre_tok is None or type(pre_tok).__name__ != \"Sequence\":\n-            self._tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(\n-                replacement=\"▁\", prepend_scheme=\"first\", split=False\n-            )\n-            self._tokenizer.normalizer = None\n-            self.add_tokens([AddedToken(token, special=True) for token in self.all_special_tokens])\n-        super()._post_init()\n-        self.update_post_processor()\n-\n \n __all__ = [\"LlamaTokenizer\", \"LlamaTokenizerFast\"]\n "
        },
        {
            "sha": "ea1d1b98e5796e7fd5ac8276c7551b40194d3daf",
            "filename": "src/transformers/models/luke/tokenization_luke.py",
            "status": "modified",
            "additions": 11,
            "deletions": 38,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fluke%2Ftokenization_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fluke%2Ftokenization_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fluke%2Ftokenization_luke.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -20,7 +20,7 @@\n from typing import Optional, Union\n \n import numpy as np\n-from tokenizers import Tokenizer, decoders, pre_tokenizers, processors\n+from tokenizers import Tokenizer, decoders, pre_tokenizers\n from tokenizers.models import BPE\n \n from ...tokenization_python import PreTrainedTokenizer\n@@ -167,6 +167,10 @@ class LukeTokenizer(TokenizersBackend):\n             Path to the vocabulary file.\n         merges_file (`str`):\n             Path to the merges file.\n+        vocab (`str` or `dict[str, int]`, *optional*):\n+            Custom vocabulary dictionary. If not provided, the vocabulary is loaded from `vocab_file`.\n+        merges (`str` or `list[str]`, *optional*):\n+            Custom merges list. If not provided, merges are loaded from `merges_file`.\n         entity_vocab_file (`str`):\n             Path to the entity vocabulary file.\n         task (`str`, *optional*):\n@@ -228,10 +232,13 @@ class LukeTokenizer(TokenizersBackend):\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n+    model = BPE\n \n     def __init__(\n         self,\n+        vocab: Optional[Union[str, dict[str, int]]] = None,\n+        merges: Optional[Union[str, list[str]]] = None,\n+        entity_vocab: Optional[Union[str, dict, list]] = None,\n         errors=\"replace\",\n         bos_token=\"<s>\",\n         eos_token=\"</s>\",\n@@ -250,37 +257,17 @@ def __init__(\n         entity_pad_token=\"[PAD]\",\n         entity_mask_token=\"[MASK]\",\n         entity_mask2_token=\"[MASK2]\",\n-        vocab: Optional[dict] = None,\n-        merges: Optional[list] = None,\n-        entity_vocab: Optional[dict] = None,\n         **kwargs,\n     ):\n         self.add_prefix_space = add_prefix_space\n \n         # Handle entity vocab file for backward compatibility\n         entity_vocab_file = kwargs.pop(\"entity_vocab_file\", None)\n-\n-        # Check if vocab/merges/entity_vocab are in kwargs\n-        if vocab is None and \"vocab\" in kwargs:\n-            vocab = kwargs.pop(\"vocab\")\n-        if merges is None and \"merges\" in kwargs:\n-            merges = kwargs.pop(\"merges\")\n         if entity_vocab is None and \"entity_vocab\" in kwargs:\n             entity_vocab = kwargs.pop(\"entity_vocab\")\n \n-        # Build vocab and merges (either from data or empty, like GPT2Tokenizer)\n-        if vocab is not None:\n-            self._vocab = (\n-                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n-            )\n-        else:\n-            self._vocab = {}\n-\n-        if merges is not None:\n-            self._merges = [tuple(merge) if isinstance(merge, list) else merge for merge in merges]\n-        else:\n-            self._merges = []\n-\n+        self._vocab = vocab or {}\n+        self._merges = merges or []\n         self._tokenizer = Tokenizer(\n             BPE(\n                 vocab=self._vocab,\n@@ -365,8 +352,6 @@ def __init__(\n \n         kwargs[\"extra_special_tokens\"] = extra_tokens\n \n-        tokenizer_object = self._tokenizer\n-\n         # Configure default special token behaviors to match LUKE formatting\n         token_type_ids_pattern = kwargs.setdefault(\"token_type_ids_pattern\", \"all_zeros\")\n         special_tokens_pattern = kwargs.setdefault(\"special_tokens_pattern\", \"cls_double_sep\")\n@@ -379,7 +364,6 @@ def __init__(\n         kwargs.setdefault(\"clean_up_tokenization_spaces\", True)\n \n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             errors=errors,\n             bos_token=bos_token,\n             eos_token=eos_token,\n@@ -401,17 +385,6 @@ def __init__(\n             entity_vocab=entity_vocab if entity_vocab_file is None else None,  # Only store if it was passed as data\n             **kwargs,\n         )\n-        self._post_init()\n-\n-    def _post_init(self):\n-        self._tokenizer.post_processor = processors.TemplateProcessing(\n-            single=f\"{self.cls_token}:0 $A:0 {self.sep_token}:0\",\n-            pair=f\"{self.cls_token}:0 $A:0 {self.sep_token}:0 {self.sep_token}:0 $B:1 {self.sep_token}:1\",\n-            special_tokens=[\n-                (self.cls_token, self.cls_token_id),\n-                (self.sep_token, self.sep_token_id),\n-            ],\n-        )\n \n     def build_inputs_with_special_tokens(\n         self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None"
        },
        {
            "sha": "e5701192e01d6400126796e9f7cfed6b45d70ad5",
            "filename": "src/transformers/models/markuplm/tokenization_markuplm.py",
            "status": "modified",
            "additions": 28,
            "deletions": 61,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -101,10 +101,10 @@ class MarkupLMTokenizer(TokenizersBackend):\n     Users should refer to this superclass for more information regarding those methods.\n \n     Args:\n-        vocab_file (`str`):\n-            Path to the vocabulary file.\n-        merges_file (`str`):\n-            Path to the merges file.\n+        vocab (`str` or `dict[str, int]`, *optional*):\n+            Custom vocabulary dictionary. If not provided, the vocabulary is loaded from `vocab_file`.\n+        merges (`str` or `list[str]`, *optional*):\n+            Custom merges list. If not provided, merges are loaded from `merges_file`.\n         errors (`str`, *optional*, defaults to `\"replace\"`):\n             Paradigm to follow when decoding bytes to UTF-8. See\n             [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.\n@@ -149,12 +149,14 @@ class MarkupLMTokenizer(TokenizersBackend):\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n+    model_input_names = [\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n+    model = BPE\n \n     def __init__(\n         self,\n         tags_dict,\n-        vocab: Optional[Union[dict, list]] = None,\n-        merges: Optional[list] = None,\n+        vocab: Optional[Union[str, dict[str, int], list[tuple[str, float]]]] = None,\n+        merges: Optional[Union[str, list[str]]] = None,\n         errors=\"replace\",\n         bos_token=\"<s>\",\n         eos_token=\"</s>\",\n@@ -172,57 +174,28 @@ def __init__(\n         trim_offsets=False,\n         **kwargs,\n     ):\n-        if kwargs.get(\"from_slow\"):\n-            logger.warning(\n-                \"MarkupLMTokenizer no longer supports initialization from a slow tokenizer. Ignoring `from_slow=True`.\"\n-            )\n-        kwargs[\"from_slow\"] = False\n         bos_token = AddedToken(bos_token, lstrip=False, rstrip=False) if isinstance(bos_token, str) else bos_token\n         eos_token = AddedToken(eos_token, lstrip=False, rstrip=False) if isinstance(eos_token, str) else eos_token\n         sep_token = AddedToken(sep_token, lstrip=False, rstrip=False) if isinstance(sep_token, str) else sep_token\n         cls_token = AddedToken(cls_token, lstrip=False, rstrip=False) if isinstance(cls_token, str) else cls_token\n         unk_token = AddedToken(unk_token, lstrip=False, rstrip=False) if isinstance(unk_token, str) else unk_token\n         pad_token = AddedToken(pad_token, lstrip=False, rstrip=False) if isinstance(pad_token, str) else pad_token\n-\n         # Mask token behave like a normal word, i.e. include the space before it\n         mask_token = AddedToken(mask_token, lstrip=True, rstrip=False) if isinstance(mask_token, str) else mask_token\n \n-        processed_vocab = vocab\n-        processed_merges = merges\n-\n-        if isinstance(processed_vocab, list):\n-            processed_vocab = {\n-                token: index for index, (token, _score) in enumerate(processed_vocab) if isinstance(token, str)\n-            }\n-        elif isinstance(processed_vocab, dict):\n-            processed_vocab = {str(token): int(index) for token, index in processed_vocab.items()}\n-\n-        if processed_vocab is None:\n-            processed_vocab = {\n+        if vocab is None:\n+            vocab = {\n                 str(pad_token): 0,\n                 str(unk_token): 1,\n                 str(cls_token): 2,\n                 str(sep_token): 3,\n                 str(mask_token): 4,\n             }\n-\n-        normalized_merges = []\n-        if processed_merges is not None:\n-            for merge in processed_merges:\n-                if isinstance(merge, tuple) and len(merge) == 2:\n-                    normalized_merges.append((merge[0], merge[1]))\n-                elif isinstance(merge, list) and len(merge) == 2:\n-                    normalized_merges.append((merge[0], merge[1]))\n-                elif isinstance(merge, str):\n-                    parts = merge.split()\n-                    if len(parts) == 2 and not merge.startswith(\"#\"):\n-                        normalized_merges.append((parts[0], parts[1]))\n-        processed_merges = normalized_merges if normalized_merges else []\n-\n+        merges = merges or []\n         tokenizer = Tokenizer(\n             BPE(\n-                vocab=processed_vocab,\n-                merges=processed_merges,\n+                vocab=vocab,\n+                merges=merges,\n                 dropout=None,\n                 continuing_subword_prefix=\"\",\n                 end_of_word_suffix=\"\",\n@@ -231,21 +204,11 @@ def __init__(\n         )\n         tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=add_prefix_space)\n         tokenizer.decoder = decoders.ByteLevel()\n-\n-        sep_token_str = str(sep_token)\n-        cls_token_str = str(cls_token)\n-        tokenizer.post_processor = processors.RobertaProcessing(\n-            sep=(sep_token_str, processed_vocab.get(sep_token_str, processed_vocab.get(\"</s>\", 2))),\n-            cls=(cls_token_str, processed_vocab.get(cls_token_str, processed_vocab.get(\"<s>\", 0))),\n-            add_prefix_space=add_prefix_space,\n-            trim_offsets=trim_offsets,\n-        )\n-\n+        self._vocab = vocab\n+        self._merges = merges\n+        self._tokenizer = tokenizer\n         super().__init__(\n-            tokenizer_object=tokenizer,\n             tags_dict=tags_dict,\n-            vocab=vocab,\n-            merges=merges,\n             errors=errors,\n             bos_token=bos_token,\n             eos_token=eos_token,\n@@ -263,14 +226,18 @@ def __init__(\n             only_label_first_subword=only_label_first_subword,\n             **kwargs,\n         )\n-        if trim_offsets:\n-            # Not implemented yet, because we need to chain two post processors which is not possible yet\n-            # We need to wait for https://github.com/huggingface/tokenizers/pull/1005\n-            # With `trim_offsets=False` we don't need to do add `processors.ByteLevel(trim_offsets=False)`\n-            # because it's not doing anything\n-            raise NotImplementedError(\n-                \"`trim_offsets=True` is not implemented for MarkupLMTokenizer. Please set it to False.\"\n-            )\n+        sep_token_str = str(sep_token)\n+        cls_token_str = str(cls_token)\n+        cls_token_id = self.cls_token_id\n+        sep_token_id = self.sep_token_id\n+        self._tokenizer.post_processor = processors.TemplateProcessing(\n+            single=f\"{cls_token_str} $A {sep_token_str}\",\n+            pair=f\"{cls_token_str} $A {sep_token_str} $B {sep_token_str}\",\n+            special_tokens=[\n+                (cls_token_str, cls_token_id),\n+                (sep_token_str, sep_token_id),\n+            ],\n+        )\n \n         self.tags_dict = tags_dict\n "
        },
        {
            "sha": "8f9960a7ee53aa4f96cdc79185ac6c00eff3345b",
            "filename": "src/transformers/models/mbart/tokenization_mbart.py",
            "status": "modified",
            "additions": 11,
            "deletions": 52,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fmbart%2Ftokenization_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fmbart%2Ftokenization_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Ftokenization_mbart.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -13,7 +13,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Optional\n+from typing import Optional, Union\n \n from tokenizers import Tokenizer, decoders, pre_tokenizers, processors\n from tokenizers.models import Unigram\n@@ -58,13 +58,14 @@ class MBartTokenizer(TokenizersBackend):\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n+    model = Unigram\n \n     prefix_tokens: list[int] = []\n     suffix_tokens: list[int] = []\n \n     def __init__(\n         self,\n+        vocab: Optional[Union[str, dict, list]] = None,\n         bos_token=\"<s>\",\n         eos_token=\"</s>\",\n         sep_token=\"</s>\",\n@@ -75,9 +76,6 @@ def __init__(\n         src_lang=None,\n         tgt_lang=None,\n         additional_special_tokens=None,\n-        vocab=None,\n-        merges=None,  # Ignored for Unigram\n-        vocab_file=None,\n         **kwargs,\n     ):\n         mask_token = AddedToken(mask_token, lstrip=True, rstrip=False) if isinstance(mask_token, str) else mask_token\n@@ -88,56 +86,20 @@ def __init__(\n                 [t for t in additional_special_tokens if t not in _additional_special_tokens]\n             )\n \n-        # MBart uses fairseq vocab alignment: <s>=0, <pad>=1, </s>=2, <unk>=3, then SPM pieces[3:], lang codes, <mask>\n-        if vocab is not None:\n-            # Handle different vocab formats (dict, list of tokens, or list of tuples)\n-            # SentencePieceExtractor returns list[tuple[str, float]] which is the expected format\n-            if isinstance(vocab, dict):\n-                vocab = [(token, 0.0) for token in vocab.keys()]\n-            elif isinstance(vocab, list) and len(vocab) > 0:\n-                if not isinstance(vocab[0], tuple):\n-                    vocab = [(token, 0.0) for token in vocab]\n-                else:\n-                    # Ensure tuples are (str, float) format\n-                    vocab = [(str(item[0]), float(item[1])) for item in vocab]\n-\n-            # Reorder to fairseq: <s>, <pad>, </s>, <unk>, ... (rest of vocab from SPM[3:])\n-            vocab_list = []\n-            vocab_list.append((str(bos_token), 0.0))\n-            vocab_list.append((str(pad_token), 0.0))\n-            vocab_list.append((str(eos_token), 0.0))\n-            vocab_list.append((str(unk_token), 0.0))\n-\n-            # Add the rest of the SentencePiece vocab (skipping first 3: <unk>, <s>, </s>)\n-            vocab_list.extend(vocab[4:])\n-\n-            # Add language codes\n-            for lang_code in FAIRSEQ_LANGUAGE_CODES:\n-                vocab_list.append((str(lang_code), 0.0))\n-\n-            # Add mask token\n-            vocab_list.append((str(mask_token), 0.0))\n-\n-            self._vocab_scores = vocab_list\n-        else:\n-            self._vocab_scores = [\n+        if vocab is None:\n+            vocab = [\n                 (str(bos_token), 0.0),\n                 (str(pad_token), 0.0),\n                 (str(eos_token), 0.0),\n                 (str(unk_token), 0.0),\n-                (\"▁\", -2.0),\n             ]\n+            vocab += [(\"▁\", -2.0)]\n             for lang_code in FAIRSEQ_LANGUAGE_CODES:\n-                self._vocab_scores.append((lang_code, 0.0))\n-            self._vocab_scores.append((str(mask_token), 0.0))\n-\n-        self._tokenizer = Tokenizer(\n-            Unigram(\n-                self._vocab_scores,\n-                unk_id=3,\n-                byte_fallback=False,\n-            )\n-        )\n+                vocab.append((lang_code, 0.0))\n+            vocab.append((str(mask_token), 0.0))\n+\n+        self._vocab = vocab\n+        self._tokenizer = Tokenizer(Unigram(self._vocab, unk_id=3, byte_fallback=False))\n \n         self._tokenizer.normalizer = None\n \n@@ -150,10 +112,7 @@ def __init__(\n \n         self._tokenizer.decoder = decoders.Metaspace(replacement=\"▁\", prepend_scheme=\"always\", split=True)\n \n-        tokenizer_object = self._tokenizer\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             bos_token=bos_token,\n             eos_token=eos_token,\n             sep_token=sep_token,"
        },
        {
            "sha": "3d685106b7d2db5a81ad9bf5eb07b4dd5d11dc32",
            "filename": "src/transformers/models/mbart50/tokenization_mbart50.py",
            "status": "modified",
            "additions": 7,
            "deletions": 10,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fmbart50%2Ftokenization_mbart50.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fmbart50%2Ftokenization_mbart50.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart50%2Ftokenization_mbart50.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -13,7 +13,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Optional\n+from typing import Optional, Union\n \n from tokenizers import Regex, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import Unigram\n@@ -79,13 +79,14 @@ class MBart50Tokenizer(TokenizersBackend):\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n+    model = Unigram\n \n     prefix_tokens: list[int] = []\n     suffix_tokens: list[int] = []\n \n     def __init__(\n         self,\n+        vocab: Optional[Union[str, dict, list]] = None,\n         src_lang=None,\n         tgt_lang=None,\n         eos_token=\"</s>\",\n@@ -94,21 +95,16 @@ def __init__(\n         unk_token=\"<unk>\",\n         pad_token=\"<pad>\",\n         mask_token=\"<mask>\",\n-        vocab=None,\n-        merges=None,  # Ignored for Unigram\n-        vocab_file=None,\n         **kwargs,\n     ):\n         mask_token = AddedToken(mask_token, lstrip=True, rstrip=False) if isinstance(mask_token, str) else mask_token\n \n-        self.vocab_file = vocab_file\n-\n         # Do not pass language codes via extra_special_tokens to super().__init__.\n         # We will mark them as special AFTER backend construction to avoid re-adding tokens\n         # when loading from pretrained files.\n \n         # Always construct a tokenizer_object without referencing external tokenizer files\n-        if vocab is not None:\n+        if isinstance(vocab, list):\n             # MBart50 uses fairseq vocab alignment matching MBart50Converter:\n             # <s>=0, <pad>=1, </s>=2, <unk>=3, then tokens, lang codes, <mask>\n \n@@ -180,9 +176,9 @@ def __init__(\n         self._tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(replacement=\"▁\", prepend_scheme=\"always\", split=True)\n \n         self._tokenizer.decoder = decoders.Metaspace(replacement=\"▁\", prepend_scheme=\"always\", split=True)\n-\n+        additional_special_tokens = kwargs.pop(\"additional_special_tokens\", []) or []\n+        additional_special_tokens.extend(FAIRSEQ_LANGUAGE_CODES)\n         super().__init__(\n-            tokenizer_object=self._tokenizer,\n             src_lang=src_lang,\n             tgt_lang=tgt_lang,\n             eos_token=eos_token,\n@@ -191,6 +187,7 @@ def __init__(\n             unk_token=unk_token,\n             pad_token=pad_token,\n             mask_token=mask_token,\n+            additional_special_tokens=additional_special_tokens,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "743f6239a2476f5f4c88f03c05edb61c96c278dc",
            "filename": "src/transformers/models/mluke/tokenization_mluke.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fmluke%2Ftokenization_mluke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fmluke%2Ftokenization_mluke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmluke%2Ftokenization_mluke.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -234,8 +234,8 @@ def __init__(\n         entity_pad_token=\"[PAD]\",\n         entity_mask_token=\"[MASK]\",\n         entity_mask2_token=\"[MASK2]\",\n-        vocab: Optional[list] = None,\n-        entity_vocab: Optional[dict] = None,\n+        vocab: Optional[Union[str, dict, list]] = None,\n+        entity_vocab: Optional[Union[str, dict, list]] = None,\n         **kwargs,\n     ) -> None:\n         # Mask token behave like a normal word, i.e. include the space before it\n@@ -263,10 +263,13 @@ def __init__(\n             entity_vocab = kwargs.pop(\"entity_vocab\")\n \n         # Build vocab from data (list of (token, score) tuples)\n-        if vocab is not None:\n+        if isinstance(vocab, list):\n             # vocab is list of (token, score) tuples from SentencePieceExtractor\n             self._vocab = [(token, float(score)) for token, score in vocab]\n             self._vocab_size = len(self._vocab)\n+        elif vocab is not None:\n+            self._vocab = vocab\n+            self._vocab_size = 0\n         else:\n             # Create minimal vocab with <unk> to satisfy Unigram requirements\n             self._vocab = [(\"<unk>\", 0.0)]\n@@ -365,10 +368,7 @@ def __init__(\n \n         kwargs[\"extra_special_tokens\"] = extra_tokens\n \n-        tokenizer_object = self._tokenizer\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             bos_token=bos_token,\n             eos_token=eos_token,\n             unk_token=unk_token,"
        },
        {
            "sha": "7597309b0df6a1c9b18f74a37c8055bd8bbb88e0",
            "filename": "src/transformers/models/mpnet/tokenization_mpnet.py",
            "status": "modified",
            "additions": 5,
            "deletions": 13,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fmpnet%2Ftokenization_mpnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fmpnet%2Ftokenization_mpnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpnet%2Ftokenization_mpnet.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -15,7 +15,7 @@\n # limitations under the License.\n \"\"\"Tokenization classes for MPNet.\"\"\"\n \n-from typing import Optional\n+from typing import Optional, Union\n \n from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import WordPiece\n@@ -38,7 +38,7 @@ class MPNetTokenizer(TokenizersBackend):\n     refer to this superclass for more information regarding those methods.\n \n     Args:\n-        vocab (`dict`, *optional*):\n+        vocab (`str` or `dict[str, int]`, *optional*):\n             Dictionary mapping tokens to their IDs. If not provided, an empty vocab is initialized.\n         do_lower_case (`bool`, *optional*, defaults to `True`):\n             Whether or not to lowercase the input when tokenizing.\n@@ -87,10 +87,11 @@ class MPNetTokenizer(TokenizersBackend):\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n+    model = WordPiece\n \n     def __init__(\n         self,\n-        vocab: Optional[dict] = None,\n+        vocab: Optional[Union[str, dict[str, int]]] = None,\n         do_lower_case=True,\n         bos_token=\"<s>\",\n         eos_token=\"</s>\",\n@@ -104,12 +105,7 @@ def __init__(\n         **kwargs,\n     ):\n         # Initialize vocab\n-        if vocab is not None:\n-            self._vocab = (\n-                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n-            )\n-        else:\n-            self._vocab = {}\n+        self._vocab = vocab if vocab is not None else {}\n \n         # Initialize the tokenizer with WordPiece model\n         self._tokenizer = Tokenizer(WordPiece(self._vocab, unk_token=str(unk_token)))\n@@ -142,11 +138,7 @@ def __init__(\n         # Mask token behave like a normal word, i.e. include the space before it\n         mask_token = AddedToken(mask_token, lstrip=True, rstrip=False) if isinstance(mask_token, str) else mask_token\n \n-        # Store for later use\n-        tokenizer_object = self._tokenizer\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             do_lower_case=do_lower_case,\n             bos_token=bos_token,\n             eos_token=eos_token,"
        },
        {
            "sha": "2105061d67fb100ca2404dd4c031538d6903beee",
            "filename": "src/transformers/models/nllb/tokenization_nllb.py",
            "status": "modified",
            "additions": 8,
            "deletions": 22,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fnllb%2Ftokenization_nllb.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fnllb%2Ftokenization_nllb.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb%2Ftokenization_nllb.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -13,7 +13,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Optional\n+from typing import Optional, Union\n \n from tokenizers import Regex, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import BPE\n@@ -83,13 +83,15 @@ class NllbTokenizer(TokenizersBackend):\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n+    model = BPE\n \n     prefix_tokens: list[int] = []\n     suffix_tokens: list[int] = []\n \n     def __init__(\n         self,\n+        vocab: Optional[Union[str, dict[str, int]]] = None,\n+        merges: Optional[Union[str, list[str]]] = None,\n         bos_token=\"<s>\",\n         eos_token=\"</s>\",\n         sep_token=\"</s>\",\n@@ -101,40 +103,27 @@ def __init__(\n         tgt_lang=None,\n         additional_special_tokens=None,\n         legacy_behaviour=False,\n-        vocab=None,\n-        merges=None,\n-        vocab_file=None,\n         **kwargs,\n     ):\n         if additional_special_tokens is None:\n             additional_special_tokens = kwargs.get(\"extra_special_tokens\", FAIRSEQ_LANGUAGE_CODES)\n \n-        self.vocab_file = vocab_file\n-\n         mask_token = (\n             AddedToken(mask_token, normalized=True, lstrip=True, special=True)\n             if isinstance(mask_token, str)\n             else mask_token\n         )\n         self.legacy_behaviour = legacy_behaviour\n \n-        if vocab is not None:\n-            if isinstance(vocab, list):\n-                self._vocab = {token: idx for idx, (token, _score) in enumerate(vocab)}\n-            else:\n-                self._vocab = vocab\n-        else:\n-            self._vocab = {\n+        if vocab is None:\n+            vocab = {\n                 str(bos_token): 0,\n                 str(pad_token): 1,\n                 str(eos_token): 2,\n                 str(unk_token): 3,\n             }\n-\n-        if merges is None:\n-            self._merges = []\n-        else:\n-            self._merges = merges\n+        self._vocab = vocab\n+        self._merges = merges or []\n \n         self._tokenizer = Tokenizer(\n             BPE(\n@@ -158,13 +147,10 @@ def __init__(\n         self._tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(replacement=\"▁\", prepend_scheme=\"always\", split=True)\n         self._tokenizer.decoder = decoders.Metaspace(replacement=\"▁\", prepend_scheme=\"always\", split=True)\n \n-        tokenizer_object = self._tokenizer\n-\n         # Remove extra_special_tokens from kwargs if present to avoid conflict\n         kwargs.pop(\"extra_special_tokens\", None)\n \n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             bos_token=bos_token,\n             eos_token=eos_token,\n             sep_token=sep_token,"
        },
        {
            "sha": "4ec273ff10f88533330abd26bedffedcbc3d4d55",
            "filename": "src/transformers/models/nougat/tokenization_nougat.py",
            "status": "modified",
            "additions": 11,
            "deletions": 59,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fnougat%2Ftokenization_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fnougat%2Ftokenization_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Ftokenization_nougat.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -380,16 +380,16 @@ class NougatTokenizer(TokenizersBackend):\n         pad_token (`str`, *optional*, defaults to `\"<pad>\"`):\n             The token used for padding, for example when batching sequences of different lengths.\n \n-        vocab (`dict`, *optional*):\n+        vocab (`str`, `dict` or `list`, *optional*):\n             Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n \n-        merges (`list`, *optional*):\n+        merges (`str` or `list`, *optional*):\n             Custom merges list. If not provided, merges are loaded from merges_file.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n+    model = BPE\n \n     def __init__(\n         self,\n@@ -398,28 +398,22 @@ def __init__(\n         bos_token: str = \"<s>\",\n         eos_token: str = \"</s>\",\n         pad_token: str = \"<pad>\",\n-        vocab: Optional[dict] = None,\n-        merges: Optional[list] = None,\n+        vocab: Optional[Union[str, dict, list]] = None,\n+        merges: Optional[Union[str, list]] = None,\n         **kwargs,\n     ):\n-        if vocab is not None:\n-            self._vocab = (\n-                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n-            )\n-        else:\n-            self._vocab = {\n+        self._vocab = (\n+            vocab\n+            if vocab is not None\n+            else {\n                 str(bos_token): 0,\n                 str(pad_token): 1,\n                 str(eos_token): 2,\n                 str(unk_token): 3,\n                 \"[START_REF]\": 4,\n             }\n-\n-        if merges is not None:\n-            self._merges = merges\n-        else:\n-            self._merges = []\n-\n+        )\n+        self._merges = merges or []\n         self._tokenizer = Tokenizer(\n             BPE(\n                 vocab=self._vocab,\n@@ -464,10 +458,7 @@ def __init__(\n         self._tokenizer.enable_truncation(max_length=4096)\n         self._tokenizer.enable_padding(length=4096, pad_id=pad_token_id, pad_token=str(pad_token))\n \n-        tokenizer_object = self._tokenizer\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             errors=errors,\n             unk_token=unk_token,\n             bos_token=bos_token,\n@@ -476,45 +467,6 @@ def __init__(\n             **kwargs,\n         )\n \n-    def _post_init(self):\n-        \"\"\"Post-initialization to ensure tokenizer settings are applied correctly.\"\"\"\n-        # Re-apply settings to ensure they're correct after loading from pretrained\n-        self._tokenizer.normalizer = normalizers.NFKC()\n-        self._tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n-            [\n-                pre_tokenizers.Split(pattern=\"SPL1T-TH1S-Pl3A5E\", behavior=\"removed\", invert=False),\n-                pre_tokenizers.Digits(individual_digits=True),\n-                pre_tokenizers.Split(\n-                    pattern=r\"[\\(\\)\\[\\]\\{\\}]|([!\\\"#\\$%\\&'\\*\\+,\\-\\./:;<=>\\?\\\\\\^_`\\|\\~])\\1*\",\n-                    behavior=\"isolated\",\n-                    invert=False,\n-                ),\n-                pre_tokenizers.Split(pattern=\"\\n\", behavior=\"isolated\", invert=False),\n-                pre_tokenizers.ByteLevel(add_prefix_space=False, trim_offsets=True, use_regex=True),\n-            ]\n-        )\n-        self._tokenizer.decoder = decoders.ByteLevel(add_prefix_space=True, trim_offsets=True, use_regex=True)\n-\n-        # Set up post processor with bos and eos tokens\n-        bos_token_id = self.bos_token_id if self.bos_token_id is not None else 0\n-        eos_token_id = self.eos_token_id if self.eos_token_id is not None else 2\n-        pad_token_id = self.pad_token_id if self.pad_token_id is not None else 1\n-        self._tokenizer.post_processor = processors.TemplateProcessing(\n-            single=f\"{self.bos_token}:0 $A:0 {self.eos_token}:0\",\n-            pair=\"$A:0 $B:1\",\n-            special_tokens=[\n-                (str(self.eos_token), eos_token_id),\n-                (str(self.bos_token), bos_token_id),\n-            ],\n-        )\n-\n-        # Enable truncation and padding\n-        self._tokenizer.enable_truncation(max_length=4096)\n-        self._tokenizer.enable_padding(length=4096, pad_id=pad_token_id, pad_token=str(self.pad_token))\n-\n-        # Call parent to handle AddedToken properties\n-        super()._post_init()\n-\n     def remove_hallucinated_references(self, text: str) -> str:\n         \"\"\"\n         Remove hallucinated or missing references from the text."
        },
        {
            "sha": "ef0e3be72545d91af10ce1c4cb338e19a8c4d647",
            "filename": "src/transformers/models/openai/tokenization_openai.py",
            "status": "modified",
            "additions": 10,
            "deletions": 46,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fopenai%2Ftokenization_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fopenai%2Ftokenization_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Ftokenization_openai.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -14,10 +14,11 @@\n # limitations under the License.\n \"\"\"Tokenization classes for OpenAI GPT.\"\"\"\n \n+from typing import Optional, Union\n+\n from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers\n from tokenizers.models import BPE\n \n-from ...convert_slow_tokenizer import generate_merges\n from ...tokenization_utils_tokenizers import TokenizersBackend\n from ...utils import logging\n \n@@ -48,40 +49,26 @@ class OpenAIGPTTokenizer(TokenizersBackend):\n         unk_token (`str`, *optional*, defaults to `\"<unk>\"`):\n             The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n             token instead.\n-        vocab (`dict`, *optional*):\n+        vocab (`str` or `dict[str, int]`, *optional*):\n             Custom vocabulary dictionary. If not provided, a blank vocabulary is initialized.\n-        merges (`list`, *optional*):\n+        merges (`str` or `list[str]`, *optional*):\n             Custom merges list. If not provided, an empty list is used.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n+    model = BPE\n \n     def __init__(\n         self,\n-        unk_token=\"<unk>\",\n-        vocab=None,\n-        merges=None,\n-        vocab_file=None,\n-        merges_file=None,\n+        vocab: Optional[Union[str, dict[str, int]]] = None,\n+        merges: Optional[Union[str, list[str]]] = None,\n+        unk_token: str = \"<unk>\",\n         **kwargs,\n     ):\n-        # Initialize vocabulary\n-        if vocab is not None:\n-            self._vocab = (\n-                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n-            )\n-        else:\n-            # Initialize minimal vocabulary with unk token\n-            self._vocab = {str(unk_token): 0}\n-\n-        # Initialize merges\n-        if merges is not None:\n-            self._merges = merges if merges is not None else generate_merges(self._vocab)\n-        else:\n-            self._merges = []\n+        self._vocab = vocab if vocab is not None else {str(unk_token): 0}\n+        self._merges = merges or []\n \n-        # Create BPE tokenizer\n         self._tokenizer = Tokenizer(\n             BPE(\n                 vocab=self._vocab,\n@@ -107,34 +94,11 @@ def __init__(\n         self._tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n         self._tokenizer.decoder = decoders.BPEDecoder(suffix=\"</w>\")\n \n-        tokenizer_object = self._tokenizer\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             unk_token=unk_token,\n             **kwargs,\n         )\n \n-        self.vocab_file = vocab_file\n-        self.merges_file = merges_file\n-\n-    def _post_init(self):\n-        \"\"\"Post-initialization to ensure tokenizer settings are applied correctly.\"\"\"\n-        # Re-apply settings to ensure they're correct after loading from pretrained\n-        self._tokenizer.normalizer = normalizers.Sequence(\n-            [\n-                normalizers.NFD(),\n-                normalizers.Lowercase(),\n-                normalizers.StripAccents(),\n-            ]\n-        )\n-\n-        self._tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n-        self._tokenizer.decoder = decoders.BPEDecoder(suffix=\"</w>\")\n-\n-        # Call parent to handle AddedToken properties\n-        super()._post_init()\n-\n     @property\n     def do_lower_case(self):\n         return True"
        },
        {
            "sha": "46dc3f66993e89e59e3452a0b9a366bae79aa2ca",
            "filename": "src/transformers/models/pegasus/tokenization_pegasus.py",
            "status": "modified",
            "additions": 17,
            "deletions": 44,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fpegasus%2Ftokenization_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fpegasus%2Ftokenization_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Ftokenization_pegasus.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -14,6 +14,8 @@\n # limitations under the License.\n \"\"\"Tokenization class for model PEGASUS.\"\"\"\n \n+from typing import Optional, Union\n+\n from tokenizers import Regex, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import Unigram\n \n@@ -70,76 +72,45 @@ class PegasusTokenizer(TokenizersBackend):\n             that uses the tokens 2 - 104 only for pretraining\n         offset (`int`, *optional*, defaults to 103):\n             Offset for additional special tokens.\n-        vocab (`dict`, *optional*):\n-            Custom vocabulary dictionary. If not provided, a blank vocabulary is initialized.\n+        vocab (`str` or `list[tuple[str, float]]`, *optional*):\n+            Custom vocabulary with `(token, score)` tuples. If not provided, a blank vocabulary is initialized.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n+    model = Unigram\n \n     def __init__(\n         self,\n+        vocab: Optional[Union[str, list[tuple[str, float]]]] = None,\n         pad_token=\"<pad>\",\n         eos_token=\"</s>\",\n         unk_token=\"<unk>\",\n         mask_token=\"<mask_2>\",\n         mask_token_sent=\"<mask_1>\",\n         additional_special_tokens=None,\n         offset=103,\n-        vocab=None,\n-        vocab_file=None,\n         **kwargs,\n     ):\n         self.offset = offset\n-        self.vocab_file = vocab_file\n \n         if additional_special_tokens is None:\n             additional_special_tokens = [mask_token_sent] if mask_token_sent is not None else []\n             additional_special_tokens += [f\"<unk_{i}>\" for i in range(2, self.offset)]\n \n-        if vocab is not None:\n-            # For Pegasus, insert special tokens at the beginning\n-            special_tokens_set = {pad_token, eos_token, mask_token_sent, mask_token, unk_token}\n-            special_tokens_set.update(additional_special_tokens)\n-\n-            # Build special tokens in correct order\n-            _vocab_list = [\n-                (str(pad_token), 0.0),\n-                (str(eos_token), 0.0),\n-            ]\n-            if mask_token_sent:\n-                _vocab_list.append((str(mask_token_sent), 0.0))\n-            for token in additional_special_tokens:\n-                if token not in [pad_token, eos_token, mask_token_sent]:\n-                    _vocab_list.append((str(token), 0.0))\n-            if mask_token not in [t for t, _ in _vocab_list]:\n-                _vocab_list.append((str(mask_token), 0.0))\n-            _vocab_list.append((str(unk_token), 0.0))\n-\n-            # Filter out special tokens from main vocab and combine\n-            filtered_vocab = [(t, s) for t, s in vocab if t not in special_tokens_set]\n-            _vocab_list = _vocab_list + filtered_vocab\n-        else:\n-            _vocab_list = [(str(unk_token), 0.0)]\n-\n-        self._vocab = {token: idx for idx, (token, _) in enumerate(_vocab_list)}\n-\n-        self._tokenizer = Tokenizer(Unigram(vocab=_vocab_list, unk_id=self._vocab.get(str(unk_token), 0)))\n+        if vocab is None:\n+            vocab = [(str(unk_token), 0.0), (str(pad_token), 0.0), (str(eos_token), 0.0), (str(mask_token), 0.0)]\n \n+        self._vocab = vocab\n+        self._tokenizer = Tokenizer(Unigram(vocab=vocab, unk_id=self._vocab.index((str(unk_token), 0.0), 1)))\n         self._tokenizer.normalizer = normalizers.Sequence(\n             [normalizers.Replace(Regex(r\"\\n\"), \" \"), normalizers.Replace(Regex(r\" {2,}\"), \" \")]\n         )\n \n-        self._tokenizer.post_processor = processors.TemplateProcessing(\n-            single=f\"$A {eos_token}\",\n-            pair=f\"$A $B {eos_token}\",\n-            special_tokens=[(str(eos_token), self._vocab.get(str(eos_token), 1))],\n-        )\n-\n-        tokenizer_object = self._tokenizer\n+        self._tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(replacement=\"▁\", prepend_scheme=\"always\", split=True)\n+        self._tokenizer.decoder = decoders.Metaspace(replacement=\"▁\", prepend_scheme=\"always\", split=True)\n \n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             pad_token=pad_token,\n             eos_token=eos_token,\n             unk_token=unk_token,\n@@ -149,9 +120,11 @@ def __init__(\n             additional_special_tokens=additional_special_tokens,\n             **kwargs,\n         )\n-\n-        self._tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(replacement=\"▁\", prepend_scheme=\"always\", split=True)\n-        self._tokenizer.decoder = decoders.Metaspace(replacement=\"▁\", prepend_scheme=\"always\", split=True)\n+        self._tokenizer.post_processor = processors.TemplateProcessing(\n+            single=f\"$A {eos_token}\",\n+            pair=f\"$A $B {eos_token}\",\n+            special_tokens=[(str(eos_token), self.convert_tokens_to_ids(str(eos_token)))],\n+        )\n \n \n __all__ = [\"PegasusTokenizer\"]"
        },
        {
            "sha": "d6526018b46342d30749d2dd772d4990e1ab5a2c",
            "filename": "src/transformers/models/plbart/tokenization_plbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fplbart%2Ftokenization_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fplbart%2Ftokenization_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Ftokenization_plbart.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -125,7 +125,6 @@ def __init__(\n         pad_token=\"<pad>\",\n         mask_token=\"<mask>\",\n         language_codes=\"base\",\n-        tokenizer_file=None,\n         src_lang=None,\n         tgt_lang=None,\n         sp_model_kwargs: Optional[dict[str, Any]] = None,\n@@ -171,7 +170,6 @@ def __init__(\n             cls_token=cls_token,\n             pad_token=pad_token,\n             mask_token=mask_token,\n-            tokenizer_file=tokenizer_file,\n             src_lang=src_lang,\n             tgt_lang=tgt_lang,\n             additional_special_tokens=_additional_special_tokens,"
        },
        {
            "sha": "2cc979345cb9669184389b1db48f022defd7e133",
            "filename": "src/transformers/models/qwen2/tokenization_qwen2.py",
            "status": "modified",
            "additions": 14,
            "deletions": 18,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fqwen2%2Ftokenization_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fqwen2%2Ftokenization_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Ftokenization_qwen2.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -14,10 +14,11 @@\n # limitations under the License.\n \"\"\"Tokenization classes for Qwen2.\"\"\"\n \n+from typing import Optional, Union\n+\n from tokenizers import AddedToken, Regex, Tokenizer, decoders, normalizers, pre_tokenizers\n from tokenizers.models import BPE\n \n-from ...tokenization_utils_base import generate_merges\n from ...tokenization_utils_tokenizers import TokenizersBackend\n from ...utils import logging\n \n@@ -38,33 +39,30 @@\n class Qwen2Tokenizer(TokenizersBackend):\n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n+    model = BPE\n \n     def __init__(\n         self,\n+        vocab: Optional[Union[str, dict[str, int]]] = None,\n+        merges: Optional[Union[str, list[str]]] = None,\n         vocab_file=None,\n         merges_file=None,\n-        unk_token=\"<|endoftext|>\",\n+        unk_token: str = \"<|endoftext|>\",\n         bos_token=None,\n-        eos_token=\"<|endoftext|>\",\n-        pad_token=\"<|endoftext|>\",\n+        eos_token: str = \"<|endoftext|>\",\n+        pad_token: str = \"<|endoftext|>\",\n         add_prefix_space=None,\n-        vocab=None,\n-        merges=None,\n         **kwargs,\n     ):\n         self.add_prefix_space = add_prefix_space if add_prefix_space is not None else False\n-\n-        if vocab is not None:\n-            self._vocab = (\n-                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n-            )\n-        else:\n-            self._vocab = {\n+        self._vocab = (\n+            vocab\n+            if vocab is not None\n+            else {\n                 \"<|endoftext|>\": 0,\n             }\n-        self._merges = merges if merges is not None else generate_merges(self._vocab)\n-\n+        )\n+        self._merges = merges or []\n         self._tokenizer = Tokenizer(\n             BPE(\n                 vocab=self._vocab,\n@@ -92,12 +90,10 @@ def __init__(\n                 ),\n             ]\n         )\n-        tokenizer_object = self._tokenizer\n \n         super().__init__(\n             vocab_file=vocab_file,\n             merges_file=merges_file,\n-            tokenizer_object=tokenizer_object,\n             unk_token=unk_token,\n             bos_token=bos_token,\n             eos_token=eos_token,"
        },
        {
            "sha": "f910d2fa1349e5ea8a644f079b94a1712908c335",
            "filename": "src/transformers/models/reformer/tokenization_reformer.py",
            "status": "modified",
            "additions": 11,
            "deletions": 28,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Freformer%2Ftokenization_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Freformer%2Ftokenization_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Ftokenization_reformer.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Tokenization class for model Reformer.\"\"\"\n \n-from typing import Optional\n+from typing import Optional, Union\n \n from tokenizers import Regex, Tokenizer, decoders, normalizers, pre_tokenizers\n from tokenizers.models import BPE\n@@ -60,38 +60,27 @@ class ReformerTokenizer(TokenizersBackend):\n             The token used for padding, for example when batching sequences of different lengths.\n         additional_special_tokens (`list[str]`, *optional*):\n             Additional special tokens used by the tokenizer.\n-        vocab (`dict`, *optional*):\n-            Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n-        merges (`list`, *optional*):\n-            Custom merges list. If not provided, merges are loaded from vocab_file.\n+        vocab (`str` or `dict[str, int]`, *optional*):\n+            Custom vocabulary dictionary. If not provided, vocabulary is loaded from `vocab_file`.\n+        merges (`str` or `list[str]`, *optional*):\n+            Custom merges list. If not provided, merges are loaded from `vocab_file`.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n+    model = BPE\n \n     def __init__(\n         self,\n-        vocab_file: Optional[str] = None,\n+        vocab: Optional[Union[str, dict[str, int]]] = None,\n+        merges: Optional[Union[str, list[str]]] = None,\n         eos_token: str = \"</s>\",\n         unk_token: str = \"<unk>\",\n         additional_special_tokens: Optional[list] = None,\n-        vocab: Optional[dict] = None,\n-        merges: Optional[list] = None,\n         **kwargs,\n     ):\n-        self.vocab_file = vocab_file\n-\n-        if vocab is not None:\n-            self._vocab = vocab\n-        else:\n-            self._vocab = {}\n-\n-        if merges is not None:\n-            # Convert lists to tuples if necessary (happens when loading from JSON)\n-            self._merges = [tuple(merge) if isinstance(merge, list) else merge for merge in merges]\n-        else:\n-            self._merges = []\n+        self._vocab = vocab or {}\n+        self._merges = merges or []\n \n         self._tokenizer = Tokenizer(\n             BPE(\n@@ -106,10 +95,7 @@ def __init__(\n \n         self._tokenizer.normalizer = normalizers.Sequence(\n             [\n-                normalizers.Replace(\"\\n\", \" \"),\n-                normalizers.Replace(\"\\r\", \" \"),\n-                normalizers.Replace(\"\\t\", \" \"),\n-                normalizers.Replace(Regex(r\" {2,}\"), \" \"),\n+                normalizers.Replace(Regex(r\"\\s{2,}|[\\n\\r\\t]\"), \" \"),\n                 normalizers.NFC(),\n                 normalizers.Strip(left=False, right=True),\n             ]\n@@ -118,10 +104,7 @@ def __init__(\n         self._tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(replacement=\"▁\", prepend_scheme=\"always\")\n         self._tokenizer.decoder = decoders.Metaspace(replacement=\"▁\", prepend_scheme=\"always\")\n \n-        tokenizer_object = self._tokenizer\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             eos_token=eos_token,\n             unk_token=unk_token,\n             additional_special_tokens=additional_special_tokens or [],"
        },
        {
            "sha": "a82df264010260eb8c1327a82ec52913bf128f46",
            "filename": "src/transformers/models/rembert/tokenization_rembert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Tokenization classes for RemBert model.\"\"\"\n \n-from typing import Optional\n+from typing import Optional, Union\n \n from tokenizers import Regex, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import Unigram\n@@ -74,11 +74,11 @@ class RemBertTokenizer(TokenizersBackend):\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n+    model = Unigram\n \n     def __init__(\n         self,\n-        vocab_file: Optional[str] = None,\n+        vocab: Optional[Union[str, list[tuple[str, float]]]] = None,\n         do_lower_case: bool = False,\n         keep_accents: bool = False,\n         bos_token: str = \"[CLS]\",\n@@ -90,11 +90,8 @@ def __init__(\n         mask_token: str = \"[MASK]\",\n         add_prefix_space: bool = True,\n         remove_space: bool = True,\n-        vocab: Optional[dict] = None,\n-        merges: Optional[list] = None,\n         **kwargs,\n     ):\n-        self.vocab_file = vocab_file\n         self.remove_space = remove_space\n         self.do_lower_case = do_lower_case\n         self.keep_accents = keep_accents\n@@ -147,11 +144,7 @@ def __init__(\n         self._tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(replacement=\"▁\", prepend_scheme=prepend_scheme)\n \n         self._tokenizer.decoder = decoders.Metaspace(replacement=\"▁\", prepend_scheme=prepend_scheme)\n-\n-        tokenizer_object = self._tokenizer\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             add_prefix_space=add_prefix_space,\n             do_lower_case=do_lower_case,\n             keep_accents=keep_accents,"
        },
        {
            "sha": "fd35bb15245859faca5175473c0a547f1991eb2b",
            "filename": "src/transformers/models/roberta/tokenization_roberta.py",
            "status": "modified",
            "additions": 18,
            "deletions": 27,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Froberta%2Ftokenization_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Froberta%2Ftokenization_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Ftokenization_roberta.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Tokenization classes for RoBERTa.\"\"\"\n \n-from typing import Optional\n+from typing import Optional, Union\n \n from tokenizers import Tokenizer, decoders, pre_tokenizers, processors\n from tokenizers.models import BPE\n@@ -59,6 +59,10 @@ class RobertaTokenizer(TokenizersBackend):\n     this superclass for more information regarding those methods.\n \n     Args:\n+        vocab (`str`, `dict` or `list`, *optional*):\n+            Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n+        merges (`str` or `list`, *optional*):\n+            Custom merges list. If not provided, merges are loaded from merges_file.\n         errors (`str`, *optional*, defaults to `\"replace\"`):\n             Paradigm to follow when decoding bytes to UTF-8. See\n             [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.\n@@ -102,18 +106,16 @@ class RobertaTokenizer(TokenizersBackend):\n             other word. (RoBERTa tokenizer detect beginning of words by the preceding space).\n         trim_offsets (`bool`, *optional*, defaults to `True`):\n             Whether the post processing step should trim offsets to avoid including whitespaces.\n-        vocab (`dict`, *optional*):\n-            Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n-        merges (`list`, *optional*):\n-            Custom merges list. If not provided, merges are loaded from merges_file.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n+    model = BPE\n \n     def __init__(\n         self,\n+        vocab: Optional[Union[str, dict[str, int]]] = None,\n+        merges: Optional[Union[str, list[str]]] = None,\n         errors: str = \"replace\",\n         bos_token: str = \"<s>\",\n         eos_token: str = \"</s>\",\n@@ -124,30 +126,22 @@ def __init__(\n         mask_token: str = \"<mask>\",\n         add_prefix_space: bool = False,\n         trim_offsets: bool = True,\n-        vocab: Optional[dict] = None,\n-        merges: Optional[list] = None,\n         **kwargs,\n     ):\n         self.add_prefix_space = add_prefix_space\n         self.trim_offsets = trim_offsets\n \n-        if vocab is not None:\n-            self._vocab = (\n-                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n-            )\n-        else:\n-            self._vocab = {\n+        if vocab is None:\n+            vocab = {\n                 str(pad_token): 0,\n                 str(unk_token): 1,\n                 str(cls_token): 2,\n                 str(sep_token): 3,\n                 str(mask_token): 4,\n             }\n+        self._vocab = vocab\n \n-        if merges is not None:\n-            self._merges = [tuple(merge) if isinstance(merge, list) else merge for merge in merges]\n-        else:\n-            self._merges = []\n+        self._merges = merges or []\n \n         self._tokenizer = Tokenizer(\n             BPE(\n@@ -162,17 +156,8 @@ def __init__(\n \n         self._tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=add_prefix_space)\n         self._tokenizer.decoder = decoders.ByteLevel()\n-        self._tokenizer.post_processor = processors.RobertaProcessing(\n-            sep=(str(sep_token), self._vocab.get(str(sep_token), 3)),\n-            cls=(str(cls_token), self._vocab.get(str(cls_token), 2)),\n-            add_prefix_space=add_prefix_space,\n-            trim_offsets=trim_offsets,\n-        )\n-\n-        tokenizer_object = self._tokenizer\n \n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             errors=errors,\n             bos_token=bos_token,\n             eos_token=eos_token,\n@@ -185,6 +170,12 @@ def __init__(\n             trim_offsets=trim_offsets,\n             **kwargs,\n         )\n+        self._tokenizer.post_processor = processors.RobertaProcessing(\n+            sep=(str(sep_token), self.sep_token_id),\n+            cls=(str(cls_token), self.cls_token_id),\n+            add_prefix_space=add_prefix_space,\n+            trim_offsets=trim_offsets,\n+        )\n \n \n __all__ = [\"RobertaTokenizer\"]"
        },
        {
            "sha": "d9216af7456f12fdce4064f12ee6f7673b633e84",
            "filename": "src/transformers/models/roformer/tokenization_roformer.py",
            "status": "modified",
            "additions": 77,
            "deletions": 412,
            "changes": 489,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_roformer.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -12,300 +12,29 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"Tokenization classes for RoFormer.\"\"\"\n+\"\"\"Tokenization class for RoFormer backed by 🤗 Tokenizers.\"\"\"\n \n-import collections\n-import os\n-import unicodedata\n from typing import Optional\n \n-from ...tokenization_python import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\n+from tokenizers import Tokenizer, decoders, models, normalizers, pre_tokenizers, processors\n+from tokenizers.pre_tokenizers import BertPreTokenizer, PreTokenizer\n+\n+from ...tokenization_utils_tokenizers import PreTrainedTokenizerFast\n from ...utils import logging\n+from .tokenization_utils import JiebaPreTokenizer\n \n \n logger = logging.get_logger(__name__)\n \n-VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\n-\n-\n-def load_vocab(vocab_file):\n-    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n-    vocab = collections.OrderedDict()\n-    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n-        tokens = reader.readlines()\n-    for index, token in enumerate(tokens):\n-        token = token.rstrip(\"\\n\")\n-        vocab[token] = index\n-    return vocab\n-\n-\n-def whitespace_tokenize(text):\n-    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n-    text = text.strip()\n-    if not text:\n-        return []\n-    tokens = text.split()\n-    return tokens\n-\n-\n-class BasicTokenizer:\n-    \"\"\"\n-    Constructs a BasicTokenizer that will run basic tokenization (punctuation splitting, lower casing, etc.).\n-\n-    Args:\n-        do_lower_case (`bool`, *optional*, defaults to `True`):\n-            Whether or not to lowercase the input when tokenizing.\n-        never_split (`Iterable`, *optional*):\n-            Collection of tokens which will never be split during tokenization. Only has an effect when\n-            `do_basic_tokenize=True`\n-        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n-            Whether or not to tokenize Chinese characters.\n-\n-            This should likely be deactivated for Japanese (see this\n-            [issue](https://github.com/huggingface/transformers/issues/328)).\n-        strip_accents (`bool`, *optional*):\n-            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n-            value for `lowercase` (as in the original BERT).\n-        do_split_on_punc (`bool`, *optional*, defaults to `True`):\n-            In some instances we want to skip the basic punctuation splitting so that later tokenization can capture\n-            the full context of the words, such as contractions.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        do_lower_case=True,\n-        never_split=None,\n-        tokenize_chinese_chars=True,\n-        strip_accents=None,\n-        do_split_on_punc=True,\n-    ):\n-        if never_split is None:\n-            never_split = []\n-        self.do_lower_case = do_lower_case\n-        self.never_split = set(never_split)\n-        self.tokenize_chinese_chars = tokenize_chinese_chars\n-        self.strip_accents = strip_accents\n-        self.do_split_on_punc = do_split_on_punc\n-\n-    def tokenize(self, text, never_split=None):\n-        \"\"\"\n-        Basic Tokenization of a piece of text. For sub-word tokenization, see WordPieceTokenizer.\n-\n-        Args:\n-            never_split (`List[str]`, *optional*)\n-                Kept for backward compatibility purposes. Now implemented directly at the base class level (see\n-                [`PreTrainedTokenizer.tokenize`]) List of token not to split.\n-        \"\"\"\n-        # union() returns a new set by concatenating the two sets.\n-        never_split = self.never_split.union(set(never_split)) if never_split else self.never_split\n-        text = self._clean_text(text)\n-\n-        # This was added on November 1st, 2018 for the multilingual and Chinese\n-        # models. This is also applied to the English models now, but it doesn't\n-        # matter since the English models were not trained on any Chinese data\n-        # and generally don't have any Chinese data in them (there are Chinese\n-        # characters in the vocabulary because Wikipedia does have some Chinese\n-        # words in the English Wikipedia.).\n-        if self.tokenize_chinese_chars:\n-            text = self._tokenize_chinese_chars(text)\n-        # prevents treating the same character with different unicode codepoints as different characters\n-        unicode_normalized_text = unicodedata.normalize(\"NFC\", text)\n-        orig_tokens = whitespace_tokenize(unicode_normalized_text)\n-        split_tokens = []\n-        for token in orig_tokens:\n-            if token not in never_split:\n-                if self.do_lower_case:\n-                    token = token.lower()\n-                    if self.strip_accents is not False:\n-                        token = self._run_strip_accents(token)\n-                elif self.strip_accents:\n-                    token = self._run_strip_accents(token)\n-            split_tokens.extend(self._run_split_on_punc(token, never_split))\n-\n-        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n-        return output_tokens\n-\n-    def _run_strip_accents(self, text):\n-        \"\"\"Strips accents from a piece of text.\"\"\"\n-        text = unicodedata.normalize(\"NFD\", text)\n-        output = []\n-        for char in text:\n-            cat = unicodedata.category(char)\n-            if cat == \"Mn\":\n-                continue\n-            output.append(char)\n-        return \"\".join(output)\n-\n-    def _run_split_on_punc(self, text, never_split=None):\n-        \"\"\"Splits punctuation on a piece of text.\"\"\"\n-        if not self.do_split_on_punc or (never_split is not None and text in never_split):\n-            return [text]\n-        chars = list(text)\n-        i = 0\n-        start_new_word = True\n-        output = []\n-        while i < len(chars):\n-            char = chars[i]\n-            if _is_punctuation(char):\n-                output.append([char])\n-                start_new_word = True\n-            else:\n-                if start_new_word:\n-                    output.append([])\n-                start_new_word = False\n-                output[-1].append(char)\n-            i += 1\n-\n-        return [\"\".join(x) for x in output]\n-\n-    def _tokenize_chinese_chars(self, text):\n-        \"\"\"Adds whitespace around any CJK character.\"\"\"\n-        output = []\n-        for char in text:\n-            cp = ord(char)\n-            if self._is_chinese_char(cp):\n-                output.append(\" \")\n-                output.append(char)\n-                output.append(\" \")\n-            else:\n-                output.append(char)\n-        return \"\".join(output)\n-\n-    def _is_chinese_char(self, cp):\n-        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n-        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n-        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n-        #\n-        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n-        # despite its name. The modern Korean Hangul alphabet is a different block,\n-        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n-        # space-separated words, so they are not treated specially and handled\n-        # like the all of the other languages.\n-        if (\n-            (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n-            or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n-        ):\n-            return True\n-\n-        return False\n-\n-    def _clean_text(self, text):\n-        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n-        output = []\n-        for char in text:\n-            cp = ord(char)\n-            if cp == 0 or cp == 0xFFFD or _is_control(char):\n-                continue\n-            if _is_whitespace(char):\n-                output.append(\" \")\n-            else:\n-                output.append(char)\n-        return \"\".join(output)\n-\n-\n-class WordpieceTokenizer:\n-    \"\"\"Runs WordPiece tokenization.\"\"\"\n-\n-    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):\n-        self.vocab = vocab\n-        self.unk_token = unk_token\n-        self.max_input_chars_per_word = max_input_chars_per_word\n-\n-    def tokenize(self, text):\n-        \"\"\"\n-        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n-        tokenization using the given vocabulary.\n-\n-        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n+VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\", \"tokenizer_file\": \"tokenizer.json\"}\n \n-        Args:\n-            text: A single token or whitespace separated tokens. This should have\n-                already been passed through *BasicTokenizer*.\n \n-        Returns:\n-            A list of wordpiece tokens.\n-        \"\"\"\n-\n-        output_tokens = []\n-        for token in whitespace_tokenize(text):\n-            chars = list(token)\n-            if len(chars) > self.max_input_chars_per_word:\n-                output_tokens.append(self.unk_token)\n-                continue\n-\n-            is_bad = False\n-            start = 0\n-            sub_tokens = []\n-            while start < len(chars):\n-                end = len(chars)\n-                cur_substr = None\n-                while start < end:\n-                    substr = \"\".join(chars[start:end])\n-                    if start > 0:\n-                        substr = \"##\" + substr\n-                    if substr in self.vocab:\n-                        cur_substr = substr\n-                        break\n-                    end -= 1\n-                if cur_substr is None:\n-                    is_bad = True\n-                    break\n-                sub_tokens.append(cur_substr)\n-                start = end\n-\n-            if is_bad:\n-                output_tokens.append(self.unk_token)\n-            else:\n-                output_tokens.extend(sub_tokens)\n-        return output_tokens\n-\n-\n-class RoFormerTokenizer(PreTrainedTokenizer):\n+class RoFormerTokenizer(PreTrainedTokenizerFast):\n     r\"\"\"\n     Construct a RoFormer tokenizer. Based on [Rust Jieba](https://pypi.org/project/rjieba/).\n \n-    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n-    this superclass for more information regarding those methods.\n-\n-    Args:\n-        vocab_file (`str`):\n-            File containing the vocabulary.\n-        do_lower_case (`bool`, *optional*, defaults to `True`):\n-            Whether or not to lowercase the input when tokenizing.\n-        do_basic_tokenize (`bool`, *optional*, defaults to `True`):\n-            Whether or not to do basic tokenization before WordPiece.\n-        never_split (`Iterable`, *optional*):\n-            Collection of tokens which will never be split during tokenization. Only has an effect when\n-            `do_basic_tokenize=True`\n-        unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n-            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n-            sequence classification or for a text and a question for question answering. It is also used as the last\n-            token of a sequence built with special tokens.\n-        pad_token (`str`, *optional*, defaults to `\"[PAD]\"`):\n-            The token used for padding, for example when batching sequences of different lengths.\n-        cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n-            The classifier token which is used when doing sequence classification (classification of the whole sequence\n-            instead of per-token classification). It is the first token of the sequence when built with special tokens.\n-        mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n-            The token used for masking values. This is the token used when training this model with masked language\n-            modeling. This is the token which the model will try to predict.\n-        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n-            Whether or not to tokenize Chinese characters.\n-\n-            This should likely be deactivated for Japanese (see this\n-            [issue](https://github.com/huggingface/transformers/issues/328)).\n-        strip_accents (`bool`, *optional*):\n-            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n-            value for `lowercase` (as in the original BERT).\n+    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n+    refer to this superclass for more information regarding those methods.\n \n     Example:\n \n@@ -315,16 +44,15 @@ class RoFormerTokenizer(PreTrainedTokenizer):\n     >>> tokenizer = RoFormerTokenizer.from_pretrained(\"junnyu/roformer_chinese_base\")\n     >>> tokenizer.tokenize(\"今天天气非常好。\")\n     ['今', '天', '天', '气', '非常', '好', '。']\n-    ```\"\"\"\n+    ```\n+    \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n \n     def __init__(\n         self,\n-        vocab_file,\n+        vocab: Optional[dict[str, int]] = None,\n         do_lower_case=True,\n-        do_basic_tokenize=True,\n-        never_split=None,\n         unk_token=\"[UNK]\",\n         sep_token=\"[SEP]\",\n         pad_token=\"[PAD]\",\n@@ -334,35 +62,19 @@ def __init__(\n         strip_accents=None,\n         **kwargs,\n     ):\n-        if not os.path.isfile(vocab_file):\n-            raise ValueError(\n-                f\"Can't find a vocabulary file at path '{vocab_file}'. To load the vocabulary from a Google pretrained\"\n-                \" model use `tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n-            )\n-        self.vocab = load_vocab(vocab_file)\n-        self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])\n-        self.do_basic_tokenize = do_basic_tokenize\n-        if do_basic_tokenize:\n-            self.basic_tokenizer = BasicTokenizer(\n-                do_lower_case=do_lower_case,\n-                never_split=never_split,\n-                tokenize_chinese_chars=tokenize_chinese_chars,\n-                strip_accents=strip_accents,\n-            )\n-        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=str(unk_token))\n-        try:\n-            import rjieba\n-        except ImportError:\n-            raise ImportError(\n-                \"You need to install rjieba to use RoFormerTokenizer. \"\n-                \"See https://pypi.org/project/rjieba/ for installation.\"\n-            )\n-        self.jieba = rjieba\n+        tokenizer = Tokenizer(models.WordPiece(vocab, unk_token=str(unk_token)))\n+        tokenizer.normalizer = normalizers.BertNormalizer(\n+            clean_text=True,\n+            handle_chinese_chars=False,\n+            strip_accents=strip_accents,\n+            lowercase=do_lower_case,\n+        )\n+        tokenizer.pre_tokenizer = pre_tokenizers.PreTokenizer.custom(JiebaPreTokenizer(vocab))\n \n+        tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n+        self._tokenizer = tokenizer\n         super().__init__(\n             do_lower_case=do_lower_case,\n-            do_basic_tokenize=do_basic_tokenize,\n-            never_split=never_split,\n             unk_token=unk_token,\n             sep_token=sep_token,\n             pad_token=pad_token,\n@@ -372,67 +84,30 @@ def __init__(\n             strip_accents=strip_accents,\n             **kwargs,\n         )\n-\n-    @property\n-    def do_lower_case(self):\n-        return self.basic_tokenizer.do_lower_case\n-\n-    @property\n-    def vocab_size(self):\n-        return len(self.vocab)\n+        cls_ = str(cls_token)\n+        sep_ = str(sep_token)\n+        self._tokenizer.post_processor = processors.TemplateProcessing(\n+            single=f\"{cls_}:0 $A:0 {sep_}:0\",\n+            pair=f\"{cls_}:0 $A:0 {sep_}:0 $B:1 {sep_}:1\",\n+            special_tokens=[\n+                (cls_, self.cls_token_id),\n+                (sep_, self.sep_token_id),\n+            ],\n+        )\n \n     def __getstate__(self):\n         state = self.__dict__.copy()\n-        state[\"jieba\"] = None\n+        tokenizer_copy = Tokenizer.from_str(state[\"_tokenizer\"].to_str())\n+        tokenizer_copy.pre_tokenizer = BertPreTokenizer()\n+        state[\"_tokenizer\"] = tokenizer_copy\n         return state\n \n     def __setstate__(self, d):\n         self.__dict__ = d\n-        import rjieba\n-\n-        self.jieba = rjieba\n-\n-    def get_vocab(self):\n-        return dict(self.vocab, **self.added_tokens_encoder)\n-\n-    def _tokenize(self, text, use_jieba=True):\n-        split_tokens = []\n-        if use_jieba:\n-            for wholword in self.jieba.cut(text, False):\n-                if wholword in self.vocab:\n-                    split_tokens.append(wholword)\n-                else:\n-                    # use bert tokenizer to _tokenize\n-                    char_list = self._tokenize(wholword, use_jieba=False)\n-                    split_tokens.extend(char_list)\n-        else:\n-            if self.do_basic_tokenize:\n-                for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n-                    # If the token is part of the never_split set\n-                    if token in self.basic_tokenizer.never_split:\n-                        split_tokens.append(token)\n-                    else:\n-                        split_tokens += self.wordpiece_tokenizer.tokenize(token)\n-            else:\n-                split_tokens = self.wordpiece_tokenizer.tokenize(text)\n-        return split_tokens\n-\n-    def _convert_token_to_id(self, token):\n-        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n-        return self.vocab.get(token, self.vocab.get(self.unk_token))\n-\n-    def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        return self.ids_to_tokens.get(index, self.unk_token)\n-\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n-        out_string = \" \".join(tokens).replace(\" ##\", \"\").strip()\n-        return out_string\n-\n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n+        vocab = self.__dict__[\"_tokenizer\"].get_vocab()\n+        self.__dict__[\"_tokenizer\"].pre_tokenizer = PreTokenizer.custom(JiebaPreTokenizer(vocab))\n+\n+    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n         adding special tokens. A RoFormer sequence has the following format:\n@@ -449,59 +124,49 @@ def build_inputs_with_special_tokens(\n         Returns:\n             `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n         \"\"\"\n-        if token_ids_1 is None:\n-            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        sep = [self.sep_token_id]\n-        return cls + token_ids_0 + sep + token_ids_1 + sep\n+        output = [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n \n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n+        if token_ids_1 is not None:\n+            output += token_ids_1 + [self.sep_token_id]\n+\n+        return output\n+\n+    def create_token_type_ids_from_sequences(\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n     ) -> list[int]:\n         \"\"\"\n-        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` method.\n+        Create token type IDs for RoFormer sequence pairs.\n \n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n+        The first sequence and associated special tokens are mapped to 0, while the second sequence (if provided) and\n+        its trailing separator are mapped to 1.\n         \"\"\"\n+        sep = [self.sep_token_id]\n+        cls = [self.cls_token_id]\n \n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n+        if token_ids_1 is None:\n+            return len(cls + token_ids_0 + sep) * [0]\n \n-        if token_ids_1 is not None:\n-            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n-        return [1] + ([0] * len(token_ids_0)) + [1]\n+        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n \n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        index = 0\n-        if os.path.isdir(save_directory):\n-            vocab_file = os.path.join(\n-                save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-            )\n-        else:\n-            vocab_file = (filename_prefix + \"-\" if filename_prefix else \"\") + save_directory\n-        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n-            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n-                if index != token_index:\n-                    logger.warning(\n-                        f\"Saving vocabulary to {vocab_file}: vocabulary indices are not consecutive.\"\n-                        \" Please check that the vocabulary is not corrupted!\"\n-                    )\n-                    index = token_index\n-                writer.write(token + \"\\n\")\n-                index += 1\n-        return (vocab_file,)\n-\n-\n-__all__ = [\"RoFormerTokenizer\"]\n+        files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n+        return tuple(files)\n+\n+    def save_pretrained(\n+        self,\n+        save_directory,\n+        legacy_format=None,\n+        filename_prefix=None,\n+        push_to_hub=False,\n+        **kwargs,\n+    ):\n+        self.backend_tokenizer.pre_tokenizer = BertPreTokenizer()\n+        result = super().save_pretrained(save_directory, legacy_format, filename_prefix, push_to_hub, **kwargs)\n+        vocab = self.backend_tokenizer.get_vocab()\n+        self.backend_tokenizer.pre_tokenizer = PreTokenizer.custom(JiebaPreTokenizer(vocab))\n+        return result\n+\n+\n+RoFormerTokenizerFast = RoFormerTokenizer\n+\n+__all__ = [\"RoFormerTokenizer\", \"RoFormerTokenizerFast\"]"
        },
        {
            "sha": "fb91a325419b901f2a486c066a0b1fdcd4f16b08",
            "filename": "src/transformers/models/roformer/tokenization_roformer_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 160,
            "changes": 160,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6df2ce9c01118036772d5d251400de3e4297d7/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_roformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6df2ce9c01118036772d5d251400de3e4297d7/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_roformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_roformer_fast.py?ref=9a6df2ce9c01118036772d5d251400de3e4297d7",
            "patch": "@@ -1,160 +0,0 @@\n-# coding=utf-8\n-# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Tokenization classes for RoFormer.\"\"\"\n-\n-import json\n-from typing import Optional\n-\n-from tokenizers import normalizers\n-from tokenizers.pre_tokenizers import BertPreTokenizer, PreTokenizer\n-\n-from ...tokenization_utils_tokenizers import PreTrainedTokenizerFast\n-from ...utils import logging\n-from .tokenization_roformer import RoFormerTokenizer\n-from .tokenization_utils import JiebaPreTokenizer\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\", \"tokenizer_file\": \"tokenizer.json\"}\n-\n-\n-class RoFormerTokenizerFast(PreTrainedTokenizerFast):\n-    r\"\"\"\n-    Construct a \"fast\" RoFormer tokenizer (backed by HuggingFace's *tokenizers* library).\n-\n-    [`RoFormerTokenizerFast`] is almost identical to [`BertTokenizerFast`] and runs end-to-end tokenization:\n-    punctuation splitting and wordpiece. There are some difference between them when tokenizing Chinese.\n-\n-    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n-    refer to this superclass for more information regarding those methods.\n-\n-    Example:\n-\n-    ```python\n-    >>> from transformers import RoFormerTokenizerFast\n-\n-    >>> tokenizer = RoFormerTokenizerFast.from_pretrained(\"junnyu/roformer_chinese_base\")\n-    >>> tokenizer.tokenize(\"今天天气非常好。\")\n-    ['今', '天', '天', '气', '非常', '好', '。']\n-    ```\"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    slow_tokenizer_class = RoFormerTokenizer\n-\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        tokenizer_file=None,\n-        do_lower_case=True,\n-        unk_token=\"[UNK]\",\n-        sep_token=\"[SEP]\",\n-        pad_token=\"[PAD]\",\n-        cls_token=\"[CLS]\",\n-        mask_token=\"[MASK]\",\n-        tokenize_chinese_chars=True,\n-        strip_accents=None,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            vocab_file,\n-            tokenizer_file=tokenizer_file,\n-            do_lower_case=do_lower_case,\n-            unk_token=unk_token,\n-            sep_token=sep_token,\n-            pad_token=pad_token,\n-            cls_token=cls_token,\n-            mask_token=mask_token,\n-            tokenize_chinese_chars=tokenize_chinese_chars,\n-            strip_accents=strip_accents,\n-            **kwargs,\n-        )\n-\n-        normalizer_state = json.loads(self.backend_tokenizer.normalizer.__getstate__())\n-        normalizer_class = getattr(normalizers, normalizer_state.pop(\"type\"))\n-        normalizer_state[\"lowercase\"] = do_lower_case\n-        normalizer_state[\"strip_accents\"] = strip_accents\n-        self.backend_tokenizer.normalizer = normalizer_class(**normalizer_state)\n-\n-        vocab = self.backend_tokenizer.get_vocab()\n-        self.backend_tokenizer.pre_tokenizer = PreTokenizer.custom(JiebaPreTokenizer(vocab))\n-\n-        self.do_lower_case = do_lower_case\n-        self.strip_accents = strip_accents\n-\n-    def _post_init(self):\n-        super()._post_init()\n-        normalizer_state = json.loads(self.backend_tokenizer.normalizer.__getstate__())\n-        normalizer_class = getattr(normalizers, normalizer_state.pop(\"type\"))\n-        normalizer_state[\"lowercase\"] = self.do_lower_case\n-        normalizer_state[\"strip_accents\"] = getattr(self, \"strip_accents\", None)\n-        self.backend_tokenizer.normalizer = normalizer_class(**normalizer_state)\n-        vocab = self.backend_tokenizer.get_vocab()\n-        self.backend_tokenizer.pre_tokenizer = PreTokenizer.custom(JiebaPreTokenizer(vocab))\n-\n-    def __getstate__(self):\n-        state = self.__dict__.copy()\n-        state[\"_tokenizer\"].pre_tokenizer = BertPreTokenizer()\n-        return state\n-\n-    def __setstate__(self, d):\n-        self.__dict__ = d\n-        vocab = self.__dict__[\"_tokenizer\"].get_vocab()\n-        self.__dict__[\"_tokenizer\"].pre_tokenizer = PreTokenizer.custom(JiebaPreTokenizer(vocab))\n-\n-    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A RoFormer sequence has the following format:\n-\n-        - single sequence: `[CLS] X [SEP]`\n-        - pair of sequences: `[CLS] A [SEP] B [SEP]`\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        output = [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-\n-        if token_ids_1 is not None:\n-            output += token_ids_1 + [self.sep_token_id]\n-\n-        return output\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n-        return tuple(files)\n-\n-    def save_pretrained(\n-        self,\n-        save_directory,\n-        legacy_format=None,\n-        filename_prefix=None,\n-        push_to_hub=False,\n-        **kwargs,\n-    ):\n-        self.backend_tokenizer.pre_tokenizer = BertPreTokenizer()\n-        result = super().save_pretrained(save_directory, legacy_format, filename_prefix, push_to_hub, **kwargs)\n-        vocab = self.backend_tokenizer.get_vocab()\n-        self.backend_tokenizer.pre_tokenizer = PreTokenizer.custom(JiebaPreTokenizer(vocab))\n-        return result\n-\n-\n-__all__ = [\"RoFormerTokenizerFast\"]"
        },
        {
            "sha": "6fbb1cd3cb0669dc4da659ae97ca6e03b57167c4",
            "filename": "src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py",
            "status": "modified",
            "additions": 27,
            "deletions": 59,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Ftokenization_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Ftokenization_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Ftokenization_seamless_m4t.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -60,7 +60,7 @@ class SeamlessM4TTokenizer(TokenizersBackend):\n     Args:\n         vocab (`list` or `dict`, *optional*):\n             List of (token, score) tuples or dict mapping tokens to indices. If not provided, uses default vocab.\n-        merges (`list`, *optional*):\n+        merges (`str` or `list`, *optional*):\n             List of merge rules for BPE model. If not provided, uses empty list.\n         bos_token (`str`, *optional*, defaults to `\"<s>\"`):\n             The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n@@ -104,15 +104,15 @@ class SeamlessM4TTokenizer(TokenizersBackend):\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n+    model = BPE\n \n-    prefix_tokens: list[int] = []\n-    suffix_tokens: list[int] = []\n+    prefix_tokens: list[int] = None\n+    suffix_tokens: list[int] = None\n \n     def __init__(\n         self,\n-        vocab: Optional[list] = None,\n-        merges: Optional[list] = None,\n+        vocab: Optional[Union[str, dict[str, int]]] = None,\n+        merges: Optional[Union[str, list[str]]] = None,\n         bos_token=\"<s>\",\n         eos_token=\"</s>\",\n         sep_token=\"</s>\",\n@@ -126,59 +126,14 @@ def __init__(\n         vocab_file=None,\n         **kwargs,\n     ):\n-        if vocab is None:\n-            vocab = {\n-                str(pad_token): 0,\n-                str(unk_token): 1,\n-                str(bos_token): 2,\n-                str(eos_token): 3,\n-            }\n-\n-        # Process vocab - SeamlessM4T uses fairseq vocab alignment: <pad>=0, <unk>=1, <s>=2, </s>=3, then SPM pieces[3:]\n-        if isinstance(vocab, list):\n-            # Convert list of (token, score) tuples to dict {token: idx}\n-            # Check if vocab is already in SeamlessM4T order (pad, unk, s, /s) or tokenizer.json order (unk, s, /s, ...)\n-            first_tokens = [str(item[0]) if isinstance(item, (list, tuple)) else str(item) for item in vocab[:4]]\n-            is_seamless_order = (\n-                len(first_tokens) >= 4\n-                and first_tokens[0] == str(pad_token)\n-                and first_tokens[1] == str(unk_token)\n-                and first_tokens[2] == str(bos_token)\n-                and first_tokens[3] == str(eos_token)\n-            )\n-\n-            if is_seamless_order:\n-                # Already in correct order, use list index directly as token ID\n-                vocab_dict = {}\n-                for idx, item in enumerate(vocab):\n-                    token = str(item[0]) if isinstance(item, (list, tuple)) else str(item)\n-                    vocab_dict[token] = idx\n-                self._vocab = vocab_dict\n-            else:\n-                # Reorder to fairseq: <pad>, <unk>, <s>, </s>, ... (rest of vocab)\n-                vocab_dict = {}\n-                vocab_dict[str(pad_token)] = 0\n-                vocab_dict[str(unk_token)] = 1\n-                vocab_dict[str(bos_token)] = 2\n-                vocab_dict[str(eos_token)] = 3\n-\n-                # Add rest of vocab starting from index 4, skipping tokens we already added\n-                idx = 4\n-                for item in vocab:\n-                    token = str(item[0]) if isinstance(item, (list, tuple)) else str(item)\n-                    if token not in vocab_dict:\n-                        vocab_dict[token] = idx\n-                        idx += 1\n-\n-                self._vocab = vocab_dict\n-        else:\n-            self._vocab = vocab\n-\n-        if merges is None:\n-            self._merges = []\n-        else:\n-            self._merges = [tuple(merge) if isinstance(merge, list) else merge for merge in merges]\n+        self._vocab = vocab or {\n+            str(pad_token): 0,\n+            str(unk_token): 1,\n+            str(bos_token): 2,\n+            str(eos_token): 3,\n+        }\n \n+        self._merges = merges or []\n         self._tokenizer = Tokenizer(\n             BPE(\n                 vocab=self._vocab,\n@@ -216,7 +171,6 @@ def __init__(\n             kwargs.setdefault(\"additional_special_tokens\", additional_special_tokens)\n \n         super().__init__(\n-            tokenizer_object=self._tokenizer,\n             bos_token=bos_token,\n             eos_token=eos_token,\n             sep_token=sep_token,\n@@ -245,6 +199,20 @@ def __init__(\n \n         self.set_tgt_lang_special_tokens(self._tgt_lang)\n \n+    @classmethod\n+    def convert_from_spm_model(cls, vocab, **kwargs):\n+        \"\"\"When converting from spm, offset is needed to account for special tokens.\"\"\"\n+        _vocab = {\n+            \"<pad>\": 0,\n+            \"<unk>\": 1,\n+            \"<s>\": 2,\n+            \"</s>\": 3,\n+        }\n+        for i, token in enumerate(list(vocab.keys())):\n+            _vocab[token] = i + 1  # offset by 1 to account for special tokens\n+        kwargs[\"vocab\"] = _vocab\n+        return kwargs\n+\n     @property\n     def src_lang(self) -> str:\n         return self._src_lang"
        },
        {
            "sha": "bfe0082592a6e927ab74b0657ce0e07bb6e73a96",
            "filename": "src/transformers/models/splinter/tokenization_splinter.py",
            "status": "modified",
            "additions": 9,
            "deletions": 28,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fsplinter%2Ftokenization_splinter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fsplinter%2Ftokenization_splinter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsplinter%2Ftokenization_splinter.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -16,7 +16,7 @@\n \"\"\"Tokenization classes for Splinter.\"\"\"\n \n import collections\n-from typing import Optional\n+from typing import Optional, Union\n \n from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import WordPiece\n@@ -72,16 +72,17 @@ class SplinterTokenizer(TokenizersBackend):\n         strip_accents (`bool`, *optional*):\n             Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n             value for `lowercase`.\n-        vocab (`dict`, *optional*):\n+        vocab (`str`, `dict` or `list`, *optional*):\n             Custom vocabulary dictionary. If not provided, a minimal vocabulary is created.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n+    model = WordPiece\n \n     def __init__(\n         self,\n+        vocab: Optional[Union[str, dict[str, int]]] = None,\n         do_lower_case: bool = True,\n         unk_token: str = \"[UNK]\",\n         sep_token: str = \"[SEP]\",\n@@ -91,15 +92,12 @@ def __init__(\n         question_token: str = \"[QUESTION]\",\n         tokenize_chinese_chars: bool = True,\n         strip_accents: Optional[bool] = None,\n-        vocab: Optional[dict] = None,\n         **kwargs,\n     ):\n-        if vocab is not None:\n-            self._vocab = (\n-                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n-            )\n-        else:\n-            self._vocab = {\n+        self._vocab = (\n+            vocab\n+            if vocab is not None\n+            else {\n                 str(pad_token): 0,\n                 str(unk_token): 1,\n                 str(cls_token): 2,\n@@ -108,6 +106,7 @@ def __init__(\n                 str(question_token): 5,\n                 \".\": 6,\n             }\n+        )\n \n         self._tokenizer = Tokenizer(WordPiece(self._vocab, unk_token=str(unk_token)))\n \n@@ -120,10 +119,7 @@ def __init__(\n         self._tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n         self._tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n \n-        tokenizer_object = self._tokenizer\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             unk_token=unk_token,\n             sep_token=sep_token,\n             pad_token=pad_token,\n@@ -136,21 +132,6 @@ def __init__(\n             **kwargs,\n         )\n \n-        if hasattr(self, \"_tokenizer\") and self._tokenizer.normalizer is not None:\n-            import json\n-\n-            pre_tok_state = json.loads(self._tokenizer.normalizer.__getstate__())\n-            if (\n-                pre_tok_state.get(\"lowercase\", do_lower_case) != do_lower_case\n-                or pre_tok_state.get(\"strip_accents\", strip_accents) != strip_accents\n-                or pre_tok_state.get(\"handle_chinese_chars\", tokenize_chinese_chars) != tokenize_chinese_chars\n-            ):\n-                pre_tok_class = getattr(normalizers, pre_tok_state.pop(\"type\"))\n-                pre_tok_state[\"lowercase\"] = do_lower_case\n-                pre_tok_state[\"strip_accents\"] = strip_accents\n-                pre_tok_state[\"handle_chinese_chars\"] = tokenize_chinese_chars\n-                self._tokenizer.normalizer = pre_tok_class(**pre_tok_state)\n-\n         self.do_lower_case = do_lower_case\n         self.tokenize_chinese_chars = tokenize_chinese_chars\n         self.strip_accents = strip_accents"
        },
        {
            "sha": "1d63a164d10776dd524aa468d935a26a9c0079aa",
            "filename": "src/transformers/models/t5/tokenization_t5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 8,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Ft5%2Ftokenization_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Ft5%2Ftokenization_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Ftokenization_t5.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -15,6 +15,7 @@\n \"\"\"Tokenization class for model T5.\"\"\"\n \n import re\n+from typing import Optional, Union\n \n from tokenizers import Tokenizer, decoders, pre_tokenizers, processors\n from tokenizers.models import Unigram\n@@ -61,26 +62,24 @@ class T5Tokenizer(TokenizersBackend):\n             calling get_sentinel_tokens method and token ids can be by calling get_sentinel_token_ids method\n         additional_special_tokens (`list[str]`, *optional*):\n             Additional special tokens used by the tokenizer.\n-        vocab (`dict`, *optional*):\n+        vocab (`str`, `dict` or `list`, *optional*):\n             Custom vocabulary dict. If not provided, a minimal vocabulary is created using the special tokens.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n+    model = Unigram\n \n     def __init__(\n         self,\n+        vocab: Optional[Union[str, list[tuple[str, float]]]] = None,\n         eos_token=\"</s>\",\n         unk_token=\"<unk>\",\n         pad_token=\"<pad>\",\n         extra_ids=100,\n         additional_special_tokens=None,\n-        vocab=None,\n-        vocab_file=None,\n         **kwargs,\n     ):\n-        self.vocab_file = vocab_file\n         self._extra_ids = extra_ids\n \n         # Handle extra_ids and additional_special_tokens\n@@ -130,10 +129,7 @@ def __init__(\n \n         self._tokenizer.decoder = decoders.Metaspace(replacement=\"▁\", prepend_scheme=\"always\", split=True)\n \n-        tokenizer_object = self._tokenizer\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             eos_token=eos_token,\n             unk_token=unk_token,\n             pad_token=pad_token,"
        },
        {
            "sha": "31f19ba6bf85c0d3f4126fa75709f2071a040dcb",
            "filename": "src/transformers/models/udop/tokenization_udop.py",
            "status": "modified",
            "additions": 5,
            "deletions": 13,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -183,10 +183,11 @@ class UdopTokenizer(TokenizersBackend):\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n+    model = Unigram\n \n     def __init__(\n         self,\n+        vocab: Optional[Union[str, list[tuple[str, float]]]] = None,\n         eos_token=\"</s>\",\n         sep_token=\"</s>\",\n         unk_token=\"<unk>\",\n@@ -196,7 +197,6 @@ def __init__(\n         pad_token_label=-100,\n         only_label_first_subword=True,\n         extra_special_tokens=None,\n-        vocab=None,\n         **kwargs,\n     ):\n         if \"additional_special_tokens\" in kwargs and \"extra_special_tokens\" not in kwargs:\n@@ -205,24 +205,17 @@ def __init__(\n             kwargs[\"extra_special_tokens\"] = extra_special_tokens\n \n         if vocab is None:\n-            vocab_scores = [(str(pad_token), 0.0), (str(eos_token), 0.0), (str(unk_token), 0.0), (\"▁\", -2.0)]\n-        elif isinstance(vocab, dict):\n-            vocab_scores = [(str(token), float(score)) for token, score in vocab.items()]\n-        elif isinstance(vocab, list) and len(vocab) > 0:\n-            if isinstance(vocab[0], (tuple, list)):\n-                vocab_scores = [(str(token), float(score)) for token, score in vocab]\n-            else:\n-                vocab_scores = [(str(token), 0.0) for token in vocab]\n+            vocab = [(str(pad_token), 0.0), (str(eos_token), 0.0), (str(unk_token), 0.0), (\"▁\", -2.0)]\n \n         unk_id = 2\n-        for idx, (token, _) in enumerate(vocab_scores):\n+        for idx, (token, _) in enumerate(vocab):\n             if token == str(unk_token):\n                 unk_id = idx\n                 break\n \n         self._tokenizer = Tokenizer(\n             Unigram(\n-                vocab_scores,\n+                vocab,\n                 unk_id=unk_id,\n                 byte_fallback=False,\n             )\n@@ -240,7 +233,6 @@ def __init__(\n         self._tokenizer.decoder = decoders.Metaspace(replacement=\"▁\", prepend_scheme=\"always\", split=True)\n \n         super().__init__(\n-            tokenizer_object=self._tokenizer,\n             eos_token=eos_token,\n             sep_token=sep_token,\n             unk_token=unk_token,"
        },
        {
            "sha": "894b7853a1fcd9b6101eb5de6934551966169d12",
            "filename": "src/transformers/models/whisper/tokenization_whisper.py",
            "status": "modified",
            "additions": 4,
            "deletions": 15,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -19,7 +19,7 @@\n import re\n import warnings\n from functools import lru_cache\n-from typing import Optional\n+from typing import Optional, Union\n \n import numpy as np\n from tokenizers import AddedToken, Tokenizer, decoders, pre_tokenizers, processors\n@@ -204,10 +204,11 @@ class WhisperTokenizer(TokenizersBackend):\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n+    model = BPE\n \n     def __init__(\n         self,\n-        vocab=None,\n+        vocab: Optional[Union[str, dict[str, int]]] = None,\n         merges=None,\n         normalizer_file=None,\n         unk_token=\"<|endoftext|>\",\n@@ -253,7 +254,6 @@ def __init__(\n         self._tokenizer.decoder = decoders.ByteLevel()\n \n         super().__init__(\n-            tokenizer_object=self._tokenizer,\n             unk_token=unk_token,\n             bos_token=bos_token,\n             eos_token=eos_token,\n@@ -276,18 +276,7 @@ def __init__(\n         self.language = language\n         self.task = task\n         self.predict_timestamps = predict_timestamps\n-\n-        self._post_init()\n-\n-    def _post_init(self):\n-        \"\"\"Post-initialization hook to set up prefix tokens after the tokenizer is fully loaded.\"\"\"\n-        super()._post_init()\n-        # Set up prefix tokens if language or task is specified (may be set from config in from_pretrained)\n-        if hasattr(self, \"language\") and hasattr(self, \"task\") and hasattr(self, \"predict_timestamps\"):\n-            if self.language is not None or self.task is not None:\n-                self.set_prefix_tokens(\n-                    language=self.language, task=self.task, predict_timestamps=self.predict_timestamps\n-                )\n+        self.set_prefix_tokens()\n \n     # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer._decode_with_timestamps\n     def _decode_with_timestamps("
        },
        {
            "sha": "919b8396268942d703f93bb18fc4e89f90fd09a7",
            "filename": "src/transformers/models/xglm/tokenization_xglm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 9,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fxglm%2Ftokenization_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fxglm%2Ftokenization_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Ftokenization_xglm.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Tokenization classes for XGLM.\"\"\"\n \n-from typing import Optional\n+from typing import Optional, Union\n \n from tokenizers import Regex, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import Unigram\n@@ -50,7 +50,7 @@ class XGLMTokenizer(TokenizersBackend):\n             The unknown token.\n         pad_token (`str`, *optional*, defaults to `\"<pad>\"`):\n             The token used for padding.\n-        vocab (`dict`, *optional*):\n+        vocab (`str`, `dict` or `list`, *optional*):\n             Custom vocabulary dictionary. If not provided, a minimal vocabulary is created.\n         merges (`list[tuple[str, str]]`, *optional*):\n             Custom merge rules for BPE. If not provided, merges are generated from the vocabulary.\n@@ -60,18 +60,17 @@ class XGLMTokenizer(TokenizersBackend):\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n+    model = Unigram\n \n     def __init__(\n         self,\n+        vocab: Optional[Union[str, list[tuple[str, float]]]] = None,\n         bos_token: str = \"<s>\",\n         eos_token: str = \"</s>\",\n         sep_token: str = \"</s>\",\n         cls_token: str = \"<s>\",\n         unk_token: str = \"<unk>\",\n         pad_token: str = \"<pad>\",\n-        vocab: Optional[dict] = None,\n-        merges: Optional[list[tuple[str, str]]] = None,\n         add_prefix_space: bool = True,\n         **kwargs,\n     ):\n@@ -106,11 +105,7 @@ def __init__(\n         prepend_scheme = \"always\" if add_prefix_space else \"never\"\n         self._tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(replacement=\"▁\", prepend_scheme=prepend_scheme)\n         self._tokenizer.decoder = decoders.Metaspace(replacement=\"▁\", prepend_scheme=prepend_scheme)\n-\n-        tokenizer_object = self._tokenizer\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             bos_token=bos_token,\n             eos_token=eos_token,\n             sep_token=sep_token,"
        },
        {
            "sha": "6c8869c9bd22b2ecfcf43140828c9c41c57b1330",
            "filename": "src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py",
            "status": "modified",
            "additions": 9,
            "deletions": 16,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Ftokenization_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Ftokenization_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Ftokenization_xlm_roberta.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License\n \"\"\"Tokenization classes for XLM-RoBERTa model (Tokenizers backend).\"\"\"\n \n-from typing import Optional\n+from typing import Optional, Union\n \n from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import Unigram\n@@ -47,26 +47,24 @@ class XLMRobertaTokenizer(TokenizersBackend):\n         pad_token (`str`, optional, defaults to `\"<pad>\"`): The padding token.\n         mask_token (`str`, optional, defaults to `\"<mask>\"`): The mask token.\n         add_prefix_space (`bool`, optional, defaults to `True`): Whether to add an initial space.\n-        vocab (`dict`, optional): Custom vocabulary dictionary.\n-        merges (`list`, optional): Custom merges list.\n+        vocab (`str`, `dict` or `list`, optional): Custom vocabulary dictionary.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n+    model = Unigram\n \n     def __init__(\n         self,\n+        vocab: Optional[Union[str, list[tuple[str, float]]]] = None,\n+        add_prefix_space: bool = True,\n         bos_token: str = \"<s>\",\n         eos_token: str = \"</s>\",\n         sep_token: str = \"</s>\",\n         cls_token: str = \"<s>\",\n         unk_token: str = \"<unk>\",\n         pad_token: str = \"<pad>\",\n         mask_token: str = \"<mask>\",\n-        add_prefix_space: bool = True,\n-        vocab: Optional[dict] = None,\n-        vocab_file: Optional[str] = None,\n         **kwargs,\n     ):\n         self.add_prefix_space = add_prefix_space\n@@ -99,11 +97,7 @@ def __init__(\n             ]\n         )\n         self._tokenizer.decoder = decoders.Metaspace(replacement=\"▁\", prepend_scheme=prepend_scheme)\n-\n-        tokenizer_object = self._tokenizer\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             bos_token=bos_token,\n             eos_token=eos_token,\n             sep_token=sep_token,\n@@ -116,14 +110,13 @@ def __init__(\n         )\n \n         self._tokenizer.post_processor = processors.TemplateProcessing(\n-            single=[\"$A\", \"</s>\"],\n-            pair=[\"$A\", \"</s>\", \"$B\", \"</s>\"],\n+            single=[str(bos_token), \"$A\", str(eos_token)],\n+            pair=[str(bos_token), \"$A\", str(eos_token), \"$B\", str(eos_token)],\n             special_tokens=[\n-                (\"</s>\", self.eos_token_id),\n+                (str(bos_token), self.bos_token_id),\n+                (str(eos_token), self.eos_token_id),\n             ],\n         )\n \n-        self.vocab_file = vocab_file\n-\n \n __all__ = [\"XLMRobertaTokenizer\"]"
        },
        {
            "sha": "a0ae1b2e3baf2a6b68ac60a8646907c213bcf518",
            "filename": "src/transformers/models/xlnet/tokenization_xlnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fxlnet%2Ftokenization_xlnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fmodels%2Fxlnet%2Ftokenization_xlnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlnet%2Ftokenization_xlnet.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Tokenization classes for XLNet model.\"\"\"\n \n-from typing import Optional\n+from typing import Optional, Union\n \n from tokenizers import AddedToken, Regex, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import Unigram\n@@ -98,10 +98,11 @@ class XLNetTokenizer(TokenizersBackend):\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     padding_side = \"left\"\n+    model = Unigram\n \n     def __init__(\n         self,\n-        vocab: Optional[list] = None,\n+        vocab: Optional[Union[str, list[tuple[str, float]]]] = None,\n         unk_id: int = 0,\n         do_lower_case=False,\n         remove_space=True,\n@@ -159,13 +160,8 @@ def __init__(\n         self.do_lower_case = do_lower_case\n         self.remove_space = remove_space\n         self.keep_accents = keep_accents\n-\n         mask_token = AddedToken(mask_token, lstrip=True, rstrip=False) if isinstance(mask_token, str) else mask_token\n-\n-        tokenizer_object = self._tokenizer\n-\n         super().__init__(\n-            tokenizer_object=tokenizer_object,\n             unk_id=unk_id,\n             do_lower_case=do_lower_case,\n             remove_space=remove_space,"
        },
        {
            "sha": "ba487eef686bdd955024b1e98a5980ecf82beda9",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 18,
            "deletions": 1,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -1460,7 +1460,24 @@ def _get_arguments_from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n         # get args from processor init signature\n         sub_processors = cls.get_attributes()\n         for sub_processor_type in sub_processors:\n-            if sub_processor_type in MODALITY_TO_AUTOPROCESSOR_MAPPING:\n+            if \"FuyuProcessor\" in cls.__name__ and \"tokenizer\" in sub_processor_type:\n+                from .tokenization_utils_tokenizers import TokenizersBackend\n+\n+                tokenizer = TokenizersBackend.from_pretrained(pretrained_model_name_or_path, **kwargs)\n+                if \"token_type_ids\" in tokenizer.model_input_names:\n+                    tokenizer.model_input_names.remove(\"token_type_ids\")\n+                args.append(tokenizer)\n+            elif \"PixtralProcessor\" in cls.__name__ and \"tokenizer\" in sub_processor_type:\n+                from tokenizers import pre_tokenizers\n+\n+                from .models.llama import LlamaTokenizer\n+\n+                tokenizer = LlamaTokenizer.from_pretrained(pretrained_model_name_or_path, **kwargs)\n+                tokenizer._tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n+                    [pre_tokenizers.ByteLevel(False), tokenizer._tokenizer.pre_tokenizer]\n+                )\n+                args.append(tokenizer)\n+            elif sub_processor_type in MODALITY_TO_AUTOPROCESSOR_MAPPING:\n                 auto_processor_class = MODALITY_TO_AUTOPROCESSOR_MAPPING[sub_processor_type]\n                 sub_processor = auto_processor_class.from_pretrained(pretrained_model_name_or_path, **kwargs)\n                 args.append(sub_processor)"
        },
        {
            "sha": "3bfb52c1ab463bc871cef5081525f781afd2823f",
            "filename": "src/transformers/tokenization_mistral_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_mistral_common.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -1986,3 +1986,7 @@ def _get_validation_mode(mode: Union[str, ValidationMode]) -> ValidationMode:\n         if mode not in [ValidationMode.finetuning, ValidationMode.test]:\n             raise ValueError(_invalid_mode_msg)\n         return mode\n+\n+\n+# Backward compatibility alias for codebases still importing the legacy name.\n+MistralCommonTokenizer = MistralCommonBackend"
        },
        {
            "sha": "c35a848d863b1982a73607e8040cb803c653726a",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 55,
            "deletions": 206,
            "changes": 261,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -27,7 +27,7 @@\n import re\n import warnings\n from collections import OrderedDict, UserDict\n-from collections.abc import Callable, Mapping, Sequence, Sized\n+from collections.abc import Callable, Collection, Mapping, Sequence, Sized\n from dataclasses import dataclass\n from pathlib import Path\n from typing import TYPE_CHECKING, Any, NamedTuple, Optional, Union\n@@ -1631,11 +1631,9 @@ def from_pretrained(\n                     f\"Calling {cls.__name__}.from_pretrained() with the path to a single file or url is not \"\n                     \"supported for this tokenizer. Use a model identifier or the path to a directory instead.\"\n                 )\n-            # Use first vocab file that's not tokenizer_file\n-            file_id = list(cls.vocab_files_names.keys())[0]\n-            if file_id == \"tokenizer_file\" and vocab_files_count > 1:\n-                file_id = [k for k in cls.vocab_files_names.keys() if k != \"tokenizer_file\"][0]\n-\n+            file_id = \"vocab_file\"\n+            if pretrained_model_name_or_path.endswith(\"tokenizer.json\"):\n+                file_id = \"tokenizer_file\"\n             vocab_files[file_id] = pretrained_model_name_or_path\n             single_file_id = file_id\n         else:\n@@ -1653,10 +1651,10 @@ def from_pretrained(\n                 }\n \n             vocab_files = {**cls.vocab_files_names, **additional_files_names}\n+\n+            # Check for versioned tokenizer files\n             if \"tokenizer_file\" in vocab_files:\n-                # Try to get the tokenizer config to see if there are versioned tokenizer files.\n                 fast_tokenizer_file = FULL_TOKENIZER_FILE\n-\n                 try:\n                     resolved_config_file = cached_file(\n                         pretrained_model_name_or_path,\n@@ -1672,43 +1670,33 @@ def from_pretrained(\n                         _raise_exceptions_for_missing_entries=False,\n                         _commit_hash=commit_hash,\n                     )\n-                except OSError:\n-                    # Re-raise any error raised by cached_file in order to get a helpful error message\n-                    raise\n+                    if resolved_config_file is not None:\n+                        with open(resolved_config_file, encoding=\"utf-8\") as reader:\n+                            tokenizer_config = json.load(reader)\n+                            if \"fast_tokenizer_files\" in tokenizer_config:\n+                                fast_tokenizer_file = get_fast_tokenizer_file(tokenizer_config[\"fast_tokenizer_files\"])\n+                        commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n                 except Exception:\n-                    # For any other exception, we throw a generic error.\n-                    raise OSError(\n-                        f\"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\n-                        \"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\n-                        f\"Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a directory \"\n-                        f\"containing all relevant files for a {cls.__name__} tokenizer.\"\n-                    )\n-\n-                commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n-                if resolved_config_file is not None:\n-                    with open(resolved_config_file, encoding=\"utf-8\") as reader:\n-                        tokenizer_config = json.load(reader)\n-                        if \"fast_tokenizer_files\" in tokenizer_config:\n-                            fast_tokenizer_file = get_fast_tokenizer_file(tokenizer_config[\"fast_tokenizer_files\"])\n+                    pass\n                 vocab_files[\"tokenizer_file\"] = fast_tokenizer_file\n \n-                # This block looks for any extra chat template files\n-                if is_local:\n-                    template_dir = Path(pretrained_model_name_or_path, CHAT_TEMPLATE_DIR)\n-                    if template_dir.is_dir():\n-                        for template_file in template_dir.glob(\"*.jinja\"):\n-                            template_name = template_file.name.removesuffix(\".jinja\")\n-                            vocab_files[f\"chat_template_{template_name}\"] = f\"{CHAT_TEMPLATE_DIR}/{template_file.name}\"\n-                else:\n-                    for template in list_repo_templates(\n-                        pretrained_model_name_or_path,\n-                        local_files_only=local_files_only,\n-                        revision=revision,\n-                        cache_dir=cache_dir,\n-                        token=token,\n-                    ):\n-                        template = template.removesuffix(\".jinja\")\n-                        vocab_files[f\"chat_template_{template}\"] = f\"{CHAT_TEMPLATE_DIR}/{template}.jinja\"\n+            # This block looks for any extra chat template files\n+            if is_local:\n+                template_dir = Path(pretrained_model_name_or_path, CHAT_TEMPLATE_DIR)\n+                if template_dir.is_dir():\n+                    for template_file in template_dir.glob(\"*.jinja\"):\n+                        template_name = template_file.name.removesuffix(\".jinja\")\n+                        vocab_files[f\"chat_template_{template_name}\"] = f\"{CHAT_TEMPLATE_DIR}/{template_file.name}\"\n+            else:\n+                for template in list_repo_templates(\n+                    pretrained_model_name_or_path,\n+                    local_files_only=local_files_only,\n+                    revision=revision,\n+                    cache_dir=cache_dir,\n+                    token=token,\n+                ):\n+                    template = template.removesuffix(\".jinja\")\n+                    vocab_files[f\"chat_template_{template}\"] = f\"{CHAT_TEMPLATE_DIR}/{template}.jinja\"\n \n         remote_files = []\n         if not is_local and not local_files_only:\n@@ -1766,11 +1754,6 @@ def from_pretrained(\n             if file_id not in resolved_vocab_files:\n                 continue\n \n-            if is_local:\n-                logger.info(f\"loading file {file_path}\")\n-            else:\n-                logger.info(f\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\")\n-\n         return cls._from_pretrained(\n             resolved_vocab_files,\n             pretrained_model_name_or_path,\n@@ -1800,29 +1783,6 @@ def _from_pretrained(\n         trust_remote_code=False,\n         **kwargs,\n     ):\n-        # We instantiate fast tokenizers based on a slow tokenizer if we don't have access to the tokenizer.json\n-        # file or if `from_slow` is set to True.\n-        from_slow = kwargs.get(\"from_slow\", False)\n-        gguf_file = kwargs.get(\"gguf_file\")\n-        has_tokenizer_file = resolved_vocab_files.get(\"tokenizer_file\", None) is not None\n-\n-        # If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\n-        # loaded directly from the GGUF file.\n-        if (from_slow or not has_tokenizer_file) and cls.slow_tokenizer_class is not None and not gguf_file:\n-            slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(\n-                copy.deepcopy(resolved_vocab_files),\n-                pretrained_model_name_or_path,\n-                copy.deepcopy(init_configuration),\n-                *init_inputs,\n-                token=token,\n-                cache_dir=cache_dir,\n-                local_files_only=local_files_only,\n-                _commit_hash=_commit_hash,\n-                **(copy.deepcopy(kwargs)),\n-            )\n-        else:\n-            slow_tokenizer = None\n-\n         # Prepare tokenizer initialization kwargs\n         # Did we saved some inputs and kwargs to reload ?\n         tokenizer_config_file = resolved_vocab_files.pop(\"tokenizer_config_file\", None)\n@@ -1831,14 +1791,16 @@ def _from_pretrained(\n                 init_kwargs = json.load(tokenizer_config_handle)\n             # used in the past to check if the tokenizer class matches the class in the repo\n             init_kwargs.pop(\"tokenizer_class\", None)\n-            if not has_tokenizer_file:\n-                init_kwargs.get(\"tokenizer_file\", None)\n             saved_init_inputs = init_kwargs.pop(\"init_inputs\", ())\n             if not init_inputs:\n                 init_inputs = saved_init_inputs\n         else:\n             init_kwargs = init_configuration\n \n+        if resolved_vocab_files.get(\"tokenizer_file\", None) is not None:\n+            init_kwargs.pop(\"add_bos_token\", None)\n+            init_kwargs.pop(\"add_eos_token\", None)\n+\n         # If independent chat template file(s) exist, they take priority over template entries in the tokenizer config\n         chat_templates = {}\n         chat_template_file = resolved_vocab_files.pop(\"chat_template_file\", None)\n@@ -1919,8 +1881,6 @@ def _from_pretrained(\n                 init_kwargs[args_name] = file_path\n         tokenizer_file = resolved_vocab_files.get(\"tokenizer_file\", None)\n \n-        if slow_tokenizer is not None:\n-            init_kwargs[\"__slow_tokenizer\"] = slow_tokenizer\n         init_kwargs[\"name_or_path\"] = pretrained_model_name_or_path\n         init_kwargs[\"is_local\"] = _is_local\n \n@@ -2039,28 +1999,12 @@ def _from_pretrained(\n             if key in init_kwargs and added_tokens_map != {} and init_kwargs[key] is not None:\n                 init_kwargs[key] = added_tokens_map.get(str(init_kwargs[key]), init_kwargs[key])\n \n-        # Track which files were loaded (if not already set by AutoTokenizer)\n-        if \"files_loaded\" not in init_kwargs:\n-            files_loaded = []\n-            # Check which files this tokenizer class actually uses based on vocab_files_names\n-            tokenizer_needs_files = set(cls.vocab_files_names.keys()) if hasattr(cls, \"vocab_files_names\") else set()\n-\n-            # If tokenizer_file is in the class's vocab_files_names and exists, prioritize it (TokenizersBackend)\n-            if \"tokenizer_file\" in tokenizer_needs_files and resolved_vocab_files.get(\"tokenizer_file\"):\n-                files_loaded.append(os.path.basename(resolved_vocab_files[\"tokenizer_file\"]))\n-            else:\n-                # Otherwise, add the actual vocab files that were used by this tokenizer class\n-                for file_key, file_path in resolved_vocab_files.items():\n-                    if (\n-                        file_path\n-                        and file_key not in [\"tokenizer_config_file\", \"special_tokens_map_file\", \"added_tokens_file\"]\n-                        and file_key in tokenizer_needs_files\n-                    ):\n-                        # Extract just the filename from the path\n-                        files_loaded.append(os.path.basename(file_path))\n-            init_kwargs[\"files_loaded\"] = files_loaded\n+        # From pretrained with the legacy fixes\n+        # for `tokenizers` based tokenizer, we actually want to have vocab and merges pre-extracted from whatever inputs\n+        # for `none` (PythonBackend) based tokenizer, we also want the vocab file / merge files not extracted.\n+        # for `sentencepiece` based tokenizer, we pass the sentencepiece model file directly.\n+        init_kwargs = cls.convert_to_native_format(**init_kwargs)\n \n-        # Instantiate the tokenizer.\n         try:\n             tokenizer = cls(*init_inputs, **init_kwargs)\n         except import_protobuf_decode_error():\n@@ -2081,118 +2025,12 @@ def _from_pretrained(\n                 \"Unable to load vocabulary from file. \"\n                 \"Please check that the provided vocabulary is accessible and not corrupted.\"\n             )\n-\n-        # If tokenizer_file exists and tokenizer has a TokenizersBackend, replace the blank tokenizer with tokenizer.json\n-        if tokenizer_file is not None and hasattr(tokenizer, \"_tokenizer\"):\n-            from tokenizers import Tokenizer as TokenizerFast\n-\n-            tokenizer._tokenizer = TokenizerFast.from_file(tokenizer_file)\n-            # Re-run post-initialization if the tokenizer has it\n-            if hasattr(tokenizer, \"_post_init\"):\n-                tokenizer._post_init()\n-            # If only SPM exists, try to get vocab and merges and init to load a tokenizers-backend\n-        else:\n-            spm_filename = find_sentencepiece_model_file(\n-                pretrained_model_name_or_path,\n-                revision=kwargs.get(\"revision\"),\n-                token=kwargs.get(\"token\"),\n-                cache_dir=kwargs.get(\"cache_dir\"),\n-                local_files_only=kwargs.get(\"local_files_only\", False),\n-                subfolder=kwargs.get(\"subfolder\", \"\"),\n-            )\n-            if spm_filename is not None:\n-                try:\n-                    resolved_spm = cached_file(\n-                        pretrained_model_name_or_path,\n-                        spm_filename,\n-                        cache_dir=kwargs.get(\"cache_dir\"),\n-                        force_download=kwargs.get(\"force_download\", False),\n-                        proxies=kwargs.get(\"proxies\"),\n-                        token=kwargs.get(\"token\"),\n-                        revision=kwargs.get(\"revision\"),\n-                        local_files_only=kwargs.get(\"local_files_only\", False),\n-                        subfolder=kwargs.get(\"subfolder\", \"\"),\n-                    )\n-                except Exception:\n-                    resolved_spm = None\n-                if resolved_spm is not None:\n-                    try:\n-                        # Mirror AutoTokenizer fallback: extract vocab/merges from SentencePiece\n-                        import inspect as _inspect\n-\n-                        from .tokenization_utils_sentencepiece import SentencePieceExtractor\n-\n-                        class_sig = _inspect.signature(getattr(cls, \"__init__\", cls))\n-                        vocab_ids, vocab_scores, merges = SentencePieceExtractor(resolved_spm).extract()\n-                        files_loaded = [spm_filename]\n-                        init_kwargs[\"backend\"] = \"tokenizers\"\n-                        init_kwargs[\"files_loaded\"] = files_loaded\n-                        # If tokenizer needs merges too (BPE), pass both; unigram models only need vocab\n-                        if \"merges\" in class_sig.parameters:\n-                            return cls.from_pretrained(\n-                                pretrained_model_name_or_path,\n-                                *init_inputs,\n-                                vocab=vocab_scores,\n-                                merges=merges,\n-                                **init_kwargs,\n-                            )\n-                        elif \"vocab\" in class_sig.parameters:\n-                            return cls.from_pretrained(\n-                                pretrained_model_name_or_path,\n-                                *init_inputs,\n-                                vocab=vocab_scores,\n-                                **init_kwargs,\n-                            )\n-                    except Exception as e:\n-                        logger.warning(\n-                            f\"Could not extract vocab/merges from the SentencePiece model to initialize a Tokenizers backend: {e}. We are falling back so we are falling back to the standard loading method.\"\n-                        )\n-                        pass\n-            # Fallback to vocab.json + merges.txt (BPE) or just vocab.json (WordLevel/WordPiece)\n-            vocab, merges, files_loaded = load_vocab_and_merges(\n-                pretrained_model_name_or_path,\n-                cache_dir=kwargs.get(\"cache_dir\"),\n-                force_download=kwargs.get(\"force_download\", False),\n-                proxies=kwargs.get(\"proxies\"),\n-                token=kwargs.get(\"token\"),\n-                revision=kwargs.get(\"revision\"),\n-                local_files_only=kwargs.get(\"local_files_only\", False),\n-                subfolder=kwargs.get(\"subfolder\", \"\"),\n-            )\n-\n-            if vocab is not None:\n-                try:\n-                    import inspect as _inspect\n-\n-                    class_sig = _inspect.signature(getattr(cls, \"__init__\", cls))\n-                    init_kwargs[\"backend\"] = \"tokenizers\"\n-                    init_kwargs[\"files_loaded\"] = files_loaded\n-\n-                    if merges is not None and \"merges\" in class_sig.parameters:\n-                        return cls.from_pretrained(\n-                            pretrained_model_name_or_path,\n-                            *init_inputs,\n-                            vocab=vocab,\n-                            merges=merges,\n-                            **init_kwargs,\n-                        )\n-                    elif \"vocab\" in class_sig.parameters:\n-                        return cls.from_pretrained(\n-                            pretrained_model_name_or_path,\n-                            *init_inputs,\n-                            vocab=vocab,\n-                            **init_kwargs,\n-                        )\n-                except Exception:\n-                    pass\n-        if added_tokens_decoder != {} and max(list(added_tokens_decoder.keys())[-1], 0) > tokenizer.vocab_size:\n-            logger.info(\n-                \"Special tokens have been added in the vocabulary, make sure the associated word embeddings are\"\n-                \" fine-tuned or trained.\"\n-            )\n-\n         return tokenizer\n \n+    @classmethod\n+    def convert_to_native_format(cls, **kwargs):\n+        return kwargs\n+\n     @classmethod\n     def convert_added_tokens(cls, obj: Union[AddedToken, Any], save=False, add_type_field=True):\n         if isinstance(obj, dict) and \"__type\" in obj and obj[\"__type\"] == \"AddedToken\":\n@@ -2273,9 +2111,13 @@ def save_pretrained(\n         )\n \n         tokenizer_config = copy.deepcopy(self.init_kwargs)\n+        tokenizer_config.pop(\"add_bos_token\", None)\n+        tokenizer_config.pop(\"add_eos_token\", None)\n \n         # Let's save the init kwargs\n         target_keys = set(self.init_kwargs.keys())\n+        target_keys.discard(\"add_bos_token\")\n+        target_keys.discard(\"add_eos_token\")\n         # Let's save the special tokens map (only the strings)\n         target_keys.update([\"model_max_length\"])\n \n@@ -3770,15 +3612,22 @@ def _get_prepend_scheme(add_prefix_space: bool, original_tokenizer) -> str:\n     return prepend_scheme\n \n \n-def generate_merges(vocab, vocab_scores: Optional[dict[str, float]] = None):\n+def generate_merges(\n+    vocab, vocab_scores: Optional[dict[str, float]] = None, skip_tokens: Optional[Collection[str]] = None\n+):\n+    skip_tokens = set(skip_tokens) if skip_tokens is not None else set()\n     reverse = vocab_scores is not None\n     vocab_scores = dict(vocab_scores) if reverse else vocab\n \n     merges = []\n     for merge, piece_score in vocab_scores.items():\n+        if merge in skip_tokens:\n+            continue\n         local = []\n         for index in range(1, len(merge)):\n             piece_l, piece_r = merge[:index], merge[index:]\n+            if piece_l in skip_tokens or piece_r in skip_tokens:\n+                continue\n             if piece_l in vocab and piece_r in vocab:\n                 local.append((piece_l, piece_r, piece_score))\n         local = sorted(local, key=lambda x: (vocab[x[0]], vocab[x[1]]))"
        },
        {
            "sha": "1310fe07cdfe9a14dfb37eaca7a88b99c79c6dc0",
            "filename": "src/transformers/tokenization_utils_tokenizers.py",
            "status": "modified",
            "additions": 165,
            "deletions": 105,
            "changes": 270,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Ftokenization_utils_tokenizers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/src%2Ftransformers%2Ftokenization_utils_tokenizers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_tokenizers.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -20,6 +20,7 @@\n import json\n import os\n from collections import defaultdict\n+from collections.abc import Iterable\n from shutil import copyfile\n from typing import Any, Optional, Union\n \n@@ -28,11 +29,9 @@\n from tokenizers import AddedToken, processors\n from tokenizers import Encoding as EncodingFast\n from tokenizers import Tokenizer as TokenizerFast\n-from tokenizers import normalizers as tokenizers_normalizers\n from tokenizers.decoders import Decoder as DecoderFast\n from tokenizers.trainers import BpeTrainer, UnigramTrainer, WordLevelTrainer, WordPieceTrainer\n \n-from .convert_slow_tokenizer import convert_slow_tokenizer\n from .integrations.ggml import convert_gguf_tokenizer\n from .modeling_gguf_pytorch_utils import load_gguf_checkpoint\n from .tokenization_utils_base import (\n@@ -42,6 +41,7 @@\n     PreTrainedTokenizerBase,\n     TextInput,\n     TruncationStrategy,\n+    generate_merges,\n )\n from .utils import PaddingStrategy, add_end_docstrings, logging\n \n@@ -91,26 +91,157 @@ class TokenizersBackend(PreTrainedTokenizerBase):\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n+    model = None\n+    _tokenizer = None\n+\n+    @classmethod\n+    def convert_to_native_format(cls, trust_remote_code=False, **kwargs):\n+        \"\"\"s\n+        Build a `tokenizers.Tokenizer` backend from the available serialization files (tokenizer.json, sentencepiece\n+        models, tekken.json, vocab/merges).\n+        \"\"\"\n+        # Preserve kwargs for possible downstream use\n+        local_kwargs = dict(kwargs)\n+        fast_tokenizer_file = local_kwargs.pop(\"tokenizer_file\", None)\n+\n+        if (\n+            fast_tokenizer_file is not None\n+            and os.path.isfile(fast_tokenizer_file)\n+            and (cls is TokenizersBackend or \"__init__\" not in cls.__dict__ or trust_remote_code)\n+        ):\n+            local_kwargs[\"tokenizer_object\"] = TokenizerFast.from_file(fast_tokenizer_file)\n+            return local_kwargs\n+        elif fast_tokenizer_file is not None and os.path.isfile(fast_tokenizer_file):\n+            # we extract vocab / merges from the tokenizer file to pass them to __init__\n+            processor = TokenizerFast.from_file(fast_tokenizer_file).post_processor\n+            with open(fast_tokenizer_file, encoding=\"utf-8\") as tokenizer_handle:\n+                tokenizer_json = json.load(tokenizer_handle)\n+            vocab = tokenizer_json.get(\"model\", {}).get(\"vocab\", None)\n+            if cls.model is None:\n+                if isinstance(vocab, list):\n+                    vocab = list(map(tuple, vocab))  # TODO just for now\n+            elif cls.model.__name__ == \"Unigram\":\n+                vocab = list(map(tuple, vocab))\n+            elif cls.model.__name__ == \"WordLevel\":\n+                vocab = {token: i for i, token in enumerate(vocab)}\n+            elif cls.model.__name__ == \"BPE\" or cls.model.__name__ == \"WordPiece\":\n+                if isinstance(vocab, list):\n+                    vocab = {token[0] if isinstance(token, list) else token: i for i, token in enumerate(vocab)}\n+            local_kwargs[\"vocab\"] = vocab\n+\n+            model_type = getattr(cls, \"model\", None)\n+            if \"merges\" in tokenizer_json.get(\"model\", {}) and (model_type and model_type.__name__ == \"BPE\"):\n+                merges = tokenizer_json[\"model\"][\"merges\"]\n+                merges = [tuple(merge.split(\" \")) if isinstance(merge, str) else tuple(merge) for merge in merges]\n+                local_kwargs[\"merges\"] = merges\n+\n+            if processor is not None:\n+                local_kwargs[\"post_processor\"] = processor\n+            return local_kwargs\n+\n+        vocab_file = local_kwargs.get(\"vocab_file\")\n+        merges_file = local_kwargs.get(\"merges_file\")\n+        vocab = local_kwargs.get(\"vocab\")\n+        merges = local_kwargs.get(\"merges\")\n+\n+        # Tekken converter (Mistral)\n+        if isinstance(vocab_file, str) and vocab_file.endswith(\"tekken.json\") and os.path.isfile(vocab_file):\n+            from .convert_slow_tokenizer import MistralConverter\n+\n+            local_kwargs[\"vocab\"], local_kwargs[\"merges\"] = MistralConverter(\n+                vocab_file=vocab_file\n+            ).extract_vocab_merges_from_model(vocab_file)\n+            return local_kwargs\n+\n+        # SentencePiece model (with TikToken fallback)\n+        if isinstance(vocab_file, str) and os.path.isfile(vocab_file) and vocab_file.endswith(\".model\"):\n+            try:\n+                from .convert_slow_tokenizer import SentencePieceExtractor\n+\n+                local_kwargs = SentencePieceExtractor(vocab_file).extract(cls.model, **local_kwargs)\n+                try:\n+                    from .convert_slow_tokenizer import SLOW_TO_FAST_CONVERTERS\n+\n+                    converter_class = SLOW_TO_FAST_CONVERTERS.get(cls.__name__)\n+                    if converter_class is not None and hasattr(converter_class, \"convert_from_spm\"):\n+                        local_kwargs = converter_class.convert_from_spm(**local_kwargs)\n+                except Exception as e:\n+                    logger.warning(\n+                        f\"Could not reorder vocab using converter for {cls.__name__} due to {e}. Falling back to raw SentencePiece extraction.\"\n+                    )\n+                # what used to be in `convert_slow`\n+                if hasattr(cls, \"convert_from_spm_model\"):\n+                    local_kwargs = cls.convert_from_spm_model(**local_kwargs)\n+            except Exception as e:  # TODO only catch deserialization error here!\n+                logger.warning(\n+                    f\"Could not extract SentencePiece model from {vocab_file} using sentencepiece library due to {e}. \"\n+                    \"Falling back to TikToken extractor.\"\n+                )\n+                from .convert_slow_tokenizer import TikTokenConverter\n+\n+                local_kwargs[\"vocab\"], local_kwargs[\"merges\"] = TikTokenConverter(\n+                    vocab_file=vocab_file, extra_special_tokens=local_kwargs.get(\"extra_special_tokens\")\n+                ).extract_vocab_merges_from_model(vocab_file)\n+            return local_kwargs\n+\n+        # Fallback to standard vocab/merges files if they existed!\n+        if vocab is None and isinstance(vocab_file, str) and os.path.isfile(vocab_file):\n+            local_kwargs[\"vocab\"] = vocab_file\n+            vocab = local_kwargs[\"vocab\"]\n+        if merges is None and isinstance(merges_file, str) and os.path.isfile(merges_file):\n+            local_kwargs[\"merges\"] = merges_file\n+            merges = local_kwargs[\"merges\"]\n+\n+        # Generate merges automatically when not provided for BPE tokenizers\n+        if merges is None and cls.model is not None and cls.model.__name__ == \"BPE\" and isinstance(vocab, dict):\n+            # Gather special tokens from kwargs to skip in merge generation\n+            def _iter_special_tokens(values: Iterable[Any]) -> list[str]:\n+                collected: list[str] = []\n+                for val in values:\n+                    if val is None:\n+                        continue\n+                    if isinstance(val, (list, tuple)):\n+                        collected.extend(_iter_special_tokens(val))\n+                    else:\n+                        collected.append(str(val))\n+                return collected\n+\n+            special_tokens_keys = [\n+                \"pad_token\",\n+                \"unk_token\",\n+                \"bos_token\",\n+                \"eos_token\",\n+                \"sep_token\",\n+                \"cls_token\",\n+                \"mask_token\",\n+                \"additional_special_tokens\",\n+                \"extra_special_tokens\",\n+            ]\n+            skip_tokens: set[str] = set()\n+            for key in special_tokens_keys:\n+                if key in local_kwargs:\n+                    skip_tokens.update(_iter_special_tokens([local_kwargs[key]]))\n+\n+            merges = generate_merges(vocab, skip_tokens=skip_tokens)\n+            local_kwargs[\"merges\"] = merges\n+        return local_kwargs\n \n     def __init__(self, *args, **kwargs):\n         tokenizer_object = kwargs.pop(\"tokenizer_object\", None)\n-        slow_tokenizer = kwargs.pop(\"__slow_tokenizer\", None)\n         gguf_file = kwargs.pop(\"gguf_file\", None)\n         fast_tokenizer_file = kwargs.pop(\"tokenizer_file\", None)\n-        from_slow = kwargs.pop(\"from_slow\", False)\n         # Note: added_tokens_decoder is NOT popped - it's passed to super().__init__() for processing\n         added_tokens_decoder = kwargs.get(\"added_tokens_decoder\", {})\n         # Store add_prefix_space before super().__init__() to ensure it's not overridden\n         add_prefix_space = kwargs.get(\"add_prefix_space\", False)\n+        vocab_file = kwargs.get(\"vocab_file\")\n \n+        fast_tokenizer = None\n         if tokenizer_object is not None:\n             fast_tokenizer = copy.deepcopy(tokenizer_object)\n-        elif fast_tokenizer_file is not None and not from_slow:\n+        elif fast_tokenizer_file is not None and os.path.isfile(fast_tokenizer_file):\n             # We have a serialization from tokenizers which let us directly build the backend\n             fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)\n-        elif slow_tokenizer:\n-            # We need to convert a slow tokenizer to build the backend\n-            fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\n         elif gguf_file is not None:\n             # We need to convert a slow tokenizer to build the backend\n             gguf_param = load_gguf_checkpoint(kwargs.get(\"vocab_file\"))\n@@ -121,30 +252,19 @@ def __init__(self, *args, **kwargs):\n             kwargs.update(tokenizer_config)\n             if len(additional_kwargs) > 0:\n                 kwargs.update(additional_kwargs)\n-        elif self.slow_tokenizer_class is not None and slow_tokenizer is not False:\n-            # We need to create and convert a slow tokenizer to build the backend\n-            slow_tokenizer = self.slow_tokenizer_class(*args, **kwargs)\n-            fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\n-        elif not slow_tokenizer:\n-            # We tried loading a slow_tokenizer with spm and failed, try to load with tiktoken\n-            self.vocab_file = kwargs.get(\"vocab_file\")\n-            # V5: Set _extra_special_tokens directly for converter\n-            self._extra_special_tokens = kwargs.get(\"extra_special_tokens\", [])\n-            fast_tokenizer = convert_slow_tokenizer(self, from_tiktoken=True)\n-            slow_tokenizer = None\n-        else:\n+        elif self._tokenizer is None:\n             raise ValueError(\n                 \"Couldn't instantiate the backend tokenizer from one of: \\n\"\n                 \"(1) a `tokenizers` library serialization file, \\n\"\n                 \"(2) a slow tokenizer instance to convert or \\n\"\n                 \"(3) an equivalent slow tokenizer class to instantiate and convert. \\n\"\n                 \"You need to have sentencepiece or tiktoken installed to convert a slow tokenizer to a fast one.\"\n             )\n+        if fast_tokenizer is not None:\n+            self._tokenizer = fast_tokenizer\n \n-        self._tokenizer = fast_tokenizer\n-\n-        if slow_tokenizer is not None:\n-            kwargs.update(slow_tokenizer.init_kwargs)\n+        if self._tokenizer is None:\n+            raise ValueError(\"The backend tokenizer is not correctly initialized.\")\n \n         _truncation = self._tokenizer.truncation\n \n@@ -169,9 +289,17 @@ def __init__(self, *args, **kwargs):\n         # Set backend to \"tokenizers\" if not already set\n         if \"backend\" not in kwargs:\n             kwargs[\"backend\"] = \"tokenizers\"\n-\n+        explicit_bos_eos_in_kwargs = \"add_bos_token\" in kwargs or \"add_eos_token\" in kwargs\n+        self._add_bos_token = kwargs.get(\"add_bos_token\", False)\n+        self._add_eos_token = kwargs.get(\"add_eos_token\", False)\n+        if post_processor := kwargs.pop(\"post_processor\", None):  # most reliable way to get the post-processor\n+            self._tokenizer.post_processor = post_processor\n+        self._should_update_post_processor = explicit_bos_eos_in_kwargs or self._tokenizer.post_processor is None\n         # We call this after having initialized the backend tokenizer because we update it.\n         super().__init__(**kwargs)\n+\n+        if vocab_file is not None:\n+            self.vocab_file = vocab_file\n         # Ensure add_prefix_space is set correctly after parent init\n         self.add_prefix_space = add_prefix_space\n         self._tokenizer.encode_special_tokens = self.split_special_tokens\n@@ -229,6 +357,12 @@ def __init__(self, *args, **kwargs):\n                 **kwargs,\n             )\n \n+        self._should_update_post_processor = (\n+            self._should_update_post_processor or self._tokenizer.post_processor is None\n+        )\n+        if self._should_update_post_processor:\n+            self.update_post_processor()\n+\n     @property\n     def is_fast(self) -> bool:\n         return True\n@@ -274,7 +408,7 @@ def update_post_processor(self):\n         # If eos_token is None and add_eos_token is True, silently disable add_eos_token\n         # This allows tokenizers to set add_eos_token even if eos_token is not configured\n         if eos is None and self.add_eos_token:\n-            self._add_eos_token = False\n+            self.add_eos_token = False\n             return\n \n         single = f\"{(bos + ':0 ') if self.add_bos_token else ''}$A:0{(' ' + eos + ':0') if self.add_eos_token else ''}\"\n@@ -321,98 +455,24 @@ def _post_init(self):\n             if token_value is None:\n                 continue\n             if isinstance(token_value, AddedToken):\n-                if self._tokenizer.token_to_id(str(token_value)) is None:\n-                    tokens_to_add.append(token_value)\n+                tokens_to_add.append(token_value)\n             elif isinstance(token_value, str):\n-                if self._tokenizer.token_to_id(token_value) is None:\n-                    tokens_to_add.append(AddedToken(token_value, special=True, normalized=False))\n+                tokens_to_add.append(AddedToken(token_value, special=True, normalized=False))\n \n         # V5: Check extra special tokens\n         for token in self._extra_special_tokens:\n             if isinstance(token, AddedToken):\n-                if self._tokenizer.token_to_id(str(token)) is None:\n-                    tokens_to_add.append(token)\n+                tokens_to_add.append(token)\n             elif isinstance(token, str):\n-                if self._tokenizer.token_to_id(token) is None:\n-                    tokens_to_add.append(AddedToken(token, special=True, normalized=False))\n+                tokens_to_add.append(AddedToken(token, special=True, normalized=False))\n \n         if tokens_to_add:\n             # Ensure special tokens are added as such to the backend\n             self.add_tokens(tokens_to_add, special_tokens=True)\n \n-        if hasattr(self, \"_add_bos_token\") or hasattr(self, \"_add_eos_token\"):\n+        if getattr(self, \"_should_update_post_processor\", True) or self._tokenizer.post_processor is None:\n             self.update_post_processor()\n \n-        # Update add_prefix_space in the pre_tokenizer if needed\n-        if hasattr(self, \"add_prefix_space\"):\n-            try:\n-                tokenizer_json = json.loads(self.backend_tokenizer.to_str())\n-                pre_tok = tokenizer_json.get(\"pre_tokenizer\", {})\n-\n-                # Recursively update add_prefix_space in pretokenizers\n-                def update_add_prefix_space(pretok_dict, value):\n-                    updated = False\n-                    if pretok_dict.get(\"type\") == \"Sequence\":\n-                        for nested in pretok_dict.get(\"pretokenizers\", []):\n-                            updated |= update_add_prefix_space(nested, value)\n-                    elif \"add_prefix_space\" in pretok_dict and pretok_dict[\"add_prefix_space\"] != value:\n-                        pretok_dict[\"add_prefix_space\"] = value\n-                        updated = True\n-                    return updated\n-\n-                if update_add_prefix_space(pre_tok, self.add_prefix_space):\n-                    self._tokenizer = TokenizerFast.from_str(json.dumps(tokenizer_json))\n-            except Exception:\n-                pass\n-\n-        # Ensure normalizer flags (lowercase/accents/chinese chars) reflect tokenizer attributes\n-        try:\n-            normalizer = self.backend_tokenizer.normalizer\n-            if normalizer is not None:\n-                norm_state = json.loads(normalizer.__getstate__())\n-                norm_type = norm_state.get(\"type\")\n-\n-                desired_lowercase = getattr(self, \"do_lower_case\", None)\n-                desired_strip_accents = getattr(self, \"strip_accents\", None)\n-                # Some tokenizers expose keep_accents instead of strip_accents\n-                if desired_strip_accents is None and hasattr(self, \"keep_accents\") and \"strip_accents\" in norm_state:\n-                    keep_accents_value = getattr(self, \"keep_accents\")\n-                    if keep_accents_value is not None:\n-                        desired_strip_accents = not keep_accents_value\n-                desired_handle_chinese = getattr(self, \"tokenize_chinese_chars\", None)\n-\n-                updated = False\n-                if (\n-                    desired_lowercase is not None\n-                    and \"lowercase\" in norm_state\n-                    and norm_state[\"lowercase\"] != desired_lowercase\n-                ):\n-                    norm_state[\"lowercase\"] = desired_lowercase\n-                    updated = True\n-                if (\n-                    desired_strip_accents is not None\n-                    and \"strip_accents\" in norm_state\n-                    and norm_state[\"strip_accents\"] != desired_strip_accents\n-                ):\n-                    norm_state[\"strip_accents\"] = desired_strip_accents\n-                    updated = True\n-                if (\n-                    desired_handle_chinese is not None\n-                    and \"handle_chinese_chars\" in norm_state\n-                    and norm_state[\"handle_chinese_chars\"] != desired_handle_chinese\n-                ):\n-                    norm_state[\"handle_chinese_chars\"] = desired_handle_chinese\n-                    updated = True\n-\n-                if updated and norm_type is not None:\n-                    norm_class = getattr(tokenizers_normalizers, norm_type, None)\n-                    if norm_class is not None:\n-                        norm_state.pop(\"type\", None)\n-                        self.backend_tokenizer.normalizer = norm_class(**norm_state)\n-        except Exception:\n-            # Best-effort: do not block initialization on normalizer reconciliation\n-            pass\n-\n     @property\n     def vocab_size(self) -> int:\n         \"\"\""
        },
        {
            "sha": "760c3cbcb22eae0527718deae911f5881cf993c7",
            "filename": "tests/models/albert/test_tokenization_albert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Falbert%2Ftest_tokenization_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Falbert%2Ftest_tokenization_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falbert%2Ftest_tokenization_albert.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -50,7 +50,7 @@ def setUpClass(cls):\n \n         extractor = SentencePieceExtractor(vocab_file)\n         vocab_ids, vocab_scores, merges = extractor.extract()\n-        tokenizer_from_vocab = AlbertTokenizer(vocab=vocab_ids, merges=merges)\n+        tokenizer_from_vocab = AlbertTokenizer(vocab=vocab_scores, merges=merges)\n         tokenizer_from_vocab.pad_token = tokenizer_from_vocab.eos_token\n \n         cls.tokenizers = [tokenizer, tokenizer_from_vocab]"
        },
        {
            "sha": "04b580f9273f5896c0b734e85db58a216c5015ee",
            "filename": "tests/models/auto/test_processor_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -51,7 +51,6 @@\n from transformers.models.auto.video_processing_auto import get_video_processor_config\n from transformers.testing_utils import TOKEN, TemporaryHubRepo, get_tests_dir, is_staging_test\n from transformers.tokenization_python import TOKENIZER_CONFIG_FILE\n-from transformers.tokenization_utils_sentencepiece import SentencePieceExtractor\n from transformers.utils import (\n     FEATURE_EXTRACTOR_NAME,\n     PROCESSOR_NAME,\n@@ -522,10 +521,7 @@ def test_push_to_hub_dynamic_processor(self):\n \n     def test_push_to_hub_with_chat_templates(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n-            # Extract vocab and merges from SentencePiece model\n-            extractor = SentencePieceExtractor(SAMPLE_VOCAB_LLAMA)\n-            vocab_ids, vocab_scores, merges = extractor.extract()\n-            tokenizer = LlamaTokenizer(vocab=vocab_scores, merges=merges)\n+            tokenizer = LlamaTokenizer.from_pretrained(SAMPLE_VOCAB_LLAMA)\n             image_processor = SiglipImageProcessor()\n             chat_template = \"default dummy template for testing purposes only\"\n             processor = LlavaProcessor("
        },
        {
            "sha": "189d46a7859f0f09d0d72ae222ee7dfe0d323ce2",
            "filename": "tests/models/auto/test_tokenization_auto.py",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fauto%2Ftest_tokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fauto%2Ftest_tokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_tokenization_auto.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -183,6 +183,14 @@ def test_from_pretrained_use_fast_toggle(self):\n         )\n         self.assertIsInstance(AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\"), BertTokenizerFast)\n \n+    @require_tokenizers\n+    @slow\n+    def test_custom_tokenizer_from_hub(self):\n+        tokenizer = AutoTokenizer.from_pretrained(\n+            \"openbmb/MiniCPM-Llama3-V-2_5\", trust_remote_code=True, revision=\"fd7f352fac0e06d0d818b23f98e3ec8c64267a57\"\n+        )\n+        self.assertTrue(tokenizer.__class__.__module__.startswith(\"transformers_modules.\"))\n+\n     @require_tokenizers\n     def test_voxtral_tokenizer_converts_from_tekken(self):\n         repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n@@ -263,6 +271,18 @@ def test_auto_tokenizer_from_local_folder_mistral_detection(self):\n         self.assertIsInstance(tokenizer2, tokenizer.__class__)\n         self.assertTrue(tokenizer2.vocab_size > 100_000)\n \n+    @require_tokenizers\n+    def test_auto_tokenizer_loads_bloom_repo_without_tokenizer_class(self):\n+        tokenizer = AutoTokenizer.from_pretrained(\"trl-internal-testing/tiny-BloomForCausalLM\")\n+        self.assertIsInstance(tokenizer, TokenizersBackend)\n+        self.assertTrue(tokenizer.is_fast)\n+\n+    @require_tokenizers\n+    def test_auto_tokenizer_loads_sentencepiece_only_repo(self):\n+        tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/tiny-mbart\")\n+        self.assertIsInstance(tokenizer, TokenizersBackend)\n+        self.assertTrue(tokenizer.is_fast)\n+\n     def test_auto_tokenizer_fast_no_slow(self):\n         tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/ctrl\")\n         # There is no fast CTRL so this always gives us a slow tokenizer."
        },
        {
            "sha": "2a77d48c28b6dae9db3d7d70415cd69758872b1d",
            "filename": "tests/models/bloom/test_tokenization_bloom.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fbloom%2Ftest_tokenization_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fbloom%2Ftest_tokenization_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbloom%2Ftest_tokenization_bloom.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -129,13 +129,6 @@ def test_encodings_from_xnli_dataset(self):\n         predicted_text = [tokenizer.decode(x, clean_up_tokenization_spaces=False) for x in output_tokens]\n         self.assertListEqual(predicted_text, input_text)\n \n-    def test_add_prefix_space_fast(self):\n-        tokenizer_w_prefix = self.get_tokenizer(add_prefix_space=True)\n-        tokenizer_wo_prefix = self.get_tokenizer(add_prefix_space=False)\n-        tokens_w_prefix = tokenizer_w_prefix.tokenize(\"Hey\")\n-        tokens_wo_prefix = tokenizer_wo_prefix.tokenize(\"Hey\")\n-        self.assertNotEqual(tokens_w_prefix, tokens_wo_prefix)\n-\n     @slow\n     def test_save_and_load_tokenizer(self):\n         return super().test_save_and_load_tokenizer()"
        },
        {
            "sha": "934b3f20a59853251765cdacf6c64ab84a4632f1",
            "filename": "tests/models/chameleon/test_processing_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fchameleon%2Ftest_processing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fchameleon%2Ftest_processing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_processing_chameleon.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -66,7 +66,7 @@ def test_special_mm_token_truncation(self):\n                 return_tensors=\"pt\",\n                 truncation=True,\n                 padding=True,\n-                max_length=2,\n+                max_length=1,\n             )\n \n     @staticmethod"
        },
        {
            "sha": "cd56afdc2fe4e78fddb361f5ea6759fd1aa3f4ed",
            "filename": "tests/models/code_llama/test_tokenization_code_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fcode_llama%2Ftest_tokenization_code_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fcode_llama%2Ftest_tokenization_code_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcode_llama%2Ftest_tokenization_code_llama.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -113,7 +113,7 @@ def test_save_and_load_tokenizer(self):\n         self.assertIn(\"new_extra_special_token\", after_tokenizer.extra_special_tokens)\n \n     def test_no_infilling_init(self):\n-        tokenizer = CodeLlamaTokenizer(SAMPLE_VOCAB, prefix_token=None, keep_accents=True)\n+        tokenizer = CodeLlamaTokenizer.from_pretrained(SAMPLE_VOCAB, prefix_token=None, keep_accents=True)\n         with self.assertRaises(ValueError):\n             tokenizer.tokenize(\"This is <FILL_ME> prefix\")\n "
        },
        {
            "sha": "faebe7e02f57d8e1275f8a625d079601ea2af193",
            "filename": "tests/models/colpali/test_processing_colpali.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -36,7 +36,7 @@ class ColPaliProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n \n     @classmethod\n     def _setup_tokenizer(cls):\n-        return GemmaTokenizer(SAMPLE_VOCAB, keep_accents=True)\n+        return GemmaTokenizer.from_pretrained(SAMPLE_VOCAB, keep_accents=True)\n \n     @classmethod\n     def _setup_image_processor(cls):"
        },
        {
            "sha": "61713fe6a2d3a00c8e021eb6e7e8ad0acf44df79",
            "filename": "tests/models/gemma3/test_processing_gemma3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fgemma3%2Ftest_processing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fgemma3%2Ftest_processing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_processing_gemma3.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -50,7 +50,9 @@ def _setup_tokenizer(cls):\n             \"boi_token\": \"<start_of_image>\",\n             \"eoi_token\": \"<end_of_image>\",\n         }\n-        tokenizer = tokenizer_class(SAMPLE_VOCAB, keep_accents=True, extra_special_tokens=extra_special_tokens)\n+        tokenizer = tokenizer_class.from_pretrained(\n+            SAMPLE_VOCAB, keep_accents=True, extra_special_tokens=extra_special_tokens\n+        )\n         return tokenizer\n \n     def test_get_num_vision_tokens(self):"
        },
        {
            "sha": "0b8482b2b665ce79f90edd097976dbd07966fd8b",
            "filename": "tests/models/idefics3/test_processing_idefics3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fidefics3%2Ftest_processing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fidefics3%2Ftest_processing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_processing_idefics3.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -29,6 +29,12 @@ class Idefics3ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     processor_class = Idefics3Processor\n     model_id = \"HuggingFaceM4/Idefics3-8B-Llama3\"\n \n+    def get_processor(self):\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n+        processor.tokenizer.add_bos_token = True\n+        processor.tokenizer.add_eos_token = False\n+        return processor\n+\n     @classmethod\n     def _setup_test_attributes(cls, processor):\n         cls.image1 = load_image("
        },
        {
            "sha": "b7842300b09960005cd0827d89b8c9c4385dd67f",
            "filename": "tests/models/llava_onevision/test_processing_llava_onevision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fllava_onevision%2Ftest_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fllava_onevision%2Ftest_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_processing_llava_onevision.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -85,7 +85,8 @@ def _setup_tokenizer(cls):\n             (\"Hello\", 0.0),\n             (\"world\", 0.0),\n         ]\n-        tokenizer = tokenizer_class(vocab=vocab_tokens, add_bos_token=True, add_eos_token=False)\n+        vocab = {token: index for index, (token, _) in enumerate(vocab_tokens)}\n+        tokenizer = tokenizer_class(vocab=vocab, add_bos_token=True, add_eos_token=False)\n         tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<image>\", \"<video>\"]})\n         if tokenizer.pad_token is None:\n             tokenizer.pad_token = \"[PAD]\""
        },
        {
            "sha": "73d4d0c09ed722857d046da216e2b13f4b56eb3c",
            "filename": "tests/models/markuplm/test_tokenization_markuplm.py",
            "status": "modified",
            "additions": 6,
            "deletions": 9,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -136,9 +136,14 @@ def get_extracted_tokenizer(self, reference_tokenizer=None):\n \n             extractor = TokenizersExtractor(tokenizer_json_path)\n             vocab_ids, vocab_scores, merges, added_tokens_decoder = extractor.extract()\n+            if _type := getattr(self.tokenizer_class, \"model\", None):\n+                if _type.__name__ == \"BPE\" or _type.__name__ == \"WordPiece\":\n+                    vocab = vocab_ids\n+                else:\n+                    vocab = vocab_scores\n \n             init_kwargs = {\n-                \"vocab\": vocab_scores,\n+                \"vocab\": vocab,\n                 \"merges\": merges,\n                 \"do_lower_case\": False,\n                 \"keep_accents\": True,\n@@ -860,14 +865,6 @@ def test_padding_to_multiple_of(self):\n                         pad_to_multiple_of=8,\n                     )\n \n-    def test_tokenizer_slow_store_full_signature(self):\n-        signature = inspect.signature(self.tokenizer_class.__init__)\n-        tokenizer = self.get_tokenizer()\n-\n-        for parameter_name, parameter in signature.parameters.items():\n-            if parameter.default != inspect.Parameter.empty:\n-                self.assertIn(parameter_name, tokenizer.init_kwargs)\n-\n     def test_special_tokens_mask_input_pairs(self):\n         tokenizers = self.get_tokenizers(do_lower_case=False)\n         for tokenizer in tokenizers:"
        },
        {
            "sha": "c3148c2478b0903444c75e7d444ec1a413dd314b",
            "filename": "tests/models/nllb/test_tokenization_nllb.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fnllb%2Ftest_tokenization_nllb.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fnllb%2Ftest_tokenization_nllb.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnllb%2Ftest_tokenization_nllb.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -172,15 +172,15 @@ def test_new_language_codes(self):\n                 extractor = SentencePieceExtractor(vocab_file)\n                 vocab_ids, vocab_scores, merges = extractor.extract()\n                 tok3 = NllbTokenizer(\n-                    vocab=vocab_scores, merges=merges, vocab_file=vocab_file, additional_special_tokens=None\n+                    vocab=vocab_ids, merges=merges, vocab_file=vocab_file, additional_special_tokens=None\n                 )\n                 self.assertEqual(len(tok3), 256204)  # legacy\n                 tok4 = NllbTokenizer(\n-                    vocab=vocab_scores, merges=merges, vocab_file=vocab_file, additional_special_tokens=[]\n+                    vocab=vocab_ids, merges=merges, vocab_file=vocab_file, additional_special_tokens=[]\n                 )\n                 self.assertEqual(len(tok4), 256002)\n                 tok5 = NllbTokenizer(\n-                    vocab=vocab_scores, merges=merges, vocab_file=vocab_file, additional_special_tokens=[code1, code2]\n+                    vocab=vocab_ids, merges=merges, vocab_file=vocab_file, additional_special_tokens=[code1, code2]\n                 )\n                 self.assertEqual(len(tok5), 256004)\n "
        },
        {
            "sha": "954bbfba9d77ebaf2075cecb6637782bc066cb9e",
            "filename": "tests/models/olmo/test_modeling_olmo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -257,7 +257,7 @@ def test_fast_special_tokens(self):\n \n     @require_tokenizers\n     def test_simple_encode_decode(self):\n-        rust_tokenizer = GPTNeoXTokenizerFast.from_pretrained(\"allenai/OLMo-1B-hf\")\n+        rust_tokenizer = GPTNeoXTokenizerFast.from_pretrained(\"allenai/OLMo-1B-hf\", add_eos_token=False)\n \n         self.assertEqual(rust_tokenizer.encode(\"This is a test\"), [1552, 310, 247, 1071])\n         self.assertEqual(rust_tokenizer.decode([1552, 310, 247, 1071], skip_special_tokens=True), \"This is a test\")\n@@ -271,10 +271,10 @@ def test_simple_encode_decode(self):\n \n         # Inner spaces showcase\n         self.assertEqual(rust_tokenizer.encode(\"Hi  Hello\"), [12764, 50276, 12092])\n-        self.assertEqual(rust_tokenizer.decode([12764, 50276, 12092], skip_special_tokens=True), \"Hi  Hello\")\n+        self.assertEqual(rust_tokenizer.decode([12764, 50276, 12092], skip_special_tokens=False), \"Hi  Hello\")\n \n         self.assertEqual(rust_tokenizer.encode(\"Hi   Hello\"), [12764, 50275, 12092])\n-        self.assertEqual(rust_tokenizer.decode([12764, 50275, 12092], skip_special_tokens=True), \"Hi   Hello\")\n+        self.assertEqual(rust_tokenizer.decode([12764, 50275, 12092], skip_special_tokens=False), \"Hi   Hello\")\n \n         self.assertEqual(rust_tokenizer.encode(\"\"), [])\n "
        },
        {
            "sha": "d1ac613f3538dd68c6f05724f050b694db2f8a4b",
            "filename": "tests/models/olmoe/test_modeling_olmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -258,10 +258,10 @@ def test_simple_encode_decode(self):\n \n         # Inner spaces showcase\n         self.assertEqual(rust_tokenizer.encode(\"Hi  Hello\"), [12764, 50276, 12092])\n-        self.assertEqual(rust_tokenizer.decode([12764, 50276, 12092], skip_special_tokens=True), \"Hi  Hello\")\n+        self.assertEqual(rust_tokenizer.decode([12764, 50276, 12092], skip_special_tokens=False), \"Hi  Hello\")\n \n         self.assertEqual(rust_tokenizer.encode(\"Hi   Hello\"), [12764, 50275, 12092])\n-        self.assertEqual(rust_tokenizer.decode([12764, 50275, 12092], skip_special_tokens=True), \"Hi   Hello\")\n+        self.assertEqual(rust_tokenizer.decode([12764, 50275, 12092], skip_special_tokens=False), \"Hi   Hello\")\n \n         self.assertEqual(rust_tokenizer.encode(\"\"), [])\n "
        },
        {
            "sha": "992d0e303e3536846601e26c899d6e477f09a8d9",
            "filename": "tests/models/paligemma/test_processing_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fpaligemma%2Ftest_processing_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fpaligemma%2Ftest_processing_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_processing_paligemma.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -37,7 +37,7 @@ def _setup_image_processor(cls):\n     @classmethod\n     def _setup_tokenizer(cls):\n         tokenizer_class = cls._get_component_class_from_processor(\"tokenizer\")\n-        tokenizer = tokenizer_class(SAMPLE_VOCAB, keep_accents=True)\n+        tokenizer = tokenizer_class.from_pretrained(SAMPLE_VOCAB, keep_accents=True)\n         tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<image>\"]})\n         return tokenizer\n "
        },
        {
            "sha": "a7a16f725c5782e0341594c1fa1943773ddcfcfc",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -52,6 +52,7 @@ class Qwen2ModelTester(CausalLMModelTester):\n @require_torch\n class Qwen2ModelTest(CausalLMModelTest, unittest.TestCase):\n     model_tester_class = Qwen2ModelTester\n+    pretrained_model_name = \"Qwen/Qwen2-0.5B\"\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n     def is_pipeline_test_to_skip("
        },
        {
            "sha": "884452f6dc3b88b392c9e2ea2d65f2b7909aad9f",
            "filename": "tests/models/rag/test_modeling_rag.py",
            "status": "modified",
            "additions": 16,
            "deletions": 28,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Frag%2Ftest_modeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Frag%2Ftest_modeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frag%2Ftest_modeling_rag.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -13,7 +13,6 @@\n # limitations under the License.\n \n \n-import json\n import os\n import shutil\n import tempfile\n@@ -25,9 +24,7 @@\n import requests\n \n from transformers import BartTokenizer, T5Tokenizer\n-from transformers.models.bert.tokenization_bert import VOCAB_FILES_NAMES as DPR_VOCAB_FILES_NAMES\n from transformers.models.dpr.tokenization_dpr import DPRContextEncoderTokenizer, DPRQuestionEncoderTokenizer\n-from transformers.models.roberta.tokenization_roberta import VOCAB_FILES_NAMES as BART_VOCAB_FILES_NAMES\n from transformers.testing_utils import (\n     cleanup,\n     get_tests_dir,\n@@ -113,8 +110,8 @@ class RagTestMixin:\n     def setUp(self):\n         self.tmpdirname = tempfile.mkdtemp()\n \n-        # DPR tok\n-        vocab_tokens = [\n+        # DPR tokenizer vocab\n+        dpr_vocab_tokens = [\n             \"[UNK]\",\n             \"[CLS]\",\n             \"[SEP]\",\n@@ -131,13 +128,9 @@ def setUp(self):\n             \"low\",\n             \"lowest\",\n         ]\n-        dpr_tokenizer_path = os.path.join(self.tmpdirname, \"dpr_tokenizer\")\n-        os.makedirs(dpr_tokenizer_path, exist_ok=True)\n-        self.vocab_file = os.path.join(dpr_tokenizer_path, DPR_VOCAB_FILES_NAMES[\"vocab_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n-            vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n+        self.dpr_vocab = {token: i for i, token in enumerate(dpr_vocab_tokens)}\n \n-        # BART tok\n+        # BART tokenizer vocab and merges\n         vocab = [\n             \"l\",\n             \"o\",\n@@ -160,34 +153,29 @@ def setUp(self):\n             \"\\u0120wider\",\n             \"<unk>\",\n         ]\n-        vocab_tokens = dict(zip(vocab, range(len(vocab))))\n-        merges = [\"#version: 0.2\", \"\\u0120 l\", \"\\u0120l o\", \"\\u0120lo w\", \"e r\", \"\"]\n-        self.special_tokens_map = {\"unk_token\": \"<unk>\"}\n-\n-        bart_tokenizer_path = os.path.join(self.tmpdirname, \"bart_tokenizer\")\n-        os.makedirs(bart_tokenizer_path, exist_ok=True)\n-        self.vocab_file = os.path.join(bart_tokenizer_path, BART_VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.merges_file = os.path.join(bart_tokenizer_path, BART_VOCAB_FILES_NAMES[\"merges_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n-            fp.write(json.dumps(vocab_tokens) + \"\\n\")\n-        with open(self.merges_file, \"w\", encoding=\"utf-8\") as fp:\n-            fp.write(\"\\n\".join(merges))\n-\n-        t5_tokenizer = T5Tokenizer(T5_SAMPLE_VOCAB)\n+        self.bart_vocab = dict(zip(vocab, range(len(vocab))))\n+        merges_raw = [\"#version: 0.2\", \"\\u0120 l\", \"\\u0120l o\", \"\\u0120lo w\", \"e r\", \"\"]\n+        self.bart_merges = []\n+        for line in merges_raw:\n+            line = line.strip()\n+            if line and not line.startswith(\"#\"):\n+                self.bart_merges.append(tuple(line.split()))\n+\n+        t5_tokenizer = T5Tokenizer(vocab_file=T5_SAMPLE_VOCAB)\n         t5_tokenizer_path = os.path.join(self.tmpdirname, \"t5_tokenizer\")\n         t5_tokenizer.save_pretrained(t5_tokenizer_path)\n \n     @cached_property\n     def dpr_tokenizer(self) -> DPRQuestionEncoderTokenizer:\n-        return DPRQuestionEncoderTokenizer.from_pretrained(os.path.join(self.tmpdirname, \"dpr_tokenizer\"))\n+        return DPRQuestionEncoderTokenizer(vocab=self.dpr_vocab)\n \n     @cached_property\n     def dpr_ctx_encoder_tokenizer(self) -> DPRContextEncoderTokenizer:\n-        return DPRContextEncoderTokenizer.from_pretrained(os.path.join(self.tmpdirname, \"dpr_tokenizer\"))\n+        return DPRContextEncoderTokenizer(vocab=self.dpr_vocab)\n \n     @cached_property\n     def bart_tokenizer(self) -> BartTokenizer:\n-        return BartTokenizer.from_pretrained(os.path.join(self.tmpdirname, \"bart_tokenizer\"))\n+        return BartTokenizer(vocab=self.bart_vocab, merges=self.bart_merges, unk_token=\"<unk>\")\n \n     @cached_property\n     def t5_tokenizer(self) -> BartTokenizer:"
        },
        {
            "sha": "45d329fc9b94599f6ab68f99f03cd990793babcf",
            "filename": "tests/models/rag/test_retrieval_rag.py",
            "status": "modified",
            "additions": 14,
            "deletions": 26,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Frag%2Ftest_retrieval_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Frag%2Ftest_retrieval_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frag%2Ftest_retrieval_rag.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -12,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import json\n import os\n import shutil\n import tempfile\n@@ -24,12 +23,10 @@\n \n from transformers import is_faiss_available\n from transformers.models.bart.configuration_bart import BartConfig\n-from transformers.models.bert.tokenization_bert import VOCAB_FILES_NAMES as DPR_VOCAB_FILES_NAMES\n from transformers.models.dpr.configuration_dpr import DPRConfig\n from transformers.models.dpr.tokenization_dpr import DPRContextEncoderTokenizer, DPRQuestionEncoderTokenizer\n from transformers.models.rag.configuration_rag import RagConfig\n from transformers.models.rag.retrieval_rag import CustomHFIndex, RagRetriever\n-from transformers.models.roberta.tokenization_roberta import VOCAB_FILES_NAMES as BART_VOCAB_FILES_NAMES\n from transformers.models.roberta.tokenization_roberta import RobertaTokenizer as BartTokenizer\n from transformers.testing_utils import require_faiss, require_sentencepiece, require_tokenizers, require_torch\n \n@@ -44,8 +41,8 @@ def setUp(self):\n         self.tmpdirname = tempfile.mkdtemp()\n         self.retrieval_vector_size = 8\n \n-        # DPR tok\n-        vocab_tokens = [\n+        # DPR tokenizer vocab\n+        self.dpr_vocab_tokens = [\n             \"[UNK]\",\n             \"[CLS]\",\n             \"[SEP]\",\n@@ -62,13 +59,9 @@ def setUp(self):\n             \"low\",\n             \"lowest\",\n         ]\n-        dpr_tokenizer_path = os.path.join(self.tmpdirname, \"dpr_tokenizer\")\n-        os.makedirs(dpr_tokenizer_path, exist_ok=True)\n-        self.vocab_file = os.path.join(dpr_tokenizer_path, DPR_VOCAB_FILES_NAMES[\"vocab_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n-            vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n+        self.dpr_vocab = {token: i for i, token in enumerate(self.dpr_vocab_tokens)}\n \n-        # BART tok\n+        # BART tokenizer vocab and merges\n         vocab = [\n             \"l\",\n             \"o\",\n@@ -91,27 +84,22 @@ def setUp(self):\n             \"\\u0120wider\",\n             \"<unk>\",\n         ]\n-        vocab_tokens = dict(zip(vocab, range(len(vocab))))\n-        merges = [\"#version: 0.2\", \"\\u0120 l\", \"\\u0120l o\", \"\\u0120lo w\", \"e r\", \"\"]\n-        self.special_tokens_map = {\"unk_token\": \"<unk>\"}\n-\n-        bart_tokenizer_path = os.path.join(self.tmpdirname, \"bart_tokenizer\")\n-        os.makedirs(bart_tokenizer_path, exist_ok=True)\n-        self.vocab_file = os.path.join(bart_tokenizer_path, BART_VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.merges_file = os.path.join(bart_tokenizer_path, BART_VOCAB_FILES_NAMES[\"merges_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n-            fp.write(json.dumps(vocab_tokens) + \"\\n\")\n-        with open(self.merges_file, \"w\", encoding=\"utf-8\") as fp:\n-            fp.write(\"\\n\".join(merges))\n+        self.bart_vocab = dict(zip(vocab, range(len(vocab))))\n+        merges_raw = [\"#version: 0.2\", \"\\u0120 l\", \"\\u0120l o\", \"\\u0120lo w\", \"e r\", \"\"]\n+        self.bart_merges = []\n+        for line in merges_raw:\n+            line = line.strip()\n+            if line and not line.startswith(\"#\"):\n+                self.bart_merges.append(tuple(line.split()))\n \n     def get_dpr_tokenizer(self) -> DPRQuestionEncoderTokenizer:\n-        return DPRQuestionEncoderTokenizer.from_pretrained(os.path.join(self.tmpdirname, \"dpr_tokenizer\"))\n+        return DPRQuestionEncoderTokenizer(vocab=self.dpr_vocab)\n \n     def get_dpr_ctx_encoder_tokenizer(self) -> DPRContextEncoderTokenizer:\n-        return DPRContextEncoderTokenizer.from_pretrained(os.path.join(self.tmpdirname, \"dpr_tokenizer\"))\n+        return DPRContextEncoderTokenizer(vocab=self.dpr_vocab)\n \n     def get_bart_tokenizer(self) -> BartTokenizer:\n-        return BartTokenizer.from_pretrained(os.path.join(self.tmpdirname, \"bart_tokenizer\"))\n+        return BartTokenizer(vocab=self.bart_vocab, merges=self.bart_merges, unk_token=\"<unk>\")\n \n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)"
        },
        {
            "sha": "b977775ca2bb00a5b86d84966edb500f38060e42",
            "filename": "tests/models/seamless_m4t/test_tokenization_seamless_m4t.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fseamless_m4t%2Ftest_tokenization_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fseamless_m4t%2Ftest_tokenization_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t%2Ftest_tokenization_seamless_m4t.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -27,7 +27,6 @@\n     require_tokenizers,\n     require_torch,\n )\n-from transformers.tokenization_utils_sentencepiece import SentencePieceExtractor\n \n from ...test_tokenization_common import TokenizerTesterMixin\n \n@@ -386,13 +385,7 @@ class CommonSpmIntegrationTests(unittest.TestCase):\n \n     @classmethod\n     def setUpClass(cls):\n-        extractor = SentencePieceExtractor(SAMPLE_VOCAB)\n-        _, vocab_scores, merges = extractor.extract()\n-\n-        tokenizer = SeamlessM4TTokenizer(\n-            vocab=vocab_scores,\n-            merges=merges,\n-        )\n+        tokenizer = SeamlessM4TTokenizer.from_pretrained(SAMPLE_VOCAB)\n         tokenizer.add_special_tokens({\"additional_special_tokens\": [AddedToken(\"<s>\", rstrip=False, lstrip=False)]})\n         cls.tokenizer = tokenizer\n         return cls"
        },
        {
            "sha": "023c4dcf9b3659d9a1f03a0c25d791725333a4fc",
            "filename": "tests/models/shieldgemma2/test_processing_shieldgemma2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fshieldgemma2%2Ftest_processing_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fshieldgemma2%2Ftest_processing_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fshieldgemma2%2Ftest_processing_shieldgemma2.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -79,7 +79,9 @@ def _setup_tokenizer(cls):\n             \"boi_token\": \"<start_of_image>\",\n             \"eoi_token\": \"<end_of_image>\",\n         }\n-        return tokenizer_class(SAMPLE_VOCAB, keep_accents=True, extra_special_tokens=extra_special_tokens)\n+        return tokenizer_class.from_pretrained(\n+            SAMPLE_VOCAB, keep_accents=True, extra_special_tokens=extra_special_tokens\n+        )\n \n     @classmethod\n     def prepare_processor_dict(cls):"
        },
        {
            "sha": "8e747e1e0b899519e8839cbb3b5bfd6e4c8a253c",
            "filename": "tests/models/whisper/test_tokenization_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fwhisper%2Ftest_tokenization_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Fmodels%2Fwhisper%2Ftest_tokenization_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_tokenization_whisper.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -237,6 +237,7 @@ def test_tokenizer_decode_prompt(self):\n         input_text = \"Hatee hatee hatee ho\"\n \n         tokenizer = self.get_tokenizer()\n+        tokenizer.set_prefix_tokens(task=None, predict_timestamps=False)\n \n         # encode prompt and input text using tokenizer\n         prompt_ids = tokenizer.get_prompt_ids(prompt_text, return_tensors=\"np\")"
        },
        {
            "sha": "f901630ee3d21ff04a1a24522d4e5b78278fe25a",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 23,
            "deletions": 22,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -440,29 +440,29 @@ def get_extracted_tokenizer(self, reference_tokenizer=None):\n         if reference_tokenizer is None:\n             reference_tokenizer = self.get_tokenizer()\n \n-        try:\n-            tokenizer_json_path = os.path.join(self.tmpdirname, \"tokenizer.json\")\n-            if not os.path.exists(tokenizer_json_path):\n-                return None\n+        tokenizer_json_path = os.path.join(self.tmpdirname, \"tokenizer.json\")\n+        if not os.path.exists(tokenizer_json_path):\n+            return None\n \n-            extractor = TokenizersExtractor(tokenizer_json_path)\n-            vocab_ids, vocab_scores, merges, added_tokens_decoder = extractor.extract()\n-\n-            # Convert added_tokens list to added_tokens_decoder dict format\n-            # This matches the format used by from_pretrained() from tokenizer_config.json\n-            tokenizer_from_extractor = self.tokenizer_class(\n-                vocab=vocab_scores,\n-                merges=merges,\n-                do_lower_case=False,\n-                keep_accents=True,\n-                added_tokens_decoder=added_tokens_decoder,\n-                **(self.from_pretrained_kwargs if self.from_pretrained_kwargs is not None else {}),\n-            )\n+        extractor = TokenizersExtractor(tokenizer_json_path)\n+        vocab_ids, vocab_scores, merges, added_tokens_decoder = extractor.extract()\n+        vocab = vocab_scores\n+        if _type := getattr(self.tokenizer_class, \"model\", None):\n+            if _type.__name__ == \"BPE\" or _type.__name__ == \"WordPiece\":\n+                vocab = vocab_ids\n+\n+        # Convert added_tokens list to added_tokens_decoder dict format\n+        # This matches the format used by from_pretrained() from tokenizer_config.jso\n+        tokenizer_from_extractor = self.tokenizer_class(\n+            vocab=vocab,\n+            merges=merges,\n+            do_lower_case=False,\n+            keep_accents=True,\n+            added_tokens_decoder=added_tokens_decoder,\n+            **(self.from_pretrained_kwargs if self.from_pretrained_kwargs is not None else {}),\n+        )\n \n-            return tokenizer_from_extractor\n-        except (TypeError, Exception):\n-            # fail and raise the error\n-            raise\n+        return tokenizer_from_extractor\n \n     def get_extracted_tokenizer_from_sentencepiece(self, reference_tokenizer=None):\n         \"\"\"\n@@ -799,7 +799,8 @@ def test_save_and_load_tokenizer(self):\n     def _run_integration_checks(self, tokenizer, tokenizer_type):\n         # Test 1: Tokens match expected\n         tokens = tokenizer.tokenize(self.integration_test_input_string)\n-        self.assertEqual(\n+        self.maxDiff = None\n+        self.assertListEqual(\n             tokens,\n             self.integration_expected_tokens,\n             f\"Tokenized tokens don't match expected for {tokenizer.__class__.__name__} ({tokenizer_type})\","
        },
        {
            "sha": "fa0d873aff8efacf53bf13bec72eae56d7a13f8c",
            "filename": "tests/tokenization/test_tokenization_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Ftokenization%2Ftest_tokenization_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/tests%2Ftokenization%2Ftest_tokenization_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftokenization%2Ftest_tokenization_fast.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -149,7 +149,7 @@ def test_training_new_tokenizer_with_bytelevel(self):\n \n         encoding_ids = new_tokenizer.encode(\"a🤗\")\n         self.assertGreater(len(encoding_ids), 0)\n-        self.assertEqual(new_tokenizer.decode(encoding_ids), \"a🤗\")\n+        self.assertEqual(new_tokenizer.decode(encoding_ids), \" a🤗\")\n \n     def test_init_from_tokenizers_model(self):\n         from tokenizers import Tokenizer"
        },
        {
            "sha": "80d6a3f3223f363ec85a9d8a97897a07f418cad9",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a13f86f6d208882d59d1200609986c5a5f49a7/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a13f86f6d208882d59d1200609986c5a5f49a7/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=73a13f86f6d208882d59d1200609986c5a5f49a7",
            "patch": "@@ -1033,6 +1033,7 @@ def find_all_documented_objects() -> list[str]:\n     \"TimmBackbone\",\n     \"TimmBackboneConfig\",\n     \"VitDetBackbone\",\n+    \"RoFormerTokenizerFast\",  # An alias\n ]\n \n "
        }
    ],
    "stats": {
        "total": 4128,
        "additions": 1274,
        "deletions": 2854
    }
}