{
    "author": "SunMarc",
    "message": "Fix amp deprecation issue (#38100)\n\napex amp is deprecated",
    "sha": "1a25fd2f6de6ebde6a957d01d3b7437e917f9cb3",
    "files": [
        {
            "sha": "ca8263cb3f00461cc487648444b36e55c83ed2b1",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 9,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a25fd2f6de6ebde6a957d01d3b7437e917f9cb3/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a25fd2f6de6ebde6a957d01d3b7437e917f9cb3/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=1a25fd2f6de6ebde6a957d01d3b7437e917f9cb3",
            "patch": "@@ -151,7 +151,6 @@\n     check_torch_load_is_safe,\n     find_labels,\n     is_accelerate_available,\n-    is_apex_available,\n     is_apollo_torch_available,\n     is_bitsandbytes_available,\n     is_datasets_available,\n@@ -191,9 +190,6 @@\n \n     DEFAULT_PROGRESS_CALLBACK = NotebookProgressCallback\n \n-if is_apex_available():\n-    from apex import amp\n-\n if is_datasets_available():\n     import datasets\n \n@@ -761,11 +757,6 @@ def __init__(\n                 self.use_cpu_amp = True\n                 self.amp_dtype = torch.bfloat16\n             elif args.half_precision_backend == \"apex\":\n-                if not is_apex_available():\n-                    raise ImportError(\n-                        \"Using FP16 with APEX but APEX is not installed, please refer to\"\n-                        \" https://www.github.com/nvidia/apex.\"\n-                    )\n                 self.use_apex = True\n \n         # Label smoothing\n@@ -1992,6 +1983,8 @@ def _wrap_model(self, model, training=True, dataloader=None):\n \n         # Mixed precision training with apex\n         if self.use_apex and training:\n+            from apex import amp\n+\n             model, self.optimizer = amp.initialize(model, self.optimizer, opt_level=self.args.fp16_opt_level)\n \n         # Multi-gpu training (should be after apex fp16 initialization) / 8bit models does not support DDP\n@@ -2579,6 +2572,8 @@ def _inner_training_loop(\n                             if is_sagemaker_mp_enabled() and args.fp16:\n                                 _grad_norm = self.optimizer.clip_master_grads(args.max_grad_norm)\n                             elif self.use_apex:\n+                                from apex import amp\n+\n                                 # Revert to normal clipping otherwise, handling Apex or full precision\n                                 _grad_norm = nn.utils.clip_grad_norm_(\n                                     amp.master_params(self.optimizer),\n@@ -3776,6 +3771,8 @@ def training_step(\n             loss = loss.mean()  # mean() to average on multi-gpu parallel training\n \n         if self.use_apex:\n+            from apex import amp\n+\n             with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                 scaled_loss.backward()\n         else:"
        },
        {
            "sha": "2d58e26af319ac80f67896c340c17870f4140dff",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a25fd2f6de6ebde6a957d01d3b7437e917f9cb3/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a25fd2f6de6ebde6a957d01d3b7437e917f9cb3/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=1a25fd2f6de6ebde6a957d01d3b7437e917f9cb3",
            "patch": "@@ -39,6 +39,7 @@\n     ExplicitEnum,\n     cached_property,\n     is_accelerate_available,\n+    is_apex_available,\n     is_ipex_available,\n     is_safetensors_available,\n     is_sagemaker_dp_enabled,\n@@ -1702,6 +1703,19 @@ def __post_init__(self):\n             if self.half_precision_backend == \"apex\":\n                 raise ValueError(\" `--half_precision_backend apex`: GPU bf16 is not supported by apex.\")\n \n+        if self.half_precision_backend == \"apex\":\n+            if not is_apex_available():\n+                raise ImportError(\n+                    \"Using FP16 with APEX but APEX is not installed, please refer to\"\n+                    \" https://www.github.com/nvidia/apex.\"\n+                )\n+            try:\n+                from apex import amp  # noqa: F401\n+            except ImportError as e:\n+                raise ImportError(\n+                    f\"apex.amp is deprecated in the latest version of apex, causing this error {e}. Either revert to an older version or use pytorch amp by setting half_precision_backend='auto' instead. See https://github.com/NVIDIA/apex/pull/1896 \"\n+                )\n+\n         if self.lr_scheduler_type == SchedulerType.REDUCE_ON_PLATEAU:\n             if self.eval_strategy == IntervalStrategy.NO:\n                 raise ValueError(\"lr_scheduler_type reduce_lr_on_plateau requires an eval strategy\")"
        }
    ],
    "stats": {
        "total": 29,
        "additions": 20,
        "deletions": 9
    }
}