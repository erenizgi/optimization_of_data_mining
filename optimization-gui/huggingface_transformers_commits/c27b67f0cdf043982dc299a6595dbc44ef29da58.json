{
    "author": "vasqu",
    "message": ":rotating_light: [`v5`] Remove relative position embeddings (for bert like models) (#41170)\n\n* remove from modeling files\n\n* remaining changes\n\n* style / copies\n\n* revert deprecated models and fixup some models\n\n* oops",
    "sha": "c27b67f0cdf043982dc299a6595dbc44ef29da58",
    "files": [
        {
            "sha": "4f4fc906af6cfca203ea9dba9d1ebbe905210df1",
            "filename": "examples/modular-transformers/modeling_dummy_bert.py",
            "status": "modified",
            "additions": 9,
            "deletions": 68,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -42,7 +42,6 @@ def __init__(self, config):\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n@@ -83,11 +82,11 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings = embeddings + position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings\n@@ -101,38 +100,13 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    use_cache: Optional[bool] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(2, 3))\n-\n-    # Relative positional embeddings\n-    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n-        query_length, key_length = query.shape[2], key.shape[2]\n-        if use_cache:\n-            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n-        else:\n-            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n-        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n-        distance = position_ids_l - position_ids_r\n-\n-        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n-        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n-\n-        if module.position_embedding_type == \"relative_key\":\n-            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores\n-        elif module.position_embedding_type == \"relative_key_query\":\n-            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n-\n-    # Scaling is shifted in case of embeddings being relative\n-    attn_weights = attn_weights * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n     if attention_mask is not None and attention_mask.ndim == 4:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n@@ -148,7 +122,7 @@ def eager_attention_forward(\n \n \n class DummyBertSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -167,12 +141,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n         self.is_causal = is_causal\n@@ -210,11 +178,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -225,16 +188,14 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         return attn_output, attn_weights\n \n \n class DummyBertCrossAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -253,12 +214,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_causal = is_causal\n         self.layer_idx = layer_idx\n@@ -300,11 +255,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -315,8 +265,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n@@ -338,15 +286,11 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class DummyBertAttention(nn.Module):\n-    def __init__(\n-        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n-    ):\n+    def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=False):\n         super().__init__()\n         self.is_cross_attention = is_cross_attention\n         attention_class = DummyBertCrossAttention if is_cross_attention else DummyBertSelfAttention\n-        self.self = attention_class(\n-            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n-        )\n+        self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = DummyBertSelfOutput(config)\n         self.pruned_heads = set()\n \n@@ -433,7 +377,6 @@ def __init__(self, config, layer_idx=None):\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n             self.crossattention = DummyBertAttention(\n                 config,\n-                position_embedding_type=\"absolute\",\n                 is_causal=False,\n                 layer_idx=layer_idx,\n                 is_cross_attention=True,\n@@ -638,8 +581,6 @@ def __init__(self, config, add_pooling_layer=True):\n \n         self.pooler = DummyBertPooler(config) if add_pooling_layer else None\n \n-        self.position_embedding_type = config.position_embedding_type\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n "
        },
        {
            "sha": "0d918423d72f2bccfa8efcfefe5564eedf64b938",
            "filename": "examples/modular-transformers/modeling_roberta.py",
            "status": "modified",
            "additions": 9,
            "deletions": 68,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_roberta.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -44,7 +44,6 @@ def __init__(self, config):\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n@@ -86,11 +85,11 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings = embeddings + position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings\n@@ -104,38 +103,13 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    use_cache: Optional[bool] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(2, 3))\n-\n-    # Relative positional embeddings\n-    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n-        query_length, key_length = query.shape[2], key.shape[2]\n-        if use_cache:\n-            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n-        else:\n-            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n-        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n-        distance = position_ids_l - position_ids_r\n-\n-        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n-        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n-\n-        if module.position_embedding_type == \"relative_key\":\n-            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores\n-        elif module.position_embedding_type == \"relative_key_query\":\n-            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n-\n-    # Scaling is shifted in case of embeddings being relative\n-    attn_weights = attn_weights * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n     if attention_mask is not None and attention_mask.ndim == 4:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n@@ -151,7 +125,7 @@ def eager_attention_forward(\n \n \n class RobertaSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -170,12 +144,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n         self.is_causal = is_causal\n@@ -213,11 +181,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -228,16 +191,14 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         return attn_output, attn_weights\n \n \n class RobertaCrossAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -256,12 +217,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_causal = is_causal\n         self.layer_idx = layer_idx\n@@ -303,11 +258,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -318,8 +268,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n@@ -341,15 +289,11 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class RobertaAttention(nn.Module):\n-    def __init__(\n-        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n-    ):\n+    def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=False):\n         super().__init__()\n         self.is_cross_attention = is_cross_attention\n         attention_class = RobertaCrossAttention if is_cross_attention else RobertaSelfAttention\n-        self.self = attention_class(\n-            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n-        )\n+        self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = RobertaSelfOutput(config)\n         self.pruned_heads = set()\n \n@@ -436,7 +380,6 @@ def __init__(self, config, layer_idx=None):\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n             self.crossattention = RobertaAttention(\n                 config,\n-                position_embedding_type=\"absolute\",\n                 is_causal=False,\n                 layer_idx=layer_idx,\n                 is_cross_attention=True,\n@@ -641,8 +584,6 @@ def __init__(self, config, add_pooling_layer=True):\n \n         self.pooler = RobertaPooler(config) if add_pooling_layer else None\n \n-        self.position_embedding_type = config.position_embedding_type\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n "
        },
        {
            "sha": "257785b63c6226804fa3e23531659c42a6f7b603",
            "filename": "src/transformers/models/albert/configuration_albert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Falbert%2Fconfiguration_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Falbert%2Fconfiguration_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fconfiguration_albert.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -68,12 +68,6 @@ class AlbertConfig(PreTrainedConfig):\n             The epsilon used by the layer normalization layers.\n         classifier_dropout_prob (`float`, *optional*, defaults to 0.1):\n             The dropout ratio for attached classifiers.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         pad_token_id (`int`, *optional*, defaults to 0):\n             Padding token id.\n         bos_token_id (`int`, *optional*, defaults to 2):\n@@ -123,7 +117,6 @@ def __init__(\n         initializer_range=0.02,\n         layer_norm_eps=1e-12,\n         classifier_dropout_prob=0.1,\n-        position_embedding_type=\"absolute\",\n         pad_token_id=0,\n         bos_token_id=2,\n         eos_token_id=3,\n@@ -147,7 +140,6 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n         self.classifier_dropout_prob = classifier_dropout_prob\n-        self.position_embedding_type = position_embedding_type\n \n \n # Copied from transformers.models.bert.configuration_bert.BertOnnxConfig with Roberta->Albert"
        },
        {
            "sha": "ee3d5b43313966a70244a22621f6965be14ebdcd",
            "filename": "src/transformers/models/albert/modeling_albert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 44,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -69,7 +69,6 @@ def __init__(self, config: AlbertConfig):\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False\n         )\n@@ -106,11 +105,11 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings = embeddings + position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings\n@@ -125,38 +124,13 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    use_cache: Optional[bool] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(2, 3))\n-\n-    # Relative positional embeddings\n-    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n-        query_length, key_length = query.shape[2], key.shape[2]\n-        if use_cache:\n-            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n-        else:\n-            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n-        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n-        distance = position_ids_l - position_ids_r\n-\n-        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n-        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n-\n-        if module.position_embedding_type == \"relative_key\":\n-            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores\n-        elif module.position_embedding_type == \"relative_key_query\":\n-            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n-\n-    # Scaling is shifted in case of embeddings being relative\n-    attn_weights = attn_weights * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n     if attention_mask is not None and attention_mask.ndim == 4:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n@@ -198,11 +172,6 @@ def __init__(self, config: AlbertConfig):\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.pruned_heads = set()\n \n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n-\n         self.is_causal = False\n \n     def prune_heads(self, heads: list[int]) -> None:\n@@ -239,11 +208,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -254,8 +218,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.attention_dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=False,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n@@ -429,7 +391,6 @@ def __init__(self, config: AlbertConfig, add_pooling_layer: bool = True):\n             self.pooler_activation = None\n \n         self.attn_implementation = config._attn_implementation\n-        self.position_embedding_type = config.position_embedding_type\n \n         # Initialize weights and apply final processing\n         self.post_init()"
        },
        {
            "sha": "2802c3baa6b40176f3c42573013f7209f30ff9c2",
            "filename": "src/transformers/models/align/configuration_align.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -62,12 +62,6 @@ class AlignTextConfig(PreTrainedConfig):\n             The epsilon used by the layer normalization layers.\n         pad_token_id (`int`, *optional*, defaults to 0):\n             Padding token id.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         use_cache (`bool`, *optional*, defaults to `True`):\n             Whether or not the model should return the last key/values attentions (not used by all models). Only\n             relevant if `config.is_decoder=True`.\n@@ -105,7 +99,6 @@ def __init__(\n         initializer_range=0.02,\n         layer_norm_eps=1e-12,\n         pad_token_id=0,\n-        position_embedding_type=\"absolute\",\n         use_cache=True,\n         **kwargs,\n     ):\n@@ -123,7 +116,6 @@ def __init__(\n         self.type_vocab_size = type_vocab_size\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n-        self.position_embedding_type = position_embedding_type\n         self.use_cache = use_cache\n         self.pad_token_id = pad_token_id\n "
        },
        {
            "sha": "9c0d093333254107d88098dc6517ec497d94a32e",
            "filename": "src/transformers/models/align/modeling_align.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -519,7 +519,6 @@ def __init__(self, config):\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n@@ -558,11 +557,11 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings += position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings"
        },
        {
            "sha": "88e49d06ca5cf37b1e1bce2baf38ba7d1533ee72",
            "filename": "src/transformers/models/altclip/configuration_altclip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -67,12 +67,6 @@ class AltCLIPTextConfig(PreTrainedConfig):\n         bos_token_id (`int`, *optional*, defaults to 0): The id of the *beginning-of-sequence* token.\n         eos_token_id (`Union[int, list[int]]`, *optional*, defaults to 2):\n             The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         use_cache (`bool`, *optional*, defaults to `True`):\n             Whether or not the model should return the last key/values attentions (not used by all models). Only\n             relevant if `config.is_decoder=True`.\n@@ -114,7 +108,6 @@ def __init__(\n         pad_token_id=1,\n         bos_token_id=0,\n         eos_token_id=2,\n-        position_embedding_type=\"absolute\",\n         use_cache=True,\n         project_dim=768,\n         **kwargs,\n@@ -134,7 +127,6 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.initializer_factor = initializer_factor\n         self.layer_norm_eps = layer_norm_eps\n-        self.position_embedding_type = position_embedding_type\n         self.use_cache = use_cache\n         self.project_dim = project_dim\n "
        },
        {
            "sha": "58bf63573e7049bb2d14ca8bd29499d3ad29b1c9",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 7,
            "deletions": 33,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -100,7 +100,6 @@ def __init__(self, config):\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n@@ -152,11 +151,11 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings = embeddings + position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings\n@@ -197,7 +196,7 @@ def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_l\n \n \n class AltRobertaSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -214,12 +213,6 @@ def __init__(self, config, position_embedding_type=None):\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n     def forward(\n         self,\n@@ -237,23 +230,6 @@ def forward(\n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n         attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n         if attention_mask is not None:\n             # Apply the attention mask is (precomputed for all layers in AltRobertaModel forward() function)\n@@ -298,11 +274,9 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class AltRobertaAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config):\n         super().__init__()\n-        self.self = ALT_ROBERTA_SELF_ATTENTION_CLASSES[config._attn_implementation](\n-            config, position_embedding_type=position_embedding_type\n-        )\n+        self.self = ALT_ROBERTA_SELF_ATTENTION_CLASSES[config._attn_implementation](config)\n         self.output = AltRobertaSelfOutput(config)\n         self.pruned_heads = set()\n "
        },
        {
            "sha": "786f0c2a665fed3cf41015db5ea33baf641dadfc",
            "filename": "src/transformers/models/bert/configuration_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fbert%2Fconfiguration_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fbert%2Fconfiguration_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fconfiguration_bert.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -65,12 +65,6 @@ class BertConfig(PreTrainedConfig):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n             The epsilon used by the layer normalization layers.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         is_decoder (`bool`, *optional*, defaults to `False`):\n             Whether the model is used as a decoder or not. If `False`, the model is used as an encoder.\n         use_cache (`bool`, *optional*, defaults to `True`):\n@@ -111,7 +105,6 @@ def __init__(\n         initializer_range=0.02,\n         layer_norm_eps=1e-12,\n         pad_token_id=0,\n-        position_embedding_type=\"absolute\",\n         use_cache=True,\n         classifier_dropout=None,\n         **kwargs,\n@@ -130,7 +123,6 @@ def __init__(\n         self.type_vocab_size = type_vocab_size\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n-        self.position_embedding_type = position_embedding_type\n         self.use_cache = use_cache\n         self.classifier_dropout = classifier_dropout\n "
        },
        {
            "sha": "d7937b264c077d4aec82953dbcca286b34b04055",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 9,
            "deletions": 68,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -67,7 +67,6 @@ def __init__(self, config):\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n@@ -108,11 +107,11 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings = embeddings + position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings\n@@ -126,38 +125,13 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    use_cache: Optional[bool] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(2, 3))\n-\n-    # Relative positional embeddings\n-    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n-        query_length, key_length = query.shape[2], key.shape[2]\n-        if use_cache:\n-            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n-        else:\n-            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n-        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n-        distance = position_ids_l - position_ids_r\n-\n-        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n-        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n-\n-        if module.position_embedding_type == \"relative_key\":\n-            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores\n-        elif module.position_embedding_type == \"relative_key_query\":\n-            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n-\n-    # Scaling is shifted in case of embeddings being relative\n-    attn_weights = attn_weights * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n     if attention_mask is not None and attention_mask.ndim == 4:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n@@ -173,7 +147,7 @@ def eager_attention_forward(\n \n \n class BertSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -192,12 +166,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n         self.is_causal = is_causal\n@@ -235,11 +203,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -250,16 +213,14 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         return attn_output, attn_weights\n \n \n class BertCrossAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -278,12 +239,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_causal = is_causal\n         self.layer_idx = layer_idx\n@@ -325,11 +280,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -340,8 +290,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n@@ -363,15 +311,11 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class BertAttention(nn.Module):\n-    def __init__(\n-        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n-    ):\n+    def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=False):\n         super().__init__()\n         self.is_cross_attention = is_cross_attention\n         attention_class = BertCrossAttention if is_cross_attention else BertSelfAttention\n-        self.self = attention_class(\n-            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n-        )\n+        self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = BertSelfOutput(config)\n         self.pruned_heads = set()\n \n@@ -458,7 +402,6 @@ def __init__(self, config, layer_idx=None):\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n             self.crossattention = BertAttention(\n                 config,\n-                position_embedding_type=\"absolute\",\n                 is_causal=False,\n                 layer_idx=layer_idx,\n                 is_cross_attention=True,\n@@ -720,8 +663,6 @@ def __init__(self, config, add_pooling_layer=True):\n \n         self.pooler = BertPooler(config) if add_pooling_layer else None\n \n-        self.position_embedding_type = config.position_embedding_type\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n "
        },
        {
            "sha": "7b5b8af4082b69ee54f0fde1143dad93540ffd23",
            "filename": "src/transformers/models/bert_generation/configuration_bert_generation.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fconfiguration_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fconfiguration_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fconfiguration_bert_generation.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -60,12 +60,6 @@ class BertGenerationConfig(PreTrainedConfig):\n             Beginning of stream token id.\n         eos_token_id (`int`, *optional*, defaults to 1):\n             End of stream token id.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         use_cache (`bool`, *optional*, defaults to `True`):\n             Whether or not the model should return the last key/values attentions (not used by all models). Only\n             relevant if `config.is_decoder=True`.\n@@ -103,7 +97,6 @@ def __init__(\n         pad_token_id=0,\n         bos_token_id=2,\n         eos_token_id=1,\n-        position_embedding_type=\"absolute\",\n         use_cache=True,\n         **kwargs,\n     ):\n@@ -120,7 +113,6 @@ def __init__(\n         self.max_position_embeddings = max_position_embeddings\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n-        self.position_embedding_type = position_embedding_type\n         self.use_cache = use_cache\n \n "
        },
        {
            "sha": "4afe75f6e80095e044a63edac3f3ca0786976f8b",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 5,
            "deletions": 61,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -70,38 +70,13 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    use_cache: Optional[bool] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(2, 3))\n-\n-    # Relative positional embeddings\n-    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n-        query_length, key_length = query.shape[2], key.shape[2]\n-        if use_cache:\n-            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n-        else:\n-            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n-        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n-        distance = position_ids_l - position_ids_r\n-\n-        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n-        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n-\n-        if module.position_embedding_type == \"relative_key\":\n-            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores\n-        elif module.position_embedding_type == \"relative_key_query\":\n-            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n-\n-    # Scaling is shifted in case of embeddings being relative\n-    attn_weights = attn_weights * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n     if attention_mask is not None and attention_mask.ndim == 4:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n@@ -118,7 +93,7 @@ def eager_attention_forward(\n \n # Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->BertGeneration\n class BertGenerationSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -137,12 +112,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n         self.is_causal = is_causal\n@@ -180,11 +149,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -195,8 +159,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n@@ -205,7 +167,7 @@ def forward(\n \n # Copied from transformers.models.bert.modeling_bert.BertCrossAttention with Bert->BertGeneration\n class BertGenerationCrossAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -224,12 +186,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_causal = is_causal\n         self.layer_idx = layer_idx\n@@ -271,11 +227,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -286,8 +237,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n@@ -296,15 +245,11 @@ def forward(\n \n # Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->BertGeneration,BERT->BERT_GENERATION\n class BertGenerationAttention(nn.Module):\n-    def __init__(\n-        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n-    ):\n+    def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=False):\n         super().__init__()\n         self.is_cross_attention = is_cross_attention\n         attention_class = BertGenerationCrossAttention if is_cross_attention else BertGenerationSelfAttention\n-        self.self = attention_class(\n-            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n-        )\n+        self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = BertGenerationSelfOutput(config)\n         self.pruned_heads = set()\n \n@@ -394,7 +339,6 @@ def __init__(self, config, layer_idx=None):\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n             self.crossattention = BertGenerationAttention(\n                 config,\n-                position_embedding_type=\"absolute\",\n                 is_causal=False,\n                 layer_idx=layer_idx,\n                 is_cross_attention=True,"
        },
        {
            "sha": "86edfb5ed8840de783091bc104d33ff434e12267",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -78,7 +78,6 @@ def __init__(self, config):\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )"
        },
        {
            "sha": "04b0f170ec7fc4b3df435a1c9b836d1ca8cceb7b",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 3,
            "deletions": 24,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -56,7 +56,6 @@ def __init__(self, config):\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n \n         self.config = config\n \n@@ -82,9 +81,9 @@ def forward(\n \n         embeddings = inputs_embeds\n \n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings += position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings\n@@ -115,10 +114,6 @@ def __init__(self, config, is_cross_attention, layer_idx=None):\n             self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n     def save_attn_gradients(self, attn_gradients):\n         self.attn_gradients = attn_gradients\n@@ -198,22 +193,6 @@ def forward(\n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            seq_length = hidden_states.size()[1]\n-            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n         attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n         if attention_mask is not None:\n             # Apply the attention mask is (precomputed for all layers in BlipTextModel forward() function)"
        },
        {
            "sha": "8e5cda91b65c8b8e02538d10fc5b861c1540679d",
            "filename": "src/transformers/models/blip_2/configuration_blip_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -146,12 +146,6 @@ class Blip2QFormerConfig(PreTrainedConfig):\n             The epsilon used by the layer normalization layers.\n         pad_token_id (`int`, *optional*, defaults to 0):\n             Index to be used for padding token.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         cross_attention_frequency (`int`, *optional*, defaults to 2):\n             The frequency of adding cross-attention to the Transformer layers.\n         encoder_hidden_size (`int`, *optional*, defaults to 1408):\n@@ -190,7 +184,6 @@ def __init__(\n         initializer_range=0.02,\n         layer_norm_eps=1e-12,\n         pad_token_id=0,\n-        position_embedding_type=\"absolute\",\n         cross_attention_frequency=2,\n         encoder_hidden_size=1408,\n         use_qformer_text_input=False,\n@@ -209,7 +202,6 @@ def __init__(\n         self.max_position_embeddings = max_position_embeddings\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n-        self.position_embedding_type = position_embedding_type\n         self.cross_attention_frequency = cross_attention_frequency\n         self.encoder_hidden_size = encoder_hidden_size\n         self.use_qformer_text_input = use_qformer_text_input"
        },
        {
            "sha": "5699e0fb91e509e35dfffdf0b15f77fe522b4db1",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 24,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -546,10 +546,6 @@ def __init__(self, config, is_cross_attention=False):\n             self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n         self.save_attention = False\n \n     def save_attn_gradients(self, attn_gradients):\n@@ -597,22 +593,6 @@ def forward(\n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            seq_length = hidden_states.size()[1]\n-            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n         attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n \n         if attention_mask is not None:\n@@ -866,7 +846,6 @@ def __init__(self, config):\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n \n     def forward(\n         self,\n@@ -885,9 +864,9 @@ def forward(\n         if input_ids is not None:\n             input_ids = input_ids.to(self.word_embeddings.weight.device)\n             embeddings = self.word_embeddings(input_ids)\n-            if self.position_embedding_type == \"absolute\":\n-                position_embeddings = self.position_embeddings(position_ids)\n-                embeddings += position_embeddings\n+\n+            position_embeddings = self.position_embeddings(position_ids)\n+            embeddings += position_embeddings\n \n             if query_embeds is not None:\n                 # `query_embeds` are kept in fp32 when we use it with Qformer"
        },
        {
            "sha": "e53a7a9bc8a56318dc5b517002820f77a42dac72",
            "filename": "src/transformers/models/bridgetower/configuration_bridgetower.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fconfiguration_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fconfiguration_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fconfiguration_bridgetower.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -133,12 +133,6 @@ class BridgeTowerTextConfig(PreTrainedConfig):\n             testing).\n         layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n             The epsilon used by the layer normalization layers.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         is_decoder (`bool`, *optional*, defaults to `False`):\n             Whether the model is used as a decoder or not. If `False`, the model is used as an encoder.\n         use_cache (`bool`, *optional*, defaults to `True`):\n@@ -177,7 +171,6 @@ def __init__(\n         pad_token_id=1,\n         bos_token_id=0,\n         eos_token_id=2,\n-        position_embedding_type=\"absolute\",\n         use_cache=True,\n         **kwargs,\n     ):\n@@ -195,7 +188,6 @@ def __init__(\n         self.max_position_embeddings = max_position_embeddings\n         self.type_vocab_size = type_vocab_size\n         self.layer_norm_eps = layer_norm_eps\n-        self.position_embedding_type = position_embedding_type\n         self.use_cache = use_cache\n         self.pad_token_id = pad_token_id\n         self.bos_token_id = bos_token_id"
        },
        {
            "sha": "30196d35f1ab133fba2175434d1dca2562b23ca0",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 9,
            "deletions": 67,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -415,38 +415,13 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    use_cache: Optional[bool] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(2, 3))\n-\n-    # Relative positional embeddings\n-    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n-        query_length, key_length = query.shape[2], key.shape[2]\n-        if use_cache:\n-            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n-        else:\n-            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n-        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n-        distance = position_ids_l - position_ids_r\n-\n-        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n-        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n-\n-        if module.position_embedding_type == \"relative_key\":\n-            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores\n-        elif module.position_embedding_type == \"relative_key_query\":\n-            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n-\n-    # Scaling is shifted in case of embeddings being relative\n-    attn_weights = attn_weights * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n     if attention_mask is not None and attention_mask.ndim == 4:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n@@ -463,7 +438,7 @@ def eager_attention_forward(\n \n # Copied from transformers.models.roberta.modeling_roberta.RobertaSelfAttention with Roberta->BridgeTower\n class BridgeTowerSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -482,12 +457,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n         self.is_causal = is_causal\n@@ -525,11 +494,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -540,8 +504,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n@@ -550,7 +512,7 @@ def forward(\n \n # Copied from transformers.models.roberta.modeling_roberta.RobertaCrossAttention with Roberta->BridgeTower\n class BridgeTowerCrossAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -569,12 +531,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_causal = is_causal\n         self.layer_idx = layer_idx\n@@ -616,11 +572,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -631,8 +582,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n@@ -641,15 +590,11 @@ def forward(\n \n # Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->BridgeTower,BERT->BRIDGE_TOWER\n class BridgeTowerAttention(nn.Module):\n-    def __init__(\n-        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n-    ):\n+    def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=False):\n         super().__init__()\n         self.is_cross_attention = is_cross_attention\n         attention_class = BridgeTowerCrossAttention if is_cross_attention else BridgeTowerSelfAttention\n-        self.self = attention_class(\n-            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n-        )\n+        self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = BridgeTowerSelfOutput(config)\n         self.pruned_heads = set()\n \n@@ -704,7 +649,6 @@ def __init__(self, config, layer_idx=None):\n         self.add_cross_attention = config.add_cross_attention\n         self.crossattention = BridgeTowerAttention(\n             config,\n-            position_embedding_type=\"absolute\",\n             is_causal=False,\n             layer_idx=layer_idx,\n             is_cross_attention=True,\n@@ -767,7 +711,6 @@ def __init__(self, config, layer_idx=None):\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n             self.crossattention = BridgeTowerAttention(\n                 config,\n-                position_embedding_type=\"absolute\",\n                 is_causal=False,\n                 layer_idx=layer_idx,\n                 is_cross_attention=True,\n@@ -895,7 +838,6 @@ def __init__(self, config):\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n@@ -947,11 +889,11 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings = embeddings + position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings"
        },
        {
            "sha": "ca323c8984cd73739785f7c35a5b2526e338c216",
            "filename": "src/transformers/models/bros/modeling_bros.py",
            "status": "modified",
            "additions": 4,
            "deletions": 26,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -129,7 +129,6 @@ def __init__(self, config):\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n         self.register_buffer(\n             \"token_type_ids\",\n@@ -169,11 +168,11 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings += position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings\n@@ -197,10 +196,6 @@ def __init__(self, config):\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n \n@@ -232,23 +227,6 @@ def forward(\n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            seq_length = hidden_states.size()[1]\n-            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n         # bbox positional encoding\n         batch_size, n_head, seq_length, d_head = query_layer.shape\n         bbox_pos_emb = bbox_pos_emb.view(seq_length, seq_length, batch_size, d_head)"
        },
        {
            "sha": "83940403eebefb7150a083cd1c1e047a70008199",
            "filename": "src/transformers/models/camembert/configuration_camembert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fcamembert%2Fconfiguration_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fcamembert%2Fconfiguration_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fconfiguration_camembert.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -65,12 +65,6 @@ class CamembertConfig(PreTrainedConfig):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n             The epsilon used by the layer normalization layers.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         is_decoder (`bool`, *optional*, defaults to `False`):\n             Whether the model is used as a decoder or not. If `False`, the model is used as an encoder.\n         use_cache (`bool`, *optional*, defaults to `True`):\n@@ -113,7 +107,6 @@ def __init__(\n         pad_token_id=1,\n         bos_token_id=0,\n         eos_token_id=2,\n-        position_embedding_type=\"absolute\",\n         use_cache=True,\n         classifier_dropout=None,\n         **kwargs,\n@@ -132,7 +125,6 @@ def __init__(\n         self.type_vocab_size = type_vocab_size\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n-        self.position_embedding_type = position_embedding_type\n         self.use_cache = use_cache\n         self.classifier_dropout = classifier_dropout\n "
        },
        {
            "sha": "892a194b7d7bd5e187af41fb28210428ebafa5f5",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 9,
            "deletions": 68,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -65,38 +65,13 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    use_cache: Optional[bool] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(2, 3))\n-\n-    # Relative positional embeddings\n-    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n-        query_length, key_length = query.shape[2], key.shape[2]\n-        if use_cache:\n-            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n-        else:\n-            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n-        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n-        distance = position_ids_l - position_ids_r\n-\n-        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n-        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n-\n-        if module.position_embedding_type == \"relative_key\":\n-            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores\n-        elif module.position_embedding_type == \"relative_key_query\":\n-            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n-\n-    # Scaling is shifted in case of embeddings being relative\n-    attn_weights = attn_weights * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n     if attention_mask is not None and attention_mask.ndim == 4:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n@@ -112,7 +87,7 @@ def eager_attention_forward(\n \n \n class CamembertSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -131,12 +106,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n         self.is_causal = is_causal\n@@ -174,11 +143,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -189,16 +153,14 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         return attn_output, attn_weights\n \n \n class CamembertCrossAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -217,12 +179,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_causal = is_causal\n         self.layer_idx = layer_idx\n@@ -264,11 +220,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -279,8 +230,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n@@ -302,15 +251,11 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class CamembertAttention(nn.Module):\n-    def __init__(\n-        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n-    ):\n+    def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=False):\n         super().__init__()\n         self.is_cross_attention = is_cross_attention\n         attention_class = CamembertCrossAttention if is_cross_attention else CamembertSelfAttention\n-        self.self = attention_class(\n-            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n-        )\n+        self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = CamembertSelfOutput(config)\n         self.pruned_heads = set()\n \n@@ -397,7 +342,6 @@ def __init__(self, config, layer_idx=None):\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n             self.crossattention = CamembertAttention(\n                 config,\n-                position_embedding_type=\"absolute\",\n                 is_causal=False,\n                 layer_idx=layer_idx,\n                 is_cross_attention=True,\n@@ -526,7 +470,6 @@ def __init__(self, config):\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n@@ -578,11 +521,11 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings = embeddings + position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings\n@@ -700,8 +643,6 @@ def __init__(self, config, add_pooling_layer=True):\n \n         self.pooler = CamembertPooler(config) if add_pooling_layer else None\n \n-        self.position_embedding_type = config.position_embedding_type\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n "
        },
        {
            "sha": "7479ae7c56815a8e567f63ee6af4f1d2f90048c8",
            "filename": "src/transformers/models/canine/modeling_canine.py",
            "status": "modified",
            "additions": 3,
            "deletions": 25,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -106,7 +106,6 @@ def __init__(self, config):\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n \n     def _hash_bucket_tensors(self, input_ids, num_hashes: int, num_buckets: int):\n         \"\"\"\n@@ -171,12 +170,11 @@ def forward(\n             )\n \n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n \n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.char_position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+        position_embeddings = self.char_position_embeddings(position_ids)\n+        embeddings += position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings\n@@ -300,10 +298,6 @@ def __init__(self, config):\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n     def forward(\n         self,\n@@ -337,22 +331,6 @@ def forward(\n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            seq_length = from_tensor.size()[1]\n-            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=from_tensor.device).view(-1, 1)\n-            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=from_tensor.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n         attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n         if attention_mask is not None:\n             if attention_mask.ndim == 3:"
        },
        {
            "sha": "73c7dd2c659684fdece68025268b38abb9c6e062",
            "filename": "src/transformers/models/chinese_clip/configuration_chinese_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -75,12 +75,6 @@ class ChineseCLIPTextConfig(PreTrainedConfig):\n             The epsilon used by the layer normalization layers.\n         pad_token_id (`int`, *optional*, defaults to 0):\n             Padding token id.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         use_cache (`bool`, *optional*, defaults to `True`):\n             Whether or not the model should return the last key/values attentions (not used by all models). Only\n             relevant if `config.is_decoder=True`.\n@@ -119,7 +113,6 @@ def __init__(\n         initializer_factor=1.0,\n         layer_norm_eps=1e-12,\n         pad_token_id=0,\n-        position_embedding_type=\"absolute\",\n         use_cache=True,\n         **kwargs,\n     ):\n@@ -138,7 +131,6 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.initializer_factor = initializer_factor\n         self.layer_norm_eps = layer_norm_eps\n-        self.position_embedding_type = position_embedding_type\n         self.use_cache = use_cache\n \n "
        },
        {
            "sha": "72b9171d60abf6b796e48576a3c8b876b5ded409",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -101,7 +101,6 @@ def __init__(self, config):\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n@@ -140,11 +139,11 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings += position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings"
        },
        {
            "sha": "69a10f0fd5d956e3c834c665e6c07056e7e1fa49",
            "filename": "src/transformers/models/clap/configuration_clap.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fclap%2Fconfiguration_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fclap%2Fconfiguration_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fconfiguration_clap.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -58,12 +58,6 @@ class ClapTextConfig(PreTrainedConfig):\n             The vocabulary size of the `token_type_ids` passed when calling [`ClapTextModel`].\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n             The epsilon used by the layer normalization layers.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         is_decoder (`bool`, *optional*, defaults to `False`):\n             Whether the model is used as a decoder or not. If `False`, the model is used as an encoder.\n         use_cache (`bool`, *optional*, defaults to `True`):\n@@ -111,7 +105,6 @@ def __init__(\n         pad_token_id=1,\n         bos_token_id=0,\n         eos_token_id=2,\n-        position_embedding_type=\"absolute\",\n         use_cache=True,\n         projection_hidden_act=\"relu\",\n         **kwargs,\n@@ -130,7 +123,6 @@ def __init__(\n         self.type_vocab_size = type_vocab_size\n         self.initializer_factor = initializer_factor\n         self.layer_norm_eps = layer_norm_eps\n-        self.position_embedding_type = position_embedding_type\n         self.use_cache = use_cache\n         self.projection_hidden_act = projection_hidden_act\n         self.projection_dim = projection_dim"
        },
        {
            "sha": "67f4146a99805b69d269b5f7c0f4d3d5caaffb49",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -971,7 +971,6 @@ def __init__(self, config):\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=True\n         )\n@@ -1023,11 +1022,11 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings = embeddings + position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings"
        },
        {
            "sha": "e1f665dcee693e0b10c51b31d592f383b20755b7",
            "filename": "src/transformers/models/data2vec/configuration_data2vec_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_text.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -64,12 +64,6 @@ class Data2VecTextConfig(PreTrainedConfig):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n             The epsilon used by the layer normalization layers.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         is_decoder (`bool`, *optional*, defaults to `False`):\n             Whether the model is used as a decoder or not. If `False`, the model is used as an encoder.\n         use_cache (`bool`, *optional*, defaults to `True`):\n@@ -112,7 +106,6 @@ def __init__(\n         pad_token_id=1,\n         bos_token_id=0,\n         eos_token_id=2,\n-        position_embedding_type=\"absolute\",\n         use_cache=True,\n         classifier_dropout=None,\n         **kwargs,\n@@ -131,7 +124,6 @@ def __init__(\n         self.type_vocab_size = type_vocab_size\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n-        self.position_embedding_type = position_embedding_type\n         self.use_cache = use_cache\n         self.classifier_dropout = classifier_dropout\n "
        },
        {
            "sha": "fe8dca2460b520375281ebf5a48386f2b6d16151",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 9,
            "deletions": 68,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -67,7 +67,6 @@ def __init__(self, config):\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n@@ -119,11 +118,11 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings = embeddings + position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings\n@@ -171,38 +170,13 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    use_cache: Optional[bool] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(2, 3))\n-\n-    # Relative positional embeddings\n-    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n-        query_length, key_length = query.shape[2], key.shape[2]\n-        if use_cache:\n-            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n-        else:\n-            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n-        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n-        distance = position_ids_l - position_ids_r\n-\n-        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n-        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n-\n-        if module.position_embedding_type == \"relative_key\":\n-            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores\n-        elif module.position_embedding_type == \"relative_key_query\":\n-            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n-\n-    # Scaling is shifted in case of embeddings being relative\n-    attn_weights = attn_weights * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n     if attention_mask is not None and attention_mask.ndim == 4:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n@@ -218,7 +192,7 @@ def eager_attention_forward(\n \n \n class Data2VecTextSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -237,12 +211,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n         self.is_causal = is_causal\n@@ -280,11 +248,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -295,16 +258,14 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         return attn_output, attn_weights\n \n \n class Data2VecTextCrossAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -323,12 +284,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_causal = is_causal\n         self.layer_idx = layer_idx\n@@ -370,11 +325,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -385,8 +335,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n@@ -408,15 +356,11 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class Data2VecTextAttention(nn.Module):\n-    def __init__(\n-        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n-    ):\n+    def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=False):\n         super().__init__()\n         self.is_cross_attention = is_cross_attention\n         attention_class = Data2VecTextCrossAttention if is_cross_attention else Data2VecTextSelfAttention\n-        self.self = attention_class(\n-            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n-        )\n+        self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = Data2VecTextSelfOutput(config)\n         self.pruned_heads = set()\n \n@@ -503,7 +447,6 @@ def __init__(self, config, layer_idx=None):\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n             self.crossattention = Data2VecTextAttention(\n                 config,\n-                position_embedding_type=\"absolute\",\n                 is_causal=False,\n                 layer_idx=layer_idx,\n                 is_cross_attention=True,\n@@ -660,8 +603,6 @@ def __init__(self, config, add_pooling_layer=True):\n \n         self.pooler = Data2VecTextPooler(config) if add_pooling_layer else None\n \n-        self.position_embedding_type = config.position_embedding_type\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n "
        },
        {
            "sha": "2b5660af48b143708b2af39173c4b252c92215b4",
            "filename": "src/transformers/models/dpr/configuration_dpr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fdpr%2Fconfiguration_dpr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fdpr%2Fconfiguration_dpr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2Fconfiguration_dpr.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -64,12 +64,6 @@ class DPRConfig(PreTrainedConfig):\n             The epsilon used by the layer normalization layers.\n         pad_token_id (`int`, *optional*, defaults to 0):\n             Padding token id.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         projection_dim (`int`, *optional*, defaults to 0):\n             Dimension of the projection for the context and question encoders. If it is set to zero (default), then no\n             projection is done.\n@@ -106,7 +100,6 @@ def __init__(\n         initializer_range=0.02,\n         layer_norm_eps=1e-12,\n         pad_token_id=0,\n-        position_embedding_type=\"absolute\",\n         projection_dim: int = 0,\n         **kwargs,\n     ):\n@@ -125,7 +118,6 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n         self.projection_dim = projection_dim\n-        self.position_embedding_type = position_embedding_type\n \n \n __all__ = [\"DPRConfig\"]"
        },
        {
            "sha": "96f5a86ba3d6e7efba77ce2e4a09e27a9e02ff47",
            "filename": "src/transformers/models/electra/configuration_electra.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Felectra%2Fconfiguration_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Felectra%2Fconfiguration_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fconfiguration_electra.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -89,12 +89,6 @@ class ElectraConfig(PreTrainedConfig):\n             Argument used when doing sequence summary. Used in the sequence classification and multiple choice models.\n \n             The dropout ratio to be used after the projection and activation.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         use_cache (`bool`, *optional*, defaults to `True`):\n             Whether or not the model should return the last key/values attentions (not used by all models). Only\n             relevant if `config.is_decoder=True`.\n@@ -138,7 +132,6 @@ def __init__(\n         summary_activation=\"gelu\",\n         summary_last_dropout=0.1,\n         pad_token_id=0,\n-        position_embedding_type=\"absolute\",\n         use_cache=True,\n         classifier_dropout=None,\n         **kwargs,\n@@ -163,7 +156,6 @@ def __init__(\n         self.summary_use_proj = summary_use_proj\n         self.summary_activation = summary_activation\n         self.summary_last_dropout = summary_last_dropout\n-        self.position_embedding_type = position_embedding_type\n         self.use_cache = use_cache\n         self.classifier_dropout = classifier_dropout\n "
        },
        {
            "sha": "4928b6d39a79b613abd71549196ac3e4776f92fb",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 9,
            "deletions": 66,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -74,7 +74,6 @@ def __init__(self, config):\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False\n         )\n@@ -113,11 +112,11 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings = embeddings + position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings\n@@ -132,38 +131,13 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    use_cache: Optional[bool] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(2, 3))\n-\n-    # Relative positional embeddings\n-    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n-        query_length, key_length = query.shape[2], key.shape[2]\n-        if use_cache:\n-            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n-        else:\n-            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n-        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n-        distance = position_ids_l - position_ids_r\n-\n-        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n-        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n-\n-        if module.position_embedding_type == \"relative_key\":\n-            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores\n-        elif module.position_embedding_type == \"relative_key_query\":\n-            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n-\n-    # Scaling is shifted in case of embeddings being relative\n-    attn_weights = attn_weights * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n     if attention_mask is not None and attention_mask.ndim == 4:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n@@ -180,7 +154,7 @@ def eager_attention_forward(\n \n # Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->Electra\n class ElectraSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -199,12 +173,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n         self.is_causal = is_causal\n@@ -242,11 +210,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -257,8 +220,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n@@ -267,7 +228,7 @@ def forward(\n \n # Copied from transformers.models.bert.modeling_bert.BertCrossAttention with Bert->Electra\n class ElectraCrossAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -286,12 +247,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_causal = is_causal\n         self.layer_idx = layer_idx\n@@ -333,11 +288,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -348,8 +298,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n@@ -373,15 +321,11 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n # Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->Electra,BERT->ELECTRA\n class ElectraAttention(nn.Module):\n-    def __init__(\n-        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n-    ):\n+    def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=False):\n         super().__init__()\n         self.is_cross_attention = is_cross_attention\n         attention_class = ElectraCrossAttention if is_cross_attention else ElectraSelfAttention\n-        self.self = attention_class(\n-            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n-        )\n+        self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = ElectraSelfOutput(config)\n         self.pruned_heads = set()\n \n@@ -471,7 +415,6 @@ def __init__(self, config, layer_idx=None):\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n             self.crossattention = ElectraAttention(\n                 config,\n-                position_embedding_type=\"absolute\",\n                 is_causal=False,\n                 layer_idx=layer_idx,\n                 is_cross_attention=True,"
        },
        {
            "sha": "36f50c0a76ecf5149568de5c37cd4ccfaa864b77",
            "filename": "src/transformers/models/ernie/configuration_ernie.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fernie%2Fconfiguration_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fernie%2Fconfiguration_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fconfiguration_ernie.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -71,12 +71,6 @@ class ErnieConfig(PreTrainedConfig):\n             The epsilon used by the layer normalization layers.\n         pad_token_id (`int`, *optional*, defaults to 0):\n             Padding token id.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         use_cache (`bool`, *optional*, defaults to `True`):\n             Whether or not the model should return the last key/values attentions (not used by all models). Only\n             relevant if `config.is_decoder=True`.\n@@ -117,7 +111,6 @@ def __init__(\n         initializer_range=0.02,\n         layer_norm_eps=1e-12,\n         pad_token_id=0,\n-        position_embedding_type=\"absolute\",\n         use_cache=True,\n         classifier_dropout=None,\n         **kwargs,\n@@ -138,7 +131,6 @@ def __init__(\n         self.use_task_id = use_task_id\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n-        self.position_embedding_type = position_embedding_type\n         self.use_cache = use_cache\n         self.classifier_dropout = classifier_dropout\n "
        },
        {
            "sha": "b40ba3c8278830055da75f32b269c6e4bb5ab182",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 8,
            "deletions": 68,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -71,7 +71,6 @@ def __init__(self, config):\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n@@ -117,11 +116,10 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings = embeddings + position_embeddings\n \n         # add `task_type_id` for ERNIE model\n         if self.use_task_id:\n@@ -143,38 +141,13 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    use_cache: Optional[bool] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(2, 3))\n-\n-    # Relative positional embeddings\n-    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n-        query_length, key_length = query.shape[2], key.shape[2]\n-        if use_cache:\n-            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n-        else:\n-            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n-        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n-        distance = position_ids_l - position_ids_r\n-\n-        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n-        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n-\n-        if module.position_embedding_type == \"relative_key\":\n-            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores\n-        elif module.position_embedding_type == \"relative_key_query\":\n-            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n-\n-    # Scaling is shifted in case of embeddings being relative\n-    attn_weights = attn_weights * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n     if attention_mask is not None and attention_mask.ndim == 4:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n@@ -190,7 +163,7 @@ def eager_attention_forward(\n \n \n class ErnieSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -209,12 +182,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n         self.is_causal = is_causal\n@@ -252,11 +219,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -267,16 +229,14 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         return attn_output, attn_weights\n \n \n class ErnieCrossAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -295,12 +255,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_causal = is_causal\n         self.layer_idx = layer_idx\n@@ -342,11 +296,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -357,8 +306,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n@@ -380,15 +327,11 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class ErnieAttention(nn.Module):\n-    def __init__(\n-        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n-    ):\n+    def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=False):\n         super().__init__()\n         self.is_cross_attention = is_cross_attention\n         attention_class = ErnieCrossAttention if is_cross_attention else ErnieSelfAttention\n-        self.self = attention_class(\n-            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n-        )\n+        self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = ErnieSelfOutput(config)\n         self.pruned_heads = set()\n \n@@ -475,7 +418,6 @@ def __init__(self, config, layer_idx=None):\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n             self.crossattention = ErnieAttention(\n                 config,\n-                position_embedding_type=\"absolute\",\n                 is_causal=False,\n                 layer_idx=layer_idx,\n                 is_cross_attention=True,\n@@ -682,8 +624,6 @@ def __init__(self, config, add_pooling_layer=True):\n \n         self.pooler = ErniePooler(config) if add_pooling_layer else None\n \n-        self.position_embedding_type = config.position_embedding_type\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n "
        },
        {
            "sha": "02a61d95e2f7fd8c39e8ddb3e5ac95726c276507",
            "filename": "src/transformers/models/ernie/modular_ernie.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -111,11 +111,10 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings = embeddings + position_embeddings\n \n         # add `task_type_id` for ERNIE model\n         if self.use_task_id:"
        },
        {
            "sha": "45e7383579f544d49df8301f1209d2e0ffa80290",
            "filename": "src/transformers/models/esm/configuration_esm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fesm%2Fconfiguration_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fesm%2Fconfiguration_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fconfiguration_esm.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -67,11 +67,7 @@ class EsmConfig(PreTrainedConfig):\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n             The epsilon used by the layer normalization layers.\n         position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\", \"rotary\"`.\n-            For positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n+            Type of position embedding. Choose either `\"absolute\"` or \"rotary\"`.\n         is_decoder (`bool`, *optional*, defaults to `False`):\n             Whether the model is used as a decoder or not. If `False`, the model is used as an encoder.\n         use_cache (`bool`, *optional*, defaults to `True`):"
        },
        {
            "sha": "60d5bd4dc3b51bff76fbe96970431c8ba7071269",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 86,
            "deletions": 57,
            "changes": 143,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithCrossAttentions,\n@@ -33,11 +34,15 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_esm import EsmConfig\n \n \n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -251,44 +256,28 @@ def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n         return position_ids.unsqueeze(0).expand(input_shape)\n \n \n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n-    # ESM applies relative position embeddings and we don't copy from Llama\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    if hasattr(module, \"position_embedding_type\") and module.position_embedding_type in [\n-        \"relative_key\",\n-        \"relative_key_query\",\n-    ]:\n-        seq_length = query.shape[2]\n-        position_ids_l = torch.arange(seq_length, dtype=torch.long, device=attn_weights.device).view(-1, 1)\n-        position_ids_r = torch.arange(seq_length, dtype=torch.long, device=attn_weights.device).view(1, -1)\n-        distance = position_ids_l - position_ids_r\n-        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n-        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n-\n-        if module.position_embedding_type == \"relative_key\":\n-            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-        elif module.position_embedding_type == \"relative_key_query\":\n-            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n-            relative_position_scores = relative_position_scores_query + relative_position_scores_key\n-\n-        attn_weights = attn_weights + relative_position_scores\n-\n-    if attention_mask is not None:\n-        causal_mask = attention_mask[:, :, :, : key.shape[-2]]\n-        attn_weights = attn_weights + causal_mask\n-\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None and attention_mask.ndim == 4:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)\n@@ -317,14 +306,12 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None, is_cros\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = config.attention_probs_dropout_prob\n+\n+        self.rotary_embeddings = None\n         self.position_embedding_type = position_embedding_type or getattr(\n             config, \"position_embedding_type\", \"absolute\"\n         )\n-        self.rotary_embeddings = None\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n-        elif self.position_embedding_type == \"rotary\":\n+        if self.position_embedding_type == \"rotary\":\n             self.rotary_embeddings = RotaryEmbedding(dim=self.attention_head_size)\n \n         self.scaling = 1.0  # For BC we apply scaling before RoPE\n@@ -362,11 +349,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type in [\"relative_key\", \"relative_key_query\"]:\n-                raise ValueError(\n-                    f\"ESM {self.config._attn_implementation} attention does not support {self.position_embedding_type} embeddings. \"\n-                    \"Set attention explicitly to 'eager' with `model.set_attn_implementation('eager')`\"\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -701,31 +683,23 @@ def forward(\n                 position_ids=position_ids,\n             )\n \n-        if self.config._attn_implementation != \"flash_attention_2\":\n-            batch_size, seq_length = inputs_embeds.shape[:-1]\n-            if attention_mask is None:\n-                attention_mask = torch.ones(((batch_size, seq_length)), device=inputs_embeds.device)\n-\n-            attention_mask: torch.Tensor = self.get_extended_attention_mask(\n-                attention_mask, input_shape=(batch_size, seq_length)\n-            )\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            inputs_embeds,\n+        )\n \n-        # If a 2D or 3D attention mask is provided for the cross-attention\n-        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n-        if self.config.is_decoder and encoder_hidden_states is not None:\n-            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n-            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n-            if encoder_attention_mask is None:\n-                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=inputs_embeds.device)\n-            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n-        else:\n-            encoder_extended_attention_mask = None\n+        encoder_attention_mask = self._update_cross_attn_mask(\n+            encoder_hidden_states,\n+            encoder_attention_mask,\n+            inputs_embeds.shape[:2],\n+            inputs_embeds,\n+        )\n \n         encoder_outputs = self.encoder(\n             inputs_embeds,\n             attention_mask=attention_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_extended_attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n             **kwargs,\n         )\n         sequence_output = encoder_outputs[0]\n@@ -736,6 +710,61 @@ def forward(\n             pooler_output=pooled_output,\n         )\n \n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n     def predict_contacts(self, tokens, attention_mask):\n         attns = self(tokens, attention_mask=attention_mask, return_dict=True, output_attentions=True).attentions\n         attns = torch.stack(attns, dim=1)  # Matches the original model layout"
        },
        {
            "sha": "604f3119327cd6608d245fb2f6cc5b06d89aefd3",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 49,
            "deletions": 98,
            "changes": 147,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -20,18 +20,18 @@\n # limitations under the License.\n \n import math\n-import warnings\n from dataclasses import dataclass\n from typing import Callable, Optional, Union\n \n import torch\n-from torch import Tensor, nn\n+from torch import nn\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithCrossAttentions,\n@@ -41,15 +41,19 @@\n     ModelOutput,\n )\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n-from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, ModuleUtilsMixin, PreTrainedModel, get_parameter_dtype\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available\n from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_evolla import EvollaConfig, SaProtConfig\n \n \n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n def create_position_ids_from_input_ids(input_ids, padding_idx):\n     \"\"\"\n     Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n@@ -225,38 +229,21 @@ def eager_attention_forward(\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n-    # EVOLLA_SA_PROT applies relative position embeddings and we don't copy from Llama\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    if hasattr(module, \"position_embedding_type\") and module.position_embedding_type in [\n-        \"relative_key\",\n-        \"relative_key_query\",\n-    ]:\n-        seq_length = query.shape[2]\n-        position_ids_l = torch.arange(seq_length, dtype=torch.long, device=attn_weights.device).view(-1, 1)\n-        position_ids_r = torch.arange(seq_length, dtype=torch.long, device=attn_weights.device).view(1, -1)\n-        distance = position_ids_l - position_ids_r\n-        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n-        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n-\n-        if module.position_embedding_type == \"relative_key\":\n-            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-        elif module.position_embedding_type == \"relative_key_query\":\n-            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n-            relative_position_scores = relative_position_scores_query + relative_position_scores_key\n-\n-        attn_weights = attn_weights + relative_position_scores\n-\n-    if attention_mask is not None:\n-        causal_mask = attention_mask[:, :, :, : key.shape[-2]]\n-        attn_weights = attn_weights + causal_mask\n-\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None and attention_mask.ndim == 4:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)\n@@ -285,14 +272,12 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None, is_cros\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = config.attention_probs_dropout_prob\n+\n+        self.rotary_embeddings = None\n         self.position_embedding_type = position_embedding_type or getattr(\n             config, \"position_embedding_type\", \"absolute\"\n         )\n-        self.rotary_embeddings = None\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n-        elif self.position_embedding_type == \"rotary\":\n+        if self.position_embedding_type == \"rotary\":\n             self.rotary_embeddings = EvollaSaProtRotaryEmbedding(dim=self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n@@ -330,11 +315,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type in [\"relative_key\", \"relative_key_query\"]:\n-                raise ValueError(\n-                    f\"ESM {self.config._attn_implementation} attention does not support {self.position_embedding_type} embeddings. \"\n-                    \"Set attention explicitly to 'eager' with `model.set_attn_implementation('eager')`\"\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -550,6 +530,7 @@ class EvollaSaProtPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"EvollaSaProtLayer\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_attention_backend = True\n \n     _can_record_outputs = {\n@@ -601,17 +582,22 @@ def forward(\n         self,\n         input_ids: Optional[torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         input_shape = input_ids.size()\n         batch_size, seq_length = input_shape\n \n         device = input_ids.device\n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length)), device=device)\n-\n         inputs_embeds = self.embeddings(input_ids=input_ids, attention_mask=attention_mask)\n-        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n-        encoder_outputs = self.encoder(inputs_embeds, attention_mask=extended_attention_mask)\n+\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            inputs_embeds,\n+        )\n+\n+        encoder_outputs = self.encoder(inputs_embeds, attention_mask=attention_mask, **kwargs)\n         sequence_output = encoder_outputs[0]\n \n         return BaseModelOutputWithPoolingAndCrossAttentions(\n@@ -621,61 +607,26 @@ def forward(\n             cross_attentions=encoder_outputs.cross_attentions,\n         )\n \n-    def get_extended_attention_mask(\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n         self,\n-        attention_mask: Tensor,\n-        input_shape: tuple[int],\n-        device: Optional[torch.device] = None,\n-        dtype: Optional[torch.dtype] = None,\n-    ) -> Tensor:\n-        \"\"\"\n-        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n-\n-        Arguments:\n-            attention_mask (`torch.Tensor`):\n-                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n-            input_shape (`Tuple[int]`):\n-                The shape of the input to the model.\n-\n-        Returns:\n-            `torch.Tensor` The extended attention mask, with a the same dtype as `attention_mask.dtype`.\n-        \"\"\"\n-        if dtype is None:\n-            dtype = get_parameter_dtype(self)\n-\n-        if not (attention_mask.dim() == 2 and self.config.is_decoder):\n-            # show warning only if it won't be shown in `create_extended_attention_mask_for_decoder`\n-            if device is not None:\n-                warnings.warn(\n-                    \"The `device` argument is deprecated and will be removed in v5 of Transformers.\", FutureWarning\n-                )\n-        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-        # ourselves in which case we just need to make it broadcastable to all heads.\n-        if attention_mask.dim() == 3:\n-            extended_attention_mask = attention_mask[:, None, :, :]\n-        elif attention_mask.dim() == 2:\n-            # Provided a padding mask of dimensions [batch_size, seq_length]\n-            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n-            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n-            if self.config.is_decoder:\n-                extended_attention_mask = ModuleUtilsMixin.create_extended_attention_mask_for_decoder(\n-                    input_shape, attention_mask, device\n-                )\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n             else:\n-                extended_attention_mask = attention_mask[:, None, None, :]\n-        else:\n-            raise ValueError(\n-                f\"Wrong shape for input_ids (shape {input_shape}) or attention_mask (shape {attention_mask.shape})\"\n-            )\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n \n-        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n-        # masked positions, this operation will create a tensor which is 0.0 for\n-        # positions we want to attend and the dtype's smallest value for masked positions.\n-        # Since we are adding it to the raw scores before the softmax, this is\n-        # effectively the same as removing these entirely.\n-        extended_attention_mask = extended_attention_mask.to(dtype=dtype)  # fp16 compatibility\n-        extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(dtype).min\n-        return extended_attention_mask\n+        return attention_mask\n \n \n class EvollaSequenceCompressorAttention(nn.Module):\n@@ -1332,9 +1283,9 @@ class EvollaPreTrainedModel(PreTrainedModel):\n         \"EvollaSequenceAlignerCrossAttention\",\n     ]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn = False  # see dependency on `EvollaSaProtProteinEncoder`\n+    _supports_flash_attn = False  # see dependency on `EvollaSequenceCompressorResampler`\n     _supports_sdpa = True\n-    _supports_flex_attn = False  # see dependency on `EvollaSaProtProteinEncoder`\n+    _supports_flex_attn = False  # see dependency on `EvollaSequenceCompressorResampler`\n \n     _can_compile_fullgraph = True\n     _supports_attention_backend = False"
        },
        {
            "sha": "69e2cb47a8441b4692fb6cf33f207dc183b0a77c",
            "filename": "src/transformers/models/evolla/modular_evolla.py",
            "status": "modified",
            "additions": 39,
            "deletions": 65,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -13,26 +13,27 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import warnings\n from dataclasses import dataclass\n from typing import Optional, Union\n \n import torch\n-from torch import Tensor, nn\n+from torch import nn\n \n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n     CausalLMOutputWithPast,\n     ModelOutput,\n )\n-from ...modeling_utils import ModuleUtilsMixin, PreTrainedModel, get_parameter_dtype\n+from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     auto_docstring,\n     can_return_tuple,\n+    is_torch_flex_attn_available,\n     logging,\n )\n from ...utils.deprecation import deprecate_kwarg\n@@ -59,6 +60,10 @@\n from .configuration_evolla import EvollaConfig, SaProtConfig\n \n \n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -145,14 +150,12 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None, is_cros\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = config.attention_probs_dropout_prob\n+\n+        self.rotary_embeddings = None\n         self.position_embedding_type = position_embedding_type or getattr(\n             config, \"position_embedding_type\", \"absolute\"\n         )\n-        self.rotary_embeddings = None\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n-        elif self.position_embedding_type == \"rotary\":\n+        if self.position_embedding_type == \"rotary\":\n             self.rotary_embeddings = EvollaSaProtRotaryEmbedding(dim=self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n@@ -195,6 +198,7 @@ class EvollaSaProtPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"EvollaSaProtLayer\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_attention_backend = True\n \n     _can_record_outputs = {\n@@ -246,17 +250,22 @@ def forward(\n         self,\n         input_ids: Optional[torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         input_shape = input_ids.size()\n         batch_size, seq_length = input_shape\n \n         device = input_ids.device\n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length)), device=device)\n-\n         inputs_embeds = self.embeddings(input_ids=input_ids, attention_mask=attention_mask)\n-        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n-        encoder_outputs = self.encoder(inputs_embeds, attention_mask=extended_attention_mask)\n+\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            inputs_embeds,\n+        )\n+\n+        encoder_outputs = self.encoder(inputs_embeds, attention_mask=attention_mask, **kwargs)\n         sequence_output = encoder_outputs[0]\n \n         return BaseModelOutputWithPoolingAndCrossAttentions(\n@@ -266,61 +275,26 @@ def forward(\n             cross_attentions=encoder_outputs.cross_attentions,\n         )\n \n-    def get_extended_attention_mask(\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n         self,\n-        attention_mask: Tensor,\n-        input_shape: tuple[int],\n-        device: Optional[torch.device] = None,\n-        dtype: Optional[torch.dtype] = None,\n-    ) -> Tensor:\n-        \"\"\"\n-        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n-\n-        Arguments:\n-            attention_mask (`torch.Tensor`):\n-                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n-            input_shape (`Tuple[int]`):\n-                The shape of the input to the model.\n-\n-        Returns:\n-            `torch.Tensor` The extended attention mask, with a the same dtype as `attention_mask.dtype`.\n-        \"\"\"\n-        if dtype is None:\n-            dtype = get_parameter_dtype(self)\n-\n-        if not (attention_mask.dim() == 2 and self.config.is_decoder):\n-            # show warning only if it won't be shown in `create_extended_attention_mask_for_decoder`\n-            if device is not None:\n-                warnings.warn(\n-                    \"The `device` argument is deprecated and will be removed in v5 of Transformers.\", FutureWarning\n-                )\n-        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-        # ourselves in which case we just need to make it broadcastable to all heads.\n-        if attention_mask.dim() == 3:\n-            extended_attention_mask = attention_mask[:, None, :, :]\n-        elif attention_mask.dim() == 2:\n-            # Provided a padding mask of dimensions [batch_size, seq_length]\n-            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n-            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n-            if self.config.is_decoder:\n-                extended_attention_mask = ModuleUtilsMixin.create_extended_attention_mask_for_decoder(\n-                    input_shape, attention_mask, device\n-                )\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n             else:\n-                extended_attention_mask = attention_mask[:, None, None, :]\n-        else:\n-            raise ValueError(\n-                f\"Wrong shape for input_ids (shape {input_shape}) or attention_mask (shape {attention_mask.shape})\"\n-            )\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n \n-        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n-        # masked positions, this operation will create a tensor which is 0.0 for\n-        # positions we want to attend and the dtype's smallest value for masked positions.\n-        # Since we are adding it to the raw scores before the softmax, this is\n-        # effectively the same as removing these entirely.\n-        extended_attention_mask = extended_attention_mask.to(dtype=dtype)  # fp16 compatibility\n-        extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(dtype).min\n-        return extended_attention_mask\n+        return attention_mask\n \n \n class EvollaSequenceCompressorAttention(nn.Module):\n@@ -786,8 +760,8 @@ def forward(\n \n \n class EvollaPreTrainedModel(LlamaPreTrainedModel):\n-    _supports_flash_attn = False  # see dependency on `EvollaSaProtProteinEncoder`\n-    _supports_flex_attn = False  # see dependency on `EvollaSaProtProteinEncoder`\n+    _supports_flash_attn = False  # see dependency on `EvollaSequenceCompressorResampler`\n+    _supports_flex_attn = False  # see dependency on `EvollaSequenceCompressorResampler`\n     _supports_attention_backend = False\n     _no_split_modules = [\n         \"EvollaDecoderLayer\","
        },
        {
            "sha": "2997d11bd4d4b2b137d4a89c5bd1b7f35cd44298",
            "filename": "src/transformers/models/flava/configuration_flava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fflava%2Fconfiguration_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fflava%2Fconfiguration_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fconfiguration_flava.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -148,12 +148,6 @@ class FlavaTextConfig(PreTrainedConfig):\n         max_position_embeddings (`int`, *optional*, defaults to 512):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 2048). For VL, max_length passed to model is 77.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         hidden_size (`int`, *optional*, defaults to 768):\n             Dimensionality of the encoder layers and the pooler layer.\n         num_hidden_layers (`int`, *optional*, defaults to 12):\n@@ -205,7 +199,6 @@ def __init__(\n         vocab_size: int = 30522,\n         type_vocab_size: int = 2,\n         max_position_embeddings: int = 512,\n-        position_embedding_type: str = \"absolute\",\n         hidden_size: int = 768,\n         num_hidden_layers: int = 12,\n         num_attention_heads: int = 12,\n@@ -224,7 +217,6 @@ def __init__(\n         self.vocab_size = vocab_size\n         self.type_vocab_size = type_vocab_size\n         self.max_position_embeddings = max_position_embeddings\n-        self.position_embedding_type = position_embedding_type\n         self.hidden_size = hidden_size\n         self.num_hidden_layers = num_hidden_layers\n         self.num_attention_heads = num_attention_heads"
        },
        {
            "sha": "2ef41bde4e470e3b2994d67501fdfd5a77f3e622",
            "filename": "src/transformers/models/flava/modeling_flava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -378,7 +378,6 @@ def __init__(self, config):\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n@@ -411,11 +410,11 @@ def forward(\n \n         inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings += position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings"
        },
        {
            "sha": "e8277d773bc136383eb97b866c52baf706ff3aa6",
            "filename": "src/transformers/models/git/configuration_git.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fgit%2Fconfiguration_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fgit%2Fconfiguration_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fconfiguration_git.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -140,12 +140,6 @@ class GitConfig(PreTrainedConfig):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n             The epsilon used by the layer normalization layers.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         use_cache (`bool`, *optional*, defaults to `True`):\n             Whether or not the model should return the last key/values attentions (not used by all models).\n         num_image_with_embedding (`int`, *optional*):\n@@ -184,7 +178,6 @@ def __init__(\n         initializer_range=0.02,\n         layer_norm_eps=1e-12,\n         pad_token_id=0,\n-        position_embedding_type=\"absolute\",\n         use_cache=True,\n         tie_word_embeddings=False,\n         bos_token_id=101,\n@@ -210,7 +203,6 @@ def __init__(\n         self.max_position_embeddings = max_position_embeddings\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n-        self.position_embedding_type = position_embedding_type\n         self.use_cache = use_cache\n         self.tie_word_embeddings = tie_word_embeddings\n         self.num_image_with_embedding = num_image_with_embedding"
        },
        {
            "sha": "5528bc1addffe6c1794f0728fb38735dab72cd4a",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 6,
            "deletions": 37,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -79,7 +79,6 @@ def __init__(self, config):\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n@@ -106,16 +105,16 @@ def forward(\n         else:\n             embeddings = inputs_embeds\n \n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings += position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings\n \n \n class GitSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -142,12 +141,6 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n@@ -187,28 +180,6 @@ def forward(\n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_values is not None:\n-                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n-                    -1, 1\n-                )\n-            else:\n-                position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n         attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n         if attention_mask is not None:\n             # Apply the attention mask is (precomputed for all layers in GitModel forward() function)\n@@ -251,11 +222,9 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class GitAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n-        self.self = GIT_SELF_ATTENTION_CLASSES[config._attn_implementation](\n-            config, position_embedding_type=position_embedding_type, layer_idx=layer_idx\n-        )\n+        self.self = GIT_SELF_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx=layer_idx)\n         self.output = GitSelfOutput(config)\n         self.pruned_heads = set()\n "
        },
        {
            "sha": "4bf1e04b95ddb0be9fb08d8062a049bcd9b66aef",
            "filename": "src/transformers/models/ibert/configuration_ibert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fibert%2Fconfiguration_ibert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fibert%2Fconfiguration_ibert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fibert%2Fconfiguration_ibert.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -65,12 +65,6 @@ class IBertConfig(PreTrainedConfig):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n             The epsilon used by the layer normalization layers.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         quant_mode (`bool`, *optional*, defaults to `False`):\n             Whether to quantize the model or not.\n         force_dequant (`str`, *optional*, defaults to `\"none\"`):\n@@ -100,7 +94,6 @@ def __init__(\n         pad_token_id=1,\n         bos_token_id=0,\n         eos_token_id=2,\n-        position_embedding_type=\"absolute\",\n         quant_mode=False,\n         force_dequant=\"none\",\n         **kwargs,\n@@ -119,7 +112,6 @@ def __init__(\n         self.type_vocab_size = type_vocab_size\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n-        self.position_embedding_type = position_embedding_type\n         self.quant_mode = quant_mode\n         self.force_dequant = force_dequant\n "
        },
        {
            "sha": "84f5576261453d57b5e716dd06aaac6a0938a4ba",
            "filename": "src/transformers/models/ibert/modeling_ibert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 12,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -73,7 +73,6 @@ def __init__(self, config):\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n \n         # End copy\n         self.padding_idx = config.pad_token_id\n@@ -132,14 +131,13 @@ def forward(\n             identity_scaling_factor=token_type_embeddings_scaling_factor,\n         )\n \n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings, position_embeddings_scaling_factor = self.position_embeddings(position_ids)\n-            embeddings, embeddings_scaling_factor = self.embeddings_act1(\n-                embeddings,\n-                embeddings_scaling_factor,\n-                identity=position_embeddings,\n-                identity_scaling_factor=position_embeddings_scaling_factor,\n-            )\n+        position_embeddings, position_embeddings_scaling_factor = self.position_embeddings(position_ids)\n+        embeddings, embeddings_scaling_factor = self.embeddings_act1(\n+            embeddings,\n+            embeddings_scaling_factor,\n+            identity=position_embeddings,\n+            identity_scaling_factor=position_embeddings_scaling_factor,\n+        )\n \n         embeddings, embeddings_scaling_factor = self.LayerNorm(embeddings, embeddings_scaling_factor)\n         embeddings = self.dropout(embeddings)\n@@ -217,9 +215,6 @@ def __init__(self, config):\n         self.output_activation = QuantAct(self.act_bit, quant_mode=self.quant_mode)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n-        if self.position_embedding_type != \"absolute\":\n-            raise ValueError(\"I-BERT only supports 'absolute' for `config.position_embedding_type`\")\n \n         self.softmax = IntSoftmax(self.act_bit, quant_mode=self.quant_mode, force_dequant=config.force_dequant)\n "
        },
        {
            "sha": "e912c3d3d83c843cb94525f504cc9780d83fa677",
            "filename": "src/transformers/models/instructblip/configuration_instructblip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Finstructblip%2Fconfiguration_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Finstructblip%2Fconfiguration_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fconfiguration_instructblip.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -146,12 +146,6 @@ class InstructBlipQFormerConfig(PreTrainedConfig):\n             The epsilon used by the layer normalization layers.\n         pad_token_id (`int`, *optional*, defaults to 0):\n             Token id used for padding sequences.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         cross_attention_frequency (`int`, *optional*, defaults to 2):\n             The frequency of adding cross-attention to the Transformer layers.\n         encoder_hidden_size (`int`, *optional*, defaults to 1408):\n@@ -188,7 +182,6 @@ def __init__(\n         initializer_range=0.02,\n         layer_norm_eps=1e-12,\n         pad_token_id=0,\n-        position_embedding_type=\"absolute\",\n         cross_attention_frequency=2,\n         encoder_hidden_size=1408,\n         **kwargs,\n@@ -206,7 +199,6 @@ def __init__(\n         self.max_position_embeddings = max_position_embeddings\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n-        self.position_embedding_type = position_embedding_type\n         self.cross_attention_frequency = cross_attention_frequency\n         self.encoder_hidden_size = encoder_hidden_size\n "
        },
        {
            "sha": "4fd6692e6be14f62ffa91841903fcb99d39c0cf4",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 24,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -451,10 +451,6 @@ def __init__(self, config, is_cross_attention=False):\n             self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n         self.save_attention = False\n \n     def save_attn_gradients(self, attn_gradients):\n@@ -502,22 +498,6 @@ def forward(\n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            seq_length = hidden_states.size()[1]\n-            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n         attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n         attention_scores_dtype = attention_scores.dtype\n \n@@ -773,7 +753,6 @@ def __init__(self, config):\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n \n         self.config = config\n \n@@ -794,9 +773,9 @@ def forward(\n \n         if input_ids is not None:\n             embeddings = self.word_embeddings(input_ids)\n-            if self.position_embedding_type == \"absolute\":\n-                position_embeddings = self.position_embeddings(position_ids.to(embeddings.device))\n-                embeddings = embeddings + position_embeddings\n+\n+            position_embeddings = self.position_embeddings(position_ids.to(embeddings.device))\n+            embeddings = embeddings + position_embeddings\n \n             if query_embeds is not None:\n                 embeddings = torch.cat((query_embeds, embeddings), dim=1)"
        },
        {
            "sha": "f8d00a9a19c42a279cac9bc1e028038aad22b87d",
            "filename": "src/transformers/models/instructblipvideo/configuration_instructblipvideo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -152,12 +152,6 @@ class InstructBlipVideoQFormerConfig(PreTrainedConfig):\n             The epsilon used by the layer normalization layers.\n         pad_token_id (`int`, *optional*, defaults to 0):\n             Token id used for padding sequences.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         cross_attention_frequency (`int`, *optional*, defaults to 2):\n             The frequency of adding cross-attention to the Transformer layers.\n         encoder_hidden_size (`int`, *optional*, defaults to 1408):\n@@ -194,7 +188,6 @@ def __init__(\n         initializer_range=0.02,\n         layer_norm_eps=1e-12,\n         pad_token_id=0,\n-        position_embedding_type=\"absolute\",\n         cross_attention_frequency=2,\n         encoder_hidden_size=1408,\n         **kwargs,\n@@ -212,7 +205,6 @@ def __init__(\n         self.max_position_embeddings = max_position_embeddings\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n-        self.position_embedding_type = position_embedding_type\n         self.cross_attention_frequency = cross_attention_frequency\n         self.encoder_hidden_size = encoder_hidden_size\n "
        },
        {
            "sha": "737a53d18a0fe1a95a700764ba5bc232a1123ef6",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 24,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -418,10 +418,6 @@ def __init__(self, config, is_cross_attention=False):\n             self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n         self.save_attention = False\n \n     def save_attn_gradients(self, attn_gradients):\n@@ -469,22 +465,6 @@ def forward(\n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            seq_length = hidden_states.size()[1]\n-            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n         attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n         attention_scores_dtype = attention_scores.dtype\n \n@@ -735,7 +715,6 @@ def __init__(self, config):\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n \n         self.config = config\n \n@@ -756,9 +735,9 @@ def forward(\n \n         if input_ids is not None:\n             embeddings = self.word_embeddings(input_ids)\n-            if self.position_embedding_type == \"absolute\":\n-                position_embeddings = self.position_embeddings(position_ids.to(embeddings.device))\n-                embeddings = embeddings + position_embeddings\n+\n+            position_embeddings = self.position_embeddings(position_ids.to(embeddings.device))\n+            embeddings = embeddings + position_embeddings\n \n             if query_embeds is not None:\n                 embeddings = torch.cat((query_embeds, embeddings), dim=1)"
        },
        {
            "sha": "3044086ccb46aab2c66be504e35489b8161cda7b",
            "filename": "src/transformers/models/lilt/configuration_lilt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Flilt%2Fconfiguration_lilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Flilt%2Fconfiguration_lilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flilt%2Fconfiguration_lilt.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -58,12 +58,6 @@ class LiltConfig(PreTrainedConfig):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n             The epsilon used by the layer normalization layers.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         classifier_dropout (`float`, *optional*):\n             The dropout ratio for the classification head.\n         channel_shrink_ratio (`int`, *optional*, defaults to 4):\n@@ -102,7 +96,6 @@ def __init__(\n         initializer_range=0.02,\n         layer_norm_eps=1e-12,\n         pad_token_id=0,\n-        position_embedding_type=\"absolute\",\n         classifier_dropout=None,\n         channel_shrink_ratio=4,\n         max_2d_position_embeddings=1024,\n@@ -122,7 +115,6 @@ def __init__(\n         self.type_vocab_size = type_vocab_size\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n-        self.position_embedding_type = position_embedding_type\n         self.classifier_dropout = classifier_dropout\n         self.channel_shrink_ratio = channel_shrink_ratio\n         self.max_2d_position_embeddings = max_2d_position_embeddings"
        },
        {
            "sha": "dadae8db246a55e37be78dec5bc06e89a8dbe3d5",
            "filename": "src/transformers/models/lilt/modeling_lilt.py",
            "status": "modified",
            "additions": 7,
            "deletions": 30,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -53,7 +53,6 @@ def __init__(self, config):\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n \n         # End copy\n         self.padding_idx = config.pad_token_id\n@@ -88,11 +87,11 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings += position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings, position_ids\n@@ -183,7 +182,7 @@ def forward(self, bbox=None, position_ids=None):\n \n \n class LiltSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -210,12 +209,6 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         )\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.channel_shrink_ratio = config.channel_shrink_ratio\n         self.layer_idx = layer_idx\n@@ -245,22 +238,6 @@ def forward(\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n         layout_attention_scores = torch.matmul(layout_query_layer, layout_key_layer.transpose(-1, -2))\n \n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            seq_length = hidden_states.size()[1]\n-            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n         tmp_attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n         tmp_layout_attention_scores = layout_attention_scores / math.sqrt(\n             self.attention_head_size // self.channel_shrink_ratio\n@@ -327,9 +304,9 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class LiltAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n-        self.self = LiltSelfAttention(config, position_embedding_type=position_embedding_type, layer_idx=layer_idx)\n+        self.self = LiltSelfAttention(config, layer_idx=layer_idx)\n         self.output = LiltSelfOutput(config)\n         self.pruned_heads = set()\n "
        },
        {
            "sha": "92a09cf72fbd90188dc0f7479379b53e0fe68bf1",
            "filename": "src/transformers/models/megatron_bert/configuration_megatron_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fconfiguration_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fconfiguration_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fconfiguration_megatron_bert.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -60,12 +60,6 @@ class MegatronBertConfig(PreTrainedConfig):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n             The epsilon used by the layer normalization layers.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         is_decoder (`bool`, *optional*, defaults to `False`):\n             Whether the model is used as a decoder or not. If `False`, the model is used as an encoder.\n         use_cache (`bool`, *optional*, defaults to `True`):\n@@ -104,7 +98,6 @@ def __init__(\n         initializer_range=0.02,\n         layer_norm_eps=1e-12,\n         pad_token_id=0,\n-        position_embedding_type=\"absolute\",\n         use_cache=True,\n         **kwargs,\n     ):\n@@ -122,7 +115,6 @@ def __init__(\n         self.type_vocab_size = type_vocab_size\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n-        self.position_embedding_type = position_embedding_type\n         self.use_cache = use_cache\n \n "
        },
        {
            "sha": "e17a4b206fe711a08676e177fb62bc07403db883",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 34,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -66,7 +66,6 @@ def __init__(self, config):\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n \n     def forward(\n         self,\n@@ -92,11 +91,10 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings += position_embeddings\n \n         # Megatron BERT moves that layer norm after the drop-out (and to each layer).\n         # embeddings = self.LayerNorm(embeddings)\n@@ -106,7 +104,7 @@ def forward(\n \n # copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->MegatronBert\n class MegatronBertSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -123,12 +121,6 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n@@ -190,28 +182,6 @@ def forward(\n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_values is not None:\n-                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n-                    -1, 1\n-                )\n-            else:\n-                position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n         attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n         if attention_mask is not None:\n             # Apply the attention mask is (precomputed for all layers in MegatronBertModel forward() function)"
        },
        {
            "sha": "3e528c1de0fe3c0c41bd34c4bab4c327db22d59f",
            "filename": "src/transformers/models/mra/configuration_mra.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fmra%2Fconfiguration_mra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fmra%2Fconfiguration_mra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmra%2Fconfiguration_mra.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -60,8 +60,6 @@ class MraConfig(PreTrainedConfig):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-5):\n             The epsilon used by the layer normalization layers.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`.\n         block_per_row (`int`, *optional*, defaults to 4):\n             Used to set the budget for the high resolution scale.\n         approx_mode (`str`, *optional*, defaults to `\"full\"`):\n@@ -103,7 +101,6 @@ def __init__(\n         type_vocab_size=1,\n         initializer_range=0.02,\n         layer_norm_eps=1e-5,\n-        position_embedding_type=\"absolute\",\n         block_per_row=4,\n         approx_mode=\"full\",\n         initial_prior_first_n_blocks=0,\n@@ -127,7 +124,6 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.type_vocab_size = type_vocab_size\n         self.layer_norm_eps = layer_norm_eps\n-        self.position_embedding_type = position_embedding_type\n         self.block_per_row = block_per_row\n         self.approx_mode = approx_mode\n         self.initial_prior_first_n_blocks = initial_prior_first_n_blocks"
        },
        {
            "sha": "7c8acd6a67af244dd2888908cbc6d2bde92f8e6c",
            "filename": "src/transformers/models/mra/modeling_mra.py",
            "status": "modified",
            "additions": 7,
            "deletions": 11,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -474,7 +474,6 @@ def __init__(self, config):\n \n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n         self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)) + 2)\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"token_type_ids\",\n             torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),\n@@ -506,18 +505,18 @@ def forward(self, input_ids=None, token_type_ids=None, position_ids=None, inputs\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings += position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings\n \n \n class MraSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -541,9 +540,6 @@ def __init__(self, config, position_embedding_type=None):\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = (\n-            position_embedding_type if position_embedding_type is not None else config.position_embedding_type\n-        )\n \n         self.num_block = (config.max_position_embeddings // 32) * config.block_per_row\n         self.num_block = min(self.num_block, int((config.max_position_embeddings // 32) ** 2))\n@@ -631,9 +627,9 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class MraAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config):\n         super().__init__()\n-        self.self = MraSelfAttention(config, position_embedding_type=position_embedding_type)\n+        self.self = MraSelfAttention(config)\n         self.output = MraSelfOutput(config)\n         self.pruned_heads = set()\n "
        },
        {
            "sha": "e27efc082384cc58ef32e976b1bc7d831e73853f",
            "filename": "src/transformers/models/nystromformer/modeling_nystromformer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 11,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -59,7 +59,6 @@ def __init__(self, config):\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)) + 2, persistent=False\n         )\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"token_type_ids\",\n             torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),\n@@ -91,18 +90,18 @@ def forward(self, input_ids=None, token_type_ids=None, position_ids=None, inputs\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings += position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings\n \n \n class NystromformerSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -128,9 +127,6 @@ def __init__(self, config, position_embedding_type=None):\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n \n         if self.conv_kernel_size is not None:\n             self.conv = nn.Conv2d(\n@@ -253,9 +249,9 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class NystromformerAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config):\n         super().__init__()\n-        self.self = NystromformerSelfAttention(config, position_embedding_type=position_embedding_type)\n+        self.self = NystromformerSelfAttention(config)\n         self.output = NystromformerSelfOutput(config)\n         self.pruned_heads = set()\n "
        },
        {
            "sha": "a8863ca4ba7c2ee1377a575fa7620c17fa81b5f1",
            "filename": "src/transformers/models/roberta/configuration_roberta.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Froberta%2Fconfiguration_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Froberta%2Fconfiguration_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fconfiguration_roberta.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -65,12 +65,6 @@ class RobertaConfig(PreTrainedConfig):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n             The epsilon used by the layer normalization layers.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         is_decoder (`bool`, *optional*, defaults to `False`):\n             Whether the model is used as a decoder or not. If `False`, the model is used as an encoder.\n         use_cache (`bool`, *optional*, defaults to `True`):\n@@ -113,7 +107,6 @@ def __init__(\n         pad_token_id=1,\n         bos_token_id=0,\n         eos_token_id=2,\n-        position_embedding_type=\"absolute\",\n         use_cache=True,\n         classifier_dropout=None,\n         **kwargs,\n@@ -132,7 +125,6 @@ def __init__(\n         self.type_vocab_size = type_vocab_size\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n-        self.position_embedding_type = position_embedding_type\n         self.use_cache = use_cache\n         self.classifier_dropout = classifier_dropout\n "
        },
        {
            "sha": "b38b41a9226ee6af10e8f55db024646dee7ac2df",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 9,
            "deletions": 68,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -68,7 +68,6 @@ def __init__(self, config):\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n@@ -120,11 +119,11 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings = embeddings + position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings\n@@ -172,38 +171,13 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    use_cache: Optional[bool] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(2, 3))\n-\n-    # Relative positional embeddings\n-    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n-        query_length, key_length = query.shape[2], key.shape[2]\n-        if use_cache:\n-            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n-        else:\n-            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n-        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n-        distance = position_ids_l - position_ids_r\n-\n-        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n-        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n-\n-        if module.position_embedding_type == \"relative_key\":\n-            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores\n-        elif module.position_embedding_type == \"relative_key_query\":\n-            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n-\n-    # Scaling is shifted in case of embeddings being relative\n-    attn_weights = attn_weights * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n     if attention_mask is not None and attention_mask.ndim == 4:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n@@ -219,7 +193,7 @@ def eager_attention_forward(\n \n \n class RobertaSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -238,12 +212,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n         self.is_causal = is_causal\n@@ -281,11 +249,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -296,16 +259,14 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         return attn_output, attn_weights\n \n \n class RobertaCrossAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -324,12 +285,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_causal = is_causal\n         self.layer_idx = layer_idx\n@@ -371,11 +326,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -386,8 +336,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n@@ -409,15 +357,11 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class RobertaAttention(nn.Module):\n-    def __init__(\n-        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n-    ):\n+    def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=False):\n         super().__init__()\n         self.is_cross_attention = is_cross_attention\n         attention_class = RobertaCrossAttention if is_cross_attention else RobertaSelfAttention\n-        self.self = attention_class(\n-            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n-        )\n+        self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = RobertaSelfOutput(config)\n         self.pruned_heads = set()\n \n@@ -504,7 +448,6 @@ def __init__(self, config, layer_idx=None):\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n             self.crossattention = RobertaAttention(\n                 config,\n-                position_embedding_type=\"absolute\",\n                 is_causal=False,\n                 layer_idx=layer_idx,\n                 is_cross_attention=True,\n@@ -670,8 +613,6 @@ def __init__(self, config, add_pooling_layer=True):\n \n         self.pooler = RobertaPooler(config) if add_pooling_layer else None\n \n-        self.position_embedding_type = config.position_embedding_type\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n "
        },
        {
            "sha": "cf10f735cbb247681afc6fdb86f093b24736b524",
            "filename": "src/transformers/models/roberta/modular_roberta.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Froberta%2Fmodular_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Froberta%2Fmodular_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodular_roberta.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -93,11 +93,11 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings = embeddings + position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings"
        },
        {
            "sha": "4ac31a300e4ac621a5ab4f64e862c100e26eab34",
            "filename": "src/transformers/models/roberta_prelayernorm/configuration_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fconfiguration_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fconfiguration_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fconfiguration_roberta_prelayernorm.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -66,12 +66,6 @@ class RobertaPreLayerNormConfig(PreTrainedConfig):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n             The epsilon used by the layer normalization layers.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         is_decoder (`bool`, *optional*, defaults to `False`):\n             Whether the model is used as a decoder or not. If `False`, the model is used as an encoder.\n         use_cache (`bool`, *optional*, defaults to `True`):\n@@ -114,7 +108,6 @@ def __init__(\n         pad_token_id=1,\n         bos_token_id=0,\n         eos_token_id=2,\n-        position_embedding_type=\"absolute\",\n         use_cache=True,\n         classifier_dropout=None,\n         **kwargs,\n@@ -133,7 +126,6 @@ def __init__(\n         self.type_vocab_size = type_vocab_size\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n-        self.position_embedding_type = position_embedding_type\n         self.use_cache = use_cache\n         self.classifier_dropout = classifier_dropout\n "
        },
        {
            "sha": "855359c447c1d393694028cc3a3a24c463808657",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 9,
            "deletions": 66,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -64,7 +64,6 @@ def __init__(self, config):\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n@@ -116,11 +115,11 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings = embeddings + position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings\n@@ -169,38 +168,13 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    use_cache: Optional[bool] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(2, 3))\n-\n-    # Relative positional embeddings\n-    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n-        query_length, key_length = query.shape[2], key.shape[2]\n-        if use_cache:\n-            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n-        else:\n-            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n-        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n-        distance = position_ids_l - position_ids_r\n-\n-        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n-        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n-\n-        if module.position_embedding_type == \"relative_key\":\n-            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores\n-        elif module.position_embedding_type == \"relative_key_query\":\n-            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n-\n-    # Scaling is shifted in case of embeddings being relative\n-    attn_weights = attn_weights * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n     if attention_mask is not None and attention_mask.ndim == 4:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n@@ -217,7 +191,7 @@ def eager_attention_forward(\n \n # Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->RobertaPreLayerNorm\n class RobertaPreLayerNormSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -236,12 +210,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n         self.is_causal = is_causal\n@@ -279,11 +247,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -294,8 +257,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n@@ -304,7 +265,7 @@ def forward(\n \n # Copied from transformers.models.bert.modeling_bert.BertCrossAttention with Bert->RobertaPreLayerNorm\n class RobertaPreLayerNormCrossAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -323,12 +284,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_causal = is_causal\n         self.layer_idx = layer_idx\n@@ -370,11 +325,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -385,8 +335,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n@@ -407,15 +355,11 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class RobertaPreLayerNormAttention(nn.Module):\n-    def __init__(\n-        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n-    ):\n+    def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=False):\n         super().__init__()\n         self.is_cross_attention = is_cross_attention\n         attention_class = RobertaPreLayerNormCrossAttention if is_cross_attention else RobertaPreLayerNormSelfAttention\n-        self.self = attention_class(\n-            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n-        )\n+        self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = RobertaPreLayerNormSelfOutput(config)\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.pruned_heads = set()\n@@ -507,7 +451,6 @@ def __init__(self, config, layer_idx=None):\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n             self.crossattention = RobertaPreLayerNormAttention(\n                 config,\n-                position_embedding_type=\"absolute\",\n                 is_causal=False,\n                 layer_idx=layer_idx,\n                 is_cross_attention=True,"
        },
        {
            "sha": "31f66e203c896660ccc06fcf97852ece0fbd3755",
            "filename": "src/transformers/models/roc_bert/configuration_roc_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Froc_bert%2Fconfiguration_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Froc_bert%2Fconfiguration_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fconfiguration_roc_bert.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -65,12 +65,6 @@ class RoCBertConfig(PreTrainedConfig):\n         use_cache (`bool`, *optional*, defaults to `True`):\n             Whether or not the model should return the last key/values attentions (not used by all models). Only\n             relevant if `config.is_decoder=True`.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         classifier_dropout (`float`, *optional*):\n             The dropout ratio for the classification head.\n         enable_pronunciation (`bool`, *optional*, defaults to `True`):\n@@ -124,7 +118,6 @@ def __init__(\n         layer_norm_eps=1e-12,\n         use_cache=True,\n         pad_token_id=0,\n-        position_embedding_type=\"absolute\",\n         classifier_dropout=None,\n         enable_pronunciation=True,\n         enable_shape=True,\n@@ -155,7 +148,6 @@ def __init__(\n         self.shape_embed_dim = shape_embed_dim\n         self.shape_vocab_size = shape_vocab_size\n         self.concat_input = concat_input\n-        self.position_embedding_type = position_embedding_type\n         self.classifier_dropout = classifier_dropout\n         super().__init__(pad_token_id=pad_token_id, **kwargs)\n "
        },
        {
            "sha": "67f98c2c3c7cd45bf8ba2c0224cc687e1b95a64d",
            "filename": "src/transformers/models/roc_bert/modeling_roc_bert.py",
            "status": "modified",
            "additions": 9,
            "deletions": 68,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -88,7 +88,6 @@ def __init__(self, config):\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"token_type_ids\",\n             torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),\n@@ -132,9 +131,8 @@ def forward(\n                 inputs_embeds = self.word_embeddings(input_ids)\n             token_type_embeddings = self.token_type_embeddings(token_type_ids)\n             embeddings = inputs_embeds + token_type_embeddings\n-            if self.position_embedding_type == \"absolute\":\n-                position_embeddings = self.position_embeddings(position_ids)\n-                embeddings += position_embeddings\n+            position_embeddings = self.position_embeddings(position_ids)\n+            embeddings = embeddings + position_embeddings\n             embeddings = self.LayerNorm(embeddings)\n             embeddings = self.dropout(embeddings)\n \n@@ -172,9 +170,8 @@ def forward(\n \n             token_type_embeddings = self.token_type_embeddings(token_type_ids)\n             embedding_in += token_type_embeddings\n-            if self.position_embedding_type == \"absolute\":\n-                position_embeddings = self.position_embeddings(position_ids)\n-                embedding_in += position_embeddings\n+            position_embeddings = self.position_embeddings(position_ids)\n+            embedding_in += position_embeddings\n \n             embedding_in = self.LayerNorm(embedding_in)\n             embedding_in = self.dropout(embedding_in)\n@@ -190,38 +187,13 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    use_cache: Optional[bool] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(2, 3))\n-\n-    # Relative positional embeddings\n-    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n-        query_length, key_length = query.shape[2], key.shape[2]\n-        if use_cache:\n-            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n-        else:\n-            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n-        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n-        distance = position_ids_l - position_ids_r\n-\n-        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n-        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n-\n-        if module.position_embedding_type == \"relative_key\":\n-            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores\n-        elif module.position_embedding_type == \"relative_key_query\":\n-            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n-\n-    # Scaling is shifted in case of embeddings being relative\n-    attn_weights = attn_weights * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n     if attention_mask is not None and attention_mask.ndim == 4:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n@@ -238,7 +210,7 @@ def eager_attention_forward(\n \n # Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->RoCBert\n class RoCBertSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -257,12 +229,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n         self.is_causal = is_causal\n@@ -300,11 +266,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -315,8 +276,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n@@ -325,7 +284,7 @@ def forward(\n \n # Copied from transformers.models.bert.modeling_bert.BertCrossAttention with Bert->RoCBert\n class RoCBertCrossAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -344,12 +303,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_causal = is_causal\n         self.layer_idx = layer_idx\n@@ -391,11 +344,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -406,8 +354,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n@@ -431,15 +377,11 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n # Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->RoCBert,BERT->ROC_BERT\n class RoCBertAttention(nn.Module):\n-    def __init__(\n-        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n-    ):\n+    def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=False):\n         super().__init__()\n         self.is_cross_attention = is_cross_attention\n         attention_class = RoCBertCrossAttention if is_cross_attention else RoCBertSelfAttention\n-        self.self = attention_class(\n-            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n-        )\n+        self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = RoCBertSelfOutput(config)\n         self.pruned_heads = set()\n \n@@ -529,7 +471,6 @@ def __init__(self, config, layer_idx=None):\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n             self.crossattention = RoCBertAttention(\n                 config,\n-                position_embedding_type=\"absolute\",\n                 is_causal=False,\n                 layer_idx=layer_idx,\n                 is_cross_attention=True,"
        },
        {
            "sha": "2f7ec4bb416b4e500f45df9b2a965167ac1b728f",
            "filename": "src/transformers/models/splinter/modeling_splinter.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -53,7 +53,6 @@ def __init__(self, config):\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n \n     def forward(\n         self,\n@@ -78,11 +77,11 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings += position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings"
        },
        {
            "sha": "54981b1bc14110b30696b53a071038804a70d412",
            "filename": "src/transformers/models/superglue/modeling_superglue.py",
            "status": "modified",
            "additions": 3,
            "deletions": 29,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -234,7 +234,7 @@ def forward(\n \n \n class SuperGlueSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -251,12 +251,6 @@ def __init__(self, config, position_embedding_type=None):\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n \n@@ -295,23 +289,6 @@ def forward(\n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n         attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n         if attention_mask is not None:\n             # Apply the attention mask is (precomputed for all layers in SuperGlueModel forward() function)\n@@ -353,12 +330,9 @@ def forward(self, hidden_states: torch.Tensor, *args) -> torch.Tensor:\n \n \n class SuperGlueAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config):\n         super().__init__()\n-        self.self = SUPERGLUE_SELF_ATTENTION_CLASSES[config._attn_implementation](\n-            config,\n-            position_embedding_type=position_embedding_type,\n-        )\n+        self.self = SUPERGLUE_SELF_ATTENTION_CLASSES[config._attn_implementation](config)\n         self.output = SuperGlueSelfOutput(config)\n         self.pruned_heads = set()\n "
        },
        {
            "sha": "bbe8cd5ed49bf5d91c7f6599475e4fadffed276b",
            "filename": "src/transformers/models/vilt/modeling_vilt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -232,7 +232,6 @@ def __init__(self, config):\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n@@ -265,11 +264,11 @@ def forward(self, input_ids=None, token_type_ids=None, position_ids=None, inputs\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings += position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings"
        },
        {
            "sha": "de02a8f59181ddc3fd4b728038ea6e7658484db8",
            "filename": "src/transformers/models/xlm_roberta/configuration_xlm_roberta.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fconfiguration_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fconfiguration_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fconfiguration_xlm_roberta.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -66,12 +66,6 @@ class XLMRobertaConfig(PreTrainedConfig):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n             The epsilon used by the layer normalization layers.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         is_decoder (`bool`, *optional*, defaults to `False`):\n             Whether the model is used as a decoder or not. If `False`, the model is used as an encoder.\n         use_cache (`bool`, *optional*, defaults to `True`):\n@@ -114,7 +108,6 @@ def __init__(\n         pad_token_id=1,\n         bos_token_id=0,\n         eos_token_id=2,\n-        position_embedding_type=\"absolute\",\n         use_cache=True,\n         classifier_dropout=None,\n         **kwargs,\n@@ -133,7 +126,6 @@ def __init__(\n         self.type_vocab_size = type_vocab_size\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n-        self.position_embedding_type = position_embedding_type\n         self.use_cache = use_cache\n         self.classifier_dropout = classifier_dropout\n "
        },
        {
            "sha": "875b2bb9406b5ef3b9a41f7faeeead17bb3bd194",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 9,
            "deletions": 68,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -65,38 +65,13 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    use_cache: Optional[bool] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(2, 3))\n-\n-    # Relative positional embeddings\n-    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n-        query_length, key_length = query.shape[2], key.shape[2]\n-        if use_cache:\n-            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n-        else:\n-            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n-        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n-        distance = position_ids_l - position_ids_r\n-\n-        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n-        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n-\n-        if module.position_embedding_type == \"relative_key\":\n-            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores\n-        elif module.position_embedding_type == \"relative_key_query\":\n-            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n-\n-    # Scaling is shifted in case of embeddings being relative\n-    attn_weights = attn_weights * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n     if attention_mask is not None and attention_mask.ndim == 4:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n@@ -112,7 +87,7 @@ def eager_attention_forward(\n \n \n class XLMRobertaSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -131,12 +106,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n         self.is_causal = is_causal\n@@ -174,11 +143,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -189,16 +153,14 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         return attn_output, attn_weights\n \n \n class XLMRobertaCrossAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -217,12 +179,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_causal = is_causal\n         self.layer_idx = layer_idx\n@@ -264,11 +220,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -279,8 +230,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n@@ -302,15 +251,11 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class XLMRobertaAttention(nn.Module):\n-    def __init__(\n-        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n-    ):\n+    def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=False):\n         super().__init__()\n         self.is_cross_attention = is_cross_attention\n         attention_class = XLMRobertaCrossAttention if is_cross_attention else XLMRobertaSelfAttention\n-        self.self = attention_class(\n-            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n-        )\n+        self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = XLMRobertaSelfOutput(config)\n         self.pruned_heads = set()\n \n@@ -397,7 +342,6 @@ def __init__(self, config, layer_idx=None):\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n             self.crossattention = XLMRobertaAttention(\n                 config,\n-                position_embedding_type=\"absolute\",\n                 is_causal=False,\n                 layer_idx=layer_idx,\n                 is_cross_attention=True,\n@@ -526,7 +470,6 @@ def __init__(self, config):\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n@@ -578,11 +521,11 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings = embeddings + position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings\n@@ -689,8 +632,6 @@ def __init__(self, config, add_pooling_layer=True):\n \n         self.pooler = XLMRobertaPooler(config) if add_pooling_layer else None\n \n-        self.position_embedding_type = config.position_embedding_type\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n "
        },
        {
            "sha": "7f3f1fda7c3d094deebbd41f588f3920f2077e56",
            "filename": "src/transformers/models/xlm_roberta_xl/configuration_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fconfiguration_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fconfiguration_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fconfiguration_xlm_roberta_xl.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -65,12 +65,6 @@ class XLMRobertaXLConfig(PreTrainedConfig):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-5):\n             The epsilon used by the layer normalization layers.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         use_cache (`bool`, *optional*, defaults to `True`):\n             Whether or not the model should return the last key/values attentions (not used by all models). Only\n             relevant if `config.is_decoder=True`.\n@@ -111,7 +105,6 @@ def __init__(\n         pad_token_id=1,\n         bos_token_id=0,\n         eos_token_id=2,\n-        position_embedding_type=\"absolute\",\n         use_cache=True,\n         classifier_dropout=None,\n         **kwargs,\n@@ -129,7 +122,6 @@ def __init__(\n         self.type_vocab_size = type_vocab_size\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n-        self.position_embedding_type = position_embedding_type\n         self.use_cache = use_cache\n         self.classifier_dropout = classifier_dropout\n "
        },
        {
            "sha": "4d26781003f7090390ea558249a6ff8c3dc80a25",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 68,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -71,7 +71,6 @@ def __init__(self, config):\n         self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n@@ -123,11 +122,10 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings = embeddings + position_embeddings\n \n         embeddings = self.dropout(embeddings)\n         return embeddings\n@@ -175,38 +173,13 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    use_cache: Optional[bool] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(2, 3))\n-\n-    # Relative positional embeddings\n-    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n-        query_length, key_length = query.shape[2], key.shape[2]\n-        if use_cache:\n-            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n-        else:\n-            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n-        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n-        distance = position_ids_l - position_ids_r\n-\n-        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n-        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n-\n-        if module.position_embedding_type == \"relative_key\":\n-            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores\n-        elif module.position_embedding_type == \"relative_key_query\":\n-            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n-\n-    # Scaling is shifted in case of embeddings being relative\n-    attn_weights = attn_weights * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n     if attention_mask is not None and attention_mask.ndim == 4:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n@@ -222,7 +195,7 @@ def eager_attention_forward(\n \n \n class XLMRobertaXLSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -241,12 +214,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n         self.is_causal = is_causal\n@@ -284,11 +251,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -299,16 +261,14 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         return attn_output, attn_weights\n \n \n class XLMRobertaXLCrossAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -327,12 +287,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_causal = is_causal\n         self.layer_idx = layer_idx\n@@ -374,11 +328,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -389,8 +338,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n@@ -411,15 +358,11 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class XLMRobertaXLAttention(nn.Module):\n-    def __init__(\n-        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n-    ):\n+    def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=False):\n         super().__init__()\n         self.is_cross_attention = is_cross_attention\n         attention_class = XLMRobertaXLCrossAttention if is_cross_attention else XLMRobertaXLSelfAttention\n-        self.self = attention_class(\n-            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n-        )\n+        self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = XLMRobertaXLSelfOutput(config)\n         self.pruned_heads = set()\n \n@@ -506,7 +449,6 @@ def __init__(self, config, layer_idx=None):\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n             self.crossattention = XLMRobertaXLAttention(\n                 config,\n-                position_embedding_type=\"absolute\",\n                 is_causal=False,\n                 layer_idx=layer_idx,\n                 is_cross_attention=True,\n@@ -677,8 +619,6 @@ def __init__(self, config, add_pooling_layer=True):\n \n         self.pooler = XLMRobertaXLPooler(config) if add_pooling_layer else None\n \n-        self.position_embedding_type = config.position_embedding_type\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n "
        },
        {
            "sha": "c1c970cf71389ccba508f6b34488c0a58c2ff090",
            "filename": "src/transformers/models/xlm_roberta_xl/modular_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodular_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodular_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodular_xlm_roberta_xl.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -102,11 +102,10 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings = embeddings + position_embeddings\n \n         embeddings = self.dropout(embeddings)\n         return embeddings\n@@ -134,10 +133,8 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class XLMRobertaXLAttention(BertAttention):\n-    def __init__(\n-        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n-    ):\n-        super().__init__(config, position_embedding_type, is_causal, layer_idx, is_cross_attention)\n+    def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=False):\n+        super().__init__(config, is_causal, layer_idx, is_cross_attention)\n         del self.LayerNorm\n \n         self.self_attn_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)"
        },
        {
            "sha": "95298b351e3305a9086c00375b2389aa9669fb5f",
            "filename": "src/transformers/models/xmod/configuration_xmod.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fxmod%2Fconfiguration_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fxmod%2Fconfiguration_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fconfiguration_xmod.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -65,12 +65,6 @@ class XmodConfig(PreTrainedConfig):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n             The epsilon used by the layer normalization layers.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         is_decoder (`bool`, *optional*, defaults to `False`):\n             Whether the model is used as a decoder or not. If `False`, the model is used as an encoder.\n         use_cache (`bool`, *optional*, defaults to `True`):\n@@ -128,7 +122,6 @@ def __init__(\n         pad_token_id=1,\n         bos_token_id=0,\n         eos_token_id=2,\n-        position_embedding_type=\"absolute\",\n         use_cache=True,\n         classifier_dropout=None,\n         pre_norm=False,\n@@ -154,7 +147,6 @@ def __init__(\n         self.type_vocab_size = type_vocab_size\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n-        self.position_embedding_type = position_embedding_type\n         self.use_cache = use_cache\n         self.classifier_dropout = classifier_dropout\n         self.pre_norm = pre_norm"
        },
        {
            "sha": "89968923e27189f1b3d58c45b7de081066e31377",
            "filename": "src/transformers/models/xmod/modeling_xmod.py",
            "status": "modified",
            "additions": 9,
            "deletions": 66,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -63,7 +63,6 @@ def __init__(self, config):\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n         )\n@@ -115,11 +114,11 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings = embeddings + position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings\n@@ -168,38 +167,13 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    use_cache: Optional[bool] = None,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(2, 3))\n-\n-    # Relative positional embeddings\n-    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n-        query_length, key_length = query.shape[2], key.shape[2]\n-        if use_cache:\n-            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n-        else:\n-            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n-        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n-        distance = position_ids_l - position_ids_r\n-\n-        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n-        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n-\n-        if module.position_embedding_type == \"relative_key\":\n-            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores\n-        elif module.position_embedding_type == \"relative_key_query\":\n-            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n-            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n-            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n-\n-    # Scaling is shifted in case of embeddings being relative\n-    attn_weights = attn_weights * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n     if attention_mask is not None and attention_mask.ndim == 4:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n@@ -216,7 +190,7 @@ def eager_attention_forward(\n \n # Copied from transformers.models.roberta.modeling_roberta.RobertaSelfAttention with Roberta->Xmod\n class XmodSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -235,12 +209,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n         self.is_causal = is_causal\n@@ -278,11 +246,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -293,8 +256,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n@@ -303,7 +264,7 @@ def forward(\n \n # Copied from transformers.models.bert.modeling_bert.BertCrossAttention with Bert->Xmod\n class XmodCrossAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -322,12 +283,6 @@ def __init__(self, config, position_embedding_type=None, is_causal=False, layer_\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = position_embedding_type or getattr(\n-            config, \"position_embedding_type\", \"absolute\"\n-        )\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            self.max_position_embeddings = config.max_position_embeddings\n-            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_causal = is_causal\n         self.layer_idx = layer_idx\n@@ -369,11 +324,6 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.position_embedding_type != \"absolute\":\n-                raise ValueError(\n-                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n-                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n-                )\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n@@ -384,8 +334,6 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout.p,\n             scaling=self.scaling,\n-            # only for relevant for non-absolute positional embeddings\n-            use_cache=past_key_value is not None,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n@@ -408,15 +356,11 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class XmodAttention(nn.Module):\n-    def __init__(\n-        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n-    ):\n+    def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=False):\n         super().__init__()\n         self.is_cross_attention = is_cross_attention\n         attention_class = XmodCrossAttention if is_cross_attention else XmodSelfAttention\n-        self.self = attention_class(\n-            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n-        )\n+        self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = XmodSelfOutput(config)\n         self.pruned_heads = set()\n         self.pre_norm = config.pre_norm\n@@ -568,7 +512,6 @@ def __init__(self, config, layer_idx=None):\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n             self.crossattention = XmodAttention(\n                 config,\n-                position_embedding_type=\"absolute\",\n                 is_causal=False,\n                 layer_idx=layer_idx,\n                 is_cross_attention=True,"
        },
        {
            "sha": "89070453ab1cb2cf43a8bf4601caebd156a717a6",
            "filename": "src/transformers/models/yoso/configuration_yoso.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fyoso%2Fconfiguration_yoso.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fyoso%2Fconfiguration_yoso.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyoso%2Fconfiguration_yoso.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -60,8 +60,6 @@ class YosoConfig(PreTrainedConfig):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n             The epsilon used by the layer normalization layers.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`.\n         use_expectation (`bool`, *optional*, defaults to `True`):\n             Whether or not to use YOSO Expectation. Overrides any effect of num_hash.\n         hash_code_len (`int`, *optional*, defaults to 9):\n@@ -106,7 +104,6 @@ def __init__(\n         type_vocab_size=1,\n         initializer_range=0.02,\n         layer_norm_eps=1e-12,\n-        position_embedding_type=\"absolute\",\n         use_expectation=True,\n         hash_code_len=9,\n         num_hash=64,\n@@ -132,7 +129,6 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.type_vocab_size = type_vocab_size\n         self.layer_norm_eps = layer_norm_eps\n-        self.position_embedding_type = position_embedding_type\n         self.use_expectation = use_expectation\n         self.hash_code_len = hash_code_len\n         self.num_hash = num_hash"
        },
        {
            "sha": "375321576dc970cd826e45dade122ace884caa78",
            "filename": "src/transformers/models/yoso/modeling_yoso.py",
            "status": "modified",
            "additions": 7,
            "deletions": 11,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fyoso%2Fmodeling_yoso.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/src%2Ftransformers%2Fmodels%2Fyoso%2Fmodeling_yoso.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyoso%2Fmodeling_yoso.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -244,7 +244,6 @@ def __init__(self, config):\n         self.register_buffer(\n             \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)) + 2, persistent=False\n         )\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n         self.register_buffer(\n             \"token_type_ids\",\n             torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),\n@@ -276,18 +275,18 @@ def forward(self, input_ids=None, token_type_ids=None, position_ids=None, inputs\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-\n         embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n+\n+        position_embeddings = self.position_embeddings(position_ids)\n+        embeddings += position_embeddings\n+\n         embeddings = self.LayerNorm(embeddings)\n         embeddings = self.dropout(embeddings)\n         return embeddings\n \n \n class YosoSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -310,9 +309,6 @@ def __init__(self, config, position_embedding_type=None):\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.position_embedding_type = (\n-            position_embedding_type if position_embedding_type is not None else config.position_embedding_type\n-        )\n \n         self.use_expectation = config.use_expectation\n         self.hash_code_len = config.hash_code_len\n@@ -449,9 +445,9 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class YosoAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config):\n         super().__init__()\n-        self.self = YosoSelfAttention(config, position_embedding_type=position_embedding_type)\n+        self.self = YosoSelfAttention(config)\n         self.output = YosoSelfOutput(config)\n         self.pruned_heads = set()\n "
        },
        {
            "sha": "6e0d5ef5603c96cd9d94569e601108f314caaf0b",
            "filename": "tests/models/albert/test_modeling_albert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -307,13 +307,6 @@ def test_for_sequence_classification(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_sequence_classification(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            config_and_inputs[0]._attn_implementation = \"eager\"\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     @slow\n     def test_model_from_pretrained(self):\n         model_name = \"albert/albert-base-v1\""
        },
        {
            "sha": "f8beb9457758f10770005bb72a27ccdfc09cbef3",
            "filename": "tests/models/bert/test_modeling_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 47,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -496,13 +496,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            config_and_inputs[0]._attn_implementation = \"eager\"\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_model_3d_mask_shapes(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         # manipulate input_mask\n@@ -588,12 +581,6 @@ def test_decoder_model_past_with_large_inputs(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n \n-    def test_decoder_model_past_with_large_inputs_relative_pos_emb(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n-        config_and_inputs[0].position_embedding_type = \"relative_key\"\n-        config_and_inputs[0]._attn_implementation = \"eager\"\n-        self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n-\n     def test_for_multiple_choice(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_multiple_choice(*config_and_inputs)\n@@ -754,40 +741,6 @@ def test_inference_no_head_absolute_embedding(self):\n \n         torch.testing.assert_close(output[:, 1:4, 1:4], expected_slice, rtol=1e-4, atol=1e-4)\n \n-    @slow\n-    def test_inference_no_head_relative_embedding_key(self):\n-        model = BertModel.from_pretrained(\n-            \"zhiheng-huang/bert-base-uncased-embedding-relative-key\", attn_implementation=\"eager\"\n-        )\n-        input_ids = torch.tensor([[0, 345, 232, 328, 740, 140, 1695, 69, 6078, 1588, 2]])\n-        attention_mask = torch.tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n-        with torch.no_grad():\n-            output = model(input_ids, attention_mask=attention_mask)[0]\n-        expected_shape = torch.Size((1, 11, 768))\n-        self.assertEqual(output.shape, expected_shape)\n-        expected_slice = torch.tensor(\n-            [[[0.0756, 0.3142, -0.5128], [0.3761, 0.3462, -0.5477], [0.2052, 0.3760, -0.1240]]]\n-        )\n-\n-        torch.testing.assert_close(output[:, 1:4, 1:4], expected_slice, rtol=1e-4, atol=1e-4)\n-\n-    @slow\n-    def test_inference_no_head_relative_embedding_key_query(self):\n-        model = BertModel.from_pretrained(\n-            \"zhiheng-huang/bert-base-uncased-embedding-relative-key-query\", attn_implementation=\"eager\"\n-        )\n-        input_ids = torch.tensor([[0, 345, 232, 328, 740, 140, 1695, 69, 6078, 1588, 2]])\n-        attention_mask = torch.tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n-        with torch.no_grad():\n-            output = model(input_ids, attention_mask=attention_mask)[0]\n-        expected_shape = torch.Size((1, 11, 768))\n-        self.assertEqual(output.shape, expected_shape)\n-        expected_slice = torch.tensor(\n-            [[[0.6496, 0.3784, 0.8203], [0.8148, 0.5656, 0.2636], [-0.0681, 0.5597, 0.7045]]]\n-        )\n-\n-        torch.testing.assert_close(output[:, 1:4, 1:4], expected_slice, rtol=1e-4, atol=1e-4)\n-\n     @slow\n     @pytest.mark.torch_export_test\n     def test_export(self):"
        },
        {
            "sha": "b34ea4c47bcf65e2dcf9ba21aefcdfb20c8742c3",
            "filename": "tests/models/big_bird/test_modeling_big_bird.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -70,7 +70,6 @@ def __init__(\n         rescale_embeddings=False,\n         block_size=8,\n         num_rand_blocks=3,\n-        position_embedding_type=\"absolute\",\n         scope=None,\n     ):\n         self.parent = parent\n@@ -101,7 +100,6 @@ def __init__(\n         self.rescale_embeddings = rescale_embeddings\n         self.block_size = block_size\n         self.num_rand_blocks = num_rand_blocks\n-        self.position_embedding_type = position_embedding_type\n \n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n@@ -145,7 +143,6 @@ def get_config(self):\n             rescale_embeddings=self.rescale_embeddings,\n             block_size=self.block_size,\n             num_random_blocks=self.num_rand_blocks,\n-            position_embedding_type=self.position_embedding_type,\n         )\n \n     def prepare_config_and_inputs_for_decoder(self):"
        },
        {
            "sha": "ece5c3c9918cd1504036fc8e2421fdab96b8741a",
            "filename": "tests/models/biogpt/test_modeling_biogpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -283,12 +283,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_biogpt_model_att_mask_past(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_biogpt_model_attention_mask_past(*config_and_inputs)"
        },
        {
            "sha": "efec72ca9b518105fb205197178834b50b867acc",
            "filename": "tests/models/bitnet/test_modeling_bitnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fbitnet%2Ftest_modeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fbitnet%2Ftest_modeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbitnet%2Ftest_modeling_bitnet.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -168,12 +168,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n \n @require_torch\n class BitNetIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "91b5b38619573e34d0294db6aefbb497ce4e8397",
            "filename": "tests/models/bros/test_modeling_bros.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fbros%2Ftest_modeling_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fbros%2Ftest_modeling_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbros%2Ftest_modeling_bros.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -353,12 +353,6 @@ def test_model(self):\n     def test_multi_gpu_data_parallel_forward(self):\n         super().test_multi_gpu_data_parallel_forward()\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_for_token_classification(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_token_classification(*config_and_inputs)"
        },
        {
            "sha": "f7b0eda6dd27aa6ddfca38bd22f2d06d7662904c",
            "filename": "tests/models/chinese_clip/test_modeling_chinese_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -342,12 +342,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_model_as_decoder(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)"
        },
        {
            "sha": "83553e8a107e193e7f3b0ae3b6b7ed2187099147",
            "filename": "tests/models/cohere/test_modeling_cohere.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -188,12 +188,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_torch_fx_output_loss(self):\n         super().test_torch_fx_output_loss()\n "
        },
        {
            "sha": "4709920e181716ccbb91e8b23d65d2ef36f51eab",
            "filename": "tests/models/data2vec/test_modeling_data2vec_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_text.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -404,13 +404,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            config_and_inputs[0]._attn_implementation = \"eager\"\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_model_as_decoder(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)\n@@ -450,12 +443,6 @@ def test_decoder_model_past_with_large_inputs(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n \n-    def test_decoder_model_past_with_large_inputs_relative_pos_emb(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n-        config_and_inputs[0].position_embedding_type = \"relative_key\"\n-        config_and_inputs[0]._attn_implementation = \"eager\"\n-        self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n-\n     def test_for_masked_lm(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)"
        },
        {
            "sha": "7f393cb1f3cc3c1d4c49655a6381826fc93b7284",
            "filename": "tests/models/dbrx/test_modeling_dbrx.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -97,12 +97,6 @@ class DbrxModelTest(CausalLMModelTest, unittest.TestCase):\n     )\n     model_tester_class = DbrxModelTester\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     @slow\n     def test_model_from_pretrained(self):\n         model_name = \"eitanturok/dbrx-tiny\""
        },
        {
            "sha": "9c4219fec811af027ea42cb6cc55bbd7214c09cd",
            "filename": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -280,12 +280,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     @parameterized.expand([(\"yarn\",)])\n     def test_model_rope_scaling_from_config(self, scaling_type):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "4aa6ab23b58560e10f912e7e556a7b090c24457e",
            "filename": "tests/models/diffllama/test_modeling_diffllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -218,12 +218,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_diffllama_sequence_classification_model(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.num_labels = 3"
        },
        {
            "sha": "e4695e596e66245de7165e516e2101aa82d607db",
            "filename": "tests/models/electra/test_modeling_electra.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Felectra%2Ftest_modeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Felectra%2Ftest_modeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Felectra%2Ftest_modeling_electra.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -439,13 +439,6 @@ def test_electra_model_as_decoder(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_electra_model_as_decoder(*config_and_inputs)\n \n-    def test_electra_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            config_and_inputs[0]._attn_implementation = \"eager\"\n-            self.model_tester.create_and_check_electra_model(*config_and_inputs)\n-\n     def test_for_masked_lm(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_electra_for_masked_lm(*config_and_inputs)"
        },
        {
            "sha": "68f84986054f99920f205bacbd1b469c50c02e1a",
            "filename": "tests/models/encoder_decoder/test_modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -801,27 +801,6 @@ def prepare_config_and_inputs(self):\n             \"labels\": decoder_token_labels,\n         }\n \n-    def test_relative_position_embeds(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-\n-        encoder_config = config_and_inputs[\"config\"]\n-        decoder_config = config_and_inputs[\"decoder_config\"]\n-\n-        encoder_config._attn_implementation = \"eager\"\n-        decoder_config._attn_implementation = \"eager\"\n-        encoder_config.position_embedding_type = \"relative_key_query\"\n-        decoder_config.position_embedding_type = \"relative_key_query\"\n-\n-        encoder_model, decoder_model = self.get_encoder_decoder_model(encoder_config, decoder_config)\n-        model = EncoderDecoderModel(encoder=encoder_model, decoder=decoder_model).eval().to(torch_device)\n-        model.config._attn_implementation = \"eager\"  # model config -> won't work\n-\n-        logits = model(\n-            input_ids=config_and_inputs[\"input_ids\"], decoder_input_ids=config_and_inputs[\"decoder_input_ids\"]\n-        ).logits\n-\n-        self.assertTrue(logits.shape, (13, 7))\n-\n     @slow\n     def test_bert2bert_summarization(self):\n         model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2bert-cnn_dailymail-fp16\")"
        },
        {
            "sha": "7eb89aad9cbb1d007130591a2ea7c2bdb3355e2f",
            "filename": "tests/models/ernie/test_modeling_ernie.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fernie%2Ftest_modeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fernie%2Ftest_modeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie%2Ftest_modeling_ernie.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -488,13 +488,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            config_and_inputs[0]._attn_implementation = \"eager\"\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_model_as_decoder(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)\n@@ -542,12 +535,6 @@ def test_decoder_model_past_with_large_inputs(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n \n-    def test_decoder_model_past_with_large_inputs_relative_pos_emb(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n-        config_and_inputs[0].position_embedding_type = \"relative_key\"\n-        config_and_inputs[0]._attn_implementation = \"eager\"\n-        self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n-\n     def test_for_multiple_choice(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_multiple_choice(*config_and_inputs)"
        },
        {
            "sha": "c4cbf971f036a4f2a14ffa3f5d88f515677b151c",
            "filename": "tests/models/esm/test_modeling_esm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -234,13 +234,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            config_and_inputs[0]._attn_implementation = \"eager\"\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_for_masked_lm(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)"
        },
        {
            "sha": "a80ab28e099b846b9649ea45cf2a7893416fc3fd",
            "filename": "tests/models/flava/test_modeling_flava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -338,7 +338,6 @@ def __init__(\n         vocab_size=102,\n         type_vocab_size=2,\n         max_position_embeddings=512,\n-        position_embedding_type=\"absolute\",\n         hidden_size=32,\n         num_hidden_layers=2,\n         num_attention_heads=4,\n@@ -360,7 +359,6 @@ def __init__(\n         self.vocab_size = vocab_size\n         self.type_vocab_size = type_vocab_size\n         self.max_position_embeddings = max_position_embeddings\n-        self.position_embedding_type = position_embedding_type\n         self.hidden_size = hidden_size\n         self.num_hidden_layers = num_hidden_layers\n         self.num_attention_heads = num_attention_heads\n@@ -401,7 +399,6 @@ def get_config(self):\n             vocab_size=self.vocab_size,\n             type_vocab_size=self.type_vocab_size,\n             max_position_embeddings=self.max_position_embeddings,\n-            position_embedding_type=self.position_embedding_type,\n             hidden_size=self.hidden_size,\n             num_hidden_layers=self.num_hidden_layers,\n             num_attention_heads=self.num_attention_heads,"
        },
        {
            "sha": "d9a7c00b8793009d1d90ace836326432188882bf",
            "filename": "tests/models/git/test_modeling_git.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -416,12 +416,6 @@ def test_batched_generate_captioning(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester._test_batched_generate_captioning(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def _check_attentions_for_generate(\n         self, batch_size, attentions, prompt_length, output_length, config, decoder_past_key_values\n     ):"
        },
        {
            "sha": "53f5ee5a2f925683e1708b9c3ac0d141e2878a7c",
            "filename": "tests/models/granite/test_modeling_granite.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -197,12 +197,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     @parameterized.expand([(\"linear\",), (\"dynamic\",)])\n     def test_model_rope_scaling_from_config(self, scaling_type):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "db87bec3ba905c22ba7a7ab261358efbab88e496",
            "filename": "tests/models/granite_speech/test_modeling_granite_speech.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -107,7 +107,6 @@ def __init__(\n             \"model_type\": \"blip_2_qformer\",\n             \"num_attention_heads\": 4,\n             \"num_hidden_layers\": 2,\n-            \"position_embedding_type\": \"absolute\",\n             \"use_qformer_text_input\": False,\n             \"vocab_size\": 30522,\n         },"
        },
        {
            "sha": "cf513daaa8ecea0559c87f711322febeaf723e19",
            "filename": "tests/models/granitemoe/test_modeling_granitemoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -196,12 +196,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     @parameterized.expand([(\"linear\",), (\"dynamic\",)])\n     def test_model_rope_scaling_from_config(self, scaling_type):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "eb941204bb1113ada8cd104128362116fcf39c81",
            "filename": "tests/models/granitemoeshared/test_modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fgranitemoeshared%2Ftest_modeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fgranitemoeshared%2Ftest_modeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoeshared%2Ftest_modeling_granitemoeshared.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -199,12 +199,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     @parameterized.expand([(\"linear\",), (\"dynamic\",)])\n     def test_model_rope_scaling_from_config(self, scaling_type):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "b56d193aa68ae07941f42dcf6ad86f305e53123d",
            "filename": "tests/models/ibert/test_modeling_ibert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fibert%2Ftest_modeling_ibert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fibert%2Ftest_modeling_ibert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fibert%2Ftest_modeling_ibert.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -264,13 +264,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        # I-BERT only supports absolute embedding\n-        for type in [\"absolute\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_for_masked_lm(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)"
        },
        {
            "sha": "422aaa22eb7ba6dabe0870a16dc5803510abf6a0",
            "filename": "tests/models/layoutlm/test_modeling_layoutlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Flayoutlm%2Ftest_modeling_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Flayoutlm%2Ftest_modeling_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlm%2Ftest_modeling_layoutlm.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -256,12 +256,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_for_masked_lm(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)"
        },
        {
            "sha": "e95aaad6b4b5138f7d4384270f8db3e6a36834d8",
            "filename": "tests/models/layoutlmv2/test_modeling_layoutlmv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Flayoutlmv2%2Ftest_modeling_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Flayoutlmv2%2Ftest_modeling_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv2%2Ftest_modeling_layoutlmv2.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -306,12 +306,6 @@ def test_model(self):\n     def test_multi_gpu_data_parallel_forward(self):\n         pass\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_for_sequence_classification(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_sequence_classification(*config_and_inputs)"
        },
        {
            "sha": "fedbd19756497b05b82bfe6d30b248e2eca4057a",
            "filename": "tests/models/layoutlmv3/test_modeling_layoutlmv3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Flayoutlmv3%2Ftest_modeling_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Flayoutlmv3%2Ftest_modeling_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_modeling_layoutlmv3.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -354,12 +354,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_for_sequence_classification(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_sequence_classification(*config_and_inputs)"
        },
        {
            "sha": "d88f47b3c0d6abe187b7463c18040f5804b6a2a2",
            "filename": "tests/models/lilt/test_modeling_lilt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Flilt%2Ftest_modeling_lilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Flilt%2Ftest_modeling_lilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flilt%2Ftest_modeling_lilt.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -265,12 +265,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_for_token_classification(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_token_classification(*config_and_inputs)"
        },
        {
            "sha": "d71586cdfbeda10671c06f93c2083215bf4b398a",
            "filename": "tests/models/modernbert/test_modeling_modernbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -294,12 +294,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_for_masked_lm(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)"
        },
        {
            "sha": "be4ff28ab06e76255ac64b4ddd674f691882d60f",
            "filename": "tests/models/mra/test_modeling_mra.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fmra%2Ftest_modeling_mra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fmra%2Ftest_modeling_mra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmra%2Ftest_modeling_mra.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -288,12 +288,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_for_masked_lm(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)"
        },
        {
            "sha": "11f7edc8b70175e93f7d2d4eca8c926dba0c7c6c",
            "filename": "tests/models/nystromformer/test_modeling_nystromformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fnystromformer%2Ftest_modeling_nystromformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fnystromformer%2Ftest_modeling_nystromformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnystromformer%2Ftest_modeling_nystromformer.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -252,12 +252,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_for_masked_lm(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)"
        },
        {
            "sha": "7c2802cc1cf69c910e1767f6b939f76bca927608",
            "filename": "tests/models/olmo/test_modeling_olmo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -190,12 +190,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     @parameterized.expand([(\"linear\",), (\"dynamic\",)])\n     def test_model_rope_scaling(self, scaling_type):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "cd0492221fbc9836f216302be821ad8f92c6e306",
            "filename": "tests/models/olmo2/test_modeling_olmo2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -191,12 +191,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     @parameterized.expand([(\"linear\",), (\"dynamic\",)])\n     def test_model_rope_scaling(self, scaling_type):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "47b6c605077af3f51be97b5265406880b2574e97",
            "filename": "tests/models/olmoe/test_modeling_olmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -202,12 +202,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     @parameterized.expand([(\"linear\",), (\"dynamic\",)])\n     def test_model_rope_scaling(self, scaling_type):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "e142f866f202cd65307f5226a7926c57c312e0dd",
            "filename": "tests/models/rembert/test_modeling_rembert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Frembert%2Ftest_modeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Frembert%2Ftest_modeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frembert%2Ftest_modeling_rembert.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -381,12 +381,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_for_masked_lm(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)"
        },
        {
            "sha": "99032b83e8ed74eeec6ea207ce98b7ef16c084f8",
            "filename": "tests/models/roberta/test_modeling_roberta.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -413,13 +413,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            config_and_inputs[0]._attn_implementation = \"eager\"\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_model_as_decoder(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)\n@@ -459,12 +452,6 @@ def test_decoder_model_past_with_large_inputs(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n \n-    def test_decoder_model_past_with_large_inputs_relative_pos_emb(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n-        config_and_inputs[0].position_embedding_type = \"relative_key\"\n-        config_and_inputs[0]._attn_implementation = \"eager\"\n-        self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n-\n     def test_for_masked_lm(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)"
        },
        {
            "sha": "f4d2adebfe52f944dbb0a479792b073288ee912b",
            "filename": "tests/models/roberta_prelayernorm/test_modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_roberta_prelayernorm.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -413,14 +413,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    # Copied from tests.models.roberta.test_modeling_roberta.RobertaModelTest.test_model_various_embeddings\n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            config_and_inputs[0]._attn_implementation = \"eager\"\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     # Copied from tests.models.roberta.test_modeling_roberta.RobertaModelTest.test_model_as_decoder\n     def test_model_as_decoder(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()"
        },
        {
            "sha": "dc92ffbefd22dc11a275a63874577c431e209acb",
            "filename": "tests/models/roc_bert/test_modeling_roc_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Froc_bert%2Ftest_modeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Froc_bert%2Ftest_modeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froc_bert%2Ftest_modeling_roc_bert.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -629,13 +629,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            config_and_inputs[0]._attn_implementation = \"eager\"\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_for_masked_lm(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)\n@@ -648,12 +641,6 @@ def test_decoder_model_past_with_large_inputs(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n \n-    def test_decoder_model_past_with_large_inputs_relative_pos_emb(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n-        config_and_inputs[0].position_embedding_type = \"relative_key\"\n-        config_and_inputs[0]._attn_implementation = \"eager\"\n-        self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n-\n     def test_for_question_answering(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_question_answering(*config_and_inputs)"
        },
        {
            "sha": "59d4537171b20981db4123eb064c78638e987270",
            "filename": "tests/models/splinter/test_modeling_splinter.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fsplinter%2Ftest_modeling_splinter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fsplinter%2Ftest_modeling_splinter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsplinter%2Ftest_modeling_splinter.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -283,12 +283,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_for_question_answering(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_question_answering(*config_and_inputs)"
        },
        {
            "sha": "a49419dd321f0221f66e2ef4f822f32c7d8a6d46",
            "filename": "tests/models/visual_bert/test_modeling_visual_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fvisual_bert%2Ftest_modeling_visual_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fvisual_bert%2Ftest_modeling_visual_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvisual_bert%2Ftest_modeling_visual_bert.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -522,12 +522,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_common()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs_for_common()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_model_for_pretraining(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_pretraining()\n         self.model_tester.create_and_check_for_pretraining(*config_and_inputs)"
        },
        {
            "sha": "e556dbf76a1da4f08e73111de65bba1e50e7bb25",
            "filename": "tests/models/xlm_roberta_xl/test_modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fxlm_roberta_xl%2Ftest_modeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fxlm_roberta_xl%2Ftest_modeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm_roberta_xl%2Ftest_modeling_xlm_roberta_xl.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -420,13 +420,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            config_and_inputs[0]._attn_implementation = \"eager\"\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_model_as_decoder(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)\n@@ -466,12 +459,6 @@ def test_decoder_model_past_with_large_inputs(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n \n-    def test_decoder_model_past_with_large_inputs_relative_pos_emb(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n-        config_and_inputs[0].position_embedding_type = \"relative_key\"\n-        config_and_inputs[0]._attn_implementation = \"eager\"\n-        self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n-\n     def test_for_masked_lm(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)"
        },
        {
            "sha": "583bd51e637588ae2acb0dcc7f6924d53ae72244",
            "filename": "tests/models/xmod/test_modeling_xmod.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fxmod%2Ftest_modeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fxmod%2Ftest_modeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxmod%2Ftest_modeling_xmod.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -418,13 +418,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            config_and_inputs[0]._attn_implementation = \"eager\"\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_model_as_decoder(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)\n@@ -464,12 +457,6 @@ def test_decoder_model_past_with_large_inputs(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n \n-    def test_decoder_model_past_with_large_inputs_relative_pos_emb(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n-        config_and_inputs[0].position_embedding_type = \"relative_key\"\n-        config_and_inputs[0]._attn_implementation = \"eager\"\n-        self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n-\n     def test_for_masked_lm(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)"
        },
        {
            "sha": "38202962626a4b2195c23b98d189af7975cb2a9f",
            "filename": "tests/models/yoso/test_modeling_yoso.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fyoso%2Ftest_modeling_yoso.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c27b67f0cdf043982dc299a6595dbc44ef29da58/tests%2Fmodels%2Fyoso%2Ftest_modeling_yoso.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fyoso%2Ftest_modeling_yoso.py?ref=c27b67f0cdf043982dc299a6595dbc44ef29da58",
            "patch": "@@ -286,12 +286,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_various_embeddings(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n-            config_and_inputs[0].position_embedding_type = type\n-            self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_for_masked_lm(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)"
        }
    ],
    "stats": {
        "total": 2665,
        "additions": 419,
        "deletions": 2246
    }
}