{
    "author": "MekkCyber",
    "message": "[kernels] Cleanup deta kernel (#41470)\n\n* cleanup deta kernel\n\n* fix modeling",
    "sha": "927aa8bef2f29296a34840b3562f9c03cc45ef81",
    "files": [
        {
            "sha": "388a73d22d4c9b561e2a887b50a1897b8cf2def9",
            "filename": "src/transformers/kernels/deta/cpu/ms_deform_attn_cpu.cpp",
            "status": "removed",
            "additions": 0,
            "deletions": 40,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/1951f3be8eb7f70afa408c61cb2c2f2c386d541a/src%2Ftransformers%2Fkernels%2Fdeta%2Fcpu%2Fms_deform_attn_cpu.cpp",
            "raw_url": "https://github.com/huggingface/transformers/raw/1951f3be8eb7f70afa408c61cb2c2f2c386d541a/src%2Ftransformers%2Fkernels%2Fdeta%2Fcpu%2Fms_deform_attn_cpu.cpp",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fdeta%2Fcpu%2Fms_deform_attn_cpu.cpp?ref=1951f3be8eb7f70afa408c61cb2c2f2c386d541a",
            "patch": "@@ -1,40 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#include <vector>\n-\n-#include <ATen/ATen.h>\n-#include <ATen/cuda/CUDAContext.h>\n-\n-\n-at::Tensor\n-ms_deform_attn_cpu_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step)\n-{\n-    AT_ERROR(\"Not implement on cpu\");\n-}\n-\n-std::vector<at::Tensor>\n-ms_deform_attn_cpu_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step)\n-{\n-    AT_ERROR(\"Not implement on cpu\");\n-}"
        },
        {
            "sha": "7eac8c8bcd1bf529bb9c13d54d2d4215c9e4c89f",
            "filename": "src/transformers/kernels/deta/cpu/ms_deform_attn_cpu.h",
            "status": "removed",
            "additions": 0,
            "deletions": 32,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/1951f3be8eb7f70afa408c61cb2c2f2c386d541a/src%2Ftransformers%2Fkernels%2Fdeta%2Fcpu%2Fms_deform_attn_cpu.h",
            "raw_url": "https://github.com/huggingface/transformers/raw/1951f3be8eb7f70afa408c61cb2c2f2c386d541a/src%2Ftransformers%2Fkernels%2Fdeta%2Fcpu%2Fms_deform_attn_cpu.h",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fdeta%2Fcpu%2Fms_deform_attn_cpu.h?ref=1951f3be8eb7f70afa408c61cb2c2f2c386d541a",
            "patch": "@@ -1,32 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#pragma once\n-#include <torch/extension.h>\n-\n-at::Tensor\n-ms_deform_attn_cpu_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step);\n-\n-std::vector<at::Tensor>\n-ms_deform_attn_cpu_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step);\n-"
        },
        {
            "sha": "8ea1d7fabe2684dbb85f00fae2c47b469687cb2c",
            "filename": "src/transformers/kernels/deta/cuda/ms_deform_attn_cuda.cu",
            "status": "removed",
            "additions": 0,
            "deletions": 156,
            "changes": 156,
            "blob_url": "https://github.com/huggingface/transformers/blob/1951f3be8eb7f70afa408c61cb2c2f2c386d541a/src%2Ftransformers%2Fkernels%2Fdeta%2Fcuda%2Fms_deform_attn_cuda.cu",
            "raw_url": "https://github.com/huggingface/transformers/raw/1951f3be8eb7f70afa408c61cb2c2f2c386d541a/src%2Ftransformers%2Fkernels%2Fdeta%2Fcuda%2Fms_deform_attn_cuda.cu",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fdeta%2Fcuda%2Fms_deform_attn_cuda.cu?ref=1951f3be8eb7f70afa408c61cb2c2f2c386d541a",
            "patch": "@@ -1,156 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#include <vector>\n-#include \"cuda/ms_deform_im2col_cuda.cuh\"\n-\n-#include <ATen/ATen.h>\n-#include <ATen/cuda/CUDAContext.h>\n-#include <cuda.h>\n-#include <cuda_runtime.h>\n-\n-#pragma once\n-#include <torch/extension.h>\n-\n-\n-at::Tensor ms_deform_attn_cuda_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step)\n-{\n-    AT_ASSERTM(value.is_contiguous(), \"value tensor has to be contiguous\");\n-    AT_ASSERTM(spatial_shapes.is_contiguous(), \"spatial_shapes tensor has to be contiguous\");\n-    AT_ASSERTM(level_start_index.is_contiguous(), \"level_start_index tensor has to be contiguous\");\n-    AT_ASSERTM(sampling_loc.is_contiguous(), \"sampling_loc tensor has to be contiguous\");\n-    AT_ASSERTM(attn_weight.is_contiguous(), \"attn_weight tensor has to be contiguous\");\n-\n-    AT_ASSERTM(value.type().is_cuda(), \"value must be a CUDA tensor\");\n-    AT_ASSERTM(spatial_shapes.type().is_cuda(), \"spatial_shapes must be a CUDA tensor\");\n-    AT_ASSERTM(level_start_index.type().is_cuda(), \"level_start_index must be a CUDA tensor\");\n-    AT_ASSERTM(sampling_loc.type().is_cuda(), \"sampling_loc must be a CUDA tensor\");\n-    AT_ASSERTM(attn_weight.type().is_cuda(), \"attn_weight must be a CUDA tensor\");\n-\n-    const int batch = value.size(0);\n-    const int spatial_size = value.size(1);\n-    const int num_heads = value.size(2);\n-    const int channels = value.size(3);\n-\n-    const int num_levels = spatial_shapes.size(0);\n-\n-    const int num_query = sampling_loc.size(1);\n-    const int num_point = sampling_loc.size(4);\n-\n-    const int im2col_step_ = std::min(batch, im2col_step);\n-\n-    AT_ASSERTM(batch % im2col_step_ == 0, \"batch(%d) must divide im2col_step(%d)\", batch, im2col_step_);\n-    \n-    auto output = at::zeros({batch, num_query, num_heads, channels}, value.options());\n-\n-    const int batch_n = im2col_step_;\n-    auto output_n = output.view({batch/im2col_step_, batch_n, num_query, num_heads, channels});\n-    auto per_value_size = spatial_size * num_heads * channels;\n-    auto per_sample_loc_size = num_query * num_heads * num_levels * num_point * 2;\n-    auto per_attn_weight_size = num_query * num_heads * num_levels * num_point;\n-    for (int n = 0; n < batch/im2col_step_; ++n)\n-    {\n-        auto columns = output_n.select(0, n);\n-        AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n-            ms_deformable_im2col_cuda(at::cuda::getCurrentCUDAStream(),\n-                value.data<scalar_t>() + n * im2col_step_ * per_value_size,\n-                spatial_shapes.data<int64_t>(),\n-                level_start_index.data<int64_t>(),\n-                sampling_loc.data<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                attn_weight.data<scalar_t>() + n * im2col_step_ * per_attn_weight_size,\n-                batch_n, spatial_size, num_heads, channels, num_levels, num_query, num_point,\n-                columns.data<scalar_t>());\n-\n-        }));\n-    }\n-\n-    output = output.view({batch, num_query, num_heads*channels});\n-\n-    return output;\n-}\n-\n-\n-std::vector<at::Tensor> ms_deform_attn_cuda_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step)\n-{\n-\n-    AT_ASSERTM(value.is_contiguous(), \"value tensor has to be contiguous\");\n-    AT_ASSERTM(spatial_shapes.is_contiguous(), \"spatial_shapes tensor has to be contiguous\");\n-    AT_ASSERTM(level_start_index.is_contiguous(), \"level_start_index tensor has to be contiguous\");\n-    AT_ASSERTM(sampling_loc.is_contiguous(), \"sampling_loc tensor has to be contiguous\");\n-    AT_ASSERTM(attn_weight.is_contiguous(), \"attn_weight tensor has to be contiguous\");\n-    AT_ASSERTM(grad_output.is_contiguous(), \"grad_output tensor has to be contiguous\");\n-\n-    AT_ASSERTM(value.type().is_cuda(), \"value must be a CUDA tensor\");\n-    AT_ASSERTM(spatial_shapes.type().is_cuda(), \"spatial_shapes must be a CUDA tensor\");\n-    AT_ASSERTM(level_start_index.type().is_cuda(), \"level_start_index must be a CUDA tensor\");\n-    AT_ASSERTM(sampling_loc.type().is_cuda(), \"sampling_loc must be a CUDA tensor\");\n-    AT_ASSERTM(attn_weight.type().is_cuda(), \"attn_weight must be a CUDA tensor\");\n-    AT_ASSERTM(grad_output.type().is_cuda(), \"grad_output must be a CUDA tensor\");\n-\n-    const int batch = value.size(0);\n-    const int spatial_size = value.size(1);\n-    const int num_heads = value.size(2);\n-    const int channels = value.size(3);\n-\n-    const int num_levels = spatial_shapes.size(0);\n-\n-    const int num_query = sampling_loc.size(1);\n-    const int num_point = sampling_loc.size(4);\n-\n-    const int im2col_step_ = std::min(batch, im2col_step);\n-\n-    AT_ASSERTM(batch % im2col_step_ == 0, \"batch(%d) must divide im2col_step(%d)\", batch, im2col_step_);\n-\n-    auto grad_value = at::zeros_like(value);\n-    auto grad_sampling_loc = at::zeros_like(sampling_loc);\n-    auto grad_attn_weight = at::zeros_like(attn_weight);\n-\n-    const int batch_n = im2col_step_;\n-    auto per_value_size = spatial_size * num_heads * channels;\n-    auto per_sample_loc_size = num_query * num_heads * num_levels * num_point * 2;\n-    auto per_attn_weight_size = num_query * num_heads * num_levels * num_point;\n-    auto grad_output_n = grad_output.view({batch/im2col_step_, batch_n, num_query, num_heads, channels});\n-    \n-    for (int n = 0; n < batch/im2col_step_; ++n)\n-    {\n-        auto grad_output_g = grad_output_n.select(0, n);\n-        AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n-            ms_deformable_col2im_cuda(at::cuda::getCurrentCUDAStream(),\n-                                    grad_output_g.data<scalar_t>(),\n-                                    value.data<scalar_t>() + n * im2col_step_ * per_value_size,\n-                                    spatial_shapes.data<int64_t>(),\n-                                    level_start_index.data<int64_t>(),\n-                                    sampling_loc.data<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                                    attn_weight.data<scalar_t>() + n * im2col_step_ * per_attn_weight_size,\n-                                    batch_n, spatial_size, num_heads, channels, num_levels, num_query, num_point,\n-                                    grad_value.data<scalar_t>() +  n * im2col_step_ * per_value_size,\n-                                    grad_sampling_loc.data<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                                    grad_attn_weight.data<scalar_t>() + n * im2col_step_ * per_attn_weight_size);\n-\n-        }));\n-    }\n-\n-    return {\n-        grad_value, grad_sampling_loc, grad_attn_weight\n-    };\n-}"
        },
        {
            "sha": "34f8ae9cb77bbaa8cb4dd25e0cb86632db9ad05d",
            "filename": "src/transformers/kernels/deta/cuda/ms_deform_attn_cuda.cuh",
            "status": "removed",
            "additions": 0,
            "deletions": 1467,
            "changes": 1467,
            "blob_url": "https://github.com/huggingface/transformers/blob/1951f3be8eb7f70afa408c61cb2c2f2c386d541a/src%2Ftransformers%2Fkernels%2Fdeta%2Fcuda%2Fms_deform_attn_cuda.cuh",
            "raw_url": "https://github.com/huggingface/transformers/raw/1951f3be8eb7f70afa408c61cb2c2f2c386d541a/src%2Ftransformers%2Fkernels%2Fdeta%2Fcuda%2Fms_deform_attn_cuda.cuh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fdeta%2Fcuda%2Fms_deform_attn_cuda.cuh?ref=1951f3be8eb7f70afa408c61cb2c2f2c386d541a",
            "patch": "@@ -1,1467 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#include <vector>\n-\n-#include <cuda.h>\n-#include <cuda_runtime.h>\n-\n-#include <cstdio>\n-#include <algorithm>\n-#include <cstring>\n-\n-#include <ATen/ATen.h>\n-#include <ATen/cuda/CUDAContext.h>\n-\n-#include <THC/THCAtomics.cuh>\n-\n-#define CUDA_KERNEL_LOOP(i, n)                          \\\n-  for (int i = blockIdx.x * blockDim.x + threadIdx.x;   \\\n-      i < (n);                                          \\\n-      i += blockDim.x * gridDim.x)\n-\n-\n-at::Tensor ms_deform_attn_cuda_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step)\n-{\n-    AT_ASSERTM(value.is_contiguous(), \"value tensor has to be contiguous\");\n-    AT_ASSERTM(spatial_shapes.is_contiguous(), \"spatial_shapes tensor has to be contiguous\");\n-    AT_ASSERTM(level_start_index.is_contiguous(), \"level_start_index tensor has to be contiguous\");\n-    AT_ASSERTM(sampling_loc.is_contiguous(), \"sampling_loc tensor has to be contiguous\");\n-    AT_ASSERTM(attn_weight.is_contiguous(), \"attn_weight tensor has to be contiguous\");\n-\n-    AT_ASSERTM(value.type().is_cuda(), \"value must be a CUDA tensor\");\n-    AT_ASSERTM(spatial_shapes.type().is_cuda(), \"spatial_shapes must be a CUDA tensor\");\n-    AT_ASSERTM(level_start_index.type().is_cuda(), \"level_start_index must be a CUDA tensor\");\n-    AT_ASSERTM(sampling_loc.type().is_cuda(), \"sampling_loc must be a CUDA tensor\");\n-    AT_ASSERTM(attn_weight.type().is_cuda(), \"attn_weight must be a CUDA tensor\");\n-\n-    const int batch = value.size(0);\n-    const int spatial_size = value.size(1);\n-    const int num_heads = value.size(2);\n-    const int channels = value.size(3);\n-\n-    const int num_levels = spatial_shapes.size(0);\n-\n-    const int num_query = sampling_loc.size(1);\n-    const int num_point = sampling_loc.size(4);\n-\n-    const int im2col_step_ = std::min(batch, im2col_step);\n-\n-    AT_ASSERTM(batch % im2col_step_ == 0, \"batch(%d) must divide im2col_step(%d)\", batch, im2col_step_);\n-    \n-    auto output = at::zeros({batch, num_query, num_heads, channels}, value.options());\n-\n-    const int batch_n = im2col_step_;\n-    auto output_n = output.view({batch/im2col_step_, batch_n, num_query, num_heads, channels});\n-    auto per_value_size = spatial_size * num_heads * channels;\n-    auto per_sample_loc_size = num_query * num_heads * num_levels * num_point * 2;\n-    auto per_attn_weight_size = num_query * num_heads * num_levels * num_point;\n-    for (int n = 0; n < batch/im2col_step_; ++n)\n-    {\n-        auto columns = output_n.select(0, n);\n-        AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n-            ms_deformable_im2col_cuda(at::cuda::getCurrentCUDAStream(),\n-                value.data<scalar_t>() + n * im2col_step_ * per_value_size,\n-                spatial_shapes.data<int64_t>(),\n-                level_start_index.data<int64_t>(),\n-                sampling_loc.data<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                attn_weight.data<scalar_t>() + n * im2col_step_ * per_attn_weight_size,\n-                batch_n, spatial_size, num_heads, channels, num_levels, num_query, num_point,\n-                columns.data<scalar_t>());\n-\n-        }));\n-    }\n-\n-    output = output.view({batch, num_query, num_heads*channels});\n-\n-    return output;\n-}\n-\n-\n-std::vector<at::Tensor> ms_deform_attn_cuda_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step)\n-{\n-\n-    AT_ASSERTM(value.is_contiguous(), \"value tensor has to be contiguous\");\n-    AT_ASSERTM(spatial_shapes.is_contiguous(), \"spatial_shapes tensor has to be contiguous\");\n-    AT_ASSERTM(level_start_index.is_contiguous(), \"level_start_index tensor has to be contiguous\");\n-    AT_ASSERTM(sampling_loc.is_contiguous(), \"sampling_loc tensor has to be contiguous\");\n-    AT_ASSERTM(attn_weight.is_contiguous(), \"attn_weight tensor has to be contiguous\");\n-    AT_ASSERTM(grad_output.is_contiguous(), \"grad_output tensor has to be contiguous\");\n-\n-    AT_ASSERTM(value.type().is_cuda(), \"value must be a CUDA tensor\");\n-    AT_ASSERTM(spatial_shapes.type().is_cuda(), \"spatial_shapes must be a CUDA tensor\");\n-    AT_ASSERTM(level_start_index.type().is_cuda(), \"level_start_index must be a CUDA tensor\");\n-    AT_ASSERTM(sampling_loc.type().is_cuda(), \"sampling_loc must be a CUDA tensor\");\n-    AT_ASSERTM(attn_weight.type().is_cuda(), \"attn_weight must be a CUDA tensor\");\n-    AT_ASSERTM(grad_output.type().is_cuda(), \"grad_output must be a CUDA tensor\");\n-\n-    const int batch = value.size(0);\n-    const int spatial_size = value.size(1);\n-    const int num_heads = value.size(2);\n-    const int channels = value.size(3);\n-\n-    const int num_levels = spatial_shapes.size(0);\n-\n-    const int num_query = sampling_loc.size(1);\n-    const int num_point = sampling_loc.size(4);\n-\n-    const int im2col_step_ = std::min(batch, im2col_step);\n-\n-    AT_ASSERTM(batch % im2col_step_ == 0, \"batch(%d) must divide im2col_step(%d)\", batch, im2col_step_);\n-\n-    auto grad_value = at::zeros_like(value);\n-    auto grad_sampling_loc = at::zeros_like(sampling_loc);\n-    auto grad_attn_weight = at::zeros_like(attn_weight);\n-\n-    const int batch_n = im2col_step_;\n-    auto per_value_size = spatial_size * num_heads * channels;\n-    auto per_sample_loc_size = num_query * num_heads * num_levels * num_point * 2;\n-    auto per_attn_weight_size = num_query * num_heads * num_levels * num_point;\n-    auto grad_output_n = grad_output.view({batch/im2col_step_, batch_n, num_query, num_heads, channels});\n-    \n-    for (int n = 0; n < batch/im2col_step_; ++n)\n-    {\n-        auto grad_output_g = grad_output_n.select(0, n);\n-        AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n-            ms_deformable_col2im_cuda(at::cuda::getCurrentCUDAStream(),\n-                                    grad_output_g.data<scalar_t>(),\n-                                    value.data<scalar_t>() + n * im2col_step_ * per_value_size,\n-                                    spatial_shapes.data<int64_t>(),\n-                                    level_start_index.data<int64_t>(),\n-                                    sampling_loc.data<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                                    attn_weight.data<scalar_t>() + n * im2col_step_ * per_attn_weight_size,\n-                                    batch_n, spatial_size, num_heads, channels, num_levels, num_query, num_point,\n-                                    grad_value.data<scalar_t>() +  n * im2col_step_ * per_value_size,\n-                                    grad_sampling_loc.data<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                                    grad_attn_weight.data<scalar_t>() + n * im2col_step_ * per_attn_weight_size);\n-\n-        }));\n-    }\n-\n-    return {\n-        grad_value, grad_sampling_loc, grad_attn_weight\n-    };\n-}\n-\n-const int CUDA_NUM_THREADS = 1024;\n-inline int GET_BLOCKS(const int N, const int num_threads)\n-{\n-  return (N + num_threads - 1) / num_threads;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ scalar_t ms_deform_attn_im2col_bilinear(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-  }\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  return val;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ void ms_deform_attn_col2im_bilinear(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c,\n-                                                   const scalar_t &top_grad,\n-                                                   const scalar_t &attn_weight,\n-                                                   scalar_t* &grad_value, \n-                                                   scalar_t* grad_sampling_loc,\n-                                                   scalar_t* grad_attn_weight)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-  const scalar_t top_grad_value = top_grad * attn_weight;\n-  scalar_t grad_h_weight = 0, grad_w_weight = 0;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-    grad_h_weight -= hw * v1;\n-    grad_w_weight -= hh * v1;\n-    atomicAdd(grad_value+ptr1, w1*top_grad_value);\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-    grad_h_weight -= lw * v2;\n-    grad_w_weight += hh * v2;\n-    atomicAdd(grad_value+ptr2, w2*top_grad_value);\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-    grad_h_weight += hw * v3;\n-    grad_w_weight -= lh * v3;\n-    atomicAdd(grad_value+ptr3, w3*top_grad_value); \n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-    grad_h_weight += lw * v4;\n-    grad_w_weight += lh * v4;\n-    atomicAdd(grad_value+ptr4, w4*top_grad_value);\n-  }\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  *grad_attn_weight = top_grad * val;\n-  *grad_sampling_loc = width * grad_w_weight * top_grad_value;\n-  *(grad_sampling_loc + 1) = height * grad_h_weight * top_grad_value;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ void ms_deform_attn_col2im_bilinear_gm(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c,\n-                                                   const scalar_t &top_grad,\n-                                                   const scalar_t &attn_weight,\n-                                                   scalar_t* &grad_value, \n-                                                   scalar_t* grad_sampling_loc,\n-                                                   scalar_t* grad_attn_weight)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-  const scalar_t top_grad_value = top_grad * attn_weight;\n-  scalar_t grad_h_weight = 0, grad_w_weight = 0;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-    grad_h_weight -= hw * v1;\n-    grad_w_weight -= hh * v1;\n-    atomicAdd(grad_value+ptr1, w1*top_grad_value);\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-    grad_h_weight -= lw * v2;\n-    grad_w_weight += hh * v2;\n-    atomicAdd(grad_value+ptr2, w2*top_grad_value);\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-    grad_h_weight += hw * v3;\n-    grad_w_weight -= lh * v3;\n-    atomicAdd(grad_value+ptr3, w3*top_grad_value); \n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-    grad_h_weight += lw * v4;\n-    grad_w_weight += lh * v4;\n-    atomicAdd(grad_value+ptr4, w4*top_grad_value);\n-  }\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  atomicAdd(grad_attn_weight, top_grad * val); \n-  atomicAdd(grad_sampling_loc, width * grad_w_weight * top_grad_value);\n-  atomicAdd(grad_sampling_loc + 1, height * grad_h_weight * top_grad_value);\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_im2col_gpu_kernel(const int n,\n-                                                const scalar_t *data_value, \n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *data_col)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    scalar_t *data_col_ptr = data_col + index;\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-    scalar_t col = 0;\n-    \n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const scalar_t *data_value_ptr = data_value + (data_value_ptr_init_offset + level_start_id * qid_stride);\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          col += ms_deform_attn_im2col_bilinear(data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col) * weight;\n-        }\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-      }\n-    }\n-    *data_col_ptr = col;\n-  }\n-}\n-\n-template <typename scalar_t, unsigned int blockSize>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    __shared__ scalar_t cache_grad_sampling_loc[blockSize * 2];\n-    __shared__ scalar_t cache_grad_attn_weight[blockSize];\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-        if (tid == 0)\n-        {\n-          scalar_t _grad_w=cache_grad_sampling_loc[0], _grad_h=cache_grad_sampling_loc[1], _grad_a=cache_grad_attn_weight[0];\n-          int sid=2;\n-          for (unsigned int tid = 1; tid < blockSize; ++tid)\n-          {\n-            _grad_w += cache_grad_sampling_loc[sid];\n-            _grad_h += cache_grad_sampling_loc[sid + 1];\n-            _grad_a += cache_grad_attn_weight[tid];\n-            sid += 2;\n-          }\n-          \n-          \n-          *grad_sampling_loc = _grad_w;\n-          *(grad_sampling_loc + 1) = _grad_h;\n-          *grad_attn_weight = _grad_a;\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t, unsigned int blockSize>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    __shared__ scalar_t cache_grad_sampling_loc[blockSize * 2];\n-    __shared__ scalar_t cache_grad_attn_weight[blockSize];\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockSize/2; s>0; s>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        { \n-          *grad_sampling_loc = cache_grad_sampling_loc[0];\n-          *(grad_sampling_loc + 1) = cache_grad_sampling_loc[1];\n-          *grad_attn_weight = cache_grad_attn_weight[0];\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v1(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-        if (tid == 0)\n-        {\n-          scalar_t _grad_w=cache_grad_sampling_loc[0], _grad_h=cache_grad_sampling_loc[1], _grad_a=cache_grad_attn_weight[0];\n-          int sid=2;\n-          for (unsigned int tid = 1; tid < blockDim.x; ++tid)\n-          {\n-            _grad_w += cache_grad_sampling_loc[sid];\n-            _grad_h += cache_grad_sampling_loc[sid + 1];\n-            _grad_a += cache_grad_attn_weight[tid];\n-            sid += 2;\n-          }\n-          \n-          \n-          *grad_sampling_loc = _grad_w;\n-          *(grad_sampling_loc + 1) = _grad_h;\n-          *grad_attn_weight = _grad_a;\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v2(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockDim.x/2, spre=blockDim.x; s>0; s>>=1, spre>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-            if (tid + (s << 1) < spre)\n-            {\n-              cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + (s << 1)];\n-              cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2 + (s << 1)];\n-              cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1 + (s << 1)];\n-            } \n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        {\n-          *grad_sampling_loc = cache_grad_sampling_loc[0];\n-          *(grad_sampling_loc + 1) = cache_grad_sampling_loc[1];\n-          *grad_attn_weight = cache_grad_attn_weight[0];\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v2_multi_blocks(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockDim.x/2, spre=blockDim.x; s>0; s>>=1, spre>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-            if (tid + (s << 1) < spre)\n-            {\n-              cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + (s << 1)];\n-              cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2 + (s << 1)];\n-              cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1 + (s << 1)];\n-            }\n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        {\n-          atomicAdd(grad_sampling_loc, cache_grad_sampling_loc[0]);\n-          atomicAdd(grad_sampling_loc + 1, cache_grad_sampling_loc[1]);\n-          atomicAdd(grad_attn_weight, cache_grad_attn_weight[0]);\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_gm(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear_gm(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            grad_sampling_loc, grad_attn_weight);\n-        }\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-void ms_deformable_im2col_cuda(cudaStream_t stream,\n-                              const scalar_t* data_value,\n-                              const int64_t* data_spatial_shapes, \n-                              const int64_t* data_level_start_index, \n-                              const scalar_t* data_sampling_loc,\n-                              const scalar_t* data_attn_weight,\n-                              const int batch_size,\n-                              const int spatial_size, \n-                              const int num_heads, \n-                              const int channels, \n-                              const int num_levels, \n-                              const int num_query,\n-                              const int num_point,\n-                              scalar_t* data_col)\n-{\n-  const int num_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_actual_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_threads = CUDA_NUM_THREADS;\n-  ms_deformable_im2col_gpu_kernel<scalar_t>\n-      <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-          0, stream>>>(\n-      num_kernels, data_value, data_spatial_shapes, data_level_start_index, data_sampling_loc, data_attn_weight, \n-      batch_size, spatial_size, num_heads, channels, num_levels, num_query, num_point, data_col);\n-  \n-  cudaError_t err = cudaGetLastError();\n-  if (err != cudaSuccess)\n-  {\n-    printf(\"error in ms_deformable_im2col_cuda: %s\\n\", cudaGetErrorString(err));\n-  }\n-\n-}\n-\n-template <typename scalar_t>\n-void ms_deformable_col2im_cuda(cudaStream_t stream,\n-                              const scalar_t* grad_col,\n-                              const scalar_t* data_value,\n-                              const int64_t * data_spatial_shapes,\n-                              const int64_t * data_level_start_index,\n-                              const scalar_t * data_sampling_loc,\n-                              const scalar_t * data_attn_weight,\n-                              const int batch_size, \n-                              const int spatial_size, \n-                              const int num_heads,\n-                              const int channels, \n-                              const int num_levels,\n-                              const int num_query,\n-                              const int num_point, \n-                              scalar_t* grad_value,\n-                              scalar_t* grad_sampling_loc,\n-                              scalar_t* grad_attn_weight)\n-{\n-  const int num_threads = (channels > CUDA_NUM_THREADS)?CUDA_NUM_THREADS:channels;\n-  const int num_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_actual_kernels = batch_size * num_query * num_heads * channels;\n-  if (channels > 1024)\n-  {\n-    if ((channels & 1023) == 0)\n-    {\n-      ms_deformable_col2im_gpu_kernel_shm_reduce_v2_multi_blocks<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-    }\n-    else\n-    {\n-      ms_deformable_col2im_gpu_kernel_gm<scalar_t>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-    }\n-  }\n-  else{\n-    switch(channels)\n-    {\n-      case 1:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 1>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 2:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 2>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 4:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 4>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 8:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 8>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 16:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 16>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 32:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 32>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 64:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 64>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 128:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 128>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 256:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 256>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 512:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 512>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 1024:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 1024>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      default:\n-        if (channels < 64)\n-        {\n-          ms_deformable_col2im_gpu_kernel_shm_reduce_v1<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-        }\n-        else\n-        {\n-          ms_deformable_col2im_gpu_kernel_shm_reduce_v2<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-        }\n-    }\n-  }\n-  cudaError_t err = cudaGetLastError();\n-  if (err != cudaSuccess)\n-  {\n-    printf(\"error in ms_deformable_col2im_cuda: %s\\n\", cudaGetErrorString(err));\n-  }\n-\n-}"
        },
        {
            "sha": "fbcf4543e66bb1162f42ce2ae57e1bac92243cb4",
            "filename": "src/transformers/kernels/deta/cuda/ms_deform_attn_cuda.h",
            "status": "removed",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/1951f3be8eb7f70afa408c61cb2c2f2c386d541a/src%2Ftransformers%2Fkernels%2Fdeta%2Fcuda%2Fms_deform_attn_cuda.h",
            "raw_url": "https://github.com/huggingface/transformers/raw/1951f3be8eb7f70afa408c61cb2c2f2c386d541a/src%2Ftransformers%2Fkernels%2Fdeta%2Fcuda%2Fms_deform_attn_cuda.h",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fdeta%2Fcuda%2Fms_deform_attn_cuda.h?ref=1951f3be8eb7f70afa408c61cb2c2f2c386d541a",
            "patch": "@@ -1,29 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#pragma once\n-#include <torch/extension.h>\n-\n-at::Tensor ms_deform_attn_cuda_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step);\n-\n-std::vector<at::Tensor> ms_deform_attn_cuda_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step);"
        },
        {
            "sha": "c0db0c88c9db2c09d7f601937ea0f6ac480913bf",
            "filename": "src/transformers/kernels/deta/cuda/ms_deform_im2col_cuda.cuh",
            "status": "removed",
            "additions": 0,
            "deletions": 1327,
            "changes": 1327,
            "blob_url": "https://github.com/huggingface/transformers/blob/1951f3be8eb7f70afa408c61cb2c2f2c386d541a/src%2Ftransformers%2Fkernels%2Fdeta%2Fcuda%2Fms_deform_im2col_cuda.cuh",
            "raw_url": "https://github.com/huggingface/transformers/raw/1951f3be8eb7f70afa408c61cb2c2f2c386d541a/src%2Ftransformers%2Fkernels%2Fdeta%2Fcuda%2Fms_deform_im2col_cuda.cuh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fdeta%2Fcuda%2Fms_deform_im2col_cuda.cuh?ref=1951f3be8eb7f70afa408c61cb2c2f2c386d541a",
            "patch": "@@ -1,1327 +0,0 @@\n-/*!\n-**************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************\n-* Modified from DCN (https://github.com/msracver/Deformable-ConvNets)\n-* Copyright (c) 2018 Microsoft\n-**************************************************************************\n-*/\n-\n-#include <cstdio>\n-#include <algorithm>\n-#include <cstring>\n-\n-#include <ATen/ATen.h>\n-#include <ATen/cuda/CUDAContext.h>\n-\n-#include <THC/THCAtomics.cuh>\n-\n-#define CUDA_KERNEL_LOOP(i, n)                          \\\n-  for (int i = blockIdx.x * blockDim.x + threadIdx.x;   \\\n-      i < (n);                                          \\\n-      i += blockDim.x * gridDim.x)\n-\n-const int CUDA_NUM_THREADS = 1024;\n-inline int GET_BLOCKS(const int N, const int num_threads)\n-{\n-  return (N + num_threads - 1) / num_threads;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ scalar_t ms_deform_attn_im2col_bilinear(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-  }\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  return val;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ void ms_deform_attn_col2im_bilinear(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c,\n-                                                   const scalar_t &top_grad,\n-                                                   const scalar_t &attn_weight,\n-                                                   scalar_t* &grad_value, \n-                                                   scalar_t* grad_sampling_loc,\n-                                                   scalar_t* grad_attn_weight)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-  const scalar_t top_grad_value = top_grad * attn_weight;\n-  scalar_t grad_h_weight = 0, grad_w_weight = 0;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-    grad_h_weight -= hw * v1;\n-    grad_w_weight -= hh * v1;\n-    atomicAdd(grad_value+ptr1, w1*top_grad_value);\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-    grad_h_weight -= lw * v2;\n-    grad_w_weight += hh * v2;\n-    atomicAdd(grad_value+ptr2, w2*top_grad_value);\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-    grad_h_weight += hw * v3;\n-    grad_w_weight -= lh * v3;\n-    atomicAdd(grad_value+ptr3, w3*top_grad_value); \n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-    grad_h_weight += lw * v4;\n-    grad_w_weight += lh * v4;\n-    atomicAdd(grad_value+ptr4, w4*top_grad_value);\n-  }\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  *grad_attn_weight = top_grad * val;\n-  *grad_sampling_loc = width * grad_w_weight * top_grad_value;\n-  *(grad_sampling_loc + 1) = height * grad_h_weight * top_grad_value;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ void ms_deform_attn_col2im_bilinear_gm(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c,\n-                                                   const scalar_t &top_grad,\n-                                                   const scalar_t &attn_weight,\n-                                                   scalar_t* &grad_value, \n-                                                   scalar_t* grad_sampling_loc,\n-                                                   scalar_t* grad_attn_weight)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-  const scalar_t top_grad_value = top_grad * attn_weight;\n-  scalar_t grad_h_weight = 0, grad_w_weight = 0;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-    grad_h_weight -= hw * v1;\n-    grad_w_weight -= hh * v1;\n-    atomicAdd(grad_value+ptr1, w1*top_grad_value);\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-    grad_h_weight -= lw * v2;\n-    grad_w_weight += hh * v2;\n-    atomicAdd(grad_value+ptr2, w2*top_grad_value);\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-    grad_h_weight += hw * v3;\n-    grad_w_weight -= lh * v3;\n-    atomicAdd(grad_value+ptr3, w3*top_grad_value); \n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-    grad_h_weight += lw * v4;\n-    grad_w_weight += lh * v4;\n-    atomicAdd(grad_value+ptr4, w4*top_grad_value);\n-  }\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  atomicAdd(grad_attn_weight, top_grad * val); \n-  atomicAdd(grad_sampling_loc, width * grad_w_weight * top_grad_value);\n-  atomicAdd(grad_sampling_loc + 1, height * grad_h_weight * top_grad_value);\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_im2col_gpu_kernel(const int n,\n-                                                const scalar_t *data_value, \n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *data_col)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    scalar_t *data_col_ptr = data_col + index;\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-    scalar_t col = 0;\n-    \n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const scalar_t *data_value_ptr = data_value + (data_value_ptr_init_offset + level_start_id * qid_stride);\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          col += ms_deform_attn_im2col_bilinear(data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col) * weight;\n-        }\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-      }\n-    }\n-    *data_col_ptr = col;\n-  }\n-}\n-\n-template <typename scalar_t, unsigned int blockSize>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    __shared__ scalar_t cache_grad_sampling_loc[blockSize * 2];\n-    __shared__ scalar_t cache_grad_attn_weight[blockSize];\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-        if (tid == 0)\n-        {\n-          scalar_t _grad_w=cache_grad_sampling_loc[0], _grad_h=cache_grad_sampling_loc[1], _grad_a=cache_grad_attn_weight[0];\n-          int sid=2;\n-          for (unsigned int tid = 1; tid < blockSize; ++tid)\n-          {\n-            _grad_w += cache_grad_sampling_loc[sid];\n-            _grad_h += cache_grad_sampling_loc[sid + 1];\n-            _grad_a += cache_grad_attn_weight[tid];\n-            sid += 2;\n-          }\n-          \n-          \n-          *grad_sampling_loc = _grad_w;\n-          *(grad_sampling_loc + 1) = _grad_h;\n-          *grad_attn_weight = _grad_a;\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t, unsigned int blockSize>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    __shared__ scalar_t cache_grad_sampling_loc[blockSize * 2];\n-    __shared__ scalar_t cache_grad_attn_weight[blockSize];\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockSize/2; s>0; s>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        { \n-          *grad_sampling_loc = cache_grad_sampling_loc[0];\n-          *(grad_sampling_loc + 1) = cache_grad_sampling_loc[1];\n-          *grad_attn_weight = cache_grad_attn_weight[0];\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v1(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-        if (tid == 0)\n-        {\n-          scalar_t _grad_w=cache_grad_sampling_loc[0], _grad_h=cache_grad_sampling_loc[1], _grad_a=cache_grad_attn_weight[0];\n-          int sid=2;\n-          for (unsigned int tid = 1; tid < blockDim.x; ++tid)\n-          {\n-            _grad_w += cache_grad_sampling_loc[sid];\n-            _grad_h += cache_grad_sampling_loc[sid + 1];\n-            _grad_a += cache_grad_attn_weight[tid];\n-            sid += 2;\n-          }\n-          \n-          \n-          *grad_sampling_loc = _grad_w;\n-          *(grad_sampling_loc + 1) = _grad_h;\n-          *grad_attn_weight = _grad_a;\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v2(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockDim.x/2, spre=blockDim.x; s>0; s>>=1, spre>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-            if (tid + (s << 1) < spre)\n-            {\n-              cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + (s << 1)];\n-              cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2 + (s << 1)];\n-              cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1 + (s << 1)];\n-            } \n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        {\n-          *grad_sampling_loc = cache_grad_sampling_loc[0];\n-          *(grad_sampling_loc + 1) = cache_grad_sampling_loc[1];\n-          *grad_attn_weight = cache_grad_attn_weight[0];\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v2_multi_blocks(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockDim.x/2, spre=blockDim.x; s>0; s>>=1, spre>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-            if (tid + (s << 1) < spre)\n-            {\n-              cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + (s << 1)];\n-              cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2 + (s << 1)];\n-              cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1 + (s << 1)];\n-            }\n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        {\n-          atomicAdd(grad_sampling_loc, cache_grad_sampling_loc[0]);\n-          atomicAdd(grad_sampling_loc + 1, cache_grad_sampling_loc[1]);\n-          atomicAdd(grad_attn_weight, cache_grad_attn_weight[0]);\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_gm(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear_gm(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            grad_sampling_loc, grad_attn_weight);\n-        }\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-void ms_deformable_im2col_cuda(cudaStream_t stream,\n-                              const scalar_t* data_value,\n-                              const int64_t* data_spatial_shapes, \n-                              const int64_t* data_level_start_index, \n-                              const scalar_t* data_sampling_loc,\n-                              const scalar_t* data_attn_weight,\n-                              const int batch_size,\n-                              const int spatial_size, \n-                              const int num_heads, \n-                              const int channels, \n-                              const int num_levels, \n-                              const int num_query,\n-                              const int num_point,\n-                              scalar_t* data_col)\n-{\n-  const int num_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_actual_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_threads = CUDA_NUM_THREADS;\n-  ms_deformable_im2col_gpu_kernel<scalar_t>\n-      <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-          0, stream>>>(\n-      num_kernels, data_value, data_spatial_shapes, data_level_start_index, data_sampling_loc, data_attn_weight, \n-      batch_size, spatial_size, num_heads, channels, num_levels, num_query, num_point, data_col);\n-  \n-  cudaError_t err = cudaGetLastError();\n-  if (err != cudaSuccess)\n-  {\n-    printf(\"error in ms_deformable_im2col_cuda: %s\\n\", cudaGetErrorString(err));\n-  }\n-\n-}\n-\n-template <typename scalar_t>\n-void ms_deformable_col2im_cuda(cudaStream_t stream,\n-                              const scalar_t* grad_col,\n-                              const scalar_t* data_value,\n-                              const int64_t * data_spatial_shapes,\n-                              const int64_t * data_level_start_index,\n-                              const scalar_t * data_sampling_loc,\n-                              const scalar_t * data_attn_weight,\n-                              const int batch_size, \n-                              const int spatial_size, \n-                              const int num_heads,\n-                              const int channels, \n-                              const int num_levels,\n-                              const int num_query,\n-                              const int num_point, \n-                              scalar_t* grad_value,\n-                              scalar_t* grad_sampling_loc,\n-                              scalar_t* grad_attn_weight)\n-{\n-  const int num_threads = (channels > CUDA_NUM_THREADS)?CUDA_NUM_THREADS:channels;\n-  const int num_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_actual_kernels = batch_size * num_query * num_heads * channels;\n-  if (channels > 1024)\n-  {\n-    if ((channels & 1023) == 0)\n-    {\n-      ms_deformable_col2im_gpu_kernel_shm_reduce_v2_multi_blocks<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-    }\n-    else\n-    {\n-      ms_deformable_col2im_gpu_kernel_gm<scalar_t>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-    }\n-  }\n-  else{\n-    switch(channels)\n-    {\n-      case 1:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 1>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 2:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 2>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 4:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 4>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 8:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 8>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 16:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 16>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 32:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 32>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 64:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 64>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 128:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 128>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 256:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 256>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 512:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 512>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 1024:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 1024>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      default:\n-        if (channels < 64)\n-        {\n-          ms_deformable_col2im_gpu_kernel_shm_reduce_v1<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-        }\n-        else\n-        {\n-          ms_deformable_col2im_gpu_kernel_shm_reduce_v2<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-        }\n-    }\n-  }\n-  cudaError_t err = cudaGetLastError();\n-  if (err != cudaSuccess)\n-  {\n-    printf(\"error in ms_deformable_col2im_cuda: %s\\n\", cudaGetErrorString(err));\n-  }\n-\n-}"
        },
        {
            "sha": "119b1fa317d1e5fcfb61a4837e560e9248db05f3",
            "filename": "src/transformers/kernels/deta/ms_deform_attn.h",
            "status": "removed",
            "additions": 0,
            "deletions": 61,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/1951f3be8eb7f70afa408c61cb2c2f2c386d541a/src%2Ftransformers%2Fkernels%2Fdeta%2Fms_deform_attn.h",
            "raw_url": "https://github.com/huggingface/transformers/raw/1951f3be8eb7f70afa408c61cb2c2f2c386d541a/src%2Ftransformers%2Fkernels%2Fdeta%2Fms_deform_attn.h",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fdeta%2Fms_deform_attn.h?ref=1951f3be8eb7f70afa408c61cb2c2f2c386d541a",
            "patch": "@@ -1,61 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#pragma once\n-\n-#include \"cpu/ms_deform_attn_cpu.h\"\n-\n-#ifdef WITH_CUDA\n-#include \"cuda/ms_deform_attn_cuda.h\"\n-#endif\n-\n-\n-at::Tensor\n-ms_deform_attn_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step)\n-{\n-    if (value.type().is_cuda())\n-    {\n-#ifdef WITH_CUDA\n-        return ms_deform_attn_cuda_forward(\n-            value, spatial_shapes, level_start_index, sampling_loc, attn_weight, im2col_step);\n-#else\n-        AT_ERROR(\"Not compiled with GPU support\");\n-#endif\n-    }\n-    AT_ERROR(\"Not implemented on the CPU\");\n-}\n-\n-std::vector<at::Tensor>\n-ms_deform_attn_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step)\n-{\n-    if (value.type().is_cuda())\n-    {\n-#ifdef WITH_CUDA\n-        return ms_deform_attn_cuda_backward(\n-            value, spatial_shapes, level_start_index, sampling_loc, attn_weight, grad_output, im2col_step);\n-#else\n-        AT_ERROR(\"Not compiled with GPU support\");\n-#endif\n-    }\n-    AT_ERROR(\"Not implemented on the CPU\");\n-}"
        },
        {
            "sha": "6ce3875568b9ba8d660c90acc805077cca98f891",
            "filename": "src/transformers/kernels/deta/vision.cpp",
            "status": "removed",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/1951f3be8eb7f70afa408c61cb2c2f2c386d541a/src%2Ftransformers%2Fkernels%2Fdeta%2Fvision.cpp",
            "raw_url": "https://github.com/huggingface/transformers/raw/1951f3be8eb7f70afa408c61cb2c2f2c386d541a/src%2Ftransformers%2Fkernels%2Fdeta%2Fvision.cpp",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fdeta%2Fvision.cpp?ref=1951f3be8eb7f70afa408c61cb2c2f2c386d541a",
            "patch": "@@ -1,16 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#include \"ms_deform_attn.h\"\n-\n-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n-  m.def(\"ms_deform_attn_forward\", &ms_deform_attn_forward, \"ms_deform_attn_forward\");\n-  m.def(\"ms_deform_attn_backward\", &ms_deform_attn_backward, \"ms_deform_attn_backward\");\n-}\n\\ No newline at end of file"
        },
        {
            "sha": "c2a9d0816e079781172a0ff0d1eeab992fde2674",
            "filename": "src/transformers/models/deprecated/deta/modeling_deta.py",
            "status": "modified",
            "additions": 59,
            "deletions": 103,
            "changes": 162,
            "blob_url": "https://github.com/huggingface/transformers/blob/927aa8bef2f29296a34840b3562f9c03cc45ef81/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/927aa8bef2f29296a34840b3562f9c03cc45ef81/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py?ref=927aa8bef2f29296a34840b3562f9c03cc45ef81",
            "patch": "@@ -16,119 +16,89 @@\n \n import copy\n import math\n-import os\n import warnings\n from dataclasses import dataclass\n-from pathlib import Path\n from typing import Optional, Union\n \n import torch\n import torch.nn.functional as F\n from torch import Tensor, nn\n-from torch.autograd import Function\n-from torch.autograd.function import once_differentiable\n \n from ....activations import ACT2FN\n from ....file_utils import (\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_scipy_available,\n-    is_torch_cuda_available,\n     is_vision_available,\n     replace_return_docstrings,\n )\n+from ....integrations.hub_kernels import use_kernel_forward_from_hub\n from ....modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import BaseModelOutput\n from ....modeling_utils import PreTrainedModel\n from ....pytorch_utils import meshgrid\n-from ....utils import is_accelerate_available, is_ninja_available, is_torchvision_available, logging, requires_backends\n+from ....utils import is_accelerate_available, is_torchvision_available, logging, requires_backends\n from ....utils.backbone_utils import load_backbone\n from .configuration_deta import DetaConfig\n \n \n logger = logging.get_logger(__name__)\n \n-MultiScaleDeformableAttention = None\n \n-\n-def load_cuda_kernels():\n-    from torch.utils.cpp_extension import load\n-\n-    global MultiScaleDeformableAttention\n-\n-    root = Path(__file__).resolve().parent.parent.parent.parent / \"kernels\" / \"deta\"\n-    src_files = [\n-        root / filename\n-        for filename in [\n-            \"vision.cpp\",\n-            os.path.join(\"cpu\", \"ms_deform_attn_cpu.cpp\"),\n-            os.path.join(\"cuda\", \"ms_deform_attn_cuda.cu\"),\n-        ]\n-    ]\n-\n-    MultiScaleDeformableAttention = load(\n-        \"MultiScaleDeformableAttention\",\n-        src_files,\n-        with_cuda=True,\n-        extra_include_paths=[str(root)],\n-        extra_cflags=[\"-DWITH_CUDA=1\"],\n-        extra_cuda_cflags=[\n-            \"-DCUDA_HAS_FP16=1\",\n-            \"-D__CUDA_NO_HALF_OPERATORS__\",\n-            \"-D__CUDA_NO_HALF_CONVERSIONS__\",\n-            \"-D__CUDA_NO_HALF2_OPERATORS__\",\n-        ],\n-    )\n-\n-\n-class MultiScaleDeformableAttentionFunction(Function):\n-    @staticmethod\n+@use_kernel_forward_from_hub(\"MultiScaleDeformableAttention\")\n+class MultiScaleDeformableAttention(nn.Module):\n     def forward(\n-        context,\n-        value,\n-        value_spatial_shapes,\n-        value_level_start_index,\n-        sampling_locations,\n-        attention_weights,\n-        im2col_step,\n+        self,\n+        value: Tensor,\n+        value_spatial_shapes: Tensor,\n+        level_start_index: Tensor,\n+        sampling_locations: Tensor,\n+        attention_weights: Tensor,\n+        im2col_step: int,\n     ):\n-        context.im2col_step = im2col_step\n-        output = MultiScaleDeformableAttention.ms_deform_attn_forward(\n-            value,\n-            value_spatial_shapes,\n-            value_level_start_index,\n-            sampling_locations,\n-            attention_weights,\n-            context.im2col_step,\n-        )\n-        context.save_for_backward(\n-            value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights\n+        batch_size, _, num_heads, hidden_dim = value.shape\n+        _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n+        value_list = value.split([height * width for height, width in value_spatial_shapes], dim=1)\n+        sampling_grids = 2 * sampling_locations - 1\n+        sampling_value_list = []\n+        for level_id, (height, width) in enumerate(value_spatial_shapes):\n+            # batch_size, height*width, num_heads, hidden_dim\n+            # -> batch_size, height*width, num_heads*hidden_dim\n+            # -> batch_size, num_heads*hidden_dim, height*width\n+            # -> batch_size*num_heads, hidden_dim, height, width\n+            value_l_ = (\n+                value_list[level_id]\n+                .flatten(2)\n+                .transpose(1, 2)\n+                .reshape(batch_size * num_heads, hidden_dim, height, width)\n+            )\n+            # batch_size, num_queries, num_heads, num_points, 2\n+            # -> batch_size, num_heads, num_queries, num_points, 2\n+            # -> batch_size*num_heads, num_queries, num_points, 2\n+            sampling_grid_l_ = sampling_grids[:, :, :, level_id].transpose(1, 2).flatten(0, 1)\n+            # batch_size*num_heads, hidden_dim, num_queries, num_points\n+            sampling_value_l_ = nn.functional.grid_sample(\n+                value_l_,\n+                sampling_grid_l_,\n+                mode=\"bilinear\",\n+                padding_mode=\"zeros\",\n+                align_corners=False,\n+            )\n+            sampling_value_list.append(sampling_value_l_)\n+        # (batch_size, num_queries, num_heads, num_levels, num_points)\n+        # -> (batch_size, num_heads, num_queries, num_levels, num_points)\n+        # -> (batch_size, num_heads, 1, num_queries, num_levels*num_points)\n+        attention_weights = attention_weights.transpose(1, 2).reshape(\n+            batch_size * num_heads, 1, num_queries, num_levels * num_points\n         )\n-        return output\n-\n-    @staticmethod\n-    @once_differentiable\n-    def backward(context, grad_output):\n-        (\n-            value,\n-            value_spatial_shapes,\n-            value_level_start_index,\n-            sampling_locations,\n-            attention_weights,\n-        ) = context.saved_tensors\n-        grad_value, grad_sampling_loc, grad_attn_weight = MultiScaleDeformableAttention.ms_deform_attn_backward(\n-            value,\n-            value_spatial_shapes,\n-            value_level_start_index,\n-            sampling_locations,\n-            attention_weights,\n-            grad_output,\n-            context.im2col_step,\n+        output = (\n+            (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights)\n+            .sum(-1)\n+            .view(batch_size, num_heads * hidden_dim, num_queries)\n         )\n-\n-        return grad_value, None, None, grad_sampling_loc, grad_attn_weight, None\n+        return output.transpose(1, 2).contiguous()\n \n \n if is_accelerate_available():\n@@ -571,12 +541,7 @@ class DetaMultiscaleDeformableAttention(nn.Module):\n     def __init__(self, config: DetaConfig, num_heads: int, n_points: int):\n         super().__init__()\n \n-        kernel_loaded = MultiScaleDeformableAttention is not None\n-        if is_torch_cuda_available() and is_ninja_available() and not kernel_loaded:\n-            try:\n-                load_cuda_kernels()\n-            except Exception as e:\n-                logger.warning(f\"Could not load the custom kernel for multi-scale deformable attention: {e}\")\n+        self.attn = MultiScaleDeformableAttention()\n \n         if config.d_model % num_heads != 0:\n             raise ValueError(\n@@ -684,23 +649,14 @@ def forward(\n         else:\n             raise ValueError(f\"Last dim of reference_points must be 2 or 4, but got {reference_points.shape[-1]}\")\n \n-        if self.disable_custom_kernels:\n-            # PyTorch implementation\n-            output = multi_scale_deformable_attention(value, spatial_shapes, sampling_locations, attention_weights)\n-        else:\n-            try:\n-                # custom kernel\n-                output = MultiScaleDeformableAttentionFunction.apply(\n-                    value,\n-                    spatial_shapes,\n-                    level_start_index,\n-                    sampling_locations,\n-                    attention_weights,\n-                    self.im2col_step,\n-                )\n-            except Exception:\n-                # PyTorch implementation\n-                output = multi_scale_deformable_attention(value, spatial_shapes, sampling_locations, attention_weights)\n+        output = self.attn(\n+            value,\n+            spatial_shapes,\n+            level_start_index,\n+            sampling_locations,\n+            attention_weights,\n+            self.im2col_step,\n+        )\n         output = self.output_proj(output)\n \n         return output, attention_weights"
        }
    ],
    "stats": {
        "total": 3290,
        "additions": 59,
        "deletions": 3231
    }
}