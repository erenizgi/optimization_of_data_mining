{
    "author": "eustlb",
    "message": "add _keep_in_fp32_modules_strict (#39058)\n\n* add _keep_in_fp32_modules_strict\n\n* complete test",
    "sha": "02ecdcfc0f7d81e90a9c8e7f9e6d636123a84254",
    "files": [
        {
            "sha": "515fb6d381196754d91e99b5e3ef2c221a10cf2b",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 33,
            "deletions": 15,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/02ecdcfc0f7d81e90a9c8e7f9e6d636123a84254/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/02ecdcfc0f7d81e90a9c8e7f9e6d636123a84254/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=02ecdcfc0f7d81e90a9c8e7f9e6d636123a84254",
            "patch": "@@ -1937,7 +1937,11 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, PushToHubMixin, PeftAdapterMi\n     _auto_class = None\n     _no_split_modules = None\n     _skip_keys_device_placement = None\n+\n     _keep_in_fp32_modules = None\n+    # the _keep_in_fp32_modules will avoid casting to anything other than float32, except bfloat16\n+    # to also prevent bfloat16 casting, use the _keep_in_fp32_modules_strict flag\n+    _keep_in_fp32_modules_strict = None\n \n     # a list of `re` patterns of `state_dict` keys that should be removed from the list of missing\n     # keys we find (keys inside the model but not in the checkpoint) and avoid unnecessary warnings.\n@@ -2049,6 +2053,7 @@ def __init__(self, config: PretrainedConfig, *inputs, **kwargs):\n         # `InstructBlipForConditionalGeneration` can dynamically update it without modifying the class attribute\n         # when a different component (e.g. language_model) is used.\n         self._keep_in_fp32_modules = copy.copy(self.__class__._keep_in_fp32_modules)\n+        self._keep_in_fp32_modules_strict = copy.copy(self.__class__._keep_in_fp32_modules_strict)\n \n         self._no_split_modules = self._no_split_modules or []\n \n@@ -2061,7 +2066,7 @@ def post_init(self):\n         self._backward_compatibility_gradient_checkpointing()\n \n         # Make sure the modules correctly exist if the flag is active\n-        if self._keep_in_fp32_modules is not None:\n+        if self._keep_in_fp32_modules is not None or self._keep_in_fp32_modules_strict is not None:\n             all_parameters = {name for name, _ in self.named_parameters() if len(name) > 0}\n             unique_module_names = set()\n             # Get all unique module names in the module graph, without the prefixes\n@@ -2070,12 +2075,21 @@ def post_init(self):\n                     [name for name in param.split(\".\") if not name.isnumeric() and name not in [\"weight\", \"bias\"]]\n                 )\n             # Check that every module in the keep_in_fp32 list is part of the module graph\n-            for module in self._keep_in_fp32_modules:\n-                if module not in unique_module_names:\n-                    raise ValueError(\n-                        f\"{module} was specified in the `_keep_in_fp32_modules` list, but is not part of the modules in\"\n-                        f\" {self.__class__.__name__}\"\n-                    )\n+            if self._keep_in_fp32_modules is not None:\n+                for module in self._keep_in_fp32_modules:\n+                    if module not in unique_module_names:\n+                        raise ValueError(\n+                            f\"{module} was specified in the `_keep_in_fp32_modules` list, but is not part of the modules in\"\n+                            f\" {self.__class__.__name__}\"\n+                        )\n+\n+            if self._keep_in_fp32_modules_strict is not None:\n+                for module in self._keep_in_fp32_modules_strict:\n+                    if module not in unique_module_names:\n+                        raise ValueError(\n+                            f\"{module} was specified in the `_keep_in_fp32_modules_strict` list, but is not part of the modules in\"\n+                            f\" {self.__class__.__name__}\"\n+                        )\n \n         # If current model is a base model, attach `base_model_tp_plan` and `base_model_pp_plan` from config\n         self._pp_plan = self.config.base_model_pp_plan.copy() if self.config.base_model_pp_plan is not None else None\n@@ -4757,20 +4771,24 @@ def from_pretrained(\n         config = model.config\n \n         # Find fp32 modules if needed\n-        keep_in_fp32_regex = None\n+        keep_in_fp32_modules = []\n         # The _keep_in_fp32_modules flag is only used to avoid bf16 -> fp16 casting precision issues. It was introduced\n         # in case of force loading a model that should stay bf16 in fp16 (which includes a few quantizers as this is a pre-processing\n         # step for e.g. bitsandbytes). See https://github.com/huggingface/transformers/issues/20287 for details.\n-        # Update: to extend _keep_in_fp32_modules flag feature, it can also be used to force modules that should stay in fp32\n         if model._keep_in_fp32_modules is not None and (\n-            torch_dtype == torch.float16\n-            or torch_dtype == torch.bfloat16\n-            or getattr(hf_quantizer, \"use_keep_in_fp32_modules\", False)\n+            torch_dtype == torch.float16 or getattr(hf_quantizer, \"use_keep_in_fp32_modules\", False)\n+        ):\n+            keep_in_fp32_modules.extend(model._keep_in_fp32_modules)\n+\n+        if model._keep_in_fp32_modules_strict is not None and (\n+            torch_dtype == torch.float16 or torch_dtype == torch.bfloat16\n         ):\n+            keep_in_fp32_modules.extend(model._keep_in_fp32_modules_strict)\n+\n+        keep_in_fp32_regex = None\n+        if keep_in_fp32_modules:\n             # We need to match exact layers, so we add either `.` on each side, or start/end of string\n-            keep_in_fp32_regex = re.compile(\n-                \"|\".join([rf\"((^|\\.){module}($|\\.))\" for module in model._keep_in_fp32_modules])\n-            )\n+            keep_in_fp32_regex = re.compile(\"|\".join([rf\"((^|\\.){module}($|\\.))\" for module in keep_in_fp32_modules]))\n \n         if hf_quantizer is not None:\n             hf_quantizer.preprocess_model("
        },
        {
            "sha": "5abc0bd3fc018ecc8d351c64578014ba8fa76240",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/02ecdcfc0f7d81e90a9c8e7f9e6d636123a84254/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/02ecdcfc0f7d81e90a9c8e7f9e6d636123a84254/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=02ecdcfc0f7d81e90a9c8e7f9e6d636123a84254",
            "patch": "@@ -1103,7 +1103,7 @@ class KyutaiSpeechToTextForConditionalGeneration(KyutaiSpeechToTextPreTrainedMod\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n-    _keep_in_fp32_modules = [\"codec_model\"]\n+    _keep_in_fp32_modules_strict = [\"codec_model\"]\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "4929c9e4bae116fa0b36279d655a253fdf7f8e58",
            "filename": "src/transformers/models/kyutai_speech_to_text/modular_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/02ecdcfc0f7d81e90a9c8e7f9e6d636123a84254/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/02ecdcfc0f7d81e90a9c8e7f9e6d636123a84254/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py?ref=02ecdcfc0f7d81e90a9c8e7f9e6d636123a84254",
            "patch": "@@ -252,7 +252,7 @@ def __init__(self, config):\n \n \n class KyutaiSpeechToTextForConditionalGeneration(LlamaForCausalLM, GenerationMixin, PreTrainedModel):\n-    _keep_in_fp32_modules = [\"codec_model\"]\n+    _keep_in_fp32_modules_strict = [\"codec_model\"]\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "780658c77af759254a905f6c4b3f8fa66c9ee91f",
            "filename": "tests/models/kyutai_speech_to_text/test_modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 76,
            "deletions": 0,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/02ecdcfc0f7d81e90a9c8e7f9e6d636123a84254/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/02ecdcfc0f7d81e90a9c8e7f9e6d636123a84254/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py?ref=02ecdcfc0f7d81e90a9c8e7f9e6d636123a84254",
            "patch": "@@ -30,6 +30,7 @@\n )\n from transformers.testing_utils import (\n     cleanup,\n+    require_accelerate,\n     require_torch,\n     require_torch_accelerator,\n     require_torch_sdpa,\n@@ -615,6 +616,81 @@ def _test_attention_implementation(self, attn_implementation):\n                 self._check_similar_generate_outputs(res_eager, res_attn, atol=1e-3, rtol=1e-3)\n \n \n+@require_torch\n+@require_accelerate\n+@slow\n+class KyutaiSpeechToTextBf16Test(unittest.TestCase):\n+    def test_bf16_fp32_conversion(self):\n+        r\"\"\"\n+        A test to check whether the argument `keep_in_fp32_modules` correctly does its job\n+        \"\"\"\n+        model_checkpoint = \"kyutai/stt-2.6b-en-trfs\"\n+        orig_import = __import__\n+        accelerate_mock = unittest.mock.Mock()\n+\n+        # mock import of accelerate\n+        def import_accelerate_mock(name, *args, **kwargs):\n+            if name == \"accelerate\":\n+                if accelerate_available:\n+                    return accelerate_mock\n+                else:\n+                    raise ImportError\n+            return orig_import(name, *args, **kwargs)\n+\n+        # Load without using `accelerate`\n+        with unittest.mock.patch(\"builtins.__import__\", side_effect=import_accelerate_mock):\n+            accelerate_available = False\n+\n+            model = KyutaiSpeechToTextForConditionalGeneration.from_pretrained(\n+                model_checkpoint, torch_dtype=torch.float16\n+            )\n+            self.assertTrue(model.codec_model.dtype == torch.float32)\n+            self.assertTrue(model.model.dtype == torch.float16)\n+            self.assertTrue(model.lm_head.weight.data.dtype == torch.float16)\n+\n+            # Load without in bf16\n+            model = KyutaiSpeechToTextForConditionalGeneration.from_pretrained(\n+                model_checkpoint, torch_dtype=torch.bfloat16\n+            )\n+            self.assertTrue(model.codec_model.dtype == torch.float32)\n+            self.assertTrue(model.model.dtype == torch.bfloat16)\n+            self.assertTrue(model.lm_head.weight.data.dtype == torch.bfloat16)\n+\n+        # Load using `accelerate` in bf16\n+        model = KyutaiSpeechToTextForConditionalGeneration.from_pretrained(\n+            model_checkpoint, torch_dtype=torch.bfloat16, device_map=\"auto\"\n+        )\n+        self.assertTrue(model.codec_model.dtype == torch.float32)\n+        self.assertTrue(model.model.dtype == torch.bfloat16)\n+        self.assertTrue(model.lm_head.weight.data.dtype == torch.bfloat16)\n+\n+        # Load using `accelerate` in bf16\n+        model = KyutaiSpeechToTextForConditionalGeneration.from_pretrained(\n+            model_checkpoint,\n+            torch_dtype=torch.bfloat16,\n+        )\n+        self.assertTrue(model.codec_model.dtype == torch.float32)\n+        self.assertTrue(model.model.dtype == torch.bfloat16)\n+        self.assertTrue(model.lm_head.weight.data.dtype == torch.bfloat16)\n+\n+        # Load without using `accelerate`\n+        model = KyutaiSpeechToTextForConditionalGeneration.from_pretrained(\n+            model_checkpoint,\n+            torch_dtype=torch.float16,\n+        )\n+        self.assertTrue(model.codec_model.dtype == torch.float32)\n+        self.assertTrue(model.model.dtype == torch.float16)\n+        self.assertTrue(model.lm_head.weight.data.dtype == torch.float16)\n+\n+        # Load using `accelerate`\n+        model = KyutaiSpeechToTextForConditionalGeneration.from_pretrained(\n+            model_checkpoint, torch_dtype=torch.float16, device_map=\"auto\"\n+        )\n+        self.assertTrue(model.codec_model.dtype == torch.float32)\n+        self.assertTrue(model.model.dtype == torch.float16)\n+        self.assertTrue(model.lm_head.weight.data.dtype == torch.float16)\n+\n+\n class KyutaiSpeechToTextForConditionalGenerationIntegrationTests(unittest.TestCase):\n     _dataset = None\n "
        }
    ],
    "stats": {
        "total": 128,
        "additions": 111,
        "deletions": 17
    }
}