{
    "author": "Cyrilvallez",
    "message": "Fix AttentionInterface following feedback (#37010)\n\n* up\n\n* typo\n\n* update doc\n\n* Update attention_interface.md",
    "sha": "2bea6bf24e4bb2da0b0652a2b8ae8450d52193de",
    "files": [
        {
            "sha": "054a0e471314bfb35becab7ec4e5c322572f8b61",
            "filename": "docs/source/en/attention_interface.md",
            "status": "modified",
            "additions": 30,
            "deletions": 8,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bea6bf24e4bb2da0b0652a2b8ae8450d52193de/docs%2Fsource%2Fen%2Fattention_interface.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bea6bf24e4bb2da0b0652a2b8ae8450d52193de/docs%2Fsource%2Fen%2Fattention_interface.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fattention_interface.md?ref=2bea6bf24e4bb2da0b0652a2b8ae8450d52193de",
            "patch": "@@ -23,13 +23,13 @@ supported models.\n Most recent models can now switch from one attention function used in the Attention layer to the other, thanks to a simple mapping.\n By default, we provide the implementation for [`sdpa`](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html),\n [`flash_attention_2`](https://github.com/Dao-AILab/flash-attention) and [`flex_attention`](https://pytorch.org/docs/stable/nn.attention.flex_attention.html#module-torch.nn.attention.flex_attention)\n-as well as `eager`, which is simple matrix multiplication without any optimization on top.  \n+as well as `eager`, which is a simple matrix multiplication without any optimization on top.  \n This is the setting you can usually choose when instantiating a model:\n \n ```python\n from transformers import AutoModelForCausalLM\n \n-model_id = \"meta-llama/Llama-3.2-1B\n+model_id = \"meta-llama/Llama-3.2-1B\"\n \n # Here, using flash attention as an example\n model = AutoModelForCausalLM.from_pretrained(model_id, attn_implementation=\"flash_attention_2\")\n@@ -43,7 +43,7 @@ from transformers import AutoModelForCausalLM, AttentionInterface\n from transformers.integrations.sdpa_attention import sdpa_attention_forward\n import torch\n \n-model_id = \"meta-llama/Llama-3.2-1B\n+model_id = \"meta-llama/Llama-3.2-1B\"\n \n def my_new_sdpa(*args, **kwargs):\n     print(\"I just entered the attention computation\")\n@@ -56,7 +56,7 @@ model = AutoModelForCausalLM.from_pretrained(model_id, attn_implementation=\"my_n\n model(torch.ones(1, 5, dtype=int))\n ```\n \n-You will see it prints \"I just entered the attention computation\" as many times as there are layers in the model (with this example, 16 times.\n+You will see it prints \"I just entered the attention computation\" as many times as there are layers in the model (with this example, 16 times).\n \n ## Dynamically switching attention function\n \n@@ -70,12 +70,12 @@ model(torch.ones(1, 5, dtype=int))\n ```\n \n and it will stop printing the statements, as it now uses the `sdpa` attention.  \n-This allows to quickly change attention function, without needing to reload the model!\n+This allows to quickly change an attention function, without needing to reload the model!\n \n-## What about new args needed in my custom function?\n+## What about new args needed in my custom attention function?\n \n But indeed, what if the new function requires a new arg to be properly used? It's no issue! Models supporting the\n-`AttentionInterface` propagates kwargs all the way to the Attention layers, and to the attention function used. That way,\n+`AttentionInterface` propagate kwargs all the way to the Attention layers, and to the used attention function. That way,\n you can simply pass the arg (as a kwargs, i.e. you need to qualify the name of the arg) in the model's forward, and it will be correctly used in the attention. However, custom attention functions have some limitations. In particular, it must follow the signature and return format of other attention functions, i.e.\n \n ```python\n@@ -103,4 +103,26 @@ model = AutoModelForCausalLM.from_pretrained(model_id, attn_implementation=\"cust\n model(torch.ones(1, 5, dtype=int), a_new_kwargs=..., another_new_kwargs=...)\n ```\n \n-If in doubt about what args/kwargs a given model sends to the attention function, simply check that model's modeling code on [GitHub](https://github.com/huggingface/transformers/tree/main/src/transformers/models)!\n\\ No newline at end of file\n+If in doubt about what args/kwargs a given model sends to the attention function, simply check that model's modeling code on [GitHub](https://github.com/huggingface/transformers/tree/main/src/transformers/models)!\n+\n+## Accessing current available implementations\n+\n+Most of the time, you will simply need to `register` a new function. If, however, you need to access an existing one,\n+and/or perform a few checks, the prefered way is to use the global `ALL_ATTENTION_FUNCTIONS`. It behaves the same way you\n+would expect from a usual Python dictionary:\n+\n+```python\n+>>> from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS\n+\n+>>> list(ALL_ATTENTION_FUNCTIONS.keys())\n+>>> ['flash_attention_2', 'flex_attention', 'sdpa']\n+\n+>>> ALL_ATTENTION_FUNCTIONS[\"sdpa\"]\n+>>> <function transformers.integrations.sdpa_attention.sdpa_attention_forward>\n+\n+>>> ALL_ATTENTION_FUNCTIONS.get(\"sdpa\", None)\n+>>> <function transformers.integrations.sdpa_attention.sdpa_attention_forward>\n+\n+# You can also globally `register` a new function directly on it\n+>>> ALL_ATTENTION_FUNCTIONS.register(\"new_func\", new_func)\n+```\n\\ No newline at end of file"
        },
        {
            "sha": "8e63c7a6af8450aff2202e1a487551343fa0028a",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bea6bf24e4bb2da0b0652a2b8ae8450d52193de/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bea6bf24e4bb2da0b0652a2b8ae8450d52193de/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=2bea6bf24e4bb2da0b0652a2b8ae8450d52193de",
            "patch": "@@ -5917,7 +5917,7 @@ class AttentionInterface(MutableMapping):\n     \"\"\"\n     Dict-like object keeping track of allowed attention functions. You can easily add a new attention function\n     with a call to `register()`. If a model needs to locally overwrite an existing attention function, say `sdpa`,\n-    it needs to declare a new instance of this class inside the `modeling.py`, and declare it on that instance.\n+    it needs to declare a new instance of this class inside the `modeling_<model>.py`, and declare it on that instance.\n     \"\"\"\n \n     # Class instance object, so that a call to `register` can be reflected into all other files correctly, even if\n@@ -5946,7 +5946,7 @@ def __delitem__(self, key):\n \n     def __iter__(self):\n         # Ensure we use all keys, with the overwritten ones on top\n-        return iter(self._global_mapping.update(self._local_mapping))\n+        return iter({**self._global_mapping, **self._local_mapping})\n \n     def __len__(self):\n         return len(self._global_mapping.keys() | self._local_mapping.keys())\n@@ -5956,7 +5956,7 @@ def register(cls, key: str, value: Callable):\n         cls._global_mapping.update({key: value})\n \n     def valid_keys(self) -> List[str]:\n-        return list(self._global_mapping.keys() | self._local_mapping.keys())\n+        return list(self.keys())\n \n \n # Global AttentionInterface shared by all models which do not need to overwrite any of the existing ones"
        }
    ],
    "stats": {
        "total": 44,
        "additions": 33,
        "deletions": 11
    }
}