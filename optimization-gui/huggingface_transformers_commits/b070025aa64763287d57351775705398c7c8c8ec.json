{
    "author": "Isotr0py",
    "message": "Add GGUF support to T5-Encoder (#36700)\n\n* add gguf support to t5encoder\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* fix\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* remove gguf from model_kwargs\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n---------\n\nSigned-off-by: Isotr0py <2037008807@qq.com>",
    "sha": "b070025aa64763287d57351775705398c7c8c8ec",
    "files": [
        {
            "sha": "1c971d0497b832086d2b5952ca40cae0343f0b39",
            "filename": "src/transformers/modeling_gguf_pytorch_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/b070025aa64763287d57351775705398c7c8c8ec/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b070025aa64763287d57351775705398c7c8c8ec/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py?ref=b070025aa64763287d57351775705398c7c8c8ec",
            "patch": "@@ -369,6 +369,7 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False, model_to_lo\n     architecture = read_field(reader, \"general.architecture\")[0]\n     model_name = read_field(reader, \"general.name\")\n \n+    updated_architecture = None\n     # in llama.cpp mistral models use the same architecture as llama. We need\n     # to add this patch to ensure things work correctly on our side.\n     if \"llama\" in architecture and \"mistral\" in model_name:\n@@ -377,6 +378,8 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False, model_to_lo\n     # It needs to be developed for supporting legacy t5.\n     elif \"t5\" in architecture or \"t5encoder\" in architecture:\n         parsed_parameters[\"config\"][\"is_gated_act\"] = True\n+        if \"t5encoder\" in architecture:\n+            parsed_parameters[\"config\"][\"architectures\"] = [\"T5EncoderModel\"]\n         updated_architecture = \"t5\"\n     else:\n         updated_architecture = architecture\n@@ -395,7 +398,7 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False, model_to_lo\n         parsed_parameters[\"config\"][\"use_qkv_bias\"] = qkv_bias\n         parsed_parameters[\"config\"][\"use_parallel_residual\"] = not use_parallel_residual\n \n-    if architecture not in GGUF_SUPPORTED_ARCHITECTURES:\n+    if architecture not in GGUF_SUPPORTED_ARCHITECTURES and updated_architecture not in GGUF_SUPPORTED_ARCHITECTURES:\n         raise ValueError(f\"GGUF model with architecture {architecture} is not supported yet.\")\n \n     # Handle tie_word_embeddings, if lm_head.weight is not present in tensors,"
        },
        {
            "sha": "727661137cd3cf7b1b8a096dac6e187478e03a73",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b070025aa64763287d57351775705398c7c8c8ec/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b070025aa64763287d57351775705398c7c8c8ec/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=b070025aa64763287d57351775705398c7c8c8ec",
            "patch": "@@ -4235,10 +4235,13 @@ def from_pretrained(\n                 token=token,\n                 revision=revision,\n                 subfolder=subfolder,\n+                gguf_file=gguf_file,\n                 _from_auto=from_auto_class,\n                 _from_pipeline=from_pipeline,\n                 **kwargs,\n             )\n+            if \"gguf_file\" in model_kwargs:\n+                model_kwargs.pop(\"gguf_file\")\n         else:\n             # In case one passes a config to `from_pretrained` + \"attn_implementation\"\n             # override the `_attn_implementation` attribute to `attn_implementation` of the kwargs"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 7,
        "deletions": 1
    }
}