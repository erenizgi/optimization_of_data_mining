{
    "author": "IlyasMoutawwakil",
    "message": "Fix pix2struct (#34374)\n\n* fix\r\n\r\n* fix and test use_cache test\r\n\r\n* style\r\n\r\n* remove atol",
    "sha": "fddbd3c13cca7a51515a039c6f2497e94905acb4",
    "files": [
        {
            "sha": "176dadd5b883e1b6bc3ddfc45820cb168eba7286",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 35,
            "deletions": 25,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/fddbd3c13cca7a51515a039c6f2497e94905acb4/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fddbd3c13cca7a51515a039c6f2497e94905acb4/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=fddbd3c13cca7a51515a039c6f2497e94905acb4",
            "patch": "@@ -762,11 +762,14 @@ def _relative_position_bucket(relative_position, bidirectional=True, num_buckets\n         return relative_buckets\n \n     # Adapted from transformers.models.t5.modeling_t5.T5Attention.compute_bias\n-    def compute_bias(self, query_length, key_length, device=None):\n+    def compute_bias(self, query_length, key_length, device=None, cache_position=None):\n         \"\"\"Compute binned relative position bias\"\"\"\n         if device is None:\n             device = self.relative_attention_bias.weight.device\n-        context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]\n+        if cache_position is None:\n+            context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]\n+        else:\n+            context_position = cache_position[:, None].to(device)\n         memory_position = torch.arange(key_length, dtype=torch.long, device=device)[None, :]\n         relative_position = memory_position - context_position  # shape (query_length, key_length)\n         relative_position_bucket = self._relative_position_bucket(\n@@ -779,6 +782,7 @@ def compute_bias(self, query_length, key_length, device=None):\n         values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, query_length, key_length)\n         return values\n \n+    # Adapted from transformers.models.t5.modeling_t5.T5Attention.forward\n     def forward(\n         self,\n         hidden_states,\n@@ -796,61 +800,66 @@ def forward(\n         Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).\n         \"\"\"\n         # Input is (batch_size, seq_length, dim)\n-        # Mask is (batch_size, 1, 1, key_length) (non-causal) or (batch_size, 1, query_length, key_length)\n+        # Mask is (batch_size, 1, 1, key_length) (non-causal) or (batch_size, 1, seq_length, key_length) (causal decoder)\n         batch_size, seq_length = hidden_states.shape[:2]\n \n         # if key_value_states are provided this layer is used as a cross-attention layer for the decoder\n         is_cross_attention = key_value_states is not None\n \n-        query_states = self.query(hidden_states).contiguous()\n+        query_states = self.query(hidden_states)\n         query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n         if past_key_value is not None:\n             is_updated = past_key_value.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                past_key_value = past_key_value.cross_attention_cache\n+                curr_past_key_value = past_key_value.cross_attention_cache\n             else:\n-                past_key_value = past_key_value.self_attention_cache\n+                curr_past_key_value = past_key_value.self_attention_cache\n \n-        # get key/value states\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value.key_cache[self.layer_idx]\n-            value_states = past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            key_states = self.key(current_states).contiguous()\n-            value_states = self.value(current_states).contiguous()\n+            key_states = self.key(current_states)\n+            value_states = self.value(current_states)\n             key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n             value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n+\n             if past_key_value is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = past_key_value.update(\n+                key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n                     past_key_value.is_updated[self.layer_idx] = True\n \n-        # compute scores\n+        # compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n         scores = torch.matmul(query_states, key_states.transpose(3, 2))\n \n         if position_bias is None:\n-            real_seq_length = cache_position[-1] + 1 if query_length is None else query_length\n-            key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]\n+            key_length = key_states.shape[-2]\n+            # cache position is 0-indexed so we add 1 to get the real length of queries (aka with past)\n+            real_seq_length = query_length if query_length is not None else cache_position[-1] + 1\n             if not self.has_relative_attention_bias:\n                 position_bias = torch.zeros(\n-                    (1, self.n_heads, real_seq_length, key_length), device=scores.device, dtype=scores.dtype\n+                    (1, self.n_heads, seq_length, key_length), device=scores.device, dtype=scores.dtype\n                 )\n                 if self.gradient_checkpointing and self.training:\n                     position_bias.requires_grad = True\n             else:\n-                position_bias = self.compute_bias(real_seq_length, key_length, device=scores.device)\n+                position_bias = self.compute_bias(\n+                    real_seq_length, key_length, device=scores.device, cache_position=cache_position\n+                )\n+                position_bias = position_bias[:, :, -seq_length:, :]\n \n             if mask is not None:\n-                position_bias = position_bias + mask  # (batch_size, n_heads, seq_length, key_length)\n+                causal_mask = mask[:, :, :, : key_states.shape[-2]]\n+                position_bias = position_bias + causal_mask\n \n         if self.pruned_heads:\n             mask = torch.ones(position_bias.shape[1])\n@@ -860,23 +869,22 @@ def forward(\n             position_bias_masked = position_bias\n \n         scores += position_bias_masked\n-        # (batch_size, n_heads, seq_length, key_length)\n-        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n \n         # (batch_size, n_heads, seq_length, key_length)\n+        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n         attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n \n         # Mask heads if we want to\n         if layer_head_mask is not None:\n             attn_weights = attn_weights * layer_head_mask\n \n         attn_output = torch.matmul(attn_weights, value_states)\n-        # (batch_size, seq_length, dim)\n-        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)\n \n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n         attn_output = self.output(attn_output)\n \n-        outputs = (attn_output,) + (past_key_value,) + (position_bias,)\n+        outputs = (attn_output, past_key_value, position_bias)\n \n         if output_attentions:\n             outputs = outputs + (attn_weights,)\n@@ -969,7 +977,10 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n             layer_idx=layer_idx,\n         )\n \n-        self.encoder_decoder_attention = Pix2StructTextLayerCrossAttention(config)\n+        self.encoder_decoder_attention = Pix2StructTextLayerCrossAttention(\n+            config,\n+            layer_idx=layer_idx,\n+        )\n \n         self.mlp = Pix2StructTextLayerFF(config)\n \n@@ -1019,7 +1030,6 @@ def forward(\n                 query_length=cache_position[-1] + 1,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n-                cache_position=cache_position,\n             )\n             hidden_states, past_key_value = cross_attention_outputs[:2]\n "
        },
        {
            "sha": "18b79f3fbc9c0492b0c35c6c81dcee38184a3a8c",
            "filename": "tests/models/pix2struct/test_modeling_pix2struct.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/fddbd3c13cca7a51515a039c6f2497e94905acb4/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fddbd3c13cca7a51515a039c6f2497e94905acb4/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py?ref=fddbd3c13cca7a51515a039c6f2497e94905acb4",
            "patch": "@@ -419,6 +419,7 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class Pix2StructModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (Pix2StructForConditionalGeneration,) if is_torch_available() else ()\n+    all_generative_model_classes = (Pix2StructForConditionalGeneration,) if is_torch_available() else {}\n     pipeline_model_mapping = {\"image-to-text\": Pix2StructForConditionalGeneration} if is_torch_available() else {}\n     fx_compatible = False\n     test_head_masking = False\n@@ -445,6 +446,16 @@ def test_model(self):\n                 ),\n             )\n \n+    def test_generative_model(self):\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_generative_model_classes:\n+            model = model_class(config).eval().to(torch_device)\n+\n+            output = model.generate(**input_dict, use_cache=False, min_new_tokens=10, max_new_tokens=10)\n+            output_use_cache = model.generate(**input_dict, use_cache=True, min_new_tokens=10, max_new_tokens=10)\n+\n+            torch.testing.assert_close(output, output_use_cache)\n+\n     @unittest.skip(reason=\"Hidden_states is tested in individual model tests\")\n     def test_hidden_states_output(self):\n         pass"
        }
    ],
    "stats": {
        "total": 71,
        "additions": 46,
        "deletions": 25
    }
}