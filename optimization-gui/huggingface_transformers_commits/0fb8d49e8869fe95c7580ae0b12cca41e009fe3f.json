{
    "author": "cyyever",
    "message": "Use Python 3.9 syntax in examples (#37279)\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
    "files": [
        {
            "sha": "557bfcd27a3308b93480028848017dcb987ad0a5",
            "filename": "examples/flax/image-captioning/create_model_from_encoder_decoder_models.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Fimage-captioning%2Fcreate_model_from_encoder_decoder_models.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Fimage-captioning%2Fcreate_model_from_encoder_decoder_models.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fimage-captioning%2Fcreate_model_from_encoder_decoder_models.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Team All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "f156057212e4e9de3ce23dc2f5924c035a7ec30e",
            "filename": "examples/flax/image-captioning/run_image_captioning_flax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Fimage-captioning%2Frun_image_captioning_flax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Fimage-captioning%2Frun_image_captioning_flax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fimage-captioning%2Frun_image_captioning_flax.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Team All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -833,8 +832,7 @@ def blockwise_data_loader(\n             # No need to shuffle here\n             loader = data_loader(rng, _ds, batch_size=batch_size, shuffle=False)\n \n-            for batch in loader:\n-                yield batch\n+            yield from loader\n \n     # Metric\n     metric = evaluate.load(\"rouge\", cache_dir=model_args.cache_dir)"
        },
        {
            "sha": "2bbb66a24ac4d11ac75b4f7d7491e36d8abf680b",
            "filename": "examples/flax/language-modeling/run_bart_dlm_flax.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Flanguage-modeling%2Frun_bart_dlm_flax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Flanguage-modeling%2Frun_bart_dlm_flax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Flanguage-modeling%2Frun_bart_dlm_flax.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Team All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -30,7 +29,7 @@\n from enum import Enum\n from itertools import chain\n from pathlib import Path\n-from typing import Dict, List, Optional\n+from typing import Optional\n \n import flax\n import jax\n@@ -294,7 +293,7 @@ def __post_init__(self):\n                 \" language modeling. \"\n             )\n \n-    def __call__(self, examples: List[Dict[str, List[int]]]) -> BatchEncoding:\n+    def __call__(self, examples: list[dict[str, list[int]]]) -> BatchEncoding:\n         # convert list to dict and tensorize input\n         batch = BatchEncoding(\n             {k: np.array([examples[i][k] for i in range(len(examples))]) for k, v in examples[0].items()}"
        },
        {
            "sha": "fb3ab65dc8b24e8a44830c940fac859768983bfb",
            "filename": "examples/flax/language-modeling/run_clm_flax.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Flanguage-modeling%2Frun_clm_flax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Flanguage-modeling%2Frun_clm_flax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Flanguage-modeling%2Frun_clm_flax.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Team All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "9b83c8db394cea0d404121fe7c6085cf16810140",
            "filename": "examples/flax/language-modeling/run_mlm_flax.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Flanguage-modeling%2Frun_mlm_flax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Flanguage-modeling%2Frun_mlm_flax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Flanguage-modeling%2Frun_mlm_flax.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Team All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -33,7 +32,7 @@\n \n # You can also adapt this script on your own masked language modeling task. Pointers for this are left as comments.\n from pathlib import Path\n-from typing import Dict, List, Optional, Tuple\n+from typing import Optional\n \n import flax\n import jax\n@@ -302,7 +301,7 @@ def __post_init__(self):\n                 \"You should pass `mlm=False` to train on causal language modeling instead.\"\n             )\n \n-    def __call__(self, examples: List[Dict[str, np.ndarray]], pad_to_multiple_of: int) -> Dict[str, np.ndarray]:\n+    def __call__(self, examples: list[dict[str, np.ndarray]], pad_to_multiple_of: int) -> dict[str, np.ndarray]:\n         # Handle dict or lists with proper padding and conversion to tensor.\n         batch = self.tokenizer.pad(examples, pad_to_multiple_of=pad_to_multiple_of, return_tensors=TensorType.NUMPY)\n \n@@ -316,7 +315,7 @@ def __call__(self, examples: List[Dict[str, np.ndarray]], pad_to_multiple_of: in\n \n     def mask_tokens(\n         self, inputs: np.ndarray, special_tokens_mask: Optional[np.ndarray]\n-    ) -> Tuple[np.ndarray, np.ndarray]:\n+    ) -> tuple[np.ndarray, np.ndarray]:\n         \"\"\"\n         Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n         \"\"\""
        },
        {
            "sha": "b376c26d32cccbcf3c346f0848acfed475f5dc11",
            "filename": "examples/flax/language-modeling/run_t5_mlm_flax.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Flanguage-modeling%2Frun_t5_mlm_flax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Flanguage-modeling%2Frun_t5_mlm_flax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Flanguage-modeling%2Frun_t5_mlm_flax.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Team All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -32,7 +31,7 @@\n from enum import Enum\n from itertools import chain\n from pathlib import Path\n-from typing import Dict, List, Optional\n+from typing import Optional\n \n import flax\n import jax\n@@ -338,7 +337,7 @@ class FlaxDataCollatorForT5MLM:\n     pad_token_id: int\n     decoder_start_token_id: int\n \n-    def __call__(self, examples: List[Dict[str, np.ndarray]]) -> BatchEncoding:\n+    def __call__(self, examples: list[dict[str, np.ndarray]]) -> BatchEncoding:\n         # convert list to dict and tensorize input\n         batch = BatchEncoding(\n             {k: np.array([examples[i][k] for i in range(len(examples))]) for k, v in examples[0].items()}"
        },
        {
            "sha": "8b9885279668093a7089614767db6cd237e64e93",
            "filename": "examples/flax/language-modeling/t5_tokenizer_model.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Flanguage-modeling%2Ft5_tokenizer_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Flanguage-modeling%2Ft5_tokenizer_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Flanguage-modeling%2Ft5_tokenizer_model.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,6 +1,7 @@\n #!/usr/bin/env python3\n import json\n-from typing import Iterator, List, Union\n+from collections.abc import Iterator\n+from typing import Union\n \n from tokenizers import AddedToken, Regex, Tokenizer, decoders, normalizers, pre_tokenizers, trainers\n from tokenizers.implementations.base_tokenizer import BaseTokenizer\n@@ -72,7 +73,7 @@ def __init__(\n \n     def train(\n         self,\n-        files: Union[str, List[str]],\n+        files: Union[str, list[str]],\n         vocab_size: int = 8000,\n         show_progress: bool = True,\n     ):"
        },
        {
            "sha": "eb506fff04855c431e3efb82db932e44206637f9",
            "filename": "examples/flax/question-answering/run_qa.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Fquestion-answering%2Frun_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Fquestion-answering%2Frun_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fquestion-answering%2Frun_qa.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Team All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -28,7 +27,7 @@\n from dataclasses import asdict, dataclass, field\n from enum import Enum\n from pathlib import Path\n-from typing import Any, Callable, Dict, Optional, Tuple\n+from typing import Any, Callable, Optional\n \n import datasets\n import evaluate\n@@ -908,8 +907,8 @@ def write_eval_metric(summary_writer, eval_metrics, step):\n \n     # region Define train step functions\n     def train_step(\n-        state: train_state.TrainState, batch: Dict[str, Array], dropout_rng: PRNGKey\n-    ) -> Tuple[train_state.TrainState, float]:\n+        state: train_state.TrainState, batch: dict[str, Array], dropout_rng: PRNGKey\n+    ) -> tuple[train_state.TrainState, float]:\n         \"\"\"Trains model with an optimizer (both in `state`) on `batch`, returning a pair `(new_state, loss)`.\"\"\"\n         dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n         start_positions = batch.pop(\"start_positions\")"
        },
        {
            "sha": "f0cc5c26a6927179d83dab56d21f4acb357884ee",
            "filename": "examples/flax/question-answering/utils_qa.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Fquestion-answering%2Futils_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Fquestion-answering%2Futils_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fquestion-answering%2Futils_qa.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2020 The HuggingFace Team All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -20,7 +19,7 @@\n import json\n import logging\n import os\n-from typing import Optional, Tuple\n+from typing import Optional\n \n import numpy as np\n from tqdm.auto import tqdm\n@@ -32,7 +31,7 @@\n def postprocess_qa_predictions(\n     examples,\n     features,\n-    predictions: Tuple[np.ndarray, np.ndarray],\n+    predictions: tuple[np.ndarray, np.ndarray],\n     version_2_with_negative: bool = False,\n     n_best_size: int = 20,\n     max_answer_length: int = 30,\n@@ -223,7 +222,7 @@ def postprocess_qa_predictions(\n     # If we have an output_dir, let's save all those dicts.\n     if output_dir is not None:\n         if not os.path.isdir(output_dir):\n-            raise EnvironmentError(f\"{output_dir} is not a directory.\")\n+            raise OSError(f\"{output_dir} is not a directory.\")\n \n         prediction_file = os.path.join(\n             output_dir, \"predictions.json\" if prefix is None else f\"{prefix}_predictions.json\"\n@@ -253,7 +252,7 @@ def postprocess_qa_predictions(\n def postprocess_qa_predictions_with_beam_search(\n     examples,\n     features,\n-    predictions: Tuple[np.ndarray, np.ndarray],\n+    predictions: tuple[np.ndarray, np.ndarray],\n     version_2_with_negative: bool = False,\n     n_best_size: int = 20,\n     max_answer_length: int = 30,\n@@ -417,7 +416,7 @@ def postprocess_qa_predictions_with_beam_search(\n     # If we have an output_dir, let's save all those dicts.\n     if output_dir is not None:\n         if not os.path.isdir(output_dir):\n-            raise EnvironmentError(f\"{output_dir} is not a directory.\")\n+            raise OSError(f\"{output_dir} is not a directory.\")\n \n         prediction_file = os.path.join(\n             output_dir, \"predictions.json\" if prefix is None else f\"{prefix}_predictions.json\""
        },
        {
            "sha": "e70618eef0ce43b03839f8509416eaf5f187fcba",
            "filename": "examples/flax/speech-recognition/run_flax_speech_recognition_seq2seq.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Fspeech-recognition%2Frun_flax_speech_recognition_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Fspeech-recognition%2Frun_flax_speech_recognition_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fspeech-recognition%2Frun_flax_speech_recognition_seq2seq.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -25,7 +24,7 @@\n from dataclasses import field\n from functools import partial\n from pathlib import Path\n-from typing import Any, Callable, Dict, List, Optional, Union\n+from typing import Any, Callable, Optional, Union\n \n import datasets\n import evaluate\n@@ -303,7 +302,7 @@ class FlaxDataCollatorSpeechSeq2SeqWithPadding:\n     pad_input_to_multiple_of: Optional[int] = None\n     pad_target_to_multiple_of: Optional[int] = None\n \n-    def __call__(self, features: List[Dict[str, Union[List[int], np.ndarray]]]) -> Dict[str, np.ndarray]:\n+    def __call__(self, features: list[dict[str, Union[list[int], np.ndarray]]]) -> dict[str, np.ndarray]:\n         # split inputs and labels since they have to be of different lengths and need\n         # different padding methods\n         model_input_name = self.processor.model_input_names[0]"
        },
        {
            "sha": "aab44c88a02cbf02671a086197fff4632ffa9811",
            "filename": "examples/flax/summarization/run_summarization_flax.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Fsummarization%2Frun_summarization_flax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Fsummarization%2Frun_summarization_flax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fsummarization%2Frun_summarization_flax.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Team All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "132be94e3184cc99f63920409d0a15e115184c0e",
            "filename": "examples/flax/test_flax_examples.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Ftest_flax_examples.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Ftest_flax_examples.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Ftest_flax_examples.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 HuggingFace Inc.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -64,7 +63,7 @@ def get_setup_file():\n def get_results(output_dir, split=\"eval\"):\n     path = os.path.join(output_dir, f\"{split}_results.json\")\n     if os.path.exists(path):\n-        with open(path, \"r\") as f:\n+        with open(path) as f:\n             return json.load(f)\n     raise ValueError(f\"can't find {path}\")\n "
        },
        {
            "sha": "b5378b8c1171c173f394aec83b3b439f5d8fd863",
            "filename": "examples/flax/text-classification/run_flax_glue.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Ftext-classification%2Frun_flax_glue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Ftext-classification%2Frun_flax_glue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Ftext-classification%2Frun_flax_glue.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -25,7 +24,7 @@\n import warnings\n from dataclasses import dataclass, field\n from pathlib import Path\n-from typing import Any, Callable, Dict, Optional, Tuple\n+from typing import Any, Callable, Optional\n \n import datasets\n import evaluate\n@@ -572,8 +571,8 @@ def write_eval_metric(summary_writer, eval_metrics, step):\n \n     # define step functions\n     def train_step(\n-        state: train_state.TrainState, batch: Dict[str, Array], dropout_rng: PRNGKey\n-    ) -> Tuple[train_state.TrainState, float]:\n+        state: train_state.TrainState, batch: dict[str, Array], dropout_rng: PRNGKey\n+    ) -> tuple[train_state.TrainState, float]:\n         \"\"\"Trains model with an optimizer (both in `state`) on `batch`, returning a pair `(new_state, loss)`.\"\"\"\n         dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n         targets = batch.pop(\"labels\")"
        },
        {
            "sha": "854d7c71366035379f8185dd584ab4a33c41e48d",
            "filename": "examples/flax/token-classification/run_flax_ner.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Ftoken-classification%2Frun_flax_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Ftoken-classification%2Frun_flax_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Ftoken-classification%2Frun_flax_ner.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -27,7 +26,7 @@\n from enum import Enum\n from itertools import chain\n from pathlib import Path\n-from typing import Any, Callable, Dict, Optional, Tuple\n+from typing import Any, Callable, Optional\n \n import datasets\n import evaluate\n@@ -651,8 +650,8 @@ def write_eval_metric(summary_writer, eval_metrics, step):\n \n     # define step functions\n     def train_step(\n-        state: train_state.TrainState, batch: Dict[str, Array], dropout_rng: PRNGKey\n-    ) -> Tuple[train_state.TrainState, float]:\n+        state: train_state.TrainState, batch: dict[str, Array], dropout_rng: PRNGKey\n+    ) -> tuple[train_state.TrainState, float]:\n         \"\"\"Trains model with an optimizer (both in `state`) on `batch`, returning a pair `(new_state, loss)`.\"\"\"\n         dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n         targets = batch.pop(\"labels\")"
        },
        {
            "sha": "4eddd36f962fa580e12a1b1a874c69c04970f660",
            "filename": "examples/flax/vision/run_image_classification.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Fvision%2Frun_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fflax%2Fvision%2Frun_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fvision%2Frun_image_classification.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Team All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "d802dbe67e83aaaddbacb4a90320bfe999a24c67",
            "filename": "examples/legacy/benchmarking/plot_csv_file.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fbenchmarking%2Fplot_csv_file.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fbenchmarking%2Fplot_csv_file.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fbenchmarking%2Fplot_csv_file.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -15,7 +15,7 @@\n import csv\n from collections import defaultdict\n from dataclasses import dataclass, field\n-from typing import List, Optional\n+from typing import Optional\n \n import matplotlib.pyplot as plt\n import numpy as np\n@@ -59,7 +59,7 @@ class PlotArguments:\n         default=None,\n         metadata={\"help\": \"Filename under which the plot will be saved. If unused no plot is saved.\"},\n     )\n-    short_model_names: Optional[List[str]] = list_field(\n+    short_model_names: Optional[list[str]] = list_field(\n         default=None, metadata={\"help\": \"List of model names that are used instead of the ones in the csv file.\"}\n     )\n "
        },
        {
            "sha": "1bdd69bbe29d6e8626f5046359619d0b168f0507",
            "filename": "examples/legacy/benchmarking/run_benchmark.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fbenchmarking%2Frun_benchmark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fbenchmarking%2Frun_benchmark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fbenchmarking%2Frun_benchmark.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2020 The HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n #"
        },
        {
            "sha": "aa1297656a909d9767e2b10701df5306fa8f8393",
            "filename": "examples/legacy/multiple_choice/run_multiple_choice.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fmultiple_choice%2Frun_multiple_choice.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fmultiple_choice%2Frun_multiple_choice.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fmultiple_choice%2Frun_multiple_choice.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n #\n@@ -18,7 +17,7 @@\n import logging\n import os\n from dataclasses import dataclass, field\n-from typing import Dict, Optional\n+from typing import Optional\n \n import numpy as np\n from utils_multiple_choice import MultipleChoiceDataset, Split, processors\n@@ -187,7 +186,7 @@ def main():\n         else None\n     )\n \n-    def compute_metrics(p: EvalPrediction) -> Dict:\n+    def compute_metrics(p: EvalPrediction) -> dict:\n         preds = np.argmax(p.predictions, axis=1)\n         return {\"acc\": simple_accuracy(preds, p.label_ids)}\n \n@@ -228,7 +227,7 @@ def compute_metrics(p: EvalPrediction) -> Dict:\n                 logger.info(\"***** Eval results *****\")\n                 for key, value in result.items():\n                     logger.info(\"  %s = %s\", key, value)\n-                    writer.write(\"%s = %s\\n\" % (key, value))\n+                    writer.write(\"{} = {}\\n\".format(key, value))\n \n                 results.update(result)\n "
        },
        {
            "sha": "ddae47e5880ce50ff3a93aec871f512ff532653c",
            "filename": "examples/legacy/multiple_choice/utils_multiple_choice.py",
            "status": "modified",
            "additions": 30,
            "deletions": 31,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fmultiple_choice%2Futils_multiple_choice.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fmultiple_choice%2Futils_multiple_choice.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fmultiple_choice%2Futils_multiple_choice.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n #\n@@ -22,7 +21,7 @@\n import os\n from dataclasses import dataclass\n from enum import Enum\n-from typing import List, Optional\n+from typing import Optional\n \n import tqdm\n from filelock import FileLock\n@@ -49,8 +48,8 @@ class InputExample:\n \n     example_id: str\n     question: str\n-    contexts: List[str]\n-    endings: List[str]\n+    contexts: list[str]\n+    endings: list[str]\n     label: Optional[str]\n \n \n@@ -62,9 +61,9 @@ class InputFeatures:\n     \"\"\"\n \n     example_id: str\n-    input_ids: List[List[int]]\n-    attention_mask: Optional[List[List[int]]]\n-    token_type_ids: Optional[List[List[int]]]\n+    input_ids: list[list[int]]\n+    attention_mask: Optional[list[list[int]]]\n+    token_type_ids: Optional[list[list[int]]]\n     label: Optional[int]\n \n \n@@ -84,7 +83,7 @@ class MultipleChoiceDataset(Dataset):\n         soon.\n         \"\"\"\n \n-        features: List[InputFeatures]\n+        features: list[InputFeatures]\n \n         def __init__(\n             self,\n@@ -149,7 +148,7 @@ class TFMultipleChoiceDataset:\n         soon.\n         \"\"\"\n \n-        features: List[InputFeatures]\n+        features: list[InputFeatures]\n \n         def __init__(\n             self,\n@@ -253,7 +252,7 @@ class RaceProcessor(DataProcessor):\n \n     def get_train_examples(self, data_dir):\n         \"\"\"See base class.\"\"\"\n-        logger.info(\"LOOKING AT {} train\".format(data_dir))\n+        logger.info(f\"LOOKING AT {data_dir} train\")\n         high = os.path.join(data_dir, \"train/high\")\n         middle = os.path.join(data_dir, \"train/middle\")\n         high = self._read_txt(high)\n@@ -262,7 +261,7 @@ def get_train_examples(self, data_dir):\n \n     def get_dev_examples(self, data_dir):\n         \"\"\"See base class.\"\"\"\n-        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n+        logger.info(f\"LOOKING AT {data_dir} dev\")\n         high = os.path.join(data_dir, \"dev/high\")\n         middle = os.path.join(data_dir, \"dev/middle\")\n         high = self._read_txt(high)\n@@ -271,7 +270,7 @@ def get_dev_examples(self, data_dir):\n \n     def get_test_examples(self, data_dir):\n         \"\"\"See base class.\"\"\"\n-        logger.info(\"LOOKING AT {} test\".format(data_dir))\n+        logger.info(f\"LOOKING AT {data_dir} test\")\n         high = os.path.join(data_dir, \"test/high\")\n         middle = os.path.join(data_dir, \"test/middle\")\n         high = self._read_txt(high)\n@@ -286,7 +285,7 @@ def _read_txt(self, input_dir):\n         lines = []\n         files = glob.glob(input_dir + \"/*txt\")\n         for file in tqdm.tqdm(files, desc=\"read files\"):\n-            with open(file, \"r\", encoding=\"utf-8\") as fin:\n+            with open(file, encoding=\"utf-8\") as fin:\n                 data_raw = json.load(fin)\n                 data_raw[\"race_id\"] = file\n                 lines.append(data_raw)\n@@ -296,7 +295,7 @@ def _create_examples(self, lines, set_type):\n         \"\"\"Creates examples for the training and dev sets.\"\"\"\n         examples = []\n         for _, data_raw in enumerate(lines):\n-            race_id = \"%s-%s\" % (set_type, data_raw[\"race_id\"])\n+            race_id = \"{}-{}\".format(set_type, data_raw[\"race_id\"])\n             article = data_raw[\"article\"]\n             for i in range(len(data_raw[\"answers\"])):\n                 truth = str(ord(data_raw[\"answers\"][i]) - ord(\"A\"))\n@@ -320,17 +319,17 @@ class SynonymProcessor(DataProcessor):\n \n     def get_train_examples(self, data_dir):\n         \"\"\"See base class.\"\"\"\n-        logger.info(\"LOOKING AT {} train\".format(data_dir))\n+        logger.info(f\"LOOKING AT {data_dir} train\")\n         return self._create_examples(self._read_csv(os.path.join(data_dir, \"mctrain.csv\")), \"train\")\n \n     def get_dev_examples(self, data_dir):\n         \"\"\"See base class.\"\"\"\n-        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n+        logger.info(f\"LOOKING AT {data_dir} dev\")\n         return self._create_examples(self._read_csv(os.path.join(data_dir, \"mchp.csv\")), \"dev\")\n \n     def get_test_examples(self, data_dir):\n         \"\"\"See base class.\"\"\"\n-        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n+        logger.info(f\"LOOKING AT {data_dir} dev\")\n \n         return self._create_examples(self._read_csv(os.path.join(data_dir, \"mctest.csv\")), \"test\")\n \n@@ -339,10 +338,10 @@ def get_labels(self):\n         return [\"0\", \"1\", \"2\", \"3\", \"4\"]\n \n     def _read_csv(self, input_file):\n-        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n+        with open(input_file, encoding=\"utf-8\") as f:\n             return list(csv.reader(f))\n \n-    def _create_examples(self, lines: List[List[str]], type: str):\n+    def _create_examples(self, lines: list[list[str]], type: str):\n         \"\"\"Creates examples for the training and dev sets.\"\"\"\n \n         examples = [\n@@ -366,17 +365,17 @@ class SwagProcessor(DataProcessor):\n \n     def get_train_examples(self, data_dir):\n         \"\"\"See base class.\"\"\"\n-        logger.info(\"LOOKING AT {} train\".format(data_dir))\n+        logger.info(f\"LOOKING AT {data_dir} train\")\n         return self._create_examples(self._read_csv(os.path.join(data_dir, \"train.csv\")), \"train\")\n \n     def get_dev_examples(self, data_dir):\n         \"\"\"See base class.\"\"\"\n-        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n+        logger.info(f\"LOOKING AT {data_dir} dev\")\n         return self._create_examples(self._read_csv(os.path.join(data_dir, \"val.csv\")), \"dev\")\n \n     def get_test_examples(self, data_dir):\n         \"\"\"See base class.\"\"\"\n-        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n+        logger.info(f\"LOOKING AT {data_dir} dev\")\n         raise ValueError(\n             \"For swag testing, the input file does not contain a label column. It can not be tested in current code \"\n             \"setting!\"\n@@ -388,10 +387,10 @@ def get_labels(self):\n         return [\"0\", \"1\", \"2\", \"3\"]\n \n     def _read_csv(self, input_file):\n-        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n+        with open(input_file, encoding=\"utf-8\") as f:\n             return list(csv.reader(f))\n \n-    def _create_examples(self, lines: List[List[str]], type: str):\n+    def _create_examples(self, lines: list[list[str]], type: str):\n         \"\"\"Creates examples for the training and dev sets.\"\"\"\n         if type == \"train\" and lines[0][-1] != \"label\":\n             raise ValueError(\"For training, the input file must contain a label column.\")\n@@ -417,24 +416,24 @@ class ArcProcessor(DataProcessor):\n \n     def get_train_examples(self, data_dir):\n         \"\"\"See base class.\"\"\"\n-        logger.info(\"LOOKING AT {} train\".format(data_dir))\n+        logger.info(f\"LOOKING AT {data_dir} train\")\n         return self._create_examples(self._read_json(os.path.join(data_dir, \"train.jsonl\")), \"train\")\n \n     def get_dev_examples(self, data_dir):\n         \"\"\"See base class.\"\"\"\n-        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n+        logger.info(f\"LOOKING AT {data_dir} dev\")\n         return self._create_examples(self._read_json(os.path.join(data_dir, \"dev.jsonl\")), \"dev\")\n \n     def get_test_examples(self, data_dir):\n-        logger.info(\"LOOKING AT {} test\".format(data_dir))\n+        logger.info(f\"LOOKING AT {data_dir} test\")\n         return self._create_examples(self._read_json(os.path.join(data_dir, \"test.jsonl\")), \"test\")\n \n     def get_labels(self):\n         \"\"\"See base class.\"\"\"\n         return [\"0\", \"1\", \"2\", \"3\"]\n \n     def _read_json(self, input_file):\n-        with open(input_file, \"r\", encoding=\"utf-8\") as fin:\n+        with open(input_file, encoding=\"utf-8\") as fin:\n             lines = fin.readlines()\n             return lines\n \n@@ -504,11 +503,11 @@ def normalize(truth):\n \n \n def convert_examples_to_features(\n-    examples: List[InputExample],\n-    label_list: List[str],\n+    examples: list[InputExample],\n+    label_list: list[str],\n     max_length: int,\n     tokenizer: PreTrainedTokenizer,\n-) -> List[InputFeatures]:\n+) -> list[InputFeatures]:\n     \"\"\"\n     Loads a data file into a list of `InputFeatures`\n     \"\"\""
        },
        {
            "sha": "5c34fbe7e07d4f89a6c354e91f08baa8ee6b5969",
            "filename": "examples/legacy/pytorch-lightning/lightning_base.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fpytorch-lightning%2Flightning_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fpytorch-lightning%2Flightning_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fpytorch-lightning%2Flightning_base.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -2,7 +2,7 @@\n import logging\n import os\n from pathlib import Path\n-from typing import Any, Dict\n+from typing import Any\n \n import pytorch_lightning as pl\n from pytorch_lightning.utilities import rank_zero_info\n@@ -201,7 +201,7 @@ def _feature_file(self, mode):\n         )\n \n     @pl.utilities.rank_zero_only\n-    def on_save_checkpoint(self, checkpoint: Dict[str, Any]) -> None:\n+    def on_save_checkpoint(self, checkpoint: dict[str, Any]) -> None:\n         save_path = self.output_dir.joinpath(\"best_tfmr\")\n         self.model.config.save_step = self.step_count\n         self.model.save_pretrained(save_path)\n@@ -282,7 +282,7 @@ def on_validation_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n         # Log results\n         for key in sorted(metrics):\n             if key not in [\"log\", \"progress_bar\"]:\n-                rank_zero_info(\"{} = {}\\n\".format(key, str(metrics[key])))\n+                rank_zero_info(f\"{key} = {str(metrics[key])}\\n\")\n \n     def on_test_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n         rank_zero_info(\"***** Test results *****\")\n@@ -292,8 +292,8 @@ def on_test_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n         with open(output_test_results_file, \"w\") as writer:\n             for key in sorted(metrics):\n                 if key not in [\"log\", \"progress_bar\"]:\n-                    rank_zero_info(\"{} = {}\\n\".format(key, str(metrics[key])))\n-                    writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))\n+                    rank_zero_info(f\"{key} = {str(metrics[key])}\\n\")\n+                    writer.write(f\"{key} = {str(metrics[key])}\\n\")\n \n \n def add_generic_args(parser, root_dir) -> None:"
        },
        {
            "sha": "5ba8309fee54b7079e0dd36213984f8873b41bf0",
            "filename": "examples/legacy/question-answering/run_squad.py",
            "status": "modified",
            "additions": 8,
            "deletions": 9,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fquestion-answering%2Frun_squad.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fquestion-answering%2Frun_squad.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fquestion-answering%2Frun_squad.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n #\n@@ -231,14 +230,14 @@ def train(args, train_dataset, model, tokenizer):\n                     if args.local_rank == -1 and args.evaluate_during_training:\n                         results = evaluate(args, model, tokenizer)\n                         for key, value in results.items():\n-                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n+                            tb_writer.add_scalar(f\"eval_{key}\", value, global_step)\n                     tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n                     tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n                     logging_loss = tr_loss\n \n                 # Save model checkpoint\n                 if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n-                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n+                    output_dir = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n                     # Take care of distributed/parallel training\n                     model_to_save = model.module if hasattr(model, \"module\") else model\n                     model_to_save.save_pretrained(output_dir)\n@@ -281,7 +280,7 @@ def evaluate(args, model, tokenizer, prefix=\"\"):\n         model = torch.nn.DataParallel(model)\n \n     # Eval!\n-    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n+    logger.info(f\"***** Running evaluation {prefix} *****\")\n     logger.info(\"  Num examples = %d\", len(dataset))\n     logger.info(\"  Batch size = %d\", args.eval_batch_size)\n \n@@ -348,11 +347,11 @@ def evaluate(args, model, tokenizer, prefix=\"\"):\n     logger.info(\"  Evaluation done in total %f secs (%f sec per example)\", evalTime, evalTime / len(dataset))\n \n     # Compute predictions\n-    output_prediction_file = os.path.join(args.output_dir, \"predictions_{}.json\".format(prefix))\n-    output_nbest_file = os.path.join(args.output_dir, \"nbest_predictions_{}.json\".format(prefix))\n+    output_prediction_file = os.path.join(args.output_dir, f\"predictions_{prefix}.json\")\n+    output_nbest_file = os.path.join(args.output_dir, f\"nbest_predictions_{prefix}.json\")\n \n     if args.version_2_with_negative:\n-        output_null_log_odds_file = os.path.join(args.output_dir, \"null_odds_{}.json\".format(prefix))\n+        output_null_log_odds_file = os.path.join(args.output_dir, f\"null_odds_{prefix}.json\")\n     else:\n         output_null_log_odds_file = None\n \n@@ -828,10 +827,10 @@ def main():\n             # Evaluate\n             result = evaluate(args, model, tokenizer, prefix=global_step)\n \n-            result = {k + (\"_{}\".format(global_step) if global_step else \"\"): v for k, v in result.items()}\n+            result = {k + (f\"_{global_step}\" if global_step else \"\"): v for k, v in result.items()}\n             results.update(result)\n \n-    logger.info(\"Results: {}\".format(results))\n+    logger.info(f\"Results: {results}\")\n \n     return results\n "
        },
        {
            "sha": "159569c3277f9a02f94c7b94896712436cffe192",
            "filename": "examples/legacy/question-answering/run_squad_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fquestion-answering%2Frun_squad_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fquestion-answering%2Frun_squad_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fquestion-answering%2Frun_squad_trainer.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n #"
        },
        {
            "sha": "9767ffbf8b30c0e797103a14cfa10f1c9f666fd5",
            "filename": "examples/legacy/run_camembert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Frun_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Frun_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Frun_camembert.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -20,10 +20,10 @@ def fill_mask(masked_input, model, tokenizer, topk=5):\n     topk_filled_outputs = []\n     for index, predicted_token_bpe in enumerate(topk_predicted_token_bpe.split(\" \")):\n         predicted_token = predicted_token_bpe.replace(\"\\u2581\", \" \")\n-        if \" {0}\".format(masked_token) in masked_input:\n+        if f\" {masked_token}\" in masked_input:\n             topk_filled_outputs.append(\n                 (\n-                    masked_input.replace(\" {0}\".format(masked_token), predicted_token),\n+                    masked_input.replace(f\" {masked_token}\", predicted_token),\n                     values[index].item(),\n                     predicted_token,\n                 )"
        },
        {
            "sha": "03d195e961c8faafb6a113c792acd7aef0113571",
            "filename": "examples/legacy/run_chinese_ref.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Frun_chinese_ref.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Frun_chinese_ref.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Frun_chinese_ref.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,7 +1,6 @@\n #!/usr/bin/env python\n import argparse\n import json\n-from typing import List\n \n from ltp import LTP\n \n@@ -42,7 +41,7 @@ def is_chinese(word: str):\n     return 1\n \n \n-def get_chinese_word(tokens: List[str]):\n+def get_chinese_word(tokens: list[str]):\n     word_set = set()\n \n     for token in tokens:\n@@ -53,7 +52,7 @@ def get_chinese_word(tokens: List[str]):\n     return word_list\n \n \n-def add_sub_symbol(bert_tokens: List[str], chinese_word_set: set()):\n+def add_sub_symbol(bert_tokens: list[str], chinese_word_set: set()):\n     if not chinese_word_set:\n         return bert_tokens\n     max_word_len = max([len(w) for w in chinese_word_set])\n@@ -77,7 +76,7 @@ def add_sub_symbol(bert_tokens: List[str], chinese_word_set: set()):\n     return bert_word\n \n \n-def prepare_ref(lines: List[str], ltp_tokenizer: LTP, bert_tokenizer: BertTokenizer):\n+def prepare_ref(lines: list[str], ltp_tokenizer: LTP, bert_tokenizer: BertTokenizer):\n     ltp_res = []\n \n     for i in range(0, len(lines), 100):\n@@ -117,7 +116,7 @@ def prepare_ref(lines: List[str], ltp_tokenizer: LTP, bert_tokenizer: BertTokeni\n def main(args):\n     # For Chinese (Ro)Bert, the best result is from : RoBERTa-wwm-ext (https://github.com/ymcui/Chinese-BERT-wwm)\n     # If we want to fine-tune these model, we have to use same tokenizer : LTP (https://github.com/HIT-SCIR/ltp)\n-    with open(args.file_name, \"r\", encoding=\"utf-8\") as f:\n+    with open(args.file_name, encoding=\"utf-8\") as f:\n         data = f.readlines()\n     data = [line.strip() for line in data if len(line) > 0 and not line.isspace()]  # avoid delimiter like '\\u2029'\n     ltp_tokenizer = LTP(args.ltp)  # faster in GPU device"
        },
        {
            "sha": "8a6b8eded34a0c5899af36bc972c14e64aabdb9a",
            "filename": "examples/legacy/run_language_modeling.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Frun_language_modeling.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Frun_language_modeling.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Frun_language_modeling.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n #\n@@ -358,7 +357,7 @@ def main():\n                 logger.info(\"***** Eval results *****\")\n                 for key in sorted(result.keys()):\n                     logger.info(\"  %s = %s\", key, str(result[key]))\n-                    writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n+                    writer.write(\"{} = {}\\n\".format(key, str(result[key])))\n \n         results.update(result)\n "
        },
        {
            "sha": "62f9d3a3c083b606cce34cd35cf02442860be541",
            "filename": "examples/legacy/run_openai_gpt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Frun_openai_gpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Frun_openai_gpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Frun_openai_gpt.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n #\n@@ -163,7 +162,7 @@ def main():\n \n     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n     n_gpu = torch.cuda.device_count()\n-    logger.info(\"device: {}, n_gpu {}\".format(device, n_gpu))\n+    logger.info(f\"device: {device}, n_gpu {n_gpu}\")\n \n     if not args.do_train and not args.do_eval:\n         raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n@@ -261,7 +260,7 @@ def tokenize_and_encode(obj):\n                     loss.item() if exp_average_loss is None else 0.7 * exp_average_loss + 0.3 * loss.item()\n                 )\n                 nb_tr_steps += 1\n-                tqdm_bar.desc = \"Training loss: {:.2e} lr: {:.2e}\".format(exp_average_loss, scheduler.get_lr()[0])\n+                tqdm_bar.desc = f\"Training loss: {exp_average_loss:.2e} lr: {scheduler.get_lr()[0]:.2e}\"\n \n     # Save a trained model\n     if args.do_train:\n@@ -313,7 +312,7 @@ def tokenize_and_encode(obj):\n             logger.info(\"***** Eval results *****\")\n             for key in sorted(result.keys()):\n                 logger.info(\"  %s = %s\", key, str(result[key]))\n-                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n+                writer.write(\"{} = {}\\n\".format(key, str(result[key])))\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "221f9cc9c98dae7b6fcb7594d73af5966cd177b5",
            "filename": "examples/legacy/run_swag.py",
            "status": "modified",
            "additions": 20,
            "deletions": 21,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Frun_swag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Frun_swag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Frun_swag.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n #\n@@ -51,7 +50,7 @@\n logger = logging.getLogger(__name__)\n \n \n-class SwagExample(object):\n+class SwagExample:\n     \"\"\"A single training/test example for the SWAG dataset.\"\"\"\n \n     def __init__(self, swag_id, context_sentence, start_ending, ending_0, ending_1, ending_2, ending_3, label=None):\n@@ -71,22 +70,22 @@ def __str__(self):\n \n     def __repr__(self):\n         attributes = [\n-            \"swag_id: {}\".format(self.swag_id),\n-            \"context_sentence: {}\".format(self.context_sentence),\n-            \"start_ending: {}\".format(self.start_ending),\n-            \"ending_0: {}\".format(self.endings[0]),\n-            \"ending_1: {}\".format(self.endings[1]),\n-            \"ending_2: {}\".format(self.endings[2]),\n-            \"ending_3: {}\".format(self.endings[3]),\n+            f\"swag_id: {self.swag_id}\",\n+            f\"context_sentence: {self.context_sentence}\",\n+            f\"start_ending: {self.start_ending}\",\n+            f\"ending_0: {self.endings[0]}\",\n+            f\"ending_1: {self.endings[1]}\",\n+            f\"ending_2: {self.endings[2]}\",\n+            f\"ending_3: {self.endings[3]}\",\n         ]\n \n         if self.label is not None:\n-            attributes.append(\"label: {}\".format(self.label))\n+            attributes.append(f\"label: {self.label}\")\n \n         return \", \".join(attributes)\n \n \n-class InputFeatures(object):\n+class InputFeatures:\n     def __init__(self, example_id, choices_features, label):\n         self.example_id = example_id\n         self.choices_features = [\n@@ -97,7 +96,7 @@ def __init__(self, example_id, choices_features, label):\n \n \n def read_swag_examples(input_file, is_training=True):\n-    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n+    with open(input_file, encoding=\"utf-8\") as f:\n         lines = list(csv.reader(f))\n \n     if is_training and lines[0][-1] != \"label\":\n@@ -179,15 +178,15 @@ def convert_examples_to_features(examples, tokenizer, max_seq_length, is_trainin\n         label = example.label\n         if example_index < 5:\n             logger.info(\"*** Example ***\")\n-            logger.info(\"swag_id: {}\".format(example.swag_id))\n+            logger.info(f\"swag_id: {example.swag_id}\")\n             for choice_idx, (tokens, input_ids, input_mask, segment_ids) in enumerate(choices_features):\n-                logger.info(\"choice: {}\".format(choice_idx))\n+                logger.info(f\"choice: {choice_idx}\")\n                 logger.info(\"tokens: {}\".format(\" \".join(tokens)))\n                 logger.info(\"input_ids: {}\".format(\" \".join(map(str, input_ids))))\n                 logger.info(\"input_mask: {}\".format(\" \".join(map(str, input_mask))))\n                 logger.info(\"segment_ids: {}\".format(\" \".join(map(str, segment_ids))))\n             if is_training:\n-                logger.info(\"label: {}\".format(label))\n+                logger.info(f\"label: {label}\")\n \n         features.append(InputFeatures(example_id=example.swag_id, choices_features=choices_features, label=label))\n \n@@ -382,14 +381,14 @@ def train(args, train_dataset, model, tokenizer):\n                     ):  # Only evaluate when single GPU otherwise metrics may not average well\n                         results = evaluate(args, model, tokenizer)\n                         for key, value in results.items():\n-                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n+                            tb_writer.add_scalar(f\"eval_{key}\", value, global_step)\n                     tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n                     tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n                     logging_loss = tr_loss\n \n                 if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n                     # Save model checkpoint\n-                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n+                    output_dir = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n                     model_to_save = (\n                         model.module if hasattr(model, \"module\") else model\n                     )  # Take care of distributed/parallel training\n@@ -423,7 +422,7 @@ def evaluate(args, model, tokenizer, prefix=\"\"):\n     eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n \n     # Eval!\n-    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n+    logger.info(f\"***** Running evaluation {prefix} *****\")\n     logger.info(\"  Num examples = %d\", len(dataset))\n     logger.info(\"  Batch size = %d\", args.eval_batch_size)\n \n@@ -466,7 +465,7 @@ def evaluate(args, model, tokenizer, prefix=\"\"):\n         logger.info(\"***** Eval results *****\")\n         for key in sorted(result.keys()):\n             logger.info(\"%s = %s\", key, str(result[key]))\n-            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n+            writer.write(\"{} = {}\\n\".format(key, str(result[key])))\n \n     return result\n \n@@ -710,10 +709,10 @@ def main():\n             # Evaluate\n             result = evaluate(args, model, tokenizer, prefix=global_step)\n \n-            result = {k + (\"_{}\".format(global_step) if global_step else \"\"): v for k, v in result.items()}\n+            result = {k + (f\"_{global_step}\" if global_step else \"\"): v for k, v in result.items()}\n             results.update(result)\n \n-    logger.info(\"Results: {}\".format(results))\n+    logger.info(f\"Results: {results}\")\n \n     return results\n "
        },
        {
            "sha": "7da9ee7fe9ca15f8930a128e59182a9ec1377ed0",
            "filename": "examples/legacy/run_transfo_xl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Frun_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Frun_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Frun_transfo_xl.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n #\n@@ -66,7 +65,7 @@ def main():\n         ptvsd.wait_for_attach()\n \n     device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n-    logger.info(\"device: {}\".format(device))\n+    logger.info(f\"device: {device}\")\n \n     # Load a pre-processed dataset\n     # You can also build the corpus yourself using TransfoXLCorpus methods\n@@ -111,7 +110,7 @@ def evaluate(eval_iter):\n                 total_loss += seq_len * loss.item()\n                 total_len += seq_len\n             total_time = time.time() - start_time\n-        logger.info(\"Time : {:.2f}s, {:.2f}ms/segment\".format(total_time, 1000 * total_time / (idx + 1)))\n+        logger.info(f\"Time : {total_time:.2f}s, {1000 * total_time / (idx + 1):.2f}ms/segment\")\n         return total_loss / total_len\n \n     # Run on test data."
        },
        {
            "sha": "93bd9839920d3b225b3a4806e416d4199387433d",
            "filename": "examples/legacy/seq2seq/old_test_fsmt_bleu_score.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fseq2seq%2Fold_test_fsmt_bleu_score.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fseq2seq%2Fold_test_fsmt_bleu_score.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fold_test_fsmt_bleu_score.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2020 Huggingface\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -13,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import io\n import json\n import unittest\n \n@@ -25,7 +23,7 @@\n \n \n filename = get_tests_dir() + \"/test_data/fsmt/fsmt_val_data.json\"\n-with io.open(filename, \"r\", encoding=\"utf-8\") as f:\n+with open(filename, encoding=\"utf-8\") as f:\n     bleu_data = json.load(f)\n \n "
        },
        {
            "sha": "41855eaed6b2f0d6fc650bcc301db171b17fbf04",
            "filename": "examples/legacy/seq2seq/run_distributed_eval.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fseq2seq%2Frun_distributed_eval.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fseq2seq%2Frun_distributed_eval.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Frun_distributed_eval.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -19,7 +19,6 @@\n from json import JSONDecodeError\n from logging import getLogger\n from pathlib import Path\n-from typing import Dict, List\n \n import torch\n from torch.utils.data import DataLoader\n@@ -55,10 +54,10 @@ def eval_data_dir(\n     task=\"summarization\",\n     local_rank=None,\n     num_return_sequences=1,\n-    dataset_kwargs: Dict = None,\n+    dataset_kwargs: dict = None,\n     prefix=\"\",\n     **generate_kwargs,\n-) -> Dict:\n+) -> dict:\n     \"\"\"Run evaluation on part of the data for one gpu and save to {save_dir}/rank_{rank}_output.json\"\"\"\n     model_name = str(model_name)\n     assert local_rank is not None\n@@ -211,7 +210,7 @@ def run_generate():\n         calc_bleu = \"translation\" in args.task\n         score_fn = calculate_bleu if calc_bleu else calculate_rouge\n         metric_name = \"bleu\" if calc_bleu else \"rouge\"\n-        metrics: Dict = score_fn(preds, labels)\n+        metrics: dict = score_fn(preds, labels)\n         metrics[\"n_obs\"] = len(preds)\n         runtime = time.time() - start_time\n         metrics[\"seconds_per_sample\"] = round(runtime / metrics[\"n_obs\"], 4)\n@@ -227,7 +226,7 @@ def run_generate():\n             shutil.rmtree(json_save_dir)\n \n \n-def combine_partial_results(partial_results) -> List:\n+def combine_partial_results(partial_results) -> list:\n     \"\"\"Concatenate partial results into one file, then sort it by id.\"\"\"\n     records = []\n     for partial_result in partial_results:\n@@ -237,7 +236,7 @@ def combine_partial_results(partial_results) -> List:\n     return preds\n \n \n-def gather_results_from_each_node(num_replicas, save_dir, timeout) -> List[Dict[str, List]]:\n+def gather_results_from_each_node(num_replicas, save_dir, timeout) -> list[dict[str, list]]:\n     # WAIT FOR lots of .json files\n     start_wait = time.time()\n     logger.info(\"waiting for all nodes to finish\")"
        },
        {
            "sha": "f5ef4f5d165431e64f641ec61eadc464a980bb92",
            "filename": "examples/legacy/seq2seq/run_eval.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fseq2seq%2Frun_eval.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fseq2seq%2Frun_eval.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Frun_eval.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -20,7 +20,6 @@\n import warnings\n from logging import getLogger\n from pathlib import Path\n-from typing import Dict, List\n \n import torch\n from tqdm import tqdm\n@@ -36,7 +35,7 @@\n \n \n def generate_summaries_or_translations(\n-    examples: List[str],\n+    examples: list[str],\n     out_file: str,\n     model_name: str,\n     batch_size: int = 8,\n@@ -45,7 +44,7 @@ def generate_summaries_or_translations(\n     task=\"summarization\",\n     prefix=None,\n     **generate_kwargs,\n-) -> Dict:\n+) -> dict:\n     \"\"\"Save model.generate results to <out_file>, and return how long it took.\"\"\"\n     fout = Path(out_file).open(\"w\", encoding=\"utf-8\")\n     model_name = str(model_name)"
        },
        {
            "sha": "e911eca57048ca5764177a7910e008950d6bc695",
            "filename": "examples/legacy/seq2seq/run_eval_search.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fseq2seq%2Frun_eval_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fseq2seq%2Frun_eval_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Frun_eval_search.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -34,7 +34,7 @@\n \n def parse_search_arg(search):\n     groups = search.split()\n-    entries = dict((g.split(\"=\") for g in groups))\n+    entries = dict(g.split(\"=\") for g in groups)\n     entry_names = list(entries.keys())\n     sets = [[f\"--{k} {v}\" for v in vs.split(\":\")] for k, vs in entries.items()]\n     matrix = [list(x) for x in itertools.product(*sets)]\n@@ -105,7 +105,7 @@ def run_search():\n     col_widths = {col: len(str(col)) for col in col_names}\n     results = []\n     for r in matrix:\n-        hparams = dict((x.replace(\"--\", \"\").split() for x in r))\n+        hparams = dict(x.replace(\"--\", \"\").split() for x in r)\n         args_exp = \" \".join(r).split()\n         args_exp.extend([\"--bs\", str(args.bs)])  # in case we need to reduce its size due to CUDA OOM\n         sys.argv = args_normal + args_exp"
        },
        {
            "sha": "afdde6614e2e5f33f822016f3f3a525705a11549",
            "filename": "examples/legacy/seq2seq/seq2seq_trainer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fseq2seq%2Fseq2seq_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fseq2seq%2Fseq2seq_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fseq2seq_trainer.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Any, Dict, List, Optional, Tuple, Union\n+from typing import Any, Optional, Union\n \n import torch\n from torch import nn\n@@ -172,10 +172,10 @@ def compute_loss(self, model, inputs):\n     def prediction_step(\n         self,\n         model: nn.Module,\n-        inputs: Dict[str, Union[torch.Tensor, Any]],\n+        inputs: dict[str, Union[torch.Tensor, Any]],\n         prediction_loss_only: bool,\n-        ignore_keys: Optional[List[str]] = None,\n-    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n+        ignore_keys: Optional[list[str]] = None,\n+    ) -> tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n         \"\"\"\n         Perform an evaluation step on :obj:`model` using obj:`inputs`.\n "
        },
        {
            "sha": "8f518822b4510ce36609a379fea5456075d2d245",
            "filename": "examples/legacy/seq2seq/test_data/fsmt/build-eval-data.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Ffsmt%2Fbuild-eval-data.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Ffsmt%2Fbuild-eval-data.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Ffsmt%2Fbuild-eval-data.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,6 +1,5 @@\n #!/usr/bin/env python\n \n-import io\n import json\n import subprocess\n \n@@ -29,5 +28,5 @@ def get_all_data(pairs, n_objs):\n \n text = get_all_data(pairs, n_objs)\n filename = \"./fsmt_val_data.json\"\n-with io.open(filename, \"w\", encoding=\"utf-8\") as f:\n+with open(filename, \"w\", encoding=\"utf-8\") as f:\n     bleu_data = json.dump(text, f, indent=2, ensure_ascii=False)"
        },
        {
            "sha": "001300f1869a3c0412a44ada4fe7393c156603da",
            "filename": "examples/legacy/seq2seq/utils.py",
            "status": "modified",
            "additions": 25,
            "deletions": 24,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fseq2seq%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Fseq2seq%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Futils.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -19,9 +19,10 @@\n import os\n import pickle\n import socket\n+from collections.abc import Iterable\n from logging import getLogger\n from pathlib import Path\n-from typing import Callable, Dict, Iterable, List, Tuple, Union\n+from typing import Callable, Union\n \n import git\n import numpy as np\n@@ -67,7 +68,7 @@ def label_smoothed_nll_loss(lprobs, target, epsilon, ignore_index=-100):\n     return loss, nll_loss\n \n \n-def lmap(f: Callable, x: Iterable) -> List:\n+def lmap(f: Callable, x: Iterable) -> list:\n     \"\"\"list(map(f, x))\"\"\"\n     return list(map(f, x))\n \n@@ -77,11 +78,11 @@ def calculate_bleu(output_lns, refs_lns, **kwargs) -> dict:\n     return {\"bleu\": round(corpus_bleu(output_lns, [refs_lns], **kwargs).score, 4)}\n \n \n-def build_compute_metrics_fn(task_name: str, tokenizer: PreTrainedTokenizer) -> Callable[[EvalPrediction], Dict]:\n+def build_compute_metrics_fn(task_name: str, tokenizer: PreTrainedTokenizer) -> Callable[[EvalPrediction], dict]:\n     def non_pad_len(tokens: np.ndarray) -> int:\n         return np.count_nonzero(tokens != tokenizer.pad_token_id)\n \n-    def decode_pred(pred: EvalPrediction) -> Tuple[List[str], List[str]]:\n+    def decode_pred(pred: EvalPrediction) -> tuple[list[str], list[str]]:\n         pred_ids = pred.predictions\n         label_ids = pred.label_ids\n         pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n@@ -91,16 +92,16 @@ def decode_pred(pred: EvalPrediction) -> Tuple[List[str], List[str]]:\n         label_str = lmap(str.strip, label_str)\n         return pred_str, label_str\n \n-    def summarization_metrics(pred: EvalPrediction) -> Dict:\n+    def summarization_metrics(pred: EvalPrediction) -> dict:\n         pred_str, label_str = decode_pred(pred)\n-        rouge: Dict = calculate_rouge(pred_str, label_str)\n+        rouge: dict = calculate_rouge(pred_str, label_str)\n         summ_len = np.round(np.mean(lmap(non_pad_len, pred.predictions)), 1)\n         rouge.update({\"gen_len\": summ_len})\n         return rouge\n \n-    def translation_metrics(pred: EvalPrediction) -> Dict:\n+    def translation_metrics(pred: EvalPrediction) -> dict:\n         pred_str, label_str = decode_pred(pred)\n-        bleu: Dict = calculate_bleu(pred_str, label_str)\n+        bleu: dict = calculate_bleu(pred_str, label_str)\n         gen_len = np.round(np.mean(lmap(non_pad_len, pred.predictions)), 1)\n         bleu.update({\"gen_len\": gen_len})\n         return bleu\n@@ -183,7 +184,7 @@ def num_tokens_in_example(i):\n             return min(self.src_lens[i], self.max_target_length)\n \n         # call fairseq cython function\n-        batch_sampler: List[List[int]] = batch_by_size(\n+        batch_sampler: list[list[int]] = batch_by_size(\n             sorted_indices,\n             num_tokens_fn=num_tokens_in_example,\n             max_tokens=max_tokens_per_batch,\n@@ -207,7 +208,7 @@ def collate_fn(self, batch):\n \n \n class LegacySeq2SeqDataset(AbstractSeq2SeqDataset):\n-    def __getitem__(self, index) -> Dict[str, torch.Tensor]:\n+    def __getitem__(self, index) -> dict[str, torch.Tensor]:\n         \"\"\"Call tokenizer on src and tgt_lines\"\"\"\n         index = index + 1  # linecache starts at 1\n         source_line = self.prefix + linecache.getline(str(self.src_file), index).rstrip(\"\\n\")\n@@ -237,7 +238,7 @@ def encode_line(self, tokenizer, line, max_length, pad_to_max_length=True, retur\n             **self.dataset_kwargs,\n         )\n \n-    def collate_fn(self, batch) -> Dict[str, torch.Tensor]:\n+    def collate_fn(self, batch) -> dict[str, torch.Tensor]:\n         input_ids = torch.stack([x[\"input_ids\"] for x in batch])\n         masks = torch.stack([x[\"attention_mask\"] for x in batch])\n         target_ids = torch.stack([x[\"labels\"] for x in batch])\n@@ -255,17 +256,17 @@ def collate_fn(self, batch) -> Dict[str, torch.Tensor]:\n class Seq2SeqDataset(AbstractSeq2SeqDataset):\n     \"\"\"A dataset that calls prepare_seq2seq_batch.\"\"\"\n \n-    def __getitem__(self, index) -> Dict[str, str]:\n+    def __getitem__(self, index) -> dict[str, str]:\n         index = index + 1  # linecache starts at 1\n         source_line = self.prefix + linecache.getline(str(self.src_file), index).rstrip(\"\\n\")\n         tgt_line = linecache.getline(str(self.tgt_file), index).rstrip(\"\\n\")\n         assert source_line, f\"empty source line for index {index}\"\n         assert tgt_line, f\"empty tgt line for index {index}\"\n         return {\"tgt_texts\": tgt_line, \"src_texts\": source_line, \"id\": index - 1}\n \n-    def collate_fn(self, batch) -> Dict[str, torch.Tensor]:\n+    def collate_fn(self, batch) -> dict[str, torch.Tensor]:\n         \"\"\"Call prepare_seq2seq_batch.\"\"\"\n-        batch_encoding: Dict[str, torch.Tensor] = self.tokenizer.prepare_seq2seq_batch(\n+        batch_encoding: dict[str, torch.Tensor] = self.tokenizer.prepare_seq2seq_batch(\n             [x[\"src_texts\"] for x in batch],\n             tgt_texts=[x[\"tgt_texts\"] for x in batch],\n             max_length=self.max_source_length,\n@@ -293,7 +294,7 @@ def __init__(self, tokenizer, data_args, decoder_start_token_id, tpu_num_cores=N\n         if data_args.tgt_lang is not None:\n             self.dataset_kwargs[\"tgt_lang\"] = data_args.tgt_lang\n \n-    def __call__(self, batch) -> Dict[str, torch.Tensor]:\n+    def __call__(self, batch) -> dict[str, torch.Tensor]:\n         if hasattr(self.tokenizer, \"prepare_seq2seq_batch\"):\n             batch = self._encode(batch)\n             input_ids, attention_mask, labels = (\n@@ -329,7 +330,7 @@ def _shift_right_t5(self, input_ids):\n         shifted_input_ids[..., 0] = self.pad_token_id\n         return shifted_input_ids\n \n-    def _encode(self, batch) -> Dict[str, torch.Tensor]:\n+    def _encode(self, batch) -> dict[str, torch.Tensor]:\n         batch_encoding = self.tokenizer.prepare_seq2seq_batch(\n             [x[\"src_texts\"] for x in batch],\n             tgt_texts=[x[\"tgt_texts\"] for x in batch],\n@@ -355,7 +356,7 @@ def __iter__(self):\n         return iter(sortish_sampler_indices(self.data, self.bs, shuffle=self.shuffle))\n \n \n-def sortish_sampler_indices(data: List, bs: int, shuffle=True) -> np.array:\n+def sortish_sampler_indices(data: list, bs: int, shuffle=True) -> np.array:\n     \"Go through the text data by order of src length with a bit of randomness. From fastai repo.\"\n     if not shuffle:\n         return np.argsort(np.array(data) * -1)\n@@ -455,7 +456,7 @@ def pickle_save(obj, path):\n         return pickle.dump(obj, f)\n \n \n-def flatten_list(summary_ids: List[List]):\n+def flatten_list(summary_ids: list[list]):\n     return list(itertools.chain.from_iterable(summary_ids))\n \n \n@@ -506,14 +507,14 @@ def extract_rouge_mid_statistics(dct):\n \n \n def calculate_rouge(\n-    pred_lns: List[str],\n-    tgt_lns: List[str],\n+    pred_lns: list[str],\n+    tgt_lns: list[str],\n     use_stemmer=True,\n     rouge_keys=ROUGE_KEYS,\n     return_precision_and_recall=False,\n     bootstrap_aggregation=True,\n     newline_sep=True,\n-) -> Dict:\n+) -> dict:\n     \"\"\"Calculate rouge using rouge_scorer package.\n \n     Args:\n@@ -590,19 +591,19 @@ def any_requires_grad(model: nn.Module) -> bool:\n \n \n def assert_all_frozen(model):\n-    model_grads: List[bool] = list(grad_status(model))\n+    model_grads: list[bool] = list(grad_status(model))\n     n_require_grad = sum(lmap(int, model_grads))\n     npars = len(model_grads)\n     assert not any(model_grads), f\"{n_require_grad / npars:.1%} of {npars} weights require grad\"\n \n \n def assert_not_all_frozen(model):\n-    model_grads: List[bool] = list(grad_status(model))\n+    model_grads: list[bool] = list(grad_status(model))\n     npars = len(model_grads)\n     assert any(model_grads), f\"none of {npars} weights require grad\"\n \n \n-def parse_numeric_n_bool_cl_kwargs(unparsed_args: List[str]) -> Dict[str, Union[int, float, bool]]:\n+def parse_numeric_n_bool_cl_kwargs(unparsed_args: list[str]) -> dict[str, Union[int, float, bool]]:\n     \"\"\"\n     Parse an argv list of unspecified command line args to a dict.\n     Assumes all values are either numeric or boolean in the form of true/false."
        },
        {
            "sha": "69b8a27ac799d757970d02f6042aa9e9c6c8e711",
            "filename": "examples/legacy/token-classification/run_ner.py",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Ftoken-classification%2Frun_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Ftoken-classification%2Frun_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Ftoken-classification%2Frun_ner.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n #\n@@ -20,7 +19,7 @@\n import sys\n from dataclasses import dataclass, field\n from importlib import import_module\n-from typing import Dict, List, Optional, Tuple\n+from typing import Optional\n \n import numpy as np\n from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\n@@ -159,7 +158,7 @@ def main():\n \n     # Prepare CONLL-2003 task\n     labels = token_classification_task.get_labels(data_args.labels)\n-    label_map: Dict[int, str] = dict(enumerate(labels))\n+    label_map: dict[int, str] = dict(enumerate(labels))\n     num_labels = len(labels)\n \n     # Load pretrained model and tokenizer\n@@ -217,7 +216,7 @@ def main():\n         else None\n     )\n \n-    def align_predictions(predictions: np.ndarray, label_ids: np.ndarray) -> Tuple[List[int], List[int]]:\n+    def align_predictions(predictions: np.ndarray, label_ids: np.ndarray) -> tuple[list[int], list[int]]:\n         preds = np.argmax(predictions, axis=2)\n \n         batch_size, seq_len = preds.shape\n@@ -233,7 +232,7 @@ def align_predictions(predictions: np.ndarray, label_ids: np.ndarray) -> Tuple[L\n \n         return preds_list, out_label_list\n \n-    def compute_metrics(p: EvalPrediction) -> Dict:\n+    def compute_metrics(p: EvalPrediction) -> dict:\n         preds_list, out_label_list = align_predictions(p.predictions, p.label_ids)\n         return {\n             \"accuracy_score\": accuracy_score(out_label_list, preds_list),\n@@ -279,7 +278,7 @@ def compute_metrics(p: EvalPrediction) -> Dict:\n                 logger.info(\"***** Eval results *****\")\n                 for key, value in result.items():\n                     logger.info(\"  %s = %s\", key, value)\n-                    writer.write(\"%s = %s\\n\" % (key, value))\n+                    writer.write(\"{} = {}\\n\".format(key, value))\n \n             results.update(result)\n \n@@ -304,13 +303,13 @@ def compute_metrics(p: EvalPrediction) -> Dict:\n             with open(output_test_results_file, \"w\") as writer:\n                 for key, value in metrics.items():\n                     logger.info(\"  %s = %s\", key, value)\n-                    writer.write(\"%s = %s\\n\" % (key, value))\n+                    writer.write(\"{} = {}\\n\".format(key, value))\n \n         # Save predictions\n         output_test_predictions_file = os.path.join(training_args.output_dir, \"test_predictions.txt\")\n         if trainer.is_world_process_zero():\n             with open(output_test_predictions_file, \"w\") as writer:\n-                with open(os.path.join(data_args.data_dir, \"test.txt\"), \"r\") as f:\n+                with open(os.path.join(data_args.data_dir, \"test.txt\")) as f:\n                     token_classification_task.write_predictions_to_file(writer, f, preds_list)\n \n     return results"
        },
        {
            "sha": "40ecf2b32acc3545a7f6d6eb3302e270fec1bd04",
            "filename": "examples/legacy/token-classification/scripts/preprocess.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Ftoken-classification%2Fscripts%2Fpreprocess.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Ftoken-classification%2Fscripts%2Fpreprocess.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Ftoken-classification%2Fscripts%2Fpreprocess.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -12,7 +12,7 @@\n tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n max_len -= tokenizer.num_special_tokens_to_add()\n \n-with open(dataset, \"rt\") as f_p:\n+with open(dataset) as f_p:\n     for line in f_p:\n         line = line.rstrip()\n "
        },
        {
            "sha": "7e406fa7757ad6f43a6ab026a0182ce24c13e325",
            "filename": "examples/legacy/token-classification/tasks.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Ftoken-classification%2Ftasks.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Ftoken-classification%2Ftasks.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Ftoken-classification%2Ftasks.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,6 +1,6 @@\n import logging\n import os\n-from typing import List, TextIO, Union\n+from typing import TextIO, Union\n \n from conllu import parse_incr\n from utils_ner import InputExample, Split, TokenClassificationTask\n@@ -14,7 +14,7 @@ def __init__(self, label_idx=-1):\n         # in NER datasets, the last column is usually reserved for NER label\n         self.label_idx = label_idx\n \n-    def read_examples_from_file(self, data_dir, mode: Union[Split, str]) -> List[InputExample]:\n+    def read_examples_from_file(self, data_dir, mode: Union[Split, str]) -> list[InputExample]:\n         if isinstance(mode, Split):\n             mode = mode.value\n         file_path = os.path.join(data_dir, f\"{mode}.txt\")\n@@ -42,7 +42,7 @@ def read_examples_from_file(self, data_dir, mode: Union[Split, str]) -> List[Inp\n                 examples.append(InputExample(guid=f\"{mode}-{guid_index}\", words=words, labels=labels))\n         return examples\n \n-    def write_predictions_to_file(self, writer: TextIO, test_input_reader: TextIO, preds_list: List):\n+    def write_predictions_to_file(self, writer: TextIO, test_input_reader: TextIO, preds_list: list):\n         example_id = 0\n         for line in test_input_reader:\n             if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n@@ -55,9 +55,9 @@ def write_predictions_to_file(self, writer: TextIO, test_input_reader: TextIO, p\n             else:\n                 logger.warning(\"Maximum sequence length exceeded: No prediction for '%s'.\", line.split()[0])\n \n-    def get_labels(self, path: str) -> List[str]:\n+    def get_labels(self, path: str) -> list[str]:\n         if path:\n-            with open(path, \"r\") as f:\n+            with open(path) as f:\n                 labels = f.read().splitlines()\n             if \"O\" not in labels:\n                 labels = [\"O\"] + labels\n@@ -71,9 +71,9 @@ def __init__(self):\n         # in CONLL2003 dataset chunk column is second-to-last\n         super().__init__(label_idx=-2)\n \n-    def get_labels(self, path: str) -> List[str]:\n+    def get_labels(self, path: str) -> list[str]:\n         if path:\n-            with open(path, \"r\") as f:\n+            with open(path) as f:\n                 labels = f.read().splitlines()\n             if \"O\" not in labels:\n                 labels = [\"O\"] + labels\n@@ -105,7 +105,7 @@ def get_labels(self, path: str) -> List[str]:\n \n \n class POS(TokenClassificationTask):\n-    def read_examples_from_file(self, data_dir, mode: Union[Split, str]) -> List[InputExample]:\n+    def read_examples_from_file(self, data_dir, mode: Union[Split, str]) -> list[InputExample]:\n         if isinstance(mode, Split):\n             mode = mode.value\n         file_path = os.path.join(data_dir, f\"{mode}.txt\")\n@@ -125,7 +125,7 @@ def read_examples_from_file(self, data_dir, mode: Union[Split, str]) -> List[Inp\n                     guid_index += 1\n         return examples\n \n-    def write_predictions_to_file(self, writer: TextIO, test_input_reader: TextIO, preds_list: List):\n+    def write_predictions_to_file(self, writer: TextIO, test_input_reader: TextIO, preds_list: list):\n         example_id = 0\n         for sentence in parse_incr(test_input_reader):\n             s_p = preds_list[example_id]\n@@ -136,9 +136,9 @@ def write_predictions_to_file(self, writer: TextIO, test_input_reader: TextIO, p\n             writer.write(out)\n             example_id += 1\n \n-    def get_labels(self, path: str) -> List[str]:\n+    def get_labels(self, path: str) -> list[str]:\n         if path:\n-            with open(path, \"r\") as f:\n+            with open(path) as f:\n                 return f.read().splitlines()\n         else:\n             return ["
        },
        {
            "sha": "0c1725b59b4ef0d4e6a45480d6049473b04b84b9",
            "filename": "examples/legacy/token-classification/utils_ner.py",
            "status": "modified",
            "additions": 17,
            "deletions": 18,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Ftoken-classification%2Futils_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Flegacy%2Ftoken-classification%2Futils_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Ftoken-classification%2Futils_ner.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n #\n@@ -19,7 +18,7 @@\n import os\n from dataclasses import dataclass\n from enum import Enum\n-from typing import List, Optional, Union\n+from typing import Optional, Union\n \n from filelock import FileLock\n \n@@ -42,8 +41,8 @@ class InputExample:\n     \"\"\"\n \n     guid: str\n-    words: List[str]\n-    labels: Optional[List[str]]\n+    words: list[str]\n+    labels: Optional[list[str]]\n \n \n @dataclass\n@@ -53,10 +52,10 @@ class InputFeatures:\n     Property names are the same names as the corresponding inputs to a model.\n     \"\"\"\n \n-    input_ids: List[int]\n-    attention_mask: List[int]\n-    token_type_ids: Optional[List[int]] = None\n-    label_ids: Optional[List[int]] = None\n+    input_ids: list[int]\n+    attention_mask: list[int]\n+    token_type_ids: Optional[list[int]] = None\n+    label_ids: Optional[list[int]] = None\n \n \n class Split(Enum):\n@@ -67,17 +66,17 @@ class Split(Enum):\n \n class TokenClassificationTask:\n     @staticmethod\n-    def read_examples_from_file(data_dir, mode: Union[Split, str]) -> List[InputExample]:\n+    def read_examples_from_file(data_dir, mode: Union[Split, str]) -> list[InputExample]:\n         raise NotImplementedError\n \n     @staticmethod\n-    def get_labels(path: str) -> List[str]:\n+    def get_labels(path: str) -> list[str]:\n         raise NotImplementedError\n \n     @staticmethod\n     def convert_examples_to_features(\n-        examples: List[InputExample],\n-        label_list: List[str],\n+        examples: list[InputExample],\n+        label_list: list[str],\n         max_seq_length: int,\n         tokenizer: PreTrainedTokenizer,\n         cls_token_at_end=False,\n@@ -91,7 +90,7 @@ def convert_examples_to_features(\n         pad_token_label_id=-100,\n         sequence_a_segment_id=0,\n         mask_padding_with_zero=True,\n-    ) -> List[InputFeatures]:\n+    ) -> list[InputFeatures]:\n         \"\"\"Loads a data file into a list of `InputFeatures`\n         `cls_token_at_end` define the location of the CLS token:\n             - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n@@ -214,7 +213,7 @@ class TokenClassificationDataset(Dataset):\n         soon.\n         \"\"\"\n \n-        features: List[InputFeatures]\n+        features: list[InputFeatures]\n         pad_token_label_id: int = nn.CrossEntropyLoss().ignore_index\n         # Use cross entropy ignore_index as padding label id so that only\n         # real label ids contribute to the loss later.\n@@ -224,7 +223,7 @@ def __init__(\n             token_classification_task: TokenClassificationTask,\n             data_dir: str,\n             tokenizer: PreTrainedTokenizer,\n-            labels: List[str],\n+            labels: list[str],\n             model_type: str,\n             max_seq_length: Optional[int] = None,\n             overwrite_cache=False,\n@@ -233,7 +232,7 @@ def __init__(\n             # Load data features from cache or dataset file\n             cached_features_file = os.path.join(\n                 data_dir,\n-                \"cached_{}_{}_{}\".format(mode.value, tokenizer.__class__.__name__, str(max_seq_length)),\n+                f\"cached_{mode.value}_{tokenizer.__class__.__name__}_{str(max_seq_length)}\",\n             )\n \n             # Make sure only the first process in distributed training processes the dataset,\n@@ -283,7 +282,7 @@ class TFTokenClassificationDataset:\n         soon.\n         \"\"\"\n \n-        features: List[InputFeatures]\n+        features: list[InputFeatures]\n         pad_token_label_id: int = -100\n         # Use cross entropy ignore_index as padding label id so that only\n         # real label ids contribute to the loss later.\n@@ -293,7 +292,7 @@ def __init__(\n             token_classification_task: TokenClassificationTask,\n             data_dir: str,\n             tokenizer: PreTrainedTokenizer,\n-            labels: List[str],\n+            labels: list[str],\n             model_type: str,\n             max_seq_length: Optional[int] = None,\n             overwrite_cache=False,"
        },
        {
            "sha": "8320a25228c2139bcdb8d5af16c8feeb6951e075",
            "filename": "examples/modular-transformers/image_processing_new_imgproc_model.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fimage_processing_new_imgproc_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fimage_processing_new_imgproc_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fimage_processing_new_imgproc_model.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -4,7 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_new_imgproc_model.py file directly. One of our CI enforces this.\n #                \n-from typing import Dict, List, Optional, Union\n+from typing import Optional, Union\n \n import numpy as np\n import torch\n@@ -74,13 +74,13 @@ class ImgprocModelImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: dict[str, int] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n         do_convert_rgb: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -101,7 +101,7 @@ def __init__(\n     def resize(\n         self,\n         image: np.ndarray,\n-        size: Dict[str, int],\n+        size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -151,13 +151,13 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Optional[Dict[str, int]] = None,\n+        size: Optional[dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         do_convert_rgb: bool = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,"
        },
        {
            "sha": "76b6dafb69a59267363a0f3b42f12e8f14ef0640",
            "filename": "examples/modular-transformers/modeling_add_function.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodeling_add_function.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodeling_add_function.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_add_function.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -5,7 +5,7 @@\n #                          modular_add_function.py file directly. One of our CI enforces this.\n #                \n # Note that zamba does not have the `apply_rotary_pos_emb` function!\n-from typing import Optional, Tuple\n+from typing import Optional\n \n import torch\n from torch import nn\n@@ -62,5 +62,5 @@ class TestAttention(nn.Module):\n     def __init__(self):\n         pass\n \n-    def forward(self) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    def forward(self) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         _ = apply_rotary_pos_emb(1, 1, 1, 1)"
        },
        {
            "sha": "f793923cb2fc3a8a03f6345abe92e57f3d03a4e4",
            "filename": "examples/modular-transformers/modeling_dummy.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -5,7 +5,7 @@\n #                          modular_dummy.py file directly. One of our CI enforces this.\n #                \n from functools import partial\n-from typing import Callable, Optional, Tuple, Union\n+from typing import Callable, Optional, Union\n \n import torch\n from torch import nn\n@@ -223,12 +223,12 @@ def __init__(self, config: DummyConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -290,9 +290,9 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n@@ -494,7 +494,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> Union[tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states"
        },
        {
            "sha": "5de9ab576e5cc93137bdd39403e0e7e444c3ecb8",
            "filename": "examples/modular-transformers/modeling_dummy_bert.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -6,7 +6,7 @@\n #                \n import math\n import os\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n from packaging import version\n@@ -136,9 +136,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         mixed_query_layer = self.query(hidden_states)\n \n         # If this is instantiated as a cross-attention module, the keys\n@@ -245,9 +245,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n             logger.warning_once(\n@@ -386,9 +386,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask,\n@@ -454,9 +454,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n         self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n@@ -532,12 +532,12 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -858,12 +858,12 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if"
        },
        {
            "sha": "4385ac7bcf389d82bb34d2e90587cb005244c52f",
            "filename": "examples/modular-transformers/modeling_from_uppercase_model.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodeling_from_uppercase_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodeling_from_uppercase_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_from_uppercase_model.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -4,7 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_from_uppercase_model.py file directly. One of our CI enforces this.\n #                \n-from typing import Optional, Tuple\n+from typing import Optional\n \n import torch\n from torch import nn\n@@ -53,7 +53,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         bsz, tgt_len, embed_dim = hidden_states.size()\n@@ -148,7 +148,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         output_attentions = False\n \n         batch_size, q_len, _ = hidden_states.size()\n@@ -226,7 +226,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n             logger.warning_once(\n@@ -322,7 +322,7 @@ def forward(\n         attention_mask: torch.Tensor,\n         causal_attention_mask: torch.Tensor,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.FloatTensor]:\n+    ) -> tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`"
        },
        {
            "sha": "e3ecbdd6974d2af3700e3390ca3ddb9c5240ec6c",
            "filename": "examples/modular-transformers/modeling_multimodal1.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -5,7 +5,7 @@\n #                          modular_multimodal1.py file directly. One of our CI enforces this.\n #                \n from functools import partial\n-from typing import Callable, Optional, Tuple, Union\n+from typing import Callable, Optional, Union\n \n import torch\n from torch import nn\n@@ -223,12 +223,12 @@ def __init__(self, config: Multimodal1TextConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -290,9 +290,9 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n@@ -494,7 +494,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> Union[tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states"
        },
        {
            "sha": "a2a9d460bda573ddbd5dca2e927f60e869b01b52",
            "filename": "examples/modular-transformers/modeling_multimodal2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -5,7 +5,7 @@\n #                          modular_multimodal2.py file directly. One of our CI enforces this.\n #                \n \n-from typing import Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n from torch import nn\n@@ -65,7 +65,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         bsz, tgt_len, embed_dim = hidden_states.size()\n@@ -152,7 +152,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n             logger.warning_once(\n@@ -233,7 +233,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         output_attentions = False\n \n         batch_size, q_len, _ = hidden_states.size()\n@@ -334,7 +334,7 @@ def forward(\n         attention_mask: torch.Tensor,\n         causal_attention_mask: torch.Tensor,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.FloatTensor]:\n+    ) -> tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -392,7 +392,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutput]:\n+    ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n@@ -587,7 +587,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = False,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Returns:\n \n@@ -671,7 +671,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Returns:\n "
        },
        {
            "sha": "712d68c100fea6509d28b924eaef5f3ec94ff589",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -4,7 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_my_new_model2.py file directly. One of our CI enforces this.\n #                \n-from typing import Callable, List, Optional, Tuple, Union\n+from typing import Callable, Optional, Union\n \n import torch\n from torch import nn\n@@ -222,12 +222,12 @@ def __init__(self, config: MyNewModel2Config, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -289,9 +289,9 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n@@ -485,15 +485,15 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,  # NOOP kwarg for now\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> Union[tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -753,14 +753,14 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> Union[tuple, SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,"
        },
        {
            "sha": "d449ca50a21833d9fb74b4d1aded342fa558ec2f",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -5,7 +5,7 @@\n #                          modular_new_task_model.py file directly. One of our CI enforces this.\n #                \n from dataclasses import dataclass\n-from typing import ClassVar, List, Optional, Tuple, Union\n+from typing import ClassVar, Optional, Union\n \n import torch\n from torch import nn\n@@ -61,9 +61,9 @@ class NewTaskModelCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: torch.FloatTensor = None\n-    past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n@@ -337,7 +337,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -347,7 +347,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         num_logits_to_keep: int = 0,\n-    ) -> Union[Tuple, NewTaskModelCausalLMOutputWithPast]:\n+    ) -> Union[tuple, NewTaskModelCausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "b03774e22851e3b84be110b36eb6b058fbbe45b4",
            "filename": "examples/modular-transformers/modeling_roberta.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_roberta.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -6,7 +6,7 @@\n #                \n import math\n import os\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n import torch.nn as nn\n@@ -139,9 +139,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         mixed_query_layer = self.query(hidden_states)\n \n         # If this is instantiated as a cross-attention module, the keys\n@@ -248,9 +248,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n             logger.warning_once(\n@@ -389,9 +389,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask,\n@@ -457,9 +457,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n         self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n@@ -535,12 +535,12 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -861,12 +861,12 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if"
        },
        {
            "sha": "2ab9865f4385daead6e057ed0911902fa9f4d438",
            "filename": "examples/modular-transformers/modeling_super.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_super.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -4,7 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_super.py file directly. One of our CI enforces this.\n #                \n-from typing import Callable, List, Optional, Tuple, Union\n+from typing import Callable, Optional, Union\n \n import torch\n from torch import nn\n@@ -222,12 +222,12 @@ def __init__(self, config: SuperConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -289,9 +289,9 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n@@ -485,14 +485,14 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> Union[tuple, BaseModelOutputWithPast]:\n         out = super().forward(\n             input_ids,\n             attention_mask,"
        },
        {
            "sha": "75811d8681c0574aaf011bf0d8854035e4253b5f",
            "filename": "examples/modular-transformers/modeling_switch_function.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodeling_switch_function.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodeling_switch_function.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_switch_function.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -5,7 +5,7 @@\n #                          modular_switch_function.py file directly. One of our CI enforces this.\n #                \n # Note that llama and cohere have different definitions for rotate_half\n-from typing import Callable, Optional, Tuple\n+from typing import Callable, Optional\n \n import torch\n from torch import nn\n@@ -123,12 +123,12 @@ def __init__(self, config: SwitchFunctionConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "34d2cd1b335e075e84c15f3c38c9c613df6eb542",
            "filename": "examples/modular-transformers/modular_dummy_bert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodular_dummy_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodular_dummy_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_dummy_bert.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,4 +1,4 @@\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n \n@@ -18,10 +18,10 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         return super().forward(input_ids)"
        },
        {
            "sha": "f1943e37e1f92bcd0f18703e5879a3a157c2102b",
            "filename": "examples/modular-transformers/modular_new_task_model.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodular_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodular_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_new_task_model.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,4 +1,4 @@\n-from typing import ClassVar, List, Optional, Union\n+from typing import ClassVar, Optional, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -29,7 +29,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "f7a3f46d44ab084e00323ca989448a73edbc4b5a",
            "filename": "examples/modular-transformers/modular_super.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodular_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fmodular-transformers%2Fmodular_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_super.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,4 +1,4 @@\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n \n@@ -15,14 +15,14 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> Union[tuple, CausalLMOutputWithPast]:\n         out = super().forward(\n             input_ids,\n             attention_mask,"
        },
        {
            "sha": "8ea627dbdd798f0859d13f3ea0c489edbce92deb",
            "filename": "examples/pytorch/audio-classification/run_audio_classification.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Faudio-classification%2Frun_audio_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Faudio-classification%2Frun_audio_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Faudio-classification%2Frun_audio_classification.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "2f5d8eef7c952edf82300f32b9bd366651d6ef3f",
            "filename": "examples/pytorch/contrastive-image-text/run_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fcontrastive-image-text%2Frun_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fcontrastive-image-text%2Frun_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontrastive-image-text%2Frun_clip.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Team All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "9d70c80ea6e216d89c2b1ddf139cdefd8ad302f0",
            "filename": "examples/pytorch/image-classification/run_image_classification.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "67ecbf6885a5c74165401193d70ba9ac46eec04f",
            "filename": "examples/pytorch/image-classification/run_image_classification_no_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification_no_trainer.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "f39fcd17c0d2a362993200954d44232678ac9a53",
            "filename": "examples/pytorch/image-pretraining/run_mae.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fimage-pretraining%2Frun_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fimage-pretraining%2Frun_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-pretraining%2Frun_mae.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "842e7c8d606156c0c5b67e857b72dbe136bbc964",
            "filename": "examples/pytorch/image-pretraining/run_mim.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fimage-pretraining%2Frun_mim.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fimage-pretraining%2Frun_mim.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-pretraining%2Frun_mim.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "e33b05c829ed41fa8d3bf9d30eb5a4cd05668ec5",
            "filename": "examples/pytorch/image-pretraining/run_mim_no_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fimage-pretraining%2Frun_mim_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fimage-pretraining%2Frun_mim_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-pretraining%2Frun_mim_no_trainer.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "6c4087120de2a7b6553981a28b0a8381afb087b8",
            "filename": "examples/pytorch/instance-segmentation/run_instance_segmentation.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Finstance-segmentation%2Frun_instance_segmentation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Finstance-segmentation%2Frun_instance_segmentation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Finstance-segmentation%2Frun_instance_segmentation.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -18,9 +17,10 @@\n import logging\n import os\n import sys\n+from collections.abc import Mapping\n from dataclasses import dataclass, field\n from functools import partial\n-from typing import Any, Dict, List, Mapping, Optional\n+from typing import Any, Optional\n \n import albumentations as A\n import numpy as np\n@@ -200,7 +200,7 @@ def get_metric(self):\n     def reset_metric(self):\n         self.metric.reset()\n \n-    def postprocess_target_batch(self, target_batch) -> List[Dict[str, torch.Tensor]]:\n+    def postprocess_target_batch(self, target_batch) -> list[dict[str, torch.Tensor]]:\n         \"\"\"Collect targets in a form of list of dictionaries with keys \"masks\", \"labels\".\"\"\"\n         batch_masks = target_batch[0]\n         batch_labels = target_batch[1]\n@@ -214,13 +214,13 @@ def postprocess_target_batch(self, target_batch) -> List[Dict[str, torch.Tensor]\n             )\n         return post_processed_targets\n \n-    def get_target_sizes(self, post_processed_targets) -> List[List[int]]:\n+    def get_target_sizes(self, post_processed_targets) -> list[list[int]]:\n         target_sizes = []\n         for target in post_processed_targets:\n             target_sizes.append(target[\"masks\"].shape[-2:])\n         return target_sizes\n \n-    def postprocess_prediction_batch(self, prediction_batch, target_sizes) -> List[Dict[str, torch.Tensor]]:\n+    def postprocess_prediction_batch(self, prediction_batch, target_sizes) -> list[dict[str, torch.Tensor]]:\n         \"\"\"Collect predictions in a form of list of dictionaries with keys \"masks\", \"labels\", \"scores\".\"\"\"\n \n         model_output = ModelOutput(class_queries_logits=prediction_batch[0], masks_queries_logits=prediction_batch[1])"
        },
        {
            "sha": "f130a77887e72b9b5fbaa5585a4d6663c473cc5b",
            "filename": "examples/pytorch/instance-segmentation/run_instance_segmentation_no_trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Finstance-segmentation%2Frun_instance_segmentation_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Finstance-segmentation%2Frun_instance_segmentation_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Finstance-segmentation%2Frun_instance_segmentation_no_trainer.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -21,9 +20,10 @@\n import math\n import os\n import sys\n+from collections.abc import Mapping\n from functools import partial\n from pathlib import Path\n-from typing import Any, Mapping\n+from typing import Any\n \n import albumentations as A\n import datasets"
        },
        {
            "sha": "44869b004b9d055669720639d70b72214bc9ad74",
            "filename": "examples/pytorch/language-modeling/run_clm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "530ff5a3fa59dec22c0766314b5a04cf78999863",
            "filename": "examples/pytorch/language-modeling/run_clm_no_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm_no_trainer.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "ac97a3c10d47742c259edba6ceaefb65cbfacd9e",
            "filename": "examples/pytorch/language-modeling/run_fim.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -551,7 +550,7 @@ def main():\n                 covariance_matrix=1e-5 * sigma,\n             )\n             new_token_embeddings = torch.stack(\n-                tuple((dist.sample() for _ in range(len(special_tokens)))),\n+                tuple(dist.sample() for _ in range(len(special_tokens))),\n                 dim=0,\n             )\n     else:\n@@ -571,7 +570,7 @@ def main():\n             covariance_matrix=1e-5 * sigma,\n         )\n         new_token_embeddings = torch.stack(\n-            tuple((dist.sample() for _ in range(len(special_tokens)))),\n+            tuple(dist.sample() for _ in range(len(special_tokens))),\n             dim=0,\n         )\n "
        },
        {
            "sha": "ae5d22c3a0abf909793be38e1f2609a3e144a6b5",
            "filename": "examples/pytorch/language-modeling/run_fim_no_trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim_no_trainer.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -518,7 +517,7 @@ def main():\n                 covariance_matrix=1e-5 * sigma,\n             )\n             new_token_embeddings = torch.stack(\n-                tuple((dist.sample() for _ in range(len(special_tokens)))),\n+                tuple(dist.sample() for _ in range(len(special_tokens))),\n                 dim=0,\n             )\n     else:\n@@ -538,7 +537,7 @@ def main():\n             covariance_matrix=1e-5 * sigma,\n         )\n         new_token_embeddings = torch.stack(\n-            tuple((dist.sample() for _ in range(len(special_tokens)))),\n+            tuple(dist.sample() for _ in range(len(special_tokens))),\n             dim=0,\n         )\n "
        },
        {
            "sha": "7a9abff0e30ecad631ad4ccfd4ad6487d955fb95",
            "filename": "examples/pytorch/language-modeling/run_mlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2020 The HuggingFace Team All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "c57e65c6e65dcd78f7f1f19bfd3a04d788d40153",
            "filename": "examples/pytorch/language-modeling/run_mlm_no_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm_no_trainer.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "0eda261d17715b7b41b7bb2fa2b5791572c07317",
            "filename": "examples/pytorch/language-modeling/run_plm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Flanguage-modeling%2Frun_plm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Flanguage-modeling%2Frun_plm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_plm.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2020 The HuggingFace Team All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "3817a2d55b7a3bdf07420449722065fbfd1147e3",
            "filename": "examples/pytorch/multiple-choice/run_swag.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright The HuggingFace Team and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "f6f140e6d6030ced58d811f57b166c0e6f668b0e",
            "filename": "examples/pytorch/multiple-choice/run_swag_no_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag_no_trainer.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright The HuggingFace Team and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "cc319d331e2f0dec5547589ac4362607ba1f62e4",
            "filename": "examples/pytorch/object-detection/run_object_detection.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -18,9 +17,10 @@\n import logging\n import os\n import sys\n+from collections.abc import Mapping\n from dataclasses import dataclass, field\n from functools import partial\n-from typing import Any, List, Mapping, Optional, Tuple, Union\n+from typing import Any, Optional, Union\n \n import albumentations as A\n import numpy as np\n@@ -60,7 +60,7 @@ class ModelOutput:\n \n \n def format_image_annotations_as_coco(\n-    image_id: str, categories: List[int], areas: List[float], bboxes: List[Tuple[float]]\n+    image_id: str, categories: list[int], areas: list[float], bboxes: list[tuple[float]]\n ) -> dict:\n     \"\"\"Format one set of image annotations to the COCO format\n \n@@ -94,7 +94,7 @@ def format_image_annotations_as_coco(\n     }\n \n \n-def convert_bbox_yolo_to_pascal(boxes: torch.Tensor, image_size: Tuple[int, int]) -> torch.Tensor:\n+def convert_bbox_yolo_to_pascal(boxes: torch.Tensor, image_size: tuple[int, int]) -> torch.Tensor:\n     \"\"\"\n     Convert bounding boxes from YOLO format (x_center, y_center, width, height) in range [0, 1]\n     to Pascal VOC format (x_min, y_min, x_max, y_max) in absolute coordinates.\n@@ -148,7 +148,7 @@ def augment_and_transform_batch(\n     return result\n \n \n-def collate_fn(batch: List[BatchFeature]) -> Mapping[str, Union[torch.Tensor, List[Any]]]:\n+def collate_fn(batch: list[BatchFeature]) -> Mapping[str, Union[torch.Tensor, list[Any]]]:\n     data = {}\n     data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n     data[\"labels\"] = [x[\"labels\"] for x in batch]"
        },
        {
            "sha": "db69a00a5776151e75fd25626cea3f3c97a3824f",
            "filename": "examples/pytorch/object-detection/run_object_detection_no_trainer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection_no_trainer.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -19,9 +18,10 @@\n import logging\n import math\n import os\n+from collections.abc import Mapping\n from functools import partial\n from pathlib import Path\n-from typing import Any, List, Mapping, Tuple, Union\n+from typing import Any, Union\n \n import albumentations as A\n import datasets\n@@ -61,7 +61,7 @@\n \n # Copied from examples/pytorch/object-detection/run_object_detection.format_image_annotations_as_coco\n def format_image_annotations_as_coco(\n-    image_id: str, categories: List[int], areas: List[float], bboxes: List[Tuple[float]]\n+    image_id: str, categories: list[int], areas: list[float], bboxes: list[tuple[float]]\n ) -> dict:\n     \"\"\"Format one set of image annotations to the COCO format\n \n@@ -96,7 +96,7 @@ def format_image_annotations_as_coco(\n \n \n # Copied from examples/pytorch/object-detection/run_object_detection.convert_bbox_yolo_to_pascal\n-def convert_bbox_yolo_to_pascal(boxes: torch.Tensor, image_size: Tuple[int, int]) -> torch.Tensor:\n+def convert_bbox_yolo_to_pascal(boxes: torch.Tensor, image_size: tuple[int, int]) -> torch.Tensor:\n     \"\"\"\n     Convert bounding boxes from YOLO format (x_center, y_center, width, height) in range [0, 1]\n     to Pascal VOC format (x_min, y_min, x_max, y_max) in absolute coordinates.\n@@ -152,7 +152,7 @@ def augment_and_transform_batch(\n \n \n # Copied from examples/pytorch/object-detection/run_object_detection.collate_fn\n-def collate_fn(batch: List[BatchFeature]) -> Mapping[str, Union[torch.Tensor, List[Any]]]:\n+def collate_fn(batch: list[BatchFeature]) -> Mapping[str, Union[torch.Tensor, list[Any]]]:\n     data = {}\n     data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n     data[\"labels\"] = [x[\"labels\"] for x in batch]"
        },
        {
            "sha": "b3101aa06b98a30c6246d28cce46d04c87e6de39",
            "filename": "examples/pytorch/old_test_xla_examples.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fold_test_xla_examples.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fold_test_xla_examples.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fold_test_xla_examples.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 HuggingFace Inc..\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -33,7 +32,7 @@ def get_results(output_dir):\n     results = {}\n     path = os.path.join(output_dir, \"all_results.json\")\n     if os.path.exists(path):\n-        with open(path, \"r\") as f:\n+        with open(path) as f:\n             results = json.load(f)\n     else:\n         raise ValueError(f\"can't find {path}\")"
        },
        {
            "sha": "dbdc52cedea2e19aa0513344627bf63368c325f7",
            "filename": "examples/pytorch/question-answering/run_qa.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fquestion-answering%2Frun_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fquestion-answering%2Frun_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_qa.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2020 The HuggingFace Team All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "6ea909a7da067b6b52f4964d5b8555d01dd0cd2d",
            "filename": "examples/pytorch/question-answering/run_qa_beam_search.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2020 The HuggingFace Team All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "84bead830a7d7817a3ded68ece2e88b8c4709f87",
            "filename": "examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search_no_trainer.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "138dd61f99a11c9b2fe9533fd9ffeb94b30368c1",
            "filename": "examples/pytorch/question-answering/run_qa_no_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_no_trainer.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "a07e34f091dbcd5504d8aaab9f4424a423fd290e",
            "filename": "examples/pytorch/question-answering/run_seq2seq_qa.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fquestion-answering%2Frun_seq2seq_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fquestion-answering%2Frun_seq2seq_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_seq2seq_qa.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Team All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -22,7 +21,7 @@\n import os\n import sys\n from dataclasses import dataclass, field\n-from typing import List, Optional, Tuple\n+from typing import Optional\n \n import datasets\n import evaluate\n@@ -469,7 +468,7 @@ def preprocess_squad_batch(\n         question_column: str,\n         context_column: str,\n         answer_column: str,\n-    ) -> Tuple[List[str], List[str]]:\n+    ) -> tuple[list[str], list[str]]:\n         questions = examples[question_column]\n         contexts = examples[context_column]\n         answers = examples[answer_column]"
        },
        {
            "sha": "3948391f6335ff971af720243a6a80d7beb818da",
            "filename": "examples/pytorch/question-answering/trainer_qa.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_qa.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2020 The HuggingFace Team All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "2492f601316a31ffd3cf5fc47e27c98a29eedd99",
            "filename": "examples/pytorch/question-answering/trainer_seq2seq_qa.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_seq2seq_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_seq2seq_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_seq2seq_qa.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Team All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -18,7 +17,7 @@\n \n import math\n import time\n-from typing import Dict, List, Optional\n+from typing import Optional\n \n from torch.utils.data import Dataset\n \n@@ -42,10 +41,10 @@ def evaluate(\n         self,\n         eval_dataset: Optional[Dataset] = None,\n         eval_examples=None,\n-        ignore_keys: Optional[List[str]] = None,\n+        ignore_keys: Optional[list[str]] = None,\n         metric_key_prefix: str = \"eval\",\n         **gen_kwargs,\n-    ) -> Dict[str, float]:\n+    ) -> dict[str, float]:\n         gen_kwargs = gen_kwargs.copy()\n \n         # Use legacy argument setting if a) the option is not explicitly passed; and b) the argument is set in the"
        },
        {
            "sha": "f0cc5c26a6927179d83dab56d21f4acb357884ee",
            "filename": "examples/pytorch/question-answering/utils_qa.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fquestion-answering%2Futils_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fquestion-answering%2Futils_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Futils_qa.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2020 The HuggingFace Team All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -20,7 +19,7 @@\n import json\n import logging\n import os\n-from typing import Optional, Tuple\n+from typing import Optional\n \n import numpy as np\n from tqdm.auto import tqdm\n@@ -32,7 +31,7 @@\n def postprocess_qa_predictions(\n     examples,\n     features,\n-    predictions: Tuple[np.ndarray, np.ndarray],\n+    predictions: tuple[np.ndarray, np.ndarray],\n     version_2_with_negative: bool = False,\n     n_best_size: int = 20,\n     max_answer_length: int = 30,\n@@ -223,7 +222,7 @@ def postprocess_qa_predictions(\n     # If we have an output_dir, let's save all those dicts.\n     if output_dir is not None:\n         if not os.path.isdir(output_dir):\n-            raise EnvironmentError(f\"{output_dir} is not a directory.\")\n+            raise OSError(f\"{output_dir} is not a directory.\")\n \n         prediction_file = os.path.join(\n             output_dir, \"predictions.json\" if prefix is None else f\"{prefix}_predictions.json\"\n@@ -253,7 +252,7 @@ def postprocess_qa_predictions(\n def postprocess_qa_predictions_with_beam_search(\n     examples,\n     features,\n-    predictions: Tuple[np.ndarray, np.ndarray],\n+    predictions: tuple[np.ndarray, np.ndarray],\n     version_2_with_negative: bool = False,\n     n_best_size: int = 20,\n     max_answer_length: int = 30,\n@@ -417,7 +416,7 @@ def postprocess_qa_predictions_with_beam_search(\n     # If we have an output_dir, let's save all those dicts.\n     if output_dir is not None:\n         if not os.path.isdir(output_dir):\n-            raise EnvironmentError(f\"{output_dir} is not a directory.\")\n+            raise OSError(f\"{output_dir} is not a directory.\")\n \n         prediction_file = os.path.join(\n             output_dir, \"predictions.json\" if prefix is None else f\"{prefix}_predictions.json\""
        },
        {
            "sha": "bfedf7c4ca946de1ddd38a0054f91a97cafa620c",
            "filename": "examples/pytorch/semantic-segmentation/run_semantic_segmentation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -258,7 +257,7 @@ def main():\n     else:\n         repo_id = data_args.dataset_name\n         filename = \"id2label.json\"\n-    id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type=\"dataset\"), \"r\"))\n+    id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type=\"dataset\")))\n     id2label = {int(k): v for k, v in id2label.items()}\n     label2id = {v: str(k) for k, v in id2label.items()}\n "
        },
        {
            "sha": "2cbc4130189b56372736a8f89badcfb8b387b967",
            "filename": "examples/pytorch/semantic-segmentation/run_semantic_segmentation_no_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation_no_trainer.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -316,7 +315,7 @@ def main():\n     else:\n         repo_id = args.dataset_name\n         filename = \"id2label.json\"\n-    id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type=\"dataset\"), \"r\"))\n+    id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type=\"dataset\")))\n     id2label = {int(k): v for k, v in id2label.items()}\n     label2id = {v: k for k, v in id2label.items()}\n "
        },
        {
            "sha": "b4ce3f71eb54773ed1fc1ef173912c0842bf0a2c",
            "filename": "examples/pytorch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fspeech-pretraining%2Frun_wav2vec2_pretraining_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fspeech-pretraining%2Frun_wav2vec2_pretraining_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-pretraining%2Frun_wav2vec2_pretraining_no_trainer.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -20,7 +19,7 @@\n import os\n from dataclasses import dataclass\n from pathlib import Path\n-from typing import Dict, List, Optional, Union\n+from typing import Optional, Union\n \n import datasets\n import torch\n@@ -328,7 +327,7 @@ class DataCollatorForWav2Vec2Pretraining:\n     mask_time_prob: Optional[float] = 0.65\n     mask_time_length: Optional[int] = 10\n \n-    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n+    def __call__(self, features: list[dict[str, Union[list[int], torch.Tensor]]]) -> dict[str, torch.Tensor]:\n         # reformat list to dict and set to pytorch format\n         batch = self.feature_extractor.pad(\n             features,\n@@ -716,7 +715,7 @@ def prepare_dataset(batch):\n                 }\n                 log_str = \"\"\n                 for k, v in train_logs.items():\n-                    log_str += \"| {}: {:.3e}\".format(k, v.item())\n+                    log_str += f\"| {k}: {v.item():.3e}\"\n \n                 if accelerator.is_local_main_process:\n                     progress_bar.write(log_str)\n@@ -773,7 +772,7 @@ def prepare_dataset(batch):\n \n         log_str = \"\"\n         for k, v in val_logs.items():\n-            log_str += \"| {}: {:.3e}\".format(k, v.item())\n+            log_str += f\"| {k}: {v.item():.3e}\"\n \n         if accelerator.is_local_main_process:\n             progress_bar.write(log_str)"
        },
        {
            "sha": "53a1f98c890a0620bdb9331191b65e5d95d88f25",
            "filename": "examples/pytorch/speech-recognition/run_speech_recognition_ctc.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -24,7 +23,7 @@\n import sys\n import warnings\n from dataclasses import dataclass, field\n-from typing import Dict, List, Optional, Union\n+from typing import Optional, Union\n \n import datasets\n import evaluate\n@@ -211,11 +210,11 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    chars_to_ignore: Optional[List[str]] = list_field(\n+    chars_to_ignore: Optional[list[str]] = list_field(\n         default=None,\n         metadata={\"help\": \"A list of characters to remove from the transcripts.\"},\n     )\n-    eval_metrics: List[str] = list_field(\n+    eval_metrics: list[str] = list_field(\n         default=[\"wer\"],\n         metadata={\"help\": \"A list of metrics the model should be evaluated on. E.g. `'wer cer'`\"},\n     )\n@@ -318,7 +317,7 @@ class DataCollatorCTCWithPadding:\n     pad_to_multiple_of_labels: Optional[int] = None\n     feature_extractor_input_name: Optional[str] = \"input_values\"\n \n-    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n+    def __call__(self, features: list[dict[str, Union[list[int], torch.Tensor]]]) -> dict[str, torch.Tensor]:\n         # split inputs and labels since they have to be of different lengths and need\n         # different padding methods\n         input_features = ["
        },
        {
            "sha": "511e7bc3d442663172e2e4c44b9ac426999d2747",
            "filename": "examples/pytorch/speech-recognition/run_speech_recognition_ctc_adapter.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc_adapter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc_adapter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc_adapter.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -24,7 +23,7 @@\n import sys\n import warnings\n from dataclasses import dataclass, field\n-from typing import Dict, List, Optional, Union\n+from typing import Optional, Union\n \n import datasets\n import evaluate\n@@ -201,11 +200,11 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    chars_to_ignore: Optional[List[str]] = list_field(\n+    chars_to_ignore: Optional[list[str]] = list_field(\n         default=None,\n         metadata={\"help\": \"A list of characters to remove from the transcripts.\"},\n     )\n-    eval_metrics: List[str] = list_field(\n+    eval_metrics: list[str] = list_field(\n         default=[\"wer\"],\n         metadata={\"help\": \"A list of metrics the model should be evaluated on. E.g. `'wer cer'`\"},\n     )\n@@ -300,7 +299,7 @@ class DataCollatorCTCWithPadding:\n     pad_to_multiple_of: Optional[int] = None\n     pad_to_multiple_of_labels: Optional[int] = None\n \n-    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n+    def __call__(self, features: list[dict[str, Union[list[int], torch.Tensor]]]) -> dict[str, torch.Tensor]:\n         # split inputs and labels since they have to be of different lengths and need\n         # different padding methods\n         input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]"
        },
        {
            "sha": "1b64ea078d61acadb01246c1d49a07e7598d60de",
            "filename": "examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_seq2seq.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -23,7 +22,7 @@\n import os\n import sys\n from dataclasses import dataclass, field\n-from typing import Any, Dict, List, Optional, Union\n+from typing import Any, Optional, Union\n \n import datasets\n import evaluate\n@@ -110,11 +109,11 @@ class ModelArguments:\n     freeze_encoder: bool = field(\n         default=False, metadata={\"help\": \"Whether to freeze the entire encoder of the seq2seq model.\"}\n     )\n-    forced_decoder_ids: List[List[int]] = field(\n+    forced_decoder_ids: list[list[int]] = field(\n         default=None,\n         metadata={\"help\": \"Deprecated. Please use the `language` and `task` arguments instead.\"},\n     )\n-    suppress_tokens: List[int] = field(\n+    suppress_tokens: list[int] = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -247,7 +246,7 @@ class DataCollatorSpeechSeq2SeqWithPadding:\n     decoder_start_token_id: int\n     forward_attention_mask: bool\n \n-    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n+    def __call__(self, features: list[dict[str, Union[list[int], torch.Tensor]]]) -> dict[str, torch.Tensor]:\n         # split inputs and labels since they have to be of different lengths and need\n         # different padding methods\n         model_input_name = self.processor.model_input_names[0]"
        },
        {
            "sha": "ce63c1c7f0699988d61f6eb87ed7e08a9a39e7e6",
            "filename": "examples/pytorch/summarization/run_summarization.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fsummarization%2Frun_summarization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fsummarization%2Frun_summarization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fsummarization%2Frun_summarization.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "4351ecffd10f629db4942a2c5dbe64aa967cac7b",
            "filename": "examples/pytorch/summarization/run_summarization_no_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fsummarization%2Frun_summarization_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Fsummarization%2Frun_summarization_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fsummarization%2Frun_summarization_no_trainer.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright The HuggingFace Team and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "923803a2da5f4e3e2ba93d35b39c9a9fb7b88637",
            "filename": "examples/pytorch/test_accelerate_examples.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Ftest_accelerate_examples.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Ftest_accelerate_examples.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftest_accelerate_examples.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 HuggingFace Inc..\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -51,7 +50,7 @@ def get_results(output_dir):\n     results = {}\n     path = os.path.join(output_dir, \"all_results.json\")\n     if os.path.exists(path):\n-        with open(path, \"r\") as f:\n+        with open(path) as f:\n             results = json.load(f)\n     else:\n         raise ValueError(f\"can't find {path}\")"
        },
        {
            "sha": "a986b426e1b4e7c11772bd94f2b555ffdcd12cc9",
            "filename": "examples/pytorch/test_pytorch_examples.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Ftest_pytorch_examples.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Ftest_pytorch_examples.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftest_pytorch_examples.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 HuggingFace Inc..\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -87,7 +86,7 @@ def get_results(output_dir):\n     results = {}\n     path = os.path.join(output_dir, \"all_results.json\")\n     if os.path.exists(path):\n-        with open(path, \"r\") as f:\n+        with open(path) as f:\n             results = json.load(f)\n     else:\n         raise ValueError(f\"can't find {path}\")"
        },
        {
            "sha": "62d71b7f28b53732be3d68e5357fcd91565880b0",
            "filename": "examples/pytorch/text-classification/run_classification.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Ftext-classification%2Frun_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Ftext-classification%2Frun_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2Frun_classification.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -21,7 +20,7 @@\n import random\n import sys\n from dataclasses import dataclass, field\n-from typing import List, Optional\n+from typing import Optional\n \n import datasets\n import evaluate\n@@ -256,7 +255,7 @@ class ModelArguments:\n     )\n \n \n-def get_label_list(raw_dataset, split=\"train\") -> List[str]:\n+def get_label_list(raw_dataset, split=\"train\") -> list[str]:\n     \"\"\"Get the list of labels from a multi-label dataset\"\"\"\n \n     if isinstance(raw_dataset[split][\"label\"][0], list):\n@@ -537,7 +536,7 @@ def main():\n         model.config.id2label = {id: label for label, id in label_to_id.items()}\n     elif not is_regression:  # classification, but not training\n         logger.info(\"using label infos in the model config\")\n-        logger.info(\"label2id: {}\".format(model.config.label2id))\n+        logger.info(f\"label2id: {model.config.label2id}\")\n         label_to_id = model.config.label2id\n     else:  # regression\n         label_to_id = None\n@@ -549,7 +548,7 @@ def main():\n         )\n     max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n \n-    def multi_labels_to_ids(labels: List[str]) -> List[float]:\n+    def multi_labels_to_ids(labels: list[str]) -> list[float]:\n         ids = [0.0] * len(label_to_id)  # BCELoss requires float as target type\n         for label in labels:\n             ids[label_to_id[label]] = 1.0\n@@ -735,7 +734,7 @@ def compute_metrics(p: EvalPrediction):\n                     else:\n                         item = label_list[item]\n                         writer.write(f\"{index}\\t{item}\\n\")\n-        logger.info(\"Predict results saved at {}\".format(output_predict_file))\n+        logger.info(f\"Predict results saved at {output_predict_file}\")\n     kwargs = {\"finetuned_from\": model_args.model_name_or_path, \"tasks\": \"text-classification\"}\n \n     if training_args.push_to_hub:"
        },
        {
            "sha": "c2930380859eb422786f3a8a5ccfe39861d68d9b",
            "filename": "examples/pytorch/text-classification/run_glue.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Ftext-classification%2Frun_glue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Ftext-classification%2Frun_glue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2Frun_glue.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "8f05b0e90bcdb437295eb46a9a248bd907baa57b",
            "filename": "examples/pytorch/text-classification/run_glue_no_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Ftext-classification%2Frun_glue_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Ftext-classification%2Frun_glue_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2Frun_glue_no_trainer.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "86ecb0f63ad7c7f02b467fdeaa0830f943edd983",
            "filename": "examples/pytorch/text-classification/run_xnli.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Ftext-classification%2Frun_xnli.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Ftext-classification%2Frun_xnli.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2Frun_xnli.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n #"
        },
        {
            "sha": "42cd9528e1ddd17e8aec5a2077b9dfe413475659",
            "filename": "examples/pytorch/text-generation/run_generation.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Ftext-generation%2Frun_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Ftext-generation%2Frun_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-generation%2Frun_generation.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n #\n@@ -19,7 +18,6 @@\n import argparse\n import inspect\n import logging\n-from typing import Tuple\n \n import torch\n from accelerate import PartialState\n@@ -271,8 +269,8 @@ def prepare_inputs_for_generation(\n         )\n \n     def _reorder_cache(\n-        self, past_key_values: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor\n-    ) -> Tuple[Tuple[torch.Tensor]]:\n+        self, past_key_values: tuple[tuple[torch.Tensor]], beam_idx: torch.Tensor\n+    ) -> tuple[tuple[torch.Tensor]]:\n         \"\"\"\n         This function is used to re-order the `past_key_values` cache if [`~PretrainedModel.beam_search`] or\n         [`~PretrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct"
        },
        {
            "sha": "5610dfb7f5deeb5c202fa77fe245434cd27ccb6d",
            "filename": "examples/pytorch/text-generation/run_generation_contrastive_search.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Ftext-generation%2Frun_generation_contrastive_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Ftext-generation%2Frun_generation_contrastive_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-generation%2Frun_generation_contrastive_search.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2022 University of Cambridge, Tencent AI Lab, DeepMind and The University of Hong Kong Authors and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "bbe85cff2e8f0aef794b65046ba92147ebc1dc76",
            "filename": "examples/pytorch/token-classification/run_ner.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2020 The HuggingFace Team All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "f9dee4a492074d56dfef9bd8d5d0221a3a34855b",
            "filename": "examples/pytorch/token-classification/run_ner_no_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Ftoken-classification%2Frun_ner_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Ftoken-classification%2Frun_ner_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftoken-classification%2Frun_ner_no_trainer.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "1310668646955173a32b639490886f63d38c4849",
            "filename": "examples/pytorch/translation/run_translation.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Ftranslation%2Frun_translation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Ftranslation%2Frun_translation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftranslation%2Frun_translation.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright The HuggingFace Team and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "ef2e928cf6595b4f854cee34297ef85f7ed8d7a6",
            "filename": "examples/pytorch/translation/run_translation_no_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Ftranslation%2Frun_translation_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fpytorch%2Ftranslation%2Frun_translation_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftranslation%2Frun_translation_no_trainer.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright The HuggingFace Team and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "006b092e671a15f9c609a1bc1d8bbc7487792cfe",
            "filename": "examples/quantization/custom_quantization.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fquantization%2Fcustom_quantization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fquantization%2Fcustom_quantization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fquantization%2Fcustom_quantization.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,5 @@\n import json\n-from typing import Any, Dict\n+from typing import Any\n \n import torch\n \n@@ -14,7 +14,7 @@ def __init__(self):\n         self.quant_method = \"custom\"\n         self.bits = 8\n \n-    def to_dict(self) -> Dict[str, Any]:\n+    def to_dict(self) -> dict[str, Any]:\n         output = {\n             \"num_bits\": self.bits,\n         }\n@@ -24,7 +24,7 @@ def __repr__(self):\n         config_dict = self.to_dict()\n         return f\"{self.__class__.__name__} {json.dumps(config_dict, indent=2, sort_keys=True)}\\n\"\n \n-    def to_diff_dict(self) -> Dict[str, Any]:\n+    def to_diff_dict(self) -> dict[str, Any]:\n         config_dict = self.to_dict()\n \n         default_config_dict = CustomConfig().to_dict()"
        },
        {
            "sha": "a61c041b447dcb6a73b388f7496c4928cff04105",
            "filename": "examples/quantization/custom_quantization_int8_example.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fquantization%2Fcustom_quantization_int8_example.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Fquantization%2Fcustom_quantization_int8_example.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fquantization%2Fcustom_quantization_int8_example.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,5 @@\n import json\n-from typing import Any, Dict, List, Optional\n+from typing import Any, Optional\n \n import torch\n import torch.nn as nn\n@@ -112,15 +112,15 @@ class Int8SymmetricConfig(QuantizationConfigMixin):\n     Configuration for INT8 symmetric quantization.\n     \"\"\"\n \n-    def __init__(self, modules_to_not_convert: Optional[List[str]] = None, **kwargs):\n+    def __init__(self, modules_to_not_convert: Optional[list[str]] = None, **kwargs):\n         self.quant_method = \"int8_symmetric\"\n         self.modules_to_not_convert = modules_to_not_convert\n \n     def __repr__(self):\n         config_dict = self.to_dict()\n         return f\"{self.__class__.__name__} {json.dumps(config_dict, indent=2, sort_keys=True)}\\n\"\n \n-    def to_diff_dict(self) -> Dict[str, Any]:\n+    def to_diff_dict(self) -> dict[str, Any]:\n         config_dict = self.to_dict()\n         default_config_dict = Int8SymmetricConfig().to_dict()\n \n@@ -164,7 +164,7 @@ def check_quantized_param(\n         model,\n         param_value: \"torch.Tensor\",\n         param_name: str,\n-        state_dict: Dict[str, Any],\n+        state_dict: dict[str, Any],\n         **kwargs,\n     ):\n         module, tensor_name = get_module_from_name(model, param_name)\n@@ -186,8 +186,8 @@ def create_quantized_param(\n         param_value: \"torch.Tensor\",\n         param_name: str,\n         target_device: \"torch.device\",\n-        state_dict: Dict[str, Any],\n-        unexpected_keys: Optional[List[str]] = None,\n+        state_dict: dict[str, Any],\n+        unexpected_keys: Optional[list[str]] = None,\n     ):\n         \"\"\"\n         Quantizes weights to INT8 symmetric format.\n@@ -202,7 +202,7 @@ def create_quantized_param(\n         module._buffers[tensor_name] = weight_quantized.to(target_device)\n         module._buffers[\"weight_scale\"] = weight_scale.to(target_device)\n \n-    def update_missing_keys(self, model, missing_keys: List[str], prefix: str) -> List[str]:\n+    def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n         not_missing_keys = []\n         for name, module in model.named_modules():\n             if isinstance(module, Int8SymmetricLinear):"
        },
        {
            "sha": "2baf1d1a032a1598e1cbc30c0d14577007f75070",
            "filename": "examples/run_on_remote.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Frun_on_remote.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Frun_on_remote.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Frun_on_remote.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -56,7 +55,7 @@\n     cluster.run([\"pip install torch --upgrade --extra-index-url https://download.pytorch.org/whl/cu117\"])\n \n     # Run example. You can bypass the CLI wrapper and paste your own code here.\n-    cluster.run([f\"python transformers/examples/{args.example} {' '.join(shlex.quote(arg) for arg in unknown)}\"])\n+    cluster.run([f\"python transformers/examples/{args.example} {shlex.join(unknown)}\"])\n \n     # Alternatively, we can just import and run a training function (especially if there's no wrapper CLI):\n     # from my_script... import train"
        },
        {
            "sha": "1b084a603ddb313fcca0573ee062549704696956",
            "filename": "examples/tensorflow/contrastive-image-text/run_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Fcontrastive-image-text%2Frun_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Fcontrastive-image-text%2Frun_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fcontrastive-image-text%2Frun_clip.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2023 The HuggingFace Team All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "c9f9c9750b938d6d9b440451ba750467f0bd3856",
            "filename": "examples/tensorflow/image-classification/run_image_classification.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Fimage-classification%2Frun_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Fimage-classification%2Frun_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fimage-classification%2Frun_image_classification.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "aa90a9db5271a4fc32481477cf58907f3981c6f1",
            "filename": "examples/tensorflow/language-modeling-tpu/prepare_tfrecord_shards.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Flanguage-modeling-tpu%2Fprepare_tfrecord_shards.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Flanguage-modeling-tpu%2Fprepare_tfrecord_shards.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Flanguage-modeling-tpu%2Fprepare_tfrecord_shards.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -178,7 +177,7 @@ def group_texts(examples):\n             for i in range(len(serialized_examples)):\n                 example = serialized_examples[i]\n                 out_file.write(example)\n-            print(\"Wrote file {} containing {} records\".format(filename, records_containing))\n+            print(f\"Wrote file {filename} containing {records_containing} records\")\n \n         shard_count += 1\n         total_records += records_containing"
        },
        {
            "sha": "7b4155f26ed1c397e63aba0952c14bf2225ee5e4",
            "filename": "examples/tensorflow/language-modeling-tpu/run_mlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Flanguage-modeling-tpu%2Frun_mlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Flanguage-modeling-tpu%2Frun_mlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Flanguage-modeling-tpu%2Frun_mlm.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "9eb9c8427b0f6d4d0e68171dc31ee112073a306a",
            "filename": "examples/tensorflow/language-modeling-tpu/train_unigram.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Flanguage-modeling-tpu%2Ftrain_unigram.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Flanguage-modeling-tpu%2Ftrain_unigram.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Flanguage-modeling-tpu%2Ftrain_unigram.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "d43530669b97677f8106ddf663cc3f115150ff6a",
            "filename": "examples/tensorflow/language-modeling/run_clm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Flanguage-modeling%2Frun_clm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Flanguage-modeling%2Frun_clm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Flanguage-modeling%2Frun_clm.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "edae71252d5ea8ad07dcbee45e52f18e2ae57c63",
            "filename": "examples/tensorflow/language-modeling/run_mlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Flanguage-modeling%2Frun_mlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Flanguage-modeling%2Frun_mlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Flanguage-modeling%2Frun_mlm.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "9b6ba4228c0ceaae1a48d31185749a8adc8620d1",
            "filename": "examples/tensorflow/multiple-choice/run_swag.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Fmultiple-choice%2Frun_swag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Fmultiple-choice%2Frun_swag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fmultiple-choice%2Frun_swag.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright The HuggingFace Team and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "28418496c2348f4e5bede741e4ebd6984fd97435",
            "filename": "examples/tensorflow/question-answering/run_qa.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Fquestion-answering%2Frun_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Fquestion-answering%2Frun_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fquestion-answering%2Frun_qa.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2020 The HuggingFace Team All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "f0cc5c26a6927179d83dab56d21f4acb357884ee",
            "filename": "examples/tensorflow/question-answering/utils_qa.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Fquestion-answering%2Futils_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Fquestion-answering%2Futils_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fquestion-answering%2Futils_qa.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2020 The HuggingFace Team All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -20,7 +19,7 @@\n import json\n import logging\n import os\n-from typing import Optional, Tuple\n+from typing import Optional\n \n import numpy as np\n from tqdm.auto import tqdm\n@@ -32,7 +31,7 @@\n def postprocess_qa_predictions(\n     examples,\n     features,\n-    predictions: Tuple[np.ndarray, np.ndarray],\n+    predictions: tuple[np.ndarray, np.ndarray],\n     version_2_with_negative: bool = False,\n     n_best_size: int = 20,\n     max_answer_length: int = 30,\n@@ -223,7 +222,7 @@ def postprocess_qa_predictions(\n     # If we have an output_dir, let's save all those dicts.\n     if output_dir is not None:\n         if not os.path.isdir(output_dir):\n-            raise EnvironmentError(f\"{output_dir} is not a directory.\")\n+            raise OSError(f\"{output_dir} is not a directory.\")\n \n         prediction_file = os.path.join(\n             output_dir, \"predictions.json\" if prefix is None else f\"{prefix}_predictions.json\"\n@@ -253,7 +252,7 @@ def postprocess_qa_predictions(\n def postprocess_qa_predictions_with_beam_search(\n     examples,\n     features,\n-    predictions: Tuple[np.ndarray, np.ndarray],\n+    predictions: tuple[np.ndarray, np.ndarray],\n     version_2_with_negative: bool = False,\n     n_best_size: int = 20,\n     max_answer_length: int = 30,\n@@ -417,7 +416,7 @@ def postprocess_qa_predictions_with_beam_search(\n     # If we have an output_dir, let's save all those dicts.\n     if output_dir is not None:\n         if not os.path.isdir(output_dir):\n-            raise EnvironmentError(f\"{output_dir} is not a directory.\")\n+            raise OSError(f\"{output_dir} is not a directory.\")\n \n         prediction_file = os.path.join(\n             output_dir, \"predictions.json\" if prefix is None else f\"{prefix}_predictions.json\""
        },
        {
            "sha": "2a2ef3fb767716df64dfc34c798472e8fa147d7a",
            "filename": "examples/tensorflow/summarization/run_summarization.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Fsummarization%2Frun_summarization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Fsummarization%2Frun_summarization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fsummarization%2Frun_summarization.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "46ed20c021d2716e092f7b925536fb8bd2f13da1",
            "filename": "examples/tensorflow/test_tensorflow_examples.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Ftest_tensorflow_examples.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Ftest_tensorflow_examples.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Ftest_tensorflow_examples.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 HuggingFace Inc.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -86,7 +85,7 @@ def get_results(output_dir):\n     results = {}\n     path = os.path.join(output_dir, \"all_results.json\")\n     if os.path.exists(path):\n-        with open(path, \"r\") as f:\n+        with open(path) as f:\n             results = json.load(f)\n     else:\n         raise ValueError(f\"can't find {path}\")"
        },
        {
            "sha": "2e9096b3642b97276b2e815fcb47c9aee02a5ade",
            "filename": "examples/tensorflow/text-classification/run_glue.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Ftext-classification%2Frun_glue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Ftext-classification%2Frun_glue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Ftext-classification%2Frun_glue.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "45b4a3e607e4e2ac8932ee23ddc3abfc4403ebef",
            "filename": "examples/tensorflow/text-classification/run_text_classification.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Ftext-classification%2Frun_text_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Ftext-classification%2Frun_text_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Ftext-classification%2Frun_text_classification.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "8a50b2a650395020c28ecd84a7fc869d7383555a",
            "filename": "examples/tensorflow/token-classification/run_ner.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Ftoken-classification%2Frun_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Ftoken-classification%2Frun_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Ftoken-classification%2Frun_ner.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "5d9771d425ec84af82f212c00793471539abbe34",
            "filename": "examples/tensorflow/translation/run_translation.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Ftranslation%2Frun_translation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftensorflow%2Ftranslation%2Frun_translation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Ftranslation%2Frun_translation.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "af19106a82764cae80e84134366523bb9a3d7b12",
            "filename": "examples/training/distributed_training.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftraining%2Fdistributed_training.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fb8d49e8869fe95c7580ae0b12cca41e009fe3f/examples%2Ftraining%2Fdistributed_training.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftraining%2Fdistributed_training.py?ref=0fb8d49e8869fe95c7580ae0b12cca41e009fe3f",
            "patch": "@@ -19,16 +19,16 @@ def run(backend):\n     tensor = torch.zeros(1)\n     # Need to put tensor on a GPU device for nccl backend\n     if backend == \"nccl\":\n-        device = torch.device(\"cuda:{}\".format(LOCAL_RANK))\n+        device = torch.device(f\"cuda:{LOCAL_RANK}\")\n         tensor = tensor.to(device)\n \n     if WORLD_RANK == 0:\n         for rank_recv in range(1, WORLD_SIZE):\n             dist.send(tensor=tensor, dst=rank_recv)\n-            print(\"worker_{} sent data to Rank {}\\n\".format(0, rank_recv))\n+            print(f\"worker_{0} sent data to Rank {rank_recv}\\n\")\n     else:\n         dist.recv(tensor=tensor, src=0)\n-        print(\"worker_{} has received data from rank {}\\n\".format(WORLD_RANK, 0))\n+        print(f\"worker_{WORLD_RANK} has received data from rank {0}\\n\")\n \n \n def init_processes(backend):"
        }
    ],
    "stats": {
        "total": 809,
        "additions": 358,
        "deletions": 451
    }
}