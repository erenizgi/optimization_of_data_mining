{
    "author": "zRzRzRzRzRzRzR",
    "message": "Add aux loss for GLM-4.5V (#41564)\n\n* add aux\n\n* update\n\n* update config to text_config\n\n* use qwen data class to avoid repeat again\n\n* format\n\n* update\n\n* use 1e-4\n\n* update\n\n* update for remove init\n\n* Apply style fixes\n\n---------\n\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>\nCo-authored-by: Raushan Turganbay <raushan@huggingface.co>",
    "sha": "59efd86da2fd2ce9d2af21b68956abf397009ec2",
    "files": [
        {
            "sha": "378caebb496e2dfb5f27df4a1fd25406a51f0caa",
            "filename": "src/transformers/models/glm4v_moe/configuration_glm4v_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/59efd86da2fd2ce9d2af21b68956abf397009ec2/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/59efd86da2fd2ce9d2af21b68956abf397009ec2/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py?ref=59efd86da2fd2ce9d2af21b68956abf397009ec2",
            "patch": "@@ -206,7 +206,8 @@ class Glm4vMoeTextConfig(PreTrainedConfig):\n                                                                     \\--k dense layers--/\n         norm_topk_prob (`bool`, *optional*, defaults to `True`):\n             Whether to normalize the topk probabilities.\n-\n+        router_aux_loss_coef (`float`, *optional*, defaults to 0.0001):\n+            The aux loss factor for the loss.\n     ```python\n     >>> from transformers import Glm4vMoeTextModel, Glm4vMoeConfig\n \n@@ -269,6 +270,7 @@ def __init__(\n         topk_group=1,\n         first_k_dense_replace=1,\n         norm_topk_prob=True,\n+        router_aux_loss_coef=0.0001,\n         **kwargs,\n     ):\n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n@@ -305,6 +307,7 @@ def __init__(\n         self.routed_scaling_factor = routed_scaling_factor\n         self.first_k_dense_replace = first_k_dense_replace\n         self.norm_topk_prob = norm_topk_prob\n+        self.router_aux_loss_coef = router_aux_loss_coef\n \n \n class Glm4vMoeConfig(PreTrainedConfig):"
        },
        {
            "sha": "372a9a88d0bd4fd02b1cd3c575ef01dd8f751fe6",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 294,
            "deletions": 198,
            "changes": 492,
            "blob_url": "https://github.com/huggingface/transformers/blob/59efd86da2fd2ce9d2af21b68956abf397009ec2/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/59efd86da2fd2ce9d2af21b68956abf397009ec2/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=59efd86da2fd2ce9d2af21b68956abf397009ec2",
            "patch": "@@ -35,12 +35,12 @@\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n+from ...modeling_outputs import ModelOutput, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_glm4v_moe import Glm4vMoeConfig, Glm4vMoeTextConfig, Glm4vMoeVisionConfig\n \n \n@@ -65,6 +65,68 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n+class Glm4vMoeTextRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: Glm4vMoeTextConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        # In contrast to other models, Glm4vMoeText has different position ids for the grids\n+        # So we expand the inv_freq to shape (3, ...)\n+        inv_freq_expanded = self.inv_freq[None, None, :, None].float().expand(3, position_ids.shape[1], -1, 1)\n+        position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for Llava outputs, with hidden states and attentions.\n+    \"\"\"\n+)\n+class Glm4vMoeModelOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+        The rope index difference between sequence length and multimodal rope.\n+    \"\"\"\n+\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[Cache] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    rope_deltas: Optional[torch.LongTensor] = None\n+\n+\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -448,6 +510,7 @@ class Glm4vMoePreTrainedModel(PreTrainedModel):\n     _can_record_outputs = {\n         \"hidden_states\": Glm4vMoeTextDecoderLayer,\n         \"attentions\": Glm4vMoeTextAttention,\n+        \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"mlp.gate\", index=0),\n     }\n \n     def _init_weights(self, module):\n@@ -456,6 +519,144 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for Glm4vMoe causal language model (or autoregressive) outputs.\n+    \"\"\"\n+)\n+class Glm4vMoeCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+        The rope index difference between sequence length and multimodal rope.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[Cache] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    rope_deltas: Optional[torch.LongTensor] = None\n+    aux_loss: Optional[torch.FloatTensor] = None\n+\n+\n+@auto_docstring\n+class Glm4vMoeTextModel(Glm4vMoePreTrainedModel):\n+    config: Glm4vMoeTextConfig\n+\n+    def __init__(self, config: Glm4vMoeTextConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [Glm4vMoeTextDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = Glm4vMoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = Glm4vMoeTextRotaryEmbedding(config=config)\n+\n+        self.gradient_checkpointing = False\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @auto_docstring\n+    @check_model_inputs()\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> MoeModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        # torch.jit.trace() doesn't support cache objects in the output\n+        if use_cache and past_key_values is None and not torch.jit.is_tracing():\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        # the hard coded `3` is for temporal, height and width.\n+        if position_ids is None:\n+            position_ids = cache_position.view(1, 1, -1).expand(3, inputs_embeds.shape[0], -1)\n+        elif position_ids.ndim == 2:\n+            position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)\n+\n+        # NOTE: we need to pass text position ids for packing. Qwen2-VL uses 3D positions\n+        # where each dim indicates visual spatial positions for temporal/height/width grids.\n+        # There are two scenarios when FA2-like packed masking might be activated.\n+        # 1. User specifically passed packed `position_ids` and no attention mask.\n+        #    In this case we expect the useer to create correct position ids for all 3 grids\n+        #    and prepend text-only position ids to it. The final tensor will be [4, bs, seq-len]\n+        # 2. User runs forward with no attention mask and no position ids. In this case, position ids\n+        #    are prepared by the model (`get_rope_index`) as `[4, bs, seq-len]` tensor. Text-only positions are\n+        #    prepended by us when creating positions so that the mask is constructed correctly. NOTE: failing to pass\n+        #    text-only positions will cause incorrect mask construction, do not change `prepare_input_for_generation`\n+        if position_ids.ndim == 3 and position_ids.shape[0] == 4:\n+            text_position_ids = position_ids[0]\n+            position_ids = position_ids[1:]\n+        else:\n+            # If inputs are not packed (usual 3D positions), do not prepare mask from position_ids\n+            text_position_ids = None\n+\n+        mask_kwargs = {\n+            \"config\": self.config,\n+            \"input_embeds\": inputs_embeds,\n+            \"attention_mask\": attention_mask,\n+            \"cache_position\": cache_position,\n+            \"past_key_values\": past_key_values,\n+            \"position_ids\": text_position_ids,\n+        }\n+        # Create the masks\n+        causal_mask = create_causal_mask(**mask_kwargs)\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for i, decoder_layer in enumerate(self.layers[: self.config.num_hidden_layers]):\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                position_embeddings=position_embeddings,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+            hidden_states = layer_outputs\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        return MoeModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n class Glm4vMoeisionMlp(nn.Module):\n     def __init__(self, config, bias: bool = False):\n         super().__init__()\n@@ -732,68 +933,6 @@ def forward(\n         return hidden_states\n \n \n-class Glm4vMoeTextRotaryEmbedding(nn.Module):\n-    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n-\n-    def __init__(self, config: Glm4vMoeTextConfig, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        # In contrast to other models, Glm4vMoeText has different position ids for the grids\n-        # So we expand the inv_freq to shape (3, ...)\n-        inv_freq_expanded = self.inv_freq[None, None, :, None].float().expand(3, position_ids.shape[1], -1, 1)\n-        position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n-@dataclass\n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    Base class for Llava outputs, with hidden states and attentions.\n-    \"\"\"\n-)\n-class Glm4vMoeModelOutputWithPast(ModelOutput):\n-    r\"\"\"\n-    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n-\n-        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-        `past_key_values` input) to speed up sequential decoding.\n-    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-        The rope index difference between sequence length and multimodal rope.\n-    \"\"\"\n-\n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[torch.FloatTensor]] = None\n-    rope_deltas: Optional[torch.LongTensor] = None\n-\n-\n class Glm4vMoeVisionModel(Glm4vMoePreTrainedModel):\n     config: Glm4vMoeVisionConfig\n     _no_split_modules = [\"Glm4vMoeVisionBlock\"]\n@@ -904,114 +1043,6 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n         return hidden_states\n \n \n-@auto_docstring\n-class Glm4vMoeTextModel(Glm4vMoePreTrainedModel):\n-    config: Glm4vMoeTextConfig\n-\n-    def __init__(self, config: Glm4vMoeTextConfig):\n-        super().__init__(config)\n-        self.padding_idx = config.pad_token_id\n-        self.vocab_size = config.vocab_size\n-\n-        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n-        self.layers = nn.ModuleList(\n-            [Glm4vMoeTextDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n-        )\n-        self.norm = Glm4vMoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.rotary_emb = Glm4vMoeTextRotaryEmbedding(config=config)\n-\n-        self.gradient_checkpointing = False\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @auto_docstring\n-    @check_model_inputs()\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[tuple, BaseModelOutputWithPast]:\n-        if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n-\n-        # torch.jit.trace() doesn't support cache objects in the output\n-        if use_cache and past_key_values is None and not torch.jit.is_tracing():\n-            past_key_values = DynamicCache(config=self.config)\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input_ids)\n-\n-        if cache_position is None:\n-            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n-            )\n-\n-        # the hard coded `3` is for temporal, height and width.\n-        if position_ids is None:\n-            position_ids = cache_position.view(1, 1, -1).expand(3, inputs_embeds.shape[0], -1)\n-        elif position_ids.ndim == 2:\n-            position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)\n-\n-        # NOTE: we need to pass text position ids for packing. Qwen2-VL uses 3D positions\n-        # where each dim indicates visual spatial positions for temporal/height/width grids.\n-        # There are two scenarios when FA2-like packed masking might be activated.\n-        # 1. User specifically passed packed `position_ids` and no attention mask.\n-        #    In this case we expect the useer to create correct position ids for all 3 grids\n-        #    and prepend text-only position ids to it. The final tensor will be [4, bs, seq-len]\n-        # 2. User runs forward with no attention mask and no position ids. In this case, position ids\n-        #    are prepared by the model (`get_rope_index`) as `[4, bs, seq-len]` tensor. Text-only positions are\n-        #    prepended by us when creating positions so that the mask is constructed correctly. NOTE: failing to pass\n-        #    text-only positions will cause incorrect mask construction, do not change `prepare_input_for_generation`\n-        if position_ids.ndim == 3 and position_ids.shape[0] == 4:\n-            text_position_ids = position_ids[0]\n-            position_ids = position_ids[1:]\n-        else:\n-            # If inputs are not packed (usual 3D positions), do not prepare mask from position_ids\n-            text_position_ids = None\n-\n-        mask_kwargs = {\n-            \"config\": self.config,\n-            \"input_embeds\": inputs_embeds,\n-            \"attention_mask\": attention_mask,\n-            \"cache_position\": cache_position,\n-            \"past_key_values\": past_key_values,\n-            \"position_ids\": text_position_ids,\n-        }\n-        # Create the masks\n-        causal_mask = create_causal_mask(**mask_kwargs)\n-\n-        hidden_states = inputs_embeds\n-\n-        # create position embeddings to be shared across the decoder layers\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n-\n-        for decoder_layer in self.layers:\n-            layer_outputs = decoder_layer(\n-                hidden_states,\n-                position_embeddings=position_embeddings,\n-                attention_mask=causal_mask,\n-                position_ids=position_ids,\n-                past_key_values=past_key_values,\n-                cache_position=cache_position,\n-                **kwargs,\n-            )\n-            hidden_states = layer_outputs\n-\n-        hidden_states = self.norm(hidden_states)\n-\n-        return BaseModelOutputWithPast(\n-            last_hidden_state=hidden_states,\n-            past_key_values=past_key_values,\n-        )\n-\n-\n @auto_docstring\n class Glm4vMoeModel(Glm4vMoePreTrainedModel):\n     base_model_prefix = \"\"\n@@ -1421,33 +1452,86 @@ def forward(\n         )\n \n \n-@dataclass\n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    Base class for Glm4vMoe causal language model (or autoregressive) outputs.\n-    \"\"\"\n-)\n-class Glm4vMoeCausalLMOutputWithPast(ModelOutput):\n+def load_balancing_loss_func(\n+    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n+    num_experts: Optional[int] = None,\n+    top_k=2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> Union[torch.Tensor, int]:\n     r\"\"\"\n-    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-        Language modeling loss (for next-token prediction).\n-    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n+    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n \n-        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-        `past_key_values` input) to speed up sequential decoding.\n-    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-        The rope index difference between sequence length and multimodal rope.\n+    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n+    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n+    experts is too unbalanced.\n+\n+    Args:\n+        gate_logits:\n+            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n+            shape [batch_size X sequence_length, num_experts].\n+        num_experts:\n+            Number of experts\n+        top_k:\n+            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n+            parameter.\n+        attention_mask (`torch.Tensor`, *optional*):\n+            The attention_mask used in forward function\n+            shape [batch_size X sequence_length] if not None.\n+\n+    Returns:\n+        The auxiliary loss.\n     \"\"\"\n+    if gate_logits is None or not isinstance(gate_logits, tuple):\n+        return 0\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[torch.FloatTensor]] = None\n-    rope_deltas: Optional[torch.LongTensor] = None\n+    if isinstance(gate_logits, tuple):\n+        compute_device = gate_logits[0].device\n+        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n+\n+    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n+\n+    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n+\n+    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n+\n+    if attention_mask is None:\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n+    else:\n+        batch_size, sequence_length = attention_mask.shape\n+        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n+        expert_attention_mask = (\n+            attention_mask[None, :, :, None, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n+            .reshape(-1, top_k, num_experts)\n+            .to(compute_device)\n+        )\n+\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n+            expert_attention_mask, dim=0\n+        )\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n+        router_per_expert_attention_mask = (\n+            attention_mask[None, :, :, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n+            .reshape(-1, num_experts)\n+            .to(compute_device)\n+        )\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n+            router_per_expert_attention_mask, dim=0\n+        )\n+\n+    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n+    return overall_loss * num_experts\n \n \n class Glm4vMoeForConditionalGeneration(Glm4vMoePreTrainedModel, GenerationMixin):\n@@ -1492,8 +1576,8 @@ def language_model(self):\n     def visual(self):\n         return self.model.visual\n \n-    @can_return_tuple\n     @auto_docstring\n+    @check_model_inputs()\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1506,7 +1590,6 @@ def forward(\n         pixel_values_videos: Optional[torch.FloatTensor] = None,\n         image_grid_thw: Optional[torch.LongTensor] = None,\n         video_grid_thw: Optional[torch.LongTensor] = None,\n-        rope_deltas: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1568,7 +1651,6 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -1577,8 +1659,22 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n \n+        aux_loss = None\n+        if kwargs.get(\"output_router_logits\", False):\n+            aux_loss = load_balancing_loss_func(\n+                outputs.router_logits,\n+                self.num_experts,\n+                self.num_experts_per_tok,\n+                attention_mask,\n+            )\n+            if labels is not None:\n+                loss += self.config.text_config.router_aux_loss_coef * aux_loss.to(\n+                    loss.device\n+                )  # make sure to reside in the same device\n+\n         return Glm4vMoeCausalLMOutputWithPast(\n             loss=loss,\n+            aux_loss=aux_loss,\n             logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,"
        },
        {
            "sha": "fbcc1de59611670e1a055f5f77a6b20383e9bf80",
            "filename": "src/transformers/models/glm4v_moe/modular_glm4v_moe.py",
            "status": "modified",
            "additions": 182,
            "deletions": 6,
            "changes": 188,
            "blob_url": "https://github.com/huggingface/transformers/blob/59efd86da2fd2ce9d2af21b68956abf397009ec2/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/59efd86da2fd2ce9d2af21b68956abf397009ec2/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py?ref=59efd86da2fd2ce9d2af21b68956abf397009ec2",
            "patch": "@@ -13,18 +13,21 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from collections.abc import Callable\n-from typing import Optional\n+from typing import Optional, Union\n \n import torch\n import torch.nn as nn\n \n-from ...cache_utils import Cache\n+from ...cache_utils import Cache, DynamicCache\n from ...configuration_utils import PreTrainedConfig\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import MoeModelOutputWithPast\n from ...modeling_rope_utils import rope_config_validation\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n-from ...utils import logging\n+from ...utils import TransformersKwargs, auto_docstring, logging\n+from ...utils.generic import OutputRecorder, check_model_inputs\n from ..deepseek_v3.modeling_deepseek_v3 import DeepseekV3NaiveMoe\n from ..glm4.modeling_glm4 import Glm4Attention\n from ..glm4_moe.configuration_glm4_moe import Glm4MoeConfig\n@@ -40,8 +43,15 @@\n from ..glm4v.configuration_glm4v import Glm4vConfig, Glm4vVisionConfig\n from ..glm4v.modeling_glm4v import (\n     Glm4vForConditionalGeneration,\n+    Glm4vTextModel,\n+    Glm4vTextRotaryEmbedding,\n     rotate_half,\n )\n+from ..qwen3_vl_moe.modeling_qwen3_vl_moe import (\n+    Qwen3VLMoeCausalLMOutputWithPast,\n+    Qwen3VLMoeModelOutputWithPast,\n+    load_balancing_loss_func,\n+)\n \n \n logger = logging.get_logger(__name__)\n@@ -51,6 +61,14 @@ class Glm4vMoeVisionConfig(Glm4vVisionConfig):\n     pass\n \n \n+class Glm4vMoeRMSNorm(Glm4MoeRMSNorm):\n+    pass\n+\n+\n+class Glm4vMoeTextRotaryEmbedding(Glm4vTextRotaryEmbedding):\n+    pass\n+\n+\n class Glm4vMoeTextConfig(Glm4MoeConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Glm4vMoeModel`]. It is used to instantiate a\n@@ -138,7 +156,8 @@ class Glm4vMoeTextConfig(Glm4MoeConfig):\n                                                                     \\--k dense layers--/\n         norm_topk_prob (`bool`, *optional*, defaults to `True`):\n             Whether to normalize the topk probabilities.\n-\n+        router_aux_loss_coef (`float`, *optional*, defaults to 0.0001):\n+            The aux loss factor for the loss.\n     ```python\n     >>> from transformers import Glm4vMoeTextModel, Glm4vMoeConfig\n \n@@ -198,6 +217,7 @@ def __init__(\n         topk_group=1,\n         first_k_dense_replace=1,\n         norm_topk_prob=True,\n+        router_aux_loss_coef=0.0001,\n         **kwargs,\n     ):\n         PreTrainedConfig.__init__(self, tie_word_embeddings=tie_word_embeddings, **kwargs)\n@@ -234,6 +254,7 @@ def __init__(\n         self.routed_scaling_factor = routed_scaling_factor\n         self.first_k_dense_replace = first_k_dense_replace\n         self.norm_topk_prob = norm_topk_prob\n+        self.router_aux_loss_coef = router_aux_loss_coef\n \n \n class Glm4vMoeConfig(Glm4vConfig):\n@@ -293,7 +314,7 @@ def __init__(\n         super().__init__()\n \n \n-class Glm4vMoeRMSNorm(Glm4MoeRMSNorm):\n+class Glm4vMoeModelOutputWithPast(Qwen3VLMoeModelOutputWithPast):\n     pass\n \n \n@@ -443,13 +464,168 @@ class Glm4vMoePreTrainedModel(Glm4MoePreTrainedModel):\n     _can_record_outputs = {\n         \"hidden_states\": Glm4vMoeTextDecoderLayer,\n         \"attentions\": Glm4vMoeTextAttention,\n+        \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"mlp.gate\", index=0),\n     }\n \n \n-class Glm4vMoeForConditionalGeneration(Glm4vForConditionalGeneration):\n+class Glm4vMoeCausalLMOutputWithPast(Qwen3VLMoeCausalLMOutputWithPast):\n     pass\n \n \n+@auto_docstring\n+class Glm4vMoeTextModel(Glm4vTextModel):\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> MoeModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        # torch.jit.trace() doesn't support cache objects in the output\n+        if use_cache and past_key_values is None and not torch.jit.is_tracing():\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        # the hard coded `3` is for temporal, height and width.\n+        if position_ids is None:\n+            position_ids = cache_position.view(1, 1, -1).expand(3, inputs_embeds.shape[0], -1)\n+        elif position_ids.ndim == 2:\n+            position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)\n+\n+        # NOTE: we need to pass text position ids for packing. Qwen2-VL uses 3D positions\n+        # where each dim indicates visual spatial positions for temporal/height/width grids.\n+        # There are two scenarios when FA2-like packed masking might be activated.\n+        # 1. User specifically passed packed `position_ids` and no attention mask.\n+        #    In this case we expect the useer to create correct position ids for all 3 grids\n+        #    and prepend text-only position ids to it. The final tensor will be [4, bs, seq-len]\n+        # 2. User runs forward with no attention mask and no position ids. In this case, position ids\n+        #    are prepared by the model (`get_rope_index`) as `[4, bs, seq-len]` tensor. Text-only positions are\n+        #    prepended by us when creating positions so that the mask is constructed correctly. NOTE: failing to pass\n+        #    text-only positions will cause incorrect mask construction, do not change `prepare_input_for_generation`\n+        if position_ids.ndim == 3 and position_ids.shape[0] == 4:\n+            text_position_ids = position_ids[0]\n+            position_ids = position_ids[1:]\n+        else:\n+            # If inputs are not packed (usual 3D positions), do not prepare mask from position_ids\n+            text_position_ids = None\n+\n+        mask_kwargs = {\n+            \"config\": self.config,\n+            \"input_embeds\": inputs_embeds,\n+            \"attention_mask\": attention_mask,\n+            \"cache_position\": cache_position,\n+            \"past_key_values\": past_key_values,\n+            \"position_ids\": text_position_ids,\n+        }\n+        # Create the masks\n+        causal_mask = create_causal_mask(**mask_kwargs)\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for i, decoder_layer in enumerate(self.layers[: self.config.num_hidden_layers]):\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                position_embeddings=position_embeddings,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+            hidden_states = layer_outputs\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        return MoeModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+class Glm4vMoeForConditionalGeneration(Glm4vForConditionalGeneration):\n+    @auto_docstring\n+    @check_model_inputs()\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        image_grid_thw: Optional[torch.LongTensor] = None,\n+        video_grid_thw: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, Glm4vMoeCausalLMOutputWithPast]:\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            pixel_values_videos=pixel_values_videos,\n+            image_grid_thw=image_grid_thw,\n+            video_grid_thw=video_grid_thw,\n+            position_ids=position_ids,\n+            attention_mask=attention_mask,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+\n+        aux_loss = None\n+        if kwargs.get(\"output_router_logits\", False):\n+            aux_loss = load_balancing_loss_func(\n+                outputs.router_logits,\n+                self.num_experts,\n+                self.num_experts_per_tok,\n+                attention_mask,\n+            )\n+            if labels is not None:\n+                loss += self.config.text_config.router_aux_loss_coef * aux_loss.to(\n+                    loss.device\n+                )  # make sure to reside in the same device\n+\n+        return Glm4vMoeCausalLMOutputWithPast(\n+            loss=loss,\n+            aux_loss=aux_loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            rope_deltas=outputs.rope_deltas,\n+        )\n+\n+\n __all__ = [\n     \"Glm4vMoeConfig\",\n     \"Glm4vMoeTextConfig\","
        }
    ],
    "stats": {
        "total": 685,
        "additions": 480,
        "deletions": 205
    }
}