{
    "author": "gante",
    "message": "Tests: upcast `logits` to `float()` (#34042)\n\nupcast",
    "sha": "e878eaa9fc4da9cec1c74ae962e89092b6832db8",
    "files": [
        {
            "sha": "1bcb6641803c0430f44d277d161770292d945c7a",
            "filename": "tests/models/granite/test_modeling_granite.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e878eaa9fc4da9cec1c74ae962e89092b6832db8/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e878eaa9fc4da9cec1c74ae962e89092b6832db8/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py?ref=e878eaa9fc4da9cec1c74ae962e89092b6832db8",
            "patch": "@@ -538,7 +538,7 @@ def test_model_3b_logits_bf16(self):\n         self.assertTrue(\n             torch.allclose(\n                 EXPECTED_SLICE.to(torch_device),\n-                out.logits[0, 0, :15],\n+                out.logits[0, 0, :15].float(),\n                 atol=1e-3,\n                 rtol=1e-3,\n             )\n@@ -558,4 +558,4 @@ def test_model_3b_logits(self):\n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[-2.0984, -3.1294, -2.8153, -2.3568, -2.7337, -2.2624, -2.6016, -2.4022]])\n \n-        self.assertTrue(torch.allclose(EXPECTED_MEAN.to(torch_device), out.logits.mean(-1), atol=1e-2, rtol=1e-2))\n+        self.assertTrue(torch.allclose(EXPECTED_MEAN.to(torch_device), out.logits.float().mean(-1), atol=1e-2, rtol=1e-2))"
        },
        {
            "sha": "124ce0c3bb5ae661803e92e85d9ee38d40159cfa",
            "filename": "tests/models/granitemoe/test_modeling_granitemoe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e878eaa9fc4da9cec1c74ae962e89092b6832db8/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e878eaa9fc4da9cec1c74ae962e89092b6832db8/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py?ref=e878eaa9fc4da9cec1c74ae962e89092b6832db8",
            "patch": "@@ -525,7 +525,9 @@ def test_model_3b_logits(self):\n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[-2.2122, -1.6632, -2.9269, -2.3344, -2.0143, -3.0146, -2.6839, -2.5610]])\n \n-        self.assertTrue(torch.allclose(EXPECTED_MEAN.to(torch_device), out.logits.mean(-1), atol=1e-2, rtol=1e-2))\n+        self.assertTrue(\n+            torch.allclose(EXPECTED_MEAN.to(torch_device), out.logits.float().mean(-1), atol=1e-2, rtol=1e-2)\n+        )\n \n         # slicing logits[0, 0, 0:15]\n         EXPECTED_SLICE = torch.tensor([[4.8785, -2.2890, -2.2892, -2.2885, -2.2890, -3.5007, -2.2897, -2.2892,\n@@ -535,7 +537,7 @@ def test_model_3b_logits(self):\n         self.assertTrue(\n             torch.allclose(\n                 EXPECTED_SLICE.to(torch_device),\n-                out.logits[0, 0, :15],\n+                out.logits[0, 0, :15].float(),\n                 atol=1e-3,\n                 rtol=1e-3,\n             )"
        },
        {
            "sha": "867f97c48a68ab0a222fbb82a061f26f90ced40c",
            "filename": "tests/models/jetmoe/test_modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e878eaa9fc4da9cec1c74ae962e89092b6832db8/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e878eaa9fc4da9cec1c74ae962e89092b6832db8/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py?ref=e878eaa9fc4da9cec1c74ae962e89092b6832db8",
            "patch": "@@ -481,7 +481,7 @@ def test_model_8b_logits(self):\n         model = JetMoeForCausalLM.from_pretrained(\"jetmoe/jetmoe-8b\")\n         input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n         with torch.no_grad():\n-            out = model(input_ids).logits.cpu()\n+            out = model(input_ids).logits.float().cpu()\n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[0.2507, -2.7073, -1.3445, -1.9363, -1.7216, -1.7370, -1.9054, -1.9792]])\n         torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, atol=1e-2, rtol=1e-2)"
        },
        {
            "sha": "d43a0fb13f367bad008b8b8cc72e41d5c20b4f7b",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 18,
            "deletions": 4,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/e878eaa9fc4da9cec1c74ae962e89092b6832db8/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e878eaa9fc4da9cec1c74ae962e89092b6832db8/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=e878eaa9fc4da9cec1c74ae962e89092b6832db8",
            "patch": "@@ -773,7 +773,14 @@ def test_model_7b_logits_bf16(self):\n             8: torch.tensor([[-6.5208, -4.1218, -4.9377, -3.2536,  0.8127, -2.9811,  1.2918, -3.3848]])\n         }\n \n-        self.assertTrue(torch.allclose(EXPECTED_MEAN[self.cuda_compute_capability_major_version].to(torch_device), out.logits.mean(-1), atol=1e-2, rtol=1e-2))\n+        self.assertTrue(\n+            torch.allclose(\n+                EXPECTED_MEAN[self.cuda_compute_capability_major_version].to(torch_device),\n+                out.logits.float().mean(-1),\n+                atol=1e-2,\n+                rtol=1e-2\n+            )\n+        )\n \n         # slicing logits[0, 0, 0:15]\n         EXPECTED_SLICE = {\n@@ -785,7 +792,7 @@ def test_model_7b_logits_bf16(self):\n         self.assertTrue(\n             torch.allclose(\n                 EXPECTED_SLICE[self.cuda_compute_capability_major_version].to(torch_device),\n-                out.logits[0, 0, :15],\n+                out.logits[0, 0, :15].float(),\n                 atol=1e-2,\n                 rtol=1e-2,\n             )\n@@ -810,7 +817,14 @@ def test_model_7b_logits(self):\n             8: torch.tensor([[-6.6544, -4.1259, -4.9840, -3.2456,  0.8261, -3.0124,  1.2971, -3.3641]])\n         }\n \n-        self.assertTrue(torch.allclose(EXPECTED_MEAN[self.cuda_compute_capability_major_version].to(torch_device), out.logits.mean(-1), atol=1e-2, rtol=1e-2))\n+        self.assertTrue(\n+            torch.allclose(\n+                EXPECTED_MEAN[self.cuda_compute_capability_major_version].to(torch_device),\n+                out.logits.float().mean(-1),\n+                atol=1e-2,\n+                rtol=1e-2\n+            )\n+        )\n \n         # slicing logits[0, 0, 0:15]\n         EXPECTED_SLICE = {\n@@ -822,7 +836,7 @@ def test_model_7b_logits(self):\n         self.assertTrue(\n             torch.allclose(\n                 EXPECTED_SLICE[self.cuda_compute_capability_major_version].to(torch_device),\n-                out.logits[0, 0, :15],\n+                out.logits[0, 0, :15].float(),\n                 atol=1e-2,\n                 rtol=1e-2,\n             )"
        },
        {
            "sha": "c24436d4b863f737dfd547f709489944a55db09c",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e878eaa9fc4da9cec1c74ae962e89092b6832db8/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e878eaa9fc4da9cec1c74ae962e89092b6832db8/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=e878eaa9fc4da9cec1c74ae962e89092b6832db8",
            "patch": "@@ -524,7 +524,7 @@ def test_model_7b_logits(self):\n         )\n         input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n         with torch.no_grad():\n-            out = model(input_ids).logits.cpu()\n+            out = model(input_ids).logits.float().cpu()\n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[-2.5548, -2.5737, -3.0600, -2.5906, -2.8478, -2.8118, -2.9325, -2.7694]])\n         torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, atol=1e-2, rtol=1e-2)"
        },
        {
            "sha": "e74785e29e9008e4372061f5150cc5a4f83e64fc",
            "filename": "tests/models/olmo/test_modeling_olmo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e878eaa9fc4da9cec1c74ae962e89092b6832db8/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e878eaa9fc4da9cec1c74ae962e89092b6832db8/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py?ref=e878eaa9fc4da9cec1c74ae962e89092b6832db8",
            "patch": "@@ -360,7 +360,7 @@ class OlmoIntegrationTest(unittest.TestCase):\n     def test_model_1b_logits(self):\n         input_ids = [[1, 306, 4658, 278, 6593, 310, 2834, 338]]\n         model = OlmoForCausalLM.from_pretrained(\"allenai/OLMo-1B-hf\", device_map=\"auto\")\n-        out = model(torch.tensor(input_ids)).logits\n+        out = model(torch.tensor(input_ids)).logits.float()\n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[2.2869, 0.3315, 0.9876, 1.4146, 1.8804, 2.0430, 1.7055, 1.2065]])\n         torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, atol=1e-2, rtol=1e-2)\n@@ -372,7 +372,7 @@ def test_model_1b_logits(self):\n     def test_model_7b_logits(self):\n         input_ids = [[1, 306, 4658, 278, 6593, 310, 2834, 338]]\n         model = OlmoForCausalLM.from_pretrained(\"allenai/OLMo-7B-hf\", device_map=\"auto\")\n-        out = model(torch.tensor(input_ids)).logits\n+        out = model(torch.tensor(input_ids)).logits.float()\n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[0.0271, 0.0249, -0.0578, -0.0870, 0.0167, 0.0710, 0.1002, 0.0677]])\n         torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, atol=1e-2, rtol=1e-2)\n@@ -384,7 +384,7 @@ def test_model_7b_logits(self):\n     def test_model_7b_twin_2t_logits(self):\n         input_ids = [[1, 306, 4658, 278, 6593, 310, 2834, 338]]\n         model = OlmoForCausalLM.from_pretrained(\"allenai/OLMo-7B-Twin-2T-hf\", device_map=\"auto\")\n-        out = model(torch.tensor(input_ids)).logits\n+        out = model(torch.tensor(input_ids)).logits.float()\n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[-0.3636, -0.3825, -0.4800, -0.3696, -0.8388, -0.9737, -0.9849, -0.8356]])\n         torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, atol=1e-2, rtol=1e-2)"
        },
        {
            "sha": "08ec1458efe146c5070160b43a666eb1793f56d1",
            "filename": "tests/models/olmoe/test_modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e878eaa9fc4da9cec1c74ae962e89092b6832db8/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e878eaa9fc4da9cec1c74ae962e89092b6832db8/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py?ref=e878eaa9fc4da9cec1c74ae962e89092b6832db8",
            "patch": "@@ -375,7 +375,7 @@ class OlmoeIntegrationTest(unittest.TestCase):\n     def test_model_7b_logits(self):\n         input_ids = [[1, 306, 4658, 278, 6593, 310, 2834, 338]]\n         model = OlmoeForCausalLM.from_pretrained(\"allenai/OLMoE-1B-7B-0924\", device_map=\"auto\")\n-        out = model(torch.tensor(input_ids)).logits\n+        out = model(torch.tensor(input_ids)).logits.float()\n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[-1.3814, -3.4450, -2.2990, -1.9542, -2.4387, -2.7941, -2.9312, -2.8309]])\n         torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, atol=1e-2, rtol=1e-2)"
        },
        {
            "sha": "99d84f9b5b5b096d7ee3d4f57a2dd077d28f7500",
            "filename": "tests/models/persimmon/test_modeling_persimmon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e878eaa9fc4da9cec1c74ae962e89092b6832db8/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e878eaa9fc4da9cec1c74ae962e89092b6832db8/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py?ref=e878eaa9fc4da9cec1c74ae962e89092b6832db8",
            "patch": "@@ -496,7 +496,7 @@ def test_model_8b_chat_logits(self):\n         model = PersimmonForCausalLM.from_pretrained(\n             \"adept/persimmon-8b-chat\", load_in_8bit=True, device_map={\"\": 0}, torch_dtype=torch.float16\n         )\n-        out = model(torch.tensor([input_ids], device=torch_device)).logits\n+        out = model(torch.tensor([input_ids], device=torch_device)).logits.float()\n \n         EXPECTED_MEAN = torch.tensor(\n             [[-11.4726, -11.1495, -11.2694, -11.2223, -10.9452, -11.0663, -11.0031, -11.1028]]"
        },
        {
            "sha": "debcf42ab38fad47666169882d0a01924ea9c9f5",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e878eaa9fc4da9cec1c74ae962e89092b6832db8/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e878eaa9fc4da9cec1c74ae962e89092b6832db8/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=e878eaa9fc4da9cec1c74ae962e89092b6832db8",
            "patch": "@@ -518,7 +518,7 @@ def test_model_450m_logits(self):\n         model = Qwen2ForCausalLM.from_pretrained(\"Qwen/Qwen2-450m-beta\", device_map=\"auto\")\n         input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n         with torch.no_grad():\n-            out = model(input_ids).logits.cpu()\n+            out = model(input_ids).logits.float().cpu()\n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[-2.5548, -2.5737, -3.0600, -2.5906, -2.8478, -2.8118, -2.9325, -2.7694]])\n         torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, atol=1e-2, rtol=1e-2)"
        },
        {
            "sha": "60df825c9b8a8d9e372851f188935c7efb4fa592",
            "filename": "tests/models/qwen2_moe/test_modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e878eaa9fc4da9cec1c74ae962e89092b6832db8/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e878eaa9fc4da9cec1c74ae962e89092b6832db8/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py?ref=e878eaa9fc4da9cec1c74ae962e89092b6832db8",
            "patch": "@@ -580,7 +580,7 @@ def test_model_a2_7b_logits(self):\n         model = Qwen2MoeForCausalLM.from_pretrained(\"Qwen/Qwen1.5-MoE-A2.7B\", device_map=\"auto\")\n         input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n         with torch.no_grad():\n-            out = model(input_ids).logits.cpu()\n+            out = model(input_ids).logits.float().cpu()\n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[-4.2125, -3.6416, -4.9136, -4.3005, -4.9938, -3.4393, -3.5195, -4.1621]])\n         torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, atol=1e-2, rtol=1e-2)"
        },
        {
            "sha": "e1f9bc2b8e8f9f12c4a6b8e87879d02c1af1c9a1",
            "filename": "tests/models/stablelm/test_modeling_stablelm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e878eaa9fc4da9cec1c74ae962e89092b6832db8/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e878eaa9fc4da9cec1c74ae962e89092b6832db8/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py?ref=e878eaa9fc4da9cec1c74ae962e89092b6832db8",
            "patch": "@@ -482,7 +482,7 @@ def test_model_stablelm_3b_4e1t_logits(self):\n         model = StableLmForCausalLM.from_pretrained(\"stabilityai/stablelm-3b-4e1t\").to(torch_device)\n         model.eval()\n \n-        output = model(**input_ids).logits\n+        output = model(**input_ids).logits.float()\n \n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[2.7146, 2.4245, 1.5616, 1.4424, 2.6790]]).to(torch_device)\n@@ -515,7 +515,7 @@ def test_model_tiny_random_stablelm_2_logits(self):\n         model = StableLmForCausalLM.from_pretrained(\"stabilityai/tiny-random-stablelm-2\").to(torch_device)\n         model.eval()\n \n-        output = model(**input_ids).logits\n+        output = model(**input_ids).logits.float()\n \n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[-2.7196, -3.6099, -2.6877, -3.1973, -3.9344]]).to(torch_device)"
        }
    ],
    "stats": {
        "total": 54,
        "additions": 35,
        "deletions": 19
    }
}