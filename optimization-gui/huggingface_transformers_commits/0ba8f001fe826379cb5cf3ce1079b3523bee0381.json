{
    "author": "hmellor",
    "message": "Add backward compatibility for methods which have been moved to `RotaryEmbeddingConfigMixin` (#42517)\n\n* Add backward compatibility for methods which have been moved to `RotaryEmbeddingConfigMixin`\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* They're not actually no-ops\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Fix type hint\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* If they're calling this function, they haven't standardised either\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* No need to BC method that wasn't in any releases\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>",
    "sha": "0ba8f001fe826379cb5cf3ce1079b3523bee0381",
    "files": [
        {
            "sha": "542c81a4eaedf946caa56e982ba9d04914dc4ed0",
            "filename": "src/transformers/modeling_rope_utils.py",
            "status": "modified",
            "additions": 19,
            "deletions": 1,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ba8f001fe826379cb5cf3ce1079b3523bee0381/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ba8f001fe826379cb5cf3ce1079b3523bee0381/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_rope_utils.py?ref=0ba8f001fe826379cb5cf3ce1079b3523bee0381",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \n import math\n+import warnings\n from functools import wraps\n from typing import TYPE_CHECKING, Optional, TypedDict\n \n@@ -656,7 +657,7 @@ def standardize_rope_params(self):\n         # Move `rope_theta` and `partial_rotary_factor` to the params dict, if not there yet\n         rope_theta = getattr(self, \"rope_theta\", None)\n         partial_rotary_factor = getattr(self, \"partial_rotary_factor\", None)\n-        rope_parameters = self.rope_parameters\n+        rope_parameters = self.rope_parameters or {}\n \n         # Case 1: RoPE param keys do not intersect with possible `layer_types` -> one global dict\n         if getattr(self, \"layer_types\", None) is None or not set(rope_parameters.keys()).issubset(self.layer_types):\n@@ -913,3 +914,20 @@ def _check_received_keys(\n             unused_keys = received_keys - required_keys\n         if unused_keys:\n             logger.warning(f\"Unrecognized keys in `rope_parameters` for 'rope_type'='{rope_type}': {unused_keys}\")\n+\n+\n+def rope_config_validation(config: RotaryEmbeddingConfigMixin, ignore_keys: Optional[set] = None):\n+    \"\"\"\n+    This is a deprecated function.\n+    It has been kept for backward compatibility with custom code models.\n+    \"\"\"\n+    warnings.warn(\n+        \"`rope_config_validation` is deprecated and has been removed. \"\n+        \"Its functionality has been moved to RotaryEmbeddingConfigMixin.validate_rope method. \"\n+        \"PreTrainedConfig inherits this class, so please call self.validate_rope() instead. \"\n+        \"Also, make sure to use the new rope_parameters syntax. \"\n+        \"You can call self.standardize_rope_params() in the meantime.\",\n+        FutureWarning,\n+    )\n+    config.standardize_rope_params()\n+    config.validate_rope(ignore_keys=ignore_keys)"
        }
    ],
    "stats": {
        "total": 20,
        "additions": 19,
        "deletions": 1
    }
}