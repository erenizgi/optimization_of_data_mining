{
    "author": "ctcanbol",
    "message": "Fix Qwen3 MoE GGUF architecture mismatch (#39976)\n\n* fix qwen3moe gguf architecture\n\n* Fix Qwen3Moe GGUF loading\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\nCo-authored-by: Jinuk Kim <jusjinuk@snu.ac.kr>",
    "sha": "085e02383c3d74a4e18e7ef8404c281b3e8ccfb5",
    "files": [
        {
            "sha": "9d89abfaa24d6406b486c2718af06d9cede3b113",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/085e02383c3d74a4e18e7ef8404c281b3e8ccfb5/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/085e02383c3d74a4e18e7ef8404c281b3e8ccfb5/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=085e02383c3d74a4e18e7ef8404c281b3e8ccfb5",
            "patch": "@@ -102,13 +102,14 @@\n         \"attention.layer_norm_rms_epsilon\": \"rms_norm_eps\",\n         \"vocab_size\": \"vocab_size\",\n     },\n-    \"qwen3moe\": {\n+    \"qwen3_moe\": {\n         \"context_length\": \"max_position_embeddings\",\n         \"block_count\": \"num_hidden_layers\",\n         \"feed_forward_length\": \"intermediate_size\",\n         \"embedding_length\": \"hidden_size\",\n         \"rope.dimension_count\": None,\n         \"rope.freq_base\": \"rope_theta\",\n+        \"attention.key_length\": \"head_dim\",\n         \"attention.head_count\": \"num_attention_heads\",\n         \"attention.head_count_kv\": \"num_key_value_heads\",\n         \"attention.layer_norm_rms_epsilon\": \"rms_norm_eps\","
        },
        {
            "sha": "7ef2725c10b0559d18859d5aa6c2c591ef38db2b",
            "filename": "src/transformers/modeling_gguf_pytorch_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/085e02383c3d74a4e18e7ef8404c281b3e8ccfb5/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/085e02383c3d74a4e18e7ef8404c281b3e8ccfb5/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py?ref=085e02383c3d74a4e18e7ef8404c281b3e8ccfb5",
            "patch": "@@ -246,6 +246,7 @@ def process(self, weights, name, **kwargs):\n TENSOR_PROCESSORS = {\n     \"llama\": LlamaTensorProcessor,\n     \"qwen2moe\": Qwen2MoeTensorProcessor,\n+    \"qwen3moe\": Qwen2MoeTensorProcessor,\n     \"bloom\": BloomTensorProcessor,\n     \"t5\": T5TensorProcessor,\n     \"t5encoder\": T5TensorProcessor,\n@@ -295,6 +296,8 @@ def get_gguf_hf_weights_map(\n         model_type = \"command-r\"\n     elif model_type == \"qwen2_moe\":\n         model_type = \"qwen2moe\"\n+    elif model_type == \"qwen3_moe\":\n+        model_type = \"qwen3moe\"\n     elif model_type == \"gemma3_text\":\n         model_type = \"gemma3\"\n     arch = None\n@@ -316,8 +319,8 @@ def get_gguf_hf_weights_map(\n     gguf_to_hf_name_map = {}\n     state_dict = hf_model.state_dict()\n     for hf_name in state_dict:\n-        # An exception for qwen2moe model, where the expert layers are packed\n-        if model_type == \"qwen2moe\" and \"mlp.experts.\" in hf_name:\n+        # An exception for qwen2moe/qwen3moe model, where the expert layers are packed\n+        if model_type in (\"qwen2moe\", \"qwen3moe\") and \"mlp.experts.\" in hf_name:\n             hf_name = re.sub(r\"mlp.experts.\\d+.\", \"mlp.experts.\", hf_name)\n \n         name, suffix = hf_name, \"\"\n@@ -391,6 +394,8 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False, model_to_lo\n \n     if \"qwen2moe\" in architecture:\n         updated_architecture = \"qwen2_moe\"\n+    elif \"qwen3moe\" in architecture:\n+        updated_architecture = \"qwen3_moe\"\n \n     # For stablelm architecture, we need to set qkv_bias and use_parallel_residual from tensors\n     # If `qkv_bias=True`, qkv_proj with bias will be present in the tensors"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 9,
        "deletions": 3
    }
}