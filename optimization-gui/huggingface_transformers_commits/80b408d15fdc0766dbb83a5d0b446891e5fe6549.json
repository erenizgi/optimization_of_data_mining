{
    "author": "yonigozlan",
    "message": "[SAM3] Compute masks once instead of per-layer, fix fa2 crash (#42543)\n\n* Compute masks once instead of per-layer, fix fa2 crash\n\n* nit\n\n* Change after review",
    "sha": "80b408d15fdc0766dbb83a5d0b446891e5fe6549",
    "files": [
        {
            "sha": "0a7777d929706ccbe4ce236546f53d6722db8c74",
            "filename": "src/transformers/models/sam3/modeling_sam3.py",
            "status": "modified",
            "additions": 75,
            "deletions": 79,
            "changes": 154,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b408d15fdc0766dbb83a5d0b446891e5fe6549/src%2Ftransformers%2Fmodels%2Fsam3%2Fmodeling_sam3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b408d15fdc0766dbb83a5d0b446891e5fe6549/src%2Ftransformers%2Fmodels%2Fsam3%2Fmodeling_sam3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3%2Fmodeling_sam3.py?ref=80b408d15fdc0766dbb83a5d0b446891e5fe6549",
            "patch": "@@ -14,9 +14,8 @@\n # limitations under the License.\n \n \n-import collections.abc\n import math\n-from collections.abc import Callable\n+from collections.abc import Callable, Iterable\n from dataclasses import dataclass\n from typing import Optional, Union\n \n@@ -40,7 +39,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import compile_compatible_method_lru_cache\n-from ...utils import auto_docstring\n+from ...utils import auto_docstring, logging\n from ...utils.generic import TransformersKwargs, check_model_inputs\n from ..auto import AutoModel\n from .configuration_sam3 import (\n@@ -54,6 +53,9 @@\n )\n \n \n+logger = logging.get_logger(__name__)\n+\n+\n @dataclass\n @auto_docstring\n class Sam3VisionEncoderOutput(ModelOutput):\n@@ -123,8 +125,8 @@ class Sam3DETRDecoderOutput(ModelOutput):\n         Decoder hidden states from all layers.\n     reference_boxes (`torch.FloatTensor` of shape `(num_layers, batch_size, num_queries, 4)`):\n         Predicted reference boxes from all decoder layers in (cx, cy, w, h) format.\n-    presence_logits (`torch.FloatTensor` of shape `(num_layers, batch_size)`, *optional*):\n-        Presence logits from all decoder layers (None if using instance queries).\n+    presence_logits (`torch.FloatTensor` of shape `(num_layers, batch_size, 1)`):\n+        Presence logits from all decoder layers indicating object presence confidence.\n     hidden_states (`tuple[torch.FloatTensor]`, *optional*):\n         Tuple of hidden states from all decoder layers.\n     attentions (`tuple[torch.FloatTensor]`, *optional*):\n@@ -133,7 +135,7 @@ class Sam3DETRDecoderOutput(ModelOutput):\n \n     intermediate_hidden_states: torch.FloatTensor = None\n     reference_boxes: torch.FloatTensor = None\n-    presence_logits: Optional[torch.FloatTensor] = None\n+    presence_logits: torch.FloatTensor = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n \n@@ -372,6 +374,19 @@ def forward(\n         if self.config._attn_implementation != \"eager\":\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n+        if (\n+            \"flash\" in self.config._attn_implementation\n+            and attention_mask is not None\n+            and attention_mask.dtype != torch.bool\n+        ):\n+            # Relative position bias tensors are represented as float masks and are incompatible with Flash Attention\n+            # Fallback to SDPA for this call only so the rest of the model can still benefit from FA\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[\"sdpa\"]\n+            logger.warning_once(\n+                \"Sam3Attention: falling back to SDPA for relative-position cross-attention because \"\n+                \"Flash Attention does not support additive bias masks.\"\n+            )\n+\n         attn_output, attn_weights = attention_interface(\n             self,\n             query,\n@@ -531,8 +546,8 @@ def __init__(self, config: Sam3ViTConfig):\n         image_size, patch_size = config.pretrain_image_size, config.patch_size\n         num_channels, hidden_size = config.num_channels, config.hidden_size\n \n-        image_size = image_size if isinstance(image_size, collections.abc.Iterable) else (image_size, image_size)\n-        patch_size = patch_size if isinstance(patch_size, collections.abc.Iterable) else (patch_size, patch_size)\n+        image_size = image_size if isinstance(image_size, Iterable) else (image_size, image_size)\n+        patch_size = patch_size if isinstance(patch_size, Iterable) else (patch_size, patch_size)\n         num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n         self.image_size = image_size\n         self.patch_size = patch_size\n@@ -1253,7 +1268,7 @@ def forward(\n         vision_feats: Tensor,\n         prompt_feats: Tensor,\n         vision_pos_encoding: Tensor,\n-        prompt_mask: Tensor,\n+        prompt_cross_attn_mask: Optional[Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         \"\"\"\n@@ -1263,7 +1278,7 @@ def forward(\n             vision_feats: Vision features [batch_size, vision_len, hidden_size] (main hidden states)\n             prompt_feats: Text prompt features [batch_size, text_len, hidden_size]\n             vision_pos_encoding: Position encoding for vision [batch_size, vision_len, hidden_size]\n-            prompt_mask: Padding mask for prompts [batch_size, text_len] where True=valid, False=padding\n+            prompt_cross_attn_mask: Cross-attention mask for prompt features\n \n         Returns:\n             Updated vision features [batch_size, vision_len, hidden_size]\n@@ -1284,15 +1299,6 @@ def forward(\n         residual = hidden_states\n         hidden_states = self.layer_norm2(hidden_states)\n \n-        prompt_cross_attn_mask = None\n-        if prompt_mask is not None:\n-            prompt_cross_attn_mask = create_bidirectional_mask(\n-                config=self.config,\n-                input_embeds=hidden_states,\n-                attention_mask=prompt_mask,\n-                encoder_hidden_states=prompt_feats,\n-            )\n-\n         hidden_states, _ = self.cross_attn(\n             query=hidden_states,\n             key=prompt_feats,\n@@ -1412,13 +1418,22 @@ def forward(\n             spatial_shapes,\n         ) = self._prepare_multilevel_features(vision_features, vision_pos_embeds)\n \n+        prompt_cross_attn_mask = None\n+        if text_mask is not None:\n+            prompt_cross_attn_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=features_flattened,\n+                attention_mask=text_mask,\n+                encoder_hidden_states=text_features,\n+            )\n+\n         hidden_states = features_flattened\n         for layer in self.layers:\n             hidden_states = layer(\n                 hidden_states,\n                 prompt_feats=text_features,\n                 vision_pos_encoding=pos_embeds_flattened,\n-                prompt_mask=text_mask,\n+                prompt_cross_attn_mask=prompt_cross_attn_mask,\n                 **kwargs,\n             )\n         return Sam3DETREncoderOutput(\n@@ -1484,31 +1499,27 @@ def forward(\n         text_features: torch.Tensor,\n         vision_features: torch.Tensor,\n         vision_pos_encoding: torch.Tensor,\n-        text_mask: Optional[torch.Tensor] = None,\n+        text_cross_attn_mask: Optional[torch.Tensor] = None,\n         vision_cross_attn_mask: Optional[torch.Tensor] = None,\n-        presence_token: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+    ) -> torch.Tensor:\n         \"\"\"\n         Forward pass for decoder layer.\n \n         Args:\n-            hidden_states: Query features [batch_size, num_queries, hidden_size]\n+            hidden_states: Query features [batch_size, num_queries + 1, hidden_size] (includes presence token at position 0)\n             query_pos: Query position embeddings [batch_size, num_queries, hidden_size]\n             text_features: Text features [batch_size, seq_len, hidden_size]\n             vision_features: Vision features [batch_size, height*width, hidden_size]\n             vision_pos_encoding: Vision position encoding [batch_size, height*width, hidden_size]\n-            text_mask: Text padding mask [batch_size, seq_len] where True=valid, False=padding\n-            vision_cross_attn_mask: Vision cross-attention mask [batch_size, num_heads, num_queries, height*width]\n-            presence_token: Optional presence token [batch_size, 1, hidden_size]\n+            text_cross_attn_mask: Text cross-attention mask\n+            vision_cross_attn_mask: Vision cross-attention mask, already expanded for presence token\n \n         Returns:\n-            Tuple of (updated hidden states, updated presence token)\n+            Updated hidden states (including presence token at position 0)\n         \"\"\"\n-        # Concatenate presence token if provided\n-        if presence_token is not None:\n-            hidden_states = torch.cat([presence_token, hidden_states], dim=1)\n-            query_pos = torch.cat([torch.zeros_like(presence_token), query_pos], dim=1)\n+        # Prepend zeros to query_pos for presence token\n+        query_pos = F.pad(query_pos, (0, 0, 1, 0), mode=\"constant\", value=0)\n \n         # Self-attention with query position encoding\n         residual = hidden_states\n@@ -1527,15 +1538,6 @@ def forward(\n         residual = hidden_states\n         query_with_pos = hidden_states + query_pos\n \n-        text_cross_attn_mask = None\n-        if text_mask is not None:\n-            text_cross_attn_mask = create_bidirectional_mask(\n-                config=self.config,\n-                input_embeds=hidden_states,\n-                attention_mask=text_mask,\n-                encoder_hidden_states=text_features,\n-            )\n-\n         attn_output, _ = self.text_cross_attn(\n             query=query_with_pos,\n             key=text_features,\n@@ -1546,20 +1548,6 @@ def forward(\n         hidden_states = residual + self.text_cross_attn_dropout(attn_output)\n         hidden_states = self.text_cross_attn_layer_norm(hidden_states)\n \n-        # Expand vision cross-attention mask for presence token if needed\n-        combined_vision_mask = vision_cross_attn_mask\n-        if presence_token is not None and combined_vision_mask is not None:\n-            batch_size, num_heads = combined_vision_mask.shape[:2]\n-            presence_mask = torch.zeros(\n-                batch_size,\n-                num_heads,\n-                1,\n-                combined_vision_mask.shape[-1],\n-                device=combined_vision_mask.device,\n-                dtype=combined_vision_mask.dtype,\n-            )\n-            combined_vision_mask = torch.cat([presence_mask, combined_vision_mask], dim=2)\n-\n         # Vision cross-attention: queries attend to vision features (with RPB)\n         residual = hidden_states\n         query_with_pos = hidden_states + query_pos\n@@ -1568,7 +1556,7 @@ def forward(\n             query=query_with_pos,\n             key=key_with_pos,\n             value=vision_features,\n-            attention_mask=combined_vision_mask,\n+            attention_mask=vision_cross_attn_mask,\n             **kwargs,\n         )\n         hidden_states = residual + self.vision_cross_attn_dropout(attn_output)\n@@ -1580,13 +1568,7 @@ def forward(\n         hidden_states = residual + self.mlp_dropout(hidden_states)\n         hidden_states = self.mlp_layer_norm(hidden_states)\n \n-        # Extract presence token if it was added\n-        presence_token_out = None\n-        if presence_token is not None:\n-            presence_token_out = hidden_states[:, :1]\n-            hidden_states = hidden_states[:, 1:]\n-\n-        return hidden_states, presence_token_out\n+        return hidden_states\n \n \n class Sam3DetrDecoder(Sam3PreTrainedModel):\n@@ -1715,11 +1697,23 @@ def forward(\n         \"\"\"\n         batch_size = vision_features.shape[0]\n \n-        hidden_states = self.query_embed.weight.unsqueeze(0).expand(batch_size, -1, -1)\n+        query_embeds = self.query_embed.weight.unsqueeze(0).expand(batch_size, -1, -1)\n         reference_boxes = self.reference_points.weight.unsqueeze(0).expand(batch_size, -1, -1)\n         reference_boxes = reference_boxes.sigmoid()\n         presence_token = self.presence_token.weight.unsqueeze(0).expand(batch_size, -1, -1)\n \n+        # Concatenate presence token with query embeddings\n+        hidden_states = torch.cat([presence_token, query_embeds], dim=1)\n+\n+        text_cross_attn_mask = None\n+        if text_mask is not None:\n+            text_cross_attn_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=hidden_states,\n+                attention_mask=text_mask,\n+                encoder_hidden_states=text_features,\n+            )\n+\n         intermediate_outputs = []\n         intermediate_boxes = [reference_boxes]\n         intermediate_presence_logits = []\n@@ -1734,43 +1728,45 @@ def forward(\n             vision_cross_attn_mask = None\n             if spatial_shapes is not None and spatial_shapes.shape[0] == 1:\n                 spatial_shape = (spatial_shapes[0, 0], spatial_shapes[0, 1])\n-                vision_cross_attn_mask = self._get_rpb_matrix(reference_boxes, spatial_shape)\n+                rpb_matrix = self._get_rpb_matrix(reference_boxes, spatial_shape)\n+                # Prepend zeros row for presence token (it attends to all vision tokens equally)\n+                vision_cross_attn_mask = F.pad(rpb_matrix, (0, 0, 1, 0), mode=\"constant\", value=0)\n \n-            hidden_states, presence_token = layer(\n+            hidden_states = layer(\n                 hidden_states,\n                 query_pos=query_pos,\n                 text_features=text_features,\n                 vision_features=vision_features,\n                 vision_pos_encoding=vision_pos_encoding,\n-                text_mask=text_mask,\n+                text_cross_attn_mask=text_cross_attn_mask,\n                 vision_cross_attn_mask=vision_cross_attn_mask,\n-                presence_token=presence_token,\n                 **kwargs,\n             )\n \n+            # Extract query hidden states (without presence token) for box refinement\n+            query_hidden_states = hidden_states[:, 1:]\n+\n             # Box refinement: predict delta and update reference boxes\n             reference_boxes_before_sigmoid = inverse_sigmoid(reference_boxes)\n-            delta_boxes = self.box_head(self.output_layer_norm(hidden_states))\n+            delta_boxes = self.box_head(self.output_layer_norm(query_hidden_states))\n             new_reference_boxes = (delta_boxes + reference_boxes_before_sigmoid).sigmoid()\n             reference_boxes = new_reference_boxes.detach()\n \n-            intermediate_outputs.append(self.output_layer_norm(hidden_states))\n+            intermediate_outputs.append(self.output_layer_norm(query_hidden_states))\n             intermediate_boxes.append(new_reference_boxes)\n \n             # Process presence token\n-            if presence_token is not None:\n-                presence_logits = self.presence_head(self.presence_layer_norm(presence_token)).squeeze(-1)\n-                presence_logits = presence_logits.clamp(\n-                    min=-self.clamp_presence_logit_max_val, max=self.clamp_presence_logit_max_val\n-                )\n-                intermediate_presence_logits.append(presence_logits)\n+            presence_hidden = hidden_states[:, :1]\n+            presence_logits = self.presence_head(self.presence_layer_norm(presence_hidden)).squeeze(-1)\n+            presence_logits = presence_logits.clamp(\n+                min=-self.clamp_presence_logit_max_val, max=self.clamp_presence_logit_max_val\n+            )\n+            intermediate_presence_logits.append(presence_logits)\n \n         # Stack outputs from all layers\n         intermediate_outputs = torch.stack(intermediate_outputs)\n         intermediate_boxes = torch.stack(intermediate_boxes[:-1])\n-        intermediate_presence_logits = (\n-            torch.stack(intermediate_presence_logits) if intermediate_presence_logits else None\n-        )\n+        intermediate_presence_logits = torch.stack(intermediate_presence_logits)\n \n         return Sam3DETRDecoderOutput(\n             intermediate_hidden_states=intermediate_outputs,"
        }
    ],
    "stats": {
        "total": 154,
        "additions": 75,
        "deletions": 79
    }
}