{
    "author": "li-plus",
    "message": "Optimize Qwen2VL vision model by precomputing cos/sin embeds before ViT blocks (#35837)\n\n* Optimize Qwen2VL vision model by precomputing cos/sin embeds before ViT blocks\n\n* Make rotary_pos_emb optional & fix type\n\n* Adapt pre-computed cos/sin to Qwen2.5VL\n\n* More concise",
    "sha": "5f0fd1185bbaea73bbd4b7c29cf2dcaddffbbdfd",
    "files": [
        {
            "sha": "8f9c7a5e6bba31b7692c0fb1d59fbd904631a9f3",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 85,
            "deletions": 32,
            "changes": 117,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0fd1185bbaea73bbd4b7c29cf2dcaddffbbdfd/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0fd1185bbaea73bbd4b7c29cf2dcaddffbbdfd/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=5f0fd1185bbaea73bbd4b7c29cf2dcaddffbbdfd",
            "patch": "@@ -160,12 +160,14 @@ def forward(self, x: torch.Tensor) -> torch.Tensor:\n         return x\n \n \n-def apply_rotary_pos_emb_flashatt(tensor: torch.Tensor, freqs: torch.Tensor) -> torch.Tensor:\n-    tensor_ = tensor.float()\n-    cos = freqs.cos().float()\n-    sin = freqs.sin().float()\n-    output = apply_rotary_emb(tensor_, cos, sin).type_as(tensor)\n-    return output\n+def apply_rotary_pos_emb_flashatt(\n+    q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n+    cos = cos.chunk(2, dim=-1)[0].contiguous()\n+    sin = sin.chunk(2, dim=-1)[0].contiguous()\n+    q_embed = apply_rotary_emb(q.float(), cos, sin).type_as(q)\n+    k_embed = apply_rotary_emb(k.float(), cos, sin).type_as(k)\n+    return q_embed, k_embed\n \n \n class Qwen2_5_VLVisionFlashAttention2(nn.Module):\n@@ -179,12 +181,26 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         cu_seqlens: torch.Tensor,\n-        rotary_pos_emb: torch.Tensor = None,\n+        rotary_pos_emb: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n     ) -> torch.Tensor:\n         seq_length = hidden_states.shape[0]\n         q, k, v = self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)\n-        q = apply_rotary_pos_emb_flashatt(q.unsqueeze(0), rotary_pos_emb).squeeze(0)\n-        k = apply_rotary_pos_emb_flashatt(k.unsqueeze(0), rotary_pos_emb).squeeze(0)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `rotary_pos_emb` (2D tensor of RoPE theta values), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.54 `rotary_pos_emb` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n+            cos = emb.cos().float()\n+            sin = emb.sin().float()\n+        else:\n+            cos, sin = position_embeddings\n+        q, k = apply_rotary_pos_emb_flashatt(q.unsqueeze(0), k.unsqueeze(0), cos, sin)\n+        q = q.squeeze(0)\n+        k = k.squeeze(0)\n \n         max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n         attn_output = flash_attn_varlen_func(q, k, v, cu_seqlens, cu_seqlens, max_seqlen, max_seqlen).reshape(\n@@ -201,16 +217,18 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-def apply_rotary_pos_emb_vision(tensor: torch.Tensor, freqs: torch.Tensor) -> torch.Tensor:\n-    orig_dtype = tensor.dtype\n-    tensor = tensor.float()\n-    cos = freqs.cos()\n-    sin = freqs.sin()\n-    cos = cos.unsqueeze(1).repeat(1, 1, 2).unsqueeze(0).float()\n-    sin = sin.unsqueeze(1).repeat(1, 1, 2).unsqueeze(0).float()\n-    output = (tensor * cos) + (rotate_half(tensor) * sin)\n-    output = output.to(orig_dtype)\n-    return output\n+def apply_rotary_pos_emb_vision(\n+    q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n+    orig_q_dtype = q.dtype\n+    orig_k_dtype = k.dtype\n+    q, k = q.float(), k.float()\n+    cos, sin = cos.unsqueeze(-2), sin.unsqueeze(-2)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    q_embed = q_embed.to(orig_q_dtype)\n+    k_embed = k_embed.to(orig_k_dtype)\n+    return q_embed, k_embed\n \n \n class Qwen2_5_VLVisionAttention(nn.Module):\n@@ -222,12 +240,27 @@ def __init__(self, dim: int, num_heads: int = 16) -> None:\n         self.proj = nn.Linear(dim, dim)\n \n     def forward(\n-        self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor, rotary_pos_emb: torch.Tensor = None\n+        self,\n+        hidden_states: torch.Tensor,\n+        cu_seqlens: torch.Tensor,\n+        rotary_pos_emb: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n     ) -> torch.Tensor:\n         seq_length = hidden_states.shape[0]\n         q, k, v = self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)\n-        q = apply_rotary_pos_emb_vision(q.unsqueeze(0), rotary_pos_emb).squeeze(0)\n-        k = apply_rotary_pos_emb_vision(k.unsqueeze(0), rotary_pos_emb).squeeze(0)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `rotary_pos_emb` (2D tensor of RoPE theta values), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.54 `rotary_pos_emb` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n+            cos = emb.cos().float()\n+            sin = emb.sin().float()\n+        else:\n+            cos, sin = position_embeddings\n+        q, k = apply_rotary_pos_emb_vision(q, k, cos, sin)\n \n         attention_mask = torch.full(\n             [1, seq_length, seq_length], torch.finfo(q.dtype).min, device=q.device, dtype=q.dtype\n@@ -256,12 +289,27 @@ def __init__(self, dim: int, num_heads: int = 16) -> None:\n         self.proj = nn.Linear(dim, dim)\n \n     def forward(\n-        self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor, rotary_pos_emb: torch.Tensor = None\n+        self,\n+        hidden_states: torch.Tensor,\n+        cu_seqlens: torch.Tensor,\n+        rotary_pos_emb: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n     ) -> torch.Tensor:\n         seq_length = hidden_states.shape[0]\n         q, k, v = self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)\n-        q = apply_rotary_pos_emb_vision(q.unsqueeze(0), rotary_pos_emb).squeeze(0)\n-        k = apply_rotary_pos_emb_vision(k.unsqueeze(0), rotary_pos_emb).squeeze(0)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `rotary_pos_emb` (2D tensor of RoPE theta values), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.54 `rotary_pos_emb` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n+            cos = emb.cos().float()\n+            sin = emb.sin().float()\n+        else:\n+            cos, sin = position_embeddings\n+        q, k = apply_rotary_pos_emb_vision(q, k, cos, sin)\n \n         attention_mask = torch.zeros([1, seq_length, seq_length], device=q.device, dtype=torch.bool)\n         for i in range(1, len(cu_seqlens)):\n@@ -293,11 +341,18 @@ def __init__(self, config, attn_implementation: str = \"sdpa\") -> None:\n         )\n         self.mlp = Qwen2_5_VLMLP(config, bias=True)\n \n-    def forward(self, hidden_states, cu_seqlens, rotary_pos_emb) -> torch.Tensor:\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        cu_seqlens: torch.Tensor,\n+        rotary_pos_emb: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+    ) -> torch.Tensor:\n         hidden_states = hidden_states + self.attn(\n             self.norm1(hidden_states),\n             cu_seqlens=cu_seqlens,\n             rotary_pos_emb=rotary_pos_emb,\n+            position_embeddings=position_embeddings,\n         )\n         hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))\n         return hidden_states\n@@ -477,6 +532,8 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n         rotary_pos_emb = rotary_pos_emb.reshape(seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)\n         rotary_pos_emb = rotary_pos_emb[window_index, :, :]\n         rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)\n+        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n+        position_embeddings = (emb.cos(), emb.sin())\n \n         cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0]).cumsum(\n             dim=0,\n@@ -495,14 +552,10 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n                 cu_seqlens_now = cu_window_seqlens\n             if self.gradient_checkpointing and self.training:\n                 hidden_states = self._gradient_checkpointing_func(\n-                    blk.__call__, hidden_states, cu_seqlens_now, rotary_pos_emb\n+                    blk.__call__, hidden_states, cu_seqlens_now, None, position_embeddings\n                 )\n             else:\n-                hidden_states = blk(\n-                    hidden_states,\n-                    cu_seqlens=cu_seqlens_now,\n-                    rotary_pos_emb=rotary_pos_emb,\n-                )\n+                hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens_now, position_embeddings=position_embeddings)\n \n         hidden_states = self.merger(hidden_states)\n         reverse_indices = torch.argsort(window_index)"
        },
        {
            "sha": "da54b231420bf0ab94b7aebdf6ece8136e399122",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 41,
            "deletions": 17,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0fd1185bbaea73bbd4b7c29cf2dcaddffbbdfd/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0fd1185bbaea73bbd4b7c29cf2dcaddffbbdfd/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=5f0fd1185bbaea73bbd4b7c29cf2dcaddffbbdfd",
            "patch": "@@ -51,7 +51,7 @@\n from ...image_utils import ImageInput, VideoInput\n from ...processing_utils import ProcessingKwargs, Unpack, VideosKwargs\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import is_flash_attn_2_available, is_torchdynamo_compiling\n+from ...utils import is_flash_attn_2_available, is_torchdynamo_compiling, logging\n \n \n if is_flash_attn_2_available():\n@@ -63,12 +63,17 @@\n     apply_rotary_emb = None\n \n \n-def apply_rotary_pos_emb_flashatt(tensor: torch.Tensor, freqs: torch.Tensor) -> torch.Tensor:\n-    tensor_ = tensor.float()\n-    cos = freqs.cos().float()\n-    sin = freqs.sin().float()\n-    output = apply_rotary_emb(tensor_, cos, sin).type_as(tensor)\n-    return output\n+logger = logging.get_logger(__name__)\n+\n+\n+def apply_rotary_pos_emb_flashatt(\n+    q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n+    cos = cos.chunk(2, dim=-1)[0].contiguous()\n+    sin = sin.chunk(2, dim=-1)[0].contiguous()\n+    q_embed = apply_rotary_emb(q.float(), cos, sin).type_as(q)\n+    k_embed = apply_rotary_emb(k.float(), cos, sin).type_as(k)\n+    return q_embed, k_embed\n \n \n class Qwen2_5_VLVisionConfig(PretrainedConfig):\n@@ -153,12 +158,26 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         cu_seqlens: torch.Tensor,\n-        rotary_pos_emb: torch.Tensor = None,\n+        rotary_pos_emb: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n     ) -> torch.Tensor:\n         seq_length = hidden_states.shape[0]\n         q, k, v = self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)\n-        q = apply_rotary_pos_emb_flashatt(q.unsqueeze(0), rotary_pos_emb).squeeze(0)\n-        k = apply_rotary_pos_emb_flashatt(k.unsqueeze(0), rotary_pos_emb).squeeze(0)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `rotary_pos_emb` (2D tensor of RoPE theta values), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.54 `rotary_pos_emb` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n+            cos = emb.cos().float()\n+            sin = emb.sin().float()\n+        else:\n+            cos, sin = position_embeddings\n+        q, k = apply_rotary_pos_emb_flashatt(q.unsqueeze(0), k.unsqueeze(0), cos, sin)\n+        q = q.squeeze(0)\n+        k = k.squeeze(0)\n \n         max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n         attn_output = flash_attn_varlen_func(q, k, v, cu_seqlens, cu_seqlens, max_seqlen, max_seqlen).reshape(\n@@ -193,11 +212,18 @@ def __init__(self, config, attn_implementation: str = \"sdpa\") -> None:\n         )\n         self.mlp = Qwen2_5_VLMLP(config, bias=True)\n \n-    def forward(self, hidden_states, cu_seqlens, rotary_pos_emb) -> torch.Tensor:\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        cu_seqlens: torch.Tensor,\n+        rotary_pos_emb: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+    ) -> torch.Tensor:\n         hidden_states = hidden_states + self.attn(\n             self.norm1(hidden_states),\n             cu_seqlens=cu_seqlens,\n             rotary_pos_emb=rotary_pos_emb,\n+            position_embeddings=position_embeddings,\n         )\n         hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))\n         return hidden_states\n@@ -337,6 +363,8 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n         rotary_pos_emb = rotary_pos_emb.reshape(seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)\n         rotary_pos_emb = rotary_pos_emb[window_index, :, :]\n         rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)\n+        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n+        position_embeddings = (emb.cos(), emb.sin())\n \n         cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0]).cumsum(\n             dim=0,\n@@ -355,14 +383,10 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n                 cu_seqlens_now = cu_window_seqlens\n             if self.gradient_checkpointing and self.training:\n                 hidden_states = self._gradient_checkpointing_func(\n-                    blk.__call__, hidden_states, cu_seqlens_now, rotary_pos_emb\n+                    blk.__call__, hidden_states, cu_seqlens_now, None, position_embeddings\n                 )\n             else:\n-                hidden_states = blk(\n-                    hidden_states,\n-                    cu_seqlens=cu_seqlens_now,\n-                    rotary_pos_emb=rotary_pos_emb,\n-                )\n+                hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens_now, position_embeddings=position_embeddings)\n \n         hidden_states = self.merger(hidden_states)\n         reverse_indices = torch.argsort(window_index)"
        },
        {
            "sha": "91ec520d5146106bbc84eaae4356356140d39418",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 81,
            "deletions": 23,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f0fd1185bbaea73bbd4b7c29cf2dcaddffbbdfd/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f0fd1185bbaea73bbd4b7c29cf2dcaddffbbdfd/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=5f0fd1185bbaea73bbd4b7c29cf2dcaddffbbdfd",
            "patch": "@@ -214,16 +214,18 @@ def apply_multimodal_rotary_pos_emb(q, k, cos, sin, mrope_section, unsqueeze_dim\n     return q_embed, k_embed\n \n \n-def apply_rotary_pos_emb_vision(tensor: torch.Tensor, freqs: torch.Tensor) -> torch.Tensor:\n-    orig_dtype = tensor.dtype\n-    tensor = tensor.float()\n-    cos = freqs.cos()\n-    sin = freqs.sin()\n-    cos = cos.unsqueeze(1).repeat(1, 1, 2).unsqueeze(0).float()\n-    sin = sin.unsqueeze(1).repeat(1, 1, 2).unsqueeze(0).float()\n-    output = (tensor * cos) + (rotate_half(tensor) * sin)\n-    output = output.to(orig_dtype)\n-    return output\n+def apply_rotary_pos_emb_vision(\n+    q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n+    orig_q_dtype = q.dtype\n+    orig_k_dtype = k.dtype\n+    q, k = q.float(), k.float()\n+    cos, sin = cos.unsqueeze(-2), sin.unsqueeze(-2)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    q_embed = q_embed.to(orig_q_dtype)\n+    k_embed = k_embed.to(orig_k_dtype)\n+    return q_embed, k_embed\n \n \n class VisionRotaryEmbedding(nn.Module):\n@@ -300,12 +302,27 @@ def __init__(self, dim: int, num_heads: int = 16) -> None:\n         self.proj = nn.Linear(dim, dim)\n \n     def forward(\n-        self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor, rotary_pos_emb: torch.Tensor = None\n+        self,\n+        hidden_states: torch.Tensor,\n+        cu_seqlens: torch.Tensor,\n+        rotary_pos_emb: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n     ) -> torch.Tensor:\n         seq_length = hidden_states.shape[0]\n         q, k, v = self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)\n-        q = apply_rotary_pos_emb_vision(q.unsqueeze(0), rotary_pos_emb).squeeze(0)\n-        k = apply_rotary_pos_emb_vision(k.unsqueeze(0), rotary_pos_emb).squeeze(0)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `rotary_pos_emb` (2D tensor of RoPE theta values), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.54 `rotary_pos_emb` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n+            cos = emb.cos().float()\n+            sin = emb.sin().float()\n+        else:\n+            cos, sin = position_embeddings\n+        q, k = apply_rotary_pos_emb_vision(q, k, cos, sin)\n \n         attention_mask = torch.full(\n             [1, seq_length, seq_length], torch.finfo(q.dtype).min, device=q.device, dtype=q.dtype\n@@ -334,12 +351,27 @@ def __init__(self, dim: int, num_heads: int = 16) -> None:\n         self.proj = nn.Linear(dim, dim)\n \n     def forward(\n-        self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor, rotary_pos_emb: torch.Tensor = None\n+        self,\n+        hidden_states: torch.Tensor,\n+        cu_seqlens: torch.Tensor,\n+        rotary_pos_emb: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n     ) -> torch.Tensor:\n         seq_length = hidden_states.shape[0]\n         q, k, v = self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)\n-        q = apply_rotary_pos_emb_vision(q.unsqueeze(0), rotary_pos_emb).squeeze(0)\n-        k = apply_rotary_pos_emb_vision(k.unsqueeze(0), rotary_pos_emb).squeeze(0)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `rotary_pos_emb` (2D tensor of RoPE theta values), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.54 `rotary_pos_emb` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n+            cos = emb.cos().float()\n+            sin = emb.sin().float()\n+        else:\n+            cos, sin = position_embeddings\n+        q, k = apply_rotary_pos_emb_vision(q, k, cos, sin)\n \n         max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n         attn_output = flash_attn_varlen_func(q, k, v, cu_seqlens, cu_seqlens, max_seqlen, max_seqlen).reshape(\n@@ -357,12 +389,27 @@ def __init__(self, dim: int, num_heads: int = 16) -> None:\n         self.proj = nn.Linear(dim, dim)\n \n     def forward(\n-        self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor, rotary_pos_emb: torch.Tensor = None\n+        self,\n+        hidden_states: torch.Tensor,\n+        cu_seqlens: torch.Tensor,\n+        rotary_pos_emb: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n     ) -> torch.Tensor:\n         seq_length = hidden_states.shape[0]\n         q, k, v = self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)\n-        q = apply_rotary_pos_emb_vision(q.unsqueeze(0), rotary_pos_emb).squeeze(0)\n-        k = apply_rotary_pos_emb_vision(k.unsqueeze(0), rotary_pos_emb).squeeze(0)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `rotary_pos_emb` (2D tensor of RoPE theta values), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.54 `rotary_pos_emb` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n+            cos = emb.cos().float()\n+            sin = emb.sin().float()\n+        else:\n+            cos, sin = position_embeddings\n+        q, k = apply_rotary_pos_emb_vision(q, k, cos, sin)\n \n         attention_mask = torch.zeros([1, seq_length, seq_length], device=q.device, dtype=torch.bool)\n         for i in range(1, len(cu_seqlens)):\n@@ -396,9 +443,18 @@ def __init__(self, config, attn_implementation: str = \"sdpa\") -> None:\n         )\n         self.mlp = VisionMlp(dim=config.embed_dim, hidden_dim=mlp_hidden_dim, hidden_act=config.hidden_act)\n \n-    def forward(self, hidden_states, cu_seqlens, rotary_pos_emb) -> torch.Tensor:\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        cu_seqlens: torch.Tensor,\n+        rotary_pos_emb: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+    ) -> torch.Tensor:\n         hidden_states = hidden_states + self.attn(\n-            self.norm1(hidden_states), cu_seqlens=cu_seqlens, rotary_pos_emb=rotary_pos_emb\n+            self.norm1(hidden_states),\n+            cu_seqlens=cu_seqlens,\n+            rotary_pos_emb=rotary_pos_emb,\n+            position_embeddings=position_embeddings,\n         )\n         hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))\n         return hidden_states\n@@ -961,6 +1017,8 @@ def rot_pos_emb(self, grid_thw):\n     def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.Tensor:\n         hidden_states = self.patch_embed(hidden_states)\n         rotary_pos_emb = self.rot_pos_emb(grid_thw)\n+        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n+        position_embeddings = (emb.cos(), emb.sin())\n \n         cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0]).cumsum(\n             dim=0,\n@@ -975,10 +1033,10 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n         for blk in self.blocks:\n             if self.gradient_checkpointing and self.training:\n                 hidden_states = self._gradient_checkpointing_func(\n-                    blk.__call__, hidden_states, cu_seqlens, rotary_pos_emb\n+                    blk.__call__, hidden_states, cu_seqlens, None, position_embeddings\n                 )\n             else:\n-                hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens, rotary_pos_emb=rotary_pos_emb)\n+                hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens, position_embeddings=position_embeddings)\n \n         return self.merger(hidden_states)\n "
        }
    ],
    "stats": {
        "total": 279,
        "additions": 207,
        "deletions": 72
    }
}