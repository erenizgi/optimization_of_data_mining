{
    "author": "LysandreJik",
    "message": "TP + EP + MoE disclaimer (#42519)",
    "sha": "3ea7ecd5ba19c669b8d08aca78063fc50f9d8331",
    "files": [
        {
            "sha": "b0c9745aeb42bb43c4a8fe8049657d8ce6d35777",
            "filename": "MIGRATION_GUIDE_V5.md",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ea7ecd5ba19c669b8d08aca78063fc50f9d8331/MIGRATION_GUIDE_V5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ea7ecd5ba19c669b8d08aca78063fc50f9d8331/MIGRATION_GUIDE_V5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/MIGRATION_GUIDE_V5.md?ref=3ea7ecd5ba19c669b8d08aca78063fc50f9d8331",
            "patch": "@@ -317,8 +317,18 @@ labels = tokenizer(text_target=tgt_texts, ...)\n ## Disclaimers for the RC0\n \n ### PEFT + MoE:\n+\n Because we are switching from the naive MOE (`nn.ModuleList` for experts) we currently have an issue with MoEs that have adapters. For more details see https://github.com/huggingface/transformers/issues/42491#issuecomment-3591485649. \n \n+_We aim for this to be fixed and released in a following release candidate in the week that follows RC0._\n+\n+### Tensor parallel and Expert parallel + MoE\n+\n+We are streamlining the MoE support with vLLM; while this is being implemented, tensor parallelism and expert parallelism aren't working as expected.\n+This is known and actively being worked on.\n+\n+_We aim for this to be fixed and released in a following release candidate in the week that follows RC0._\n+\n ### Custom pretrained models:\n For anyone inheriting from a `transformers` `PreTrainedModel`, the weights are automatically initialized with the common scheme: \n ```python"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 10,
        "deletions": 0
    }
}