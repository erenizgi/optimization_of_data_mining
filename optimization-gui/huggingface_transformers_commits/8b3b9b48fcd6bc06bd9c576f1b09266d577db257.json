{
    "author": "eljandoubi",
    "message": "exclude fsdp from delay_optimizer_creation (#34140)\n\n* exclude fsdp from delay_optimizer_creation\r\n\r\n* add test case for trainer: FSDP mode and fp8 as mixed precision\r\n\r\n* rearrange imports\r\n\r\n* ruff formatted\r\n\r\n* adapt _init_fsdp to fp8\r\n\r\n* use _init_fsdp only when resume_from_checkpoint\r\n\r\n* In case of FDP, self.layer will be CheckpointWrapper which has no len() method\r\n\r\n* delete _init_fsdp\r\n\r\n* solve conflict\r\n\r\n* fix conflict\r\n\r\n* make fixup",
    "sha": "8b3b9b48fcd6bc06bd9c576f1b09266d577db257",
    "files": [
        {
            "sha": "2781e9e102e05071d492d4e39871c1c27b1e5e92",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b3b9b48fcd6bc06bd9c576f1b09266d577db257/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b3b9b48fcd6bc06bd9c576f1b09266d577db257/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=8b3b9b48fcd6bc06bd9c576f1b09266d577db257",
            "patch": "@@ -144,6 +144,7 @@\n \n if is_accelerate_available():\n     from accelerate.state import AcceleratorState, PartialState\n+    from accelerate.utils.imports import is_fp8_available\n \n \n if is_pytest_available():\n@@ -1000,6 +1001,13 @@ def require_torch_fp16(test_case):\n     )(test_case)\n \n \n+def require_fp8(test_case):\n+    \"\"\"Decorator marking a test that requires supports for fp8\"\"\"\n+    return unittest.skipUnless(is_accelerate_available() and is_fp8_available(), \"test requires fp8 support\")(\n+        test_case\n+    )\n+\n+\n def require_torch_bf16(test_case):\n     \"\"\"Decorator marking a test that requires a device that supports bf16\"\"\"\n     return unittest.skipUnless("
        },
        {
            "sha": "64cb5c6bd4ddbb3b2a9f0d7e44b6484569c5eff4",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b3b9b48fcd6bc06bd9c576f1b09266d577db257/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b3b9b48fcd6bc06bd9c576f1b09266d577db257/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=8b3b9b48fcd6bc06bd9c576f1b09266d577db257",
            "patch": "@@ -2209,7 +2209,7 @@ def _inner_training_loop(\n             else:\n                 debug_overflow = DebugUnderflowOverflow(self.model)  # noqa\n \n-        delay_optimizer_creation = is_sagemaker_mp_enabled() or self.is_fsdp_xla_enabled or self.is_fsdp_enabled\n+        delay_optimizer_creation = is_sagemaker_mp_enabled() or self.is_fsdp_xla_enabled\n \n         # We need to reset the scheduler, as its parameters may be different on subsequent calls\n         if self._created_lr_scheduler:\n@@ -2258,9 +2258,12 @@ def _inner_training_loop(\n         # FSDP-XLA, SageMaker MP/DP, DataParallel, IPEX\n         use_accelerator_prepare = True if model is self.model else False\n \n+        # configure fsdp plugin for qlora if any\n+        if use_accelerator_prepare:\n+            self._fsdp_qlora_plugin_updates()\n+\n         if delay_optimizer_creation:\n             if use_accelerator_prepare:\n-                self._fsdp_qlora_plugin_updates()\n                 self.model = self.accelerator.prepare(self.model)\n             self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n "
        },
        {
            "sha": "4bcf5de04520e2174c36f17f653f680a30746594",
            "filename": "tests/trainer/test_trainer_fsdp.py",
            "status": "modified",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b3b9b48fcd6bc06bd9c576f1b09266d577db257/tests%2Ftrainer%2Ftest_trainer_fsdp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b3b9b48fcd6bc06bd9c576f1b09266d577db257/tests%2Ftrainer%2Ftest_trainer_fsdp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_fsdp.py?ref=8b3b9b48fcd6bc06bd9c576f1b09266d577db257",
            "patch": "@@ -20,6 +20,8 @@\n     execute_subprocess_async,\n     get_torch_dist_unique_port,\n     require_accelerate,\n+    require_fp8,\n+    require_fsdp,\n     require_torch_multi_gpu,\n )\n \n@@ -64,6 +66,7 @@ def __getitem__(self, i: int) -> str:\n class TestFSDPTrainer(TestCasePlus):\n     @require_accelerate\n     @require_torch_multi_gpu\n+    @require_fsdp\n     def test_trainer(self):\n         output_dir = self.get_auto_remove_tmp_dir()\n         cmd = [\n@@ -86,6 +89,35 @@ def test_trainer(self):\n         # successful return here == success - any errors would have caused an error in the sub-call\n \n \n+class TestFSDPTrainerFP8(TestCasePlus):\n+    @require_accelerate\n+    @require_torch_multi_gpu\n+    @require_fsdp\n+    @require_fp8\n+    def test_trainer(self):\n+        output_dir = self.get_auto_remove_tmp_dir()\n+        cmd = [\n+            \"accelerate\",\n+            \"launch\",\n+            \"--use_fsdp\",\n+            \"--main_process_port\",\n+            f\"{get_torch_dist_unique_port()}\",\n+            \"--num_processes\",\n+            f\"{torch.cuda.device_count()}\",\n+            \"--mixed_precision\",\n+            \"fp8\",\n+            \"--fsdp_transformer_layer_cls_to_wrap\",\n+            \"GPT2Block\",\n+            f\"{self.test_file_dir}/test_trainer_fsdp.py\",\n+            \"--output_dir\",\n+            f\"{output_dir}\",\n+            \"--report_to\",\n+            \"none\",\n+        ]\n+        execute_subprocess_async(cmd, env=self.get_env())\n+        # successful return here == success - any errors would have caused an error in the sub-call\n+\n+\n if __name__ == \"__main__\":\n     parser = HfArgumentParser((Seq2SeqTrainingArguments,))\n     training_args = parser.parse_args_into_dataclasses()[0]"
        }
    ],
    "stats": {
        "total": 47,
        "additions": 45,
        "deletions": 2
    }
}