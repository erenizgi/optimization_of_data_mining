{
    "author": "zucchini-nlp",
    "message": "Llava Onevision: add model (#32673)\n\n* working version\r\n\r\n* fix copies\r\n\r\n* update\r\n\r\n* tests\r\n\r\n* update docs\r\n\r\n* codestyle\r\n\r\n* add more tests\r\n\r\n* add returns for docs\r\n\r\n* clean up\r\n\r\n* Update src/transformers/models/llava_onevision/processing_llava_onevision.py\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* updates\r\n\r\n* codestyle\r\n\r\n* style\r\n\r\n* shouldn't be reversed\r\n\r\n* [run-slow] llava_onevision\r\n\r\n* [run-slow] llava_onevision\r\n\r\n* add pooling in videos\r\n\r\n* [run-slow] llava_onevision\r\n\r\n* num-logits-to-keep\r\n\r\n* [run-slow] llava_onevision\r\n\r\n* [run-slow] llava_onevision\r\n\r\n* Update tests/test_modeling_common.py\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* video matched orig impl\r\n\r\n* fix tests\r\n\r\n* chat template was modified\r\n\r\n* Update docs/source/en/model_doc/llava_onevision.md\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* add morer info in the doc page\r\n\r\n---------\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>",
    "sha": "43df47d8e78238021a4273746fc469336f948314",
    "files": [
        {
            "sha": "bf8c20186d676e7db50e5ef0ca88ee4e2b4c97c2",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -836,6 +836,8 @@\n         title: LLaVA-NeXT\n       - local: model_doc/llava_next_video\n         title: LLaVa-NeXT-Video\n+      - local: model_doc/llava_onevision\n+        title: LLaVA-Onevision\n       - local: model_doc/lxmert\n         title: LXMERT\n       - local: model_doc/matcha"
        },
        {
            "sha": "8e3a4da8b021de017f9b15f02082b2d096987200",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -189,6 +189,7 @@ Flax), PyTorch, and/or TensorFlow.\n |                         [LLaVa](model_doc/llava)                         |       ✅        |         ❌         |      ❌      |\n |                    [LLaVA-NeXT](model_doc/llava_next)                    |       ✅        |         ❌         |      ❌      |\n |              [LLaVa-NeXT-Video](model_doc/llava_next_video)              |       ✅        |         ❌         |      ❌      |\n+|               [LLaVA-Onevision](model_doc/llava_onevision)               |       ✅        |         ❌         |      ❌      |\n |                    [Longformer](model_doc/longformer)                    |       ✅        |         ✅         |      ❌      |\n |                        [LongT5](model_doc/longt5)                        |       ✅        |         ❌         |      ✅      |\n |                          [LUKE](model_doc/luke)                          |       ✅        |         ❌         |      ❌      |"
        },
        {
            "sha": "64a127abca4c281e46bf47592742408e7b38a02a",
            "filename": "docs/source/en/model_doc/llava_onevision.md",
            "status": "added",
            "additions": 319,
            "deletions": 0,
            "changes": 319,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -0,0 +1,319 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# LLaVA-Onevision\n+\n+## Overview\n+\n+The LLaVA-Onevision model was proposed in [LLaVA-OneVision: Easy Visual Task Transfer](https://arxiv.org/abs/2408.03326) by <Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li\n+\n+LLaVA-Onevision is a Vision-Language Model that can generate text conditioned on one or several images/videos. The model consists of SigLIP vision encoder and a Qwen2 language backbone. The images are processed with anyres-9 technique where the image is split into 9 patches to better process high resolution images and capture as much details as possible. However, videos are pooled to a total sequence length of 196 tokens each frame for more memory efficient computation. LLaVA-Onevision is available in three sizes: 0.5B, 7B and 72B and achieves remarkable performance on benchmark evaluations.\n+\n+The abstract from the paper is the following:\n+\n+*We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\n+developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. Our experimental results demonstrate that\n+LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios:\n+single-image, multi-image, and video scenarios. Importantly, the design of LLaVAOneVision allows strong transfer learning across different modalities/scenarios,\n+yielding new emerging capabilities. In particular, strong video understanding and\n+cross-scenario capabilities are demonstrated through task transfer from images to\n+videos.*\n+\n+\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/llava-ov-acrhitecture.png\"\n+alt=\"drawing\" width=\"600\"/>\n+\n+<small> LLaVA=Onevision architecture. Taken from the <a href=\"https://arxiv.org/abs/2408.03326\">original paper.</a> </small>\n+\n+Tips:\n+\n+- We advise users to use `padding_side=\"left\"` when computing batched generation as it leads to more accurate results. Simply make sure to call `processor.tokenizer.padding_side = \"left\"` before generating.\n+\n+<Tip warning={true}>\n+\n+- Llava-Onevision uses different number of patches for images and thus has to pad the inputs inside modeling code, aside from the padding done when processing the inputs. The default setting is \"left-padding\" if model is in `eval()` mode, otherwise \"right-padding\".\n+\n+</Tip>\n+\n+- Note that the model should use a specific prompt format, on which the large language model (LLM) was trained. You can use the processor's `apply_chat_template` to format your prompts correctly. For that you have to construct a conversation history, passing a plain string will not format your prompt. Each message in the conversation history for chat templates is a dictionary with keys \"role\" and \"content\". The \"content\" should be a list of dictionaries, for \"text\" and \"image\" modalities.\n+\n+We will use [llava-onevision-qwen2-7b-si-hf](https://huggingface.co/llava-hf/llava-onevision-qwen2-7b-si-hf) and a conversation history of text and image. Each content field has to be a list of dicts, as follows:\n+\n+```python\n+from transformers import AutoProcessor\n+\n+processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-si-hf\")\n+\n+conversation = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\"},\n+            {\"type\": \"text\", \"text\": \"What’s shown in this image?\"},\n+        ],\n+    },\n+    {\n+        \"role\": \"assistant\",\n+        \"content\": [{\"type\": \"text\", \"text\": \"This image shows a red stop sign.\"},]\n+    },\n+    {\n+\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"text\", \"text\": \"Describe the image in more details.\"},\n+        ],\n+    },\n+]\n+\n+text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n+\n+# Note that the template simply formats your prompt, you still have to tokenize it and obtain pixel values for your images\n+print(text_prompt)\n+>>> \"<|im_start|>user\\n<image>What is shown in this image?<|im_end|>\\n<|im_start|>assistant\\nPage showing the list of options.<|im_end|>\"\n+```\n+\n+This model was contributed by [RaushanTurganbay](https://huggingface.co/RaushanTurganbay).\n+The original code can be found [here](https://github.com/LLaVA-VL/LLaVA-NeXT/tree/main).\n+\n+\n+## Usage example\n+\n+### Single image inference\n+\n+Here's how to load the model and perform inference in half-precision (`torch.float16`):\n+\n+```python\n+from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n+import torch\n+from PIL import Image\n+import requests\n+\n+processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\") \n+model = LlavaOnevisionForConditionalGeneration.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\", torch_dtype=torch.float16, low_cpu_mem_usage=True) \n+model.to(\"cuda:0\")\n+\n+# prepare image and text prompt, using the appropriate prompt template\n+url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n+image = Image.open(requests.get(url, stream=True).raw)\n+\n+conversation = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\"},\n+            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+        ],\n+    },\n+]\n+prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n+inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(\"cuda:0\", torch.float16)\n+\n+# autoregressively complete prompt\n+output = model.generate(**inputs, max_new_tokens=100)\n+print(processor.decode(output[0], skip_special_tokens=True))\n+'user\\n\\nWhat is shown in this image?\\nassistant\\nThe image shows a radar chart, also known as a spider chart or a star chart, which is used to compare multiple quantitative variables. Each axis represents a different variable, and the chart is filled with'\n+```\n+\n+### Multi image inference\n+\n+LLaVa-Onevision can perform inference with multiple images as input, where images either belong to the same prompt or different prompts (in batched inference). For that you have to use checkpoints with an \"ov\" suffix. Here is how you can do it:\n+\n+```python\n+import requests\n+from PIL import Image\n+import torch\n+from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n+\n+# Load the model in half-precision\n+model = LlavaOnevisionForConditionalGeneration.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\")\n+\n+# Get three different images\n+url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n+image_stop = Image.open(requests.get(url, stream=True).raw)\n+\n+url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+image_cats = Image.open(requests.get(url, stream=True).raw)\n+\n+url = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg\"\n+image_snowman = Image.open(requests.get(url, stream=True).raw)\n+\n+# Prepare a batch of two prompts, where the first one is a multi-turn conversation and the second is not\n+conversation_1 = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\"},\n+            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+            ],\n+    },\n+    {\n+        \"role\": \"assistant\",\n+        \"content\": [\n+            {\"type\": \"text\", \"text\": \"There is a red stop sign in the image.\"},\n+            ],\n+    },\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\"},\n+            {\"type\": \"text\", \"text\": \"What about this image? How many cats do you see?\"},\n+            ],\n+    },\n+]\n+\n+conversation_2 = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\"},\n+            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+            ],\n+    },\n+]\n+\n+prompt_1 = processor.apply_chat_template(conversation_1, add_generation_prompt=True)\n+prompt_2 = processor.apply_chat_template(conversation_2, add_generation_prompt=True)\n+prompts = [prompt_1, prompt_2]\n+\n+# We can simply feed images in the order they have to be used in the text prompt\n+inputs = processor(images=[image_stop, image_cats, image_snowman], text=prompts, padding=True, return_tensors=\"pt\").to(model.device, torch.float16)\n+\n+# Generate\n+generate_ids = model.generate(**inputs, max_new_tokens=30)\n+processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n+['user\\n\\nWhat is shown in this image?\\nassistant\\nThere is a red stop sign in the image.\\nuser\\n\\nWhat about this image? How many cats do you see?\\nassistant\\ntwo', 'user\\n\\nWhat is shown in this image?\\nassistant\\n']\n+```\n+\n+### Video inference\n+\n+LLaVa-Onevision also can perform inference with videos as input, where video frames are treated as multiple images. Here is how you can do it:\n+\n+```python\n+import av\n+import numpy as np\n+from huggingface_hub import hf_hub_download\n+\n+import torch\n+from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n+\n+# Load the model in half-precision\n+model = LlavaOnevisionForConditionalGeneration.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\")\n+\n+\n+def read_video_pyav(container, indices):\n+    '''\n+    Decode the video with PyAV decoder.\n+    Args:\n+        container (`av.container.input.InputContainer`): PyAV container.\n+        indices (`List[int]`): List of frame indices to decode.\n+    Returns:\n+        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n+    '''\n+    frames = []\n+    container.seek(0)\n+    start_index = indices[0]\n+    end_index = indices[-1]\n+    for i, frame in enumerate(container.decode(video=0)):\n+        if i > end_index:\n+            break\n+        if i >= start_index and i in indices:\n+            frames.append(frame)\n+    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n+\n+# Load the video as an np.array, sampling uniformly 8 frames (can sample more for longer videos, up to 32 frames)\n+video_path = hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\")\n+container = av.open(video_path)\n+total_frames = container.streams.video[0].frames\n+indices = np.arange(0, total_frames, total_frames / 8).astype(int)\n+video = read_video_pyav(container, indices)\n+\n+# For videos we have to feed a \"video\" type instead of \"image\"\n+conversation = [\n+    {\n+\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"video\"},\n+            {\"type\": \"text\", \"text\": \"Why is this video funny?\"},\n+            ],\n+    },\n+]\n+\n+prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n+inputs = processor(videos=list(video), text=prompt, return_tensors=\"pt\").to(\"cuda:0\", torch.float16)\n+\n+out = model.generate(**inputs, max_new_tokens=60)\n+processor.batch_decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n+[\"user\\n\\nWhy is this video funny?\\nassistant\\nThe video appears to be humorous because it shows a young child, who is wearing glasses and holding a book, seemingly reading with a serious and focused expression. The child's glasses are a bit oversized for their face, which adds a comical touch, as it's a common trope to see children wearing\"]\n+```\n+\n+## Model optimization\n+\n+### Quantization using Bitsandbytes\n+\n+The model can be loaded in 8 or 4 bits, greatly reducing the memory requirements while maintaining the performance of the original model. First make sure to install bitsandbytes, `pip install bitsandbytes` and make sure to have access to a CUDA compatible GPU device. Simply change the snippet above with:\n+\n+```python\n+from transformers import LlavaOnevisionForConditionalGeneration, BitsAndBytesConfig\n+\n+# specify how to quantize the model\n+quantization_config = BitsAndBytesConfig(\n+    load_in_4bit=True,\n+    bnb_4bit_quant_type=\"nf4\",\n+    bnb_4bit_compute_dtype=torch.float16,\n+)\n+\n+model = LlavaOnevisionForConditionalGeneration.from_pretrained(model_id, quantization_config=quantization_config, device_map=\"auto\")\n+```\n+\n+### Use Flash-Attention 2 to further speed-up generation\n+\n+First make sure to install flash-attn. Refer to the [original repository of Flash Attention](https://github.com/Dao-AILab/flash-attention) regarding that package installation. Simply change the snippet above with:\n+\n+```python\n+from transformers import LlavaOnevisionForConditionalGeneration\n+\n+model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n+    model_id, \n+    torch_dtype=torch.float16, \n+    low_cpu_mem_usage=True,\n+    use_flash_attention_2=True\n+).to(0)\n+```\n+\n+\n+## LlavaOnevisionConfig\n+\n+[[autodoc]] LlavaOnevisionConfig\n+\n+## LlavaOnevisionProcessor\n+\n+[[autodoc]] LlavaOnevisionProcessor\n+\n+## LlavaOnevisionImageProcessor\n+\n+[[autodoc]] LlavaOnevisionImageProcessor\n+\n+## LlavaOnevisionVideoProcessor\n+\n+[[autodoc]] LlavaOnevisionVideoProcessor\n+\n+## LlavaOnevisionForConditionalGeneration\n+\n+[[autodoc]] LlavaOnevisionForConditionalGeneration\n+    - forward"
        },
        {
            "sha": "82bafc6b429561e68a13efd990d42dcf6efdf922",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -60,6 +60,7 @@ FlashAttention-2 is currently supported for the following architectures:\n * [Llava](https://huggingface.co/docs/transformers/model_doc/llava)\n * [Llava-NeXT](https://huggingface.co/docs/transformers/model_doc/llava_next)\n * [Llava-NeXT-Video](https://huggingface.co/docs/transformers/model_doc/llava_next_video)\n+* [LLaVA-Onevision](https://huggingface.co/docs/transformers/model_doc/llava_onevision)\n * [VipLlava](https://huggingface.co/docs/transformers/model_doc/vipllava)\n * [VideoLlava](https://huggingface.co/docs/transformers/model_doc/video_llava)\n * [M2M100](https://huggingface.co/docs/transformers/model_doc/m2m_100)\n@@ -226,6 +227,7 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [JetMoe](https://huggingface.co/docs/transformers/model_doc/jetmoe#transformers.JetMoeModel)\n * [Jamba](https://huggingface.co/docs/transformers/model_doc/jamba#transformers.JambaModel)\n * [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)\n+* [LLaVA-Onevision](https://huggingface.co/docs/transformers/model_doc/llava_onevision)\n * [Mistral](https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel)\n * [Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral#transformers.MixtralModel)\n * [Musicgen](https://huggingface.co/docs/transformers/model_doc/musicgen#transformers.MusicgenModel)"
        },
        {
            "sha": "4f4b17ac84f1fbd81848a54089b0982d2de3316a",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -533,6 +533,7 @@\n         \"LlavaNextVideoConfig\",\n         \"LlavaNextVideoProcessor\",\n     ],\n+    \"models.llava_onevision\": [\"LlavaOnevisionConfig\", \"LlavaOnevisionProcessor\"],\n     \"models.longformer\": [\n         \"LongformerConfig\",\n         \"LongformerTokenizer\",\n@@ -1183,6 +1184,9 @@\n     _import_structure[\"models.levit\"].extend([\"LevitFeatureExtractor\", \"LevitImageProcessor\"])\n     _import_structure[\"models.llava_next\"].append(\"LlavaNextImageProcessor\")\n     _import_structure[\"models.llava_next_video\"].append(\"LlavaNextVideoImageProcessor\")\n+    _import_structure[\"models.llava_onevision\"].extend(\n+        [\"LlavaOnevisionImageProcessor\", \"LlavaOnevisionVideoProcessor\"]\n+    )\n     _import_structure[\"models.mask2former\"].append(\"Mask2FormerImageProcessor\")\n     _import_structure[\"models.maskformer\"].extend([\"MaskFormerFeatureExtractor\", \"MaskFormerImageProcessor\"])\n     _import_structure[\"models.mobilenet_v1\"].extend([\"MobileNetV1FeatureExtractor\", \"MobileNetV1ImageProcessor\"])\n@@ -2532,6 +2536,12 @@\n             \"LlavaNextVideoPreTrainedModel\",\n         ]\n     )\n+    _import_structure[\"models.llava_onevision\"].extend(\n+        [\n+            \"LlavaOnevisionForConditionalGeneration\",\n+            \"LlavaOnevisionPreTrainedModel\",\n+        ]\n+    )\n     _import_structure[\"models.longformer\"].extend(\n         [\n             \"LongformerForMaskedLM\",\n@@ -5308,6 +5318,10 @@\n         LlavaNextVideoConfig,\n         LlavaNextVideoProcessor,\n     )\n+    from .models.llava_onevision import (\n+        LlavaOnevisionConfig,\n+        LlavaOnevisionProcessor,\n+    )\n     from .models.longformer import (\n         LongformerConfig,\n         LongformerTokenizer,\n@@ -5993,6 +6007,7 @@\n         from .models.levit import LevitFeatureExtractor, LevitImageProcessor\n         from .models.llava_next import LlavaNextImageProcessor\n         from .models.llava_next_video import LlavaNextVideoImageProcessor\n+        from .models.llava_onevision import LlavaOnevisionImageProcessor, LlavaOnevisionVideoProcessor\n         from .models.mask2former import Mask2FormerImageProcessor\n         from .models.maskformer import (\n             MaskFormerFeatureExtractor,\n@@ -7113,6 +7128,10 @@\n             LlavaNextVideoForConditionalGeneration,\n             LlavaNextVideoPreTrainedModel,\n         )\n+        from .models.llava_onevision import (\n+            LlavaOnevisionForConditionalGeneration,\n+            LlavaOnevisionPreTrainedModel,\n+        )\n         from .models.longformer import (\n             LongformerForMaskedLM,\n             LongformerForMultipleChoice,"
        },
        {
            "sha": "1cdd1ae135e113ed3ad33b5402b38595dc6c893a",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -1030,6 +1030,7 @@ def __init__(\n \n         self.batch_size = batch_size or max_batch_size\n         self.max_cache_len = config.max_position_embeddings if max_cache_len is None else max_cache_len\n+\n         # Some model define a custom `head_dim` != config.hidden_size // config.num_attention_heads\n         self.head_dim = (\n             config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads"
        },
        {
            "sha": "fa2b82ab4c2ba496aeb563b4ebdeb7a54a6cf524",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -1450,8 +1450,8 @@ def _get_cache(\n                     cache_dtype = self.get_output_embeddings().weight.dtype\n \n             cache_kwargs = {\n-                \"config\": self.config,\n-                \"batch_size\": batch_size,\n+                \"config\": self.config if hasattr(self.config, \"text_config\") else self.config,\n+                \"max_batch_size\": batch_size,\n                 \"max_cache_len\": max_cache_len,\n                 \"device\": device,\n                 \"dtype\": cache_dtype,\n@@ -2353,7 +2353,11 @@ def _dola_decoding(\n         this_peer_finished = False\n \n         # prepare layers for DoLa decoding\n-        final_layer = self.config.num_hidden_layers\n+        final_layer = (\n+            self.config.text_config.num_hidden_layers\n+            if hasattr(self.config, \"text_config\")\n+            else self.config.num_hidden_layers\n+        )\n         # if the model has tied word embeddings, we skip the word embeddings (0-th) layer and start from the 2nd layer,\n         # as the early exit from word embeddings will become identity function\n         # if the model is really shallow (<=2 layers), we use the 1st layer if it's not the final layer and the 0-th"
        },
        {
            "sha": "26b96def67d9928ebe7a736beeda4892c6ef8fd9",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -132,6 +132,7 @@\n     llava,\n     llava_next,\n     llava_next_video,\n+    llava_onevision,\n     longformer,\n     longt5,\n     luke,"
        },
        {
            "sha": "fa1a7fb88eafa8621d861ef3b00d711518e7a9e0",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -149,6 +149,7 @@\n         (\"llava\", \"LlavaConfig\"),\n         (\"llava_next\", \"LlavaNextConfig\"),\n         (\"llava_next_video\", \"LlavaNextVideoConfig\"),\n+        (\"llava_onevision\", \"LlavaOnevisionConfig\"),\n         (\"longformer\", \"LongformerConfig\"),\n         (\"longt5\", \"LongT5Config\"),\n         (\"luke\", \"LukeConfig\"),\n@@ -444,6 +445,7 @@\n         (\"llava\", \"LLaVa\"),\n         (\"llava_next\", \"LLaVA-NeXT\"),\n         (\"llava_next_video\", \"LLaVa-NeXT-Video\"),\n+        (\"llava_onevision\", \"LLaVA-Onevision\"),\n         (\"longformer\", \"Longformer\"),\n         (\"longt5\", \"LongT5\"),\n         (\"luke\", \"LUKE\"),"
        },
        {
            "sha": "c83c43518a6a313d61c602b67491c46fef761ddd",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -99,6 +99,7 @@\n             (\"llava\", (\"CLIPImageProcessor\",)),\n             (\"llava_next\", (\"LlavaNextImageProcessor\",)),\n             (\"llava_next_video\", (\"LlavaNextVideoImageProcessor\",)),\n+            (\"llava_onevision\", (\"LlavaOnevisionImageProcessor\",)),\n             (\"mask2former\", (\"Mask2FormerImageProcessor\",)),\n             (\"maskformer\", (\"MaskFormerImageProcessor\",)),\n             (\"mgp-str\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),"
        },
        {
            "sha": "45a9c4d0d078b799c32544027951d4b453db9895",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -314,6 +314,7 @@\n         (\"llava\", \"LlavaForConditionalGeneration\"),\n         (\"llava_next\", \"LlavaNextForConditionalGeneration\"),\n         (\"llava_next_video\", \"LlavaNextVideoForConditionalGeneration\"),\n+        (\"llava_onevision\", \"LlavaOnevisionForConditionalGeneration\"),\n         (\"longformer\", \"LongformerForMaskedLM\"),\n         (\"luke\", \"LukeForMaskedLM\"),\n         (\"lxmert\", \"LxmertForPreTraining\"),\n@@ -729,6 +730,7 @@\n         (\"llava\", \"LlavaForConditionalGeneration\"),\n         (\"llava_next\", \"LlavaNextForConditionalGeneration\"),\n         (\"llava_next_video\", \"LlavaNextVideoForConditionalGeneration\"),\n+        (\"llava_onevision\", \"LlavaOnevisionForConditionalGeneration\"),\n         (\"paligemma\", \"PaliGemmaForConditionalGeneration\"),\n         (\"pix2struct\", \"Pix2StructForConditionalGeneration\"),\n         (\"qwen2_vl\", \"Qwen2VLForConditionalGeneration\"),"
        },
        {
            "sha": "7f49e0e8d99730ff6671a6a2b2987acbd6cc44fd",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -73,6 +73,7 @@\n         (\"llava\", \"LlavaProcessor\"),\n         (\"llava_next\", \"LlavaNextProcessor\"),\n         (\"llava_next_video\", \"LlavaNextVideoProcessor\"),\n+        (\"llava_onevision\", \"LlavaOnevisionProcessor\"),\n         (\"markuplm\", \"MarkupLMProcessor\"),\n         (\"mctct\", \"MCTCTProcessor\"),\n         (\"mgp-str\", \"MgpstrProcessor\"),"
        },
        {
            "sha": "c8eb06db04a0987cb37372eda0327bff9bb5d4c2",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -257,6 +257,7 @@\n                 ),\n             ),\n             (\"llava\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n+            (\"llava-onevision\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n             (\"llava_next\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n             (\"llava_next_video\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n             (\"longformer\", (\"LongformerTokenizer\", \"LongformerTokenizerFast\" if is_tokenizers_available() else None)),"
        },
        {
            "sha": "f16948a8f74017f44a75e3966a47e8a73853cd85",
            "filename": "src/transformers/models/llava_onevision/__init__.py",
            "status": "added",
            "additions": 72,
            "deletions": 0,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2Fllava_onevision%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2Fllava_onevision%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2F__init__.py?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -0,0 +1,72 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available, is_vision_available\n+\n+\n+_import_structure = {\n+    \"configuration_llava_onevision\": [\"LlavaOnevisionConfig\"],\n+    \"processing_llava_onevision\": [\"LlavaOnevisionProcessor\"],\n+}\n+\n+try:\n+    if not is_vision_available():\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    pass\n+else:\n+    _import_structure[\"image_processing_llava_onevision\"] = [\"LlavaOnevisionImageProcessor\"]\n+\n+    _import_structure[\"video_processing_llava_onevision\"] = [\"LlavaOnevisionVideoProcessor\"]\n+\n+try:\n+    if not is_torch_available():\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    pass\n+else:\n+    _import_structure[\"modeling_llava_onevision\"] = [\n+        \"LlavaOnevisionForConditionalGeneration\",\n+        \"LlavaOnevisionPreTrainedModel\",\n+    ]\n+\n+if TYPE_CHECKING:\n+    from .configuration_llava_onevision import LlavaOnevisionConfig\n+    from .processing_llava_onevision import LlavaOnevisionProcessor\n+\n+    try:\n+        if not is_vision_available():\n+            raise OptionalDependencyNotAvailable()\n+    except OptionalDependencyNotAvailable:\n+        pass\n+    else:\n+        from .image_processing_llava_onevision import LlavaOnevisionImageProcessor\n+        from .video_processing_llava_onevision import LlavaOnevisionVideoProcessor\n+\n+    try:\n+        if not is_torch_available():\n+            raise OptionalDependencyNotAvailable()\n+    except OptionalDependencyNotAvailable:\n+        pass\n+    else:\n+        from .modeling_llava_onevision import (\n+            LlavaOnevisionForConditionalGeneration,\n+            LlavaOnevisionPreTrainedModel,\n+        )\n+\n+else:\n+    import sys\n+\n+    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure)"
        },
        {
            "sha": "eef86c6c8c019b3708d5f9e5ffe8cb2ed31c7f6f",
            "filename": "src/transformers/models/llava_onevision/configuration_llava_onevision.py",
            "status": "added",
            "additions": 183,
            "deletions": 0,
            "changes": 183,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconfiguration_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconfiguration_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconfiguration_llava_onevision.py?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -0,0 +1,183 @@\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import (\n+    logging,\n+)\n+from ..auto import CONFIG_MAPPING\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class LlavaOnevisionConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`LlavaOnevisionForConditionalGeneration`]. It is used to instantiate an\n+    Llava-NeXT model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the [llava-hf/llava-onevision-qwen2-7b-ov-hf](https://huggingface.co/llava-hf/llava-onevision-qwen2-7b-ov-hf)\n+    model.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `SiglipVisionConfig`):\n+            The config object or dictionary of the vision backbone.\n+        text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `Qwen2Config`):\n+            The config object or dictionary of the text backbone.\n+        image_token_index (`int`, *optional*, defaults to 151646):\n+            The image token index to encode the image prompt.\n+        video_token_index (`int`, *optional*, defaults to 151647):\n+            The video token index to encode the video prompt.\n+        projector_hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The activation function used by the multimodal projector.\n+        vision_feature_select_strategy (`str`, *optional*, defaults to `\"full\"`):\n+            The feature selection strategy used to select the vision feature from the vision backbone.\n+            Can be one of `\"default\"` or `\"full\"`. If `\"default\"`, the CLS token is removed from the vision features.\n+            If `\"full\"`, the full vision features are used.\n+        vision_feature_layer (`int`, *optional*, defaults to -1):\n+            The index of the layer to select the vision feature.\n+        vision_aspect_ratio (`str`, *optional*, defaults to `\"anyres_max_9\"`):\n+            Aspect ratio used when processong image features. The default value is \"anyres_max_9\".\n+        image_grid_pinpoints (`List`, *optional*):\n+            A list of possible resolutions to use for processing high resolution images. Each item in the list should be a tuple or list\n+            of the form `(height, width)`.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether the model's input and output word embeddings should be tied.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import LlavaOnevisionForConditionalGeneration, LlavaOnevisionConfig, SiglipVisionConfig, Qwen2Config\n+\n+    >>> # Initializing a CLIP-vision config\n+    >>> vision_config = SiglipVisionConfig()\n+\n+    >>> # Initializing a Llama config\n+    >>> text_config = Qwen2Config()\n+\n+    >>> # Initializing a Llava-Next llava-hf/llava-onevision-qwen2-7b-ov-hf style configuration\n+    >>> configuration = LlavaOnevisionConfig(vision_config, text_config)\n+\n+    >>> # Initializing a model from the llava-hf/llava-onevision-qwen2-7b-ov-hf style configuration\n+    >>> model = LlavaOnevisionForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"llava_onevision\"\n+    is_composition = False\n+\n+    def __init__(\n+        self,\n+        vision_config=None,\n+        text_config=None,\n+        image_token_index=151646,\n+        video_token_index=151647,\n+        projector_hidden_act=\"gelu\",\n+        vision_feature_select_strategy=\"full\",\n+        vision_feature_layer=-1,\n+        vision_aspect_ratio=\"anyres_max_9\",\n+        image_grid_pinpoints=None,\n+        tie_word_embeddings=False,\n+        **kwargs,\n+    ):\n+        self.image_token_index = image_token_index\n+        self.video_token_index = video_token_index\n+        self.projector_hidden_act = projector_hidden_act\n+\n+        if vision_feature_select_strategy not in [\"default\", \"full\"]:\n+            raise ValueError(\n+                \"vision_feature_select_strategy should be one of 'default', 'full'.\"\n+                f\"Got: {vision_feature_select_strategy}\"\n+            )\n+\n+        self.vision_feature_select_strategy = vision_feature_select_strategy\n+        self.vision_feature_layer = vision_feature_layer\n+        self.vision_aspect_ratio = vision_aspect_ratio\n+        image_grid_pinpoints = (\n+            image_grid_pinpoints\n+            if image_grid_pinpoints is not None\n+            else [\n+                [384, 384],\n+                [384, 768],\n+                [384, 1152],\n+                [384, 1536],\n+                [384, 1920],\n+                [384, 2304],\n+                [768, 384],\n+                [768, 768],\n+                [768, 1152],\n+                [768, 1536],\n+                [768, 1920],\n+                [768, 2304],\n+                [1152, 384],\n+                [1152, 768],\n+                [1152, 1152],\n+                [1152, 1536],\n+                [1152, 1920],\n+                [1152, 2304],\n+                [1536, 384],\n+                [1536, 768],\n+                [1536, 1152],\n+                [1536, 1536],\n+                [1536, 1920],\n+                [1536, 2304],\n+                [1920, 384],\n+                [1920, 768],\n+                [1920, 1152],\n+                [1920, 1536],\n+                [1920, 1920],\n+                [1920, 2304],\n+                [2304, 384],\n+                [2304, 768],\n+                [2304, 1152],\n+                [2304, 1536],\n+                [2304, 1920],\n+                [2304, 2304],\n+            ]\n+        )\n+        self.image_grid_pinpoints = image_grid_pinpoints\n+\n+        if isinstance(vision_config, dict):\n+            vision_config[\"model_type\"] = (\n+                vision_config[\"model_type\"] if \"model_type\" in vision_config else \"siglip_vision_model\"\n+            )\n+            vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n+        elif vision_config is None:\n+            vision_config = CONFIG_MAPPING[\"siglip_vision_model\"](\n+                hidden_size=1152,\n+                intermediate_size=4304,\n+                patch_size=14,\n+                image_size=384,\n+                num_hidden_layers=26,\n+                num_attention_heads=14,\n+                vision_use_head=False,\n+            )\n+\n+        self.vision_config = vision_config\n+\n+        if isinstance(text_config, dict):\n+            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"qwen2\"\n+            text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n+        elif text_config is None:\n+            text_config = CONFIG_MAPPING[\"qwen2\"]()\n+\n+        self.text_config = text_config\n+\n+        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)"
        },
        {
            "sha": "e8d51f99e67f32643917ba53ebad278b1233d2cf",
            "filename": "src/transformers/models/llava_onevision/convert_llava_onevision_weights_to_hf.py",
            "status": "added",
            "additions": 360,
            "deletions": 0,
            "changes": 360,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconvert_llava_onevision_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconvert_llava_onevision_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconvert_llava_onevision_weights_to_hf.py?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -0,0 +1,360 @@\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"Convert LLaVa-Onevision checkpoints from the original repository.\n+\n+URL: https://github.com/LLaVA-VL/LLaVA-NeXT/tree/main\n+\n+\"\"\"\n+\n+import argparse\n+import gc\n+import glob\n+import json\n+from pathlib import Path\n+\n+import requests\n+import torch\n+from accelerate import init_empty_weights\n+from huggingface_hub import hf_hub_download, snapshot_download\n+from PIL import Image\n+from safetensors import safe_open\n+\n+from transformers import (\n+    AddedToken,\n+    AutoConfig,\n+    AutoTokenizer,\n+    LlavaOnevisionConfig,\n+    LlavaOnevisionForConditionalGeneration,\n+    LlavaOnevisionImageProcessor,\n+    LlavaOnevisionProcessor,\n+    LlavaOnevisionVideoProcessor,\n+    SiglipVisionConfig,\n+)\n+\n+\n+KEYS_TO_MODIFY_MAPPING = {\n+    \"model.vision_tower.\": \"\",\n+    \"model.mm_projector\": \"multi_modal_projector\",\n+    \"model\": \"model.model\",\n+    \"vision_model.model\": \"vision_model\",\n+    \"lm_head\": \"language_model.lm_head\",\n+    \"model.model\": \"language_model.model\",\n+    \"multi_modal_projector.0\": \"multi_modal_projector.linear_1\",\n+    \"multi_modal_projector.2\": \"multi_modal_projector.linear_2\",\n+    \"language_model.model.image_newline\": \"image_newline\",\n+}\n+\n+chat_template = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n'}}{# Render all images first #}{% for content in message['content'] | selectattr('type', 'equalto', 'image') %}{{ '<image>\\n' }}{% endfor %}{# Render all video then #}{% for content in message['content'] | selectattr('type', 'equalto', 'video') %}{{ '<video>\\n' }}{% endfor %}{# Render all text next #}{% if message['role'] != 'assistant' %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{{ content['text'] }}{% endfor %}{% else %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{% generation %}{{ content['text'] }}{% endgeneration %}{% endfor %}{% endif %}{{'<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n+\n+\n+def load_original_state_dict(model_id):\n+    directory_path = snapshot_download(repo_id=model_id, allow_patterns=[\"*.safetensors\"])\n+\n+    original_state_dict = {}\n+    for path in glob.glob(f\"{directory_path}/*\"):\n+        if path.endswith(\".safetensors\"):\n+            with safe_open(path, framework=\"pt\", device=\"cpu\") as f:\n+                for key in f.keys():\n+                    original_state_dict[key] = f.get_tensor(key)\n+\n+    # tied wieghts so lm.head is not saved. Let's clone to load state dict\n+    if \"lm_head.weight\" not in original_state_dict:\n+        original_state_dict[\"lm_head.weight\"] = original_state_dict[\"model.embed_tokens.weight\"].clone()\n+\n+    return original_state_dict\n+\n+\n+def convert_state_dict_to_hf(state_dict):\n+    new_state_dict = {}\n+    for key, value in state_dict.items():\n+        if key.endswith(\".inv_freq\"):\n+            continue\n+        for key_to_modify, new_key in KEYS_TO_MODIFY_MAPPING.items():\n+            if key_to_modify in key:\n+                key = key.replace(key_to_modify, new_key)\n+\n+        new_state_dict[key] = value.to(torch.float16)\n+    return new_state_dict\n+\n+\n+def load_image():\n+    url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n+    image = Image.open(requests.get(url, stream=True).raw)\n+    return image\n+\n+\n+def convert_llava_to_hf(model_id, pytorch_dump_folder_path, push_to_hub=False):\n+    # load original config\n+    filepath = hf_hub_download(repo_id=model_id, filename=\"config.json\", repo_type=\"model\")\n+    # read json\n+    with open(filepath) as f:\n+        data = json.load(f)\n+        print(data)\n+\n+    if model_id in [\"lmms-lab/llava-onevision-qwen2-0.5b-ov\", \"lmms-lab/llava-onevision-qwen2-0.5b-si\"]:\n+        text_model_id = \"Qwen/Qwen2-0.5B-Instruct\"\n+    elif model_id in [\"lmms-lab/llava-onevision-qwen2-7b-ov\", \"lmms-lab/llava-onevision-qwen2-7b-si\"]:\n+        text_model_id = \"Qwen/Qwen2-7B-Instruct\"\n+    elif model_id in [\"lmms-lab/llava-onevision-qwen2-72b-ov\", \"lmms-lab/llava-onevision-qwen2-72b-si\"]:\n+        text_model_id = \"Qwen/Qwen2-72B-Instruct\"\n+\n+    vision_model_id = data[\"mm_vision_tower\"]\n+    torch.set_default_dtype(torch.float16)\n+    text_config = AutoConfig.from_pretrained(text_model_id)\n+\n+    tokenizer = AutoTokenizer.from_pretrained(text_model_id, use_fast=True)\n+    tokenizer.add_tokens(AddedToken(\"<image>\", special=True, normalized=False), special_tokens=True)\n+    tokenizer.add_tokens(AddedToken(\"<video>\", special=True, normalized=False), special_tokens=True)\n+\n+    image_processor = LlavaOnevisionImageProcessor.from_pretrained(vision_model_id)\n+    video_processor = LlavaOnevisionVideoProcessor.from_pretrained(vision_model_id)\n+    processor = LlavaOnevisionProcessor(\n+        tokenizer=tokenizer,\n+        video_processor=video_processor,\n+        image_processor=image_processor,\n+        num_image_tokens=729,\n+        vision_feature_select_strategy=\"full\",\n+        chat_template=chat_template,\n+    )\n+\n+    vision_config = SiglipVisionConfig(\n+        hidden_size=1152,\n+        image_size=384,\n+        intermediate_size=4304,\n+        num_attention_heads=16,\n+        num_hidden_layers=26,  # drop the last layer\n+        patch_size=14,\n+        vision_use_head=False,  # no head\n+    ).to_dict()\n+\n+    config = LlavaOnevisionConfig(\n+        text_config=text_config.to_dict(),\n+        vision_config=vision_config,\n+        use_image_newline_parameter=True,\n+    )\n+\n+    with init_empty_weights():\n+        model = LlavaOnevisionForConditionalGeneration(config)\n+\n+    # load original state dict\n+    state_dict = load_original_state_dict(model_id)\n+    state_dict = convert_state_dict_to_hf(state_dict)\n+    model.load_state_dict(state_dict, assign=True)\n+    model.eval()\n+\n+    pre_expansion_embeddings = model.language_model.model.embed_tokens.weight.data\n+    mu = torch.mean(pre_expansion_embeddings, dim=0).float()\n+    n = pre_expansion_embeddings.size()[0]\n+    sigma = ((pre_expansion_embeddings - mu).T @ (pre_expansion_embeddings - mu)) / n\n+    dist = torch.distributions.multivariate_normal.MultivariateNormal(mu, covariance_matrix=1e-5 * sigma)\n+\n+    # We add an image token so we resize the model\n+    # Pad to 64 for performance reasons\n+    # Qwen-based models have extra unused space in the vocab size already, so no need to resize\n+    pad_shape = 64\n+    vocab_size = config.text_config.vocab_size\n+    num_tokens = vocab_size + 2\n+    model.resize_token_embeddings(num_tokens, pad_to_multiple_of=pad_shape)\n+    model.language_model.model.embed_tokens.weight.data[vocab_size:] = torch.stack(\n+        tuple(\n+            (dist.sample() for _ in range(model.language_model.model.embed_tokens.weight.data[vocab_size:].shape[0]))\n+        ),\n+        dim=0,\n+    )\n+    model.language_model.lm_head.weight.data[vocab_size:] = torch.stack(\n+        tuple((dist.sample() for _ in range(model.language_model.lm_head.weight.data[vocab_size:].shape[0]))),\n+        dim=0,\n+    )\n+\n+    print(f\"Saving model and processor for {model_id} to {pytorch_dump_folder_path}\")\n+    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n+    model.save_pretrained(pytorch_dump_folder_path)\n+    processor.save_pretrained(pytorch_dump_folder_path)\n+\n+    # Make space so we can load the model properly now.\n+    del state_dict\n+    gc.collect()\n+\n+    # Load everything back for inference tests in float32 because prev script was written as that\n+    # Though it's mostly loaded in fp16 as original weights are in fp16\n+    model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n+        pytorch_dump_folder_path, torch_dtype=\"float16\", device_map=\"auto\"\n+    )\n+    processor = LlavaOnevisionProcessor.from_pretrained(pytorch_dump_folder_path)\n+    device = model.device\n+\n+    # prepare inputs\n+    image = load_image()\n+    prompt = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<image>\\nWhat is shown in this image?<|im_end|>\\n<|im_start|>assistant\\n\"\n+    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(torch.float16)\n+\n+    # verify inputs\n+    filepath = hf_hub_download(\n+        repo_id=\"RaushanTurganbay/test-image\", filename=\"llava_onevision_pixel_values.pt\", repo_type=\"dataset\"\n+    )\n+    original_pixel_values = torch.load(filepath, map_location=\"cpu\")\n+    assert torch.allclose(original_pixel_values, inputs.pixel_values.half())\n+\n+    image_sizes = torch.tensor([[899, 1024]])\n+    assert image_sizes[0].tolist() == inputs.image_sizes[0].tolist()\n+\n+    # verify single forward pass\n+    print(\"Single forward pass\")\n+    with torch.inference_mode():\n+        inputs = inputs.to(device)\n+        outputs = model(**inputs)\n+        print(\"Shape of logits:\", outputs.logits.shape)\n+        print(\"First values of logits:\", outputs.logits[0, :3, :3])\n+\n+        if model_id == \"lmms-lab/llava-onevision-qwen2-0.5b-si\":\n+            # Not yet checked against reference\n+            expected_slice = torch.tensor(\n+                [[-12.1953, -14.6797, -12.7891], [0.5840, -0.8467, 1.3799], [3.6055, 4.5430, 9.9062]],\n+                dtype=torch.float32,\n+                device=device,\n+            )\n+        elif model_id == \"lmms-lab/llava-onevision-qwen2-0.5b-ov\":\n+            # Not yet checked against reference\n+            expected_slice = torch.tensor(\n+                [[-12.0234, -14.3828, -12.7500], [2.3594, 1.0000, 3.9336], [3.6582, 4.7148, 9.1172]],\n+                dtype=torch.float32,\n+                device=device,\n+            )\n+        elif model_id == \"lmms-lab/llava-onevision-qwen2-7b-si\":\n+            # Not yet checked against reference\n+            expected_slice = torch.tensor(\n+                [[1.7656, 3.3418, 1.4033], [0.0757, 0.7427, 3.5098], [6.7109, 5.6797, 9.3828]],\n+                dtype=torch.float32,\n+                device=device,\n+            )\n+        elif model_id == \"lmms-lab/llava-onevision-qwen2-7b-ov\":\n+            # Not yet checked against reference\n+            expected_slice = torch.tensor(\n+                [[1.8496, 3.4219, 1.3135], [3.0996, 3.0117, 3.1484], [4.2422, 4.7109, 9.9688]],\n+                dtype=torch.float32,\n+                device=device,\n+            )\n+        elif model_id == \"lmms-lab/llava-onevision-qwen2-72b-si\":\n+            # Not yet checked against reference\n+            expected_slice = torch.tensor(\n+                [[4.1875, 4.4883, 2.7910], [1.2949, 5.1328, 3.1582], [0.9390, 6.4531, 8.4375]],\n+                dtype=torch.float32,\n+                device=device,\n+            )\n+        elif model_id == \"lmms-lab/llava-onevision-qwen2-72b-ov\":\n+            # Not yet checked against reference\n+            expected_slice = torch.tensor(\n+                [[4.2930, 4.7305, 2.7363], [1.7529, 5.0742, 3.9590], [1.3936, 6.3438, 9.3984]],\n+                dtype=torch.float32,\n+                device=device,\n+            )\n+        else:\n+            raise ValueError(f\"Model {model_id} not supported\")\n+\n+        assert torch.allclose(outputs.logits[0, :3, :3], expected_slice, atol=1e-4)\n+        print(\"Logits are ok!\")\n+\n+    # verify generation\n+    output_ids = model.generate(\n+        **inputs,\n+        max_new_tokens=100,\n+        use_cache=True,\n+    )\n+\n+    generated_text = processor.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n+\n+    print(\"Generated text:\", repr(generated_text))\n+\n+    if model_id == \"lmms-lab/llava-onevision-qwen2-0.5b-si\":\n+        expected_text = \"system\\nYou are a helpful assistant.\\nuser\\n\\nWhat is shown in this image?\\nassistant\\nThe image is a radar chart that shows the performance of different algorithms or models in a specific domain, such as image classification or natural language processing. The chart is color-coded to represent different algorithms, with each color corresponding to a specific algorithm. The algorithms are labeled as BLIP-2, InstructBLIP, Owen-VL-Chat, and LLaVA-1.5. The chart also includes a legend at the bottom that explains the color coding and the algorithms represented.\"\n+    elif model_id == \"lmms-lab/llava-onevision-qwen2-0.5b-ov\":\n+        expected_text = \"system\\nYou are a helpful assistant.\\nuser\\n\\nWhat is shown in this image?\\nassistant\\nThe image is a radar chart that compares the performance of different models in a specific task, likely related to natural language processing or machine learning. The chart is divided into different categories, each represented by a different color and labeled with the name of the model or technique used. The models are evaluated based on their performance metrics, such as BLEU-2, InstructBLIP, Qwen-VL-Chat, and LLaVA-1.5. The radar chart helps to visualize the relative\"\n+    elif model_id == \"lmms-lab/llava-onevision-qwen2-7b-si\":\n+        expected_text = \"system\\nYou are a helpful assistant.\\nuser\\n\\nWhat is shown in this image?\\nassistant\\nThis image is a radar chart that compares the performance of different models on various metrics. The models being compared are BLIP-2, InstructBLIP, and Qwen-VL-Chat. The metrics being compared are VQA, QA, GQA, VQA-av2, and VQA-av2. The chart shows that BLIP-2 performs the best on all metrics, followed by InstructBLIP and Qwen-VL-Chat.\"\n+    elif model_id == \"lmms-lab/llava-onevision-qwen2-7b-ov\":\n+        expected_text = \"system\\nYou are a helpful assistant.\\nuser\\n\\nWhat is shown in this image?\\nassistant\\nThe image shows a radar chart, also known as a spider chart or a star chart, which is used to compare multiple quantitative variables. Each axis represents a different variable, and the chart is filled with data points that represent the performance or values of different entities across these variables.\\n\\nIn this particular radar chart, the variables are represented on the axes, and the performance of different models or systems is shown by the lines connecting the data points. The models or systems are labeled along the bottom of the chart,\"\n+    elif model_id == \"lmms-lab/llava-onevision-qwen2-72b-si\":\n+        expected_text = \"system\\nYou are a helpful assistant.\\nuser\\n\\nWhat is shown in this image?\\nassistant\\nThe image shows a radar chart, which is a graphical method of displaying multivariate data in the form of a two-dimensional chart of three or more quantitative variables represented on axes starting from the same point. The chart is used to compare the performance of different models or systems across various benchmarks or metrics.\\n\\nIn this specific radar chart, there are multiple axes, each representing a different benchmark or metric, such as VQA2, GQA, TextVQA, and others. The chart includes several colored lines\"\n+    elif model_id == \"lmms-lab/llava-onevision-qwen2-72b-ov\":\n+        expected_text = \"system\\nYou are a helpful assistant.\\nuser\\n\\nWhat is shown in this image?\\nassistant\\nThe image is a radar chart comparing the performance of different models on various multimodal benchmarks. The models compared are BLIP-2, InstructBLIP, POPE, QWen-VL-Chat, and LLava-1.5. The benchmarks include VQAv2, GQA, TextVQA, SQA-IMG, VizWiz, MM-IMDb, MM-VQA, MM-IMDb-CN, MM-IMDb-EN, MM-\"\n+    else:\n+        raise ValueError(f\"Model {model_id} not supported\")\n+\n+    assert generated_text == expected_text\n+    print(\"Generated text is ok!\")\n+\n+    # verify batched generation\n+    print(\"Batched generation...\")\n+    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    cats_image = Image.open(requests.get(url, stream=True).raw)\n+\n+    inputs = processor(\n+        images=[image, cats_image],\n+        text=[prompt, prompt],\n+        padding=True,\n+        return_tensors=\"pt\",\n+    ).to(device, torch.float16)\n+\n+    for k, v in inputs.items():\n+        print(k, v.shape)\n+\n+    print(\"Image sizes:\", inputs.image_sizes)\n+\n+    # make sure image_sizes are the same\n+    # as otherwise batched generation doesn't work\n+    inputs.image_sizes[1] = inputs.image_sizes[0]\n+\n+    print(\"Batched generation...\")\n+    output_ids = model.generate(\n+        **inputs,\n+        max_new_tokens=20,\n+        use_cache=True,\n+    )\n+\n+    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n+    print(outputs)\n+\n+    if push_to_hub:\n+        checkpoint_name = model_id.split(\"/\")[-1]\n+        print(f\"Pushing to repo llava-hf/{checkpoint_name}-hf\")\n+        model.push_to_hub(f\"llava-hf/{checkpoint_name}-hf\")\n+        processor.push_to_hub(f\"llava-hf/{checkpoint_name}-hf\")\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--model_id\",\n+        help=\"Hub location of the model to convert\",\n+        default=\"lmms-lab/llava-onevision-qwen2-0.5b-ov\",\n+        choices=[\n+            \"lmms-lab/llava-onevision-qwen2-0.5b-ov\",\n+            \"lmms-lab/llava-onevision-qwen2-0.5b-si\",\n+            \"lmms-lab/llava-onevision-qwen2-7b-si\",\n+            \"lmms-lab/llava-onevision-qwen2-7b-ov\",\n+            \"lmms-lab/llava-onevision-qwen2-72b-si\",\n+            \"lmms-lab/llava-onevision-qwen2-72b-ov\",\n+        ],\n+        required=False,\n+    )\n+    parser.add_argument(\n+        \"--pytorch_dump_folder_path\", type=str, required=True, help=\"Path to the output PyTorch model directory.\"\n+    )\n+    parser.add_argument(\n+        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the 🤗 hub.\"\n+    )\n+    args = parser.parse_args()\n+\n+    convert_llava_to_hf(args.model_id, args.pytorch_dump_folder_path, args.push_to_hub)"
        },
        {
            "sha": "3dddcdd148a416c45021c67bbe0e3d578b5df51e",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision.py",
            "status": "added",
            "additions": 711,
            "deletions": 0,
            "changes": 711,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -0,0 +1,711 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Image processor class for LLaVa-Onevision.\"\"\"\n+\n+import math\n+from typing import Dict, Iterable, List, Optional, Tuple, Union\n+\n+import numpy as np\n+\n+from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict, select_best_resolution\n+from ...image_transforms import (\n+    PaddingMode,\n+    convert_to_rgb,\n+    pad,\n+    resize,\n+    to_channel_dimension_format,\n+)\n+from ...image_utils import (\n+    OPENAI_CLIP_MEAN,\n+    OPENAI_CLIP_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    get_image_size,\n+    infer_channel_dimension_format,\n+    is_scaled_image,\n+    is_valid_image,\n+    to_numpy_array,\n+    valid_images,\n+    validate_preprocess_arguments,\n+)\n+from ...utils import TensorType, is_vision_available, logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+\n+# Copied from transformers.models.llava_next.image_processing_llava_next.make_batched_images\n+def make_batched_images(images) -> List[List[ImageInput]]:\n+    \"\"\"\n+    Accepts images in list or nested list format, and makes a list of images for preprocessing.\n+\n+    Args:\n+        images (`Union[List[List[ImageInput]], List[ImageInput], ImageInput]`):\n+            The input image.\n+\n+    Returns:\n+        list: A list of images.\n+    \"\"\"\n+    if isinstance(images, (list, tuple)) and isinstance(images[0], (list, tuple)) and is_valid_image(images[0][0]):\n+        return [img for img_list in images for img in img_list]\n+\n+    elif isinstance(images, (list, tuple)) and is_valid_image(images[0]):\n+        return images\n+\n+    elif is_valid_image(images):\n+        return [images]\n+\n+    raise ValueError(f\"Could not make batched video from {images}\")\n+\n+\n+# Copied from transformers.models.llava_next.image_processing_llava_next.divide_to_patches\n+def divide_to_patches(image: np.array, patch_size: int, input_data_format) -> List[np.array]:\n+    \"\"\"\n+    Divides an image into patches of a specified size.\n+\n+    Args:\n+        image (`np.array`):\n+            The input image.\n+        patch_size (`int`):\n+            The size of each patch.\n+        input_data_format (`ChannelDimension` or `str`):\n+            The channel dimension format of the input image.\n+\n+    Returns:\n+        list: A list of np.array representing the patches.\n+    \"\"\"\n+    patches = []\n+    height, width = get_image_size(image, channel_dim=input_data_format)\n+    for i in range(0, height, patch_size):\n+        for j in range(0, width, patch_size):\n+            if input_data_format == ChannelDimension.LAST:\n+                patch = image[i : i + patch_size, j : j + patch_size]\n+            else:\n+                patch = image[:, i : i + patch_size, j : j + patch_size]\n+            patches.append(patch)\n+\n+    return patches\n+\n+\n+# Copied from transformers.models.llava_next.image_processing_llava_next.expand_to_square\n+def expand_to_square(image: np.array, background_color, input_data_format) -> np.array:\n+    \"\"\"\n+    Expands an image to a square by adding a background color.\n+    \"\"\"\n+\n+    height, width = get_image_size(image, channel_dim=input_data_format)\n+    if width == height:\n+        return image\n+    elif width > height:\n+        result = np.ones((width, width, image.shape[2]), dtype=image.dtype) * background_color\n+        result[(width - height) // 2 : (width - height) // 2 + height, :] = image\n+        return result\n+    else:\n+        result = np.ones((height, height, image.shape[2]), dtype=image.dtype) * background_color\n+        result[:, (height - width) // 2 : (height - width) // 2 + width] = image\n+        return result\n+\n+\n+# Copied from transformers.models.llava_next.image_processing_llava_next._get_patch_output_size\n+def _get_patch_output_size(image, target_resolution, input_data_format):\n+    original_height, original_width = get_image_size(image, channel_dim=input_data_format)\n+    target_height, target_width = target_resolution\n+\n+    scale_w = target_width / original_width\n+    scale_h = target_height / original_height\n+\n+    if scale_w < scale_h:\n+        new_width = target_width\n+        new_height = min(math.ceil(original_height * scale_w), target_height)\n+    else:\n+        new_height = target_height\n+        new_width = min(math.ceil(original_width * scale_h), target_width)\n+\n+    return new_height, new_width\n+\n+\n+class LlavaOnevisionImageProcessor(BaseImageProcessor):\n+    r\"\"\"\n+    Constructs a LLaVa-Onevisino-Video video processor. Based on [`SiglipImageProcessor`] with incorporation of processing each video frame.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by\n+            `do_resize` in the `preprocess` method.\n+        size (`Dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 224}`):\n+            Size of the image after resizing. The shortest edge of the image is resized to size[\"shortest_edge\"], with\n+            the longest edge resized to keep the input aspect ratio. Can be overridden by `size` in the `preprocess`\n+            method.\n+        image_grid_pinpoints (`List` *optional*, defaults to `[[672, 336], [336, 672], [672, 672], [336, 1008], [1008, 336]]`):\n+            A list of possible resolutions to use for processing high resolution images. The best resolution is selected\n+            based on the original size of the image. Can be overridden by `image_grid_pinpoints` in the `preprocess`\n+            method. Not used for processinf videos.\n+        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n+            Resampling filter to use if resizing the image. Can be overridden by `resample` in the `preprocess` method.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n+            the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in the `preprocess`\n+            method.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the image. Can be overridden by `do_normalize` in the `preprocess` method.\n+        image_mean (`float` or `List[float]`, *optional*, defaults to `[0.48145466, 0.4578275, 0.40821073]`):\n+            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n+            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n+        image_std (`float` or `List[float]`, *optional*, defaults to `[0.26862954, 0.26130258, 0.27577711]`):\n+            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n+            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n+            Can be overridden by the `image_std` parameter in the `preprocess` method.\n+        do_pad (`bool`, *optional*, defaults to `True`):\n+                Whether to pad the image. If `True`, will pad the patch dimension of the images in the batch to the largest\n+                number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n+        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n+            Whether to convert the image to RGB.\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values_videos\"]\n+\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        size: Dict[str, int] = None,\n+        image_grid_pinpoints: List = None,\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n+        do_normalize: bool = True,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_pad: Optional[bool] = True,\n+        do_convert_rgb: bool = True,\n+        **kwargs,\n+    ) -> None:\n+        super().__init__(**kwargs)\n+        size = size if size is not None else {\"height\": 384, \"width\": 384}\n+        size = get_size_dict(size, default_to_square=False)\n+        image_grid_pinpoints = (\n+            image_grid_pinpoints\n+            if image_grid_pinpoints is not None\n+            else [\n+                [384, 384],\n+                [384, 768],\n+                [384, 1152],\n+                [384, 1536],\n+                [384, 1920],\n+                [384, 2304],\n+                [768, 384],\n+                [768, 768],\n+                [768, 1152],\n+                [768, 1536],\n+                [768, 1920],\n+                [768, 2304],\n+                [1152, 384],\n+                [1152, 768],\n+                [1152, 1152],\n+                [1152, 1536],\n+                [1152, 1920],\n+                [1152, 2304],\n+                [1536, 384],\n+                [1536, 768],\n+                [1536, 1152],\n+                [1536, 1536],\n+                [1536, 1920],\n+                [1536, 2304],\n+                [1920, 384],\n+                [1920, 768],\n+                [1920, 1152],\n+                [1920, 1536],\n+                [1920, 1920],\n+                [1920, 2304],\n+                [2304, 384],\n+                [2304, 768],\n+                [2304, 1152],\n+                [2304, 1536],\n+                [2304, 1920],\n+                [2304, 2304],\n+            ]\n+        )\n+\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.image_grid_pinpoints = image_grid_pinpoints\n+        self.resample = resample\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean if image_mean is not None else OPENAI_CLIP_MEAN\n+        self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n+        self.do_pad = do_pad\n+        self.do_convert_rgb = do_convert_rgb\n+\n+    # Copied from transformers.models.llava_next.image_processing_llava_next.LlavaNextImageProcessor.pad\n+    def pad(\n+        self,\n+        image: np.ndarray,\n+        padding: Union[int, Tuple[int, int], Iterable[Tuple[int, int]]],\n+        mode: PaddingMode = PaddingMode.CONSTANT,\n+        constant_values: Union[float, Iterable[float]] = 0.0,\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> np.ndarray:\n+        \"\"\"\n+        Pads the `image` with the specified `padding` and `mode`. Padding can be in the (`height`, `width`)\n+        dimension of in the (`num_patches`) dimension. In the second case an iterable if tuples is expected\n+        as input.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                The image to pad.\n+            padding (`int` or `Tuple[int, int]` or `Iterable[Tuple[int, int]]`):\n+                Padding to apply to the edges of the height, width axes. Can be one of three formats:\n+                - `((before_height, after_height), (before_width, after_width))` unique pad widths for each axis.\n+                - `((before, after),)` yields same before and after pad for height and width.\n+                - `(pad,)` or int is a shortcut for before = after = pad width for all axes.\n+            mode (`PaddingMode`):\n+                The padding mode to use. Can be one of:\n+                    - `\"constant\"`: pads with a constant value.\n+                    - `\"reflect\"`: pads with the reflection of the vector mirrored on the first and last values of the\n+                    vector along each axis.\n+                    - `\"replicate\"`: pads with the replication of the last value on the edge of the array along each axis.\n+                    - `\"symmetric\"`: pads with the reflection of the vector mirrored along the edge of the array.\n+            constant_values (`float` or `Iterable[float]`, *optional*):\n+                The value to use for the padding if `mode` is `\"constant\"`.\n+            data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the output image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                If unset, will use same as the input image.\n+            input_data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the input image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                If unset, will use the inferred format of the input image.\n+\n+        Returns:\n+            `np.ndarray`: The padded image.\n+\n+        \"\"\"\n+\n+        # call the general `pad` if padding on `height/width`, otherwise it's the `num_patched` dim\n+        if isinstance(padding, int) or len(padding) != 4:\n+            return pad(image, padding, mode, constant_values, data_format, input_data_format)\n+\n+        if input_data_format is None:\n+            input_data_format = infer_channel_dimension_format(image)\n+        if mode == PaddingMode.CONSTANT:\n+            image = np.pad(image, padding, mode=\"constant\", constant_values=constant_values)\n+        elif mode == PaddingMode.REFLECT:\n+            image = np.pad(image, padding, mode=\"reflect\")\n+        elif mode == PaddingMode.REPLICATE:\n+            image = np.pad(image, padding, mode=\"edge\")\n+        elif mode == PaddingMode.SYMMETRIC:\n+            image = np.pad(image, padding, mode=\"symmetric\")\n+        else:\n+            raise ValueError(f\"Invalid padding mode: {mode}\")\n+        image = (\n+            to_channel_dimension_format(image, data_format, input_data_format) if data_format is not None else image\n+        )\n+        return image\n+\n+    # Copied from transformers.models.llava_next.image_processing_llava_next.LlavaNextImageProcessor._resize_for_patching\n+    def _resize_for_patching(\n+        self, image: np.array, target_resolution: tuple, resample, input_data_format: ChannelDimension\n+    ) -> np.array:\n+        \"\"\"\n+        Resizes an image to a target resolution while maintaining aspect ratio.\n+\n+        Args:\n+            image (np.array):\n+                The input image.\n+            target_resolution (tuple):\n+                The target resolution (height, width) of the image.\n+            resample (`PILImageResampling`):\n+                Resampling filter to use if resizing the image.\n+            input_data_format (`ChannelDimension` or `str`):\n+                The channel dimension format of the input image.\n+\n+        Returns:\n+            np.array: The resized and padded image.\n+        \"\"\"\n+        new_height, new_width = _get_patch_output_size(image, target_resolution, input_data_format)\n+\n+        # Resize the image\n+        resized_image = resize(image, (new_height, new_width), resample=resample, input_data_format=input_data_format)\n+\n+        return resized_image\n+\n+    # Copied from transformers.models.llava_next.image_processing_llava_next.LlavaNextImageProcessor._pad_for_patching\n+    def _pad_for_patching(\n+        self, image: np.array, target_resolution: tuple, input_data_format: ChannelDimension\n+    ) -> np.array:\n+        \"\"\"\n+        Pad an image to a target resolution while maintaining aspect ratio.\n+        \"\"\"\n+        target_height, target_width = target_resolution\n+        new_height, new_width = _get_patch_output_size(image, target_resolution, input_data_format)\n+\n+        paste_x = (target_width - new_width) // 2\n+        paste_y = (target_height - new_height) // 2\n+\n+        padded_image = self.pad(image, padding=((paste_y, paste_y), (paste_x, paste_x)))\n+\n+        return padded_image\n+\n+    # Copied from transformers.models.llava_next.image_processing_llava_next.LlavaNextImageProcessor.get_image_patches\n+    def get_image_patches(\n+        self,\n+        image: np.array,\n+        grid_pinpoints,\n+        size: tuple,\n+        patch_size: int,\n+        resample: PILImageResampling,\n+        data_format: ChannelDimension,\n+        input_data_format: ChannelDimension,\n+    ) -> List[np.array]:\n+        \"\"\"\n+        Process an image with variable resolutions by dividing it into patches.\n+\n+        Args:\n+            image (np.array):\n+                The input image to be processed.\n+            grid_pinpoints (List):\n+                A string representation of a list of possible resolutions.\n+            size (`tuple`):\n+                Size to resize the original image to.\n+            patch_size (`int`):\n+                Size of the patches to divide the image into.\n+            resample (`PILImageResampling`):\n+                Resampling filter to use if resizing the image.\n+            data_format (`ChannelDimension` or `str`):\n+                The channel dimension format for the output image.\n+            input_data_format (`ChannelDimension` or `str`):\n+                The channel dimension format of the input image.\n+\n+        Returns:\n+            List[np.array]: A list of NumPy arrays containing the processed image patches.\n+        \"\"\"\n+        if not isinstance(grid_pinpoints, list):\n+            raise TypeError(\"grid_pinpoints must be a list of possible resolutions.\")\n+\n+        possible_resolutions = grid_pinpoints\n+\n+        image_size = get_image_size(image, channel_dim=input_data_format)\n+        best_resolution = select_best_resolution(image_size, possible_resolutions)\n+        resized_image = self._resize_for_patching(\n+            image, best_resolution, resample=resample, input_data_format=input_data_format\n+        )\n+        padded_image = self._pad_for_patching(resized_image, best_resolution, input_data_format=input_data_format)\n+\n+        patches = divide_to_patches(padded_image, patch_size=patch_size, input_data_format=input_data_format)\n+\n+        # make sure that all patches are in the input data format\n+        patches = [\n+            to_channel_dimension_format(patch, channel_dim=data_format, input_channel_dim=input_data_format)\n+            for patch in patches\n+        ]\n+\n+        resized_original_image = resize(\n+            image,\n+            size=size,\n+            resample=resample,\n+            data_format=data_format,\n+            input_data_format=input_data_format,\n+        )\n+\n+        image_patches = [resized_original_image] + patches\n+\n+        return image_patches\n+\n+    # Copied from transformers.models.llava_next.image_processing_llava_next.LlavaNextImageProcessor._pad_for_batching\n+    def _pad_for_batching(\n+        self,\n+        pixel_values: List[np.ndarray],\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ):\n+        \"\"\"\n+        Pads images on the `num_of_patches` dimension with zeros to form a batch of same number of patches.\n+\n+        Args:\n+            pixel_values (`List[np.ndarray]`):\n+                An array of pixel values of each images of shape (`batch_size`, `num_patches`, `image_in_3D`)\n+            data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the output image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                If unset, will use same as the input image.\n+            input_data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the input image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                If unset, will use the inferred format of the input image.\n+\n+        Returns:\n+            List[`np.ndarray`]: The padded images.\n+        \"\"\"\n+        max_patch = max(len(x) for x in pixel_values)\n+        pixel_values = [\n+            self.pad(\n+                image,\n+                padding=((0, max_patch - image.shape[0]), (0, 0), (0, 0), (0, 0)),\n+                data_format=data_format,\n+                input_data_format=input_data_format,\n+            )\n+            for image in pixel_values\n+        ]\n+\n+        return pixel_values\n+\n+    def _preprocess(\n+        self,\n+        images: ImageInput,\n+        do_resize: bool = None,\n+        size: Dict[str, int] = None,\n+        resample: PILImageResampling = None,\n+        do_rescale: bool = None,\n+        rescale_factor: float = None,\n+        do_normalize: bool = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_convert_rgb: bool = None,\n+        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> Image.Image:\n+        \"\"\"\n+        Args:\n+            images (`ImageInput`):\n+                Batch of frames (one video) to preprocess. Expects a batch of frames with pixel values ranging from 0 to 255. If\n+                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+                Size of the image after resizing. Shortest edge of the image is resized to size[\"shortest_edge\"], with\n+                the longest edge resized to keep the input aspect ratio.\n+            resample (`int`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n+                has an effect if `do_resize` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image.\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n+            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n+                `True`.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        \"\"\"\n+        if do_resize:\n+            images = [\n+                resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n+                for image in images\n+            ]\n+\n+        if do_rescale:\n+            images = [\n+                self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+                for image in images\n+            ]\n+\n+        if do_normalize:\n+            images = [\n+                self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n+                for image in images\n+            ]\n+\n+        images = [\n+            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n+        ]\n+\n+        return images\n+\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        do_resize: bool = None,\n+        size: Dict[str, int] = None,\n+        image_grid_pinpoints: List = None,\n+        resample: PILImageResampling = None,\n+        do_rescale: bool = None,\n+        rescale_factor: float = None,\n+        do_normalize: bool = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_pad: Optional[bool] = None,\n+        do_convert_rgb: bool = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ):\n+        \"\"\"\n+        Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+                Size of the image after resizing. Shortest edge of the image is resized to size[\"shortest_edge\"], with\n+                the longest edge resized to keep the input aspect ratio.\n+            image_grid_pinpoints (`List` *optional*, defaults to `self.image_grid_pinpoints`):\n+                A list of possible resolutions to use for processing high resolution images. The best resolution is\n+                selected based on the original size of the image.\n+            resample (`int`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n+                has an effect if `do_resize` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image.\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n+            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n+                `True`.\n+            do_pad (`bool`, *optional*, defaults to `self.do_pad`):\n+                Whether to pad the image. If `True`, will pad the patch dimension of the images in the batch to the largest\n+                number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n+            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n+                Whether to convert the image to RGB.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                - Unset: Return a list of `np.ndarray`.\n+                - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n+                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+                - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+\n+        \"\"\"\n+        do_resize = do_resize if do_resize is not None else self.do_resize\n+        size = size if size is not None else self.size\n+        image_grid_pinpoints = image_grid_pinpoints if image_grid_pinpoints is not None else self.image_grid_pinpoints\n+        resample = resample if resample is not None else self.resample\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+        do_pad = do_pad if do_pad is not None else self.do_pad\n+        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+\n+        images = make_batched_images(images)\n+\n+        if not valid_images(images):\n+            raise ValueError(\n+                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+\n+        validate_preprocess_arguments(\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+        )\n+\n+        if do_convert_rgb:\n+            images = [convert_to_rgb(image) for image in images]\n+\n+        # All transformations expect numpy arrays.\n+        images = [to_numpy_array(image) for image in images]\n+\n+        if is_scaled_image(images[0]) and do_rescale:\n+            logger.warning_once(\n+                \"It looks like you are trying to rescale already rescaled images. If the input\"\n+                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+            )\n+\n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(images[0])\n+\n+        new_images = []\n+        image_sizes = [get_image_size(image, channel_dim=input_data_format) for image in images]\n+        for image in images:\n+            # convert image into a list of patches\n+            # we intentially use the same data format as the input data format\n+            size_tuple = (\n+                (size[\"height\"], size[\"width\"])\n+                if \"height\" in size and \"width\" in size\n+                else (size[\"shortest_edge\"], size[\"shortest_edge\"])\n+            )\n+            image_patches = self.get_image_patches(\n+                image,\n+                image_grid_pinpoints,\n+                size=size_tuple,\n+                patch_size=size[\"height\"],\n+                resample=resample,\n+                data_format=input_data_format,\n+                input_data_format=input_data_format,\n+            )\n+\n+            # preprocess patches\n+            pixel_values = self._preprocess(\n+                image_patches,\n+                do_resize=do_resize,\n+                size=size_tuple,\n+                resample=resample,\n+                do_rescale=do_rescale,\n+                rescale_factor=rescale_factor,\n+                do_normalize=do_normalize,\n+                image_mean=image_mean,\n+                image_std=image_std,\n+                data_format=data_format,\n+                input_data_format=input_data_format,\n+            )\n+            pixel_values = np.array(pixel_values)\n+            new_images.append(pixel_values)\n+\n+        if do_pad:\n+            processed_images = self._pad_for_batching(new_images)\n+\n+        return BatchFeature(\n+            data={\"pixel_values\": processed_images, \"image_sizes\": image_sizes}, tensor_type=return_tensors\n+        )"
        },
        {
            "sha": "9496c385702749bb7981a20073b497014458b6c7",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "added",
            "additions": 727,
            "deletions": 0,
            "changes": 727,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -0,0 +1,727 @@\n+# coding=utf-8\n+# Copyright 2024 the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch Llava-Onevision model.\"\"\"\n+\n+import math\n+from dataclasses import dataclass\n+from typing import List, Optional, Tuple, Union\n+\n+import numpy as np\n+import torch\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from ... import PreTrainedModel\n+from ...activations import ACT2FN\n+from ...image_processing_utils import select_best_resolution\n+from ...modeling_outputs import ModelOutput\n+from ...utils import (\n+    add_start_docstrings,\n+    logging,\n+)\n+from ..auto import AutoModel, AutoModelForCausalLM\n+from .configuration_llava_onevision import LlavaOnevisionConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+_CONFIG_FOR_DOC = \"LlavaNextConfig\"\n+\n+\n+# Copied from transformers.models.llava_next.modeling_llava_next.get_anyres_image_grid_shape\n+def get_anyres_image_grid_shape(image_size, grid_pinpoints, patch_size):\n+    \"\"\"\n+    Calculate the shape of the image patch grid after the preprocessing for images of any resolution.\n+\n+    Args:\n+        image_size (`tuple`):\n+            The size of the input image in the format (width, height).\n+        grid_pinpoints (`List`):\n+            A list containing possible resolutions. Each item in the list should be a tuple or list\n+            of the form `(height, width)`.\n+        patch_size (`int`):\n+            The size of each image patch.\n+\n+    Returns:\n+        tuple: The shape of the image patch grid in the format (width, height).\n+    \"\"\"\n+    if not isinstance(grid_pinpoints, list):\n+        raise TypeError(\"grid_pinpoints should be a list of tuples or lists\")\n+\n+    # ! VERY IMPORTANT if image_size is tensor, must convert to into tuple, otherwise it will cause wrong calculate\n+    if not isinstance(image_size, (list, tuple)):\n+        if not isinstance(image_size, (torch.Tensor, np.ndarray)):\n+            raise TypeError(\n+                f\"image_size invalid type: {type(image_size)} not valid, should be either list, tuple, np.ndarray or tensor\"\n+            )\n+        image_size = image_size.tolist()\n+\n+    height, width = select_best_resolution(image_size, grid_pinpoints)\n+    return height // patch_size, width // patch_size\n+\n+\n+# Copied from transformers.models.llava_next.modeling_llava_next.image_size_to_num_patches\n+def image_size_to_num_patches(image_size, grid_pinpoints, patch_size: int):\n+    \"\"\"\n+    Calculate the number of patches after the preprocessing for images of any resolution.\n+\n+    Args:\n+        image_size (`torch.LongTensor` or `np.ndarray` or `Tuple[int, int]`):\n+            The size of the input image in the format (height, width). ?\n+        grid_pinpoints (`List`):\n+            A list containing possible resolutions. Each item in the list should be a tuple or list\n+            of the form `(height, width)`.\n+        patch_size (`int`):\n+            The size of each image patch.\n+\n+    Returns:\n+        int: the number of patches\n+    \"\"\"\n+    if not isinstance(grid_pinpoints, list):\n+        raise TypeError(\"grid_pinpoints should be a list of tuples or lists\")\n+\n+    # ! VERY IMPORTANT if image_size is tensor, must convert to into tuple, otherwise it will cause wrong calculate\n+    if not isinstance(image_size, (list, tuple)):\n+        if not isinstance(image_size, (torch.Tensor, np.ndarray)):\n+            raise TypeError(f\"image_size invalid type {type(image_size)} with value {image_size}\")\n+        image_size = image_size.tolist()\n+\n+    best_resolution = select_best_resolution(image_size, grid_pinpoints)\n+    height, width = best_resolution\n+    num_patches = 0\n+    # consider change to ceil(height/patch_size)*ceil(width/patch_size) + 1\n+    for i in range(0, height, patch_size):\n+        for j in range(0, width, patch_size):\n+            num_patches += 1\n+    # add the base patch\n+    num_patches += 1\n+    return num_patches\n+\n+\n+# Copied from transformers.models.llava_next.modeling_llava_next.unpad_image\n+def unpad_image(tensor, original_size):\n+    \"\"\"\n+    Unpads a PyTorch tensor of a padded and resized image.\n+\n+    Args:\n+        tensor (`torch.Tensor`):\n+            The image tensor, assumed to be of shape (num_channels, height, width).\n+        original_size (`tuple`):\n+            The original size of the image (height, width).\n+\n+    Returns:\n+        `torch.Tensor`: The unpadded image tensor.\n+    \"\"\"\n+    original_height, original_width = original_size\n+    current_height, current_width = tensor.shape[1:]\n+\n+    original_aspect_ratio = original_width / original_height\n+    current_aspect_ratio = current_width / current_height\n+\n+    if original_aspect_ratio > current_aspect_ratio:\n+        scale_factor = current_width / original_width\n+        new_height = int(original_height * scale_factor)\n+        padding = (current_height - new_height) // 2\n+        unpadded_tensor = tensor[:, padding : current_height - padding, :]\n+    else:\n+        scale_factor = current_height / original_height\n+        new_width = int(original_width * scale_factor)\n+        padding = (current_width - new_width) // 2\n+        unpadded_tensor = tensor[:, :, padding : current_width - padding]\n+\n+    return unpadded_tensor\n+\n+\n+@dataclass\n+# Copied from transformers.models.idefics.modeling_idefics.IdeficsCausalLMOutputWithPast with Idefics->LlavaOnevision\n+class LlavaOnevisionCausalLMOutputWithPast(ModelOutput):\n+    \"\"\"\n+    Base class for LlavaOnevision causal language model (or autoregressive) outputs.\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+            Language modeling loss (for next-token prediction).\n+        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+            Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n+            sequence_length, hidden_size)`.\n+\n+            image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: torch.FloatTensor = None\n+    past_key_values: Optional[List[torch.FloatTensor]] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+\n+\n+# Copied from transformers.models.llava.modeling_llava.LlavaMultiModalProjector with Llava->LlavaOnevision\n+class LlavaOnevisionMultiModalProjector(nn.Module):\n+    def __init__(self, config: LlavaOnevisionConfig):\n+        super().__init__()\n+\n+        self.linear_1 = nn.Linear(config.vision_config.hidden_size, config.text_config.hidden_size, bias=True)\n+        self.act = ACT2FN[config.projector_hidden_act]\n+        self.linear_2 = nn.Linear(config.text_config.hidden_size, config.text_config.hidden_size, bias=True)\n+\n+    def forward(self, image_features):\n+        hidden_states = self.linear_1(image_features)\n+        hidden_states = self.act(hidden_states)\n+        hidden_states = self.linear_2(hidden_states)\n+        return hidden_states\n+\n+\n+LLAVA_ONEVISION_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`LlavaNextConfig`] or [`LlavaNextVisionConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare LLaVA-Onevision Model outputting raw hidden-states without any specific head on top.\",\n+    LLAVA_ONEVISION_START_DOCSTRING,\n+)\n+class LlavaOnevisionPreTrainedModel(PreTrainedModel):\n+    config_class = LlavaOnevisionConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"LlavaOnevisionVisionAttention\"]\n+    _skip_keys_device_placement = \"past_key_values\"\n+    _supports_flash_attn_2 = True\n+    _supports_cache_class = True\n+    _supports_static_cache = False  # Qwen2 doesn't but llava has no reasons to not support\n+    _supports_quantized_cache = True\n+    _supports_sdpa = True\n+\n+    # Copied from transformers.models.llava_next.modeling_llava_next.LlavaNextPreTrainedModel._init_weights\n+    def _init_weights(self, module):\n+        # important: this ported version of LlavaNext isn't meant for training from scratch - only\n+        # inference and fine-tuning - so the proper init weights code has been removed - the original codebase\n+        # https://github.com/haotian-liu/LLaVA/tree/main/llava_next should serve for that purpose\n+        std = (\n+            self.config.initializer_range\n+            if hasattr(self.config, \"initializer_range\")\n+            else self.config.text_config.initializer_range\n+        )\n+\n+        if hasattr(module, \"class_embedding\"):\n+            module.class_embedding.data.normal_(mean=0.0, std=std)\n+\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+\n+LLAVA_ONEVISION_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n+            The tensors corresponding to the input images. Pixel values can be obtained using\n+            [`AutoImageProcessor`]. See [`LlavaNextImageProcessor.__call__`] for details. [`LlavaProcessor`] uses\n+            [`LlavaNextImageProcessor`] for processing images.\n+        image_sizes (`torch.LongTensor` of shape `(batch_size, 2)`, *optional*):\n+            The sizes of the images in the batch, being (height, width) for each image.\n+        pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, frames, num_channels, image_size, image_size)):\n+            The tensors corresponding to the input videos. Pixel values can be obtained using\n+            [`LlavaNextVideoProcessor`]. See [`LlavaNextVideoProcessor.__call__`] for details. [`LlavaProcessor`] uses\n+            [`LlavaNextVideoProcessor`] for processing videos.\n+        image_sizes_videos (`torch.LongTensor` of shape `(batch_size, frames, 2)`, *optional*):\n+            The sizes of the videos in the batch, being (height, width) for each frame in the video.\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n+            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n+            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n+            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        vision_feature_layer (`int`, *optional*, defaults to -2):\n+            The index of the layer to select the vision feature.\n+        vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n+            The feature selection strategy used to select the vision feature from the vision backbone.\n+            Can be one of `\"default\"` or `\"full\"`. If `\"default\"`, the CLS token is removed from the vision features.\n+            If `\"full\"`, the full vision features are used.\n+        vision_aspect_ratio (`str`, *optional*, defaults to `\"anyres_max_9\"`):\n+            Aspect ratio used when processong image features. The default value is \"anyres_max_9\".\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The LLaVA-Onevision model which consists of a vision backbone and a language model.\"\"\",\n+    LLAVA_ONEVISION_START_DOCSTRING,\n+)\n+class LlavaOnevisionForConditionalGeneration(LlavaOnevisionPreTrainedModel):\n+    def __init__(self, config: LlavaOnevisionConfig):\n+        super().__init__(config)\n+        self.vision_tower = AutoModel.from_config(\n+            config.vision_config, attn_implementation=config._attn_implementation\n+        )\n+\n+        self.multi_modal_projector = LlavaOnevisionMultiModalProjector(config)\n+        embed_std = 1 / math.sqrt(config.text_config.hidden_size)\n+        self.image_newline = nn.Parameter(torch.randn(config.text_config.hidden_size, dtype=self.dtype) * embed_std)\n+\n+        self.vocab_size = config.text_config.vocab_size\n+        self.language_model = AutoModelForCausalLM.from_config(\n+            config.text_config, attn_implementation=config._attn_implementation\n+        )\n+        self.post_init()\n+\n+    # Copied from transformers.models.llava_next.modeling_llava_next.LlavaNextForConditionalGeneration.get_input_embeddings\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    # Copied from transformers.models.llava_next.modeling_llava_next.LlavaNextForConditionalGeneration.set_input_embeddings\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    # Copied from transformers.models.llava_next.modeling_llava_next.LlavaNextForConditionalGeneration.get_output_embeddings\n+    def get_output_embeddings(self):\n+        return self.language_model.get_output_embeddings()\n+\n+    # Copied from transformers.models.llava_next.modeling_llava_next.LlavaNextForConditionalGeneration.set_output_embeddings\n+    def set_output_embeddings(self, new_embeddings):\n+        self.language_model.set_output_embeddings(new_embeddings)\n+\n+    # Copied from transformers.models.llava_next.modeling_llava_next.LlavaNextForConditionalGeneration.set_decoder\n+    def set_decoder(self, decoder):\n+        self.language_model.set_decoder(decoder)\n+\n+    # Copied from transformers.models.llava_next.modeling_llava_next.LlavaNextForConditionalGeneration.get_decoder\n+    def get_decoder(self):\n+        return self.language_model.get_decoder()\n+\n+    # Copied from transformers.models.llava_next.modeling_llava_next.LlavaNextForConditionalGeneration.tie_weights\n+    def tie_weights(self):\n+        return self.language_model.tie_weights()\n+\n+    def pack_image_features(self, image_features, image_sizes, image_newline=None, vision_aspect_ratio=\"anyres_max_9\"):\n+        \"\"\"\n+        Reshape, unpad and then pack each image_feature into a single image_features tensor containing all visual vectors.\n+\n+        Args:\n+            image_features (`List[torch.Tensor]` of length num_images, each of shape `(num_patches, image_length, embed_dim)`)\n+                List of image feature tensor, each contains all the visual feature of all patches.\n+            image_sizes (`torch.Tensor` of shape `(num_images, 2)`)\n+                Actual image size of each images (H, W).\n+            image_newline (`torch.Tensor` of shape `(embed_dim)`)\n+                New line embedding vector.\n+            vision_aspect_ratio (`str`, *optional*, \"anyres_max_9\"):\n+                Aspect ratio used when processong image features. The default value is \"anyres_max_9\".\n+        Returns:\n+            image_features (`torch.Tensor` of shape `(all_feat_len, embed_dim)`)\n+            feature_lens (`List[int]`)\n+                token length of each image in image_features\n+        \"\"\"\n+        new_image_features = []\n+        feature_lens = []\n+        for image_idx, image_feature in enumerate(image_features):\n+            if image_feature.shape[0] > 1:\n+                base_image_feature = image_feature[0]\n+                image_feature = image_feature[1:]\n+                height = width = self.config.vision_config.image_size // self.config.vision_config.patch_size\n+                if height * width != base_image_feature.shape[0]:\n+                    raise ValueError(\"The number of patches is not consistent with the image size.\")\n+                num_patch_height, num_patch_width = get_anyres_image_grid_shape(\n+                    image_sizes[image_idx],\n+                    self.config.image_grid_pinpoints,\n+                    self.config.vision_config.image_size,\n+                )\n+                image_feature = image_feature.view(num_patch_height, num_patch_width, height, width, -1)\n+                image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n+                image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n+                image_feature = unpad_image(image_feature, image_sizes[image_idx])\n+                max_num_patches = int(vision_aspect_ratio.strip(\"anyres_max_\"))\n+                channels, curr_height, curr_width = image_feature.shape\n+                ratio = math.sqrt(curr_height * curr_width / (max_num_patches * height**2))\n+                if ratio > 1.1:\n+                    image_feature = image_feature[None]\n+                    image_feature = nn.functional.interpolate(\n+                        image_feature, [int(curr_height // ratio), int(curr_width // ratio)], mode=\"bilinear\"\n+                    )[0]\n+                if image_newline is not None:\n+                    image_feature = torch.cat(\n+                        (\n+                            image_feature,\n+                            image_newline[:, None, None]\n+                            .expand(*image_feature.shape[:-1], 1)\n+                            .to(image_feature.device, image_feature.dtype),\n+                        ),\n+                        dim=-1,\n+                    )\n+                image_feature = image_feature.flatten(1, 2).transpose(0, 1)\n+                image_feature = torch.cat((base_image_feature, image_feature), dim=0)\n+            else:\n+                image_feature = image_feature[0]\n+                if image_newline is not None:\n+                    image_feature = torch.cat((image_feature, image_newline[None].to(image_feature)), dim=0)\n+            new_image_features.append(image_feature)\n+            feature_lens.append(image_feature.size(0))\n+        image_features = torch.cat(new_image_features, dim=0)\n+        feature_lens = torch.tensor(feature_lens, dtype=torch.long, device=image_features.device)\n+        return image_features, feature_lens\n+\n+    def apply_pooling(self, image_features):\n+        height = width = self.config.vision_config.image_size // self.config.vision_config.patch_size\n+        batch_frames, seq_len, dim = image_features.shape\n+        image_features = image_features.view(batch_frames, height, width, -1)\n+        image_features = image_features.permute(0, 3, 1, 2).contiguous()\n+\n+        height, weight = image_features.shape[2:]\n+        scaled_shape = [math.ceil(height / 2), math.ceil(weight / 2)]\n+        image_features = nn.functional.interpolate(image_features, size=scaled_shape, mode=\"bilinear\")\n+\n+        image_features = image_features.permute(0, 2, 3, 1)\n+        image_features = image_features.view(batch_frames, -1, dim)\n+        return image_features\n+\n+    @add_start_docstrings(LLAVA_ONEVISION_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        image_sizes: Optional[torch.LongTensor] = None,\n+        pixel_values_videos: torch.FloatTensor = None,\n+        image_sizes_videos: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        vision_feature_layer: Optional[int] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        vision_aspect_ratio: Optional[str] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n+    ) -> Union[Tuple, LlavaOnevisionCausalLMOutputWithPast]:\n+        r\"\"\"\n+        Args:\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n+\n+        Returns:\n+            [`~LlavaOnevisionCausalLMOutputWithPast`] (if `return_dict=True`) or a `tuple`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> import torch\n+        >>> from transformers import LlavaOnevisionProcessor, LlavaOnevisionForConditionalGeneration\n+\n+        >>> model = LlavaOnevisionForConditionalGeneration.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\", torch_dtype=\"float16\", device_map=\"cuda:0\")\n+        >>> processor = LlavaOnevisionProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\")\n+\n+        >>> conversation = [\n+        ...     {\n+        ...       \"role\": \"user\",\n+        ...       \"content\": [\n+        ...           {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+        ...           {\"type\": \"image\"},\n+        ...         ],\n+        ...     },\n+        ... ]\n+        >>> prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n+\n+        >>> image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> raw_image = Image.open(requests.get(image_file, stream=True).raw)\n+        >>> inputs = processor(text=prompt, images=raw_image, return_tensors='pt').to(0, torch.float16)\n+\n+        >>> output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+        >>> processor.batch_decode(output, skip_special_tokens=True)[0]\n+        \"user\\n\\nWhat is shown in this image?\\nassistant\\ncat\"\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+        vision_aspect_ratio = (\n+            vision_aspect_ratio if vision_aspect_ratio is not None else self.config.vision_aspect_ratio\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\n+                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if (pixel_values is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both pixel_values/pixel_values_videos and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        # Images are processed with Anyres\n+        if pixel_values is not None:\n+            image_num_patches = [\n+                image_size_to_num_patches(\n+                    image_size=imsize,\n+                    grid_pinpoints=self.config.image_grid_pinpoints,\n+                    patch_size=self.config.vision_config.image_size,\n+                )\n+                for imsize in image_sizes\n+            ]\n+\n+            # unpad extra patches and concatenate them\n+            if pixel_values.dim() == 5:\n+                _pixel_values_list = [\n+                    pix_val[:num_patch] for pix_val, num_patch in zip(pixel_values, image_num_patches)\n+                ]\n+                # [batch_size*frames*num_patches, num_channels, height, width] where frames=1 for images\n+                pixel_values = torch.cat(_pixel_values_list, dim=0)\n+            elif pixel_values.dim() != 4:\n+                raise ValueError(f\"pixel_values of shape {pixel_values.shape}, expect to be of 4 or 5 dimensions\")\n+\n+            image_features = self.vision_tower(pixel_values, output_hidden_states=True)\n+            selected_image_feature = image_features.hidden_states[vision_feature_layer]\n+\n+            if vision_feature_select_strategy == \"default\":\n+                selected_image_feature = selected_image_feature[:, 1:]\n+            elif vision_feature_select_strategy == \"full\":\n+                selected_image_feature = selected_image_feature\n+            image_features = self.multi_modal_projector(selected_image_feature)\n+\n+            image_features = torch.split(image_features, image_num_patches, dim=0)\n+            image_features, feature_lens = self.pack_image_features(\n+                image_features,\n+                image_sizes,\n+                image_newline=self.image_newline,\n+                vision_aspect_ratio=vision_aspect_ratio,\n+            )\n+\n+            special_image_mask = (\n+                (input_ids == self.config.image_token_index)\n+                .unsqueeze(-1)\n+                .expand_as(inputs_embeds)\n+                .to(inputs_embeds.device)\n+            )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        # Video are simply embedded and further pooled to decrease seq len\n+        if pixel_values_videos is not None:\n+            batch_size, frames, channels, height, width = pixel_values_videos.shape\n+            pixel_values_videos = pixel_values_videos.view(batch_size * frames, channels, height, width)\n+            video_features = self.vision_tower(pixel_values_videos, output_hidden_states=True)\n+            selected_video_feature = video_features.hidden_states[vision_feature_layer]\n+\n+            if vision_feature_select_strategy == \"default\":\n+                selected_video_feature = selected_video_feature[:, 1:]\n+            elif vision_feature_select_strategy == \"full\":\n+                selected_video_feature = selected_video_feature\n+            video_features = self.multi_modal_projector(selected_video_feature)\n+\n+            video_features = self.apply_pooling(video_features)\n+            video_features = video_features.reshape(batch_size, frames * video_features.shape[1], -1)\n+            image_newline = self.image_newline[None, None, :].repeat(batch_size, 1, 1).to(video_features.device)\n+            video_features = torch.cat((video_features, image_newline), dim=1)\n+            video_features = video_features.flatten(0, 1)\n+\n+            special_video_mask = (\n+                (input_ids == self.config.video_token_index)\n+                .unsqueeze(-1)\n+                .expand_as(inputs_embeds)\n+                .to(inputs_embeds.device)\n+            )\n+            video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_video_mask, video_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+            num_logits_to_keep=num_logits_to_keep,\n+        )\n+\n+        logits = outputs[0]\n+\n+        loss = None\n+        if labels is not None:\n+            # Shift so that tokens < n predict n\n+            if attention_mask is not None:\n+                shift_attention_mask = attention_mask[..., 1:]\n+                shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n+                shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n+            else:\n+                shift_logits = logits[..., :-1, :].contiguous()\n+                shift_labels = labels[..., 1:].contiguous()\n+            # Flatten the tokens\n+            loss_fct = nn.CrossEntropyLoss()\n+            loss = loss_fct(\n+                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n+            )\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return (loss,) + output if loss is not None else output\n+\n+        return LlavaOnevisionCausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        inputs_embeds=None,\n+        pixel_values=None,\n+        image_sizes=None,\n+        pixel_values_videos=None,\n+        image_sizes_videos=None,\n+        attention_mask=None,\n+        cache_position=None,\n+        num_logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        model_inputs = self.language_model.prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            num_logits_to_keep=num_logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        if cache_position[0] == 0:\n+            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+            # Otherwise we need pixel values to be passed to model\n+            model_inputs[\"pixel_values\"] = pixel_values\n+            model_inputs[\"image_sizes\"] = image_sizes\n+            model_inputs[\"pixel_values_videos\"] = pixel_values_videos\n+            model_inputs[\"image_sizes_videos\"] = image_sizes_videos\n+\n+        return model_inputs"
        },
        {
            "sha": "e050ec3f31deea2890cd30ab23d72f4dde317701",
            "filename": "src/transformers/models/llava_onevision/processing_llava_onevision.py",
            "status": "added",
            "additions": 274,
            "deletions": 0,
            "changes": 274,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -0,0 +1,274 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"\n+Processor class for LLaVa-Onevision.\n+\"\"\"\n+\n+import math\n+import sys\n+from typing import Iterable, List, Union\n+\n+\n+if sys.version_info >= (3, 11):\n+    from typing import Unpack\n+else:\n+    from typing_extensions import Unpack\n+\n+from ...feature_extraction_utils import BatchFeature\n+from ...image_processing_utils import select_best_resolution\n+from ...image_utils import ImageInput, VideoInput, get_image_size, to_numpy_array\n+from ...processing_utils import (\n+    ProcessingKwargs,\n+    ProcessorMixin,\n+)\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+\n+\n+class LlavaOnevisionProcessorKwargs(ProcessingKwargs, total=False):\n+    # see processing_utils.ProcessingKwargs documentation for usage.\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": False,\n+        },\n+        \"image_kwargs\": {},\n+        \"video_kwargs\": {},\n+    }\n+\n+\n+class LlavaOnevisionProcessor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a LLaVa-Onevision processor which wraps a LLaVa-Onevision video processor, LLaVa-NeXT image processor and a LLaMa tokenizer into a single processor.\n+\n+    [`LlavaNextProcessor`] offers all the functionalities of [`LlavaOnevisionVideoProcessor`], [`LlavaNextImageProcessor`] and [`LlamaTokenizerFast`]. See the\n+    [`~LlavaOnevisionVideoProcessor.__call__`], [`~LlavaNextProcessor.__call__`] and [`~LlavaNextProcessor.decode`] for more information.\n+\n+    Args:\n+        image_processor ([`LlavaNextImageProcessor`], *optional*):\n+            The image processor is a required input.\n+        tokenizer ([`LlamaTokenizerFast`], *optional*):\n+            The tokenizer is a required input.\n+        video_processor ([`LlavaOnevisionVideoProcessor`], *optional*):\n+            The video processor is a required input.\n+        num_image_tokens (`int`, *optional*):\n+            Number of image tokens for one imagethat will be returned by vision tower.\n+        vision_feature_select_strategy (`str`, *optional*):\n+            The feature selection strategy used to select the vision feature from the vision backbone.\n+            Shoudl be same as in model's config\n+        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n+            in a chat into a tokenizable string.\n+        image_token (`str`, *optional*, defaults to `\"<image>\"`):\n+            Special token used to denote image location.\n+        video_token (`str`, *optional*, defaults to `\"<video>\"`):\n+            Special token used to denote video location.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"tokenizer\", \"video_processor\"]\n+    valid_kwargs = [\n+        \"chat_template\",\n+        \"num_image_tokens\",\n+        \"vision_feature_select_strategy\",\n+        \"image_token\",\n+        \"video_token\",\n+    ]\n+    image_processor_class = \"AutoImageProcessor\"\n+    tokenizer_class = \"AutoTokenizer\"\n+    video_processor_class = \"LlavaOnevisionVideoProcessor\"\n+\n+    def __init__(\n+        self,\n+        image_processor=None,\n+        tokenizer=None,\n+        video_processor=None,\n+        num_image_tokens=None,\n+        vision_feature_select_strategy=None,\n+        chat_template=None,\n+        image_token=\"<image>\",\n+        video_token=\"<video>\",\n+        **kwargs: Unpack[LlavaOnevisionProcessorKwargs],\n+    ):\n+        self.num_image_tokens = num_image_tokens\n+        self.vision_feature_select_strategy = vision_feature_select_strategy\n+        self.image_token = image_token\n+        self.video_token = video_token\n+        super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)\n+\n+    def __call__(\n+        self,\n+        images: ImageInput = None,\n+        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        videos: VideoInput = None,\n+        **kwargs,\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n+        and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n+        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        LlavaNextImageProcessor's [`~LlavaNextImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        of the above two methods for more information.\n+\n+        Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            text (`str`, `List[str]`, `List[List[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            videos (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+              `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+            - **pixel_values_videos** -- Pixel values of a video input to be fed to a model. Returned when `videos` is not `None`.\n+            - **image_sizes** -- Size of each image that will be used to unpad an image. Returned when `images` is not `None`.\n+        \"\"\"\n+\n+        output_kwargs = self._merge_kwargs(\n+            LlavaOnevisionProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n+        if isinstance(text, str):\n+            text = [text]\n+        elif not isinstance(text, list) and not isinstance(text[0], str):\n+            raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+\n+        image_inputs = video_inputs = {}\n+\n+        if images is not None:\n+            image_inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n+\n+            image_sizes = iter(image_inputs[\"image_sizes\"])\n+            height, width = get_image_size(\n+                to_numpy_array(image_inputs[\"pixel_values\"][0][0]),\n+                channel_dim=output_kwargs[\"images_kwargs\"].get(\"data_format\"),\n+            )\n+            text = self._expand_image_tokens(text, image_sizes, height, width, self.image_token)\n+\n+        if videos is not None:\n+            video_inputs = self.video_processor(videos, **output_kwargs[\"videos_kwargs\"])\n+\n+            one_video = to_numpy_array(video_inputs[\"pixel_values_videos\"][0])\n+            height, width = get_image_size(one_video[0], channel_dim=output_kwargs[\"images_kwargs\"].get(\"data_format\"))\n+            num_frames = one_video.shape[0]  # frame dim is always after batch dim\n+            patches_height_width = int(math.sqrt(self.num_image_tokens))\n+            pooled_height_width = math.ceil(patches_height_width / 2)\n+            num_video_tokens = (num_frames * pooled_height_width * pooled_height_width) + 1  # +1 for newline token\n+            text = [sample.replace(self.video_token, self.video_token * num_video_tokens) for sample in text]\n+\n+        # Padding side can be in TextKwargs but is not accepted by the tokenizer\n+        _ = output_kwargs[\"text_kwargs\"].pop(\"padding_side\", None)\n+        text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+        return BatchFeature(data={**text_inputs, **image_inputs, **video_inputs})\n+\n+    def _expand_image_tokens(\n+        self,\n+        text: List[TextInput],\n+        image_sizes: Iterable[Union[List[int], int]],\n+        height: int,\n+        width: int,\n+        special_token: str,\n+        num_frames: int = 1,\n+    ):\n+        prompt_strings = []\n+        for sample in text:\n+            while special_token in sample:\n+                image_size_list = next(image_sizes)\n+                orig_height, orig_width = image_size_list[0] if num_frames != 1 else image_size_list\n+                num_image_tokens = self._get_number_of_features(orig_height, orig_width, height, width)\n+                if self.vision_feature_select_strategy == \"default\":\n+                    num_image_tokens -= 1\n+                sample = sample.replace(special_token, \"<placeholder>\" * num_image_tokens * num_frames, 1)\n+            prompt_strings.append(sample)\n+        text = [sample.replace(\"<placeholder>\", special_token) for sample in prompt_strings]\n+        return text\n+\n+    def _get_number_of_features(self, orig_height: int, orig_width: int, height: int, width: int) -> int:\n+        image_grid_pinpoints = self.image_processor.image_grid_pinpoints\n+\n+        height_best_resolution, width_best_resolution = select_best_resolution(\n+            [orig_height, orig_width], image_grid_pinpoints\n+        )\n+        scale_height, scale_width = height_best_resolution // height, width_best_resolution // width\n+\n+        patches_height = patches_width = int(math.sqrt(self.num_image_tokens))\n+        unpadded_features, newline_features = self._get_unpadded_features(\n+            orig_height, orig_width, patches_height, patches_width, scale_height, scale_width\n+        )\n+\n+        # The base patch covers the entire image (no CLS for SigLIP)\n+        base_features = self.num_image_tokens\n+        num_image_tokens = unpadded_features + newline_features + base_features\n+        return num_image_tokens\n+\n+    def _get_unpadded_features(self, height, width, patches_height, patches_width, scale_height, scale_width):\n+        \"\"\"\n+        Get number of features for a given image with height/width. LLaVA-NeXT is different from LLaVA\n+        because it divided each image into patches depending on its resolution. Therefore we need to calculate how many\n+        patches an image is divided into and get the number of features from that.\n+        \"\"\"\n+        current_height = patches_height * scale_height\n+        current_width = patches_width * scale_width\n+\n+        original_aspect_ratio = width / height\n+        current_aspect_ratio = current_width / current_height\n+        if original_aspect_ratio > current_aspect_ratio:\n+            new_height = int(height * (current_width / width))\n+            padding = (current_height - new_height) // 2\n+            current_height -= padding * 2\n+        else:\n+            new_width = int(width * (current_height / height))\n+            padding = (current_width - new_width) // 2\n+            current_width -= padding * 2\n+\n+        unpadded_features = current_height * current_width\n+        newline_features = current_height\n+\n+        ratio = math.sqrt(current_height * current_width / (9 * patches_height**2))\n+        if ratio > 1.1:\n+            unpadded_features = int(current_height // ratio) * int(current_width // ratio)\n+            newline_features = int(current_height // ratio)\n+\n+        return (unpadded_features, newline_features)\n+\n+    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.decode with CLIP->Llama\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    @property\n+    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.model_input_names\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))"
        },
        {
            "sha": "bd63c45618af940ff07425e703550eb3cee57ce2",
            "filename": "src/transformers/models/llava_onevision/video_processing_llava_onevision.py",
            "status": "added",
            "additions": 335,
            "deletions": 0,
            "changes": 335,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fvideo_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fvideo_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fvideo_processing_llava_onevision.py?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -0,0 +1,335 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Video processor class for LLaVa-Onevision.\"\"\"\n+\n+from typing import Dict, List, Optional, Union\n+\n+from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n+from ...image_transforms import (\n+    convert_to_rgb,\n+    resize,\n+    to_channel_dimension_format,\n+)\n+from ...image_utils import (\n+    OPENAI_CLIP_MEAN,\n+    OPENAI_CLIP_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    VideoInput,\n+    infer_channel_dimension_format,\n+    is_scaled_image,\n+    is_valid_image,\n+    to_numpy_array,\n+    valid_images,\n+    validate_preprocess_arguments,\n+)\n+from ...utils import TensorType, is_vision_available, logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+\n+def make_batched_videos(videos) -> List[VideoInput]:\n+    if isinstance(videos, (list, tuple)) and isinstance(videos[0], (list, tuple)) and is_valid_image(videos[0][0]):\n+        return videos\n+\n+    elif isinstance(videos, (list, tuple)) and is_valid_image(videos[0]):\n+        if isinstance(videos[0], Image.Image) or len(videos[0].shape) == 3:\n+            return [videos]\n+        elif len(videos[0].shape) == 4:\n+            return [list(video) for video in videos]\n+\n+    elif is_valid_image(videos) and len(videos.shape) == 4:\n+        return [list(videos)]\n+\n+    raise ValueError(f\"Could not make batched video from {videos}\")\n+\n+\n+class LlavaOnevisionVideoProcessor(BaseImageProcessor):\n+    r\"\"\"\n+    Constructs a LLaVa-Onevisino-Video video processor. Based on [`SiglipImageProcessor`] with incorporation of processing each video frame.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by\n+            `do_resize` in the `preprocess` method.\n+        size (`Dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 224}`):\n+            Size of the image after resizing. The shortest edge of the image is resized to size[\"shortest_edge\"], with\n+            the longest edge resized to keep the input aspect ratio. Can be overridden by `size` in the `preprocess`\n+            method.\n+        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n+            Resampling filter to use if resizing the image. Can be overridden by `resample` in the `preprocess` method.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n+            the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in the `preprocess`\n+            method.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the image. Can be overridden by `do_normalize` in the `preprocess` method.\n+        image_mean (`float` or `List[float]`, *optional*, defaults to `[0.48145466, 0.4578275, 0.40821073]`):\n+            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n+            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n+        image_std (`float` or `List[float]`, *optional*, defaults to `[0.26862954, 0.26130258, 0.27577711]`):\n+            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n+            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n+            Can be overridden by the `image_std` parameter in the `preprocess` method.\n+        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n+            Whether to convert the image to RGB.\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values_videos\"]\n+\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        size: Dict[str, int] = None,\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n+        do_normalize: bool = True,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_convert_rgb: bool = True,\n+        **kwargs,\n+    ) -> None:\n+        super().__init__(**kwargs)\n+        size = size if size is not None else {\"height\": 384, \"width\": 384}\n+        size = get_size_dict(size, default_to_square=False)\n+\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.resample = resample\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean if image_mean is not None else OPENAI_CLIP_MEAN\n+        self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n+        self.do_convert_rgb = do_convert_rgb\n+\n+    def _preprocess(\n+        self,\n+        images: ImageInput,\n+        do_resize: bool = None,\n+        size: Dict[str, int] = None,\n+        resample: PILImageResampling = None,\n+        do_rescale: bool = None,\n+        rescale_factor: float = None,\n+        do_normalize: bool = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_convert_rgb: bool = None,\n+        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> Image.Image:\n+        \"\"\"\n+        Args:\n+            images (`ImageInput`):\n+                Batch of frames (one video) to preprocess. Expects a batch of frames with pixel values ranging from 0 to 255. If\n+                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+                Size of the image after resizing. Shortest edge of the image is resized to size[\"shortest_edge\"], with\n+                the longest edge resized to keep the input aspect ratio.\n+            resample (`int`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n+                has an effect if `do_resize` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image.\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n+            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n+                `True`.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        \"\"\"\n+        if do_convert_rgb:\n+            images = [convert_to_rgb(image) for image in images]\n+\n+        # All transformations expect numpy arrays.\n+        images = [to_numpy_array(image) for image in images]\n+\n+        if is_scaled_image(images[0]) and do_rescale:\n+            logger.warning_once(\n+                \"It looks like you are trying to rescale already rescaled videos. If the input\"\n+                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+            )\n+\n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(images[0])\n+\n+        if do_resize:\n+            images = [\n+                resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n+                for image in images\n+            ]\n+\n+        if do_rescale:\n+            images = [\n+                self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+                for image in images\n+            ]\n+\n+        if do_normalize:\n+            images = [\n+                self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n+                for image in images\n+            ]\n+\n+        images = [\n+            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n+        ]\n+\n+        return images\n+\n+    def preprocess(\n+        self,\n+        videos: VideoInput,\n+        do_resize: bool = None,\n+        size: Dict[str, int] = None,\n+        resample: PILImageResampling = None,\n+        do_rescale: bool = None,\n+        rescale_factor: float = None,\n+        do_normalize: bool = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_convert_rgb: bool = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ):\n+        \"\"\"\n+        Args:\n+            videos (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+                Size of the image after resizing. Shortest edge of the image is resized to size[\"shortest_edge\"], with\n+                the longest edge resized to keep the input aspect ratio.\n+            resample (`int`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n+                has an effect if `do_resize` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image.\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n+            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n+                `True`.\n+            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n+                Whether to convert the image to RGB.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                - Unset: Return a list of `np.ndarray`.\n+                - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n+                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+                - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+\n+        \"\"\"\n+        do_resize = do_resize if do_resize is not None else self.do_resize\n+        size = size if size is not None else self.size\n+        resample = resample if resample is not None else self.resample\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+\n+        videos = make_batched_videos(videos)\n+\n+        if not valid_images(videos[0]):\n+            raise ValueError(\n+                \"Invalid video type. Must be a list consisting of PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+\n+        validate_preprocess_arguments(\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+        )\n+\n+        size_tuple = (\n+            (size[\"height\"], size[\"width\"])\n+            if \"height\" in size and \"width\" in size\n+            else (size[\"shortest_edge\"], size[\"shortest_edge\"])\n+        )\n+\n+        pixel_values = [\n+            self._preprocess(\n+                video,\n+                do_resize=do_resize,\n+                size=size_tuple,\n+                resample=resample,\n+                do_rescale=do_rescale,\n+                rescale_factor=rescale_factor,\n+                do_normalize=do_normalize,\n+                image_mean=image_mean,\n+                image_std=image_std,\n+                do_convert_rgb=do_convert_rgb,\n+                data_format=data_format,\n+                input_data_format=input_data_format,\n+            )\n+            for video in videos\n+        ]\n+\n+        return BatchFeature(\n+            data={\"pixel_values_videos\": pixel_values},\n+            tensor_type=return_tensors,\n+        )"
        },
        {
            "sha": "ee28c01189b4393f7dd85a38c65d88ac72e8820c",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -390,6 +390,8 @@ def to_dict(self) -> Dict[str, Any]:\n             del output[\"image_processor\"]\n         if \"feature_extractor\" in output:\n             del output[\"feature_extractor\"]\n+        if \"chat_template\" in output:\n+            del output[\"chat_template\"]\n \n         # Some attributes have different names but containing objects that are not simple strings\n         output = {"
        },
        {
            "sha": "ddf7608155e843ead50afb4d91f8b61dd32b0f3b",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -5374,6 +5374,20 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class LlavaOnevisionForConditionalGeneration(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class LlavaOnevisionPreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class LongformerForMaskedLM(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "2493954a518b2c892f315dfbfc1b91207555c2c4",
            "filename": "src/transformers/utils/dummy_vision_objects.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -373,6 +373,20 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"vision\"])\n \n \n+class LlavaOnevisionImageProcessor(metaclass=DummyObject):\n+    _backends = [\"vision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"vision\"])\n+\n+\n+class LlavaOnevisionVideoProcessor(metaclass=DummyObject):\n+    _backends = [\"vision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"vision\"])\n+\n+\n class Mask2FormerImageProcessor(metaclass=DummyObject):\n     _backends = [\"vision\"]\n "
        },
        {
            "sha": "439ea58ae977670a2b0564caa5e4816cf69716bc",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 13,
            "deletions": 6,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -491,6 +491,7 @@ def test_greedy_generate_dict_outputs_use_cache(self):\n                 self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n             else:\n                 self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + input_ids.shape[-1])\n+\n             self._check_outputs(output_generate, input_ids, model.config, use_cache=True)\n \n     @pytest.mark.generate\n@@ -630,6 +631,7 @@ def test_beam_search_generate_dict_outputs_use_cache(self):\n                 self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n             else:\n                 self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + input_ids.shape[-1])\n+\n             self._check_outputs(\n                 output_generate, input_ids, model.config, use_cache=True, num_return_sequences=beam_kwargs[\"num_beams\"]\n             )\n@@ -986,6 +988,7 @@ def test_contrastive_generate_dict_outputs_use_cache(self):\n                 self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n             else:\n                 self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + input_ids.shape[-1])\n+\n             self._check_outputs(output_generate, input_ids, model.config, use_cache=True)\n \n     @pytest.mark.generate\n@@ -1152,6 +1155,7 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n             output_assisted = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)\n \n             # The two outputs must match and their shape must be as expected\n+\n             self.assertListEqual(output_greedy.sequences.tolist(), output_assisted.sequences.tolist())\n             for output in (output_greedy, output_assisted):\n                 self._check_outputs(output, input_ids, model.config, use_cache=True)\n@@ -1216,6 +1220,7 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n             output_prompt_lookup = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)\n \n             # The two outputs must match and their shape must be as expected\n+\n             self.assertListEqual(output_greedy.sequences.tolist(), output_prompt_lookup.sequences.tolist())\n             for output in (output_greedy, output_prompt_lookup):\n                 self._check_outputs(output, input_ids, model.config, use_cache=True)\n@@ -1453,8 +1458,10 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n             next_logits_wo_padding = model(**model_kwargs).logits[:, -1, :]\n \n             # With left-padding (length 32)\n+            # can hardcode pad_token to be 0 as we'll do attn masking anyway\n+            pad_token_id = config.pad_token_id if getattr(config, \"pad_token_id\") is not None else 0\n             pad_size = (input_ids.shape[0], 32)\n-            padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * config.pad_token_id\n+            padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * pad_token_id\n             padded_input_ids = torch.cat((padding, input_ids), dim=1)\n             padded_attention_mask = torch.cat((torch.zeros_like(padding), attention_mask), dim=1)\n             model_kwargs = _prepare_model_kwargs(padded_input_ids, padded_attention_mask, signature)\n@@ -1765,15 +1772,14 @@ def test_generate_with_static_cache(self):\n             }\n \n             max_cache_len = seq_length + max_new_tokens\n+            config = config.text_config if hasattr(config, \"text_config\") else config\n             head_dim = (\n-                model.config.head_dim\n-                if hasattr(model.config, \"head_dim\")\n-                else model.config.hidden_size // model.config.num_attention_heads\n+                config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads\n             )\n             num_key_value_heads = (\n-                model.config.num_attention_heads\n+                config.num_attention_heads\n                 if getattr(config, \"num_key_value_heads\", None) is None\n-                else model.config.num_key_value_heads\n+                else config.num_key_value_heads\n             )\n             num_hidden_layers = config.num_hidden_layers\n             results = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)\n@@ -1922,6 +1928,7 @@ def test_assisted_decoding_with_num_logits_to_keep(self):\n \n     def _check_outputs(self, output, input_ids, config, use_cache=False, num_return_sequences=1):\n         batch_size, seq_length = input_ids.shape\n+        config = config.text_config if hasattr(config, \"text_config\") else config\n         num_sequences_in_output = batch_size * num_return_sequences\n \n         gen_len = ("
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/llava_onevision/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/tests%2Fmodels%2Fllava_onevision%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/tests%2Fmodels%2Fllava_onevision%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2F__init__.py?ref=43df47d8e78238021a4273746fc469336f948314"
        },
        {
            "sha": "47b6ef86c5dd10a38d80776cf6bb329eb9ec9b84",
            "filename": "tests/models/llava_onevision/test_image_processing_llava_onevision.py",
            "status": "added",
            "additions": 291,
            "deletions": 0,
            "changes": 291,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/tests%2Fmodels%2Fllava_onevision%2Ftest_image_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/tests%2Fmodels%2Fllava_onevision%2Ftest_image_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_image_processing_llava_onevision.py?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -0,0 +1,291 @@\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import numpy as np\n+\n+from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import LlavaOnevisionImageProcessor, LlavaOnevisionVideoProcessor\n+\n+\n+class LlavaOnevisionImageProcessingTester(unittest.TestCase):\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        image_size=20,\n+        min_resolution=30,\n+        max_resolution=400,\n+        do_resize=True,\n+        size=None,\n+        do_normalize=True,\n+        image_mean=OPENAI_CLIP_MEAN,\n+        image_std=OPENAI_CLIP_STD,\n+        do_convert_rgb=True,\n+    ):\n+        size = size if size is not None else {\"height\": 20, \"width\": 20}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_convert_rgb = do_convert_rgb\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_convert_rgb\": self.do_convert_rgb,\n+        }\n+\n+    def expected_output_image_shape(self, images):\n+        return self.num_channels, self.size[\"height\"], self.size[\"width\"]\n+\n+    # Copied from tests.models.clip.test_image_processing_clip.CLIPImageProcessingTester.prepare_image_inputs\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        return prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+\n+    # Copied from tests.models.llava_next_video.test_image_processing_llava_next_video.LlavaNextVideoProcessingTester.prepare_video_inputs\n+    def prepare_video_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        images = prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+\n+        # let's simply copy the frames to fake a long video-clip\n+        if numpify or torchify:\n+            videos = []\n+            for image in images:\n+                if numpify:\n+                    video = image[None, ...].repeat(8, 0)\n+                else:\n+                    video = image[None, ...].repeat(8, 1, 1, 1)\n+                videos.append(video)\n+        else:\n+            videos = []\n+            for pil_image in images:\n+                videos.append([pil_image] * 8)\n+\n+        return videos\n+\n+\n+@require_torch\n+@require_vision\n+class LlavaOnevisionImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    image_processing_class = LlavaOnevisionImageProcessor if is_vision_available() else None\n+    video_processing_class = LlavaOnevisionVideoProcessor if is_vision_available() else None\n+\n+    # Copied from tests.models.clip.test_image_processing_clip.CLIPImageProcessingTest.setUp with CLIP->LlavaOnevision\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = LlavaOnevisionImageProcessingTester(self)\n+\n+    @property\n+    # Copied from tests.models.clip.test_image_processing_clip.CLIPImageProcessingTest.image_processor_dict\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    def test_image_processor_properties(self):\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+        self.assertTrue(hasattr(image_processing, \"size\"))\n+        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+        self.assertTrue(hasattr(image_processing, \"image_std\"))\n+        self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n+        self.assertTrue(hasattr(image_processing, \"image_grid_pinpoints\"))\n+\n+    def test_video_processor_properties(self):\n+        image_processing = self.video_processing_class(**self.image_processor_dict)\n+        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+        self.assertTrue(hasattr(image_processing, \"size\"))\n+        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+        self.assertTrue(hasattr(image_processing, \"image_std\"))\n+        self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n+\n+    def test_image_processor_from_dict_with_kwargs(self):\n+        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n+        self.assertEqual(image_processor.size, {\"height\": 20, \"width\": 20})\n+\n+        image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42)\n+        self.assertEqual(image_processor.size, {\"shortest_edge\": 42})\n+\n+    def test_call_pil(self):\n+        # Initialize image_processing\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        # create random PIL images\n+        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n+        for image in image_inputs:\n+            self.assertIsInstance(image, Image.Image)\n+\n+        # Test not batched input\n+        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = (1, 1522, 3, 20, 20)\n+        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+        # Test batched\n+        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = (7, 1522, 3, 20, 20)\n+        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+    def test_call_numpy(self):\n+        # Initialize image_processing\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        # create random numpy tensors\n+        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, numpify=True)\n+        for image in image_inputs:\n+            self.assertIsInstance(image, np.ndarray)\n+\n+        # Test not batched input\n+        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = (1, 1522, 3, 20, 20)\n+        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+        # Test batched\n+        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = (7, 1522, 3, 20, 20)\n+        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+    def test_call_pytorch(self):\n+        # Initialize image_processing\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        # create random PyTorch tensors\n+        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n+\n+        for image in image_inputs:\n+            self.assertIsInstance(image, torch.Tensor)\n+\n+        # Test not batched input\n+        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = (1, 1522, 3, 20, 20)\n+        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+        # Test batched\n+        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = (7, 1522, 3, 20, 20)\n+        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+    @unittest.skip(\n+        reason=\"LlavaOnevisionImageProcessor doesn't treat 4 channel PIL and numpy consistently yet\"\n+    )  # FIXME raushan\n+    def test_call_numpy_4_channels(self):\n+        pass\n+\n+    def test_nested_input(self):\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n+\n+        # Test batched as a list of images\n+        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = (7, 1522, 3, 20, 20)\n+        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+        # Test batched as a nested list of images, where each sublist is one batch\n+        image_inputs_nested = [image_inputs[:3], image_inputs[3:]]\n+        encoded_images_nested = image_processing(image_inputs_nested, return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = (7, 1522, 3, 20, 20)\n+        self.assertEqual(tuple(encoded_images_nested.shape), expected_output_image_shape)\n+\n+        # Image processor should return same pixel values, independently of input format\n+        self.assertTrue((encoded_images_nested == encoded_images).all())\n+\n+    def test_call_pil_video(self):\n+        # Initialize image_processing\n+        video_processing = self.video_processing_class(**self.image_processor_dict)\n+        # create random numpy tensors\n+        video_inputs = self.image_processor_tester.prepare_video_inputs(equal_resolution=True)\n+        for video in video_inputs:\n+            self.assertIsInstance(video[0], Image.Image)\n+\n+        encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\").pixel_values_videos\n+        expected_output_video_shape = (1, 8, 3, 20, 20)\n+        self.assertEqual(tuple(encoded_videos.shape), expected_output_video_shape)\n+\n+        # Test batched\n+        encoded_videos = video_processing(video_inputs, return_tensors=\"pt\").pixel_values_videos\n+        expected_output_video_shape = (7, 8, 3, 20, 20)\n+        self.assertEqual(tuple(encoded_videos.shape), expected_output_video_shape)\n+\n+    def test_call_numpy_video(self):\n+        # Initialize image_processing\n+        video_processing = self.video_processing_class(**self.image_processor_dict)\n+        # create random numpy tensors\n+        video_inputs = self.image_processor_tester.prepare_video_inputs(equal_resolution=True, numpify=True)\n+        for video in video_inputs:\n+            self.assertIsInstance(video, np.ndarray)\n+\n+        encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\").pixel_values_videos\n+        expected_output_video_shape = (1, 8, 3, 20, 20)\n+        self.assertEqual(tuple(encoded_videos.shape), expected_output_video_shape)\n+\n+        # Test batched\n+        encoded_videos = video_processing(video_inputs, return_tensors=\"pt\").pixel_values_videos\n+        expected_output_video_shape = (7, 8, 3, 20, 20)\n+        self.assertEqual(tuple(encoded_videos.shape), expected_output_video_shape)\n+\n+    def test_call_pytorch_video(self):\n+        # Initialize image_processing\n+        video_processing = self.video_processing_class(**self.image_processor_dict)\n+        # create random PyTorch tensors\n+        video_inputs = self.image_processor_tester.prepare_video_inputs(equal_resolution=True, torchify=True)\n+        for video in video_inputs:\n+            self.assertIsInstance(video, torch.Tensor)\n+\n+        # Test not batched input\n+        encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\").pixel_values_videos\n+        expected_output_video_shape = (1, 8, 3, 20, 20)\n+        self.assertEqual(tuple(encoded_videos.shape), expected_output_video_shape)\n+\n+        # Test batched\n+        encoded_videos = video_processing(video_inputs, return_tensors=\"pt\").pixel_values_videos\n+        expected_output_video_shape = (7, 8, 3, 20, 20)\n+        self.assertEqual(tuple(encoded_videos.shape), expected_output_video_shape)"
        },
        {
            "sha": "2300e7b29c486d1083c72f33a0f760bd452cb4eb",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "added",
            "additions": 523,
            "deletions": 0,
            "changes": 523,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -0,0 +1,523 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Llava-NeXT model.\"\"\"\n+\n+import gc\n+import unittest\n+\n+import numpy as np\n+import requests\n+from huggingface_hub import hf_hub_download\n+\n+from transformers import (\n+    AutoProcessor,\n+    LlavaOnevisionConfig,\n+    LlavaOnevisionForConditionalGeneration,\n+    is_torch_available,\n+    is_vision_available,\n+)\n+from transformers.testing_utils import (\n+    require_bitsandbytes,\n+    require_torch,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import (\n+    ModelTesterMixin,\n+    _config_zero_init,\n+    floats_tensor,\n+    ids_tensor,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+else:\n+    is_torch_greater_or_equal_than_2_0 = False\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+\n+class LlavaOnevisionVisionText2TextModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        ignore_index=-100,\n+        image_token_index=0,\n+        projector_hidden_act=\"gelu\",\n+        seq_length=7,\n+        vision_feature_select_strategy=\"full\",\n+        vision_feature_layer=-1,\n+        text_config={\n+            \"model_type\": \"qwen2\",\n+            \"seq_length\": 7,\n+            \"is_training\": True,\n+            \"use_input_mask\": True,\n+            \"use_token_type_ids\": False,\n+            \"use_labels\": True,\n+            \"vocab_size\": 99,\n+            \"hidden_size\": 32,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 4,\n+            \"num_key_value_heads\": 4,\n+            \"intermediate_size\": 37,\n+            \"hidden_act\": \"gelu\",\n+            \"hidden_dropout_prob\": 0.1,\n+            \"attention_probs_dropout_prob\": 0.1,\n+            \"max_position_embeddings\": 580,\n+            \"type_vocab_size\": 16,\n+            \"type_sequence_label_size\": 2,\n+            \"initializer_range\": 0.02,\n+            \"num_labels\": 3,\n+            \"num_choices\": 4,\n+            \"pad_token_id\": 0,\n+        },\n+        is_training=True,\n+        vision_config={\n+            \"image_size\": 16,\n+            \"patch_size\": 2,\n+            \"num_channels\": 3,\n+            \"is_training\": True,\n+            \"hidden_size\": 32,\n+            \"projection_dim\": 32,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 4,\n+            \"intermediate_size\": 37,\n+            \"dropout\": 0.1,\n+            \"attention_dropout\": 0.1,\n+            \"initializer_range\": 0.02,\n+        },\n+    ):\n+        self.parent = parent\n+        self.ignore_index = ignore_index\n+        self.image_token_index = image_token_index\n+        self.projector_hidden_act = projector_hidden_act\n+        self.vision_feature_select_strategy = vision_feature_select_strategy\n+        self.vision_feature_layer = vision_feature_layer\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.seq_length = seq_length\n+\n+        self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n+        self.vocab_size = text_config[\"vocab_size\"]\n+        self.hidden_size = text_config[\"hidden_size\"]\n+        self.num_attention_heads = text_config[\"num_attention_heads\"]\n+        self.is_training = is_training\n+\n+        self.batch_size = 3\n+        self.num_channels = 3\n+        self.image_size = 30\n+        self.encoder_seq_length = 7\n+        self.image_grid_pinpoints = [[32, 32]]\n+\n+    def get_config(self):\n+        return LlavaOnevisionConfig(\n+            text_config=self.text_config,\n+            vision_config=self.vision_config,\n+            ignore_index=self.ignore_index,\n+            image_token_index=self.image_token_index,\n+            projector_hidden_act=self.projector_hidden_act,\n+            vision_feature_select_strategy=self.vision_feature_select_strategy,\n+            vision_feature_layer=self.vision_feature_layer,\n+            image_grid_pinpoints=self.image_grid_pinpoints,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor(\n+            [\n+                self.batch_size,\n+                9,\n+                self.vision_config[\"num_channels\"],\n+                self.vision_config[\"image_size\"],\n+                self.vision_config[\"image_size\"],\n+            ]\n+        )\n+        config = self.get_config()\n+\n+        return config, pixel_values\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size - 2) + 2\n+        attention_mask = torch.ones(input_ids.shape, dtype=torch.long).to(torch_device)\n+        # we are giving 3 images let's make sure we pass in 3 image tokens\n+        input_ids[:, 1] = config.image_token_index\n+        labels = torch.zeros((self.batch_size, self.seq_length), dtype=torch.long, device=torch_device)\n+        # maskout where the image token is\n+        labels[:, 1] == self.ignore_index\n+        inputs_dict = {\n+            \"pixel_values\": pixel_values,\n+            \"image_sizes\": torch.tensor(\n+                [[self.vision_config[\"image_size\"], self.vision_config[\"image_size\"]]] * self.batch_size\n+            ),\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+            \"labels\": labels,\n+        }\n+        return config, inputs_dict\n+\n+    def create_and_check_llava_onevision_model_fp16_forward(\n+        self, config, input_ids, pixel_values, attention_mask, image_sizes\n+    ):\n+        model = LlavaOnevisionForConditionalGeneration(config=config)\n+        model.to(torch_device)\n+        model.half()\n+        model.eval()\n+        logits = model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            image_sizes=image_sizes,\n+            pixel_values=pixel_values.to(torch.bfloat16),\n+            return_dict=True,\n+        )[\"logits\"]\n+        self.parent.assertFalse(torch.isnan(logits).any().item())\n+\n+    def create_and_check_llava_onevision_model_fp16_autocast_forward(\n+        self, config, input_ids, pixel_values, attention_mask, image_sizes\n+    ):\n+        config.torch_dtype = torch.float16\n+        model = LlavaOnevisionForConditionalGeneration(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n+            logits = model(\n+                input_ids=input_ids,\n+                attention_mask=attention_mask,\n+                image_sizes=image_sizes,\n+                pixel_values=pixel_values.to(torch.bfloat16),\n+                return_dict=True,\n+            )[\"logits\"]\n+        self.parent.assertFalse(torch.isnan(logits).any().item())\n+\n+\n+@require_torch\n+class LlavaOnevisionForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Model tester for `LlavaOnevisionForConditionalGeneration`.\n+    \"\"\"\n+\n+    all_model_classes = (LlavaOnevisionForConditionalGeneration,) if is_torch_available() else ()\n+    all_generative_model_classes = (LlavaOnevisionForConditionalGeneration,) if is_torch_available() else ()\n+    test_pruning = False\n+    test_head_masking = False\n+\n+    def setUp(self):\n+        self.model_tester = LlavaOnevisionVisionText2TextModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=LlavaOnevisionConfig, has_text_modality=False)\n+\n+    def test_initialization(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        configs_no_init = _config_zero_init(config)\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            for name, param in model.named_parameters():\n+                # LLaVa Onevision has SigLIP backbone which init weights differently from CLIP\n+                if \"image_newline\" in name or \"vision_tower\" in name:\n+                    continue\n+                elif param.requires_grad:\n+                    self.assertIn(\n+                        ((param.data.mean() * 1e9).round() / 1e9).item(),\n+                        [0.0, 1.0],\n+                        msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                    )\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n+    def test_inputs_embeds(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            wte = model.get_input_embeddings()\n+            inputs[\"inputs_embeds\"] = wte(input_ids)\n+\n+            with torch.no_grad():\n+                model(**inputs)\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n+    # while some other models require pixel_values to be present\n+    def test_inputs_embeds_matches_input_ids(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            inputs_embeds = model.get_input_embeddings()(input_ids)\n+\n+            with torch.no_grad():\n+                out_ids = model(input_ids=input_ids, **inputs)[0]\n+                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n+            self.assertTrue(torch.allclose(out_embeds, out_ids))\n+\n+    @unittest.skip(\n+        reason=\"This architecure seem to not compute gradients properly when using GC, SiglipVisionModel does not support standalone training\"\n+    )\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecure seem to not compute gradients properly when using GC, SiglipVisionModel does not support standalone training\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecure seem to not compute gradients properly when using GC, SiglipVisionModel does not support standalone training\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    @unittest.skip(\"VLMs can't do assisted decoding yet!\")\n+    def test_assisted_decoding_with_num_logits_to_keep(self):\n+        pass\n+\n+\n+@require_torch\n+class LlavaOnevisionForConditionalGenerationIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        self.processor = AutoProcessor.from_pretrained(\n+            \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\", padding_side=\"left\"\n+        )\n+        image_file = hf_hub_download(\n+            repo_id=\"raushan-testing-hf/images_test\", filename=\"llava_v1_5_radar.jpg\", repo_type=\"dataset\"\n+        )\n+        video_file = hf_hub_download(\n+            repo_id=\"raushan-testing-hf/videos-test\", filename=\"video_demo.npy\", repo_type=\"dataset\"\n+        )\n+        self.image = Image.open(image_file)\n+        self.video = np.load(video_file)\n+        self.prompt_image = \"user\\n<image>\\nWhat do you see in this image?<|im_end|>\\n<|im_start|>assistant\\n\"\n+        self.prompt_video = \"user\\n<video>\\nWhat do you see in this video?<|im_end|>\\n<|im_start|>assistant\\n\"\n+\n+    def tearDown(self):\n+        gc.collect()\n+        torch.cuda.empty_cache()\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_small_model_integration_test(self):\n+        model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n+            \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\", torch_dtype=\"float16\", device_map=torch_device\n+        )\n+\n+        inputs = self.processor(images=self.image, text=self.prompt_image, return_tensors=\"pt\").to(\n+            torch_device, torch.float16\n+        )\n+        self.assertTrue(inputs.input_ids.shape[1] == 6567)  # should expand num-image-tokens times\n+        self.assertTrue(inputs.pixel_values.shape == torch.Size([1, 10, 3, 384, 384]))\n+        self.assertTrue(inputs.image_sizes.tolist() == [[899, 1024]])\n+\n+        # verify single forward pass\n+        inputs = inputs.to(torch_device)\n+        with torch.no_grad():\n+            output = model(**inputs)\n+\n+        expected_slice = torch.tensor(\n+            [[-12.3125, -14.5625, -12.8750], [3.4023, 5.0508, 9.5469], [3.5762, 4.4922, 7.8906]],\n+            dtype=torch.float32,\n+            device=torch_device,\n+        )\n+        self.assertTrue(torch.allclose(output.logits[0, :3, :3], expected_slice, atol=1e-3))\n+\n+        # verify generation\n+        output = model.generate(**inputs, max_new_tokens=100)\n+        EXPECTED_DECODED_TEXT = 'user\\n\\nWhat do you see in this image?\\nassistant\\nThe image is a radar chart that compares the performance of different models in a specific task, likely related to natural language processing or machine learning. The chart is divided into several axes, each representing a different model or method. The models are color-coded and labeled with their respective names. The axes are labeled with terms such as \"VQA,\" \"GQA,\" \"MQA,\" \"VIZ,\" \"TextVQA,\" \"SQA-IMG,\" and \"MQE.\" The radar chart shows'  # fmt: skip\n+\n+        self.assertEqual(\n+            self.processor.decode(output[0], skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_small_model_integration_test_batch(self):\n+        model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n+            \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\", torch_dtype=\"float16\", device_map=torch_device\n+        )\n+\n+        inputs = self.processor(\n+            text=[self.prompt_image, self.prompt_video],\n+            images=self.image,\n+            videos=self.video,\n+            return_tensors=\"pt\",\n+            padding=True,\n+        ).to(torch_device, torch.float16)\n+\n+        output = model.generate(**inputs, max_new_tokens=20)\n+\n+        EXPECTED_DECODED_TEXT = ['user\\n\\nWhat do you see in this image?\\nassistant\\nThe image is a radar chart that compares the performance of different models in a specific task, likely related', 'user\\n\\nWhat do you see in this video?\\nassistant\\nA child wearing a light blue sleeveless top and pink pants is seen sitting on a bed, eng']  # fmt: skip\n+\n+        self.assertEqual(\n+            self.processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_small_model_integration_test_video(self):\n+        # related to (#29835)\n+        model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n+            \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\",\n+            torch_dtype=\"float16\",\n+            device_map=torch_device,\n+        )\n+\n+        inputs = self.processor(text=self.prompt_video, videos=self.video, return_tensors=\"pt\").to(\n+            torch_device, torch.float16\n+        )\n+\n+        # verify generation\n+        output = model.generate(**inputs, max_new_tokens=40)\n+        EXPECTED_DECODED_TEXT = 'user\\n\\nWhat do you see in this video?\\nassistant\\nA child wearing a light blue sleeveless top and pink pants is seen sitting on a bed, engrossed in reading a book.'  # fmt: skip\n+\n+        self.assertEqual(\n+            self.processor.decode(output[0], skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_small_model_integration_test_multi_image(self):\n+        # related to (#29835)\n+        model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n+            \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\",\n+            torch_dtype=\"float16\",\n+            device_map=torch_device,\n+        )\n+\n+        url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n+        image = Image.open(requests.get(url, stream=True).raw)\n+        prompt = (\n+            \"user\\n<image><image>\\nWhat is the difference between these images?<|im_end|>\\n<|im_start|>assistant\\n\"\n+        )\n+        inputs = self.processor(text=prompt, images=[self.image, image], return_tensors=\"pt\").to(\n+            torch_device, torch.float16\n+        )\n+\n+        # verify generation\n+        output = model.generate(**inputs, max_new_tokens=40)\n+        EXPECTED_DECODED_TEXT = \"user\\n\\nWhat is the difference between these images?\\nassistant\\nThe images you've provided appear to be related to a graphical representation of a radar chart, which is a type of data visualization used to show the distribution of a particular variable across a geographic area. The\"  # fmt: skip\n+\n+        self.assertEqual(\n+            self.processor.decode(output[0], skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_small_model_integration_test_multi_video(self):\n+        # related to (#29835)\n+        model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n+            \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\",\n+            torch_dtype=\"float16\",\n+            device_map=torch_device,\n+        )\n+\n+        prompt = \"user\\n<video><video>\\nAre these videos identical?<|im_end|>\\n<|im_start|>assistant\\n\"\n+        inputs = self.processor(text=prompt, videos=[self.video, self.video], return_tensors=\"pt\").to(\n+            torch_device, torch.float16\n+        )\n+\n+        # verify generation\n+        output = model.generate(**inputs, max_new_tokens=40)\n+        EXPECTED_DECODED_TEXT = \"user\\n\\nAre these videos identical?\\nassistant\\nNo, the video is not identical; it shows slight variations in the child's actions and the background.\"  # fmt: skip\n+\n+        self.assertEqual(\n+            self.processor.decode(output[0], skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_small_model_integration_test_batch_different_resolutions(self):\n+        model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n+            \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\", torch_dtype=\"float16\", device_map=torch_device\n+        )\n+\n+        url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        lowres_url = \"https://4.img-dpreview.com/files/p/TS560x560~forums/56876524/03975b28741443319e9a94615e35667e\"\n+        cats_image = Image.open(requests.get(url, stream=True).raw)\n+        lowres_img = Image.open(requests.get(lowres_url, stream=True).raw)\n+\n+        inputs = self.processor(\n+            text=[self.prompt_image, self.prompt_image],\n+            images=[lowres_img, cats_image],\n+            return_tensors=\"pt\",\n+            padding=True,\n+        ).to(torch_device, torch.float16)\n+\n+        # verify generation\n+        output = model.generate(**inputs, max_new_tokens=50)\n+        EXPECTED_DECODED_TEXT = ['user\\n\\nWhat do you see in this image?\\nassistant\\nThe image shows a scene from a wildlife camera, likely a security camera, capturing a moment in a natural setting. It features two deer, one larger and one smaller, grazing on the grass. The environment is foggy, suggesting early morning or late', 'user\\n\\nWhat do you see in this image?\\nassistant\\nIn the tranquil setting of this image, two cats are enjoying a peaceful nap on a vibrant pink blanket. The cat on the left, with its gray and black striped fur, is lying on its side, its head comfortably resting on the blanket. Its']  # fmt: skip\n+        self.assertEqual(\n+            self.processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_small_model_integration_test_batch_matches_single(self):\n+        model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n+            \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\",\n+            torch_dtype=\"float16\",\n+            device_map=torch_device,\n+        )\n+\n+        url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        lowres_url = \"https://4.img-dpreview.com/files/p/TS560x560~forums/56876524/03975b28741443319e9a94615e35667e\"\n+        cats_image = Image.open(requests.get(url, stream=True).raw)\n+        lowres_img = Image.open(requests.get(lowres_url, stream=True).raw)\n+\n+        inputs_batched = self.processor(\n+            text=[self.prompt_image, self.prompt_image],\n+            images=[lowres_img, cats_image],\n+            return_tensors=\"pt\",\n+            padding=True,\n+        ).to(torch_device, torch.float16)\n+\n+        inputs_single = self.processor(\n+            text=self.prompt_image, images=lowres_img, return_tensors=\"pt\", padding=True\n+        ).to(torch_device, torch.float16)\n+\n+        # verify generation\n+        output_batched = model.generate(**inputs_batched, max_new_tokens=50)\n+        output_single = model.generate(**inputs_single, max_new_tokens=50)\n+\n+        self.assertEqual(\n+            self.processor.decode(output_batched[0], skip_special_tokens=True),\n+            self.processor.decode(output_single[0], skip_special_tokens=True),\n+        )"
        },
        {
            "sha": "e045f2ba7f0ba2b4317fc6a4012ece369a0ded76",
            "filename": "tests/models/llava_onevision/test_processing_llava_onevision.py",
            "status": "added",
            "additions": 277,
            "deletions": 0,
            "changes": 277,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/tests%2Fmodels%2Fllava_onevision%2Ftest_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/tests%2Fmodels%2Fllava_onevision%2Ftest_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_processing_llava_onevision.py?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -0,0 +1,277 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import shutil\n+import tempfile\n+import unittest\n+\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_vision_available():\n+    from transformers import (\n+        AutoProcessor,\n+        LlavaOnevisionImageProcessor,\n+        LlavaOnevisionProcessor,\n+        LlavaOnevisionVideoProcessor,\n+        Qwen2TokenizerFast,\n+    )\n+\n+\n+@require_vision\n+class LlavaOnevisionProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = LlavaOnevisionProcessor\n+\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+        image_processor = LlavaOnevisionImageProcessor()\n+        video_processor = LlavaOnevisionVideoProcessor()\n+        tokenizer = Qwen2TokenizerFast.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n+\n+        processor = LlavaOnevisionProcessor(\n+            video_processor=video_processor, image_processor=image_processor, tokenizer=tokenizer\n+        )\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    def get_Video_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).video_processor\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n+\n+    def test_chat_template(self):\n+        processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\")\n+        expected_prompt = \"<|im_start|>user <image>\\nWhat is shown in this image?<|im_end|><|im_start|>assistant\\n\"\n+\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                ],\n+            },\n+        ]\n+\n+        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n+        self.assertEqual(expected_prompt, formatted_prompt)\n+\n+    @require_torch\n+    @require_vision\n+    def test_image_processor_defaults_preserved_by_image_kwargs(self):\n+        # Rewrite as llava-next image processor return pixel values with an added dimesion for image patches\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\", size=(234, 234))\n+        video_processor = self.get_component(\"video_processor\", size=(234, 234))\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117)\n+\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input)\n+        # added dimension for image patches\n+        self.assertEqual(len(inputs[\"pixel_values\"][0][0][0]), 234)\n+\n+    @require_torch\n+    @require_vision\n+    def test_kwargs_overrides_default_image_processor_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\", crop_size=(234, 234))\n+        video_processor = self.get_component(\"video_processor\", size=(234, 234))\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117)\n+\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input, size=[224, 224])\n+        # added dimension for image patches\n+        self.assertEqual(len(inputs[\"pixel_values\"][0][0][0]), 224)\n+\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs(self):\n+        image_processor = self.get_component(\"image_processor\")\n+        video_processor = self.get_component(\"video_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            size={\"height\": 214, \"width\": 214},\n+            padding=\"max_length\",\n+            max_length=76,\n+        )\n+\n+        # added dimension for image patches\n+        self.assertEqual(inputs[\"pixel_values\"].shape[3], 214)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs_batched(self):\n+        image_processor = self.get_component(\"image_processor\")\n+        video_processor = self.get_component(\"video_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = [\"lower newer\", \"upper older longer string\"]\n+        image_input = self.prepare_image_inputs() * 2\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            size={\"height\": 214, \"width\": 214},\n+            padding=\"longest\",\n+            max_length=76,\n+        )\n+        self.assertEqual(inputs[\"pixel_values\"].shape[3], 214)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 5)\n+\n+    @require_torch\n+    @require_vision\n+    def test_structured_kwargs_nested(self):\n+        image_processor = self.get_component(\"image_processor\")\n+        video_processor = self.get_component(\"video_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\"size\": {\"height\": 214, \"width\": 214}},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[3], 214)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    @require_torch\n+    @require_vision\n+    def test_structured_kwargs_nested_from_dict(self):\n+        image_processor = self.get_component(\"image_processor\")\n+        video_processor = self.get_component(\"video_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\"size\": {\"height\": 214, \"width\": 214}},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+        self.assertEqual(inputs[\"pixel_values\"].shape[3], 214)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    @require_torch\n+    @require_vision\n+    def test_doubly_passed_kwargs(self):\n+        image_processor = self.get_component(\"image_processor\")\n+        video_processor = self.get_component(\"video_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = [\"lower newer\"]\n+        image_input = self.prepare_image_inputs()\n+        with self.assertRaises(ValueError):\n+            _ = processor(\n+                text=input_str,\n+                images=image_input,\n+                images_kwargs={\"size\": {\"height\": 222, \"width\": 222}},\n+                size={\"height\": 214, \"width\": 214},\n+            )\n+\n+    @require_vision\n+    @require_torch\n+    def test_kwargs_overrides_default_tokenizer_kwargs(self):\n+        image_processor = self.get_component(\"image_processor\")\n+        video_processor = self.get_component(\"video_processor\")\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117)\n+\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\", max_length=112)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 112)\n+\n+    @require_vision\n+    @require_torch\n+    def test_tokenizer_defaults_preserved_by_kwargs(self):\n+        image_processor = self.get_component(\"image_processor\")\n+        video_processor = self.get_component(\"video_processor\")\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117)\n+\n+        processor = self.processor_class(\n+            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n+        )\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 117)"
        },
        {
            "sha": "6290289a8cac2cb44c3f4af3d3a746e2c7b656e1",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/43df47d8e78238021a4273746fc469336f948314/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43df47d8e78238021a4273746fc469336f948314/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=43df47d8e78238021a4273746fc469336f948314",
            "patch": "@@ -4808,6 +4808,8 @@ def test_forward_with_num_logits_to_keep(self):\n             batch_size, sequence_length = inputs[\"input_ids\"].shape\n             vocab_size = config.get_text_config().vocab_size\n             model = model_class(config).to(device=torch_device).eval()\n+            # some models have labels but `num_logits_to_keep` should not be used in train mode\n+            _ = inputs.pop(\"labels\", None)\n \n             # num_logits_to_keep=0 is a special case meaning \"keep all logits\"\n             all_logits = model(**inputs, num_logits_to_keep=0).logits"
        }
    ],
    "stats": {
        "total": 4166,
        "additions": 4157,
        "deletions": 9
    }
}