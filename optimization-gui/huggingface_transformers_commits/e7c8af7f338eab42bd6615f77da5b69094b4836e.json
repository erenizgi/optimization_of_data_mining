{
    "author": "OmarManzoor",
    "message": "Add sdpa for DistilBert (#33724)\n\n* Add sdpa for DistilBert\r\n\r\n* [run_slow] distilbert\r\n\r\n* [run_slow] distilbert\r\n\r\n* [run_slow] distilbert\r\n\r\n* Try without slow tests\r\n\r\n* [run_slow] distilbert\r\n\r\n* [run_slow] distilbert",
    "sha": "e7c8af7f338eab42bd6615f77da5b69094b4836e",
    "files": [
        {
            "sha": "10f7c2d757a21ad0eafed836d48d8c8bc71e9146",
            "filename": "docs/source/en/model_doc/distilbert.md",
            "status": "modified",
            "additions": 47,
            "deletions": 0,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/e7c8af7f338eab42bd6615f77da5b69094b4836e/docs%2Fsource%2Fen%2Fmodel_doc%2Fdistilbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e7c8af7f338eab42bd6615f77da5b69094b4836e/docs%2Fsource%2Fen%2Fmodel_doc%2Fdistilbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdistilbert.md?ref=e7c8af7f338eab42bd6615f77da5b69094b4836e",
            "patch": "@@ -66,6 +66,53 @@ contributed by [kamalkraj](https://huggingface.co/kamalkraj). The original code\n     * predicting the masked tokens correctly (but no next-sentence objective)\n     * a cosine similarity between the hidden states of the student and the teacher model\n \n+### Using Scaled Dot Product Attention (SDPA)\n+\n+PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function \n+encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the \n+[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) \n+or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n+page for more information.\n+\n+SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set \n+`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n+\n+```\n+from transformers import DistilBertModel\n+model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n+```\n+\n+For the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n+\n+On a local benchmark (NVIDIA GeForce RTX 2060-8GB, PyTorch 2.3.1, OS Ubuntu 20.04) with `float16` and the `distilbert-base-uncased` model with\n+a MaskedLM head, we saw the following speedups during training and inference.\n+\n+#### Training\n+\n+| num_training_steps | batch_size | seq_len | is cuda | Time per batch (eager - s) | Time per batch (sdpa - s) | Speedup (%) | Eager peak mem (MB) | sdpa peak mem (MB) | Mem saving (%) |\n+|--------------------|------------|---------|---------|----------------------------|---------------------------|-------------|---------------------|--------------------|----------------|\n+| 100                | 1          | 128     | False   | 0.010                      | 0.008                     | 28.870      | 397.038             | 399.629            | -0.649         |\n+| 100                | 1          | 256     | False   | 0.011                      | 0.009                     | 20.681      | 412.505             | 412.606            | -0.025         |\n+| 100                | 2          | 128     | False   | 0.011                      | 0.009                     | 23.741      | 412.213             | 412.606            | -0.095         |\n+| 100                | 2          | 256     | False   | 0.015                      | 0.013                     | 16.502      | 427.491             | 425.787            | 0.400          |\n+| 100                | 4          | 128     | False   | 0.015                      | 0.013                     | 13.828      | 427.491             | 425.787            | 0.400          |\n+| 100                | 4          | 256     | False   | 0.025                      | 0.022                     | 12.882      | 594.156             | 502.745            | 18.182         |\n+| 100                | 8          | 128     | False   | 0.023                      | 0.022                     | 8.010       | 545.922             | 502.745            | 8.588          |\n+| 100                | 8          | 256     | False   | 0.046                      | 0.041                     | 12.763      | 983.450             | 798.480            | 23.165         |\n+\n+#### Inference\n+\n+| num_batches | batch_size | seq_len | is cuda | is half | use mask | Per token latency eager (ms) | Per token latency SDPA (ms) | Speedup (%) | Mem eager (MB) | Mem BT (MB) | Mem saved (%) |\n+|-------------|------------|---------|---------|---------|----------|-----------------------------|-----------------------------|-------------|----------------|--------------|---------------|\n+| 50          | 2          | 64      | True    | True    | True     | 0.032                       | 0.025                       | 28.192      | 154.532        | 155.531      | -0.642        |\n+| 50          | 2          | 128     | True    | True    | True     | 0.033                       | 0.025                       | 32.636      | 157.286        | 157.482      | -0.125        |\n+| 50          | 4          | 64      | True    | True    | True     | 0.032                       | 0.026                       | 24.783      | 157.023        | 157.449      | -0.271        |\n+| 50          | 4          | 128     | True    | True    | True     | 0.034                       | 0.028                       | 19.299      | 162.794        | 162.269      | 0.323         |\n+| 50          | 8          | 64      | True    | True    | True     | 0.035                       | 0.028                       | 25.105      | 160.958        | 162.204      | -0.768        |\n+| 50          | 8          | 128     | True    | True    | True     | 0.052                       | 0.046                       | 12.375      | 173.155        | 171.844      | 0.763         |\n+| 50          | 16         | 64      | True    | True    | True     | 0.051                       | 0.045                       | 12.882      | 172.106        | 171.713      | 0.229         |\n+| 50          | 16         | 128     | True    | True    | True     | 0.096                       | 0.081                       | 18.524      | 191.257        | 191.517      | -0.136        |\n+\n \n ## Resources\n "
        },
        {
            "sha": "ed3b26029d00943ca872a9f132f1bc09be163d63",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e7c8af7f338eab42bd6615f77da5b69094b4836e/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e7c8af7f338eab42bd6615f77da5b69094b4836e/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=e7c8af7f338eab42bd6615f77da5b69094b4836e",
            "patch": "@@ -219,6 +219,7 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [Dbrx](https://huggingface.co/docs/transformers/model_doc/dbrx#transformers.DbrxModel)\n * [DeiT](https://huggingface.co/docs/transformers/model_doc/deit#transformers.DeiTModel)\n * [Dinov2](https://huggingface.co/docs/transformers/en/model_doc/dinov2)\n+* [DistilBert](https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertModel)\n * [Dpr](https://huggingface.co/docs/transformers/model_doc/dpr#transformers.DprReader)\n * [Falcon](https://huggingface.co/docs/transformers/model_doc/falcon#transformers.FalconModel)\n * [Gemma](https://huggingface.co/docs/transformers/model_doc/gemma#transformers.GemmaModel)"
        },
        {
            "sha": "36e35594b3d3c6d36447849ef46a623c83a69267",
            "filename": "src/transformers/models/distilbert/modeling_distilbert.py",
            "status": "modified",
            "additions": 96,
            "deletions": 1,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/e7c8af7f338eab42bd6615f77da5b69094b4836e/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e7c8af7f338eab42bd6615f77da5b69094b4836e/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py?ref=e7c8af7f338eab42bd6615f77da5b69094b4836e",
            "patch": "@@ -29,6 +29,7 @@\n from ...activations import get_activation\n from ...configuration_utils import PretrainedConfig\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa\n from ...modeling_outputs import (\n     BaseModelOutput,\n     MaskedLMOutput,\n@@ -38,7 +39,12 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import (\n+    apply_chunking_to_forward,\n+    find_pruneable_heads_and_indices,\n+    is_torch_greater_or_equal_than_2_2,\n+    prune_linear_layer,\n+)\n from ...utils import (\n     add_code_sample_docstrings,\n     add_start_docstrings,\n@@ -329,6 +335,86 @@ def reshape(x: torch.Tensor) -> torch.Tensor:\n             return (attn_output,)\n \n \n+class DistilBertSdpaAttention(MultiHeadSelfAttention):\n+    def __init__(self, config: PretrainedConfig):\n+        super().__init__(config=config)\n+        self.dropout_prob = config.attention_dropout\n+        self.require_contiguous_qkv = not is_torch_greater_or_equal_than_2_2\n+\n+    def forward(\n+        self,\n+        query: torch.Tensor,\n+        key: torch.Tensor,\n+        value: torch.Tensor,\n+        mask: torch.Tensor,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+    ) -> Tuple[torch.Tensor, ...]:\n+        \"\"\"\n+        Parameters:\n+            query: torch.tensor(bs, seq_length, dim)\n+            key: torch.tensor(bs, seq_length, dim)\n+            value: torch.tensor(bs, seq_length, dim)\n+            mask: torch.tensor(bs, seq_length)\n+\n+        Returns:\n+            weights: torch.tensor(bs, n_heads, seq_length, seq_length) Attention weights context: torch.tensor(bs,\n+            seq_length, dim) Contextualized layer. Optional: only if `output_attentions=True`\n+        \"\"\"\n+        if output_attentions or head_mask is not None:\n+            logger.warning_once(\n+                \"DistilBertSdpaAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support\"\n+                \" `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying\"\n+                \" the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be\"\n+                ' removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                query,\n+                key,\n+                value,\n+                mask,\n+                head_mask,\n+                output_attentions,\n+            )\n+\n+        batch_size, _, _ = query.size()\n+        dim_per_head = self.dim // self.n_heads\n+\n+        def shape(x: torch.Tensor) -> torch.Tensor:\n+            \"\"\"separate heads\"\"\"\n+            return x.view(batch_size, -1, self.n_heads, dim_per_head).transpose(1, 2)\n+\n+        def unshape(x: torch.Tensor) -> torch.Tensor:\n+            \"\"\"group heads\"\"\"\n+            return x.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * dim_per_head)\n+\n+        q = shape(self.q_lin(query))  # (bs, n_heads, q_length, dim_per_head)\n+        k = shape(self.k_lin(key))  # (bs, n_heads, k_length, dim_per_head)\n+        v = shape(self.v_lin(value))  # (bs, n_heads, k_length, dim_per_head)\n+\n+        # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n+        # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577\n+        if self.require_contiguous_qkv and q.device.type == \"cuda\" and mask is not None:\n+            q = q.contiguous()\n+            k = k.contiguous()\n+            v = v.contiguous()\n+\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+            q,\n+            k,\n+            v,\n+            attn_mask=mask,\n+            dropout_p=self.dropout_prob if self.training else 0.0,\n+            is_causal=False,\n+        )\n+\n+        attn_output = unshape(attn_output)\n+        attn_output = self.out_lin(attn_output)\n+\n+        return (attn_output,)\n+\n+\n class FFN(nn.Module):\n     def __init__(self, config: PretrainedConfig):\n         super().__init__()\n@@ -353,6 +439,7 @@ def ff_chunk(self, input: torch.Tensor) -> torch.Tensor:\n DISTILBERT_ATTENTION_CLASSES = {\n     \"eager\": MultiHeadSelfAttention,\n     \"flash_attention_2\": DistilBertFlashAttention2,\n+    \"sdpa\": DistilBertSdpaAttention,\n }\n \n \n@@ -503,6 +590,7 @@ class DistilBertPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"distilbert\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n \n     def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights.\"\"\"\n@@ -589,6 +677,7 @@ def __init__(self, config: PretrainedConfig):\n         self.embeddings = Embeddings(config)  # Embeddings\n         self.transformer = Transformer(config)  # Encoder\n         self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n+        self._use_sdpa = config._attn_implementation == \"sdpa\"\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -689,6 +778,7 @@ def forward(\n \n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n+        head_mask_is_none = head_mask is None\n         # Prepare head mask if needed\n         head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n \n@@ -700,6 +790,11 @@ def forward(\n             if attention_mask is None:\n                 attention_mask = torch.ones(input_shape, device=device)  # (bs, seq_length)\n \n+            if self._use_sdpa and head_mask_is_none and not output_attentions:\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    attention_mask, embeddings.dtype, tgt_len=input_shape[1]\n+                )\n+\n         return self.transformer(\n             x=embeddings,\n             attn_mask=attention_mask,"
        }
    ],
    "stats": {
        "total": 145,
        "additions": 144,
        "deletions": 1
    }
}