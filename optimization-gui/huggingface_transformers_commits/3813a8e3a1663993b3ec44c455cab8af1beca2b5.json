{
    "author": "Aki-07",
    "message": "Add VideoMAE video processor  (#41534)\n\n* Add video processor for VideoMAE\n\n* Document VideoMAE video processor\n\n* Add regression tests for VideoMAE video processor\n\n* refactor: Use direct batch key access for pixel_values_videos\n\n* test: add parity test for VideoMAEVideoProcessor vs VideoMAEImageProcessor\n\n* docs(videomae): update model docstring example to demonstrate VideoMAEVideoProcessor (TorchCodec-based decoding and sampling)",
    "sha": "3813a8e3a1663993b3ec44c455cab8af1beca2b5",
    "files": [
        {
            "sha": "74afaf7fdcadafa17a7059fb58ca95728ee95583",
            "filename": "docs/source/en/model_doc/videomae.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3813a8e3a1663993b3ec44c455cab8af1beca2b5/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideomae.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3813a8e3a1663993b3ec44c455cab8af1beca2b5/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideomae.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideomae.md?ref=3813a8e3a1663993b3ec44c455cab8af1beca2b5",
            "patch": "@@ -90,6 +90,11 @@ to fine-tune a VideoMAE model on a custom dataset.\n [[autodoc]] VideoMAEImageProcessor\n     - preprocess\n \n+## VideoMAEVideoProcessor\n+\n+[[autodoc]] VideoMAEVideoProcessor\n+    - preprocess\n+\n ## VideoMAEModel\n \n [[autodoc]] VideoMAEModel"
        },
        {
            "sha": "78956269331f9d6fe0f5224e50c16ced002ef3a1",
            "filename": "src/transformers/models/auto/video_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3813a8e3a1663993b3ec44c455cab8af1beca2b5/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3813a8e3a1663993b3ec44c455cab8af1beca2b5/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py?ref=3813a8e3a1663993b3ec44c455cab8af1beca2b5",
            "patch": "@@ -62,6 +62,7 @@\n             (\"sam2_video\", \"Sam2VideoVideoProcessor\"),\n             (\"smolvlm\", \"SmolVLMVideoProcessor\"),\n             (\"video_llava\", \"VideoLlavaVideoProcessor\"),\n+            (\"videomae\", \"VideoMAEVideoProcessor\"),\n             (\"vjepa2\", \"VJEPA2VideoProcessor\"),\n         ]\n     )"
        },
        {
            "sha": "4a89abf714d55456934891db08cc2344327bf90d",
            "filename": "src/transformers/models/videomae/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3813a8e3a1663993b3ec44c455cab8af1beca2b5/src%2Ftransformers%2Fmodels%2Fvideomae%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3813a8e3a1663993b3ec44c455cab8af1beca2b5/src%2Ftransformers%2Fmodels%2Fvideomae%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2F__init__.py?ref=3813a8e3a1663993b3ec44c455cab8af1beca2b5",
            "patch": "@@ -22,6 +22,7 @@\n     from .feature_extraction_videomae import *\n     from .image_processing_videomae import *\n     from .modeling_videomae import *\n+    from .video_processing_videomae import *\n else:\n     import sys\n "
        },
        {
            "sha": "fab1c5b5f7e04b0e34bff337c185e2b494488190",
            "filename": "src/transformers/models/videomae/modeling_videomae.py",
            "status": "modified",
            "additions": 14,
            "deletions": 111,
            "changes": 125,
            "blob_url": "https://github.com/huggingface/transformers/blob/3813a8e3a1663993b3ec44c455cab8af1beca2b5/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3813a8e3a1663993b3ec44c455cab8af1beca2b5/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py?ref=3813a8e3a1663993b3ec44c455cab8af1beca2b5",
            "patch": "@@ -440,72 +440,25 @@ def forward(\n         Examples:\n \n         ```python\n-        >>> import av\n-        >>> import numpy as np\n-\n-        >>> from transformers import AutoImageProcessor, VideoMAEModel\n+        >>> import torch\n+        >>> from transformers import VideoMAEVideoProcessor, VideoMAEModel\n         >>> from huggingface_hub import hf_hub_download\n \n-        >>> np.random.seed(0)\n-\n-\n-        >>> def read_video_pyav(container, indices):\n-        ...     '''\n-        ...     Decode the video with PyAV decoder.\n-        ...     Args:\n-        ...         container (`av.container.input.InputContainer`): PyAV container.\n-        ...         indices (`list[int]`): List of frame indices to decode.\n-        ...     Returns:\n-        ...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n-        ...     '''\n-        ...     frames = []\n-        ...     container.seek(0)\n-        ...     start_index = indices[0]\n-        ...     end_index = indices[-1]\n-        ...     for i, frame in enumerate(container.decode(video=0)):\n-        ...         if i > end_index:\n-        ...             break\n-        ...         if i >= start_index and i in indices:\n-        ...             frames.append(frame)\n-        ...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n-\n-\n-        >>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n-        ...     '''\n-        ...     Sample a given number of frame indices from the video.\n-        ...     Args:\n-        ...         clip_len (`int`): Total number of frames to sample.\n-        ...         frame_sample_rate (`int`): Sample every n-th frame.\n-        ...         seg_len (`int`): Maximum allowed index of sample's last frame.\n-        ...     Returns:\n-        ...         indices (`list[int]`): List of sampled frame indices\n-        ...     '''\n-        ...     converted_len = int(clip_len * frame_sample_rate)\n-        ...     end_idx = np.random.randint(converted_len, seg_len)\n-        ...     start_idx = end_idx - converted_len\n-        ...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n-        ...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n-        ...     return indices\n-\n-\n-        >>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n-        >>> file_path = hf_hub_download(\n+        >>> # replace this with your own video file\n+        >>> video_path = hf_hub_download(\n         ...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n         ... )\n-        >>> container = av.open(file_path)\n \n-        >>> # sample 16 frames\n-        >>> indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n-        >>> video = read_video_pyav(container, indices)\n-\n-        >>> image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n+        >>> video_processor = VideoMAEVideoProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n         >>> model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\n \n         >>> # prepare video for the model\n-        >>> inputs = image_processor(list(video), return_tensors=\"pt\")\n+        >>> inputs = video_processor(video_path, return_tensors=\"pt\")\n \n         >>> # forward pass\n-        >>> outputs = model(**inputs)\n+        >>> with torch.no_grad():\n+        ...     outputs = model(**inputs)\n+\n         >>> last_hidden_states = outputs.last_hidden_state\n         >>> list(last_hidden_states.shape)\n         [1, 1568, 768]\n@@ -764,69 +717,19 @@ def forward(\n         Examples:\n \n         ```python\n-        >>> import av\n         >>> import torch\n-        >>> import numpy as np\n-\n-        >>> from transformers import AutoImageProcessor, VideoMAEForVideoClassification\n+        >>> from transformers import VideoMAEVideoProcessor, VideoMAEForVideoClassification\n         >>> from huggingface_hub import hf_hub_download\n \n-        >>> np.random.seed(0)\n-\n-\n-        >>> def read_video_pyav(container, indices):\n-        ...     '''\n-        ...     Decode the video with PyAV decoder.\n-        ...     Args:\n-        ...         container (`av.container.input.InputContainer`): PyAV container.\n-        ...         indices (`list[int]`): List of frame indices to decode.\n-        ...     Returns:\n-        ...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n-        ...     '''\n-        ...     frames = []\n-        ...     container.seek(0)\n-        ...     start_index = indices[0]\n-        ...     end_index = indices[-1]\n-        ...     for i, frame in enumerate(container.decode(video=0)):\n-        ...         if i > end_index:\n-        ...             break\n-        ...         if i >= start_index and i in indices:\n-        ...             frames.append(frame)\n-        ...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n-\n-\n-        >>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n-        ...     '''\n-        ...     Sample a given number of frame indices from the video.\n-        ...     Args:\n-        ...         clip_len (`int`): Total number of frames to sample.\n-        ...         frame_sample_rate (`int`): Sample every n-th frame.\n-        ...         seg_len (`int`): Maximum allowed index of sample's last frame.\n-        ...     Returns:\n-        ...         indices (`list[int]`): List of sampled frame indices\n-        ...     '''\n-        ...     converted_len = int(clip_len * frame_sample_rate)\n-        ...     end_idx = np.random.randint(converted_len, seg_len)\n-        ...     start_idx = end_idx - converted_len\n-        ...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n-        ...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n-        ...     return indices\n-\n-\n-        >>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n-        >>> file_path = hf_hub_download(\n+        >>> # replace this with your own video file\n+        >>> video_path = hf_hub_download(\n         ...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n         ... )\n-        >>> container = av.open(file_path)\n-\n-        >>> # sample 16 frames\n-        >>> indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n-        >>> video = read_video_pyav(container, indices)\n \n-        >>> image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n+        >>> video_processor = VideoMAEVideoProcessor.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n         >>> model = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n \n-        >>> inputs = image_processor(list(video), return_tensors=\"pt\")\n+        >>> inputs = video_processor(video_path, return_tensors=\"pt\")\n \n         >>> with torch.no_grad():\n         ...     outputs = model(**inputs)"
        },
        {
            "sha": "6ee491a493e02e9472dc068a05569d215a86a387",
            "filename": "src/transformers/models/videomae/video_processing_videomae.py",
            "status": "added",
            "additions": 43,
            "deletions": 0,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/3813a8e3a1663993b3ec44c455cab8af1beca2b5/src%2Ftransformers%2Fmodels%2Fvideomae%2Fvideo_processing_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3813a8e3a1663993b3ec44c455cab8af1beca2b5/src%2Ftransformers%2Fmodels%2Fvideomae%2Fvideo_processing_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2Fvideo_processing_videomae.py?ref=3813a8e3a1663993b3ec44c455cab8af1beca2b5",
            "patch": "@@ -0,0 +1,43 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Video processor class for VideoMAE.\"\"\"\n+\n+from ...image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD, PILImageResampling\n+from ...video_processing_utils import BaseVideoProcessor\n+\n+\n+class VideoMAEVideoProcessor(BaseVideoProcessor):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    size = {\"shortest_edge\": 224}\n+    default_to_square = False\n+    crop_size = {\"height\": 224, \"width\": 224}\n+    do_resize = True\n+    do_center_crop = True\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    do_normalize = True\n+    do_convert_rgb = True\n+    do_sample_frames = False  # Set to False for backward compatibility with image processor workflows.\n+    model_input_names = [\"pixel_values\"]\n+\n+    def preprocess(self, videos, **kwargs):\n+        batch = super().preprocess(videos, **kwargs)\n+        batch[\"pixel_values\"] = batch.pop(\"pixel_values_videos\")\n+        return batch\n+\n+\n+__all__ = [\"VideoMAEVideoProcessor\"]"
        },
        {
            "sha": "809152216558d862b69bf152e1c43f85ceb9be30",
            "filename": "tests/models/videomae/test_video_processing_videomae.py",
            "status": "added",
            "additions": 160,
            "deletions": 0,
            "changes": 160,
            "blob_url": "https://github.com/huggingface/transformers/blob/3813a8e3a1663993b3ec44c455cab8af1beca2b5/tests%2Fmodels%2Fvideomae%2Ftest_video_processing_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3813a8e3a1663993b3ec44c455cab8af1beca2b5/tests%2Fmodels%2Fvideomae%2Ftest_video_processing_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideomae%2Ftest_video_processing_videomae.py?ref=3813a8e3a1663993b3ec44c455cab8af1beca2b5",
            "patch": "@@ -0,0 +1,160 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import torch\n+from PIL import Image\n+\n+from transformers.image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n+from transformers.testing_utils import require_torch, require_torchvision, require_vision\n+from transformers.utils import is_torchvision_available, is_vision_available\n+\n+from ...test_video_processing_common import VideoProcessingTestMixin, prepare_video_inputs\n+\n+\n+if is_vision_available():\n+    if is_torchvision_available():\n+        from transformers import VideoMAEImageProcessor, VideoMAEVideoProcessor\n+\n+\n+class VideoMAEVideoProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=5,\n+        num_frames=8,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=80,\n+        do_resize=True,\n+        size=None,\n+        do_center_crop=True,\n+        crop_size=None,\n+        do_rescale=True,\n+        rescale_factor=1 / 255,\n+        do_normalize=True,\n+        image_mean=IMAGENET_STANDARD_MEAN,\n+        image_std=IMAGENET_STANDARD_STD,\n+        do_convert_rgb=True,\n+    ):\n+        super().__init__()\n+        size = size if size is not None else {\"shortest_edge\": 20}\n+        crop_size = crop_size if crop_size is not None else {\"height\": 18, \"width\": 18}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_frames = num_frames\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_center_crop = do_center_crop\n+        self.crop_size = crop_size\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_convert_rgb = do_convert_rgb\n+\n+    def prepare_video_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"do_center_crop\": self.do_center_crop,\n+            \"crop_size\": self.crop_size,\n+            \"do_rescale\": self.do_rescale,\n+            \"rescale_factor\": self.rescale_factor,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_convert_rgb\": self.do_convert_rgb,\n+        }\n+\n+    def expected_output_video_shape(self, videos):\n+        return self.num_frames, self.num_channels, self.crop_size[\"height\"], self.crop_size[\"width\"]\n+\n+    def prepare_video_inputs(self, equal_resolution=False, return_tensors=\"pil\"):\n+        videos = prepare_video_inputs(\n+            batch_size=self.batch_size,\n+            num_frames=self.num_frames,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            return_tensors=return_tensors,\n+        )\n+\n+        return videos\n+\n+\n+@require_torch\n+@require_vision\n+@require_torchvision\n+class VideoMAEVideoProcessingTest(VideoProcessingTestMixin, unittest.TestCase):\n+    fast_video_processing_class = VideoMAEVideoProcessor if is_torchvision_available() else None\n+    input_name = \"pixel_values\"\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.video_processor_tester = VideoMAEVideoProcessingTester(self)\n+\n+    @property\n+    def video_processor_dict(self):\n+        return self.video_processor_tester.prepare_video_processor_dict()\n+\n+    def test_video_processor_properties(self):\n+        video_processing = self.fast_video_processing_class(**self.video_processor_dict)\n+        self.assertTrue(hasattr(video_processing, \"do_resize\"))\n+        self.assertTrue(hasattr(video_processing, \"size\"))\n+        self.assertTrue(hasattr(video_processing, \"do_center_crop\"))\n+        self.assertTrue(hasattr(video_processing, \"center_crop\"))\n+        self.assertTrue(hasattr(video_processing, \"do_normalize\"))\n+        self.assertTrue(hasattr(video_processing, \"image_mean\"))\n+        self.assertTrue(hasattr(video_processing, \"image_std\"))\n+        self.assertTrue(hasattr(video_processing, \"do_convert_rgb\"))\n+        self.assertTrue(hasattr(video_processing, \"model_input_names\"))\n+        self.assertIn(\"pixel_values\", video_processing.model_input_names)\n+\n+    def test_pixel_value_identity(self):\n+        \"\"\"\n+        Verify that VideoMAEVideoProcessor (TorchCodec-based) produces pixel tensors\n+        numerically similar to those from VideoMAEImageProcessor (PIL-based).\n+        Minor (<1%) differences are expected due to color conversion and interpolation.\n+        \"\"\"\n+        video = self.video_processor_tester.prepare_video_inputs(return_tensors=\"np\")\n+        video_processor = VideoMAEVideoProcessor(**self.video_processor_dict)\n+        image_processor = VideoMAEImageProcessor(**self.video_processor_dict)\n+\n+        video_frames_np = video[0]\n+        video_frames_pil = [Image.fromarray(frame.astype(\"uint8\")) for frame in video_frames_np]\n+        video_out = video_processor(video_frames_pil, return_tensors=\"pt\")\n+        image_out = image_processor(video_frames_pil, return_tensors=\"pt\")\n+\n+        torch.testing.assert_close(\n+            video_out[\"pixel_values\"],\n+            image_out[\"pixel_values\"],\n+            rtol=5e-2,\n+            atol=1e-2,\n+            msg=(\n+                \"Pixel values differ slightly between VideoMAEVideoProcessor \"\n+                \"and VideoMAEImageProcessor. \"\n+                \"Differences ≤1% are expected due to YUV→RGB conversion and \"\n+                \"interpolation behavior in different decoders.\"\n+            ),\n+        )"
        }
    ],
    "stats": {
        "total": 335,
        "additions": 224,
        "deletions": 111
    }
}