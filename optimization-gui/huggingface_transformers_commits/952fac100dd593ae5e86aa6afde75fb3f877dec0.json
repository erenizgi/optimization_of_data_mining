{
    "author": "cyyever",
    "message": "Enable SIM rules (#39806)\n\n* Enable SIM rules\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* More fixes\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "952fac100dd593ae5e86aa6afde75fb3f877dec0",
    "files": [
        {
            "sha": "b8f4676a2b46b8188619924e133d1ef240a7ab4a",
            "filename": "pyproject.toml",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/pyproject.toml",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/pyproject.toml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/pyproject.toml?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -19,10 +19,13 @@ line-length = 119\n \n [tool.ruff.lint]\n # Never enforce `E501` (line length violations).\n-ignore = [\"C901\", \"E501\", \"E741\", \"F402\", \"F823\"]\n+# SIM300: Yoda condition detected\n+# SIM212: Checks for if expressions that check against a negated condition.\n+# SIM905: Consider using a list literal instead of `str.split`\n+ignore = [\"C901\", \"E501\", \"E741\", \"F402\", \"F823\", \"SIM1\", \"SIM300\", \"SIM212\", \"SIM905\"]\n # RUF013: Checks for the use of implicit Optional\n #  in type annotations when the default parameter value is None.\n-select = [\"C\", \"E\", \"F\", \"I\", \"W\", \"RUF013\", \"UP006\", \"PERF102\", \"PLC1802\", \"PLC0208\"]\n+select = [\"C\", \"E\", \"F\", \"I\", \"W\", \"RUF013\", \"UP006\", \"PERF102\", \"PLC1802\", \"PLC0208\",\"SIM\"]\n extend-safe-fixes = [\"UP006\"]\n \n # Ignore import violations in all `__init__.py` files."
        },
        {
            "sha": "801901896d1221a206079ae90141d736cac83318",
            "filename": "src/transformers/commands/serving.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fcommands%2Fserving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fcommands%2Fserving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fserving.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -901,7 +901,7 @@ def generate_chat_completion(self, req: dict) -> Generator[str, None, None]:\n         inputs = processor.apply_chat_template(\n             processor_inputs,\n             add_generation_prompt=True,\n-            tools=req.get(\"tools\", None),\n+            tools=req.get(\"tools\"),\n             return_tensors=\"pt\",\n             return_dict=True,\n             tokenize=True,"
        },
        {
            "sha": "92dbacbe5d036b76cb1d52cabf6638b736f73494",
            "filename": "src/transformers/data/data_collator.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdata_collator.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -183,7 +183,7 @@ def tf_default_data_collator(features: list[InputDataClass]) -> dict[str, Any]:\n     if label_col_name is not None:\n         if isinstance(first[label_col_name], tf.Tensor):\n             dtype = tf.int64 if first[label_col_name].dtype.is_integer else tf.float32\n-        elif isinstance(first[label_col_name], np.ndarray) or isinstance(first[label_col_name], np.generic):\n+        elif isinstance(first[label_col_name], (np.ndarray, np.generic)):\n             dtype = tf.int64 if np.issubdtype(first[label_col_name].dtype, np.integer) else tf.float32\n         elif isinstance(first[label_col_name], (tuple, list)):\n             dtype = tf.int64 if isinstance(first[label_col_name][0], int) else tf.float32"
        },
        {
            "sha": "177cfeeda6a6e218922f8894eaf627c34347e382",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -647,8 +647,8 @@ def prepare_inputs_for_generation(\n \n             # If it's not defined, it means the model uses the new general mask API\n             if causal_mask_creation_function is None:  # can't be found\n-                token_type_ids = model_inputs.get(\"token_type_ids\", None)\n-                position_ids = model_inputs.get(position_ids_key, None)\n+                token_type_ids = model_inputs.get(\"token_type_ids\")\n+                position_ids = model_inputs.get(position_ids_key)\n                 # Some models may overwrite the general one\n                 causal_mask_creation_function = getattr(self, \"create_masks_for_generate\", create_masks_for_generate)\n                 attention_mask = causal_mask_creation_function("
        },
        {
            "sha": "298a11551787f94bde399bbd61b33ad675a21756",
            "filename": "src/transformers/integrations/flex_attention.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflex_attention.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -284,7 +284,7 @@ def score_mod(score, batch_idx, head_idx, q_idx, kv_idx):\n     num_local_query_heads = query.shape[1]\n \n     # When running TP this helps:\n-    if not ((num_local_query_heads & (num_local_query_heads - 1)) == 0):\n+    if (num_local_query_heads & (num_local_query_heads - 1)) != 0:\n         key = repeat_kv(key, query.shape[1] // key.shape[1])\n         value = repeat_kv(value, query.shape[1] // value.shape[1])\n         enable_gqa = False"
        },
        {
            "sha": "00b55c6bb7b8ce622287077754e4e0884541066a",
            "filename": "src/transformers/integrations/vptq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fintegrations%2Fvptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fintegrations%2Fvptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fvptq.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -45,7 +45,7 @@ def replace_with_vptq_linear(\n             should not be passed by the user.\n     \"\"\"\n \n-    modules_to_not_convert = [\"lm_head\"] if not modules_to_not_convert else modules_to_not_convert\n+    modules_to_not_convert = modules_to_not_convert if modules_to_not_convert else [\"lm_head\"]\n \n     for name, module in model.named_children():\n         if current_key_name is None:"
        },
        {
            "sha": "ab7fc4615b473d59c903260a8c1ec80b24f4af7b",
            "filename": "src/transformers/keras_callbacks.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fkeras_callbacks.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fkeras_callbacks.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkeras_callbacks.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -167,7 +167,7 @@ def _postprocess_predictions_or_labels(self, inputs):\n             # If it's a dict with only one key, just return the array\n             if len(outputs) == 1:\n                 outputs = list(outputs.values())[0]\n-        elif isinstance(inputs[0], list) or isinstance(inputs[0], tuple):\n+        elif isinstance(inputs[0], (tuple, list)):\n             outputs = []\n             for input_list in zip(*inputs):\n                 outputs.append(self._concatenate_batches(input_list))"
        },
        {
            "sha": "a8c1965ec463f5854656f3b5064b23f0d981cacd",
            "filename": "src/transformers/models/aya_vision/configuration_aya_vision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Faya_vision%2Fconfiguration_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Faya_vision%2Fconfiguration_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fconfiguration_aya_vision.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -81,9 +81,7 @@ def __init__(\n         self.vision_feature_layer = vision_feature_layer\n \n         if isinstance(vision_config, dict):\n-            vision_config[\"model_type\"] = (\n-                vision_config[\"model_type\"] if \"model_type\" in vision_config else \"siglip_vision_model\"\n-            )\n+            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"siglip_vision_model\")\n             vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n         elif vision_config is None:\n             vision_config = CONFIG_MAPPING[\"siglip_vision_model\"](\n@@ -99,7 +97,7 @@ def __init__(\n         self.vision_config = vision_config\n \n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"cohere2\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"cohere2\")\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             text_config = CONFIG_MAPPING[\"cohere2\"]()"
        },
        {
            "sha": "616e9ed6653b17144555ab6a127a29eb5120c40e",
            "filename": "src/transformers/models/biogpt/convert_biogpt_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fconvert_biogpt_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fconvert_biogpt_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fconvert_biogpt_original_pytorch_checkpoint_to_pytorch.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -148,7 +148,7 @@ def rewrite_dict_keys(d):\n     # (1) remove word breaking symbol, (2) add word ending symbol where the word is not broken up,\n     # e.g.: d = {'le@@': 5, 'tt@@': 6, 'er': 7} => {'le': 5, 'tt': 6, 'er</w>': 7}\n     d2 = dict((re.sub(r\"@@$\", \"\", k), v) if k.endswith(\"@@\") else (re.sub(r\"$\", \"</w>\", k), v) for k, v in d.items())\n-    keep_keys = \"<s> <pad> </s> <unk>\".split()\n+    keep_keys = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\"]\n     # restore the special tokens\n     for k in keep_keys:\n         del d2[f\"{k}</w>\"]"
        },
        {
            "sha": "c8077eb8719ecbeb2dcdccfba1035a86f5608355",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -1303,7 +1303,7 @@ def _prepare_model_inputs(\n \n         # Check if conditioning_embeds are provided or not, if yes then concatenate the bos_token_id at the end of the conditioning_embeds.\n         # Then we must subtract the positional_ids because during the forward pass it will be added anyways, so we must cancel them out here.\n-        conditioning_embeds = model_kwargs.get(\"conditioning_embeds\", None)\n+        conditioning_embeds = model_kwargs.get(\"conditioning_embeds\")\n \n         if conditioning_embeds is not None:\n             mel_start_token_embedding = self.model.decoder.input_embeds_layer("
        },
        {
            "sha": "acc40fcf85711961a12f9536e6f4c0c97e94f59e",
            "filename": "src/transformers/models/cohere2_vision/configuration_cohere2_vision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fconfiguration_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fconfiguration_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fconfiguration_cohere2_vision.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -57,9 +57,7 @@ def __init__(\n         self.alignment_intermediate_size = alignment_intermediate_size\n \n         if isinstance(vision_config, dict):\n-            vision_config[\"model_type\"] = (\n-                vision_config[\"model_type\"] if \"model_type\" in vision_config else \"siglip_vision_model\"\n-            )\n+            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"siglip_vision_model\")\n             vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n         elif vision_config is None:\n             vision_config = CONFIG_MAPPING[\"siglip_vision_model\"](\n@@ -73,7 +71,7 @@ def __init__(\n         self.vision_config = vision_config\n \n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"cohere2\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"cohere2\")\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             text_config = CONFIG_MAPPING[\"cohere2\"](tie_word_embeddings=True)"
        },
        {
            "sha": "f82b01de48d1fd1ece52572ad34d55631a9110f3",
            "filename": "src/transformers/models/deepseek_vl/image_processing_deepseek_vl_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -66,7 +66,7 @@ class DeepseekVLImageProcessorFast(BaseImageProcessorFast):\n \n     def __init__(self, **kwargs: Unpack[DeepseekVLFastImageProcessorKwargs]):\n         super().__init__(**kwargs)\n-        if kwargs.get(\"image_mean\", None) is None:\n+        if kwargs.get(\"image_mean\") is None:\n             background_color = (127, 127, 127)\n         else:\n             background_color = tuple([int(x * 255) for x in kwargs.get(\"image_mean\")])"
        },
        {
            "sha": "d0b22c70dcf11462c059d8d750397ef8a542fd45",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -93,11 +93,11 @@ class DeepseekVLHybridImageProcessorFast(BaseImageProcessorFast):\n     high_res_resample = PILImageResampling.BICUBIC\n \n     def __init__(self, **kwargs: Unpack[DeepseekVLHybridFastImageProcessorKwargs]):\n-        if kwargs.get(\"image_mean\", None) is None:\n+        if kwargs.get(\"image_mean\") is None:\n             background_color = (127, 127, 127)\n         else:\n             background_color = tuple([int(x * 255) for x in kwargs.get(\"image_mean\")])\n-        if kwargs.get(\"high_res_image_mean\", None) is None:\n+        if kwargs.get(\"high_res_image_mean\") is None:\n             high_res_background_color = (127, 127, 127)\n         else:\n             high_res_background_color = tuple([int(x * 255) for x in kwargs.get(\"high_res_image_mean\")])"
        },
        {
            "sha": "058bbc4b8aa5cee03b9299e0aa83da172e9bd700",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -749,11 +749,11 @@ class DeepseekVLHybridImageProcessorFast(DeepseekVLImageProcessorFast):\n     high_res_resample = PILImageResampling.BICUBIC\n \n     def __init__(self, **kwargs: Unpack[DeepseekVLHybridFastImageProcessorKwargs]):\n-        if kwargs.get(\"image_mean\", None) is None:\n+        if kwargs.get(\"image_mean\") is None:\n             background_color = (127, 127, 127)\n         else:\n             background_color = tuple([int(x * 255) for x in kwargs.get(\"image_mean\")])\n-        if kwargs.get(\"high_res_image_mean\", None) is None:\n+        if kwargs.get(\"high_res_image_mean\") is None:\n             high_res_background_color = (127, 127, 127)\n         else:\n             high_res_background_color = tuple([int(x * 255) for x in kwargs.get(\"high_res_image_mean\")])"
        },
        {
            "sha": "07a83a1cb0a9966fd687b8a0e476f3281447f22e",
            "filename": "src/transformers/models/fsmt/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Ffsmt%2Fconvert_fsmt_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Ffsmt%2Fconvert_fsmt_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fconvert_fsmt_original_pytorch_checkpoint_to_pytorch.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -79,7 +79,7 @@ def rewrite_dict_keys(d):\n     # (1) remove word breaking symbol, (2) add word ending symbol where the word is not broken up,\n     # e.g.: d = {'le@@': 5, 'tt@@': 6, 'er': 7} => {'le': 5, 'tt': 6, 'er</w>': 7}\n     d2 = dict((re.sub(r\"@@$\", \"\", k), v) if k.endswith(\"@@\") else (re.sub(r\"$\", \"</w>\", k), v) for k, v in d.items())\n-    keep_keys = \"<s> <pad> </s> <unk>\".split()\n+    keep_keys = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\"]\n     # restore the special tokens\n     for k in keep_keys:\n         del d2[f\"{k}</w>\"]"
        },
        {
            "sha": "17df6ba9eaa3f7ea0542356aecbc26a39fede3e1",
            "filename": "src/transformers/models/janus/image_processing_janus_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -71,7 +71,7 @@ class JanusImageProcessorFast(BaseImageProcessorFast):\n     valid_kwargs = JanusFastImageProcessorKwargs\n \n     def __init__(self, **kwargs: Unpack[JanusFastImageProcessorKwargs]):\n-        if kwargs.get(\"image_mean\", None) is None:\n+        if kwargs.get(\"image_mean\") is None:\n             background_color = (127, 127, 127)\n         else:\n             background_color = tuple([int(x * 255) for x in kwargs.get(\"image_mean\")])"
        },
        {
            "sha": "615aacdf873718cfcb0cb77699924bd2f799cc9f",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -857,9 +857,7 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n         elif isinstance(module, JetMoeParallelExperts):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-        elif isinstance(module, JetMoeMoA):\n-            module.bias.data.zero_()\n-        elif isinstance(module, JetMoeMoE):\n+        elif isinstance(module, (JetMoeMoA, JetMoeMoE)):\n             module.bias.data.zero_()\n \n "
        },
        {
            "sha": "e49ccde7e2d79427a38b9907ab12016ef4c50814",
            "filename": "src/transformers/models/mm_grounding_dino/configuration_mm_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconfiguration_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconfiguration_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconfiguration_mm_grounding_dino.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -260,7 +260,7 @@ def __init__(\n         self.disable_custom_kernels = disable_custom_kernels\n         # Text backbone\n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"bert\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"bert\")\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             text_config = CONFIG_MAPPING[\"bert\"]()"
        },
        {
            "sha": "aea644fdd656f46c48dd5fd2102b28e381037f5d",
            "filename": "src/transformers/models/mm_grounding_dino/modular_mm_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodular_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodular_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodular_mm_grounding_dino.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -268,7 +268,7 @@ def __init__(\n         self.disable_custom_kernels = disable_custom_kernels\n         # Text backbone\n         if isinstance(text_config, dict):\n-            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"bert\"\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"bert\")\n             text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n         elif text_config is None:\n             text_config = CONFIG_MAPPING[\"bert\"]()"
        },
        {
            "sha": "d6af3e57e55e4a65cfaa529651aa4615cb4d2878",
            "filename": "src/transformers/models/oneformer/modeling_oneformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -2794,11 +2794,7 @@ def _init_weights(self, module: nn.Module):\n             nn.init.constant_(module.output_proj.bias.data, 0.0)\n         elif isinstance(module, OneFormerPixelDecoder):\n             nn.init.normal_(module.level_embed, std=0)\n-        elif isinstance(module, OneFormerTransformerDecoderLayer):\n-            for p in module.parameters():\n-                if p.dim() > 1:\n-                    nn.init.xavier_uniform_(p, gain=xavier_std)\n-        elif isinstance(module, OneFormerTransformerDecoderQueryTransformer):\n+        elif isinstance(module, (OneFormerTransformerDecoderLayer, OneFormerTransformerDecoderQueryTransformer)):\n             for p in module.parameters():\n                 if p.dim() > 1:\n                     nn.init.xavier_uniform_(p, gain=xavier_std)"
        },
        {
            "sha": "cc69cf6d27926837a5d7a8882af94729ea68b1ce",
            "filename": "src/transformers/pipelines/fill_mask.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fpipelines%2Ffill_mask.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fpipelines%2Ffill_mask.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ffill_mask.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -197,7 +197,7 @@ def get_target_ids(self, targets, top_k=None):\n             vocab = {}\n         target_ids = []\n         for target in targets:\n-            id_ = vocab.get(target, None)\n+            id_ = vocab.get(target)\n             if id_ is None:\n                 input_ids = self.tokenizer(\n                     target,"
        },
        {
            "sha": "efa70ca1851f775e9add37d74a135a69b58990db",
            "filename": "src/transformers/pipelines/token_classification.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -427,9 +427,11 @@ def aggregate_overlapping_entities(self, entities):\n             if previous_entity[\"start\"] <= entity[\"start\"] < previous_entity[\"end\"]:\n                 current_length = entity[\"end\"] - entity[\"start\"]\n                 previous_length = previous_entity[\"end\"] - previous_entity[\"start\"]\n-                if current_length > previous_length:\n-                    previous_entity = entity\n-                elif current_length == previous_length and entity[\"score\"] > previous_entity[\"score\"]:\n+                if (\n+                    current_length > previous_length\n+                    or current_length == previous_length\n+                    and entity[\"score\"] > previous_entity[\"score\"]\n+                ):\n                     previous_entity = entity\n             else:\n                 aggregated_entities.append(previous_entity)"
        },
        {
            "sha": "4934d27fb6052510a681a5cd71b8453fa451d563",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -2637,9 +2637,7 @@ def nested_simplify(obj, decimals=3):\n         return nested_simplify(obj.tolist())\n     elif isinstance(obj, Mapping):\n         return {nested_simplify(k, decimals): nested_simplify(v, decimals) for k, v in obj.items()}\n-    elif isinstance(obj, (str, int, np.int64)):\n-        return obj\n-    elif obj is None:\n+    elif isinstance(obj, (str, int, np.int64)) or obj is None:\n         return obj\n     elif is_torch_available() and isinstance(obj, torch.Tensor):\n         return nested_simplify(obj.tolist(), decimals)"
        },
        {
            "sha": "a362a7c8b066178bdf59f7aaf8395616b0fb0322",
            "filename": "src/transformers/tokenization_mistral_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_mistral_common.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -1784,9 +1784,7 @@ def from_pretrained(\n                 pathlib_repo_file = Path(path)\n                 file_name = pathlib_repo_file.name\n                 suffix = \"\".join(pathlib_repo_file.suffixes)\n-                if file_name == \"tekken.json\":\n-                    valid_tokenizer_files.append(file_name)\n-                elif suffix in sentencepiece_suffixes:\n+                if file_name == \"tekken.json\" or suffix in sentencepiece_suffixes:\n                     valid_tokenizer_files.append(file_name)\n \n             if len(valid_tokenizer_files) == 0:"
        },
        {
            "sha": "1528d085d66a897e3acbd3c48096beb2b89269b0",
            "filename": "src/transformers/tokenization_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Ftokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Ftokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -877,9 +877,11 @@ def get_input_ids(text):\n \n         input_ids = []\n         for ids_or_pair_ids in batch_text_or_text_pairs:\n-            if not isinstance(ids_or_pair_ids, (list, tuple)):\n-                ids, pair_ids = ids_or_pair_ids, None\n-            elif is_split_into_words and not isinstance(ids_or_pair_ids[0], (list, tuple)):\n+            if (\n+                not isinstance(ids_or_pair_ids, (list, tuple))\n+                or is_split_into_words\n+                and not isinstance(ids_or_pair_ids[0], (list, tuple))\n+            ):\n                 ids, pair_ids = ids_or_pair_ids, None\n             else:\n                 ids, pair_ids = ids_or_pair_ids"
        },
        {
            "sha": "c32516b167fe54f2d29e64d221e86e63c806f592",
            "filename": "src/transformers/trainer_pt_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 11,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_pt_utils.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -153,9 +153,7 @@ def find_batch_size(tensors):\n             result = find_batch_size(value)\n             if result is not None:\n                 return result\n-    elif isinstance(tensors, torch.Tensor):\n-        return tensors.shape[0] if len(tensors.shape) >= 1 else None\n-    elif isinstance(tensors, np.ndarray):\n+    elif isinstance(tensors, (torch.Tensor, np.ndarray)):\n         return tensors.shape[0] if len(tensors.shape) >= 1 else None\n \n \n@@ -634,10 +632,7 @@ def __init__(\n         self.batch_size = batch_size\n         if lengths is None:\n             model_input_name = model_input_name if model_input_name is not None else \"input_ids\"\n-            if (\n-                not (isinstance(dataset[0], dict) or isinstance(dataset[0], BatchEncoding))\n-                or model_input_name not in dataset[0]\n-            ):\n+            if not isinstance(dataset[0], (dict, BatchEncoding)) or model_input_name not in dataset[0]:\n                 raise ValueError(\n                     \"Can only automatically infer lengths for datasets whose items are dictionaries with an \"\n                     f\"'{model_input_name}' key.\"\n@@ -697,10 +692,7 @@ def __init__(\n \n         if lengths is None:\n             model_input_name = model_input_name if model_input_name is not None else \"input_ids\"\n-            if (\n-                not (isinstance(dataset[0], dict) or isinstance(dataset[0], BatchEncoding))\n-                or model_input_name not in dataset[0]\n-            ):\n+            if not isinstance(dataset[0], (dict, BatchEncoding)) or model_input_name not in dataset[0]:\n                 raise ValueError(\n                     \"Can only automatically infer lengths for datasets whose items are dictionaries with an \"\n                     f\"'{model_input_name}' key.\""
        },
        {
            "sha": "6f9e099944fc0ed172e11e1036b1f48eec76dbaf",
            "filename": "src/transformers/utils/auto_docstring.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fauto_docstring.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -1404,8 +1404,8 @@ def _process_regular_parameters(\n                     param_type = f\"[`{class_name}`]\"\n                 else:\n                     param_type = f\"[`{param_type.split('.')[-1]}`]\"\n-            elif param_type == \"\" and False:  # TODO: Enforce typing for all parameters\n-                print(f\"ðŸš¨ {param_name} for {func.__qualname__} in file {func.__code__.co_filename} has no type\")\n+            # elif param_type == \"\" and False:  # TODO: Enforce typing for all parameters\n+            #     print(f\"ðŸš¨ {param_name} for {func.__qualname__} in file {func.__code__.co_filename} has no type\")\n             param_type = param_type if \"`\" in param_type else f\"`{param_type}`\"\n             # Format the parameter docstring\n             if additional_info:\n@@ -1833,7 +1833,7 @@ def auto_class_docstring(cls, custom_intro=None, custom_args=None, checkpoint=No\n             docstring += set_min_indent(f\"\\n{docstring_init}\", indent_level)\n         elif is_dataclass:\n             # No init function, we have a data class\n-            docstring += \"\\nArgs:\\n\" if not docstring_args else docstring_args\n+            docstring += docstring_args if docstring_args else \"\\nArgs:\\n\"\n             source_args_dict = get_args_doc_from_source(ModelOutputArgs)\n             doc_class = cls.__doc__ if cls.__doc__ else \"\"\n             documented_kwargs, _ = parse_docstring(doc_class)"
        },
        {
            "sha": "8a48cf9cbb4afc38dfc0ec58cb089b25a5f1b5ff",
            "filename": "src/transformers/utils/fx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Futils%2Ffx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Futils%2Ffx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Ffx.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -749,9 +749,7 @@ def check_proxy(a):\n             tracer = found_proxies[0].tracer\n             if op_type == \"call_function\":\n                 target = function\n-            elif op_type == \"call_method\":\n-                target = function.__name__\n-            elif op_type == \"get_attr\":\n+            elif op_type == \"call_method\" or op_type == \"get_attr\":\n                 target = function.__name__\n             else:\n                 raise ValueError(f\"op_type {op_type} not supported.\")"
        },
        {
            "sha": "163ee0410944fd923f530045afe1940b556a7b24",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -451,9 +451,7 @@ def get_torch_major_and_minor_version() -> str:\n \n \n def is_torch_sdpa_available():\n-    if not is_torch_available():\n-        return False\n-    elif _torch_version == \"N/A\":\n+    if not is_torch_available() or _torch_version == \"N/A\":\n         return False\n \n     # NOTE: MLU is OK with non-contiguous inputs.\n@@ -467,9 +465,7 @@ def is_torch_sdpa_available():\n \n \n def is_torch_flex_attn_available():\n-    if not is_torch_available():\n-        return False\n-    elif _torch_version == \"N/A\":\n+    if not is_torch_available() or _torch_version == \"N/A\":\n         return False\n \n     # TODO check if some bugs cause push backs on the exact version\n@@ -1088,7 +1084,7 @@ def is_ninja_available():\n     [ninja](https://ninja-build.org/) build system is available on the system, `False` otherwise.\n     \"\"\"\n     try:\n-        subprocess.check_output(\"ninja --version\".split())\n+        subprocess.check_output([\"ninja\", \"--version\"])\n     except Exception:\n         return False\n     else:"
        },
        {
            "sha": "99b1450a0d59501a34172687e221c2682e0ddb30",
            "filename": "tests/deepspeed/test_deepspeed.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fdeepspeed%2Ftest_deepspeed.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -1222,7 +1222,7 @@ def test_inference(self, dtype):\n \n         # this is just inference, so no optimizer should be loaded\n         # it only works for z3 (makes no sense with z1-z2)\n-        fp32 = True if dtype == \"fp32\" else False\n+        fp32 = dtype == \"fp32\"\n         self.run_and_check(\n             stage=ZERO3,\n             dtype=dtype,\n@@ -1337,13 +1337,7 @@ def run_trainer(\n \n         if do_eval:\n             actions += 1\n-            args.extend(\n-                \"\"\"\n-            --do_eval\n-            --max_eval_samples 16\n-            --per_device_eval_batch_size 2\n-            \"\"\".split()\n-            )\n+            args.extend([\"--do_eval\", \"--max_eval_samples\", \"16\", \"--per_device_eval_batch_size\", \"2\"])\n \n         assert actions > 0, \"need at least do_train or do_eval for the test to run\"\n "
        },
        {
            "sha": "82d3cdb9c3ce5011b2aa1c43f388d8a0e250fac1",
            "filename": "tests/extended/test_trainer_ext.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/tests%2Fextended%2Ftest_trainer_ext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/tests%2Fextended%2Ftest_trainer_ext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fextended%2Ftest_trainer_ext.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -315,9 +315,7 @@ def run_trainer(\n             --eval_steps {str(eval_steps)}\n         \"\"\".split()\n \n-        args_predict = \"\"\"\n-            --do_predict\n-        \"\"\".split()\n+        args_predict = [\"--do_predict\"]\n \n         args = []\n         if do_train:\n@@ -330,11 +328,11 @@ def run_trainer(\n             args += args_predict\n \n         if predict_with_generate:\n-            args += \"--predict_with_generate\".split()\n+            args += [\"--predict_with_generate\"]\n \n         if do_train:\n             if optim == \"adafactor\":\n-                args += \"--adafactor\".split()\n+                args += [\"--adafactor\"]\n             else:\n                 args += f\"--optim {optim}\".split()\n "
        },
        {
            "sha": "77e2de37c74180600833f17d7f12dc8c2f208d45",
            "filename": "tests/generation/test_fsdp.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/tests%2Fgeneration%2Ftest_fsdp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/tests%2Fgeneration%2Ftest_fsdp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_fsdp.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -122,7 +122,7 @@ def test_fsdp_generate(self):\n             --master_port={get_torch_dist_unique_port()}\n             {self.test_file_dir}/test_fsdp.py\n         \"\"\".split()\n-        args = \"--fsdp\".split()\n+        args = [\"--fsdp\"]\n         cmd = [\"torchrun\"] + distributed_args + args\n         execute_subprocess_async(cmd, env=self.get_env())\n         # successful return here == success - any errors would have caused an error in the sub-call\n@@ -135,7 +135,7 @@ def test_fsdp2_generate(self):\n             --master_port={get_torch_dist_unique_port()}\n             {self.test_file_dir}/test_fsdp.py\n         \"\"\".split()\n-        args = \"--fsdp2\".split()\n+        args = [\"--fsdp2\"]\n         cmd = [\"torchrun\"] + distributed_args + args\n         execute_subprocess_async(cmd, env=self.get_env())\n         # successful return here == success - any errors would have caused an error in the sub-call"
        },
        {
            "sha": "6f82b14853e37cba579d624ec1804e0cafc39e02",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/952fac100dd593ae5e86aa6afde75fb3f877dec0/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/952fac100dd593ae5e86aa6afde75fb3f877dec0/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=952fac100dd593ae5e86aa6afde75fb3f877dec0",
            "patch": "@@ -5017,7 +5017,7 @@ def test_push_to_hub_in_organization(self):\n \n     def get_commit_history(self, repo):\n         commit_logs = subprocess.run(\n-            \"git log\".split(),\n+            [\"git\", \"log\"],\n             capture_output=True,\n             check=True,\n             encoding=\"utf-8\","
        }
    ],
    "stats": {
        "total": 149,
        "additions": 60,
        "deletions": 89
    }
}