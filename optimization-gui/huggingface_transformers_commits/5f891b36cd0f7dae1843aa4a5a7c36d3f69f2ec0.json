{
    "author": "cyyever",
    "message": "Fix typing of tuples (#41028)\n\n* Fix tuple typing\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* More fixes\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* More fixes\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0",
    "files": [
        {
            "sha": "ef872b6c172c5abbfb7c37929883c404e7c299f0",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0",
            "patch": "@@ -131,7 +131,7 @@ def max_across_indices(values: Iterable[Any]) -> list[Any]:\n     return [max(values_i) for values_i in zip(*values)]\n \n \n-def get_max_height_width(images: list[\"torch.Tensor\"]) -> tuple[int]:\n+def get_max_height_width(images: list[\"torch.Tensor\"]) -> tuple[int, ...]:\n     \"\"\"\n     Get the maximum height and width across all images in a batch.\n     \"\"\""
        },
        {
            "sha": "6d234ef37e5bcb1da7acd2ab7492f913a03c1bde",
            "filename": "src/transformers/image_transforms.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0/src%2Ftransformers%2Fimage_transforms.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0/src%2Ftransformers%2Fimage_transforms.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_transforms.py?ref=5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0",
            "patch": "@@ -245,7 +245,7 @@ def get_size_with_aspect_ratio(image_size, size, max_size=None) -> tuple[int, in\n # Logic adapted from torchvision resizing logic: https://github.com/pytorch/vision/blob/511924c1ced4ce0461197e5caa64ce5b9e558aab/torchvision/transforms/functional.py#L366\n def get_resize_output_image_size(\n     input_image: np.ndarray,\n-    size: Union[int, tuple[int, int], list[int], tuple[int]],\n+    size: Union[int, tuple[int, int], list[int], tuple[int, ...]],\n     default_to_square: bool = True,\n     max_size: Optional[int] = None,\n     input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "1747f6fa477b56bc349f2c535705382ebc84af3c",
            "filename": "src/transformers/modeling_outputs.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0/src%2Ftransformers%2Fmodeling_outputs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0/src%2Ftransformers%2Fmodeling_outputs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_outputs.py?ref=5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0",
            "patch": "@@ -1651,7 +1651,7 @@ class Seq2SeqTSPredictionOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    params: Optional[tuple[torch.FloatTensor]] = None\n+    params: Optional[tuple[torch.FloatTensor, ...]] = None\n     past_key_values: Optional[EncoderDecoderCache] = None\n     decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None"
        },
        {
            "sha": "a132a763ca05b561e509282828d61b1557edea9e",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0",
            "patch": "@@ -1644,7 +1644,7 @@ def create_extended_attention_mask_for_decoder(input_shape, attention_mask, devi\n     def get_extended_attention_mask(\n         self,\n         attention_mask: Tensor,\n-        input_shape: tuple[int],\n+        input_shape: tuple[int, ...],\n         device: Optional[torch.device] = None,\n         dtype: Optional[torch.dtype] = None,\n     ) -> Tensor:"
        },
        {
            "sha": "b3edad05327fb6dc28fc7b6d59e6d21acf7efc29",
            "filename": "src/transformers/onnx/config.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0/src%2Ftransformers%2Fonnx%2Fconfig.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0/src%2Ftransformers%2Fonnx%2Fconfig.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fonnx%2Fconfig.py?ref=5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0",
            "patch": "@@ -608,7 +608,7 @@ def outputs(self) -> Mapping[str, Mapping[int, str]]:\n         return common_outputs\n \n     @property\n-    def num_layers(self) -> tuple[int]:\n+    def num_layers(self) -> tuple[int, ...]:\n         try:\n             num_layers = super().num_layers\n             num_layers = (num_layers, num_layers)\n@@ -624,7 +624,7 @@ def num_layers(self) -> tuple[int]:\n         return num_layers\n \n     @property\n-    def num_attention_heads(self) -> tuple[int]:\n+    def num_attention_heads(self) -> tuple[int, ...]:\n         try:\n             num_attention_heads = super().num_attention_heads\n             num_attention_heads = (num_attention_heads, num_attention_heads)"
        },
        {
            "sha": "20f32d994461f18d95ff5d225f67bdabc7a4dd12",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0",
            "patch": "@@ -56,20 +56,13 @@\n \n GenericTensor = Union[list[\"GenericTensor\"], \"torch.Tensor\"]\n \n-if is_torch_available():\n+if is_torch_available() or TYPE_CHECKING:\n     import torch\n     from torch.utils.data import DataLoader, Dataset\n \n     from ..modeling_utils import PreTrainedModel\n-\n-    # Re-export for backward compatibility\n-    from .pt_utils import KeyDataset\n else:\n     Dataset = None\n-    KeyDataset = None\n-\n-if TYPE_CHECKING:\n-    from ..modeling_utils import PreTrainedModel\n \n \n logger = logging.get_logger(__name__)\n@@ -200,7 +193,7 @@ def inner(items):\n def load_model(\n     model,\n     config: AutoConfig,\n-    model_classes: Optional[tuple[type]] = None,\n+    model_classes: Optional[tuple[type, ...]] = None,\n     task: Optional[str] = None,\n     **model_kwargs,\n ):"
        },
        {
            "sha": "0b67041bfccc3e30d5e984c82c4f5a712b2fc8ff",
            "filename": "src/transformers/tokenization_mistral_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_mistral_common.py?ref=5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0",
            "patch": "@@ -1819,7 +1819,7 @@ def save_pretrained(\n         repo_url: Optional[str] = None,\n         organization: Optional[str] = None,\n         **kwargs,\n-    ) -> tuple[str]:\n+    ) -> tuple[str, ...]:\n         \"\"\"\n         Save the full tokenizer state.\n "
        },
        {
            "sha": "1264fb2392c8ca01e90bcfe2c4b0b177b604c0b1",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0",
            "patch": "@@ -2420,7 +2420,7 @@ def save_pretrained(\n         filename_prefix: Optional[str] = None,\n         push_to_hub: bool = False,\n         **kwargs,\n-    ) -> tuple[str]:\n+    ) -> tuple[str, ...]:\n         \"\"\"\n         Save the full tokenizer state.\n \n@@ -2585,10 +2585,10 @@ def save_pretrained(\n     def _save_pretrained(\n         self,\n         save_directory: Union[str, os.PathLike],\n-        file_names: tuple[str],\n+        file_names: tuple[str, ...],\n         legacy_format: Optional[bool] = None,\n         filename_prefix: Optional[str] = None,\n-    ) -> tuple[str]:\n+    ) -> tuple[str, ...]:\n         \"\"\"\n         Save a tokenizer using the slow-tokenizer/legacy format: vocabulary + added tokens.\n \n@@ -2617,7 +2617,7 @@ def _save_pretrained(\n \n         return file_names + vocab_files + (added_tokens_file,)\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str, ...]:\n         \"\"\"\n         Save only the vocabulary of the tokenizer (vocabulary + added tokens).\n \n@@ -2631,7 +2631,7 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n                 An optional prefix to add to the named of the saved files.\n \n         Returns:\n-            `Tuple(str)`: Paths to the files saved.\n+            `tuple(str)`: Paths to the files saved.\n         \"\"\"\n         raise NotImplementedError\n "
        },
        {
            "sha": "fe4873d61b378410c90db80b4b19094f3c6ab292",
            "filename": "src/transformers/tokenization_utils_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_fast.py?ref=5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0",
            "patch": "@@ -695,10 +695,10 @@ def _decode(\n     def _save_pretrained(\n         self,\n         save_directory: Union[str, os.PathLike],\n-        file_names: tuple[str],\n+        file_names: tuple[str, ...],\n         legacy_format: Optional[bool] = None,\n         filename_prefix: Optional[str] = None,\n-    ) -> tuple[str]:\n+    ) -> tuple[str, ...]:\n         \"\"\"\n         Save a tokenizer using the slow-tokenizer/legacy format: vocabulary + added tokens as well as in a unique JSON\n         file containing {config + vocab + added-tokens}."
        },
        {
            "sha": "d2f6277282d984384e29f177ff6c92fa5bc9e327",
            "filename": "src/transformers/utils/backbone_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0/src%2Ftransformers%2Futils%2Fbackbone_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0/src%2Ftransformers%2Futils%2Fbackbone_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fbackbone_utils.py?ref=5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0",
            "patch": "@@ -76,7 +76,7 @@ def verify_out_features_out_indices(\n \n def _align_output_features_output_indices(\n     out_features: Optional[list[str]],\n-    out_indices: Optional[Union[list[int], tuple[int]]],\n+    out_indices: Optional[Union[list[int], tuple[int, ...]]],\n     stage_names: list[str],\n ):\n     \"\"\"\n@@ -284,7 +284,7 @@ def out_indices(self):\n         return self._out_indices\n \n     @out_indices.setter\n-    def out_indices(self, out_indices: Union[tuple[int], list[int]]):\n+    def out_indices(self, out_indices: Union[tuple[int, ...], list[int]]):\n         \"\"\"\n         Set the out_indices attribute. This will also update the out_features attribute to match the new out_indices.\n         \"\"\""
        },
        {
            "sha": "994ab6a6b8880de980c16b79ebb0ffae91164db3",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=5f891b36cd0f7dae1843aa4a5a7c36d3f69f2ec0",
            "patch": "@@ -380,7 +380,7 @@ def __reduce__(self):\n         args = tuple(getattr(self, field.name) for field in fields(self))\n         return callable, args, *remaining\n \n-    def to_tuple(self) -> tuple[Any]:\n+    def to_tuple(self) -> tuple:\n         \"\"\"\n         Convert self to a tuple containing all the attributes/keys that are not `None`.\n         \"\"\""
        }
    ],
    "stats": {
        "total": 45,
        "additions": 19,
        "deletions": 26
    }
}