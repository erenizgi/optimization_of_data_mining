{
    "author": "LckyLke",
    "message": "Updated Megatron conversion script for gpt2 checkpoints  (#38969)\n\n* update script to support new megatron gpt format\n\n* fixed quality failures\n\n---------\n\nCo-authored-by: Luke Friedrichs <LckyLke>",
    "sha": "2c58705dc23ce869e82b1a6ca225ad718916e8d5",
    "files": [
        {
            "sha": "c0054fab3f1650928c3ebdd71d053cb3bc6fb5ef",
            "filename": "src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py",
            "status": "modified",
            "additions": 85,
            "deletions": 13,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c58705dc23ce869e82b1a6ca225ad718916e8d5/src%2Ftransformers%2Fmodels%2Fmegatron_gpt2%2Fconvert_megatron_gpt2_checkpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c58705dc23ce869e82b1a6ca225ad718916e8d5/src%2Ftransformers%2Fmodels%2Fmegatron_gpt2%2Fconvert_megatron_gpt2_checkpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_gpt2%2Fconvert_megatron_gpt2_checkpoint.py?ref=2c58705dc23ce869e82b1a6ca225ad718916e8d5",
            "patch": "@@ -148,14 +148,17 @@ def convert_megatron_checkpoint(args, input_state_dict, config):\n     transformer = lm[\"transformer\"] if \"transformer\" in lm.keys() else lm[\"encoder\"]\n \n     # The regex to extract layer names.\n-    layer_re = re.compile(r\"layers\\.(\\d+)\\.([a-z0-9_.]+)\\.([a-z]+)\")\n+    layer_re = re.compile(r\"layers\\.(\\d+)\\.([a-z0-9_.]+)\\.([a-z0-9_]+)\")\n \n     # The simple map of names for \"automated\" rules.\n     megatron_to_transformers = {\n         \"attention.dense\": \".attn.c_proj.\",\n         \"self_attention.dense\": \".attn.c_proj.\",\n+        \"self_attention.proj\": \".attn.c_proj.\",  # New format\n         \"mlp.dense_h_to_4h\": \".mlp.c_fc.\",\n         \"mlp.dense_4h_to_h\": \".mlp.c_proj.\",\n+        \"layernorm_mlp.fc1\": \".mlp.c_fc.\",  # New format\n+        \"layernorm_mlp.fc2\": \".mlp.c_proj.\",  # New format\n     }\n \n     # Extract the layers.\n@@ -165,24 +168,63 @@ def convert_megatron_checkpoint(args, input_state_dict, config):\n \n         # Stop if that's not a layer\n         if m is None:\n-            break\n+            continue\n \n         # The index of the layer.\n         layer_idx = int(m.group(1))\n         # The name of the operation.\n         op_name = m.group(2)\n         # Is it a weight or a bias?\n         weight_or_bias = m.group(3)\n-\n         # The name of the layer.\n         layer_name = f\"transformer.h.{layer_idx}\"\n \n-        # For layernorm(s), simply store the layer norm.\n-        if op_name.endswith(\"layernorm\"):\n-            ln_name = \"ln_1\" if op_name.startswith(\"input\") else \"ln_2\"\n-            output_state_dict[layer_name + \".\" + ln_name + \".\" + weight_or_bias] = val\n+        # Handle _extra_state keys (skip them)\n+        if weight_or_bias == \"_extra_state\":\n+            continue\n \n-        # Transpose the QKV matrix.\n+        # For layernorm(s), simply store the layer norm.\n+        if op_name.endswith(\"layernorm\") or weight_or_bias.startswith(\"layer_norm\"):\n+            if weight_or_bias.startswith(\"layer_norm\"):\n+                # New format: layers.X.self_attention.layernorm_qkv.layer_norm_weight\n+                if op_name == \"self_attention.layernorm_qkv\":\n+                    ln_name = \"ln_1\"  # Pre-attention layer norm\n+                elif op_name == \"layernorm_mlp\":\n+                    ln_name = \"ln_2\"  # Pre-MLP layer norm\n+                else:\n+                    ln_name = \"ln_1\" if op_name.startswith(\"input\") else \"ln_2\"\n+\n+                param_name = \"weight\" if weight_or_bias == \"layer_norm_weight\" else \"bias\"\n+                output_state_dict[layer_name + \".\" + ln_name + \".\" + param_name] = val\n+            else:\n+                # Old format\n+                ln_name = \"ln_1\" if op_name.startswith(\"input\") else \"ln_2\"\n+                output_state_dict[layer_name + \".\" + ln_name + \".\" + weight_or_bias] = val\n+\n+        # Handle QKV projections - new format: self_attention.layernorm_qkv.weight/bias\n+        elif op_name == \"self_attention.layernorm_qkv\" and weight_or_bias in [\"weight\", \"bias\"]:\n+            if weight_or_bias == \"weight\":\n+                # Insert a tensor of 1x1xDxD bias.\n+                causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=torch.float16)).view(\n+                    1, 1, n_positions, n_positions\n+                )\n+                output_state_dict[layer_name + \".attn.bias\"] = causal_mask\n+\n+                # Insert a \"dummy\" tensor for masked_bias.\n+                masked_bias = torch.tensor(-1e4, dtype=torch.float16)\n+                output_state_dict[layer_name + \".attn.masked_bias\"] = masked_bias\n+\n+                out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n+                # Megatron stores (3*D) x D but transformers-GPT2 expects D x 3*D.\n+                out_val = out_val.transpose(0, 1).contiguous()\n+                # Store.\n+                output_state_dict[layer_name + \".attn.c_attn.weight\"] = out_val\n+            else:  # bias\n+                out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n+                # Store. No change of shape.\n+                output_state_dict[layer_name + \".attn.c_attn.bias\"] = out_val\n+\n+        # Transpose the QKV matrix - old format.\n         elif (\n             op_name == \"attention.query_key_value\" or op_name == \"self_attention.query_key_value\"\n         ) and weight_or_bias == \"weight\":\n@@ -202,7 +244,7 @@ def convert_megatron_checkpoint(args, input_state_dict, config):\n             # Store.\n             output_state_dict[layer_name + \".attn.c_attn.weight\"] = out_val\n \n-        # Transpose the bias.\n+        # Transpose the bias - old format.\n         elif (\n             op_name == \"attention.query_key_value\" or op_name == \"self_attention.query_key_value\"\n         ) and weight_or_bias == \"bias\":\n@@ -212,20 +254,50 @@ def convert_megatron_checkpoint(args, input_state_dict, config):\n \n         # Transpose the weights.\n         elif weight_or_bias == \"weight\":\n+            # DEBUG: Check if op_name exists in the mapping\n+            if op_name not in megatron_to_transformers:\n+                continue\n             out_name = megatron_to_transformers[op_name]\n             output_state_dict[layer_name + out_name + \"weight\"] = val.transpose(0, 1)\n \n         # Copy the bias.\n         elif weight_or_bias == \"bias\":\n+            # DEBUG: Check if op_name exists in the mapping\n+            if op_name not in megatron_to_transformers:\n+                continue\n             out_name = megatron_to_transformers[op_name]\n             output_state_dict[layer_name + out_name + \"bias\"] = val\n \n+        # Handle new format MLP weights/biases\n+        elif weight_or_bias in [\"fc1_weight\", \"fc2_weight\", \"fc1_bias\", \"fc2_bias\"]:\n+            if weight_or_bias == \"fc1_weight\":\n+                output_state_dict[layer_name + \".mlp.c_fc.weight\"] = val.transpose(0, 1)\n+            elif weight_or_bias == \"fc1_bias\":\n+                output_state_dict[layer_name + \".mlp.c_fc.bias\"] = val\n+            elif weight_or_bias == \"fc2_weight\":\n+                output_state_dict[layer_name + \".mlp.c_proj.weight\"] = val.transpose(0, 1)\n+            elif weight_or_bias == \"fc2_bias\":\n+                output_state_dict[layer_name + \".mlp.c_proj.bias\"] = val\n+\n+        else:\n+            print(\n+                f\"DEBUG: Unhandled key: {key} (layer {layer_idx}, op_name: '{op_name}', weight_or_bias: '{weight_or_bias}')\"\n+            )\n+\n     # DEBUG.\n     assert config.n_layer == layer_idx + 1\n \n-    # The final layernorm.\n-    output_state_dict[\"transformer.ln_f.weight\"] = transformer[\"final_layernorm.weight\"]\n-    output_state_dict[\"transformer.ln_f.bias\"] = transformer[\"final_layernorm.bias\"]\n+    # The final layernorm - handle both old and new formats.\n+    if \"final_layernorm.weight\" in transformer:\n+        # Old format\n+        output_state_dict[\"transformer.ln_f.weight\"] = transformer[\"final_layernorm.weight\"]\n+        output_state_dict[\"transformer.ln_f.bias\"] = transformer[\"final_layernorm.bias\"]\n+    elif \"final_norm.weight\" in transformer:\n+        # New format\n+        output_state_dict[\"transformer.ln_f.weight\"] = transformer[\"final_norm.weight\"]\n+        output_state_dict[\"transformer.ln_f.bias\"] = transformer[\"final_norm.bias\"]\n+    else:\n+        print(\"WARNING: Could not find final layer norm weights!\")\n \n     # For LM head, transformers' wants the matrix to weight embeddings.\n     output_state_dict[\"lm_head.weight\"] = word_embeddings\n@@ -265,7 +337,7 @@ def main():\n             with checkpoint.open(\"release/mp_rank_00/model_optim_rng.pt\") as pytorch_dict:\n                 input_state_dict = torch.load(pytorch_dict, map_location=\"cpu\", weights_only=True)\n     else:\n-        input_state_dict = torch.load(args.path_to_checkpoint, map_location=\"cpu\", weights_only=True)\n+        input_state_dict = torch.load(args.path_to_checkpoint, map_location=\"cpu\", weights_only=False)\n \n     ds_args = input_state_dict.get(\"args\", None)\n "
        }
    ],
    "stats": {
        "total": 98,
        "additions": 85,
        "deletions": 13
    }
}