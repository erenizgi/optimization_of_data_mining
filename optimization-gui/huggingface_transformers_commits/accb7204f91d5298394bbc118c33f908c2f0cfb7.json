{
    "author": "VladOS95-cyber",
    "message": "Add Pytorch Tensor Parallel support for Qwen2, Qwen2Moe, Starcoder2 (#35007)\n\n* add base tp plan for qwen2 and qwen2moe\n\n* add parallel tp for starcoder2\n\n* fix modular conversion\n\n* add infer dim for qkv states\n\n* Update src/transformers/models/qwen2_moe/configuration_qwen2_moe.py\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "accb7204f91d5298394bbc118c33f908c2f0cfb7",
    "files": [
        {
            "sha": "e94281b29fe1a838d67caf6ba38eb6946ab749ac",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/accb7204f91d5298394bbc118c33f908c2f0cfb7/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/accb7204f91d5298394bbc118c33f908c2f0cfb7/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=accb7204f91d5298394bbc118c33f908c2f0cfb7",
            "patch": "@@ -310,9 +310,9 @@ def forward(\n         key_states = self.k_proj(hidden_states)\n         value_states = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n         kv_seq_len = key_states.shape[-2]\n         if past_key_value is not None:\n@@ -423,9 +423,9 @@ def forward(\n         key_states = self.k_proj(hidden_states)\n         value_states = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n         cos, sin = self.rotary_emb(value_states, position_ids)\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
        },
        {
            "sha": "16ce924b9f16f63dbe043cb2e2eb2713be2f4fb9",
            "filename": "src/transformers/models/qwen2/configuration_qwen2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/accb7204f91d5298394bbc118c33f908c2f0cfb7/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/accb7204f91d5298394bbc118c33f908c2f0cfb7/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py?ref=accb7204f91d5298394bbc118c33f908c2f0cfb7",
            "patch": "@@ -129,6 +129,17 @@ class Qwen2Config(PretrainedConfig):\n     model_type = \"qwen2\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n \n+    # Default tensor parallel plan for base model `Qwen2`\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+\n     def __init__(\n         self,\n         vocab_size=151936,"
        },
        {
            "sha": "36c5271c5c5e6c1e664da9f5641b88160b192c68",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/accb7204f91d5298394bbc118c33f908c2f0cfb7/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/accb7204f91d5298394bbc118c33f908c2f0cfb7/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=accb7204f91d5298394bbc118c33f908c2f0cfb7",
            "patch": "@@ -292,9 +292,9 @@ def forward(\n         key_states = self.k_proj(hidden_states)\n         value_states = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n         if position_embeddings is None:\n             logger.warning_once(\n@@ -378,9 +378,9 @@ def forward(\n         key_states = self.k_proj(hidden_states)\n         value_states = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n         if position_embeddings is None:\n             logger.warning_once(\n@@ -502,9 +502,9 @@ def forward(\n         key_states = self.k_proj(hidden_states)\n         value_states = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n         if position_embeddings is None:\n             logger.warning_once(\n@@ -1077,6 +1077,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n class Qwen2ForCausalLM(Qwen2PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "ed72ef6e465e524fd01d14ae97af09e6a84ffd93",
            "filename": "src/transformers/models/qwen2_moe/configuration_qwen2_moe.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/accb7204f91d5298394bbc118c33f908c2f0cfb7/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/accb7204f91d5298394bbc118c33f908c2f0cfb7/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py?ref=accb7204f91d5298394bbc118c33f908c2f0cfb7",
            "patch": "@@ -150,6 +150,17 @@ class Qwen2MoeConfig(PretrainedConfig):\n     model_type = \"qwen2_moe\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n \n+    # Default tensor parallel plan for base model `Qwen2Moe`\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+\n     def __init__(\n         self,\n         vocab_size=151936,"
        },
        {
            "sha": "6c5cbec2220e23a3aa7655c7a8b976926c732624",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/accb7204f91d5298394bbc118c33f908c2f0cfb7/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/accb7204f91d5298394bbc118c33f908c2f0cfb7/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=accb7204f91d5298394bbc118c33f908c2f0cfb7",
            "patch": "@@ -376,9 +376,9 @@ def forward(\n         key_states = self.k_proj(hidden_states)\n         value_states = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n         if position_embeddings is None:\n             logger.warning_once(\n@@ -465,9 +465,9 @@ def forward(\n         key_states = self.k_proj(hidden_states)\n         value_states = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n         if position_embeddings is None:\n             logger.warning_once(\n@@ -590,9 +590,9 @@ def forward(\n         key_states = self.k_proj(hidden_states)\n         value_states = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n         if position_embeddings is None:\n             logger.warning_once(\n@@ -1257,6 +1257,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n class Qwen2MoeForCausalLM(Qwen2MoePreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "5749eb68358468595cc2c5b909009b2b79b69dd7",
            "filename": "src/transformers/models/starcoder2/configuration_starcoder2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/accb7204f91d5298394bbc118c33f908c2f0cfb7/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/accb7204f91d5298394bbc118c33f908c2f0cfb7/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py?ref=accb7204f91d5298394bbc118c33f908c2f0cfb7",
            "patch": "@@ -134,6 +134,15 @@ class Starcoder2Config(PretrainedConfig):\n \n     model_type = \"starcoder2\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    # Default tensor parallel plan for base model `Starcoder2`\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.c_fc\": \"colwise\",\n+        \"layers.*.mlp.c_proj\": \"colwise\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "89b4194d1b157861f9ab92a9d5b457b7ba7aedc9",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/accb7204f91d5298394bbc118c33f908c2f0cfb7/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/accb7204f91d5298394bbc118c33f908c2f0cfb7/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=accb7204f91d5298394bbc118c33f908c2f0cfb7",
            "patch": "@@ -271,9 +271,9 @@ def forward(\n         key_states = self.k_proj(hidden_states)\n         value_states = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n         if position_embeddings is None:\n             logger.warning_once(\n@@ -355,9 +355,9 @@ def forward(\n         key_states = self.k_proj(hidden_states)\n         value_states = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n         if position_embeddings is None:\n             logger.warning_once(\n@@ -470,9 +470,9 @@ def forward(\n         key_states = self.k_proj(hidden_states)\n         value_states = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n         if position_embeddings is None:\n             logger.warning_once(\n@@ -1043,6 +1043,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n class Starcoder2ForCausalLM(Starcoder2PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "b5d74bf7feb39f1ee02c8726557807b825befc36",
            "filename": "src/transformers/models/starcoder2/modular_starcoder2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/accb7204f91d5298394bbc118c33f908c2f0cfb7/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/accb7204f91d5298394bbc118c33f908c2f0cfb7/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py?ref=accb7204f91d5298394bbc118c33f908c2f0cfb7",
            "patch": "@@ -136,9 +136,9 @@ def forward(\n         key_states = self.k_proj(hidden_states)\n         value_states = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n         if position_embeddings is None:\n             logger.warning_once(\n@@ -220,9 +220,9 @@ def forward(\n         key_states = self.k_proj(hidden_states)\n         value_states = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n         if position_embeddings is None:\n             logger.warning_once(\n@@ -335,9 +335,9 @@ def forward(\n         key_states = self.k_proj(hidden_states)\n         value_states = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n         if position_embeddings is None:\n             logger.warning_once("
        }
    ],
    "stats": {
        "total": 118,
        "additions": 76,
        "deletions": 42
    }
}