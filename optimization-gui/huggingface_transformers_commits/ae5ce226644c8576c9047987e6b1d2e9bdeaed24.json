{
    "author": "sywangyi",
    "message": "from_pretrained should handle xpu case (#37382)\n\n* from_pretrained should handle xpu case\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>\n\n* fmt\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>\n\n---------\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>",
    "sha": "ae5ce226644c8576c9047987e6b1d2e9bdeaed24",
    "files": [
        {
            "sha": "76526c360c1cd09e52983736cc1134a24abe945a",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ae5ce226644c8576c9047987e6b1d2e9bdeaed24/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ae5ce226644c8576c9047987e6b1d2e9bdeaed24/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=ae5ce226644c8576c9047987e6b1d2e9bdeaed24",
            "patch": "@@ -3989,6 +3989,9 @@ def from_pretrained(\n                     elif device_type == \"cpu\":\n                         cpu_backend = \"ccl\" if int(os.environ.get(\"CCL_WORKER_COUNT\", 0)) else \"gloo\"\n                         torch.distributed.init_process_group(cpu_backend, rank=rank, world_size=world_size)\n+                    elif device_type == \"xpu\":\n+                        torch.distributed.init_process_group(\"ccl\", rank=rank, world_size=world_size)\n+                        torch.xpu.set_device(int(os.environ[\"LOCAL_RANK\"]))\n \n                 except Exception as e:\n                     raise EnvironmentError(\n@@ -3997,7 +4000,10 @@ def from_pretrained(\n                     ) from e\n \n             # Get device with index assuming equal number of devices per host\n-            index = None if device_type == \"cpu\" else torch.cuda.current_device()\n+            if device_type == \"xpu\":\n+                index = torch.xpu.current_device()\n+            else:\n+                index = None if device_type == \"cpu\" else torch.cuda.current_device()\n             tp_device = torch.device(device_type, index)\n \n             if index is not None and index > 0:"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 7,
        "deletions": 1
    }
}