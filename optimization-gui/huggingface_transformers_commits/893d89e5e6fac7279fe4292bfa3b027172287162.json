{
    "author": "zucchini-nlp",
    "message": "[omni modality] support composite processor config (#38142)\n\n* dump ugly option to check again tomorrow\n\n* tiny update\n\n* do not save as nested dict yet!\n\n* fix and add tests\n\n* fix dia audio tokenizers\n\n* rename the flag and fix new model Evolla\n\n* fix style\n\n* address comments\n\n* broken from different PRp\n\n* fix saving layoutLM\n\n* delete print\n\n* delete!",
    "sha": "893d89e5e6fac7279fe4292bfa3b027172287162",
    "files": [
        {
            "sha": "b5d9e8f72e0a764c4ac4e5f2ce626272fbd723e2",
            "filename": "src/transformers/feature_extraction_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/893d89e5e6fac7279fe4292bfa3b027172287162/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893d89e5e6fac7279fe4292bfa3b027172287162/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_utils.py?ref=893d89e5e6fac7279fe4292bfa3b027172287162",
            "patch": "@@ -27,9 +27,9 @@\n from .dynamic_module_utils import custom_object_save\n from .utils import (\n     FEATURE_EXTRACTOR_NAME,\n+    PROCESSOR_NAME,\n     PushToHubMixin,\n     TensorType,\n-    cached_file,\n     copy_func,\n     download_url,\n     is_flax_available,\n@@ -44,6 +44,7 @@\n     logging,\n     requires_backends,\n )\n+from .utils.hub import cached_files\n \n \n if TYPE_CHECKING:\n@@ -505,9 +506,9 @@ def get_feature_extractor_dict(\n             feature_extractor_file = FEATURE_EXTRACTOR_NAME\n             try:\n                 # Load from local folder or from cache or download from model Hub and cache\n-                resolved_feature_extractor_file = cached_file(\n+                resolved_feature_extractor_files = cached_files(\n                     pretrained_model_name_or_path,\n-                    feature_extractor_file,\n+                    filenames=[feature_extractor_file, PROCESSOR_NAME],\n                     cache_dir=cache_dir,\n                     force_download=force_download,\n                     proxies=proxies,\n@@ -517,7 +518,9 @@ def get_feature_extractor_dict(\n                     token=token,\n                     user_agent=user_agent,\n                     revision=revision,\n+                    _raise_exceptions_for_missing_entries=False,\n                 )\n+                resolved_feature_extractor_file = resolved_feature_extractor_files[0]\n             except OSError:\n                 # Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\n                 # the original exception.\n@@ -536,6 +539,7 @@ def get_feature_extractor_dict(\n             with open(resolved_feature_extractor_file, encoding=\"utf-8\") as reader:\n                 text = reader.read()\n             feature_extractor_dict = json.loads(text)\n+            feature_extractor_dict = feature_extractor_dict.get(\"feature_extractor\", feature_extractor_dict)\n \n         except json.JSONDecodeError:\n             raise OSError("
        },
        {
            "sha": "4f84c29a9939e14fea2587de6d360457584ca63e",
            "filename": "src/transformers/image_processing_base.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/893d89e5e6fac7279fe4292bfa3b027172287162/src%2Ftransformers%2Fimage_processing_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893d89e5e6fac7279fe4292bfa3b027172287162/src%2Ftransformers%2Fimage_processing_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_base.py?ref=893d89e5e6fac7279fe4292bfa3b027172287162",
            "patch": "@@ -26,14 +26,15 @@\n from .image_utils import is_valid_image, load_image\n from .utils import (\n     IMAGE_PROCESSOR_NAME,\n+    PROCESSOR_NAME,\n     PushToHubMixin,\n-    cached_file,\n     copy_func,\n     download_url,\n     is_offline_mode,\n     is_remote_url,\n     logging,\n )\n+from .utils.hub import cached_files\n \n \n ImageProcessorType = TypeVar(\"ImageProcessorType\", bound=\"ImageProcessingMixin\")\n@@ -329,9 +330,9 @@ def get_image_processor_dict(\n             image_processor_file = image_processor_filename\n             try:\n                 # Load from local folder or from cache or download from model Hub and cache\n-                resolved_image_processor_file = cached_file(\n+                resolved_image_processor_files = cached_files(\n                     pretrained_model_name_or_path,\n-                    image_processor_file,\n+                    filenames=[image_processor_file, PROCESSOR_NAME],\n                     cache_dir=cache_dir,\n                     force_download=force_download,\n                     proxies=proxies,\n@@ -341,7 +342,9 @@ def get_image_processor_dict(\n                     user_agent=user_agent,\n                     revision=revision,\n                     subfolder=subfolder,\n+                    _raise_exceptions_for_missing_entries=False,\n                 )\n+                resolved_image_processor_file = resolved_image_processor_files[0]\n             except OSError:\n                 # Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\n                 # the original exception.\n@@ -360,6 +363,7 @@ def get_image_processor_dict(\n             with open(resolved_image_processor_file, encoding=\"utf-8\") as reader:\n                 text = reader.read()\n             image_processor_dict = json.loads(text)\n+            image_processor_dict = image_processor_dict.get(\"image_processor\", image_processor_dict)\n \n         except json.JSONDecodeError:\n             raise OSError("
        },
        {
            "sha": "59dc75114b9880a27924572eba78f2d4f9bfd494",
            "filename": "src/transformers/models/smolvlm/processing_smolvlm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/893d89e5e6fac7279fe4292bfa3b027172287162/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893d89e5e6fac7279fe4292bfa3b027172287162/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py?ref=893d89e5e6fac7279fe4292bfa3b027172287162",
            "patch": "@@ -179,6 +179,8 @@ def __init__(\n \n     def expand_text_with_image_tokens(self, text, image_rows, image_cols):\n         prompt_strings = []\n+        image_rows = image_rows if image_rows is not None else [[0] * len(text)]\n+        image_cols = image_cols if image_cols is not None else [[0] * len(text)]\n         for sample, sample_rows, sample_cols in zip(text, image_rows, image_cols):\n             # Replace the image token with fake tokens around the expanded image token sequence of length `image_seq_len`\n             image_prompt_strings = []\n@@ -325,8 +327,8 @@ def __call__(\n             images = make_nested_list_of_images(images)\n             vision_inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n \n-            image_rows = vision_inputs.pop(\"rows\", [[0] * len(text)])\n-            image_cols = vision_inputs.pop(\"cols\", [[0] * len(text)])\n+            image_rows = vision_inputs.pop(\"rows\", None)\n+            image_cols = vision_inputs.pop(\"cols\", None)\n             inputs.update(vision_inputs)\n \n             if text is not None:"
        },
        {
            "sha": "baa98881ff3380fe546a4e7d6b2f421943a66e78",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 108,
            "deletions": 79,
            "changes": 187,
            "blob_url": "https://github.com/huggingface/transformers/blob/893d89e5e6fac7279fe4292bfa3b027172287162/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893d89e5e6fac7279fe4292bfa3b027172287162/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=893d89e5e6fac7279fe4292bfa3b027172287162",
            "patch": "@@ -568,7 +568,7 @@ def check_argument_for_proper_class(self, argument_name, argument):\n \n         return proper_class\n \n-    def to_dict(self) -> dict[str, Any]:\n+    def to_dict(self, legacy_serialization=True) -> dict[str, Any]:\n         \"\"\"\n         Serializes this instance to a Python dictionary.\n \n@@ -580,50 +580,61 @@ def to_dict(self) -> dict[str, Any]:\n         # Get the kwargs in `__init__`.\n         sig = inspect.signature(self.__init__)\n         # Only save the attributes that are presented in the kwargs of `__init__`.\n-        attrs_to_save = sig.parameters\n-        # Don't save attributes like `tokenizer`, `image processor` etc.\n-        attrs_to_save = [x for x in attrs_to_save if x not in self.__class__.attributes]\n+        attrs_to_save = list(sig.parameters)\n         # extra attributes to be kept\n         attrs_to_save += [\"auto_map\"]\n \n-        output = {k: v for k, v in output.items() if k in attrs_to_save}\n-\n-        output[\"processor_class\"] = self.__class__.__name__\n+        if legacy_serialization:\n+            # Don't save attributes like `tokenizer`, `image processor` etc. in processor config if `legacy=True`\n+            attrs_to_save = [x for x in attrs_to_save if x not in self.__class__.attributes]\n \n         if \"tokenizer\" in output:\n             del output[\"tokenizer\"]\n-        if \"image_processor\" in output:\n-            del output[\"image_processor\"]\n-        if \"video_processor\" in output:\n-            del output[\"video_processor\"]\n-        if \"feature_extractor\" in output:\n-            del output[\"feature_extractor\"]\n+        if \"qformer_tokenizer\" in output:\n+            del output[\"qformer_tokenizer\"]\n+        if \"protein_tokenizer\" in output:\n+            del output[\"protein_tokenizer\"]\n         if \"chat_template\" in output:\n             del output[\"chat_template\"]\n-        if \"audio_tokenizer\" in output:\n-            del output[\"audio_tokenizer\"]\n \n-        # Some attributes have different names but containing objects that are not simple strings\n+        # Serialize attributes as a dict\n         output = {\n-            k: v\n+            k: v.to_dict() if isinstance(v, PushToHubMixin) else v\n             for k, v in output.items()\n-            if not (isinstance(v, PushToHubMixin) or v.__class__.__name__ == \"BeamSearchDecoderCTC\")\n+            if (\n+                k in attrs_to_save  # keep all attributes that have to be serialized\n+                and v.__class__.__name__ != \"BeamSearchDecoderCTC\"  # remove attributes with that are objects\n+                and (\n+                    (legacy_serialization and not isinstance(v, PushToHubMixin)) or not legacy_serialization\n+                )  # remove `PushToHubMixin` objects\n+            )\n         }\n \n+        # Special case, add `audio_tokenizer` dict which points to model weights and path\n+        if not legacy_serialization and \"audio_tokenizer\" in output:\n+            audio_tokenizer_dict = {\n+                \"audio_tokenizer_class\": self.audio_tokenizer.__class__.__name__,\n+                \"audio_tokenizer_name_or_path\": self.audio_tokenizer.name_or_path,\n+            }\n+            # Update or overwrite, what do audio tokenizers expect when loading?\n+            output[\"audio_tokenizer\"] = audio_tokenizer_dict\n+\n+        output[\"processor_class\"] = self.__class__.__name__\n+\n         return output\n \n-    def to_json_string(self) -> str:\n+    def to_json_string(self, legacy_serialization=True) -> str:\n         \"\"\"\n         Serializes this instance to a JSON string.\n \n         Returns:\n             `str`: String containing all the attributes that make up this feature_extractor instance in JSON format.\n         \"\"\"\n-        dictionary = self.to_dict()\n+        dictionary = self.to_dict(legacy_serialization=legacy_serialization)\n \n         return json.dumps(dictionary, indent=2, sort_keys=True) + \"\\n\"\n \n-    def to_json_file(self, json_file_path: Union[str, os.PathLike]):\n+    def to_json_file(self, json_file_path: Union[str, os.PathLike], legacy_serialization=True):\n         \"\"\"\n         Save this instance to a JSON file.\n \n@@ -632,14 +643,14 @@ def to_json_file(self, json_file_path: Union[str, os.PathLike]):\n                 Path to the JSON file in which this processor instance's parameters will be saved.\n         \"\"\"\n         with open(json_file_path, \"w\", encoding=\"utf-8\") as writer:\n-            writer.write(self.to_json_string())\n+            writer.write(self.to_json_string(legacy_serialization=legacy_serialization))\n \n     def __repr__(self):\n         attributes_repr = [f\"- {name}: {repr(getattr(self, name))}\" for name in self.attributes]\n         attributes_repr = \"\\n\".join(attributes_repr)\n         return f\"{self.__class__.__name__}:\\n{attributes_repr}\\n\\n{self.to_json_string()}\"\n \n-    def save_pretrained(self, save_directory, push_to_hub: bool = False, **kwargs):\n+    def save_pretrained(self, save_directory, push_to_hub: bool = False, legacy_serialization: bool = True, **kwargs):\n         \"\"\"\n         Saves the attributes of this processor (feature extractor, tokenizer...) in the specified directory so that it\n         can be reloaded using the [`~ProcessorMixin.from_pretrained`] method.\n@@ -660,6 +671,10 @@ def save_pretrained(self, save_directory, push_to_hub: bool = False, **kwargs):\n                 Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n                 repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n                 namespace).\n+            legacy_serialization (`bool`, *optional*, defaults to `True`):\n+                Whether or not to save processor attributes in separate config files (legacy) or in processor's config\n+                file as a nested dict. Saving all attributes in a single dict will become the default in future versions.\n+                Set to `legacy_serialization=True` until then.\n             kwargs (`dict[str, Any]`, *optional*):\n                 Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n         \"\"\"\n@@ -694,15 +709,19 @@ def save_pretrained(self, save_directory, push_to_hub: bool = False, **kwargs):\n         save_jinja_files = kwargs.get(\"save_jinja_files\", True)\n \n         for attribute_name in self.attributes:\n-            attribute = getattr(self, attribute_name)\n-            # Include the processor class in the attribute config so this processor can then be reloaded with the\n-            # `AutoProcessor` API.\n-            if hasattr(attribute, \"_set_processor_class\"):\n-                attribute._set_processor_class(self.__class__.__name__)\n+            # Save the tokenizer in its own vocab file. The other attributes are saved as part of `processor_config.json`\n             if attribute_name == \"tokenizer\":\n+                attribute = getattr(self, attribute_name)\n+                if hasattr(attribute, \"_set_processor_class\"):\n+                    attribute._set_processor_class(self.__class__.__name__)\n+\n                 # Propagate save_jinja_files to tokenizer to ensure we don't get conflicts\n                 attribute.save_pretrained(save_directory, save_jinja_files=save_jinja_files)\n-            else:\n+            elif legacy_serialization:\n+                attribute = getattr(self, attribute_name)\n+                # Include the processor class in attribute config so this processor can then be reloaded with `AutoProcessor` API.\n+                if hasattr(attribute, \"_set_processor_class\"):\n+                    attribute._set_processor_class(self.__class__.__name__)\n                 attribute.save_pretrained(save_directory)\n \n         if self._auto_class is not None:\n@@ -720,9 +739,7 @@ def save_pretrained(self, save_directory, push_to_hub: bool = False, **kwargs):\n             save_directory, LEGACY_PROCESSOR_CHAT_TEMPLATE_FILE\n         )  # Legacy filename\n         chat_template_dir = os.path.join(save_directory, CHAT_TEMPLATE_DIR)\n-        output_audio_tokenizer_file = os.path.join(save_directory, AUDIO_TOKENIZER_NAME)\n \n-        processor_dict = self.to_dict()\n         # Save `chat_template` in its own file. We can't get it from `processor_dict` as we popped it in `to_dict`\n         # to avoid serializing chat template in json config file. So let's get it from `self` directly\n         if self.chat_template is not None:\n@@ -763,24 +780,39 @@ def save_pretrained(self, save_directory, push_to_hub: bool = False, **kwargs):\n                     \"separate files using the `save_jinja_files` argument.\"\n                 )\n \n-        if self.audio_tokenizer is not None:\n-            audio_tokenizer_class = self.audio_tokenizer.__class__.__name__\n-            audio_tokenizer_name_or_path = self.audio_tokenizer.name_or_path\n+        if legacy_serialization:\n+            output_audio_tokenizer_file = os.path.join(save_directory, AUDIO_TOKENIZER_NAME)\n+            processor_dict = self.to_dict()\n \n-            audio_tokenizer_dict = {\n-                \"audio_tokenizer_class\": audio_tokenizer_class,\n-                \"audio_tokenizer_name_or_path\": audio_tokenizer_name_or_path,\n-            }\n-            audio_tokenizer_json = json.dumps(audio_tokenizer_dict, indent=2, sort_keys=True) + \"\\n\"\n+            # For now, let's not save to `processor_config.json` if the processor doesn't have extra attributes and\n+            # `auto_map` is not specified.\n+            if set(processor_dict.keys()) != {\"processor_class\"}:\n+                self.to_json_file(output_processor_file)\n+                logger.info(f\"processor saved in {output_processor_file}\")\n \n-            with open(output_audio_tokenizer_file, \"w\", encoding=\"utf-8\") as writer:\n-                writer.write(audio_tokenizer_json)\n+            if set(processor_dict.keys()) == {\"processor_class\"}:\n+                return_files = []\n+            else:\n+                return_files = [output_processor_file]\n+\n+            if self.audio_tokenizer is not None:\n+                audio_tokenizer_class = self.audio_tokenizer.__class__.__name__\n+                audio_tokenizer_name_or_path = self.audio_tokenizer.name_or_path\n+                audio_tokenizer_dict = {\n+                    \"audio_tokenizer_class\": audio_tokenizer_class,\n+                    \"audio_tokenizer_name_or_path\": audio_tokenizer_name_or_path,\n+                }\n+                audio_tokenizer_json = json.dumps(audio_tokenizer_dict, indent=2, sort_keys=True) + \"\\n\"\n+                with open(output_audio_tokenizer_file, \"w\", encoding=\"utf-8\") as writer:\n+                    writer.write(audio_tokenizer_json)\n \n-        # For now, let's not save to `processor_config.json` if the processor doesn't have extra attributes and\n-        # `auto_map` is not specified.\n-        if set(processor_dict.keys()) != {\"processor_class\"}:\n-            self.to_json_file(output_processor_file)\n+        # Create a unified `preprocessor_config.json` and save all attributes as a composite config, except for tokenizers\n+        # NOTE: this will become the default way to save all processor attrbiutes in future versions. Toggled off for now to give\n+        # us time for smoother transition\n+        else:\n+            self.to_json_file(output_processor_file, legacy_serialization=False)\n             logger.info(f\"processor saved in {output_processor_file}\")\n+            return_files = [output_processor_file]\n \n         if push_to_hub:\n             self._upload_modified_files(\n@@ -791,9 +823,7 @@ def save_pretrained(self, save_directory, push_to_hub: bool = False, **kwargs):\n                 token=kwargs.get(\"token\"),\n             )\n \n-        if set(processor_dict.keys()) == {\"processor_class\"}:\n-            return []\n-        return [output_processor_file]\n+        return return_files\n \n     @classmethod\n     def get_processor_dict(\n@@ -998,43 +1028,24 @@ def get_processor_dict(\n         if chat_templates:\n             kwargs[\"chat_template\"] = chat_templates\n \n-        # Same as chat template, adding as kwarg after loading the model\n-        audio_tokenizer = None\n-        if resolved_audio_tokenizer_file is not None:\n-            with open(resolved_audio_tokenizer_file, \"r\", encoding=\"utf-8\") as reader:\n-                # The json contains the references we need to init the correct model\n-                audio_tokenizer_references = json.load(reader)\n-                audio_tokenizer_class = cls.get_possibly_dynamic_module(\n-                    audio_tokenizer_references[\"audio_tokenizer_class\"]\n-                )\n-                audio_tokenizer_path = audio_tokenizer_references[\"audio_tokenizer_name_or_path\"]\n-\n-            audio_tokenizer = audio_tokenizer_class.from_pretrained(audio_tokenizer_path, **audio_tokenizer_kwargs)\n-\n-        if audio_tokenizer is not None:\n-            kwargs[\"audio_tokenizer\"] = audio_tokenizer\n-\n         # Existing processors on the Hub created before #27761 being merged don't have `processor_config.json` (if not\n         # updated afterward), and we need to keep `from_pretrained` work. So here it fallbacks to the empty dict.\n         # (`cached_file` called using `_raise_exceptions_for_missing_entries=False` to avoid exception)\n         # However, for models added in the future, we won't get the expected error if this file is missing.\n         if resolved_processor_file is None:\n             # In any case we need to pass `chat_template` if it is available\n             processor_dict = {}\n-            if \"chat_template\" in kwargs:\n-                processor_dict[\"chat_template\"] = kwargs.pop(\"chat_template\")\n-            if \"audio_tokenizer\" in kwargs:\n-                processor_dict[\"audio_tokenizer\"] = kwargs.pop(\"audio_tokenizer\")\n-            return processor_dict, kwargs\n-\n-        try:\n-            # Load processor dict\n-            with open(resolved_processor_file, encoding=\"utf-8\") as reader:\n-                text = reader.read()\n-            processor_dict = json.loads(text)\n+        else:\n+            try:\n+                # Load processor dict\n+                with open(resolved_processor_file, encoding=\"utf-8\") as reader:\n+                    text = reader.read()\n+                processor_dict = json.loads(text)\n \n-        except json.JSONDecodeError:\n-            raise OSError(f\"It looks like the config file at '{resolved_processor_file}' is not a valid JSON file.\")\n+            except json.JSONDecodeError:\n+                raise OSError(\n+                    f\"It looks like the config file at '{resolved_processor_file}' is not a valid JSON file.\"\n+                )\n \n         if is_local:\n             logger.info(f\"loading configuration file {resolved_processor_file}\")\n@@ -1049,8 +1060,26 @@ def get_processor_dict(\n \n         if \"chat_template\" in kwargs:\n             processor_dict[\"chat_template\"] = kwargs.pop(\"chat_template\")\n-        if \"audio_tokenizer\" in kwargs:\n-            processor_dict[\"audio_tokenizer\"] = kwargs.pop(\"audio_tokenizer\")\n+\n+        # Audio tokenizer needs to load the model checkpoint first, because the saved\n+        # json file contains only references to the model path and repo id\n+        if resolved_audio_tokenizer_file is not None or \"audio_tokenizer\" in processor_dict:\n+            if resolved_audio_tokenizer_file is not None:\n+                reader = open(resolved_audio_tokenizer_file, \"r\", encoding=\"utf-8\")\n+                audio_tokenizer_dict = reader.read()\n+                audio_tokenizer_dict = json.loads(audio_tokenizer_dict)\n+            else:\n+                audio_tokenizer_dict = processor_dict[\"audio_tokenizer\"]\n+\n+            audio_tokenizer_class = cls.get_possibly_dynamic_module(audio_tokenizer_dict[\"audio_tokenizer_class\"])\n+            audio_tokenizer_path = audio_tokenizer_dict[\"audio_tokenizer_name_or_path\"]\n+            processor_dict[\"audio_tokenizer\"] = audio_tokenizer_class.from_pretrained(\n+                audio_tokenizer_path, **audio_tokenizer_kwargs\n+            )\n+\n+        # Pop attributes if saved in a single processor dict, they are loaded in `_get_arguments_from_pretrained`\n+        for attribute in cls.attributes:\n+            processor_dict.pop(attribute, None)\n \n         return processor_dict, kwargs\n "
        },
        {
            "sha": "f6c2f11684df58ebbe1c1eaec452e2826a7d3906",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/893d89e5e6fac7279fe4292bfa3b027172287162/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893d89e5e6fac7279fe4292bfa3b027172287162/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=893d89e5e6fac7279fe4292bfa3b027172287162",
            "patch": "@@ -438,12 +438,11 @@ def cached_files(\n                         f\"'https://huggingface.co/{path_or_repo_id}/tree/{revision_}' for available files.\"\n                     )\n                 else:\n-                    return None\n+                    continue\n             existing_files.append(resolved_file)\n \n-    # All files exist\n-    if len(existing_files) == len(full_filenames):\n-        return existing_files\n+    if os.path.isdir(path_or_repo_id):\n+        return existing_files if existing_files else None\n \n     if cache_dir is None:\n         cache_dir = TRANSFORMERS_CACHE"
        },
        {
            "sha": "90aa5f43de9e318fc7f83ac3dcc68ac89215d738",
            "filename": "src/transformers/video_processing_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 31,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/893d89e5e6fac7279fe4292bfa3b027172287162/src%2Ftransformers%2Fvideo_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893d89e5e6fac7279fe4292bfa3b027172287162/src%2Ftransformers%2Fvideo_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_processing_utils.py?ref=893d89e5e6fac7279fe4292bfa3b027172287162",
            "patch": "@@ -35,10 +35,11 @@\n )\n from .processing_utils import Unpack, VideosKwargs\n from .utils import (\n+    IMAGE_PROCESSOR_NAME,\n+    PROCESSOR_NAME,\n     VIDEO_PROCESSOR_NAME,\n     TensorType,\n     add_start_docstrings,\n-    cached_file,\n     copy_func,\n     download_url,\n     is_offline_mode,\n@@ -49,6 +50,7 @@\n     is_torchvision_v2_available,\n     logging,\n )\n+from .utils.hub import cached_files\n from .utils.import_utils import requires\n from .video_utils import (\n     VideoInput,\n@@ -463,7 +465,7 @@ def from_pretrained(\n                   [`~video_processing_utils.VideoProcessorBase.save_pretrained`] method, e.g.,\n                   `./my_model_directory/`.\n                 - a path or url to a saved video processor JSON *file*, e.g.,\n-                  `./my_model_directory/preprocessor_config.json`.\n+                  `./my_model_directory/video_preprocessor_config.json`.\n             cache_dir (`str` or `os.PathLike`, *optional*):\n                 Path to a directory in which a downloaded pretrained model video processor should be cached if the\n                 standard cache should not be used.\n@@ -518,7 +520,7 @@ def from_pretrained(\n         video_processor = LlavaOnevisionVideoProcessor.from_pretrained(\n             \"./test/saved_model/\"\n         )  # E.g. video processor (or model) was saved using *save_pretrained('./test/saved_model/')*\n-        video_processor = LlavaOnevisionVideoProcessor.from_pretrained(\"./test/saved_model/preprocessor_config.json\")\n+        video_processor = LlavaOnevisionVideoProcessor.from_pretrained(\"./test/saved_model/video_preprocessor_config.json\")\n         video_processor = LlavaOnevisionVideoProcessor.from_pretrained(\n             \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\", do_normalize=False, foo=False\n         )\n@@ -673,14 +675,13 @@ def get_video_processor_dict(\n             video_processor_file = pretrained_model_name_or_path\n             resolved_video_processor_file = download_url(pretrained_model_name_or_path)\n         else:\n+            video_processor_file = VIDEO_PROCESSOR_NAME\n             try:\n-                # Try to load with a new config name first and if not successful try with\n-                # the old file name. In case we can load with old name only, raise a deprecation warning\n-                # Deprecated until v5.0\n-                video_processor_file = VIDEO_PROCESSOR_NAME\n-                resolved_video_processor_file = cached_file(\n+                # Try to load with a new config name first and if not successfull try with the old file name\n+                # NOTE: we will gradually change to saving all processor configs as nested dict in PROCESSOR_NAME\n+                resolved_video_processor_files = cached_files(\n                     pretrained_model_name_or_path,\n-                    video_processor_file,\n+                    filenames=[VIDEO_PROCESSOR_NAME, IMAGE_PROCESSOR_NAME, PROCESSOR_NAME],\n                     cache_dir=cache_dir,\n                     force_download=force_download,\n                     proxies=proxies,\n@@ -690,29 +691,10 @@ def get_video_processor_dict(\n                     user_agent=user_agent,\n                     revision=revision,\n                     subfolder=subfolder,\n+                    _raise_exceptions_for_missing_entries=False,\n                 )\n-            except OSError:\n-                video_processor_file = \"preprocessor_config.json\"\n-                resolved_video_processor_file = cached_file(\n-                    pretrained_model_name_or_path,\n-                    video_processor_file,\n-                    cache_dir=cache_dir,\n-                    force_download=force_download,\n-                    proxies=proxies,\n-                    resume_download=resume_download,\n-                    local_files_only=local_files_only,\n-                    token=token,\n-                    user_agent=user_agent,\n-                    revision=revision,\n-                    subfolder=subfolder,\n-                )\n-                logger.warning_once(\n-                    \"You have video processor config saved in `preprocessor.json` file which is deprecated. \"\n-                    \"Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename \"\n-                    \"the file or load and save the processor back which renames it automatically. \"\n-                    \"Loading from `preprocessor.json` will be removed in v5.0.\"\n-                )\n-            except OSError:\n+                resolved_video_processor_file = resolved_video_processor_files[0]\n+            except EnvironmentError:\n                 # Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\n                 # the original exception.\n                 raise\n@@ -730,6 +712,7 @@ def get_video_processor_dict(\n             with open(resolved_video_processor_file, \"r\", encoding=\"utf-8\") as reader:\n                 text = reader.read()\n             video_processor_dict = json.loads(text)\n+            video_processor_dict = video_processor_dict.get(\"video_processor\", video_processor_dict)\n \n         except json.JSONDecodeError:\n             raise OSError("
        },
        {
            "sha": "7ecdf1756bb416bb81328f5125013f21740c0fca",
            "filename": "tests/models/auto/test_feature_extraction_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/893d89e5e6fac7279fe4292bfa3b027172287162/tests%2Fmodels%2Fauto%2Ftest_feature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893d89e5e6fac7279fe4292bfa3b027172287162/tests%2Fmodels%2Fauto%2Ftest_feature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_feature_extraction_auto.py?ref=893d89e5e6fac7279fe4292bfa3b027172287162",
            "patch": "@@ -95,7 +95,7 @@ def test_revision_not_found(self):\n     def test_feature_extractor_not_found(self):\n         with self.assertRaisesRegex(\n             EnvironmentError,\n-            \"hf-internal-testing/config-no-model does not appear to have a file named preprocessor_config.json.\",\n+            \"Can't load feature extractor for 'hf-internal-testing/config-no-model'.\",\n         ):\n             _ = AutoFeatureExtractor.from_pretrained(\"hf-internal-testing/config-no-model\")\n "
        },
        {
            "sha": "bb45b5abca383d536db0bcbeee706573ccfd2361",
            "filename": "tests/models/auto/test_image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/893d89e5e6fac7279fe4292bfa3b027172287162/tests%2Fmodels%2Fauto%2Ftest_image_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893d89e5e6fac7279fe4292bfa3b027172287162/tests%2Fmodels%2Fauto%2Ftest_image_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_image_processing_auto.py?ref=893d89e5e6fac7279fe4292bfa3b027172287162",
            "patch": "@@ -144,7 +144,7 @@ def test_revision_not_found(self):\n     def test_image_processor_not_found(self):\n         with self.assertRaisesRegex(\n             EnvironmentError,\n-            \"hf-internal-testing/config-no-model does not appear to have a file named preprocessor_config.json.\",\n+            \"Can't load image processor for 'hf-internal-testing/config-no-model'.\",\n         ):\n             _ = AutoImageProcessor.from_pretrained(\"hf-internal-testing/config-no-model\")\n "
        },
        {
            "sha": "1aa89d92cb2dd05a0560e4b1c9f6c38a57d98014",
            "filename": "tests/models/auto/test_video_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/893d89e5e6fac7279fe4292bfa3b027172287162/tests%2Fmodels%2Fauto%2Ftest_video_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893d89e5e6fac7279fe4292bfa3b027172287162/tests%2Fmodels%2Fauto%2Ftest_video_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_video_processing_auto.py?ref=893d89e5e6fac7279fe4292bfa3b027172287162",
            "patch": "@@ -143,7 +143,7 @@ def test_revision_not_found(self):\n     def test_video_processor_not_found(self):\n         with self.assertRaisesRegex(\n             EnvironmentError,\n-            \"hf-internal-testing/config-no-model does not appear to have a file named preprocessor_config.json.\",\n+            \"Can't load video processor for 'hf-internal-testing/config-no-model'.\",\n         ):\n             _ = AutoVideoProcessor.from_pretrained(\"hf-internal-testing/config-no-model\")\n "
        },
        {
            "sha": "ae84ef4ff21ea6ad97607c6ee10bd43c576820e4",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/893d89e5e6fac7279fe4292bfa3b027172287162/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893d89e5e6fac7279fe4292bfa3b027172287162/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=893d89e5e6fac7279fe4292bfa3b027172287162",
            "patch": "@@ -217,6 +217,36 @@ def test_processor_from_and_save_pretrained(self):\n                     if \"tokenizer\" not in attribute:\n                         self.assertEqual(repr(attribute_first), repr(attribute_second))\n \n+    def test_processor_from_and_save_pretrained_as_nested_dict(self):\n+        processor_first = self.get_processor()\n+\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            # Save with `legacy_serialization=False` so that all attrbiutes are saved in one json file\n+            saved_files = processor_first.save_pretrained(tmpdirname, legacy_serialization=False)\n+            check_json_file_has_correct_format(saved_files[0])\n+\n+            # Load it back and check if loaded correctly\n+            processor_second = self.processor_class.from_pretrained(tmpdirname)\n+            self.assertEqual(processor_second.to_dict(), processor_first.to_dict())\n+\n+            # Try to load each attribute separately from saved directory\n+            for attribute in processor_first.attributes:\n+                attribute_class_name = getattr(processor_first, f\"{attribute}_class\")\n+                if isinstance(attribute_class_name, tuple):\n+                    if attribute == \"image_processor\":\n+                        # TODO: @yoni, change logic in v4.52 (when use_fast set to True by default)\n+                        attribute_class_name = attribute_class_name[0]\n+                    else:\n+                        attribute_class_name = attribute_class_name[-1]\n+\n+                attribute_class = processor_class_from_name(attribute_class_name)\n+                attribute_reloaded = attribute_class.from_pretrained(tmpdirname)\n+                attribute_first = getattr(processor_first, attribute)\n+\n+                # tokenizer repr contains model-path from where we loaded\n+                if \"tokenizer\" not in attribute:\n+                    self.assertEqual(repr(attribute_first), repr(attribute_reloaded))\n+\n     def test_model_input_names(self):\n         processor = self.get_processor()\n "
        }
    ],
    "stats": {
        "total": 301,
        "additions": 176,
        "deletions": 125
    }
}