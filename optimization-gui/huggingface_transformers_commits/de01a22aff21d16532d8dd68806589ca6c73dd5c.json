{
    "author": "wangzhen0518",
    "message": "Fix edge case for tokenize (#36277) (#36555)\n\n* Fix edge case for tokenize (#36277)\n\n* Fix tokenizing dtype for float input cases\n\n* add test for empty input string\n\n* deal empty list of list like [[]]\n\n* add tests for tokenizer for models with input that is not plain text",
    "sha": "de01a22aff21d16532d8dd68806589ca6c73dd5c",
    "files": [
        {
            "sha": "a8b19ef967f2c080d3fb4b03490f1c15844167ad",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de01a22aff21d16532d8dd68806589ca6c73dd5c/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de01a22aff21d16532d8dd68806589ca6c73dd5c/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=de01a22aff21d16532d8dd68806589ca6c73dd5c",
            "patch": "@@ -32,6 +32,7 @@\n     is_g2p_en_available,\n     is_librosa_available,\n     is_mistral_common_available,\n+    is_mlx_available,\n     is_pretty_midi_available,\n )\n \n@@ -239,6 +240,7 @@\n         \"is_flax_available\",\n         \"is_keras_nlp_available\",\n         \"is_matplotlib_available\",\n+        \"is_mlx_available\",\n         \"is_phonemizer_available\",\n         \"is_psutil_available\",\n         \"is_py3nvml_available\","
        },
        {
            "sha": "18f365dd2a94cf1dc81e10563575e0cf00d22d54",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 39,
            "deletions": 8,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/de01a22aff21d16532d8dd68806589ca6c73dd5c/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de01a22aff21d16532d8dd68806589ca6c73dd5c/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=de01a22aff21d16532d8dd68806589ca6c73dd5c",
            "patch": "@@ -87,6 +87,17 @@ def import_protobuf_decode_error(error_message=\"\"):\n         raise ImportError(PROTOBUF_IMPORT_ERROR.format(error_message))\n \n \n+def flatten(arr: list):\n+    res = []\n+    if len(arr) > 0:\n+        for sub_arr in arr:\n+            if isinstance(arr[0], (list, tuple)):\n+                res.extend(flatten(sub_arr))\n+            else:\n+                res.append(sub_arr)\n+    return res\n+\n+\n if is_tokenizers_available():\n     from tokenizers import AddedToken\n     from tokenizers import Encoding as EncodingFast\n@@ -714,45 +725,65 @@ def convert_to_tensors(\n                 )\n             import tensorflow as tf\n \n-            as_tensor = tf.constant\n+            def as_tensor(value, dtype=None):\n+                if len(flatten(value)) == 0 and dtype is None:\n+                    dtype = tf.int32\n+                return tf.constant(value, dtype=dtype)\n+\n             is_tensor = tf.is_tensor\n+\n         elif tensor_type == TensorType.PYTORCH:\n             if not is_torch_available():\n                 raise ImportError(\"Unable to convert output to PyTorch tensors format, PyTorch is not installed.\")\n             import torch\n \n-            is_tensor = torch.is_tensor\n-\n             def as_tensor(value, dtype=None):\n-                if isinstance(value, list) and isinstance(value[0], np.ndarray):\n+                if isinstance(value, list) and len(value) > 0 and isinstance(value[0], np.ndarray):\n                     return torch.from_numpy(np.array(value))\n-                return torch.tensor(value)\n+                if len(flatten(value)) == 0 and dtype is None:\n+                    dtype = torch.int64\n+                return torch.tensor(value, dtype=dtype)\n+\n+            is_tensor = torch.is_tensor\n \n         elif tensor_type == TensorType.JAX:\n             if not is_flax_available():\n                 raise ImportError(\"Unable to convert output to JAX tensors format, JAX is not installed.\")\n             import jax.numpy as jnp  # noqa: F811\n \n-            as_tensor = jnp.array\n+            def as_tensor(value, dtype=None):\n+                if len(flatten(value)) == 0 and dtype is None:\n+                    dtype = jnp.int32\n+                return jnp.array(value, dtype=dtype)\n+\n             is_tensor = is_jax_tensor\n \n         elif tensor_type == TensorType.MLX:\n             if not is_mlx_available():\n                 raise ImportError(\"Unable to convert output to MLX tensors format, MLX is not installed.\")\n             import mlx.core as mx\n \n-            as_tensor = mx.array\n+            def as_tensor(value, dtype=None):\n+                if len(flatten(value)) == 0 and dtype is None:\n+                    dtype = mx.int32\n+                return mx.array(value, dtype=dtype)\n \n             def is_tensor(obj):\n                 return isinstance(obj, mx.array)\n         else:\n \n             def as_tensor(value, dtype=None):\n-                if isinstance(value, (list, tuple)) and isinstance(value[0], (list, tuple, np.ndarray)):\n+                if (\n+                    isinstance(value, (list, tuple))\n+                    and len(value) > 0\n+                    and isinstance(value[0], (list, tuple, np.ndarray))\n+                ):\n                     value_lens = [len(val) for val in value]\n                     if len(set(value_lens)) > 1 and dtype is None:\n                         # we have a ragged list so handle explicitly\n                         value = as_tensor([np.asarray(val) for val in value], dtype=object)\n+                if len(flatten(value)) == 0 and dtype is None:\n+                    dtype = np.int64\n                 return np.asarray(value, dtype=dtype)\n \n             is_tensor = is_numpy_array"
        },
        {
            "sha": "a96f3b5b6aad27e23a2b31bfda5a6a46f04d906c",
            "filename": "tests/models/layoutlmv2/test_tokenization_layoutlmv2.py",
            "status": "modified",
            "additions": 91,
            "deletions": 0,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/de01a22aff21d16532d8dd68806589ca6c73dd5c/tests%2Fmodels%2Flayoutlmv2%2Ftest_tokenization_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de01a22aff21d16532d8dd68806589ca6c73dd5c/tests%2Fmodels%2Flayoutlmv2%2Ftest_tokenization_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv2%2Ftest_tokenization_layoutlmv2.py?ref=de01a22aff21d16532d8dd68806589ca6c73dd5c",
            "patch": "@@ -25,6 +25,8 @@\n     AddedToken,\n     LayoutLMv2TokenizerFast,\n     SpecialTokensMixin,\n+    is_flax_available,\n+    is_mlx_available,\n     is_tf_available,\n     is_torch_available,\n     logging,\n@@ -100,6 +102,38 @@ def get_question_words_and_boxes_batch(self):\n \n         return questions, words, boxes\n \n+    def get_empty_words_and_boxes(self):\n+        words = [\"test\", \"empty\", \"\"]\n+        boxes = [[423, 237, 440, 251], [427, 272, 441, 287], [419, 115, 437, 129]]\n+\n+        return words, boxes\n+\n+    def get_empty_words_and_boxes_batch(self):\n+        words = [[\"test\", \"empty\", \"\"], [\"one\", \"more\", \"empty\", \"\"]]\n+        boxes = [\n+            [[423, 237, 440, 251], [427, 272, 441, 287], [419, 115, 437, 129]],\n+            [[961, 885, 992, 912], [256, 38, 330, 58], [256, 38, 330, 58], [336, 42, 353, 57]],\n+        ]\n+\n+        return words, boxes\n+\n+    def get_empty_question_words_and_boxes(self):\n+        question = \"\"\n+        words = [\"test\", \"empty\", \"\"]\n+        boxes = [[423, 237, 440, 251], [427, 272, 441, 287], [419, 115, 437, 129]]\n+\n+        return question, words, boxes\n+\n+    def get_empty_question_words_and_boxes_batch(self):\n+        questions = [\"what's his name?\", \"\"]\n+        words = [[\"test\", \"empty\", \"\"], [\"one\", \"more\", \"empty\", \"\"]]\n+        boxes = [\n+            [[423, 237, 440, 251], [427, 272, 441, 287], [419, 115, 437, 129]],\n+            [[961, 885, 992, 912], [256, 38, 330, 58], [256, 38, 330, 58], [336, 42, 353, 57]],\n+        ]\n+\n+        return questions, words, boxes\n+\n     @classmethod\n     def setUpClass(cls):\n         super().setUpClass()\n@@ -2386,3 +2420,60 @@ def test_chat_template_return_assistant_tokens_mask(self):\n     @unittest.skip(\"Chat is not supported\")\n     def test_chat_template_return_assistant_tokens_mask_truncated(self):\n         pass\n+\n+    def test_empty_input_string(self):\n+        tokenizer_return_type = []\n+        output_tensor_type = []\n+\n+        if is_torch_available():\n+            import numpy as np\n+            import torch\n+\n+            tokenizer_return_type.append(\"pt\")\n+            output_tensor_type.append(torch.int64)\n+            tokenizer_return_type.append(\"np\")\n+            output_tensor_type.append(np.int64)\n+\n+        if is_tf_available():\n+            import tensorflow as tf\n+\n+            tokenizer_return_type.append(\"tf\")\n+            output_tensor_type.append(tf.int32)\n+\n+        if is_flax_available():\n+            import jax.numpy as jnp\n+\n+            tokenizer_return_type.append(\"jax\")\n+            output_tensor_type.append(jnp.int32)\n+\n+        if is_mlx_available():\n+            import mlx.core as mx\n+\n+            tokenizer_return_type.append(\"mlx\")\n+            output_tensor_type.append(mx.int32)\n+\n+        if len(tokenizer_return_type) == 0:\n+            self.skipTest(reason=\"No expected framework from PT, TF, JAX or MLX found\")\n+\n+        tokenizers = self.get_tokenizers()\n+        for tokenizer in tokenizers:\n+            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n+                words, boxes = self.get_empty_words_and_boxes()\n+                for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n+                    output = tokenizer(words, boxes=boxes, return_tensors=return_type)\n+                    self.assertEqual(output.input_ids.dtype, target_type)\n+\n+                question, words, boxes = self.get_empty_question_words_and_boxes()\n+                for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n+                    output = tokenizer(words, boxes=boxes, return_tensors=return_type)\n+                    self.assertEqual(output.input_ids.dtype, target_type)\n+\n+                words, boxes = self.get_empty_words_and_boxes_batch()\n+                for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n+                    output = tokenizer(words, boxes=boxes, padding=True, return_tensors=return_type)\n+                    self.assertEqual(output.input_ids.dtype, target_type)\n+\n+                question, words, boxes = self.get_empty_question_words_and_boxes_batch()\n+                for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n+                    output = tokenizer(words, boxes=boxes, padding=True, return_tensors=return_type)\n+                    self.assertEqual(output.input_ids.dtype, target_type)"
        },
        {
            "sha": "c487e662bf9a673f21d4e648e351d4426a2becb8",
            "filename": "tests/models/layoutlmv3/test_tokenization_layoutlmv3.py",
            "status": "modified",
            "additions": 91,
            "deletions": 0,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/de01a22aff21d16532d8dd68806589ca6c73dd5c/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de01a22aff21d16532d8dd68806589ca6c73dd5c/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py?ref=de01a22aff21d16532d8dd68806589ca6c73dd5c",
            "patch": "@@ -26,6 +26,8 @@\n     AddedToken,\n     LayoutLMv3TokenizerFast,\n     SpecialTokensMixin,\n+    is_flax_available,\n+    is_mlx_available,\n     is_tf_available,\n     is_torch_available,\n     logging,\n@@ -92,6 +94,38 @@ def get_question_words_and_boxes_batch(self):\n \n         return questions, words, boxes\n \n+    def get_empty_words_and_boxes(self):\n+        words = [\"test\", \"empty\", \"\"]\n+        boxes = [[423, 237, 440, 251], [427, 272, 441, 287], [419, 115, 437, 129]]\n+\n+        return words, boxes\n+\n+    def get_empty_words_and_boxes_batch(self):\n+        words = [[\"test\", \"empty\", \"\"], [\"one\", \"more\", \"empty\", \"\"]]\n+        boxes = [\n+            [[423, 237, 440, 251], [427, 272, 441, 287], [419, 115, 437, 129]],\n+            [[961, 885, 992, 912], [256, 38, 330, 58], [256, 38, 330, 58], [336, 42, 353, 57]],\n+        ]\n+\n+        return words, boxes\n+\n+    def get_empty_question_words_and_boxes(self):\n+        question = \"\"\n+        words = [\"test\", \"empty\", \"\"]\n+        boxes = [[423, 237, 440, 251], [427, 272, 441, 287], [419, 115, 437, 129]]\n+\n+        return question, words, boxes\n+\n+    def get_empty_question_words_and_boxes_batch(self):\n+        questions = [\"what's his name?\", \"\"]\n+        words = [[\"test\", \"empty\", \"\"], [\"one\", \"more\", \"empty\", \"\"]]\n+        boxes = [\n+            [[423, 237, 440, 251], [427, 272, 441, 287], [419, 115, 437, 129]],\n+            [[961, 885, 992, 912], [256, 38, 330, 58], [256, 38, 330, 58], [336, 42, 353, 57]],\n+        ]\n+\n+        return questions, words, boxes\n+\n     @classmethod\n     def setUpClass(cls):\n         super().setUpClass()\n@@ -2310,3 +2344,60 @@ def test_chat_template_return_assistant_tokens_mask(self):\n     @unittest.skip(\"Chat is not supported\")\n     def test_chat_template_return_assistant_tokens_mask_truncated(self):\n         pass\n+\n+    def test_empty_input_string(self):\n+        tokenizer_return_type = []\n+        output_tensor_type = []\n+\n+        if is_torch_available():\n+            import numpy as np\n+            import torch\n+\n+            tokenizer_return_type.append(\"pt\")\n+            output_tensor_type.append(torch.int64)\n+            tokenizer_return_type.append(\"np\")\n+            output_tensor_type.append(np.int64)\n+\n+        if is_tf_available():\n+            import tensorflow as tf\n+\n+            tokenizer_return_type.append(\"tf\")\n+            output_tensor_type.append(tf.int32)\n+\n+        if is_flax_available():\n+            import jax.numpy as jnp\n+\n+            tokenizer_return_type.append(\"jax\")\n+            output_tensor_type.append(jnp.int32)\n+\n+        if is_mlx_available():\n+            import mlx.core as mx\n+\n+            tokenizer_return_type.append(\"mlx\")\n+            output_tensor_type.append(mx.int32)\n+\n+        if len(tokenizer_return_type) == 0:\n+            self.skipTest(reason=\"No expected framework from PT, TF, JAX or MLX found\")\n+\n+        tokenizers = self.get_tokenizers()\n+        for tokenizer in tokenizers:\n+            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n+                words, boxes = self.get_empty_words_and_boxes()\n+                for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n+                    output = tokenizer(words, boxes=boxes, return_tensors=return_type)\n+                    self.assertEqual(output.input_ids.dtype, target_type)\n+\n+                question, words, boxes = self.get_empty_question_words_and_boxes()\n+                for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n+                    output = tokenizer(words, boxes=boxes, return_tensors=return_type)\n+                    self.assertEqual(output.input_ids.dtype, target_type)\n+\n+                words, boxes = self.get_empty_words_and_boxes_batch()\n+                for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n+                    output = tokenizer(words, boxes=boxes, padding=True, return_tensors=return_type)\n+                    self.assertEqual(output.input_ids.dtype, target_type)\n+\n+                question, words, boxes = self.get_empty_question_words_and_boxes_batch()\n+                for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n+                    output = tokenizer(words, boxes=boxes, padding=True, return_tensors=return_type)\n+                    self.assertEqual(output.input_ids.dtype, target_type)"
        },
        {
            "sha": "21bd0a469dd4afebfb40e87907eb16aedb1f8d1a",
            "filename": "tests/models/layoutxlm/test_tokenization_layoutxlm.py",
            "status": "modified",
            "additions": 91,
            "deletions": 0,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/de01a22aff21d16532d8dd68806589ca6c73dd5c/tests%2Fmodels%2Flayoutxlm%2Ftest_tokenization_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de01a22aff21d16532d8dd68806589ca6c73dd5c/tests%2Fmodels%2Flayoutxlm%2Ftest_tokenization_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutxlm%2Ftest_tokenization_layoutxlm.py?ref=de01a22aff21d16532d8dd68806589ca6c73dd5c",
            "patch": "@@ -23,6 +23,8 @@\n     AddedToken,\n     LayoutXLMTokenizerFast,\n     SpecialTokensMixin,\n+    is_flax_available,\n+    is_mlx_available,\n     is_tf_available,\n     is_torch_available,\n     logging,\n@@ -94,6 +96,38 @@ def get_question_words_and_boxes_batch(self):\n \n         return questions, words, boxes\n \n+    def get_empty_words_and_boxes(self):\n+        words = [\"test\", \"empty\", \"\"]\n+        boxes = [[423, 237, 440, 251], [427, 272, 441, 287], [419, 115, 437, 129]]\n+\n+        return words, boxes\n+\n+    def get_empty_words_and_boxes_batch(self):\n+        words = [[\"test\", \"empty\", \"\"], [\"one\", \"more\", \"empty\", \"\"]]\n+        boxes = [\n+            [[423, 237, 440, 251], [427, 272, 441, 287], [419, 115, 437, 129]],\n+            [[961, 885, 992, 912], [256, 38, 330, 58], [256, 38, 330, 58], [336, 42, 353, 57]],\n+        ]\n+\n+        return words, boxes\n+\n+    def get_empty_question_words_and_boxes(self):\n+        question = \"\"\n+        words = [\"test\", \"empty\", \"\"]\n+        boxes = [[423, 237, 440, 251], [427, 272, 441, 287], [419, 115, 437, 129]]\n+\n+        return question, words, boxes\n+\n+    def get_empty_question_words_and_boxes_batch(self):\n+        questions = [\"what's his name?\", \"\"]\n+        words = [[\"test\", \"empty\", \"\"], [\"one\", \"more\", \"empty\", \"\"]]\n+        boxes = [\n+            [[423, 237, 440, 251], [427, 272, 441, 287], [419, 115, 437, 129]],\n+            [[961, 885, 992, 912], [256, 38, 330, 58], [256, 38, 330, 58], [336, 42, 353, 57]],\n+        ]\n+\n+        return questions, words, boxes\n+\n     @classmethod\n     def setUpClass(cls):\n         super().setUpClass()\n@@ -1880,3 +1914,60 @@ def test_chat_template_return_assistant_tokens_mask(self):\n     @unittest.skip(\"Chat is not supported\")\n     def test_chat_template_return_assistant_tokens_mask_truncated(self):\n         pass\n+\n+    def test_empty_input_string(self):\n+        tokenizer_return_type = []\n+        output_tensor_type = []\n+\n+        if is_torch_available():\n+            import numpy as np\n+            import torch\n+\n+            tokenizer_return_type.append(\"pt\")\n+            output_tensor_type.append(torch.int64)\n+            tokenizer_return_type.append(\"np\")\n+            output_tensor_type.append(np.int64)\n+\n+        if is_tf_available():\n+            import tensorflow as tf\n+\n+            tokenizer_return_type.append(\"tf\")\n+            output_tensor_type.append(tf.int32)\n+\n+        if is_flax_available():\n+            import jax.numpy as jnp\n+\n+            tokenizer_return_type.append(\"jax\")\n+            output_tensor_type.append(jnp.int32)\n+\n+        if is_mlx_available():\n+            import mlx.core as mx\n+\n+            tokenizer_return_type.append(\"mlx\")\n+            output_tensor_type.append(mx.int32)\n+\n+        if len(tokenizer_return_type) == 0:\n+            self.skipTest(reason=\"No expected framework from PT, TF, JAX or MLX found\")\n+\n+        tokenizers = self.get_tokenizers()\n+        for tokenizer in tokenizers:\n+            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n+                words, boxes = self.get_empty_words_and_boxes()\n+                for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n+                    output = tokenizer(words, boxes=boxes, return_tensors=return_type)\n+                    self.assertEqual(output.input_ids.dtype, target_type)\n+\n+                question, words, boxes = self.get_empty_question_words_and_boxes()\n+                for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n+                    output = tokenizer(words, boxes=boxes, return_tensors=return_type)\n+                    self.assertEqual(output.input_ids.dtype, target_type)\n+\n+                words, boxes = self.get_empty_words_and_boxes_batch()\n+                for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n+                    output = tokenizer(words, boxes=boxes, padding=True, return_tensors=return_type)\n+                    self.assertEqual(output.input_ids.dtype, target_type)\n+\n+                question, words, boxes = self.get_empty_question_words_and_boxes_batch()\n+                for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n+                    output = tokenizer(words, boxes=boxes, padding=True, return_tensors=return_type)\n+                    self.assertEqual(output.input_ids.dtype, target_type)"
        },
        {
            "sha": "ee08dbf51b0151400afeb03032a16276e4883f5b",
            "filename": "tests/models/markuplm/test_tokenization_markuplm.py",
            "status": "modified",
            "additions": 101,
            "deletions": 0,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/de01a22aff21d16532d8dd68806589ca6c73dd5c/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de01a22aff21d16532d8dd68806589ca6c73dd5c/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py?ref=de01a22aff21d16532d8dd68806589ca6c73dd5c",
            "patch": "@@ -26,6 +26,8 @@\n     AddedToken,\n     MarkupLMTokenizerFast,\n     SpecialTokensMixin,\n+    is_flax_available,\n+    is_mlx_available,\n     is_tf_available,\n     is_torch_available,\n     logging,\n@@ -102,6 +104,48 @@ def get_question_nodes_and_xpaths_batch(self):\n \n         return questions, nodes, xpaths\n \n+    def get_empty_nodes_and_xpaths(self):\n+        nodes = [\"test\", \"empty\", \"\"]\n+        xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\"]\n+\n+        return nodes, xpaths\n+\n+    def get_empty_nodes_and_xpaths_batch(self):\n+        nodes = [[\"test\", \"empty\", \"\"], [\"one\", \"more\", \"empty\", \"\"]]\n+        xpaths = [\n+            [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\"],\n+            [\n+                \"/html/body/div/li[2]/div/span\",\n+                \"/html/body/div/li[2]/div/span\",\n+                \"/html/body/div/li[2]/div/span\",\n+                \"/html/body/div/li[2]/div/span\",\n+            ],\n+        ]\n+\n+        return nodes, xpaths\n+\n+    def get_empty_question_nodes_and_xpaths(self):\n+        question = \"\"\n+        nodes = [\"test\", \"empty\", \"\"]\n+        xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\"]\n+\n+        return question, nodes, xpaths\n+\n+    def get_empty_question_nodes_and_xpaths_batch(self):\n+        questions = [\"what's his name?\", \"\"]\n+        nodes = [[\"test\", \"empty\", \"\"], [\"one\", \"more\", \"empty\", \"\"]]\n+        xpaths = [\n+            [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\"],\n+            [\n+                \"/html/body/div/li[2]/div/span\",\n+                \"/html/body/div/li[2]/div/span\",\n+                \"/html/body/div/li[2]/div/span\",\n+                \"/html/body/div/li[2]/div/span\",\n+            ],\n+        ]\n+\n+        return questions, nodes, xpaths\n+\n     @unittest.skip(reason=\"Chat template tests don't play well with table/layout models.\")\n     def test_chat_template_batched(self):\n         pass\n@@ -2218,3 +2262,60 @@ def test_chat_template_return_assistant_tokens_mask(self):\n     @unittest.skip(\"Chat is not supported\")\n     def test_chat_template_return_assistant_tokens_mask_truncated(self):\n         pass\n+\n+    def test_empty_input_string(self):\n+        tokenizer_return_type = []\n+        output_tensor_type = []\n+\n+        if is_torch_available():\n+            import numpy as np\n+            import torch\n+\n+            tokenizer_return_type.append(\"pt\")\n+            output_tensor_type.append(torch.int64)\n+            tokenizer_return_type.append(\"np\")\n+            output_tensor_type.append(np.int64)\n+\n+        if is_tf_available():\n+            import tensorflow as tf\n+\n+            tokenizer_return_type.append(\"tf\")\n+            output_tensor_type.append(tf.int32)\n+\n+        if is_flax_available():\n+            import jax.numpy as jnp\n+\n+            tokenizer_return_type.append(\"jax\")\n+            output_tensor_type.append(jnp.int32)\n+\n+        if is_mlx_available():\n+            import mlx.core as mx\n+\n+            tokenizer_return_type.append(\"mlx\")\n+            output_tensor_type.append(mx.int32)\n+\n+        if len(tokenizer_return_type) == 0:\n+            self.skipTest(reason=\"No expected framework from PT, TF, JAX or MLX found\")\n+\n+        tokenizers = self.get_tokenizers()\n+        for tokenizer in tokenizers:\n+            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n+                nodes, xpaths = self.get_empty_nodes_and_xpaths()\n+                for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n+                    output = tokenizer(nodes, xpaths=xpaths, return_tensors=return_type)\n+                    self.assertEqual(output.input_ids.dtype, target_type)\n+\n+                question, nodes, xpaths = self.get_empty_question_nodes_and_xpaths()\n+                for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n+                    output = tokenizer(nodes, xpaths=xpaths, return_tensors=return_type)\n+                    self.assertEqual(output.input_ids.dtype, target_type)\n+\n+                nodes, xpaths = self.get_empty_nodes_and_xpaths_batch()\n+                for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n+                    output = tokenizer(nodes, xpaths=xpaths, padding=True, return_tensors=return_type)\n+                    self.assertEqual(output.input_ids.dtype, target_type)\n+\n+                question, nodes, xpaths = self.get_empty_question_nodes_and_xpaths_batch()\n+                for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n+                    output = tokenizer(nodes, xpaths=xpaths, padding=True, return_tensors=return_type)\n+                    self.assertEqual(output.input_ids.dtype, target_type)"
        },
        {
            "sha": "86c6f6c2124e12fc27e092ffdcd60338811dd4ec",
            "filename": "tests/models/tapas/test_tokenization_tapas.py",
            "status": "modified",
            "additions": 58,
            "deletions": 1,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/de01a22aff21d16532d8dd68806589ca6c73dd5c/tests%2Fmodels%2Ftapas%2Ftest_tokenization_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de01a22aff21d16532d8dd68806589ca6c73dd5c/tests%2Fmodels%2Ftapas%2Ftest_tokenization_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftapas%2Ftest_tokenization_tapas.py?ref=de01a22aff21d16532d8dd68806589ca6c73dd5c",
            "patch": "@@ -21,7 +21,7 @@\n import pandas as pd\n from parameterized import parameterized\n \n-from transformers import AddedToken\n+from transformers import AddedToken, is_flax_available, is_mlx_available, is_tf_available, is_torch_available\n from transformers.models.tapas.tokenization_tapas import (\n     VOCAB_FILES_NAMES,\n     BasicTokenizer,\n@@ -1165,3 +1165,60 @@ def test_chat_template_return_assistant_tokens_mask(self):\n     @unittest.skip(\"Chat is not supported\")\n     def test_chat_template_return_assistant_tokens_mask_truncated(self):\n         pass\n+\n+    def test_empty_input_string(self):\n+        sequences = [\n+            \"Testing batch encode plus\",\n+            \"Testing batch encode plus with different sequence lengths\",\n+            \"Testing batch encode plus with different sequence lengths correctly pads\",\n+        ]\n+        tokenizer_return_type = []\n+        output_tensor_type = []\n+\n+        if is_torch_available():\n+            import numpy as np\n+            import torch\n+\n+            tokenizer_return_type.append(\"pt\")\n+            output_tensor_type.append(torch.int64)\n+            tokenizer_return_type.append(\"np\")\n+            output_tensor_type.append(np.int64)\n+\n+        if is_tf_available():\n+            import tensorflow as tf\n+\n+            tokenizer_return_type.append(\"tf\")\n+            output_tensor_type.append(tf.int32)\n+\n+        if is_flax_available():\n+            import jax.numpy as jnp\n+\n+            tokenizer_return_type.append(\"jax\")\n+            output_tensor_type.append(jnp.int32)\n+\n+        if is_mlx_available():\n+            import mlx.core as mx\n+\n+            tokenizer_return_type.append(\"mlx\")\n+            output_tensor_type.append(mx.int32)\n+\n+        if len(tokenizer_return_type) == 0:\n+            self.skipTest(reason=\"No expected framework from PT, TF, JAX or MLX found\")\n+\n+        tokenizers = self.get_tokenizers()\n+        for tokenizer in tokenizers:\n+            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n+                table = self.get_table(tokenizer, length=0)\n+                for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n+                    output = tokenizer(table, sequences[0], return_tensors=return_type)\n+                    self.assertEqual(output.input_ids.dtype, target_type)\n+\n+                table = self.get_table(tokenizer, length=10)\n+                for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n+                    output = tokenizer(table, sequences[1], return_tensors=return_type)\n+                    self.assertEqual(output.input_ids.dtype, target_type)\n+\n+                table = self.get_table(tokenizer, length=0)\n+                for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n+                    output = tokenizer(table, sequences, padding=True, return_tensors=return_type)\n+                    self.assertEqual(output.input_ids.dtype, target_type)"
        },
        {
            "sha": "3a05d98bfc0a854f6ecb4ea69cfc3fdffbd28156",
            "filename": "tests/models/udop/test_tokenization_udop.py",
            "status": "modified",
            "additions": 91,
            "deletions": 0,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/de01a22aff21d16532d8dd68806589ca6c73dd5c/tests%2Fmodels%2Fudop%2Ftest_tokenization_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de01a22aff21d16532d8dd68806589ca6c73dd5c/tests%2Fmodels%2Fudop%2Ftest_tokenization_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_tokenization_udop.py?ref=de01a22aff21d16532d8dd68806589ca6c73dd5c",
            "patch": "@@ -22,6 +22,8 @@\n     SpecialTokensMixin,\n     UdopTokenizer,\n     UdopTokenizerFast,\n+    is_flax_available,\n+    is_mlx_available,\n     is_tf_available,\n     is_torch_available,\n     logging,\n@@ -91,6 +93,38 @@ def get_question_words_and_boxes_batch(self):\n \n         return questions, words, boxes\n \n+    def get_empty_words_and_boxes(self):\n+        words = [\"test\", \"empty\", \"\"]\n+        boxes = [[423, 237, 440, 251], [427, 272, 441, 287], [419, 115, 437, 129]]\n+\n+        return words, boxes\n+\n+    def get_empty_words_and_boxes_batch(self):\n+        words = [[\"test\", \"empty\", \"\"], [\"one\", \"more\", \"empty\", \"\"]]\n+        boxes = [\n+            [[423, 237, 440, 251], [427, 272, 441, 287], [419, 115, 437, 129]],\n+            [[961, 885, 992, 912], [256, 38, 330, 58], [256, 38, 330, 58], [336, 42, 353, 57]],\n+        ]\n+\n+        return words, boxes\n+\n+    def get_empty_question_words_and_boxes(self):\n+        question = \"\"\n+        words = [\"test\", \"empty\", \"\"]\n+        boxes = [[423, 237, 440, 251], [427, 272, 441, 287], [419, 115, 437, 129]]\n+\n+        return question, words, boxes\n+\n+    def get_empty_question_words_and_boxes_batch(self):\n+        questions = [\"what's his name?\", \"\"]\n+        words = [[\"test\", \"empty\", \"\"], [\"one\", \"more\", \"empty\", \"\"]]\n+        boxes = [\n+            [[423, 237, 440, 251], [427, 272, 441, 287], [419, 115, 437, 129]],\n+            [[961, 885, 992, 912], [256, 38, 330, 58], [256, 38, 330, 58], [336, 42, 353, 57]],\n+        ]\n+\n+        return questions, words, boxes\n+\n     @classmethod\n     def setUpClass(cls):\n         super().setUpClass()\n@@ -1849,3 +1883,60 @@ def test_split_special_tokens(self):\n \n                 output_tokens_reloaded_unsplit = fast_from_saved.tokenize(special_sentence, split_special_tokens=False)\n                 self.assertTrue(special_token in output_tokens_reloaded_unsplit)\n+\n+    def test_empty_input_string(self):\n+        tokenizer_return_type = []\n+        output_tensor_type = []\n+\n+        if is_torch_available():\n+            import numpy as np\n+            import torch\n+\n+            tokenizer_return_type.append(\"pt\")\n+            output_tensor_type.append(torch.int64)\n+            tokenizer_return_type.append(\"np\")\n+            output_tensor_type.append(np.int64)\n+\n+        if is_tf_available():\n+            import tensorflow as tf\n+\n+            tokenizer_return_type.append(\"tf\")\n+            output_tensor_type.append(tf.int32)\n+\n+        if is_flax_available():\n+            import jax.numpy as jnp\n+\n+            tokenizer_return_type.append(\"jax\")\n+            output_tensor_type.append(jnp.int32)\n+\n+        if is_mlx_available():\n+            import mlx.core as mx\n+\n+            tokenizer_return_type.append(\"mlx\")\n+            output_tensor_type.append(mx.int32)\n+\n+        if len(tokenizer_return_type) == 0:\n+            self.skipTest(reason=\"No expected framework from PT, TF, JAX or MLX found\")\n+\n+        tokenizers = self.get_tokenizers()\n+        for tokenizer in tokenizers:\n+            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n+                words, boxes = self.get_empty_words_and_boxes()\n+                for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n+                    output = tokenizer(words, boxes=boxes, return_tensors=return_type)\n+                    self.assertEqual(output.input_ids.dtype, target_type)\n+\n+                question, words, boxes = self.get_empty_question_words_and_boxes()\n+                for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n+                    output = tokenizer(words, boxes=boxes, return_tensors=return_type)\n+                    self.assertEqual(output.input_ids.dtype, target_type)\n+\n+                words, boxes = self.get_empty_words_and_boxes_batch()\n+                for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n+                    output = tokenizer(words, boxes=boxes, padding=True, return_tensors=return_type)\n+                    self.assertEqual(output.input_ids.dtype, target_type)\n+\n+                question, words, boxes = self.get_empty_question_words_and_boxes_batch()\n+                for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n+                    output = tokenizer(words, boxes=boxes, padding=True, return_tensors=return_type)\n+                    self.assertEqual(output.input_ids.dtype, target_type)"
        },
        {
            "sha": "b0e6bcf2ce296115f46c75c70f0da72b8e8b82f2",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 45,
            "deletions": 0,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/de01a22aff21d16532d8dd68806589ca6c73dd5c/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de01a22aff21d16532d8dd68806589ca6c73dd5c/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=de01a22aff21d16532d8dd68806589ca6c73dd5c",
            "patch": "@@ -42,6 +42,9 @@\n     SpecialTokensMixin,\n     Trainer,\n     TrainingArguments,\n+    is_flax_available,\n+    is_mlx_available,\n+    is_tf_available,\n     is_torch_available,\n     logging,\n )\n@@ -4685,3 +4688,45 @@ def test_rust_tokenizer_add_prefix_space(self, add_prefix_space):\n             # Only the ByteLevel pre-tokenizer has the `add_prefix_space` attribute, we have to ensure that it's set correctly\n             if hasattr(fast_tokenizer.backend_tokenizer.pre_tokenizer, \"add_prefix_space\"):\n                 self.assertEqual(fast_tokenizer.backend_tokenizer.pre_tokenizer.add_prefix_space, add_prefix_space)\n+\n+    def test_empty_input_string(self):\n+        empty_input_string = \"\"\n+        tokenizer_return_type = []\n+        output_tensor_type = []\n+\n+        if is_torch_available():\n+            import numpy as np\n+            import torch\n+\n+            tokenizer_return_type.append(\"pt\")\n+            output_tensor_type.append(torch.int64)\n+            tokenizer_return_type.append(\"np\")\n+            output_tensor_type.append(np.int64)\n+\n+        if is_tf_available():\n+            import tensorflow as tf\n+\n+            tokenizer_return_type.append(\"tf\")\n+            output_tensor_type.append(tf.int32)\n+\n+        if is_flax_available():\n+            import jax.numpy as jnp\n+\n+            tokenizer_return_type.append(\"jax\")\n+            output_tensor_type.append(jnp.int32)\n+\n+        if is_mlx_available():\n+            import mlx.core as mx\n+\n+            tokenizer_return_type.append(\"mlx\")\n+            output_tensor_type.append(mx.int32)\n+\n+        if len(tokenizer_return_type) == 0:\n+            self.skipTest(reason=\"No expected framework from PT, TF, JAX or MLX found\")\n+\n+        tokenizers = self.get_tokenizers()\n+        for tokenizer in tokenizers:\n+            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n+                for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n+                    output = tokenizer(empty_input_string, return_tensors=return_type)\n+                    self.assertEqual(output.input_ids.dtype, target_type)"
        }
    ],
    "stats": {
        "total": 618,
        "additions": 609,
        "deletions": 9
    }
}