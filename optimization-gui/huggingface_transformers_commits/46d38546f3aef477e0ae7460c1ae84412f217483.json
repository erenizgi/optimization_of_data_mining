{
    "author": "aayush226",
    "message": "Standardize RAG model card (#40222)\n\n* Standardize RAG model card\n\nUpdate rag.md to follow the new Hugging Face model card template:\n- Added friendly overview in plain language\n- Added pipeline and AutoModel usage examples\n- Included quantization example with BitsAndBytesConfig\n- Added notes and resources sections\n- Removed abstract and FlashAttention badge\n\n* Standardize RAG model card\n\nUpdate rag.md to follow the new Hugging Face model card template:\n- Added friendly overview in plain language\n- Added AutoModel usage example\n- Included quantization example with BitsAndBytesConfig",
    "sha": "46d38546f3aef477e0ae7460c1ae84412f217483",
    "files": [
        {
            "sha": "55de807ccd09e34cf99ff0a21cf2f5e5ba42754e",
            "filename": "docs/source/en/model_doc/rag.md",
            "status": "modified",
            "additions": 67,
            "deletions": 40,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/46d38546f3aef477e0ae7460c1ae84412f217483/docs%2Fsource%2Fen%2Fmodel_doc%2Frag.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/46d38546f3aef477e0ae7460c1ae84412f217483/docs%2Fsource%2Fen%2Fmodel_doc%2Frag.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frag.md?ref=46d38546f3aef477e0ae7460c1ae84412f217483",
            "patch": "@@ -17,48 +17,75 @@ rendered properly in your Markdown viewer.\n \n # RAG\n \n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n-<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<div style=\"float: right;\">\n+  <div class=\"flex flex-wrap space-x-1\">\n+    <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+    <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+  </div>\n </div>\n \n-## Overview\n-\n-Retrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and\n-sequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generate\n-outputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing\n-both retrieval and generation to adapt to downstream tasks.\n-\n-It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://huggingface.co/papers/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir\n-Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.\n-\n-The abstract from the paper is the following:\n-\n-*Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve\n-state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely\n-manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind\n-task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge\n-remain open research problems. Pre-trained models with a differentiable access mechanism to explicit nonparametric\n-memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a\n-general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained\n-parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a\n-pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a\n-pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages\n-across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our\n-models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks,\n-outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation\n-tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art\n-parametric-only seq2seq baseline.*\n-\n-This model was contributed by [ola13](https://huggingface.co/ola13).\n-\n-## Usage tips\n-\n-Retrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. \n-RAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq \n-modules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt \n-to downstream tasks.\n+[Retrieval-Augmented Generation (RAG)](https://huggingface.co/papers/2005.11401) combines a pretrained language model (parametric memory) with access to an external data source (non-parametric memory) by means of a pretrained neural retriever. RAG fetches relevant passages and conditions its generation on them during inference. This often makes the answers more factual and lets you update knowledge by changing the index instead of retraining the whole model.\n+\n+You can find all the original RAG checkpoints under the [AI at Meta](https://huggingface.co/facebook/models?search=rag) organization.\n+\n+> [!TIP]\n+> This model was contributed by [ola13](https://huggingface.co/ola13).\n+>\n+> Click on the RAG models in the right sidebar for more examples of how to apply RAG to different language tasks.\n+\n+The examples below demonstrates how to generate text with [`AutoModel`].\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration\n+\n+tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n+retriever = RagRetriever.from_pretrained(\n+    \"facebook/dpr-ctx_encoder-single-nq-base\", dataset=\"wiki_dpr\", index_name=\"compressed\"\n+)\n+\n+model = RagSequenceForGeneration.from_pretrained(\n+    \"facebook/rag-token-nq\",\n+    retriever=retriever,\n+    torch_dtype=\"auto\",\n+    attn_implementation=\"flash_attention_2\",\n+)\n+input_dict = tokenizer.prepare_seq2seq_batch(\"How many people live in Paris?\", return_tensors=\"pt\")\n+generated = model.generate(input_ids=input_dict[\"input_ids\"])\n+print(tokenizer.batch_decode(generated, skip_special_tokens=True)[0])\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+Quantization reduces memory by storing weights in lower precision. See the [Quantization](../quantization/overview) overview for supported backends.\n+The example below uses [bitsandbytes](../quantization/bitsandbytes) to quantize the weights to 4-bits.\n+\n+```py\n+import torch\n+from transformers import BitsAndBytesConfig, RagTokenizer, RagRetriever, RagSequenceForGeneration\n+\n+bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n+\n+tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n+retriever = RagRetriever.from_pretrained(\n+    \"facebook/dpr-ctx_encoder-single-nq-base\", dataset=\"wiki_dpr\", index_name=\"compressed\"\n+)\n+\n+model = RagSequenceForGeneration.from_pretrained(\n+    \"facebook/rag-token-nq\",\n+    retriever=retriever,\n+    quantization_config=bnb,   # quantizes generator weights\n+    device_map=\"auto\",\n+)\n+input_dict = tokenizer.prepare_seq2seq_batch(\"How many people live in Paris?\", return_tensors=\"pt\")\n+generated = model.generate(input_ids=input_dict[\"input_ids\"])\n+print(tokenizer.batch_decode(generated, skip_special_tokens=True)[0])\n+```\n \n ## RagConfig\n "
        }
    ],
    "stats": {
        "total": 107,
        "additions": 67,
        "deletions": 40
    }
}