{
    "author": "jackzhxng",
    "message": "Rewrite for loop in get_image_features with torch ops for export (#42822)\n\n* Rewrite ministral 3 for loop with torch ops for export\n\n* Fix Llava as well\n\n* Format\n\n* make fix-copies\n\n* Accidentally reversed ministral changes\n\n* Update modular mistral3\n\n* Update .md\n\n* Update .md",
    "sha": "a33ef4f94eb0df76147fa1eff69a39e0f875b8f7",
    "files": [
        {
            "sha": "16cc3cbcf3900489415a6eaf0eeca87566814397",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/a33ef4f94eb0df76147fa1eff69a39e0f875b8f7/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a33ef4f94eb0df76147fa1eff69a39e0f875b8f7/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=a33ef4f94eb0df76147fa1eff69a39e0f875b8f7",
            "patch": "@@ -202,10 +202,11 @@ def get_image_features(\n         image_features = self.multi_modal_projector(selected_image_feature)\n \n         if \"image_sizes\" in kwargs:\n-            split_sizes = [\n-                (height // self.vision_tower.patch_size) * (width // self.vision_tower.patch_size)\n-                for height, width in kwargs[\"image_sizes\"]\n-            ]\n+            split_sizes = (\n+                (torch.as_tensor(kwargs[\"image_sizes\"], device=image_features.device) // self.vision_tower.patch_size)\n+                .prod(dim=-1)\n+                .tolist()\n+            )\n             image_features = torch.split(image_features.squeeze(0), split_sizes)\n         else:\n             image_features = list(image_features)"
        },
        {
            "sha": "2bfc0e18a2c92f9b109c5102d87d4107795676fa",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a33ef4f94eb0df76147fa1eff69a39e0f875b8f7/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a33ef4f94eb0df76147fa1eff69a39e0f875b8f7/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=a33ef4f94eb0df76147fa1eff69a39e0f875b8f7",
            "patch": "@@ -252,7 +252,9 @@ def get_image_features(\n \n         image_features = self.multi_modal_projector(selected_image_feature.squeeze(0), image_sizes)\n         downsample_ratio = self.vision_tower.patch_size * self.config.spatial_merge_size\n-        split_sizes = [(height // downsample_ratio) * (width // downsample_ratio) for height, width in image_sizes]\n+        split_sizes = (\n+            (torch.as_tensor(image_sizes, device=image_features.device) // downsample_ratio).prod(dim=-1).tolist()\n+        )\n         image_features = torch.split(image_features.squeeze(0), split_sizes)\n         return image_features\n "
        },
        {
            "sha": "a59dc80c74c09a0d2a8250281453d6e3382da485",
            "filename": "src/transformers/models/mistral3/modular_mistral3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a33ef4f94eb0df76147fa1eff69a39e0f875b8f7/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a33ef4f94eb0df76147fa1eff69a39e0f875b8f7/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py?ref=a33ef4f94eb0df76147fa1eff69a39e0f875b8f7",
            "patch": "@@ -157,7 +157,9 @@ def get_image_features(\n \n         image_features = self.multi_modal_projector(selected_image_feature.squeeze(0), image_sizes)\n         downsample_ratio = self.vision_tower.patch_size * self.config.spatial_merge_size\n-        split_sizes = [(height // downsample_ratio) * (width // downsample_ratio) for height, width in image_sizes]\n+        split_sizes = (\n+            (torch.as_tensor(image_sizes, device=image_features.device) // downsample_ratio).prod(dim=-1).tolist()\n+        )\n         image_features = torch.split(image_features.squeeze(0), split_sizes)\n         return image_features\n "
        }
    ],
    "stats": {
        "total": 17,
        "additions": 11,
        "deletions": 6
    }
}