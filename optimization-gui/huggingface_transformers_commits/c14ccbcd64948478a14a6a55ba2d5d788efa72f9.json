{
    "author": "guangy10",
    "message": "Olmo is ExecuTorch Compatible (#34181)\n\nCo-authored-by: Guang Yang <guangyang@fb.com>",
    "sha": "c14ccbcd64948478a14a6a55ba2d5d788efa72f9",
    "files": [
        {
            "sha": "fbe73248d00b7cd71b8d7427361a6dc15bbec3f9",
            "filename": "tests/models/olmo/test_modeling_olmo.py",
            "status": "modified",
            "additions": 64,
            "deletions": 0,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/c14ccbcd64948478a14a6a55ba2d5d788efa72f9/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c14ccbcd64948478a14a6a55ba2d5d788efa72f9/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py?ref=c14ccbcd64948478a14a6a55ba2d5d788efa72f9",
            "patch": "@@ -16,9 +16,11 @@\n \n import unittest\n \n+from packaging import version\n from parameterized import parameterized\n \n from transformers import OlmoConfig, is_torch_available, set_seed\n+from transformers.generation.configuration_utils import GenerationConfig\n from transformers.models.auto.tokenization_auto import AutoTokenizer\n from transformers.models.gpt_neox.tokenization_gpt_neox_fast import GPTNeoXTokenizerFast\n from transformers.testing_utils import (\n@@ -449,3 +451,65 @@ def test_simple_encode_decode(self):\n         self.assertEqual(rust_tokenizer.encode(\"  \"), [50276])\n \n         self.assertEqual(rust_tokenizer.encode(\" Hello\"), [24387])\n+\n+    @slow\n+    def test_export_static_cache(self):\n+        if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n+\n+        from transformers.integrations.executorch import (\n+            TorchExportableModuleWithStaticCache,\n+            convert_and_export_with_cache,\n+        )\n+\n+        olmo_model = \"allenai/OLMo-1B-hf\"\n+\n+        tokenizer = AutoTokenizer.from_pretrained(olmo_model, pad_token=\"</s>\", padding_side=\"right\")\n+        EXPECTED_TEXT_COMPLETION = [\n+            \"Simply put, the theory of relativity states that \\nthe speed of light is the same in all reference frames.\\n\\nThe speed of light\",\n+        ]\n+        max_generation_length = tokenizer(EXPECTED_TEXT_COMPLETION, return_tensors=\"pt\", padding=True)[\n+            \"input_ids\"\n+        ].shape[-1]\n+\n+        # Load model\n+        device = \"cpu\"\n+        dtype = torch.bfloat16\n+        cache_implementation = \"static\"\n+        attn_implementation = \"sdpa\"\n+        batch_size = 1\n+        model = OlmoForCausalLM.from_pretrained(\n+            olmo_model,\n+            device_map=device,\n+            torch_dtype=dtype,\n+            attn_implementation=attn_implementation,\n+            generation_config=GenerationConfig(\n+                use_cache=True,\n+                cache_implementation=cache_implementation,\n+                max_length=max_generation_length,\n+                cache_config={\n+                    \"batch_size\": batch_size,\n+                    \"max_cache_len\": max_generation_length,\n+                },\n+            ),\n+        )\n+\n+        prompts = [\"Simply put, the theory of relativity states that \"]\n+        prompt_tokens = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n+        prompt_token_ids = prompt_tokens[\"input_ids\"]\n+        max_new_tokens = max_generation_length - prompt_token_ids.shape[-1]\n+\n+        # Static Cache + eager\n+        eager_generated_ids = model.generate(\n+            **prompt_tokens, max_new_tokens=max_new_tokens, do_sample=False, cache_implementation=cache_implementation\n+        )\n+        eager_generated_text = tokenizer.batch_decode(eager_generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, eager_generated_text)\n+\n+        # Static Cache + export\n+        exported_program = convert_and_export_with_cache(model)\n+        ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n+            exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n+        )\n+        ep_generated_text = tokenizer.batch_decode(ep_generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, ep_generated_text)"
        }
    ],
    "stats": {
        "total": 64,
        "additions": 64,
        "deletions": 0
    }
}