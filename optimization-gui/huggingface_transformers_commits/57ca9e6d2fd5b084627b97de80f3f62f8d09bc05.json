{
    "author": "unknown",
    "message": "Allow compressed-tensors quantized model to be trained (#34520)\n\n* populate quantization_config for kv-cache-scheme only configs\n\n* make compressed-tensors quantized models trainable\n\n* populate versions on quant config\n\n* pass oneshot then finetune\n\n* remove breakpoint\n\n* SunMarc comments and fix to_dict logic\n\n* lint\n\n* lint\n\n* test\n\n* comment\n\n* comments'",
    "sha": "57ca9e6d2fd5b084627b97de80f3f62f8d09bc05",
    "files": [
        {
            "sha": "d6303b230204e92859cb116e3d6d17251e6b2ecc",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/57ca9e6d2fd5b084627b97de80f3f62f8d09bc05/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57ca9e6d2fd5b084627b97de80f3f62f8d09bc05/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=57ca9e6d2fd5b084627b97de80f3f62f8d09bc05",
            "patch": "@@ -226,6 +226,11 @@ def _dequantize(self, model):\n             f\"{self.quantization_config.quant_method} has no implementation of `dequantize`, please raise an issue on GitHub.\"\n         )\n \n+    @property\n+    def is_qat_trainable(self) -> bool:\n+        \"\"\"Flag indicating whether the quantized model can carry out quantization aware training\"\"\"\n+        return False\n+\n     @abstractmethod\n     def _process_model_before_weight_loading(self, model, **kwargs): ...\n "
        },
        {
            "sha": "61e940886d942f7df4c1371b882c74925ccd3a78",
            "filename": "src/transformers/quantizers/quantizer_compressed_tensors.py",
            "status": "modified",
            "additions": 12,
            "deletions": 5,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/57ca9e6d2fd5b084627b97de80f3f62f8d09bc05/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57ca9e6d2fd5b084627b97de80f3f62f8d09bc05/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py?ref=57ca9e6d2fd5b084627b97de80f3f62f8d09bc05",
            "patch": "@@ -65,12 +65,19 @@ def _process_model_before_weight_loading(self, model, **kwargs):\n         ct_quantization_config = self.compressor.quantization_config\n         apply_quantization_config(model, ct_quantization_config, run_compressed=True)\n \n-    def _process_model_after_weight_loading(self, model, **kwargs):\n+    def _process_model_after_weight_loading(self, model, **kwargs) -> None:\n         pass\n \n     @property\n-    def is_trainable(self):\n-        return False\n+    def is_trainable(self) -> bool:\n+        \"\"\"Models quantized using compressed tensors can be finetuned\"\"\"\n+        return True\n \n-    def is_serializable(self, safe_serialization=None):\n-        return False\n+    @property\n+    def is_qat_trainable(self) -> bool:\n+        \"\"\"Loaded Models can carry out quantization aware training\"\"\"\n+        return True\n+\n+    def is_serializable(self, safe_serialization=None) -> bool:\n+        \"\"\"Models quantized using compressed tensors can be saved to disk\"\"\"\n+        return True"
        },
        {
            "sha": "e3a2292aed891d11dffb4adf7dbd59d0556f4536",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/57ca9e6d2fd5b084627b97de80f3f62f8d09bc05/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57ca9e6d2fd5b084627b97de80f3f62f8d09bc05/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=57ca9e6d2fd5b084627b97de80f3f62f8d09bc05",
            "patch": "@@ -540,14 +540,18 @@ def __init__(\n             getattr(model, \"hf_quantizer\", None) is not None and model.hf_quantizer.is_trainable\n         )\n \n+        _is_model_quantized_and_qat_trainable = getattr(model, \"hf_quantizer\", None) is not None and getattr(\n+            model.hf_quantizer, \"is_qat_trainable\", False\n+        )\n+\n         # Filter out quantized + compiled models\n         if _is_quantized_and_base_model and hasattr(model, \"_orig_mod\"):\n             raise ValueError(\n                 \"You cannot fine-tune quantized model with `torch.compile()` make sure to pass a non-compiled model when fine-tuning a quantized model with PEFT\"\n             )\n \n         # At this stage the model is already loaded\n-        if _is_quantized_and_base_model and not _is_peft_model(model):\n+        if _is_quantized_and_base_model and not _is_peft_model(model) and not _is_model_quantized_and_qat_trainable:\n             raise ValueError(\n                 \"You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of\"\n                 \" the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft\""
        },
        {
            "sha": "bacbca94cd823f8c59053de21e65f4d85a0a7df0",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 14,
            "deletions": 6,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/57ca9e6d2fd5b084627b97de80f3f62f8d09bc05/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57ca9e6d2fd5b084627b97de80f3f62f8d09bc05/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=57ca9e6d2fd5b084627b97de80f3f62f8d09bc05",
            "patch": "@@ -1150,6 +1150,7 @@ def from_dict(cls, config_dict, return_unused_kwargs=False, **kwargs):\n         Returns:\n             [`QuantizationConfigMixin`]: The configuration object instantiated from those parameters.\n         \"\"\"\n+\n         if \"quantization_config\" in config_dict:\n             config_dict = dict(\n                 sparsity_config=config_dict.get(\"sparsity_config\"),\n@@ -1160,16 +1161,23 @@ def from_dict(cls, config_dict, return_unused_kwargs=False, **kwargs):\n \n     def to_dict(self) -> Dict[str, Any]:\n         \"\"\"\n+        Quantization config to be added to config.json\n+\n         Serializes this instance to a Python dictionary. Returns:\n             `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n         \"\"\"\n-        quantization_config = self.quantization_config.dict() if self.quantization_config is not None else None\n-        sparsity_config = self.sparsity_config.dict() if self.sparsity_config is not None else None\n+        quantization_config = {}\n+        if self.quantization_config is not None:\n+            quantization_config = self.quantization_config.dict()\n+        else:\n+            quantization_config[\"quant_method\"] = QuantizationMethod.COMPRESSED_TENSORS\n \n-        return {\n-            \"quantization_config\": quantization_config,\n-            \"sparsity_config\": sparsity_config,\n-        }\n+        if self.sparsity_config is not None:\n+            quantization_config[\"sparsity_config\"] = self.sparsity_config.dict()\n+        else:\n+            quantization_config[\"sparsity_config\"] = {}\n+\n+        return quantization_config\n \n     def to_diff_dict(self) -> Dict[str, Any]:\n         \"\"\""
        }
    ],
    "stats": {
        "total": 48,
        "additions": 36,
        "deletions": 12
    }
}