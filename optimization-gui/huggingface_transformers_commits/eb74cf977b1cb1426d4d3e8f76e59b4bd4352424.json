{
    "author": "ydshieh",
    "message": "Use one `utils/notification_service.py` (#38379)\n\n* step 1\n\n* step 2\n\n* step 3\n\n* step 4\n\n* step 5\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "eb74cf977b1cb1426d4d3e8f76e59b4bd4352424",
    "files": [
        {
            "sha": "36c113190ca3a741d1671e3daa6bc9df1c92ef8f",
            "filename": ".github/workflows/self-scheduled.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb74cf977b1cb1426d4d3e8f76e59b4bd4352424/.github%2Fworkflows%2Fself-scheduled.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb74cf977b1cb1426d4d3e8f76e59b4bd4352424/.github%2Fworkflows%2Fself-scheduled.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled.yml?ref=eb74cf977b1cb1426d4d3e8f76e59b4bd4352424",
            "patch": "@@ -593,8 +593,7 @@ jobs:\n     secrets: inherit\n \n   check_new_failures:\n-    # TODO: work on `run_quantization_torch_gpu`\n-    if: ${{ always() && inputs.ci_event == 'Daily CI' && inputs.job != 'run_quantization_torch_gpu' && needs.send_results.result == 'success' }}\n+    if: ${{ always() && inputs.ci_event == 'Daily CI' && needs.send_results.result == 'success' }}\n     name: Check new failures\n     needs: send_results\n     uses: ./.github/workflows/check_failed_tests.yml"
        },
        {
            "sha": "5ef7494696459da6c9ef456280c97fb5226f4171",
            "filename": ".github/workflows/slack-report.yml",
            "status": "modified",
            "additions": 6,
            "deletions": 31,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb74cf977b1cb1426d4d3e8f76e59b4bd4352424/.github%2Fworkflows%2Fslack-report.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb74cf977b1cb1426d4d3e8f76e59b4bd4352424/.github%2Fworkflows%2Fslack-report.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fslack-report.yml?ref=eb74cf977b1cb1426d4d3e8f76e59b4bd4352424",
            "patch": "@@ -58,7 +58,7 @@ jobs:\n           fi\n \n       - name: Send message to Slack\n-        if: ${{ inputs.job != 'run_quantization_torch_gpu' }}\n+        shell: bash\n         env:\n           CI_SLACK_BOT_TOKEN: ${{ secrets.CI_SLACK_BOT_TOKEN }}\n           CI_SLACK_CHANNEL_ID: ${{ secrets.CI_SLACK_CHANNEL_ID }}\n@@ -79,39 +79,14 @@ jobs:\n           pip install huggingface_hub\n           pip install slack_sdk\n           pip show slack_sdk\n-          python utils/notification_service.py \"${{ inputs.folder_slices }}\"\n-\n-      # Upload complete failure tables, as they might be big and only truncated versions could be sent to Slack.\n-      - name: Failure table artifacts\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ci_results_${{ inputs.job }}\n-          path: ci_results_${{ inputs.job }}\n-\n-      - uses: actions/checkout@v4\n-      - uses: actions/download-artifact@v4\n-      - name: Send message to Slack for quantization workflow\n-        if: ${{ inputs.job == 'run_quantization_torch_gpu' }}\n-        env:\n-          CI_SLACK_BOT_TOKEN: ${{ secrets.CI_SLACK_BOT_TOKEN }}\n-          ACCESS_REPO_INFO_TOKEN: ${{ secrets.ACCESS_REPO_INFO_TOKEN }}\n-          SLACK_REPORT_CHANNEL: ${{ inputs.slack_report_channel }}\n-          CI_EVENT: ${{ inputs.ci_event }}\n-          CI_SHA: ${{ github.sha }}\n-          CI_TEST_JOB: ${{ inputs.job }}\n-          SETUP_STATUS: ${{ inputs.setup_status }}\n-          REPORT_REPO_ID: ${{ inputs.report_repo_id }}\n-        # We pass `needs.setup.outputs.quantization_matrix` as the argument. A processing in `notification_service_quantization.py` to change\n-        # `quantization/bnb` to `quantization_bnb` is required, as the artifact names use `_` instead of `/`.\n-        run: |\n-          pip install huggingface_hub\n-          pip install slack_sdk\n-          pip show slack_sdk\n-          python utils/notification_service_quantization.py \"${{ inputs.quantization_matrix }}\"\n+          if [ \"${{ inputs.quantization_matrix }}\" != \"\" ]; then\n+            python utils/notification_service.py \"${{ inputs.quantization_matrix }}\"\n+          else\n+            python utils/notification_service.py \"${{ inputs.folder_slices }}\"\n+          fi          \n \n       # Upload complete failure tables, as they might be big and only truncated versions could be sent to Slack.\n       - name: Failure table artifacts\n-        if: ${{ inputs.job == 'run_quantization_torch_gpu' }}\n         uses: actions/upload-artifact@v4\n         with:\n           name: ci_results_${{ inputs.job }}"
        },
        {
            "sha": "5c54809b262dc3ebde8aff32af8b06244584e67a",
            "filename": "utils/notification_service.py",
            "status": "modified",
            "additions": 140,
            "deletions": 80,
            "changes": 220,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb74cf977b1cb1426d4d3e8f76e59b4bd4352424/utils%2Fnotification_service.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb74cf977b1cb1426d4d3e8f76e59b4bd4352424/utils%2Fnotification_service.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnotification_service.py?ref=eb74cf977b1cb1426d4d3e8f76e59b4bd4352424",
            "patch": "@@ -31,15 +31,26 @@\n \n \n # A map associating the job names (specified by `inputs.job` in a workflow file) with the keys of\n-# `additional_files`. This is used to remove some entries in `additional_files` that are not concerned by a\n-# specific job. See below.\n+# `additional_files`.\n job_to_test_map = {\n     \"run_models_gpu\": \"Models\",\n     \"run_trainer_and_fsdp_gpu\": \"Trainer & FSDP\",\n     \"run_pipelines_torch_gpu\": \"PyTorch pipelines\",\n     \"run_pipelines_tf_gpu\": \"TensorFlow pipelines\",\n     \"run_examples_gpu\": \"Examples directory\",\n     \"run_torch_cuda_extensions_gpu\": \"DeepSpeed\",\n+    \"run_quantization_torch_gpu\": \"Quantization\",\n+}\n+\n+# The values are used as the file names where to save the corresponding CI job results.\n+test_to_result_name = {\n+    \"Models\": \"model\",\n+    \"Trainer & FSDP\": \"trainer_and_fsdp\",\n+    \"PyTorch pipelines\": \"torch_pipeline\",\n+    \"TensorFlow pipelines\": \"tf_pipeline\",\n+    \"Examples directory\": \"example\",\n+    \"DeepSpeed\": \"deepspeed\",\n+    \"Quantization\": \"quantization\",\n }\n \n NON_MODEL_TEST_MODULES = [\n@@ -53,6 +64,8 @@\n     \"sagemaker\",\n     \"trainer\",\n     \"utils\",\n+    \"fsdp\",\n+    \"quantization\",\n ]\n \n \n@@ -221,7 +234,6 @@ def failures(self) -> Dict:\n                 \"type\": \"plain_text\",\n                 \"text\": (\n                     f\"There were {self.n_failures} failures, out of {self.n_tests} tests.\\n\"\n-                    f\"Number of model failures: {self.n_model_failures}.\\n\"\n                     f\"The suite ran in {self.time}.\"\n                 ),\n                 \"emoji\": True,\n@@ -276,6 +288,10 @@ def get_device_report(report, rjust=6):\n \n     @property\n     def category_failures(self) -> Dict:\n+        if job_name != \"run_models_gpu\":\n+            category_failures_report = \"\"\n+            return {\"type\": \"section\", \"text\": {\"type\": \"mrkdwn\", \"text\": category_failures_report}}\n+\n         model_failures = [v[\"failed\"] for v in self.model_results.values()]\n \n         category_failures = {}\n@@ -301,7 +317,7 @@ def category_failures(self) -> Dict:\n \n         header = \"Single |  Multi | Category\\n\"\n         category_failures_report = prepare_reports(\n-            title=\"The following modeling categories had failures\", header=header, reports=individual_reports\n+            title=\"The following categories had failures\", header=header, reports=individual_reports\n         )\n \n         return {\"type\": \"section\", \"text\": {\"type\": \"mrkdwn\", \"text\": category_failures_report}}\n@@ -355,25 +371,40 @@ def per_model_sum(model_category_dict):\n         }\n \n         for k, v in self.model_results.items():\n+            # The keys in `model_results` may contain things like `models_vit` or `quantization_autoawq`\n+            # Remove the prefix to make the report cleaner.\n+            k = k.replace(\"models_\", \"\").replace(\"quantization_\", \"\")\n             if k in NON_MODEL_TEST_MODULES:\n-                pass\n+                continue\n \n             if sum(per_model_sum(v).values()):\n                 dict_failed = dict(v[\"failed\"])\n-                pytorch_specific_failures = dict_failed.pop(\"PyTorch\")\n-                tensorflow_specific_failures = dict_failed.pop(\"TensorFlow\")\n-                other_failures = dicts_to_sum(dict_failed.values())\n \n-                failures[k] = {\n-                    \"PyTorch\": pytorch_specific_failures,\n-                    \"TensorFlow\": tensorflow_specific_failures,\n-                    \"other\": other_failures,\n-                }\n+                # Model job has a special form for reporting\n+                if job_name == \"run_models_gpu\":\n+                    pytorch_specific_failures = dict_failed.pop(\"PyTorch\")\n+                    tensorflow_specific_failures = dict_failed.pop(\"TensorFlow\")\n+                    other_failures = dicts_to_sum(dict_failed.values())\n+\n+                    failures[k] = {\n+                        \"PyTorch\": pytorch_specific_failures,\n+                        \"TensorFlow\": tensorflow_specific_failures,\n+                        \"other\": other_failures,\n+                    }\n+\n+                else:\n+                    test_name = job_to_test_map[job_name]\n+                    specific_failures = dict_failed.pop(test_name)\n+                    failures[k] = {\n+                        test_name: specific_failures,\n+                    }\n \n         model_reports = []\n         other_module_reports = []\n \n         for key, value in non_model_failures.items():\n+            key = key.replace(\"models_\", \"\").replace(\"quantization_\", \"\")\n+\n             if key in NON_MODEL_TEST_MODULES:\n                 device_report = self.get_device_report(value)\n \n@@ -386,44 +417,60 @@ def per_model_sum(model_category_dict):\n                     other_module_reports.append(report)\n \n         for key, value in failures.items():\n-            device_report_values = [\n-                value[\"PyTorch\"][\"single\"],\n-                value[\"PyTorch\"][\"multi\"],\n-                value[\"TensorFlow\"][\"single\"],\n-                value[\"TensorFlow\"][\"multi\"],\n-                sum(value[\"other\"].values()),\n-            ]\n+            # Model job has a special form for reporting\n+            if job_name == \"run_models_gpu\":\n+                device_report_values = [\n+                    value[\"PyTorch\"][\"single\"],\n+                    value[\"PyTorch\"][\"multi\"],\n+                    value[\"TensorFlow\"][\"single\"],\n+                    value[\"TensorFlow\"][\"multi\"],\n+                    sum(value[\"other\"].values()),\n+                ]\n+\n+            else:\n+                test_name = job_to_test_map[job_name]\n+                device_report_values = [\n+                    value[test_name][\"single\"],\n+                    value[test_name][\"multi\"],\n+                ]\n \n             if sum(device_report_values):\n-                device_report = \" | \".join([str(x).rjust(9) for x in device_report_values]) + \" | \"\n+                # This is related to `model_header` below\n+                rjust_width = 9 if job_name == \"run_models_gpu\" else 6\n+                device_report = \" | \".join([str(x).rjust(rjust_width) for x in device_report_values]) + \" | \"\n                 report = f\"{device_report}{key}\"\n \n                 model_reports.append(report)\n \n         # (Possibly truncated) reports for the current workflow run - to be sent to Slack channels\n-        model_header = \"Single PT |  Multi PT | Single TF |  Multi TF |     Other | Category\\n\"\n+        if job_name == \"run_models_gpu\":\n+            model_header = \"Single PT |  Multi PT | Single TF |  Multi TF |     Other | Category\\n\"\n+        else:\n+            model_header = \"Single |  Multi | Category\\n\"\n+\n+        # Used when calling `prepare_reports` below to prepare the `title` argument\n+        label = test_to_result_name[job_to_test_map[job_name]]\n+\n         sorted_model_reports = sorted(model_reports, key=lambda s: s.split(\"| \")[-1])\n         model_failures_report = prepare_reports(\n-            title=\"These following model modules had failures\", header=model_header, reports=sorted_model_reports\n+            title=f\"These following {label} modules had failures\", header=model_header, reports=sorted_model_reports\n         )\n \n         module_header = \"Single |  Multi | Category\\n\"\n         sorted_module_reports = sorted(other_module_reports, key=lambda s: s.split(\"| \")[-1])\n         module_failures_report = prepare_reports(\n-            title=\"The following non-model modules had failures\", header=module_header, reports=sorted_module_reports\n+            title=f\"The following {label} modules had failures\", header=module_header, reports=sorted_module_reports\n         )\n \n         # To be sent to Slack channels\n-        model_failure_sections = [\n-            {\"type\": \"section\", \"text\": {\"type\": \"mrkdwn\", \"text\": model_failures_report}},\n-            {\"type\": \"section\", \"text\": {\"type\": \"mrkdwn\", \"text\": module_failures_report}},\n-        ]\n+        model_failure_sections = [{\"type\": \"section\", \"text\": {\"type\": \"mrkdwn\", \"text\": model_failures_report}}]\n+        model_failure_sections.append({\"type\": \"section\", \"text\": {\"type\": \"mrkdwn\", \"text\": module_failures_report}})\n \n         # Save the complete (i.e. no truncation) failure tables (of the current workflow run)\n         # (to be uploaded as artifacts)\n \n         model_failures_report = prepare_reports(\n-            title=\"These following model modules had failures\",\n+            title=f\"These following {label} modules had failures\",\n             header=model_header,\n             reports=sorted_model_reports,\n             to_truncate=False,\n@@ -433,7 +480,7 @@ def per_model_sum(model_category_dict):\n             fp.write(model_failures_report)\n \n         module_failures_report = prepare_reports(\n-            title=\"The following non-model modules had failures\",\n+            title=f\"The following {label} modules had failures\",\n             header=module_header,\n             reports=sorted_module_reports,\n             to_truncate=False,\n@@ -511,7 +558,10 @@ def payload(self) -> str:\n             blocks.append(self.failures)\n \n         if self.n_model_failures > 0:\n-            blocks.append(self.category_failures)\n+            block = self.category_failures\n+            if block[\"text\"][\"text\"]:\n+                blocks.append(block)\n+\n             for block in self.model_failures:\n                 if block[\"text\"][\"text\"]:\n                     blocks.append(block)\n@@ -565,7 +615,7 @@ def payload(self) -> str:\n                         pattern = r\"<(https://github.com/huggingface/transformers/actions/runs/.+?/job/.+?)\\|(.+?)>\"\n                         items = re.findall(pattern, line)\n                     elif \"tests/\" in line:\n-                        if \"tests/models/\" in line:\n+                        if \"tests/models/\" in line or \"tests/quantization/\" in line:\n                             model = line.split(\"/\")[2]\n                         else:\n                             model = line.split(\"/\")[1]\n@@ -609,7 +659,7 @@ def payload(self) -> str:\n                         \"text\": {\n                             \"type\": \"mrkdwn\",\n                             # TODO: We should NOT assume it's always Nvidia CI, but it's the case at this moment.\n-                            \"text\": f\"*There are {nb_new_failed_tests} failed tests unique to this run*\\n\\n(compared to Nvidia CI: <https://github.com/huggingface/transformers/actions/runs/{prev_workflow_run_id}|{prev_workflow_run_id}>)\",\n+                            \"text\": f\"*There are {nb_new_failed_tests} failed tests unique to {'this run' if not is_amd_daily_ci_workflow else 'AMD'}*\\n\\n(compared to Nvidia CI: <https://github.com/huggingface/transformers/actions/runs/{prev_workflow_run_id}|{prev_workflow_run_id}>)\",\n                         },\n                         \"accessory\": {\n                             \"type\": \"button\",\n@@ -1058,13 +1108,24 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n     # In our usage in `.github/workflows/slack-report.yml`, we always pass an argument when calling this script.\n     # The argument could be an empty string `\"\"` if a job doesn't depend on the job `setup`.\n     if arguments[0] == \"\":\n-        models = []\n+        job_matrix = []\n     else:\n-        model_list_as_str = arguments[0]\n+        job_matrix_as_str = arguments[0]\n         try:\n-            folder_slices = ast.literal_eval(model_list_as_str)\n-            # Need to change from elements like `models/bert` to `models_bert` (the ones used as artifact names).\n-            models = [x.replace(\"models/\", \"models_\") for folders in folder_slices for x in folders]\n+            folder_slices = ast.literal_eval(job_matrix_as_str)\n+            if len(folder_slices) > 0:\n+                if isinstance(folder_slices[0], list):\n+                    # Need to change from elements like `models/bert` to `models_bert` (the ones used as artifact names).\n+                    job_matrix = [\n+                        x.replace(\"models/\", \"models_\").replace(\"quantization/\", \"quantization_\")\n+                        for folders in folder_slices\n+                        for x in folders\n+                    ]\n+                elif isinstance(folder_slices[0], str):\n+                    job_matrix = [\n+                        x.replace(\"models/\", \"models_\").replace(\"quantization/\", \"quantization_\")\n+                        for x in folder_slices\n+                    ]\n         except Exception:\n             Message.error_out(title, ci_title)\n             raise ValueError(\"Errored out.\")\n@@ -1084,7 +1145,7 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n \n     available_artifacts = retrieve_available_artifacts()\n \n-    modeling_categories = [\n+    test_categories = [\n         \"PyTorch\",\n         \"TensorFlow\",\n         \"Flax\",\n@@ -1093,35 +1154,34 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n         \"Trainer\",\n         \"ONNX\",\n         \"Auto\",\n+        \"Quantization\",\n         \"Unclassified\",\n     ]\n \n     job_name = os.getenv(\"CI_TEST_JOB\")\n-    report_name_prefix = \"run_models_gpu\"\n-    if job_name == \"run_trainer_and_fsdp_gpu\":\n-        report_name_prefix = job_name\n+    report_name_prefix = job_name\n \n     # This dict will contain all the information relative to each model:\n     # - Failures: the total, as well as the number of failures per-category defined above\n     # - Success: total\n     # - Time spent: as a comma-separated list of elapsed time\n     # - Failures: as a line-break separated list of errors\n-    model_results = {\n-        model: {\n-            \"failed\": {m: {\"unclassified\": 0, \"single\": 0, \"multi\": 0} for m in modeling_categories},\n+    matrix_job_results = {\n+        matrix_name: {\n+            \"failed\": {m: {\"unclassified\": 0, \"single\": 0, \"multi\": 0} for m in test_categories},\n             \"success\": 0,\n             \"time_spent\": \"\",\n             \"failures\": {},\n             \"job_link\": {},\n         }\n-        for model in models\n-        if f\"{report_name_prefix}_{model}_test_reports\" in available_artifacts\n+        for matrix_name in job_matrix\n+        if f\"{report_name_prefix}_{matrix_name}_test_reports\" in available_artifacts\n     }\n \n     unclassified_model_failures = []\n \n-    for model in model_results.keys():\n-        for artifact_path_dict in available_artifacts[f\"{report_name_prefix}_{model}_test_reports\"].paths:\n+    for matrix_name in matrix_job_results.keys():\n+        for artifact_path_dict in available_artifacts[f\"{report_name_prefix}_{matrix_name}_test_reports\"].paths:\n             path = artifact_path_dict[\"path\"]\n             artifact_gpu = artifact_path_dict[\"gpu\"]\n \n@@ -1133,13 +1193,14 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n             if \"stats\" in artifact:\n                 # Link to the GitHub Action job\n                 job = artifact_name_to_job_map[path]\n-                model_results[model][\"job_link\"][artifact_gpu] = job[\"html_url\"]\n+                matrix_job_results[matrix_name][\"job_link\"][artifact_gpu] = job[\"html_url\"]\n                 failed, success, time_spent = handle_test_results(artifact[\"stats\"])\n-                model_results[model][\"success\"] += success\n-                model_results[model][\"time_spent\"] += time_spent[1:-1] + \", \"\n+                matrix_job_results[matrix_name][\"success\"] += success\n+                matrix_job_results[matrix_name][\"time_spent\"] += time_spent[1:-1] + \", \"\n \n                 stacktraces = handle_stacktraces(artifact[\"failures_line\"])\n \n+                # TODO: ???\n                 for line in artifact[\"summary_short\"].split(\"\\n\"):\n                     if line.startswith(\"FAILED \"):\n                         # Avoid the extra `FAILED` entry given by `run_test_using_subprocess` causing issue when calling\n@@ -1150,38 +1211,45 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n                         line = line[len(\"FAILED \") :]\n                         line = line.split()[0].replace(\"\\n\", \"\")\n \n-                        if artifact_gpu not in model_results[model][\"failures\"]:\n-                            model_results[model][\"failures\"][artifact_gpu] = []\n+                        if artifact_gpu not in matrix_job_results[matrix_name][\"failures\"]:\n+                            matrix_job_results[matrix_name][\"failures\"][artifact_gpu] = []\n \n                         trace = pop_default(stacktraces, 0, \"Cannot retrieve error message.\")\n-                        model_results[model][\"failures\"][artifact_gpu].append({\"line\": line, \"trace\": trace})\n+                        matrix_job_results[matrix_name][\"failures\"][artifact_gpu].append(\n+                            {\"line\": line, \"trace\": trace}\n+                        )\n+\n+                        # TODO: How to deal wit this\n+\n+                        if re.search(\"tests/quantization\", line):\n+                            matrix_job_results[matrix_name][\"failed\"][\"Quantization\"][artifact_gpu] += 1\n \n-                        if re.search(\"test_modeling_tf_\", line):\n-                            model_results[model][\"failed\"][\"TensorFlow\"][artifact_gpu] += 1\n+                        elif re.search(\"test_modeling_tf_\", line):\n+                            matrix_job_results[matrix_name][\"failed\"][\"TensorFlow\"][artifact_gpu] += 1\n \n                         elif re.search(\"test_modeling_flax_\", line):\n-                            model_results[model][\"failed\"][\"Flax\"][artifact_gpu] += 1\n+                            matrix_job_results[matrix_name][\"failed\"][\"Flax\"][artifact_gpu] += 1\n \n                         elif re.search(\"test_modeling\", line):\n-                            model_results[model][\"failed\"][\"PyTorch\"][artifact_gpu] += 1\n+                            matrix_job_results[matrix_name][\"failed\"][\"PyTorch\"][artifact_gpu] += 1\n \n                         elif re.search(\"test_tokenization\", line):\n-                            model_results[model][\"failed\"][\"Tokenizers\"][artifact_gpu] += 1\n+                            matrix_job_results[matrix_name][\"failed\"][\"Tokenizers\"][artifact_gpu] += 1\n \n                         elif re.search(\"test_pipelines\", line):\n-                            model_results[model][\"failed\"][\"Pipelines\"][artifact_gpu] += 1\n+                            matrix_job_results[matrix_name][\"failed\"][\"Pipelines\"][artifact_gpu] += 1\n \n                         elif re.search(\"test_trainer\", line):\n-                            model_results[model][\"failed\"][\"Trainer\"][artifact_gpu] += 1\n+                            matrix_job_results[matrix_name][\"failed\"][\"Trainer\"][artifact_gpu] += 1\n \n                         elif re.search(\"onnx\", line):\n-                            model_results[model][\"failed\"][\"ONNX\"][artifact_gpu] += 1\n+                            matrix_job_results[matrix_name][\"failed\"][\"ONNX\"][artifact_gpu] += 1\n \n                         elif re.search(\"auto\", line):\n-                            model_results[model][\"failed\"][\"Auto\"][artifact_gpu] += 1\n+                            matrix_job_results[matrix_name][\"failed\"][\"Auto\"][artifact_gpu] += 1\n \n                         else:\n-                            model_results[model][\"failed\"][\"Unclassified\"][artifact_gpu] += 1\n+                            matrix_job_results[matrix_name][\"failed\"][\"Unclassified\"][artifact_gpu] += 1\n                             unclassified_model_failures.append(line)\n \n     # Additional runs\n@@ -1315,20 +1383,10 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n             if \"workflow_run\" in event_payload:\n                 is_scheduled_ci_run = event_payload[\"workflow_run\"][\"event\"] == \"schedule\"\n \n-    # The values are used as the file names where to save the corresponding CI job results.\n-    test_to_result_name = {\n-        \"Models\": \"model\",\n-        \"Trainer & FSDP\": \"trainer_and_fsdp\",\n-        \"PyTorch pipelines\": \"torch_pipeline\",\n-        \"TensorFlow pipelines\": \"tf_pipeline\",\n-        \"Examples directory\": \"example\",\n-        \"DeepSpeed\": \"deepspeed\",\n-    }\n-\n     test_name_and_result_pairs = []\n-    if len(model_results) > 0:\n+    if len(matrix_job_results) > 0:\n         test_name = job_to_test_map[job_name]\n-        test_name_and_result_pairs.append((test_name, model_results))\n+        test_name_and_result_pairs.append((test_name, matrix_job_results))\n \n     for test_name, result in additional_results.items():\n         test_name_and_result_pairs.append((test_name, result))\n@@ -1346,8 +1404,8 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n         )\n \n     # Let's create a file contain job --> job link\n-    if len(model_results) > 0:\n-        target_results = model_results\n+    if len(matrix_job_results) > 0:\n+        target_results = matrix_job_results\n     else:\n         target_results = additional_results[job_to_test_map[job_name]]\n \n@@ -1360,6 +1418,8 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n     for job, job_result in sorted_dict:\n         if job.startswith(\"models_\"):\n             job = job[len(\"models_\") :]\n+        elif job.startswith(\"quantization_\"):\n+            job = job[len(\"quantization_\") :]\n         job_links[job] = job_result[\"job_link\"]\n \n     with open(f\"ci_results_{job_name}/job_links.json\", \"w\", encoding=\"UTF-8\") as fp:\n@@ -1424,7 +1484,7 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n     message = Message(\n         title,\n         ci_title,\n-        model_results,\n+        matrix_job_results,\n         additional_results,\n         selected_warnings=selected_warnings,\n         prev_ci_artifacts=prev_ci_artifacts,"
        },
        {
            "sha": "b533a7a9cf1eb5db24b4266281e51092f548838d",
            "filename": "utils/notification_service_quantization.py",
            "status": "removed",
            "additions": 0,
            "deletions": 294,
            "changes": 294,
            "blob_url": "https://github.com/huggingface/transformers/blob/98328fd9a10c28e668db7491db33e725ddb3e984/utils%2Fnotification_service_quantization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/98328fd9a10c28e668db7491db33e725ddb3e984/utils%2Fnotification_service_quantization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnotification_service_quantization.py?ref=98328fd9a10c28e668db7491db33e725ddb3e984",
            "patch": "@@ -1,294 +0,0 @@\n-# Copyright 2024 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import ast\n-import json\n-import os\n-import sys\n-import time\n-from typing import Dict\n-\n-from get_ci_error_statistics import get_jobs\n-from get_previous_daily_ci import get_last_daily_ci_run\n-from huggingface_hub import HfApi\n-from notification_service import (\n-    Message,\n-    handle_stacktraces,\n-    handle_test_results,\n-    prepare_reports,\n-    retrieve_artifact,\n-    retrieve_available_artifacts,\n-)\n-from slack_sdk import WebClient\n-\n-\n-api = HfApi()\n-client = WebClient(token=os.environ[\"CI_SLACK_BOT_TOKEN\"])\n-\n-\n-class QuantizationMessage(Message):\n-    def __init__(\n-        self,\n-        title: str,\n-        results: Dict,\n-    ):\n-        self.title = title\n-\n-        # Failures and success of the modeling tests\n-        self.n_success = sum(r[\"success\"] for r in results.values())\n-        self.single_gpu_failures = sum(r[\"failed\"][\"single\"] for r in results.values())\n-        self.multi_gpu_failures = sum(r[\"failed\"][\"multi\"] for r in results.values())\n-        self.n_failures = self.single_gpu_failures + self.multi_gpu_failures\n-\n-        self.n_tests = self.n_failures + self.n_success\n-        self.results = results\n-        self.thread_ts = None\n-\n-    @property\n-    def payload(self) -> str:\n-        blocks = [self.header]\n-\n-        if self.n_failures > 0:\n-            blocks.append(self.failures_overwiew)\n-            blocks.append(self.failures_detailed)\n-\n-        if self.n_failures == 0:\n-            blocks.append(self.no_failures)\n-\n-        return json.dumps(blocks)\n-\n-    @property\n-    def time(self) -> str:\n-        all_results = self.results.values()\n-        time_spent = []\n-        for r in all_results:\n-            if len(r[\"time_spent\"]):\n-                time_spent.extend([x for x in r[\"time_spent\"].split(\", \") if len(x.strip())])\n-        total_secs = 0\n-\n-        for time in time_spent:\n-            time_parts = time.split(\":\")\n-\n-            # Time can be formatted as xx:xx:xx, as .xx, or as x.xx if the time spent was less than a minute.\n-            if len(time_parts) == 1:\n-                time_parts = [0, 0, time_parts[0]]\n-\n-            hours, minutes, seconds = int(time_parts[0]), int(time_parts[1]), float(time_parts[2])\n-            total_secs += hours * 3600 + minutes * 60 + seconds\n-\n-        hours, minutes, seconds = total_secs // 3600, (total_secs % 3600) // 60, total_secs % 60\n-        return f\"{int(hours)}h{int(minutes)}m{int(seconds)}s\"\n-\n-    @property\n-    def failures_overwiew(self) -> Dict:\n-        return {\n-            \"type\": \"section\",\n-            \"text\": {\n-                \"type\": \"plain_text\",\n-                \"text\": (\n-                    f\"There were {self.n_failures} failures, out of {self.n_tests} tests.\\n\"\n-                    f\"The suite ran in {self.time}.\"\n-                ),\n-                \"emoji\": True,\n-            },\n-            \"accessory\": {\n-                \"type\": \"button\",\n-                \"text\": {\"type\": \"plain_text\", \"text\": \"Check Action results\", \"emoji\": True},\n-                \"url\": f\"https://github.com/huggingface/transformers/actions/runs/{os.environ['GITHUB_RUN_ID']}\",\n-            },\n-        }\n-\n-    @property\n-    def failures_detailed(self) -> Dict:\n-        failures = {k: v[\"failed\"] for k, v in self.results.items()}\n-\n-        individual_reports = []\n-        for key, value in failures.items():\n-            device_report = self.get_device_report(value)\n-            if sum(value.values()):\n-                report = f\"{device_report}{key}\"\n-                individual_reports.append(report)\n-\n-        header = \"Single |  Multi | Category\\n\"\n-        failures_report = prepare_reports(\n-            title=\"The following quantization tests had failures\", header=header, reports=individual_reports\n-        )\n-\n-        return {\"type\": \"section\", \"text\": {\"type\": \"mrkdwn\", \"text\": failures_report}}\n-\n-    def post(self):\n-        payload = self.payload\n-        print(\"Sending the following payload\")\n-        print(json.dumps({\"blocks\": json.loads(payload)}))\n-\n-        text = f\"{self.n_failures} failures out of {self.n_tests} tests,\" if self.n_failures else \"All tests passed.\"\n-\n-        self.thread_ts = client.chat_postMessage(\n-            channel=SLACK_REPORT_CHANNEL_ID,\n-            blocks=payload,\n-            text=text,\n-        )\n-\n-    def post_reply(self):\n-        if self.thread_ts is None:\n-            raise ValueError(\"Can only post reply if a post has been made.\")\n-\n-        for job, job_result in self.results.items():\n-            if len(job_result[\"failures\"]):\n-                for device, failures in job_result[\"failures\"].items():\n-                    blocks = self.get_reply_blocks(\n-                        job,\n-                        job_result,\n-                        failures,\n-                        device,\n-                        text=f\"Number of failures: {job_result['failed'][device]}\",\n-                    )\n-\n-                    print(\"Sending the following reply\")\n-                    print(json.dumps({\"blocks\": blocks}))\n-\n-                    client.chat_postMessage(\n-                        channel=\"#transformers-ci-daily-quantization\",\n-                        text=f\"Results for {job}\",\n-                        blocks=blocks,\n-                        thread_ts=self.thread_ts[\"ts\"],\n-                    )\n-                    time.sleep(1)\n-\n-\n-if __name__ == \"__main__\":\n-    setup_status = os.environ.get(\"SETUP_STATUS\")\n-    SLACK_REPORT_CHANNEL_ID = os.environ[\"SLACK_REPORT_CHANNEL\"]\n-    setup_failed = True if setup_status is not None and setup_status != \"success\" else False\n-\n-    # This env. variable is set in workflow file (under the job `send_results`).\n-    ci_event = os.environ[\"CI_EVENT\"]\n-\n-    title = f\"ðŸ¤— Results of the {ci_event} - {os.getenv('CI_TEST_JOB')}.\"\n-\n-    if setup_failed:\n-        Message.error_out(\n-            title, ci_title=\"\", runner_not_available=False, runner_failed=False, setup_failed=setup_failed\n-        )\n-        exit(0)\n-\n-    arguments = sys.argv[1:][0]\n-    try:\n-        quantization_matrix = ast.literal_eval(arguments)\n-        # Need to change from elements like `quantization/bnb` to `quantization_bnb` (the ones used as artifact names).\n-        quantization_matrix = [x.replace(\"quantization/\", \"quantization_\") for x in quantization_matrix]\n-    except SyntaxError:\n-        Message.error_out(title, ci_title=\"\")\n-        raise ValueError(\"Errored out.\")\n-\n-    available_artifacts = retrieve_available_artifacts()\n-\n-    quantization_results = {\n-        quant: {\n-            \"failed\": {\"single\": 0, \"multi\": 0},\n-            \"success\": 0,\n-            \"time_spent\": \"\",\n-            \"failures\": {},\n-            \"job_link\": {},\n-        }\n-        for quant in quantization_matrix\n-        if f\"run_quantization_torch_gpu_{quant}_test_reports\" in available_artifacts\n-    }\n-\n-    github_actions_jobs = get_jobs(\n-        workflow_run_id=os.environ[\"GITHUB_RUN_ID\"], token=os.environ[\"ACCESS_REPO_INFO_TOKEN\"]\n-    )\n-    github_actions_job_links = {job[\"name\"]: job[\"html_url\"] for job in github_actions_jobs}\n-\n-    artifact_name_to_job_map = {}\n-    for job in github_actions_jobs:\n-        for step in job[\"steps\"]:\n-            if step[\"name\"].startswith(\"Test suite reports artifacts: \"):\n-                artifact_name = step[\"name\"][len(\"Test suite reports artifacts: \") :]\n-                artifact_name_to_job_map[artifact_name] = job\n-                break\n-\n-    for quant in quantization_results.keys():\n-        for artifact_path in available_artifacts[f\"run_quantization_torch_gpu_{quant}_test_reports\"].paths:\n-            artifact = retrieve_artifact(artifact_path[\"path\"], artifact_path[\"gpu\"])\n-            if \"stats\" in artifact:\n-                # Link to the GitHub Action job\n-                job = artifact_name_to_job_map[artifact_path[\"path\"]]\n-                quantization_results[quant][\"job_link\"][artifact_path[\"gpu\"]] = job[\"html_url\"]\n-                failed, success, time_spent = handle_test_results(artifact[\"stats\"])\n-                quantization_results[quant][\"failed\"][artifact_path[\"gpu\"]] += failed\n-                quantization_results[quant][\"success\"] += success\n-                quantization_results[quant][\"time_spent\"] += time_spent[1:-1] + \", \"\n-\n-                stacktraces = handle_stacktraces(artifact[\"failures_line\"])\n-\n-                for line in artifact[\"summary_short\"].split(\"\\n\"):\n-                    if line.startswith(\"FAILED \"):\n-                        line = line[len(\"FAILED \") :]\n-                        line = line.split()[0].replace(\"\\n\", \"\")\n-\n-                        if artifact_path[\"gpu\"] not in quantization_results[quant][\"failures\"]:\n-                            quantization_results[quant][\"failures\"][artifact_path[\"gpu\"]] = []\n-\n-                        quantization_results[quant][\"failures\"][artifact_path[\"gpu\"]].append(\n-                            {\"line\": line, \"trace\": stacktraces.pop(0)}\n-                        )\n-\n-    job_name = os.getenv(\"CI_TEST_JOB\")\n-\n-    # if it is not a scheduled run, upload the reports to a subfolder under `report_repo_folder`\n-    report_repo_subfolder = \"\"\n-    if os.getenv(\"GITHUB_EVENT_NAME\") != \"schedule\":\n-        report_repo_subfolder = f\"{os.getenv('GITHUB_RUN_NUMBER')}-{os.getenv('GITHUB_RUN_ID')}\"\n-        report_repo_subfolder = f\"runs/{report_repo_subfolder}\"\n-\n-    workflow_run = get_last_daily_ci_run(\n-        token=os.environ[\"ACCESS_REPO_INFO_TOKEN\"], workflow_run_id=os.getenv(\"GITHUB_RUN_ID\")\n-    )\n-    workflow_run_created_time = workflow_run[\"created_at\"]\n-    workflow_id = workflow_run[\"workflow_id\"]\n-\n-    report_repo_folder = workflow_run_created_time.split(\"T\")[0]\n-\n-    if report_repo_subfolder:\n-        report_repo_folder = f\"{report_repo_folder}/{report_repo_subfolder}\"\n-\n-    if not os.path.isdir(os.path.join(os.getcwd(), f\"ci_results_{job_name}\")):\n-        os.makedirs(os.path.join(os.getcwd(), f\"ci_results_{job_name}\"))\n-\n-    nvidia_daily_ci_workflow = \"huggingface/transformers/.github/workflows/self-scheduled-caller.yml\"\n-    is_nvidia_daily_ci_workflow = os.environ.get(\"GITHUB_WORKFLOW_REF\").startswith(nvidia_daily_ci_workflow)\n-    is_scheduled_ci_run = os.environ.get(\"GITHUB_EVENT_NAME\") == \"schedule\"\n-\n-    with open(f\"ci_results_{job_name}/quantization_results.json\", \"w\", encoding=\"UTF-8\") as fp:\n-        json.dump(quantization_results, fp, indent=4, ensure_ascii=False)\n-\n-    report_repo_id = os.getenv(\"REPORT_REPO_ID\")\n-\n-    # upload results to Hub dataset (only for the scheduled daily CI run on `main`)\n-    api.upload_file(\n-        path_or_fileobj=f\"ci_results_{job_name}/quantization_results.json\",\n-        path_in_repo=f\"{report_repo_folder}/ci_results_{job_name}/quantization_results.json\",\n-        repo_id=report_repo_id,\n-        repo_type=\"dataset\",\n-        token=os.environ.get(\"TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN\", None),\n-    )\n-\n-    message = QuantizationMessage(\n-        title,\n-        results=quantization_results,\n-    )\n-\n-    message.post()\n-    message.post_reply()"
        }
    ],
    "stats": {
        "total": 554,
        "additions": 147,
        "deletions": 407
    }
}