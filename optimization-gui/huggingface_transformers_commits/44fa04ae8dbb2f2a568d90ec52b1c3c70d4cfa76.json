{
    "author": "jerryzh168",
    "message": "Include output embedding as well with `include_embedding` flag (#37935)\n\n* Include output embedding as well with `include_embedding` flag\n\nSummary:\natt\n\nTest Plan:\npython tests/quantization/torchao_integration/test_torchao.py -k test_include_embedding\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\n* format\n\n* rename include_embedding to include_input_output_embeddings\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "44fa04ae8dbb2f2a568d90ec52b1c3c70d4cfa76",
    "files": [
        {
            "sha": "030b0de0c0c8e15986f62a86e7b26489e31209ec",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/44fa04ae8dbb2f2a568d90ec52b1c3c70d4cfa76/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44fa04ae8dbb2f2a568d90ec52b1c3c70d4cfa76/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=44fa04ae8dbb2f2a568d90ec52b1c3c70d4cfa76",
            "patch": "@@ -185,10 +185,14 @@ def _process_model_before_weight_loading(\n         self.modules_to_not_convert = self.get_modules_to_not_convert(\n             model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n         )\n-        if self.quantization_config.include_embedding:\n+        if self.quantization_config.include_input_output_embeddings:\n             input_emb = model.get_input_embeddings()\n             input_emb_names = [name for name, module in model.named_modules() if id(module) == id(input_emb)]\n-            self.modules_to_not_convert = [x for x in self.modules_to_not_convert if x not in input_emb_names]\n+            output_emb = model.get_output_embeddings()\n+            output_emb_names = [name for name, module in model.named_modules() if id(module) == id(output_emb)]\n+            self.modules_to_not_convert = [\n+                x for x in self.modules_to_not_convert if x not in input_emb_names + output_emb_names\n+            ]\n         return\n \n     def check_quantized_param(\n@@ -213,7 +217,7 @@ def check_quantized_param(\n             # we only quantize the weight of nn.Linear and nn.Embedding\n             module, tensor_name = get_module_from_name(model, param_name)\n             _QUANTIZABLE = [torch.nn.Linear]\n-            if self.quantization_config.include_embedding:\n+            if self.quantization_config.include_input_output_embeddings:\n                 _QUANTIZABLE.append(torch.nn.Embedding)\n             return isinstance(module, tuple(_QUANTIZABLE)) and (tensor_name == \"weight\")\n "
        },
        {
            "sha": "ec2a6c76deee7cfc34b5975d32f74c2bdb4eeda8",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/44fa04ae8dbb2f2a568d90ec52b1c3c70d4cfa76/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44fa04ae8dbb2f2a568d90ec52b1c3c70d4cfa76/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=44fa04ae8dbb2f2a568d90ec52b1c3c70d4cfa76",
            "patch": "@@ -1554,7 +1554,7 @@ class TorchAoConfig(QuantizationConfigMixin):\n     quant_type: Union[str, \"AOBaseConfig\"]  # noqa: F821\n     modules_to_not_convert: Optional[List]\n     quant_type_kwargs: Dict[str, Any]\n-    include_embedding: bool\n+    include_input_output_embeddings: bool\n     untie_embedding_weights: bool\n \n     \"\"\"This is a config class for torchao quantization/sparsity techniques.\n@@ -1617,15 +1617,15 @@ def __init__(\n         self,\n         quant_type: Union[str, \"AOBaseConfig\"],  # noqa: F821\n         modules_to_not_convert: Optional[List] = None,\n-        include_embedding: bool = False,\n+        include_input_output_embeddings: bool = False,\n         untie_embedding_weights: bool = False,\n         **kwargs,\n     ):\n         self.quant_method = QuantizationMethod.TORCHAO\n         self.quant_type = quant_type\n         self.modules_to_not_convert = modules_to_not_convert\n         self.quant_type_kwargs = kwargs.get(\"quant_type_kwargs\", kwargs)\n-        self.include_embedding = include_embedding\n+        self.include_input_output_embeddings = include_input_output_embeddings\n         self.untie_embedding_weights = untie_embedding_weights\n         self.post_init()\n "
        },
        {
            "sha": "8f1c15c94d6de7fc90e5ce4f67fcb4b212e15af5",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/44fa04ae8dbb2f2a568d90ec52b1c3c70d4cfa76/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44fa04ae8dbb2f2a568d90ec52b1c3c70d4cfa76/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=44fa04ae8dbb2f2a568d90ec52b1c3c70d4cfa76",
            "patch": "@@ -201,7 +201,7 @@ def test_int8_dynamic_activation_int8_weight_quant(self):\n         self.assertTrue(tokenizer.decode(output[0], skip_special_tokens=True) in EXPECTED_OUTPUT)\n \n     @require_torchao_version_greater_or_equal(\"0.11.0\")\n-    def test_include_embedding(self):\n+    def test_include_input_output_embeddings(self):\n         weight_dtype = torch.int8\n         granularity = PerAxis(0)\n         mapping_type = MappingType.ASYMMETRIC\n@@ -210,16 +210,19 @@ def test_include_embedding(self):\n             granularity=granularity,\n             mapping_type=mapping_type,\n         )\n-        config = AOPerModuleConfig({\"_default\": None, \"model.embed_tokens\": embedding_config})\n-        # need set `include_embedding` to True\n-        quant_config = TorchAoConfig(quant_type=config, include_embedding=True)\n+        config = AOPerModuleConfig(\n+            {\"_default\": None, \"model.embed_tokens\": embedding_config, \"lm_head\": embedding_config}\n+        )\n+        # need set `include_input_output_embeddings` to True\n+        quant_config = TorchAoConfig(quant_type=config, include_input_output_embeddings=True)\n         quantized_model = AutoModelForCausalLM.from_pretrained(\n             self.model_name,\n             device_map=self.device,\n             quantization_config=quant_config,\n         )\n         # making sure embedding is quantized\n         self.assertTrue(isinstance(quantized_model.model.embed_tokens.weight, AffineQuantizedTensor))\n+        self.assertTrue(isinstance(quantized_model.lm_head.weight, AffineQuantizedTensor))\n         tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n \n         input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(self.device)"
        }
    ],
    "stats": {
        "total": 27,
        "additions": 17,
        "deletions": 10
    }
}