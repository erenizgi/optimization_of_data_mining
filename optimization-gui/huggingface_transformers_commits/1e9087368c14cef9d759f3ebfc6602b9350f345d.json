{
    "author": "zucchini-nlp",
    "message": "[internvl] fix chat template (#37656)\n\n* fix chat template\n\n* update\n\n* update conversion\n\n* rename `fake_image_token` in tests",
    "sha": "1e9087368c14cef9d759f3ebfc6602b9350f345d",
    "files": [
        {
            "sha": "4ac56c853777cc0669ab91f654b6e8a7765dc02e",
            "filename": "docs/source/en/model_doc/internvl.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e9087368c14cef9d759f3ebfc6602b9350f345d/docs%2Fsource%2Fen%2Fmodel_doc%2Finternvl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e9087368c14cef9d759f3ebfc6602b9350f345d/docs%2Fsource%2Fen%2Fmodel_doc%2Finternvl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finternvl.md?ref=1e9087368c14cef9d759f3ebfc6602b9350f345d",
            "patch": "@@ -257,6 +257,7 @@ InternVL models can also handle video inputs. Here is an example of how to perfo\n ...     add_generation_prompt=True,\n ...     tokenize=True,\n ...     return_dict=True,\n+...     num_frames=8,\n >>> ).to(model.device, dtype=torch.float16)\n \n >>> output = model.generate(**inputs, max_new_tokens=25)"
        },
        {
            "sha": "539085c4714226cc864b78ca4b6e47c662715efe",
            "filename": "src/transformers/models/internvl/convert_internvl_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e9087368c14cef9d759f3ebfc6602b9350f345d/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e9087368c14cef9d759f3ebfc6602b9350f345d/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py?ref=1e9087368c14cef9d759f3ebfc6602b9350f345d",
            "patch": "@@ -312,6 +312,7 @@ def write_tokenizer(save_dir: str, push_to_hub: bool = False, path: str = None,\n                 \"start_image_token\": \"<img>\",\n                 \"end_image_token\": \"</img>\",\n                 \"context_image_token\": \"<IMG_CONTEXT>\",\n+                \"video_token\": \"<video>\",\n             },\n         )\n         tokenizer.model_max_length = CONTEXT_LENGTH"
        },
        {
            "sha": "2bb6ed2d97d65b578e4707672af26d059d39a3b7",
            "filename": "src/transformers/models/internvl/processing_internvl.py",
            "status": "modified",
            "additions": 54,
            "deletions": 80,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e9087368c14cef9d759f3ebfc6602b9350f345d/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e9087368c14cef9d759f3ebfc6602b9350f345d/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py?ref=1e9087368c14cef9d759f3ebfc6602b9350f345d",
            "patch": "@@ -14,13 +14,11 @@\n # limitations under the License.\n \n \n-from functools import partial\n-from typing import Dict, List, Optional, Union\n+from typing import List, Optional, Union\n \n import numpy as np\n \n from transformers.processing_utils import (\n-    AllKwargsForChatTemplate,\n     ImagesKwargs,\n     ProcessingKwargs,\n     ProcessorMixin,\n@@ -34,6 +32,7 @@\n     VideoInput,\n     VideoMetadata,\n     concatenate_list,\n+    load_video,\n     make_batched_videos,\n     make_flat_list_of_images,\n )\n@@ -75,20 +74,12 @@ class InternVLProcessor(ProcessorMixin):\n             image_seq_length = (config.image_size // config.patch_size) ** 2 * (config.scale_factor**2)\n         chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n             in a chat into a tokenizable string.\n-        fake_image_token (`str`, *optional*, defaults to `\"<image>\"`):\n-            The token to use for the image placeholder in the text. This token will be replaced by the\n-            appropriate image tokens when processing the text with images.\n-        fake_video_token (`str`, *optional*, defaults to `\"<video>\"`):\n-            The token to use for the video placeholder in the text. This token will be replaced by the\n-            appropriate image tokens when processing the text with videos.\n     \"\"\"\n \n     attributes = [\"image_processor\", \"tokenizer\"]\n     valid_kwargs = [\n         \"chat_template\",\n         \"image_seq_length\",\n-        \"fake_image_token\",\n-        \"fake_video_token\",\n     ]\n     image_processor_class = \"AutoImageProcessor\"\n     tokenizer_class = \"AutoTokenizer\"\n@@ -99,16 +90,14 @@ def __init__(\n         tokenizer=None,\n         image_seq_length: int = 256,\n         chat_template=None,\n-        fake_image_token=\"<image>\",\n-        fake_video_token=\"<video>\",\n         **kwargs,\n     ):\n         self.image_seq_length = image_seq_length\n-        self.fake_image_token = fake_image_token\n-        self.fake_video_token = fake_video_token\n         self.start_image_token = tokenizer.start_image_token\n         self.end_image_token = tokenizer.end_image_token\n-        self.context_image_token = tokenizer.context_image_token\n+        self.image_token = tokenizer.context_image_token\n+        self.video_token = tokenizer.video_token\n+        self.image_token_id = tokenizer.context_image_token_id\n \n         super().__init__(image_processor, tokenizer, chat_template=chat_template, **kwargs)\n \n@@ -131,24 +120,24 @@ def _insert_media_placeholders(\n         video_index = 0\n         processed_text = []\n         image_video_patches = []\n+        replace_strings = []\n         # Support interleaved image and video in prompts:\n         # Processed patches of images and videos are inserted in `image_video_patches` in the order they appear in the prompts\n         for prompt in text:\n             new_prompt = prompt\n-            while self.fake_image_token in new_prompt or self.fake_video_token in new_prompt:\n-                if self.fake_image_token in new_prompt and (\n-                    self.fake_video_token not in new_prompt\n-                    or new_prompt.index(self.fake_image_token) < new_prompt.index(self.fake_video_token)\n+            while self.image_token in new_prompt or self.video_token in new_prompt:\n+                if self.image_token in new_prompt and (\n+                    self.video_token not in new_prompt\n+                    or new_prompt.index(self.image_token) < new_prompt.index(self.video_token)\n                 ):\n                     # Get the slice of patches corresponding to the current image\n                     start_index = image_num_patches_indices[image_index - 1] if image_index > 0 else 0\n                     end_index = image_num_patches_indices[image_index]\n                     image_video_patches.append(image_pixel_values[start_index:end_index])\n                     # Replace the corresponding image placeholder with the correct number of image tokens\n-                    new_prompt = new_prompt.replace(\n-                        self.fake_image_token,\n-                        f\"{self.start_image_token}{self.context_image_token * self.image_seq_length * image_num_patches[image_index]}{self.end_image_token}\",\n-                        1,\n+                    new_prompt = new_prompt.replace(self.image_token, \"<placeholder>\", 1)\n+                    replace_strings.append(\n+                        f\"{self.start_image_token}{self.image_token * self.image_seq_length * image_num_patches[image_index]}{self.end_image_token}\"\n                     )\n                     image_index += 1\n                 else:\n@@ -163,11 +152,15 @@ def _insert_media_placeholders(\n                     # Get the number of patches per frame and replace the video placeholder with the correct number of image tokens\n                     num_patches = list(video_num_patches[current_patch_index:end_patch_index])\n                     video_prompt = \"\\n\".join(\n-                        f\"Frame{i + 1}: {self.start_image_token}{self.context_image_token * self.image_seq_length * num_patches[i]}{self.end_image_token}\"\n+                        f\"Frame{i + 1}: {self.start_image_token}{self.image_token * self.image_seq_length * num_patches[i]}{self.end_image_token}\"\n                         for i in range(len(num_patches))\n                     )\n-                    new_prompt = new_prompt.replace(self.fake_video_token, video_prompt, 1)\n+                    replace_strings.append(video_prompt)\n+                    new_prompt = new_prompt.replace(self.video_token, \"<placeholder>\", 1)\n                     video_index += 1\n+            while \"<placeholder>\" in new_prompt:\n+                replace_str = replace_strings.pop(0)\n+                new_prompt = new_prompt.replace(\"<placeholder>\", replace_str, 1)\n             processed_text.append(new_prompt)\n \n         return processed_text, image_video_patches, image_index, video_index\n@@ -269,9 +262,11 @@ def __call__(\n             # Concatenate the interleaved image and video patches (function agnostic to the patches type (list, numpy array, torch tensor))\n             image_videos_inputs = {\"pixel_values\": concatenate_list(image_video_patches)}\n \n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n         text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+        self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\"])\n \n-        return BatchFeature(data={**text_inputs, **image_videos_inputs})\n+        return BatchFeature(data={**text_inputs, **image_videos_inputs}, tensor_type=return_tensors)\n \n     def sample_indices_fn(\n         self, metadata: VideoMetadata, num_frames: int = None, initial_shift: Union[bool, float, int] = True\n@@ -290,15 +285,13 @@ def sample_indices_fn(\n         Returns:\n             `np.ndarray`: Array of frame indices to sample.\n         \"\"\"\n+        num_frames = num_frames if num_frames is not None else metadata.total_num_frames\n+\n         if initial_shift is True:\n             initial_shift = metadata.total_num_frames / num_frames / 2\n-        if num_frames is not None:\n-            indices = np.arange(\n-                initial_shift, metadata.total_num_frames, metadata.total_num_frames / num_frames\n-            ).astype(int)\n-        else:\n-            indices = np.arange(initial_shift, metadata.total_num_frames).astype(int)\n-\n+        indices = np.arange(initial_shift, metadata.total_num_frames, metadata.total_num_frames / num_frames).astype(\n+            int\n+        )\n         return indices\n \n     def batch_decode(self, *args, **kwargs):\n@@ -321,58 +314,39 @@ def model_input_names(self):\n         image_processor_input_names = self.image_processor.model_input_names\n         return list(tokenizer_input_names) + list(image_processor_input_names)\n \n-    # Add model-specific video sampling method when applying the template\n-    def apply_chat_template(\n+    # TODO: raushan, has to be public method under `VideoProcessorBase` when API is added\n+    def _load_video_for_model(\n         self,\n-        conversation: Union[List[Dict[str, str]], List[List[Dict[str, str]]]],\n-        chat_template: Optional[str] = None,\n-        num_frames: int = 8,\n-        initial_shift: Union[bool, float, int] = True,\n-        video_load_backend=\"pyav\",\n-        **kwargs: Unpack[AllKwargsForChatTemplate],\n-    ):\n+        video: Union[str, \"VideoInput\"],\n+        num_frames: Optional[int],\n+        backend: str = \"pyav\",\n+        initial_shift: bool = True,\n+        **kwargs,\n+    ) -> np.array:\n         \"\"\"\n-        Similar to the `apply_chat_template` method on tokenizers, this method applies a Jinja template to input\n-        conversations to turn them into a single tokenizable string.\n-\n-        The input is expected to be in the following format, where each message content is a list consisting of text and\n-        optionally image or video inputs. One can also provide an image, video, URL or local path which will be used to form\n-        `pixel_values` when `return_dict=True`. If not provided, one will get only the formatted text, optionally tokenized text.\n-\n-        conversation = [\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\"type\": \"image\", \"image\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n-                    {\"type\": \"text\", \"text\": \"Please describe this image in detail.\"},\n-                ],\n-            },\n-        ]\n+        Loads `video` to a numpy array.\n \n         Args:\n-            conversation (`Union[List[Dict, [str, str]], List[List[Dict[str, str]]]]`):\n-                The conversation to format.\n-            chat_template (`Optional[str]`, *optional*):\n-                The Jinja template to use for formatting the conversation. If not provided, the tokenizer's\n-                chat template is used.\n-            num_frames (`int`, *optional*, defaults to 8):\n-                Number of frames to sample from a video when using the default `sample_indices_fn`.\n-            initial_shift (`bool`, `float` or `int`, defaults to `0`):\n-                The initial shift to apply when sampling frames using the default `sample_indices_fn`.\n-                If `True`, the shift is set so that frames are sampled from the middle of the video.\n+            video (`str` or `VideoInput`):\n+                The video to convert to the numpy array format. Can be a link to video or local path.\n+            num_frames (`int`, *optional*):\n+                Number of frames to sample uniformly. If not passed, the whole video is loaded.\n+            backend (`str`, *optional*, defaults to `\"pyav\"`):\n+                The backend to use when loading the video. Can be any of [\"decord\", \"pyav\", \"opencv\", \"torchvision\"]. Defaults to \"pyav\".\n+            initial_shift (`bool`, *optional*, defaults to `True`):\n+                The initial shift to apply when sampling frames. If `True`, the shift is set so that frames are sampled from the middle of the video.\n+\n+        Returns:\n+            Tuple[`np.array`, Dict]: A tuple containing:\n+                - Numpy array of frames in RGB (shape: [num_frames, height, width, 3]).\n+                - Metadata dictionary.\n         \"\"\"\n-        sample_indices_fn = kwargs.pop(\n-            \"sample_indices_fn\", partial(self.sample_indices_fn, num_frames=num_frames, initial_shift=initial_shift)\n-        )\n \n-        return super().apply_chat_template(\n-            conversation,\n-            chat_template,\n-            video_load_backend=video_load_backend,\n-            num_frames=num_frames,\n-            sample_indices_fn=sample_indices_fn,\n-            **kwargs,\n-        )\n+        def sample_indices_fn_func(metadata, **fn_kwargs):\n+            return self.sample_indices_fn(metadata, num_frames=num_frames, initial_shift=initial_shift, **fn_kwargs)\n+\n+        video, metadata = load_video(video, backend=backend, sample_indices_fn=sample_indices_fn_func)\n+        return video, metadata\n \n \n __all__ = [\"InternVLProcessor\"]"
        },
        {
            "sha": "b62690ce705cc4d8480b444611bb72cfb1e32501",
            "filename": "tests/models/internvl/test_modeling_internvl.py",
            "status": "modified",
            "additions": 24,
            "deletions": 12,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e9087368c14cef9d759f3ebfc6602b9350f345d/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e9087368c14cef9d759f3ebfc6602b9350f345d/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py?ref=1e9087368c14cef9d759f3ebfc6602b9350f345d",
            "patch": "@@ -296,7 +296,9 @@ def test_qwen2_small_model_integration_generate(self):\n         url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n         image = Image.open(requests.get(url, stream=True).raw)\n \n-        prompt = \"<|im_start|>user\\n<image>\\nPlease describe the image explicitly.<|im_end|>\\n<|im_start|>assistant\\n\"\n+        prompt = (\n+            \"<|im_start|>user\\n<IMG_CONTEXT>\\nPlease describe the image explicitly.<|im_end|>\\n<|im_start|>assistant\\n\"\n+        )\n         inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.bfloat16)\n         with torch.no_grad():\n             generate_ids = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n@@ -314,7 +316,9 @@ def test_qwen2_small_model_integration_forward(self):\n         url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n         image = Image.open(requests.get(url, stream=True).raw)\n \n-        prompt = \"<|im_start|>user\\n<image>\\nPlease describe the image explicitly.<|im_end|>\\n<|im_start|>assistant\\n\"\n+        prompt = (\n+            \"<|im_start|>user\\n<IMG_CONTEXT>\\nPlease describe the image explicitly.<|im_end|>\\n<|im_start|>assistant\\n\"\n+        )\n         inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.bfloat16)\n \n         # Forward\n@@ -378,8 +382,8 @@ def test_qwen2_small_model_integration_batched_generate(self):\n         )\n         # Prepare inputs\n         prompt = [\n-            \"<|im_start|>user\\n<image>\\nWrite a haiku for this image<|im_end|>\\n<|im_start|>assistant\\n\",\n-            \"<|im_start|>user\\n<image>\\nDescribe this image<|im_end|>\\n<|im_start|>assistant\\n\",\n+            \"<|im_start|>user\\n<IMG_CONTEXT>\\nWrite a haiku for this image<|im_end|>\\n<|im_start|>assistant\\n\",\n+            \"<|im_start|>user\\n<IMG_CONTEXT>\\nDescribe this image<|im_end|>\\n<|im_start|>assistant\\n\",\n         ]\n         image1 = Image.open(requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True).raw)\n         image2 = Image.open(requests.get(\"https://www.ilankelman.org/stopsigns/australia.jpg\", stream=True).raw)\n@@ -414,8 +418,8 @@ def test_qwen2_small_model_integration_batched_generate_multi_image(self):\n         )\n         # Prepare inputs\n         prompt = [\n-            \"<|im_start|>user\\n<image>\\nWrite a haiku for this image<|im_end|>\\n<|im_start|>assistant\\n\",\n-            \"<|im_start|>user\\n<image><image>\\nWhat are the differences between these two images?<|im_end|>\\n<|im_start|>assistant\\n\",\n+            \"<|im_start|>user\\n<IMG_CONTEXT>\\nWrite a haiku for this image<|im_end|>\\n<|im_start|>assistant\\n\",\n+            \"<|im_start|>user\\n<IMG_CONTEXT><IMG_CONTEXT>\\nWhat are the differences between these two images?<|im_end|>\\n<|im_start|>assistant\\n\",\n         ]\n         image1 = Image.open(requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True).raw)\n         image2 = Image.open(\n@@ -485,6 +489,7 @@ def test_qwen2_medium_model_integration_video(self):\n             tokenize=True,\n             return_dict=True,\n             return_tensors=\"pt\",\n+            num_frames=8,\n         ).to(torch_device, dtype=torch.float16)\n \n         output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n@@ -552,6 +557,7 @@ def test_qwen2_small_model_integration_interleaved_images_videos(self):\n             return_dict=True,\n             return_tensors=\"pt\",\n             padding=True,\n+            num_frames=8,\n         ).to(torch_device, dtype=torch.bfloat16)\n \n         output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n@@ -601,7 +607,9 @@ def test_llama_small_model_integration_generate(self):\n         url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n         image = Image.open(requests.get(url, stream=True).raw)\n \n-        prompt = \"<|im_start|>user\\n<image>\\nPlease describe the image explicitly.<|im_end|>\\n<|im_start|>assistant\\n\"\n+        prompt = (\n+            \"<|im_start|>user\\n<IMG_CONTEXT>\\nPlease describe the image explicitly.<|im_end|>\\n<|im_start|>assistant\\n\"\n+        )\n         inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.bfloat16)\n         with torch.no_grad():\n             generate_ids = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n@@ -619,7 +627,9 @@ def test_llama_small_model_integration_forward(self):\n         url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n         image = Image.open(requests.get(url, stream=True).raw)\n \n-        prompt = \"<|im_start|>user\\n<image>\\nPlease describe the image explicitly.<|im_end|>\\n<|im_start|>assistant\\n\"\n+        prompt = (\n+            \"<|im_start|>user\\n<IMG_CONTEXT>\\nPlease describe the image explicitly.<|im_end|>\\n<|im_start|>assistant\\n\"\n+        )\n         inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.bfloat16)\n \n         # Forward\n@@ -687,8 +697,8 @@ def test_llama_small_model_integration_batched_generate(self):\n         )\n         # Prepare inputs\n         prompt = [\n-            \"<|im_start|>user\\n<image>\\nWrite a haiku for this image<|im_end|>\\n<|im_start|>assistant\\n\",\n-            \"<|im_start|>user\\n<image>\\nDescribe this image<|im_end|>\\n<|im_start|>assistant\\n\",\n+            \"<|im_start|>user\\n<IMG_CONTEXT>\\nWrite a haiku for this image<|im_end|>\\n<|im_start|>assistant\\n\",\n+            \"<|im_start|>user\\n<IMG_CONTEXT>\\nDescribe this image<|im_end|>\\n<|im_start|>assistant\\n\",\n         ]\n         image1 = Image.open(requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True).raw)\n         image2 = Image.open(requests.get(\"https://www.ilankelman.org/stopsigns/australia.jpg\", stream=True).raw)\n@@ -724,8 +734,8 @@ def test_llama_small_model_integration_batched_generate_multi_image(self):\n         )\n         # Prepare inputs\n         prompt = [\n-            \"<|im_start|>user\\n<image>\\nWrite a haiku for this image<|im_end|>\\n<|im_start|>assistant\\n\",\n-            \"<|im_start|>user\\n<image><image>\\nWhat are the difference between these two images?<|im_end|>\\n<|im_start|>assistant\\n\",\n+            \"<|im_start|>user\\n<IMG_CONTEXT>\\nWrite a haiku for this image<|im_end|>\\n<|im_start|>assistant\\n\",\n+            \"<|im_start|>user\\n<IMG_CONTEXT><IMG_CONTEXT>\\nWhat are the difference between these two images?<|im_end|>\\n<|im_start|>assistant\\n\",\n         ]\n         image1 = Image.open(requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True).raw)\n         image2 = Image.open(\n@@ -795,6 +805,7 @@ def test_llama_medium_model_integration_video(self):\n             tokenize=True,\n             return_dict=True,\n             return_tensors=\"pt\",\n+            num_frames=8,\n         ).to(torch_device, dtype=torch.float16)\n \n         output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n@@ -862,6 +873,7 @@ def test_llama_small_model_integration_interleaved_images_videos(self):\n             return_dict=True,\n             return_tensors=\"pt\",\n             padding=True,\n+            num_frames=8,\n         ).to(torch_device, dtype=torch.bfloat16)\n \n         output = model.generate(**inputs, do_sample=False, max_new_tokens=25)"
        },
        {
            "sha": "05ac03f0a86a2c1a297baca11f85439f85f06a81",
            "filename": "tests/models/internvl/test_processor_internvl.py",
            "status": "modified",
            "additions": 7,
            "deletions": 27,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e9087368c14cef9d759f3ebfc6602b9350f345d/tests%2Fmodels%2Finternvl%2Ftest_processor_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e9087368c14cef9d759f3ebfc6602b9350f345d/tests%2Fmodels%2Finternvl%2Ftest_processor_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_processor_internvl.py?ref=1e9087368c14cef9d759f3ebfc6602b9350f345d",
            "patch": "@@ -64,7 +64,8 @@ def setUpClass(cls):\n             **processor_kwargs,\n         )\n         processor.save_pretrained(cls.tmpdirname)\n-        cls.image_token = processor.fake_image_token\n+        cls.image_token = processor.image_token\n+        cls.video_token = processor.video_token\n \n     @staticmethod\n     def prepare_processor_dict():\n@@ -138,6 +139,7 @@ def test_process_interleaved_images_videos(self):\n             return_dict=True,\n             return_tensors=\"pt\",\n             padding=True,\n+            num_frames=8,\n         )\n \n         # Process non batched inputs to check if the pixel_values and input_ids are reconstructed in the correct order when batched together\n@@ -150,6 +152,7 @@ def test_process_interleaved_images_videos(self):\n                 return_dict=True,\n                 return_tensors=\"pt\",\n                 padding=True,\n+                num_frames=8,\n             )\n             # We slice with [-inputs[\"input_ids\"].shape[1] :] as the input_ids are left padded\n             torch.testing.assert_close(\n@@ -223,6 +226,7 @@ def _process_messages_for_chat_template(\n             tokenize=True,\n             return_dict=True,\n             return_tensors=\"np\",\n+            num_frames=8,\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n \n@@ -272,30 +276,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), num_frames)\n \n-        # Load with `video_fps` arg\n-        video_fps = 1\n-        out_dict_with_video = processor.apply_chat_template(\n-            messages,\n-            add_generation_prompt=True,\n-            tokenize=True,\n-            return_dict=True,\n-            video_fps=video_fps,\n-            num_frames=None,  # force to use default num_frames\n-            return_tensors=\"np\",\n-        )\n-        self.assertTrue(self.videos_input_name in out_dict_with_video)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), video_fps * 10)\n-\n-        # Load with `video_fps` and `num_frames` args, should raise an error\n-        with self.assertRaises(ValueError):\n-            out_dict_with_video = processor.apply_chat_template(\n-                messages,\n-                add_generation_prompt=True,\n-                tokenize=True,\n-                return_dict=True,\n-                video_fps=video_fps,\n-                num_frames=num_frames,\n-            )\n+        # Load with `video_fps` arg is not possible with InternVL (skip)\n \n         # Load without any arg should use the default loading method\n         out_dict_with_video = processor.apply_chat_template(\n@@ -305,8 +286,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n             return_dict=True,\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n-        # Difference with common tests, InternVLProcessor returns flattened video features, and uses 8 frames by default\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 8)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 300)\n \n         # Load video as a list of frames (i.e. images). NOTE: each frame should have same size\n         # because we assume they come from one video"
        }
    ],
    "stats": {
        "total": 206,
        "additions": 87,
        "deletions": 119
    }
}