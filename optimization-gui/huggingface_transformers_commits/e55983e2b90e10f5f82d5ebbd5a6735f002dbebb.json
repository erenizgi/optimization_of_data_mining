{
    "author": "ydshieh",
    "message": "Fix `aya_vision` test (#38674)\n\n* fix 1: load_in_4bit=True,\n\n* fix 2: decorateor\n\n* fixfix 2: breakpoint\n\n* fixfix 3: update\n\n* fixfix 4: fast\n\n* fixfix 5: cond\n\n* fixfix 5: cond\n\n* fixfix 6: cuda 8\n\n* ruff\n\n* breakpoint\n\n* dtype\n\n* a10\n\n* a10\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "e55983e2b90e10f5f82d5ebbd5a6735f002dbebb",
    "files": [
        {
            "sha": "e4596591e2514278b6c2fafe628b531bbe10ee52",
            "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
            "status": "modified",
            "additions": 73,
            "deletions": 24,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/e55983e2b90e10f5f82d5ebbd5a6735f002dbebb/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e55983e2b90e10f5f82d5ebbd5a6735f002dbebb/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py?ref=e55983e2b90e10f5f82d5ebbd5a6735f002dbebb",
            "patch": "@@ -27,6 +27,7 @@\n from transformers.testing_utils import (\n     Expectations,\n     cleanup,\n+    get_device_properties,\n     require_deterministic_for_xpu,\n     require_read_token,\n     require_torch,\n@@ -330,19 +331,39 @@ def test_batching_equivalence(self):\n @require_read_token\n @require_torch\n class AyaVisionIntegrationTest(unittest.TestCase):\n-    def setUp(self):\n-        self.model_checkpoint = \"CohereForAI/aya-vision-8b\"\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.model_checkpoint = \"CohereForAI/aya-vision-8b\"\n+        cls.model = None\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        del cls.model_checkpoint\n+        cleanup(torch_device, gc_collect=True)\n \n     def tearDown(self):\n         cleanup(torch_device, gc_collect=True)\n \n+    @classmethod\n+    def get_model(cls):\n+        # Use 4-bit on T4\n+        load_in_4bit = get_device_properties()[0] == \"cuda\" and get_device_properties()[1] < 8\n+        torch_dtype = None if load_in_4bit else torch.float16\n+\n+        if cls.model is None:\n+            cls.model = AyaVisionForConditionalGeneration.from_pretrained(\n+                cls.model_checkpoint,\n+                device_map=torch_device,\n+                torch_dtype=torch_dtype,\n+                load_in_4bit=load_in_4bit,\n+            )\n+        return cls.model\n+\n     @slow\n     @require_torch_accelerator\n     def test_small_model_integration_forward(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n-        model = AyaVisionForConditionalGeneration.from_pretrained(\n-            self.model_checkpoint, device_map=torch_device, torch_dtype=torch.float16\n-        )\n+        model = self.get_model()\n         messages = [\n             {\n                 \"role\": \"user\",\n@@ -361,7 +382,17 @@ def test_small_model_integration_forward(self):\n             output = model(**inputs)\n \n         actual_logits = output.logits[0, -1, :5].cpu()\n-        expected_logits = torch.tensor([0.4109, 0.1532, 0.8018, 2.1328, 0.5483], dtype=torch.float16)\n+\n+        EXPECTED_LOGITS = Expectations(\n+            {\n+                (\"xpu\", 3): [0.4109, 0.1532, 0.8018, 2.1328, 0.5483],\n+                # 4-bit\n+                (\"cuda\", 7): [0.1097, 0.3481, 3.8340, 9.7969, 2.0488],\n+                (\"cuda\", 8): [1.6396, 0.6094, 3.1992, 8.5234, 2.1875],\n+            }\n+        )  # fmt: skip\n+        expected_logits = torch.tensor(EXPECTED_LOGITS.get_expectation(), dtype=torch.float16)\n+\n         self.assertTrue(\n             torch.allclose(actual_logits, expected_logits, atol=0.1),\n             f\"Actual logits: {actual_logits}\"\n@@ -374,9 +405,7 @@ def test_small_model_integration_forward(self):\n     @require_deterministic_for_xpu\n     def test_small_model_integration_generate_text_only(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n-        model = AyaVisionForConditionalGeneration.from_pretrained(\n-            self.model_checkpoint, device_map=torch_device, torch_dtype=torch.float16\n-        )\n+        model = self.get_model()\n         messages = [\n             {\n                 \"role\": \"user\",\n@@ -398,7 +427,9 @@ def test_small_model_integration_generate_text_only(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"Whispers on the breeze,\\nLeaves dance under moonlit sky,\\nNature's quiet song.\",\n-                (\"cuda\", 7): \"Whispers on the breeze,\\nLeaves dance under moonlit skies,\\nNature's quiet song.\",\n+                # 4-bit\n+                (\"cuda\", 7): \"Sure, here's a haiku for you:\\n\\nMorning dew sparkles,\\nPetals unfold in sunlight,\\n\",\n+                (\"cuda\", 8): \"Whispers on the breeze,\\nLeaves dance under moonlit skies,\\nNature's quiet song.\",\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -409,9 +440,7 @@ def test_small_model_integration_generate_text_only(self):\n     @require_torch_accelerator\n     def test_small_model_integration_generate_chat_template(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n-        model = AyaVisionForConditionalGeneration.from_pretrained(\n-            self.model_checkpoint, device_map=torch_device, torch_dtype=torch.float16\n-        )\n+        model = self.get_model()\n         messages = [\n             {\n                 \"role\": \"user\",\n@@ -430,16 +459,24 @@ def test_small_model_integration_generate_chat_template(self):\n             decoded_output = processor.decode(\n                 generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n             )\n-        expected_output = \"The image depicts a cozy scene of two cats resting on a bright pink blanket. The cats,\"  # fmt: skip\n+\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): \"The image depicts a cozy scene of two cats resting on a bright pink blanket. The cats,\",\n+                # 4-bit\n+                (\"cuda\", 7): 'The image depicts two cats comfortably resting on a pink blanket spread across a sofa. The cats,',\n+                (\"cuda\", 8): 'The image depicts a cozy scene of two cats resting on a bright pink blanket. The cats,',\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n+\n         self.assertEqual(decoded_output, expected_output)\n \n     @slow\n     @require_torch_accelerator\n     def test_small_model_integration_batched_generate(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n-        model = AyaVisionForConditionalGeneration.from_pretrained(\n-            self.model_checkpoint, device_map=torch_device, torch_dtype=torch.float16\n-        )\n+        model = self.get_model()\n         # Prepare inputs\n         messages = [\n             [\n@@ -472,7 +509,9 @@ def test_small_model_integration_batched_generate(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest lake.\",\n-                (\"cuda\", 7): \"Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest scene.\",\n+                # 4-bit\n+                (\"cuda\", 7): \"Wooden bridge stretches\\nMirrored lake below, mountains rise\\nPeaceful, serene\",\n+                (\"cuda\", 8): 'Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest scene.',\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -485,7 +524,16 @@ def test_small_model_integration_batched_generate(self):\n \n         # Check second output\n         decoded_output = processor.decode(output[1, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n-        expected_output = 'This image captures a vibrant street scene in a bustling urban area, likely in an Asian city. The focal point is a'  # fmt: skip\n+\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): 'This image captures a vibrant street scene in a bustling urban area, likely in an Asian city. The focal point is a',\n+                # 4-bit\n+                (\"cuda\", 7): 'This vibrant image captures a bustling street scene in a multicultural urban area, featuring a traditional Chinese gate adorned with intricate red and',\n+                (\"cuda\", 8): 'This image captures a vibrant street scene in a bustling urban area, likely in an Asian city. The focal point is a',\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n \n         self.assertEqual(\n             decoded_output,\n@@ -498,9 +546,7 @@ def test_small_model_integration_batched_generate(self):\n     @require_deterministic_for_xpu\n     def test_small_model_integration_batched_generate_multi_image(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n-        model = AyaVisionForConditionalGeneration.from_pretrained(\n-            self.model_checkpoint, device_map=torch_device, torch_dtype=torch.float16\n-        )\n+        model = self.get_model()\n         # Prepare inputs\n         messages = [\n             [\n@@ -543,7 +589,8 @@ def test_small_model_integration_batched_generate_multi_image(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest lake.\",\n-                (\"cuda\", 7): \"Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest scene.\",\n+                (\"cuda\", 7): 'Wooden bridge stretches\\nMirrored lake below, mountains rise\\nPeaceful, serene',\n+                (\"cuda\", 8): 'Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest scene.',\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -559,10 +606,12 @@ def test_small_model_integration_batched_generate_multi_image(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"The first image showcases the Statue of Liberty, a colossal neoclassical sculpture on Liberty Island in New York Harbor. Standing at \",\n-                (\"cuda\", 7): \"The first image showcases the Statue of Liberty, a colossal neoclassical sculpture on Liberty Island in New York Harbor. Standing at a\",\n+                (\"cuda\", 7): 'The first image showcases the Statue of Liberty, a monumental sculpture located on Liberty Island in New York Harbor. Standing atop a',\n+                (\"cuda\", 8): 'The first image showcases the Statue of Liberty, a colossal neoclassical sculpture on Liberty Island in New York Harbor. Standing at ',\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n+\n         self.assertEqual(\n             decoded_output,\n             expected_output,"
        },
        {
            "sha": "4e17bea44fa30eebf4cfede06e0e47f865ff5011",
            "filename": "tests/models/aya_vision/test_processor_aya_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e55983e2b90e10f5f82d5ebbd5a6735f002dbebb/tests%2Fmodels%2Faya_vision%2Ftest_processor_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e55983e2b90e10f5f82d5ebbd5a6735f002dbebb/tests%2Fmodels%2Faya_vision%2Ftest_processor_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_processor_aya_vision.py?ref=e55983e2b90e10f5f82d5ebbd5a6735f002dbebb",
            "patch": "@@ -17,7 +17,7 @@\n import unittest\n \n from transformers import AutoProcessor, AutoTokenizer, AyaVisionProcessor\n-from transformers.testing_utils import require_read_token, require_torch, require_vision\n+from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n@@ -31,7 +31,6 @@\n     from transformers import GotOcr2ImageProcessor\n \n \n-@require_read_token\n @require_vision\n class AyaVisionProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     processor_class = AyaVisionProcessor"
        }
    ],
    "stats": {
        "total": 100,
        "additions": 74,
        "deletions": 26
    }
}