{
    "author": "remi-or",
    "message": "Fixes for EncoderDecoderCache (#40008)\n\n* Add expectation to t5 for rocm 9.4\n\n* Made EncoderDecoderCache compatible with nn.DataParallel\n\n* Fixed t5gemma EncoderDecoderCache\n\n* Added todos in autoformer\n\n* Ruff\n\n* Init is self-contained\n\n* Review compliance\n\n* Fixed kwargs init of EncoderDecoderCache",
    "sha": "2bcf9f6c7ef7f84b3c3aeec51e117429ecdc1254",
    "files": [
        {
            "sha": "0c1524c6164a8f1f7920b0212516ea098dcc92fb",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 30,
            "deletions": 15,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bcf9f6c7ef7f84b3c3aeec51e117429ecdc1254/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bcf9f6c7ef7f84b3c3aeec51e117429ecdc1254/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=2bcf9f6c7ef7f84b3c3aeec51e117429ecdc1254",
            "patch": "@@ -1464,13 +1464,31 @@ class EncoderDecoderCache(Cache):\n     ```\n     \"\"\"\n \n-    def __init__(self, self_attention_cache: Cache, cross_attention_cache: Cache):\n-        self.self_attention_cache = self_attention_cache\n-        self.cross_attention_cache = cross_attention_cache\n+    def __init__(self, *caches) -> None:\n+        # For dp and ddp support, if only one argument is passed, it should be an iterable of tuples of tensors\n+        if len(caches) == 1:\n+            self.self_attention_cache = DynamicCache()\n+            self.cross_attention_cache = DynamicCache()\n+            # Populate cache from the iterable\n+            for layer_idx, key_value_states in enumerate(caches[0]):\n+                key_states, value_states = key_value_states[:2]\n+                self.self_attention_cache.update(key_states, value_states, layer_idx)\n+                if len(key_value_states) > 2:\n+                    key_states, value_states = key_value_states[2:]\n+                    self.cross_attention_cache.update(key_states, value_states, layer_idx)\n+        # Otherwise, we should get two arguments, a self-attention cache and a cross-attention cache\n+        elif len(caches) == 2:\n+            if not isinstance(caches[0], Cache) or not isinstance(caches[1], Cache):\n+                raise TypeError(f\"One of the two arguments is not a Cache: {type(caches[0]) = }, {type(caches[1]) = }\")\n+            self.self_attention_cache = caches[0]\n+            self.cross_attention_cache = caches[1]\n+        # Error case\n+        else:\n+            raise ValueError(f\"Expected 1 or 2 arguments, got {len(caches)}\")\n \n         self.is_updated = {}\n-        for layer_idx in range(len(cross_attention_cache)):\n-            self.is_updated[layer_idx] = bool(cross_attention_cache.get_seq_length(layer_idx) > 0)\n+        for layer_idx in range(len(self.cross_attention_cache)):\n+            self.is_updated[layer_idx] = bool(self.cross_attention_cache.get_seq_length(layer_idx) > 0)\n \n     def __repr__(self) -> str:\n         return (\n@@ -1527,21 +1545,18 @@ def to_legacy_cache(self) -> tuple[tuple[torch.Tensor]]:\n \n     @classmethod\n     def from_legacy_cache(\n-        cls, past_key_values: tuple[tuple[torch.FloatTensor, torch.FloatTensor], ...]\n+        cls, past_key_values: Optional[Iterable[tuple[torch.FloatTensor, ...]]]\n     ) -> \"EncoderDecoderCache\":\n         \"\"\"Converts a cache in the legacy cache format into an equivalent `EncoderDecoderCache`.\"\"\"\n+        cache = cls(DynamicCache(), DynamicCache())\n         if past_key_values is None:\n             logger.warning_once(\"past_key_values should not be None in from_legacy_cache()\")\n-        cache = cls(\n-            self_attention_cache=DynamicCache(),\n-            cross_attention_cache=DynamicCache(),\n-        )\n-        if past_key_values is not None:\n-            for layer_idx in range(len(past_key_values)):\n-                key_states, value_states = past_key_values[layer_idx][:2]\n+        else:\n+            for layer_idx, key_value_states in enumerate(past_key_values):\n+                key_states, value_states = key_value_states[:2]\n                 cache.self_attention_cache.update(key_states, value_states, layer_idx)\n-                if len(past_key_values[layer_idx]) > 2:\n-                    key_states, value_states = past_key_values[layer_idx][2:]\n+                if len(key_value_states) > 2:\n+                    key_states, value_states = key_value_states[2:]\n                     cache.cross_attention_cache.update(key_states, value_states, layer_idx)\n                     cache.is_updated[layer_idx] = True\n         return cache"
        },
        {
            "sha": "8c3fc0fa4e5331ff81288a9a1e6c0c95e4750a78",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bcf9f6c7ef7f84b3c3aeec51e117429ecdc1254/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bcf9f6c7ef7f84b3c3aeec51e117429ecdc1254/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=2bcf9f6c7ef7f84b3c3aeec51e117429ecdc1254",
            "patch": "@@ -446,9 +446,7 @@ def forward(\n             elif isinstance(past_key_values, DynamicCache):\n                 past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n             elif past_key_values is None:\n-                past_key_values = EncoderDecoderCache(\n-                    self_attention_cache=DynamicCache(), cross_attention_cache=DynamicCache()\n-                )\n+                past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n \n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None"
        },
        {
            "sha": "fc704fd2392060621a538962869b6ba21fa8be6d",
            "filename": "tests/models/t5/test_modeling_t5.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bcf9f6c7ef7f84b3c3aeec51e117429ecdc1254/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bcf9f6c7ef7f84b3c3aeec51e117429ecdc1254/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py?ref=2bcf9f6c7ef7f84b3c3aeec51e117429ecdc1254",
            "patch": "@@ -25,6 +25,7 @@\n from transformers.models.auto.modeling_auto import MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES\n from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_4\n from transformers.testing_utils import (\n+    Expectations,\n     cleanup,\n     require_accelerate,\n     require_sentencepiece,\n@@ -1200,7 +1201,12 @@ def test_small_integration_test(self):\n         loss = model(input_ids.to(torch_device), labels=labels.to(torch_device)).loss\n         mtf_score = -(labels.shape[-1] * loss.item())\n \n-        EXPECTED_SCORE = -19.0845\n+        EXPECTED_SCORE = Expectations(\n+            {\n+                (None, None): -19.0845,\n+                (\"rocm\", (9, 4)): -19.0846,\n+            }\n+        ).get_expectation()\n         self.assertTrue(abs(mtf_score - EXPECTED_SCORE) < 1e-4)\n \n     @slow"
        },
        {
            "sha": "abfc7e74c64610d40dc593ea4c4b3059e1618cb1",
            "filename": "tests/models/t5gemma/test_modeling_t5gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2bcf9f6c7ef7f84b3c3aeec51e117429ecdc1254/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2bcf9f6c7ef7f84b3c3aeec51e117429ecdc1254/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py?ref=2bcf9f6c7ef7f84b3c3aeec51e117429ecdc1254",
            "patch": "@@ -1386,10 +1386,6 @@ def test_flex_attention_with_grads(self):\n             # If this does not raise an error, the test passes (see https://github.com/huggingface/transformers/pull/35605)\n             _ = model(**dummy_inputs)\n \n-    @unittest.skip(\"EncoderDecoderCache can't be gathered because it is not iterable.\")\n-    def test_multi_gpu_data_parallel_forward(self):\n-        pass\n-\n \n class T5GemmaEncoderOnlyModelTester:\n     config_class = T5GemmaConfig"
        }
    ],
    "stats": {
        "total": 61,
        "additions": 38,
        "deletions": 23
    }
}