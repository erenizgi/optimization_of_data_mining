{
    "author": "remi-or",
    "message": "Better continuous batching tests (#42699)\n\n* No more size 0 cuda graph\n\n* Better tests for CB\n\n* compile fix for CB test\n\n* style\n\n* More cleanup and cuda exclusive\n\n* Returned to slow tests\n\n* Change decorator order\n\n* Restore XPU change\n\n* Rebase fixes",
    "sha": "f8e5ae6a5072cfea78c5d052ebe61b63c9cb10cc",
    "files": [
        {
            "sha": "56b8e336e282325f38ee0d6d97b8028852c81ba3",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8e5ae6a5072cfea78c5d052ebe61b63c9cb10cc/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8e5ae6a5072cfea78c5d052ebe61b63c9cb10cc/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=f8e5ae6a5072cfea78c5d052ebe61b63c9cb10cc",
            "patch": "@@ -66,7 +66,7 @@ def pad_by_intervals(size: int, max_value: int, nb_intervals: int) -> int:\n     interval_size = max_value // nb_intervals\n     if interval_size == 0:\n         return max_value\n-    padded = ceil(size / interval_size) * interval_size\n+    padded = ceil(size / interval_size) * interval_size if size > 0 else interval_size\n     return min(padded, max_value)\n \n \n@@ -713,6 +713,7 @@ def _process_logit(self, batch_data: dict, logits: torch.Tensor, logit_processor\n         # Handle shape compatibility: logit processors expect 2D tensors [batch_size, vocab_size]\n         # but continuous batching always produces 3D tensors [batch_size, seq_len, vocab_size]\n         batch_size, seq_len, vocab_size = logits.shape\n+        # NOTE: to be an exact match with generate, we should also convert logits2d to float32 here, but it's not needed in practice\n         logits_2d = logits.view(batch_size * seq_len, vocab_size)\n         input_ids_2d = batch_data[\"input_ids\"].view(batch_size * seq_len)\n         # Process with 2D tensors\n@@ -869,7 +870,7 @@ def stop(self, block: bool = True, timeout: float | None = None) -> None:\n             logger.warning(\"\\nBatch processor was not initialized.\")\n         else:\n             if self.batch_processor.cache.use_prefix_sharing:\n-                logger.warning(\n+                logger.info(\n                     f\"\\nPrefix sharing was on. Total prefix length: {self.batch_processor.cache._total_prefix_length}\"\n                 )\n "
        },
        {
            "sha": "41124260103871edd2c52f9b11d939dc877228c7",
            "filename": "tests/generation/test_continuous_batching.py",
            "status": "modified",
            "additions": 203,
            "deletions": 296,
            "changes": 499,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8e5ae6a5072cfea78c5d052ebe61b63c9cb10cc/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8e5ae6a5072cfea78c5d052ebe61b63c9cb10cc/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_continuous_batching.py?ref=f8e5ae6a5072cfea78c5d052ebe61b63c9cb10cc",
            "patch": "@@ -12,29 +12,62 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import gc\n+import itertools\n import unittest\n \n import torch\n from parameterized import parameterized\n \n-from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, LogitsProcessorList\n+from transformers import (\n+    AutoConfig,\n+    AutoModelForCausalLM,\n+    AutoTokenizer,\n+    CompileConfig,\n+    GenerationConfig,\n+    LogitsProcessorList,\n+)\n from transformers.generation.continuous_batching.cache import group_layers_by_attn_type\n from transformers.generation.continuous_batching.continuous_api import build_attention_mask\n from transformers.testing_utils import (\n     Expectations,\n     require_deterministic_for_xpu,\n-    require_kernels,\n-    require_read_token,\n     require_torch_accelerator,\n     slow,\n     torch_device,\n )\n-\n-\n-ALLOW_EXPECTED_OUTPUTS = True  # this is a debug flag when you want to measure deviation between CB and non-CB gen\n-\n-\n-class ContinuousBatchingTest(unittest.TestCase):\n+from transformers.utils import is_flash_attn_2_available, is_kernels_available\n+\n+\n+def flush_memory(flush_compile: bool = True) -> None:\n+    gc.collect()\n+    # If needed, flush everything related to torch.compile\n+    if flush_compile:\n+        # Dynamo resets\n+        torch._dynamo.reset()\n+        torch._dynamo.reset_code_caches()\n+        if hasattr(torch._inductor, \"codecache\"):\n+            # Clear FX graph cache\n+            if hasattr(torch._inductor.codecache, \"FxGraphCache\"):\n+                torch._inductor.codecache.FxGraphCache.clear()\n+            # Clear PyCodeCache\n+            if hasattr(torch._inductor.codecache, \"PyCodeCache\"):\n+                torch._inductor.codecache.PyCodeCache.cache_clear()\n+            # Clear TritonFuture cache (for async compilation)\n+            if hasattr(torch._inductor.codecache, \"TritonFuture\"):\n+                if hasattr(torch._inductor.codecache.TritonFuture, \"_compile_cache\"):\n+                    torch._inductor.codecache.TritonFuture._compile_cache.clear()\n+    # Clear CUDA cache\n+    if torch.cuda.is_available():\n+        torch.cuda.empty_cache()\n+        torch.cuda.synchronize()\n+    elif torch.xpu.is_available():\n+        torch.xpu.empty_cache()\n+        torch.xpu.synchronize()\n+    gc.collect()\n+\n+\n+class ContinuousBatchingNonGenerationTest(unittest.TestCase):\n     @parameterized.expand(\n         [\n             (None, None, \"0\"),\n@@ -54,6 +87,7 @@ def test_group_layers(\n         sliding_window: int | None,\n         expected_groups: str,\n     ) -> None:\n+        \"\"\"Test the layer grouping algorithm of the hybrid allocator.\"\"\"\n         # Take a config and change the layer_types attribute to the mix we want\n         config = AutoConfig.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n \n@@ -113,6 +147,7 @@ def test_attention_mask(\n         sliding_window: int,  # the sliding window size, 1 means no sliding window\n         str_expected_mask: list[str],  # the attention mask, broken down by line as a string of 0s and 1s\n     ) -> None:\n+        \"\"\"Tests the correctness of the attention mask used in the continuous batching API.\"\"\"\n         # Build expected mask\n         minus_inf = torch.finfo(torch.float32).min\n         expected_mask = torch.empty((cumulative_seqlens_q[-1], cumulative_seqlens_k[-1]), dtype=torch.float32)\n@@ -138,343 +173,219 @@ def test_attention_mask(\n                 f\"Actual mask:\\n{str_mask}\"\n             )\n \n+\n+class ContinuousBatchingGenerationTest(unittest.TestCase):\n+    # -----------------------------------------------Parity tests----------------------------------------------- #\n+    #         Ensure continuous batching and non-continuous batching generation produce the same outputs         #\n+    # ---------------------------------------------------------------------------------------------------------- #\n     @require_deterministic_for_xpu\n-    def _continuous_batching_parity(\n-        self, model_id: str, attn_implementation: str, expected_outputs: dict[str, str]\n+    def _test_continuous_batching_parity(\n+        self,\n+        model_id: str,\n+        allow_prefix_sharing: bool,\n+        attn_implementation: str,\n+        use_cuda_graph: bool,\n+        use_compile: bool,\n+        max_new_tokens: int = 20,\n     ) -> None:\n-        # Prepare common elements\n+        \"\"\"Tests the parity between continuous batching and non-continuous batching generation.\"\"\"\n+\n+        # Skip the test if Flash Attention 2 is required but not available\n+        if attn_implementation == \"flash_attention_2\" and not (is_flash_attn_2_available() or is_kernels_available()):\n+            self.skipTest(\"Flash Attention 2 is not available and neither is the kernels library. Skipping test.\")\n+        # Skip the test if cuda graph is on but the device is not CUDA\n+        if use_cuda_graph and torch_device != \"cuda\":\n+            self.skipTest(\"CUDA graph is only supported on CUDA devices. Skipping test.\")\n+\n+        # Prepare continuous batching inputs\n         tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n-        prompts = [\n-            \"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her \"\n-                \"friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh \"\n-                \"duck egg. How much in dollars does she make every day at the farmers' market? The answer is:\",\n-            \"A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take? \"\n-                \"The answer is:\",\n-            \"Josh decides to try flipping a house. He buys a house for $80,000 and then puts in $50,000 in repairs. \"\n-                \"This increased the value of the house by 150%. How much profit did he make? The answer is:\",\n+        tokenizer.pad_token = tokenizer.eos_token\n+        user_messages = [\n+            \"Josh decides to try flipping a house. He buys a house for $80,000 and then puts in $50,000 in repairs. This increased the value of the house by 150%. How much profit did he make?\",\n+            \"A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take?\",\n+            \"A basket contains 25 oranges among which 1 is bad, 20% are unripe, 2 are sour and the rest are good. How many oranges are good?\",\n         ]  # fmt: skip\n-        batched_inputs = [tokenizer.encode(prompt) for prompt in prompts]\n+        chats = [[{\"role\": \"user\", \"content\": user_message}] for user_message in user_messages]\n+        tokenized = [tokenizer.apply_chat_template(chat, add_generation_prompt=True) for chat in chats]\n+        input_ids = [(x if isinstance(x, list) else x[\"input_ids\"]) for x in tokenized]\n+        print(f\"{input_ids[0] = } {type(input_ids[0]) = }\")\n+\n+        # Eager and SDPA implementations get a precision boost to account for the fact that an attention mask is used in\n+        # continuous batching but not in generate\n+        dtype = \"auto\" if attn_implementation == \"flash_attention_2\" else torch.float32\n \n         # Generation with continuous batching\n-        model = AutoModelForCausalLM.from_pretrained(model_id, attn_implementation=attn_implementation, dtype=\"auto\")\n+        model = AutoModelForCausalLM.from_pretrained(model_id, attn_implementation=attn_implementation, dtype=dtype)\n         model = model.to(torch_device).eval()\n-        model.generation_config.max_new_tokens = 40\n+        model.generation_config.max_new_tokens = max_new_tokens\n         model.generation_config.do_sample = False\n-        model.generation_config.use_cuda_graph = False\n-\n-        cb_outputs = model.generate_batch(inputs=batched_inputs, generation_config=model.generation_config)\n+        model.generation_config.use_cuda_graph = use_cuda_graph\n+        if use_compile:\n+            model.generation_config.compile_config = CompileConfig(fullgraph=True, mode=\"default\")\n \n-        # Generation without continuous batching\n-        if attn_implementation == \"paged|sdpa\":\n-            non_cb_attn_implementation = \"sdpa\"\n-        elif attn_implementation == \"paged|eager\":\n-            non_cb_attn_implementation = \"eager\"\n-        elif attn_implementation == \"paged|flash_attention_2\":\n-            non_cb_attn_implementation = \"eager\"\n-        else:\n-            raise ValueError(f\"Invalid attention implementation: {attn_implementation}\")\n+        # Generation with continuous batching\n+        continuous_batching_outputs = model.generate_batch(\n+            inputs=input_ids, generation_config=model.generation_config, allow_prefix_sharing=allow_prefix_sharing\n+        )\n \n-        # We regenerate the model because just changing the attn_implementation does not work\n-        model = AutoModelForCausalLM.from_pretrained(\n-            model_id, attn_implementation=non_cb_attn_implementation, dtype=\"auto\"\n+        # Prepare non-continuous batching inputs\n+        inputs = tokenizer.apply_chat_template(\n+            chats,\n+            add_generation_prompt=True,\n+            return_tensors=\"pt\",\n+            padding=True,\n+            return_dict=True,\n+            return_attention_mask=True,\n         )\n+        num_input_tokens = inputs.input_ids.shape[1]\n+\n+        # Generation without continuous batching\n+        model = AutoModelForCausalLM.from_pretrained(model_id, attn_implementation=attn_implementation, dtype=dtype)\n         model = model.to(torch_device).eval()\n-        model.generation_config.max_new_tokens = 40\n+        model.generation_config.max_new_tokens = max_new_tokens\n         model.generation_config.do_sample = False\n-        model.generation_config.use_cuda_graph = False\n-\n-        for request_id, request in cb_outputs.items():\n-            # Generate without continuous batching\n-            input_ids = torch.tensor([request.prompt_ids]).to(torch_device)\n-            attention_mask = torch.ones_like(input_ids)\n-            outputs = model.generate(\n-                input_ids, attention_mask=attention_mask, generation_config=model.generation_config\n-            )\n-            generated_tokens = outputs[0][input_ids.shape[1] :]\n-            non_cb_decoded_output = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n-            input_ids = input_ids.tolist()[0]\n-\n-            # Check that the generated output with and without CB match\n-            cb_decoded_output = tokenizer.decode(request.generated_tokens, skip_special_tokens=True)\n-            outputs_match = non_cb_decoded_output == cb_decoded_output\n-\n-            # If they dont, that might be expected: the outputs can differ slightly due to numerical differences\n-            # If that's the case, there is an expected output ready\n-            if not outputs_match:\n-                expected_output = expected_outputs.get(request_id) if ALLOW_EXPECTED_OUTPUTS else None\n-\n-                if expected_output is None:\n-                    self.fail(\n-                        f\"Test {request_id = } failed, no expected output was provided.\\nRef:\"\n-                        f\"{repr(non_cb_decoded_output)}\\nOut:{repr(cb_decoded_output)}\"\n-                    )\n-                else:\n-                    self.assertEqual(\n-                        expected_output,\n-                        cb_decoded_output,\n-                        msg=f\"Test {request_id = } failed, expected output did not match.\\n\"\n-                        f\"Exp:{repr(expected_output)}\\nOut:{repr(cb_decoded_output)}\",\n-                    )\n-\n-    # Eager tests\n-    @require_torch_accelerator\n-    @require_read_token\n-    @slow\n-    def test_continuous_batching_parity_llama_eager(self) -> None:\n-        expected_outputs = Expectations({\n-            (\"rocm\", (9, 4)): {\n-                \"req_0\": \" $16. How did I get that answer? I used the following equation: 16 - 3 - 4 = 9. 9 x $2 = $18. $18 -\"\n-            },\n-            (\"cuda\", (9, 0)): {\n-                \"req_1\": \" 3 bolts of blue fiber and 1.5 bolts of white fiber. The total number of bolts is 4.5. The total number of bolts is 4.5. The total\",\n-                \"req_2\": \" $50,000. This is because the value of the house increased by 150%, which means that the value of the house increased by $50,000. This is because the value of the\"\n-            },\n-            (\"xpu\", None): {\n-                \"req_1\": \" 3 bolts of blue fiber and 1.5 bolts of white fiber. The answer is not 3.5 bolts of blue fiber and 1.5 bolts of white fiber. The answer'\",\n-                \"req_2\": \" $50,000. This is because the value of the house increased by 150%, which means that the value of the house increased by $50,000. This is because the value of the\"\n-            },\n-        }).get_expectation()  # fmt: skip\n-        self._continuous_batching_parity(\"meta-llama/Llama-3.1-8B\", \"paged|eager\", expected_outputs)\n-\n-    @require_torch_accelerator\n-    @slow\n-    def test_continuous_batching_parity_gemma_eager(self) -> None:\n-        expected_outputs = Expectations({\n-            (\"rocm\", (9, 4)): {\n-                \"req_1\": \" \\n\\n**Answer:** 3 bolts\\n\\n**Solution:**\\n\\n* **White fiber:** The robe needs half as much white fiber as blue fiber, so it needs 2 bolts / 2 =\"\n-            },\n-            (\"cuda\", (9, 0)): {\n-                \"req_0\": \"\\n\\n**$12**\\n\\n**Here's how to solve it:**\\n\\n* **Eggs eaten:** 3\\n* **Eggs left:** 16 - 3 = 13\",\n-                \"req_1\": \" \\n \\n 2 + 1 = 3 bolts \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \"\n-            },\n-            (\"xpu\", None): {\n-                \"req_0\": \"\\n\\n**$12**\\n\\n**Here's how to solve it:**\\n\\n* **Eggs eaten:** 3\\n* **Eggs left:** 16 - 3 = 13\",\n-                \"req_1\": \" \\n \\n 2 + 1 = 3 bolts \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \",\n-                \"req_2\": \"\\n\\n**$100,000**\\n\\n**Explanation:**\\n\\nHere's how to calculate the profit:\\n\\n1. **Calculate the total cost:** $80,00\",\n-            },\n-        }).get_expectation()  # fmt: skip\n-        self._continuous_batching_parity(\"google/gemma-2-2b-it\", \"paged|eager\", expected_outputs)\n-\n-    # FIXME: set expected_outputs\n-    # @require_torch_accelerator\n-    # @slow\n-    # def test_continuous_batching_parity_qwen_eager(self) -> None:\n-    #     expected_outputs = {}\n-    #     self._continuous_batching_parity(\"Qwen/Qwen3-4B-Instruct-2507\", \"paged|eager\", expected_outputs)\n-\n-    # FIXME: OOMs\n-    # @require_torch_accelerator\n-    # @slow\n-    # def test_continuous_batching_parity_gpt_oss_eager(self) -> None:\n-    #     expected_outputs = Expectations({\n-    #         (\"cuda\", (9, 0)): {\n-    #             \"req_1\": \" 2.5 bolts. The question: \\\"What is the name of the puzzle that involves a robe taking 2 bolts of blue fiber and half that much white fiber?\\\" The answer: \\\"The\",\n-    #             \"req_2\": \" 50%.\\\"\\n\\nWe need to parse: He buys a house for $80,000. He puts in $50,000 in repairs. This increased the value of the house by 150%.\"\n-    #         },\n-    #         (\"xpu\", None): {\n-    #             \"req_1\": \" 2.5 bolts. The question: \\\"What is the name of the puzzle that involves a robe taking 2 bolts of blue fiber and half that much white fiber?\\\" The answer: \\\"The\",\n-    #             \"req_2\": \" 50%.\\\"\\n\\nWe need to parse: He buys a house for $80,000. He puts in $50,000 in repairs. This increased the value of the house by 150%.\"\n-    #         },\n-    #     }).get_expectation()  # fmt: skip\n-    #     self._continuous_batching_parity(\"openai/gpt-oss-20b\", \"paged|eager\", expected_outputs)\n-\n-    # SDPA tests\n-    @require_read_token\n-    @require_torch_accelerator\n-    @slow\n-    def test_continuous_batching_parity_llama_sdpa(self) -> None:\n-        expected_outputs = Expectations({\n-            (\"rocm\", (9, 4)): {\n-                \"req_2\": \" $50,000. This is because the value of the house increased by 150%, which means that the value of the house increased by $50,000. This is because the value of the\"\n-            },\n-            (\"xpu\", None): {\n-                \"req_2\": \" $50,000. This is because the value of the house increased by 150%, which means that the value of the house increased by $50,000. This is because the value of the\"\n-            },\n-        }).get_expectation()  # fmt: skip\n-        self._continuous_batching_parity(\"meta-llama/Llama-3.1-8B\", \"paged|sdpa\", expected_outputs)\n-\n-    @require_torch_accelerator\n-    @slow\n-    def test_continuous_batching_parity_gemma_sdpa(self) -> None:\n-        expected_outputs = Expectations({\n-            (\"cuda\", (9, 0)): {\n-                \"req_1\": \" \\n\\n**Answer:** 3 bolts\\n\\n**Solution:**\\n\\n* **White fiber:** The robe needs half as much white fiber as blue fiber, so it needs 2 bolts / 2 =\",\n-            },\n-            (\"xpu\", None): {\n-                \"req_1\": \" \\n\\n**Answer:** 3 bolts\\n\\n**Solution:**\\n\\n* **White fiber:** The robe needs half as much white fiber as blue fiber, so it needs 2 bolts / 2 =\",\n-            },\n-        }).get_expectation()  # fmt: skip\n-        self._continuous_batching_parity(\"google/gemma-2-2b-it\", \"paged|sdpa\", expected_outputs)\n-\n-    # FIXME: set expected_outputs\n-    # @require_torch_accelerator\n-    # @slow\n-    # def test_continuous_batching_parity_qwen_sdpa(self) -> None:\n-    #     expected_outputs = {}\n-    #     self._continuous_batching_parity(\"Qwen/Qwen3-4B-Instruct-2507\", \"paged|sdpa\", expected_outputs)\n+        model.generation_config.use_cuda_graph = use_cuda_graph\n+        if use_compile:\n+            model.generation_config.compile_config = CompileConfig(fullgraph=True, mode=\"default\")\n+\n+        generate_outputs = model.generate(**inputs.to(torch_device), generation_config=model.generation_config)\n+\n+        for i, user_message in enumerate(user_messages):\n+            continuous_batching_output = continuous_batching_outputs[f\"req_{i}\"].generated_tokens\n+            generate_output = generate_outputs[i][num_input_tokens:].tolist()\n+            while generate_output[-1] == model.generation_config.pad_token_id:\n+                generate_output.pop()\n+\n+            if continuous_batching_output != generate_output:\n+                decoded_continuous_batching_output = tokenizer.decode(continuous_batching_output)\n+                decoded_generate_output = tokenizer.decode(generate_output)\n+                msg = f\"Test failed for {model_id = } {allow_prefix_sharing = }, {attn_implementation = }, {use_cuda_graph = }, {use_compile = }\\n\"\n+                msg += f\"User message              : {repr(user_message)}\\n\"\n+                msg += f\"Continuous batching output: {repr(decoded_continuous_batching_output)}\\n\"\n+                msg += f\"Generate output           : {repr(decoded_generate_output)}\"\n+                self.fail(msg)\n+\n+        del model\n+        flush_memory(flush_compile=use_compile)\n \n-    # GPT-OSS is not compatible with SDPA because it has an attention sink. TODO: is this fixable?\n-\n-    # Flash attention test\n+    @parameterized.expand(\n+        list(\n+            itertools.product(\n+                [False, True],\n+                [\"eager\", \"sdpa\", \"flash_attention_2\"],\n+                [False, True],\n+                [False, True],\n+            )\n+        )\n+    )\n     @require_torch_accelerator\n-    @require_kernels\n     @slow\n-    def test_continuous_batching_parity_llama_flash(self) -> None:\n-        expected_outputs = Expectations({\n-            (\"cuda\", (9, 0)): {\n-                \"req_1\": \" 3 bolts of blue fiber and 1.5 bolts of white fiber. The total number of bolts is 4.5 bolts. The total number of bolts is 4.5 bolts.\",\n-            },\n-            (\"xpu\", None): {\n-                \"req_1\": \" 3 bolts of blue fiber and 1.5 bolts of white fiber. The total number of bolts is 4.5 bolts. The total number of bolts is 4.5 bolts.\",\n-            },\n-        }).get_expectation()  # fmt: skip\n-        self._continuous_batching_parity(\"meta-llama/Llama-3.1-8B\", \"paged|flash_attention_2\", expected_outputs)\n+    def test_continuous_batching_config_combinations(\n+        self,\n+        allow_prefix_sharing: bool,\n+        attn_implementation: str,\n+        use_cuda_graph: bool,\n+        use_compile: bool,\n+    ) -> None:\n+        model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n+        self._test_continuous_batching_parity(\n+            model_id, allow_prefix_sharing, attn_implementation, use_cuda_graph, use_compile\n+        )\n \n-    @require_torch_accelerator\n-    @require_kernels\n-    @slow\n-    def test_continuous_batching_parity_gemma_flash(self) -> None:\n-        expected_outputs = Expectations({\n-            (\"cuda\", (9, 0)): {\n-                \"req_1\": \" \\n \\n 2 + 1 = 3 bolts \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \",\n-            },\n-            (\"xpu\", None): {\n-                \"req_0\": \"\\n\\n**$128**\\n\\n**Here's how to solve it:**\\n\\n* **Eggs eaten:** 3\\n* **Eggs left:** 16 - 3 = 1\",\n-                \"req_1\":  \"\\n\\n**Answer:** 3 bolts\\n\\n**Solution:**\\n\\n* **White fiber:** The robe needs half as much white fiber as blue fiber, so it needs 2 bolts / 2 =\",\n-            },\n-        }).get_expectation()  # fmt: skip\n-        self._continuous_batching_parity(\"google/gemma-2-2b-it\", \"paged|flash_attention_2\", expected_outputs)\n+    # FIXME: Qwen2.5-0.5B-Instruct is not here because it's  broken (it uses a repetition penalty logits processor)\n+    # TODO: replace gemma2 with a tiny version of GPT-OSS? That way we can test sliding window AND attention sink\n \n+    @parameterized.expand(\n+        list(\n+            itertools.product(\n+                [\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"google/gemma-2-2b-it\"],\n+                [False, True],\n+                [False, True],\n+            )\n+        )\n+    )\n     @require_torch_accelerator\n-    @require_kernels\n     @slow\n-    def test_continuous_batching_parity_qwen_flash(self) -> None:\n-        expected_outputs = Expectations({\n-            (\"xpu\", None): {\n-                \"req_1\":  \" 3.5 bolts.\\n\\nLet's break it down step by step:\\n\\n- Blue fiber: 2 bolts\\n- White fiber: half of 2 bolts = 1 bolt\\n\\nTotal = \",\n-            },\n-        }).get_expectation()  # fmt: skip\n-        self._continuous_batching_parity(\"Qwen/Qwen3-4B-Instruct-2507\", \"paged|flash_attention_2\", expected_outputs)\n+    def test_continuous_batching_diverse_models(self, model_id: str, use_cuda_graph: bool, use_compile: bool) -> None:\n+        try:\n+            self._test_continuous_batching_parity(model_id, True, \"flash_attention_2\", use_cuda_graph, use_compile)\n+        finally:\n+            flush_memory(flush_compile=use_compile)\n \n     @require_torch_accelerator\n-    @require_kernels\n-    @slow\n-    def test_continuous_batching_parity_gpt_oss_flash(self) -> None:\n-        expected_outputs = {}\n-        self._continuous_batching_parity(\"openai/gpt-oss-20b\", \"paged|flash_attention_2\", expected_outputs)\n-\n-    def test_attn_implementation(self) -> None:\n-        model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n-        manager = model.init_continuous_batching()\n-        assert \"paged|sdpa\" == manager.model.config._attn_implementation\n-\n-        model = AutoModelForCausalLM.from_pretrained(\"gpt2\", _attn_implementation=\"eager\")\n-        manager = model.init_continuous_batching()\n-        assert \"paged|eager\" == manager.model.config._attn_implementation\n+    def test_continuous_batching_fast(self) -> None:\n+        model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n+        self._test_continuous_batching_parity(model_id, False, \"sdpa\", False, False)\n \n     @require_torch_accelerator\n-    def test_streaming_request(self) -> None:\n+    def test_continuous_batching_long_generate(self) -> None:\n+        model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n+        self._test_continuous_batching_parity(model_id, True, \"flash_attention_2\", True, True, max_new_tokens=80)\n+\n+    # ---------------------------------------Streaming tests--------------------------------------- #\n+    #           Ensures the requests have the right behavior with and without streaming             #\n+    # --------------------------------------------------------------------------------------------- #\n+    def _test_streaming_or_not_request(self, with_streaming: bool, with_non_streaming: bool) -> None:\n         model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n         max_new_tokens = 3\n \n-        tokenizer = AutoTokenizer.from_pretrained(model_id)\n-        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n-\n+        model = AutoModelForCausalLM.from_pretrained(model_id)\n         manager = model.init_continuous_batching()\n         manager.logit_processor = LogitsProcessorList()\n         manager.start()\n \n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n         messages = [{\"content\": \"What is the Transformers library known for?\", \"role\": \"user\"}]\n-\n         inputs = tokenizer.apply_chat_template(\n             messages, return_tensors=\"pt\", add_generation_prompt=True, return_dict=False\n         ).to(model.device)[0]\n \n-        request_id = manager.add_request(inputs, max_new_tokens=max_new_tokens, streaming=True)\n+        # Test with non-streaming\n+        if with_non_streaming:\n+            request_id = manager.add_request(inputs, max_new_tokens=max_new_tokens, streaming=False)\n \n-        # In streaming mode, the total number of generated tokens is incremented by 1 on each iteration\n-        chunk_1 = next(manager.request_id_iter(request_id))\n-        self.assertEqual(len(chunk_1.generated_tokens), 1)\n+            # In non-streaming mode, the total number of generated tokens is equal to the max new tokens\n+            chunk = next(manager.request_id_iter(request_id))\n+            self.assertEqual(len(chunk.generated_tokens), max_new_tokens)\n \n-        chunk_2 = next(manager.request_id_iter(request_id))\n-        self.assertEqual(len(chunk_2.generated_tokens), 2)\n-\n-        chunk_3 = next(manager.request_id_iter(request_id))\n-        self.assertEqual(len(chunk_3.generated_tokens), 3)\n-\n-        manager.stop(block=True)\n-\n-    @require_torch_accelerator\n-    def test_non_streaming_request(self) -> None:\n-        model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n-        max_new_tokens = 3\n-\n-        tokenizer = AutoTokenizer.from_pretrained(model_id)\n-        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n-\n-        manager = model.init_continuous_batching()\n-        manager.logit_processor = LogitsProcessorList()\n-        manager.start()\n-\n-        messages = [{\"content\": \"What is the Transformers library known for?\", \"role\": \"user\"}]\n-\n-        inputs = tokenizer.apply_chat_template(\n-            messages, return_tensors=\"pt\", add_generation_prompt=True, return_dict=False\n-        ).to(model.device)[0]\n+        # Test with streaming\n+        if with_streaming:\n+            request_id = manager.add_request(inputs, max_new_tokens=max_new_tokens, streaming=True)\n \n-        request_id = manager.add_request(inputs, max_new_tokens=max_new_tokens, streaming=False)\n+            # In streaming mode, the total number of generated tokens is incremented by 1 on each iteration\n+            chunk_1 = next(manager.request_id_iter(request_id))\n+            self.assertEqual(len(chunk_1.generated_tokens), 1)\n \n-        chunk = next(manager.request_id_iter(request_id))\n+            chunk_2 = next(manager.request_id_iter(request_id))\n+            self.assertEqual(len(chunk_2.generated_tokens), 2)\n \n-        # In non-streaming mode, the total number of generated tokens is equal to the max new tokens\n-        self.assertEqual(len(chunk.generated_tokens), max_new_tokens)\n+            chunk_3 = next(manager.request_id_iter(request_id))\n+            self.assertEqual(len(chunk_3.generated_tokens), 3)\n \n         manager.stop(block=True)\n \n     @require_torch_accelerator\n-    def test_streaming_and_non_streaming_requests_can_alternate(self) -> None:\n-        model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n-        max_new_tokens = 3\n-\n-        tokenizer = AutoTokenizer.from_pretrained(model_id)\n-        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n-\n-        manager = model.init_continuous_batching()\n-        manager.logit_processor = LogitsProcessorList()\n-        manager.start()\n-\n-        messages = [{\"content\": \"What is the Transformers library known for?\", \"role\": \"user\"}]\n-\n-        inputs = tokenizer.apply_chat_template(\n-            messages, return_tensors=\"pt\", add_generation_prompt=True, return_dict=False\n-        ).to(model.device)[0]\n-\n-        # Non-streaming request\n-        request_id = manager.add_request(inputs, max_new_tokens=max_new_tokens, streaming=False)\n-        chunk = next(manager.request_id_iter(request_id))\n-        self.assertEqual(len(chunk.generated_tokens), max_new_tokens)\n-\n-        # Streaming request works afterward\n-        request_id = manager.add_request(inputs, max_new_tokens=max_new_tokens, streaming=True)\n-\n-        chunk_1 = next(manager.request_id_iter(request_id))\n-        self.assertEqual(len(chunk_1.generated_tokens), 1)\n-\n-        chunk_2 = next(manager.request_id_iter(request_id))\n-        self.assertEqual(len(chunk_2.generated_tokens), 2)\n+    def test_streaming_request(self) -> None:\n+        self._test_streaming_or_not_request(with_streaming=True, with_non_streaming=False)\n \n-        chunk_3 = next(manager.request_id_iter(request_id))\n-        self.assertEqual(len(chunk_3.generated_tokens), 3)\n+    @require_torch_accelerator\n+    def test_non_streaming_request(self) -> None:\n+        self._test_streaming_or_not_request(with_streaming=False, with_non_streaming=True)\n \n-        manager.stop(block=True)\n+    @require_torch_accelerator\n+    def test_streaming_and_non_streaming_requests_can_alternate(self) -> None:\n+        self._test_streaming_or_not_request(with_streaming=True, with_non_streaming=True)\n \n+    # -----------------------------------------Misc. tests----------------------------------------- #\n+    #                     Various tests that don't fit into the other categories                    #\n+    # --------------------------------------------------------------------------------------------- #\n     @require_torch_accelerator\n     def test_prefix_sharing(self) -> None:\n         model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n         max_new_tokens = 32\n \n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n-        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n+        model = AutoModelForCausalLM.from_pretrained(model_id)\n \n         generation_config = GenerationConfig(do_sample=False, block_size=32)\n         with model.continuous_batching_context_manager(generation_config=generation_config) as manager:\n@@ -530,7 +441,3 @@ def test_prefix_sharing(self) -> None:\n             (\"rocm\", (9, 4)): [785, 80532, 6733, 374, 3881, 369, 1181, 5726, 311, 1855, 323, 36635, 3460, 12934, 4128, 4119, 11, 2670, 1846, 429, 646, 6923, 1467, 11, 14683, 1467, 11, 323, 2736, 1008, 4128, 13904],\n         }).get_expectation()  # fmt: skip\n         self.assertEqual(chunk_no_reuse.generated_tokens, expected_generated_tokens)\n-\n-\n-# FIXME: the gemma test seem broken, there is a message about cuda graphs and the sdpa and flash expecteations are\n-# inverted on CUDA. On AMD they do fine."
        }
    ],
    "stats": {
        "total": 504,
        "additions": 206,
        "deletions": 298
    }
}