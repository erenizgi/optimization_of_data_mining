{
    "author": "warner-benjamin",
    "message": "Add ModernBERT to Transformers (#35158)\n\n* initial cut of modernbert for transformers\r\n\r\n* small bug fixes\r\n\r\n* fixes\r\n\r\n* Update import\r\n\r\n* Use compiled mlp->mlp_norm to match research implementation\r\n\r\n* Propagate changes in modular to modeling\r\n\r\n* Replace duplicate attn_out_dropout in favor of attention_dropout\r\n\r\ncc @warner-benjamin let me know if the two should remain separate!\r\n\r\n* Update BOS to CLS and EOS to SEP\r\n\r\nPlease confirm @warner-benjamin\r\n\r\n* Set default classifier bias to False, matching research repo\r\n\r\n* Update tie_word_embeddings description\r\n\r\n* Fix _init_weights for ForMaskedLM\r\n\r\n* Match base_model_prefix\r\n\r\n* Add compiled_head to match research repo outputs\r\n\r\n* Fix imports for ModernBertForMaskedLM\r\n\r\n* Just use \"gelu\" default outright for classifier\r\n\r\n* Fix config name typo: initalizer -> initializer\r\n\r\n* Remove some unused parameters in docstring. Still lots to edit there!\r\n\r\n* Compile the embeddings forward\r\n\r\nNot having this resulted in very slight differences - so small it wasn't even noticed for the base model, only for the large model.\r\n\r\nBut the tiny difference for large propagated at the embedding layer through the rest of the model, leading to notable differences of ~0.0084 average per value, up to 0.2343 for the worst case.\r\n\r\n* Add drafts for ForSequenceClassification/ForTokenClassification\r\n\r\n* Add initial SDPA support (not exactly equivalent to FA2 yet!)\r\n\r\nDuring testing, FA2 and SDPA still differ by about 0.0098 per value in the token embeddings. It still predicts the correct mask fills, but I'd like to get it fully 1-1 if possible.\r\n\r\n* Only use attention dropout if training\r\n\r\n* Add initial eager attention support (also not equivalent to FA2 yet!)\r\n\r\nFrustratingly, I also can't get eager to be equivalent to FA2 (or sdpa), but it does get really close, i.e. avg ~0.010 difference per value.\r\n\r\nEspecially if I use fp32 for both FA2&eager, avg ~0.0029 difference per value\r\n\r\nThe fill-mask results are good with eager.\r\n\r\n* Add initial tests, output_attentions, output_hidden_states, prune_heads\r\n\r\nTests are based on BERT, not all tests pass yet: 23 failed, 79 passed, 100 skipped\r\n\r\n* Remove kwargs from ModernBertForMaskedLM\r\n\r\nDisable sparse_prediction by default to match the normal HF, can be enabled via config\r\n\r\n* Remove/adjust/skip improper tests; warn if padding but no attn mask\r\n\r\n* Run formatting etc.\r\n\r\n* Run python utils/custom_init_isort.py\r\n\r\n* FlexAttention with unpadded sequences(matches FA2 within bf16 numerics)\r\n\r\n* Reformat init_weights based on review\r\n\r\n* self -> module in attention forwards\r\n\r\n* Remove if config.tie_word_embeddings\r\n\r\n* Reformat output projection on a different line\r\n\r\n* Remove pruning\r\n\r\n* Remove assert\r\n\r\n* Call contiguous() to simplify paths\r\n\r\n* Remove prune_qkv_linear_layer\r\n\r\n* Format code\r\n\r\n* Keep as kwargs, only use if needed\r\n\r\n* Remove unused codepaths & related config options\r\n\r\n* Remove 3d attn_mask test; fix token classification tuple output\r\n\r\n* Reorder: attention_mask above position_ids, fixes gradient checkpointing\r\n\r\n* Fix usage if no FA2 or torch v2.5+\r\n\r\n* Make torch.compile/triton optional\r\n\r\nShould we rename 'compile'? It's a bit vague\r\n\r\n* Separate pooling options into separate functions (cls, mean) - cls as default\r\n\r\n* Simplify _pad_modernbert_output, remove unused labels path\r\n\r\n* Update tied weights to remove decoder.weight, simplify decoder loading\r\n\r\n* Adaptively set config.compile based on hf_device_map/device/resize, etc.\r\n\r\n* Update ModernBertConfig docstring\r\n\r\n* Satisfy some consistency checks, add unfinished docs\r\n\r\n* Only set compile to False if there's more than 1 device\r\n\r\n* Add docstrings for public ModernBert classes\r\n\r\n* Dont replace docstring returns - ends up being duplicate\r\n\r\n* Fix mistake in toctree\r\n\r\n* Reformat toctree\r\n\r\n* Patched FlexAttention, SDPA, Eager with Local Attention\r\n\r\n* Implement FA2 -> SDPA -> Eager attn_impl defaulting, crucial\r\n\r\nboth to match the original performance, and to get the highest inference speed without requiring users to manually pick FA2\r\n\r\n* Patch test edge case with Idefics3 not working with 'attn_implementation=\"sdpa\"'\r\n\r\n* Repad all_hidden_states as well\r\n\r\n* rename config.compile to reference_compile\r\n\r\n* disable flex_attention since it crashes\r\n\r\n* Update modernbert.md\r\n\r\n* Using dtype min to mask in eager\r\n\r\n* Fully remove flex attention for now\r\n\r\nIt's only compatible with the nightly torch 2.6, so we'll leave it be for now. It's also slower than eager/sdpa.\r\n\r\nAlso, update compile -> reference_compile in one more case\r\n\r\n* Call contiguous to allow for .view()\r\n\r\n* Copyright 2020 -> 2024\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\r\n\r\n* Update/simplify __init__ structure\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\r\n\r\n* Remove \"... if dropout_prob > 0 else identity\"\r\n\r\nAs dropout with 0.0 should be efficient like identity\r\n\r\n* re-use existing pad/unpad functions instead of creating new ones\r\n\r\n* remove flexattention method\r\n\r\n* Compute attention_mask and local_attention_mask once in modeling\r\n\r\n* Simplify sequence classification prediction heads, only CLS now\r\n\r\nUsers can make custom heads if they feel like it\r\n\r\nAlso removes the unnecessary pool parameter\r\n\r\n* Simplify module.training in eager attn\r\n\r\n* Also export ModernBertPreTrainedModel\r\n\r\n* Update the documentation with links to finetuning scripts\r\n\r\n* Explain local_attention_mask parameter in docstring\r\n\r\n* Simplify _autoset_attn_implementation, rely on super()\r\n\r\n* Keep \"in\" to initialize Prediction head\r\n\r\nDoublechecked with Benjamin that it's correct/what we used for pretraining\r\n\r\n* add back mean pooling\r\n\r\n* Use the pooling head in TokenClassification\r\n\r\n* update copyright\r\n\r\n* Reset config._attn_implementation_internal on failure\r\n\r\n* Allow optional attention_mask in ForMaskedLM head\r\n\r\n* fix failing run_slow tests\r\n\r\n* Add links to the paper\r\n\r\n* Remove unpad_no_grad, always pad/unpad without gradients\r\n\r\n* local_attention_mask -> sliding_window_mask\r\n\r\n* Revert \"Use the pooling head in TokenClassification\"\r\n\r\nThis reverts commit 99c38badd1dbce01d7aef41095fbf2f5cce87279.\r\n\r\nThere was no real motivation, no info on whether having this bigger head does anything useful.\r\n\r\n* Simplify pooling, 2 options via if-else\r\n\r\n---------\r\n\r\nCo-authored-by: Tom Aarsen <37621491+tomaarsen@users.noreply.github.com>\r\nCo-authored-by: Tom Aarsen <Cubiegamedev@gmail.com>\r\nCo-authored-by: Said Taghadouini <taghadouinisaid@gmail.com>\r\nCo-authored-by: Benjamin Clavi√© <ben@clavie.eu>\r\nCo-authored-by: Antoine Chaffin <ant54600@hotmail.fr>\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "667ed5635e6fd7e2df4fc23012746b1c0cbb7575",
    "files": [
        {
            "sha": "8138dd41d80c1289ca71496f3b1527f99a3c40fa",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=667ed5635e6fd7e2df4fc23012746b1c0cbb7575",
            "patch": "@@ -498,6 +498,8 @@\n         title: mLUKE\n       - local: model_doc/mobilebert\n         title: MobileBERT\n+      - local: model_doc/modernbert\n+        title: ModernBert\n       - local: model_doc/mpnet\n         title: MPNet\n       - local: model_doc/mpt"
        },
        {
            "sha": "967049d89cbe128e27fc739c0945081e8da468a7",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=667ed5635e6fd7e2df4fc23012746b1c0cbb7575",
            "patch": "@@ -232,6 +232,7 @@ Flax), PyTorch, and/or TensorFlow.\n |                  [MobileNetV2](model_doc/mobilenet_v2)                   |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                     [MobileViT](model_doc/mobilevit)                     |       ‚úÖ        |         ‚úÖ         |      ‚ùå      |\n |                   [MobileViTV2](model_doc/mobilevitv2)                   |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n+|                    [ModernBERT](model_doc/modernbert)                    |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                         [Moshi](model_doc/moshi)                         |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                         [MPNet](model_doc/mpnet)                         |       ‚úÖ        |         ‚úÖ         |      ‚ùå      |\n |                           [MPT](model_doc/mpt)                           |       ‚úÖ        |         ‚ùå         |      ‚ùå      |"
        },
        {
            "sha": "ab09f38ff12154e212765bbde054c2ddb9ad6dd7",
            "filename": "docs/source/en/model_doc/modernbert.md",
            "status": "added",
            "additions": 91,
            "deletions": 0,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert.md?ref=667ed5635e6fd7e2df4fc23012746b1c0cbb7575",
            "patch": "@@ -0,0 +1,91 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# ModernBert\n+\n+<div class=\"flex flex-wrap space-x-1\">\n+<a href=\"https://huggingface.co/models?filter=modernbert\">\n+<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-modernbert-blueviolet\">\n+</a>\n+<a href=\"https://arxiv.org/abs/2412.13663\">\n+<img alt=\"Paper page\" src=\"https://img.shields.io/badge/Paper%20page-2412.13663-green\">\n+</a>\n+</div>\n+\n+## Overview\n+\n+The ModernBert model was proposed in [Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference](https://arxiv.org/abs/2412.13663) by Benjamin Warner, Antoine Chaffin, Benjamin Clavi√©, Orion Weller, Oskar Hallstr√∂m, Said Taghadouini, Alexis Galalgher, Raja Bisas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Grifin Adams, Jeremy Howard and Iacopo Poli.\n+\n+It is a refresh of the traditional encoder architecture, as used in previous models such as [BERT](https://huggingface.co/docs/transformers/en/model_doc/bert) and [RoBERTa](https://huggingface.co/docs/transformers/en/model_doc/roberta). \n+\n+It builds on BERT and implements many modern architectural improvements which have been developed since its original release, such as:\n+- [Rotary Positional Embeddings](https://huggingface.co/blog/designing-positional-encoding) to support sequences of up to 8192 tokens.\n+- [Unpadding](https://arxiv.org/abs/2208.08124) to ensure no compute is wasted on padding tokens, speeding up processing time for batches with mixed-length sequences.\n+- [GeGLU](https://arxiv.org/abs/2002.05202) Replacing the original MLP layers with GeGLU layers, shown to improve performance.\n+- [Alternating Attention](https://arxiv.org/abs/2004.05150v2) where most attention layers employ a sliding window of 128 tokens, with Global Attention only used every 3 layers.\n+- [Flash Attention](https://github.com/Dao-AILab/flash-attention) to speed up processing.\n+- A model designed following recent [The Case for Co-Designing Model Architectures with Hardware](https://arxiv.org/abs/2401.14489), ensuring maximum efficiency across inference GPUs.\n+- Modern training data scales (2 trillion tokens) and mixtures (including code ande math data)\n+\n+The abstract from the paper is the following:\n+\n+*Encoder-only transformer models such as BERT offer a great performance-size tradeoff for retrieval and classification tasks with respect to larger decoder-only models. Despite being the workhorse of numerous production pipelines, there have been limited Pareto improvements to BERT since its release. In this paper, we introduce ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders. Trained on 2 trillion tokens with a native 8192 sequence length, ModernBERT models exhibit state-of-the-art results on a large pool of evaluations encompassing diverse classification tasks and both single and multi-vector retrieval on different domains (including code). In addition to strong downstream performance, ModernBERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs.*\n+\n+The original code can be found [here](https://github.com/answerdotai/modernbert).\n+\n+## Resources\n+\n+A list of official Hugging Face and community (indicated by üåé) resources to help you get started with ModernBert.\n+\n+<PipelineTag pipeline=\"sentence-similarity\"/>\n+\n+- A script on how to [finetune for text similarity or information retrieval with Sentence Transformers](https://github.com/AnswerDotAI/ModernBERT/blob/main/examples/train_st.py). üåé\n+- A script on how to [finetune for information retrieval with PyLate](https://github.com/AnswerDotAI/ModernBERT/blob/main/examples/train_pylate.py). üåé\n+\n+<PipelineTag pipeline=\"fill-mask\"/>\n+\n+- [Masked language modeling task guide](../tasks/masked_language_modeling)\n+\n+\n+## ModernBertConfig\n+\n+[[autodoc]] ModernBertConfig\n+\n+<frameworkcontent>\n+<pt>\n+\n+## ModernBertModel\n+\n+[[autodoc]] ModernBertModel\n+    - forward\n+\n+## ModernBertForMaskedLM\n+\n+[[autodoc]] ModernBertForMaskedLM\n+    - forward\n+\n+## ModernBertForSequenceClassification\n+\n+[[autodoc]] ModernBertForSequenceClassification\n+    - forward\n+\n+## ModernBertForTokenClassification\n+\n+[[autodoc]] ModernBertForTokenClassification\n+    - forward\n+\n+</pt>\n+</frameworkcontent>"
        },
        {
            "sha": "930f41b6fefba7bd4a2538f9e59added3da8e16c",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=667ed5635e6fd7e2df4fc23012746b1c0cbb7575",
            "patch": "@@ -74,6 +74,7 @@ FlashAttention-2 is currently supported for the following architectures:\n * [MBart](https://huggingface.co/docs/transformers/model_doc/mbart#transformers.MBartModel)\n * [Mistral](https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel)\n * [Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral#transformers.MixtralModel)\n+* [ModernBert](https://huggingface.co/docs/transformers/model_doc/modernbert#transformers.ModernBert)\n * [Moshi](https://huggingface.co/docs/transformers/model_doc/moshi#transformers.MoshiModel)\n * [Musicgen](https://huggingface.co/docs/transformers/model_doc/musicgen#transformers.MusicgenModel)\n * [MusicGen Melody](https://huggingface.co/docs/transformers/model_doc/musicgen_melody#transformers.MusicgenMelodyModel)\n@@ -265,6 +266,7 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [Mistral](https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel)\n * [Mllama](https://huggingface.co/docs/transformers/model_doc/mllama#transformers.MllamaForConditionalGeneration)\n * [Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral#transformers.MixtralModel)\n+* [ModernBert](https://huggingface.co/docs/transformers/model_doc/modernbert#transformers.ModernBert)\n * [Moshi](https://huggingface.co/docs/transformers/model_doc/moshi#transformers.MoshiModel)\n * [Musicgen](https://huggingface.co/docs/transformers/model_doc/musicgen#transformers.MusicgenModel)\n * [MusicGen Melody](https://huggingface.co/docs/transformers/model_doc/musicgen_melody#transformers.MusicgenMelodyModel)"
        },
        {
            "sha": "600d3d217fa8a9db510f8baa4412dd7253a46bac",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=667ed5635e6fd7e2df4fc23012746b1c0cbb7575",
            "patch": "@@ -606,6 +606,7 @@\n     \"models.mobilenet_v2\": [\"MobileNetV2Config\"],\n     \"models.mobilevit\": [\"MobileViTConfig\"],\n     \"models.mobilevitv2\": [\"MobileViTV2Config\"],\n+    \"models.modernbert\": [\"ModernBertConfig\"],\n     \"models.moshi\": [\n         \"MoshiConfig\",\n         \"MoshiDepthConfig\",\n@@ -2869,6 +2870,15 @@\n             \"MobileViTV2PreTrainedModel\",\n         ]\n     )\n+    _import_structure[\"models.modernbert\"].extend(\n+        [\n+            \"ModernBertForMaskedLM\",\n+            \"ModernBertForSequenceClassification\",\n+            \"ModernBertForTokenClassification\",\n+            \"ModernBertModel\",\n+            \"ModernBertPreTrainedModel\",\n+        ]\n+    )\n     _import_structure[\"models.moshi\"].extend(\n         [\n             \"MoshiForCausalLM\",\n@@ -5565,6 +5575,7 @@\n     from .models.mobilevitv2 import (\n         MobileViTV2Config,\n     )\n+    from .models.modernbert import ModernBertConfig\n     from .models.moshi import (\n         MoshiConfig,\n         MoshiDepthConfig,\n@@ -7556,6 +7567,13 @@\n             MobileViTV2Model,\n             MobileViTV2PreTrainedModel,\n         )\n+        from .models.modernbert import (\n+            ModernBertForMaskedLM,\n+            ModernBertForSequenceClassification,\n+            ModernBertForTokenClassification,\n+            ModernBertModel,\n+            ModernBertPreTrainedModel,\n+        )\n         from .models.moshi import (\n             MoshiForCausalLM,\n             MoshiForConditionalGeneration,"
        },
        {
            "sha": "7f6aaaa44264ca647b43fb021f88419b9297abb8",
            "filename": "src/transformers/loss/loss_utils.py",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src%2Ftransformers%2Floss%2Floss_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src%2Ftransformers%2Floss%2Floss_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_utils.py?ref=667ed5635e6fd7e2df4fc23012746b1c0cbb7575",
            "patch": "@@ -47,6 +47,22 @@ def ForCausalLMLoss(\n     return loss\n \n \n+def ForMaskedLMLoss(\n+    logits, labels, vocab_size: int, num_items_in_batch: int = None, ignore_index: int = -100, **kwargs\n+):\n+    # Upcast to float if we need to compute the loss to avoid potential precision issues\n+    logits = logits.float()\n+\n+    # Flatten the tokens\n+    logits = logits.view(-1, vocab_size)\n+    labels = labels.view(-1)\n+    # Enable model parallelism\n+\n+    labels = labels.to(logits.device)\n+    loss = fixed_cross_entropy(logits, labels, num_items_in_batch, ignore_index, **kwargs)\n+    return loss\n+\n+\n def ForSequenceClassificationLoss(labels, pooled_logits, config, **kwargs):\n     num_labels = config.num_labels\n     if config.problem_type is None:\n@@ -101,6 +117,7 @@ def ForTokenClassification(logits, labels, config, **kwargs):\n \n LOSS_MAPPING = {\n     \"ForCausalLM\": ForCausalLMLoss,\n+    \"ForMaskedLM\": ForMaskedLMLoss,\n     \"ForQuestionAnswering\": ForQuestionAnsweringLoss,\n     \"ForSequenceClassification\": ForSequenceClassificationLoss,\n     \"ForTokenClassification\": ForTokenClassification,"
        },
        {
            "sha": "7fcaddde704cf7eb707104bc865734e0f5f30d9e",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=667ed5635e6fd7e2df4fc23012746b1c0cbb7575",
            "patch": "@@ -167,6 +167,7 @@\n     mobilenet_v2,\n     mobilevit,\n     mobilevitv2,\n+    modernbert,\n     moshi,\n     mpnet,\n     mpt,"
        },
        {
            "sha": "69ce8efa10c76c671693756cc0c58645029e6461",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=667ed5635e6fd7e2df4fc23012746b1c0cbb7575",
            "patch": "@@ -187,6 +187,7 @@\n         (\"mobilenet_v2\", \"MobileNetV2Config\"),\n         (\"mobilevit\", \"MobileViTConfig\"),\n         (\"mobilevitv2\", \"MobileViTV2Config\"),\n+        (\"modernbert\", \"ModernBertConfig\"),\n         (\"moshi\", \"MoshiConfig\"),\n         (\"mpnet\", \"MPNetConfig\"),\n         (\"mpt\", \"MptConfig\"),\n@@ -510,6 +511,7 @@\n         (\"mobilenet_v2\", \"MobileNetV2\"),\n         (\"mobilevit\", \"MobileViT\"),\n         (\"mobilevitv2\", \"MobileViTV2\"),\n+        (\"modernbert\", \"ModernBERT\"),\n         (\"moshi\", \"Moshi\"),\n         (\"mpnet\", \"MPNet\"),\n         (\"mpt\", \"MPT\"),"
        },
        {
            "sha": "e8a2dece4324763938d7e5b7032b35529fd8fd86",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=667ed5635e6fd7e2df4fc23012746b1c0cbb7575",
            "patch": "@@ -176,6 +176,7 @@\n         (\"mobilenet_v2\", \"MobileNetV2Model\"),\n         (\"mobilevit\", \"MobileViTModel\"),\n         (\"mobilevitv2\", \"MobileViTV2Model\"),\n+        (\"modernbert\", \"ModernBertModel\"),\n         (\"moshi\", \"MoshiModel\"),\n         (\"mpnet\", \"MPNetModel\"),\n         (\"mpt\", \"MptModel\"),\n@@ -838,6 +839,7 @@\n         (\"mega\", \"MegaForMaskedLM\"),\n         (\"megatron-bert\", \"MegatronBertForMaskedLM\"),\n         (\"mobilebert\", \"MobileBertForMaskedLM\"),\n+        (\"modernbert\", \"ModernBertForMaskedLM\"),\n         (\"mpnet\", \"MPNetForMaskedLM\"),\n         (\"mra\", \"MraForMaskedLM\"),\n         (\"mvp\", \"MvpForConditionalGeneration\"),\n@@ -992,6 +994,7 @@\n         (\"mistral\", \"MistralForSequenceClassification\"),\n         (\"mixtral\", \"MixtralForSequenceClassification\"),\n         (\"mobilebert\", \"MobileBertForSequenceClassification\"),\n+        (\"modernbert\", \"ModernBertForSequenceClassification\"),\n         (\"mpnet\", \"MPNetForSequenceClassification\"),\n         (\"mpt\", \"MptForSequenceClassification\"),\n         (\"mra\", \"MraForSequenceClassification\"),\n@@ -1178,6 +1181,7 @@\n         (\"mistral\", \"MistralForTokenClassification\"),\n         (\"mixtral\", \"MixtralForTokenClassification\"),\n         (\"mobilebert\", \"MobileBertForTokenClassification\"),\n+        (\"modernbert\", \"ModernBertForTokenClassification\"),\n         (\"mpnet\", \"MPNetForTokenClassification\"),\n         (\"mpt\", \"MptForTokenClassification\"),\n         (\"mra\", \"MraForTokenClassification\"),"
        },
        {
            "sha": "350c230f142c1518834795bf6b665eb508f46141",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=667ed5635e6fd7e2df4fc23012746b1c0cbb7575",
            "patch": "@@ -313,6 +313,7 @@\n             (\"mllama\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n             (\"mluke\", (\"MLukeTokenizer\" if is_sentencepiece_available() else None, None)),\n             (\"mobilebert\", (\"MobileBertTokenizer\", \"MobileBertTokenizerFast\" if is_tokenizers_available() else None)),\n+            (\"modernbert\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n             (\"moshi\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n             (\"mpnet\", (\"MPNetTokenizer\", \"MPNetTokenizerFast\" if is_tokenizers_available() else None)),\n             (\"mpt\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),"
        },
        {
            "sha": "18317742981909460973138da806584ddfc4a390",
            "filename": "src/transformers/models/modernbert/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src%2Ftransformers%2Fmodels%2Fmodernbert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src%2Ftransformers%2Fmodels%2Fmodernbert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2F__init__.py?ref=667ed5635e6fd7e2df4fc23012746b1c0cbb7575",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_modernbert import *\n+    from .modeling_modernbert import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "13e9edf067efc4c27e58daca94114c634ed0da37",
            "filename": "src/transformers/models/modernbert/configuration_modernbert.py",
            "status": "added",
            "additions": 213,
            "deletions": 0,
            "changes": 213,
            "blob_url": "https://github.com/huggingface/transformers/blob/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fconfiguration_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fconfiguration_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fconfiguration_modernbert.py?ref=667ed5635e6fd7e2df4fc23012746b1c0cbb7575",
            "patch": "@@ -0,0 +1,213 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/modernbert/modular_modernbert.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_modernbert.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# Copyright 2024 Answer.AI, LightOn, and contributors, and the HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Literal\n+\n+from ...configuration_utils import PretrainedConfig\n+\n+\n+class ModernBertConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`ModernBertModel`]. It is used to instantiate an ModernBert\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the ModernBERT-base.\n+    e.g. [answerdotai/ModernBERT-base](https://huggingface.co/answerdotai/ModernBERT-base)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 50368):\n+            Vocabulary size of the ModernBert model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`ModernBertModel`]\n+        hidden_size (`int`, *optional*, defaults to 768):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 1152):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 22):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 12):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        hidden_activation (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the decoder. Will default to `\"gelu\"`\n+            if not specified.\n+        max_position_embeddings (`int`, *optional*, defaults to 8192):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        initializer_cutoff_factor (`float`, *optional*, defaults to 2.0):\n+            The cutoff factor for the truncated_normal_initializer for initializing all weight matrices.\n+        norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the rms normalization layers.\n+        norm_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use bias in the normalization layers.\n+        pad_token_id (`int`, *optional*, defaults to 50283):\n+            Padding token id.\n+        eos_token_id (`int`, *optional*, defaults to 50282):\n+            End of stream token id.\n+        bos_token_id (`int`, *optional*, defaults to 50281):\n+            Beginning of stream token id.\n+        cls_token_id (`int`, *optional*, defaults to 50281):\n+            Classification token id.\n+        sep_token_id (`int`, *optional*, defaults to 50282):\n+            Separation token id.\n+        global_rope_theta (`float`, *optional*, defaults to 160000.0):\n+            The base period of the global RoPE embeddings.\n+        attention_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        global_attn_every_n_layers (`int`, *optional*, defaults to 3):\n+            The number of layers between global attention layers.\n+        local_attention (`int`, *optional*, defaults to 128):\n+            The window size for local attention.\n+        local_rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the local RoPE embeddings.\n+        embedding_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the embeddings.\n+        mlp_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use bias in the MLP layers.\n+        mlp_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the MLP layers.\n+        decoder_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use bias in the decoder layers.\n+        classifier_pooling (`str`, *optional*, defaults to `\"cls\"`):\n+            The pooling method for the classifier. Should be either `\"cls\"` or `\"mean\"`. In local attention layers, the\n+            CLS token doesn't attend to all tokens on long sequences.\n+        classifier_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the classifier.\n+        classifier_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use bias in the classifier.\n+        classifier_activation (`str`, *optional*, defaults to `\"gelu\"`):\n+            The activation function for the classifier.\n+        deterministic_flash_attn (`bool`, *optional*, defaults to `False`):\n+            Whether to use deterministic flash attention. If `False`, inference will be faster but not deterministic.\n+        sparse_prediction (`bool`, *optional*, defaults to `False`):\n+            Whether to use sparse prediction for the masked language model instead of returning the full dense logits.\n+        sparse_pred_ignore_index (`int`, *optional*, defaults to -100):\n+            The index to ignore for the sparse prediction.\n+        reference_compile (`bool`, *optional*):\n+            Whether to compile the layers of the model which were compiled during pretraining. If `None`, then parts of\n+            the model will be compiled if 1) `triton` is installed, 2) the model is not on MPS, 3) the model is not\n+            shared between devices, and 4) the model is not resized after initialization. If `True`, then the model may\n+            be faster in some scenarios.\n+\n+    Examples:\n+\n+    ```python\n+    >>> from transformers import ModernBertModel, ModernBertConfig\n+\n+    >>> # Initializing a ModernBert style configuration\n+    >>> configuration = ModernBertConfig()\n+\n+    >>> # Initializing a model from the modernbert-base style configuration\n+    >>> model = ModernBertModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"modernbert\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    def __init__(\n+        self,\n+        vocab_size=50368,\n+        hidden_size=768,\n+        intermediate_size=1152,\n+        num_hidden_layers=22,\n+        num_attention_heads=12,\n+        hidden_activation=\"gelu\",\n+        max_position_embeddings=8192,\n+        initializer_range=0.02,\n+        initializer_cutoff_factor=2.0,\n+        norm_eps=1e-5,\n+        norm_bias=False,\n+        pad_token_id=50283,\n+        eos_token_id=50282,\n+        bos_token_id=50281,\n+        cls_token_id=50281,\n+        sep_token_id=50282,\n+        global_rope_theta=160000.0,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        global_attn_every_n_layers=3,\n+        local_attention=128,\n+        local_rope_theta=10000.0,\n+        embedding_dropout=0.0,\n+        mlp_bias=False,\n+        mlp_dropout=0.0,\n+        decoder_bias=True,\n+        classifier_pooling: Literal[\"cls\", \"mean\"] = \"cls\",\n+        classifier_dropout=0.0,\n+        classifier_bias=False,\n+        classifier_activation=\"gelu\",\n+        deterministic_flash_attn=False,\n+        sparse_prediction=False,\n+        sparse_pred_ignore_index=-100,\n+        reference_compile=None,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            cls_token_id=cls_token_id,\n+            sep_token_id=sep_token_id,\n+            **kwargs,\n+        )\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.initializer_range = initializer_range\n+        self.initializer_cutoff_factor = initializer_cutoff_factor\n+        self.norm_eps = norm_eps\n+        self.norm_bias = norm_bias\n+        self.global_rope_theta = global_rope_theta\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.hidden_activation = hidden_activation\n+        self.global_attn_every_n_layers = global_attn_every_n_layers\n+        self.local_attention = local_attention\n+        self.local_rope_theta = local_rope_theta\n+        self.embedding_dropout = embedding_dropout\n+        self.mlp_bias = mlp_bias\n+        self.mlp_dropout = mlp_dropout\n+        self.decoder_bias = decoder_bias\n+        self.classifier_pooling = classifier_pooling\n+        self.classifier_dropout = classifier_dropout\n+        self.classifier_bias = classifier_bias\n+        self.classifier_activation = classifier_activation\n+        self.deterministic_flash_attn = deterministic_flash_attn\n+        self.sparse_prediction = sparse_prediction\n+        self.sparse_pred_ignore_index = sparse_pred_ignore_index\n+        self.reference_compile = reference_compile\n+\n+        if self.classifier_pooling not in [\"cls\", \"mean\"]:\n+            raise ValueError(\n+                f'Invalid value for `classifier_pooling`, should be either \"cls\" or \"mean\", but is {self.classifier_pooling}.'\n+            )\n+\n+\n+__all__ = [\"ModernBertConfig\"]"
        },
        {
            "sha": "db8d98893f96fe2eb5b6b41e5ab92f7571814500",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "added",
            "additions": 1322,
            "deletions": 0,
            "changes": 1322,
            "blob_url": "https://github.com/huggingface/transformers/blob/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=667ed5635e6fd7e2df4fc23012746b1c0cbb7575",
            "patch": "@@ -0,0 +1,1322 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/modernbert/modular_modernbert.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_modernbert.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# Copyright 2024 Answer.AI, LightOn, and contributors, and the HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from typing import Dict, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import nn\n+from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+\n+from ...activations import ACT2FN\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_outputs import BaseModelOutput, MaskedLMOutput, SequenceClassifierOutput, TokenClassifierOutput\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    add_code_sample_docstrings,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    is_flash_attn_2_available,\n+    logging,\n+)\n+from ...utils.import_utils import is_triton_available\n+from .configuration_modernbert import ModernBertConfig\n+\n+\n+if is_flash_attn_2_available():\n+    from flash_attn.flash_attn_interface import flash_attn_varlen_qkvpacked_func\n+    from flash_attn.layers.rotary import RotaryEmbedding\n+    from flash_attn.ops.triton.rotary import apply_rotary\n+else:\n+    RotaryEmbedding = object\n+\n+logger = logging.get_logger(__name__)\n+\n+_CHECKPOINT_FOR_DOC = \"answerdotai/ModernBERT-base\"\n+_CONFIG_FOR_DOC = \"ModernBertConfig\"\n+\n+\n+class ApplyRotaryEmbUnpad(torch.autograd.Function):\n+    @staticmethod\n+    def forward(\n+        ctx,\n+        qkv,\n+        cos,\n+        sin,\n+        cu_seqlens: Optional[torch.Tensor] = None,\n+        max_seqlen: Optional[int] = None,\n+    ):\n+        # (total_nnz, 3, nheads, headdim)\n+        qkv = qkv.contiguous()\n+        total_nnz, _three, _nheads, headdim = qkv.shape\n+        # We need qkv to be contiguous so that when we reshape to combine (3, nheads) dimensions,\n+        # we get the same tensor\n+        # qk = rearrange(qkv[:, :2], \"b_s t h d -> b_s (t h) d\")\n+        qk = qkv[:, :2].view(total_nnz, -1, headdim)\n+        apply_rotary(\n+            qk,\n+            cos,\n+            sin,\n+            seqlen_offsets=0,\n+            cu_seqlens=cu_seqlens,\n+            max_seqlen=max_seqlen,\n+            interleaved=False,\n+            inplace=True,\n+        )\n+\n+        ctx.save_for_backward(cos, sin, cu_seqlens)\n+        ctx.max_seqlen = max_seqlen\n+        return qkv\n+\n+    @staticmethod\n+    def backward(ctx, do):\n+        cos, sin, cu_seqlens = ctx.saved_tensors\n+        do = do.contiguous()\n+        total_nnz, _three, _nheads, headdim = do.shape\n+        # We need dqkv to be contiguous so that when we reshape to combine (3, nheads) dimensions,\n+        # we get the same tensor\n+        dqk = do[:, :2].view(total_nnz, -1, headdim)\n+        apply_rotary(\n+            dqk,\n+            cos,\n+            sin,\n+            seqlen_offsets=0,\n+            cu_seqlens=cu_seqlens,\n+            max_seqlen=ctx.max_seqlen,\n+            interleaved=False,\n+            inplace=True,\n+            conjugate=True,\n+        )\n+\n+        return do, None, None, None, None, None, None\n+\n+\n+def apply_rotary_unpadded(\n+    qkv,\n+    cos,\n+    sin,\n+    cu_seqlens: Optional[torch.Tensor] = None,\n+    max_seqlen: Optional[int] = None,\n+):\n+    \"\"\"\n+    Arguments:\n+        qkv: (total_nnz, 3, nheads, headdim) - input tensor for packed QKV.\n+        cos, sin: (seqlen_rotary, rotary_dim / 2)\n+        interleaved: if True, rotate pairs of even and odd dimensions (GPT-J style) instead\n+            of 1st half and 2nd half (GPT-NeoX style).\n+        inplace: if True, apply rotary embedding in-place.\n+        seqlen_offsets: (batch_size,) or int. Each sequence in x is shifted by this amount.\n+            Most commonly used in inference when we have KV cache.\n+        cu_seqlens: (batch + 1,) or None\n+        max_seqlen: int\n+    Return:\n+        out: (total_nnz, dim)\n+    rotary_dim must be <= headdim\n+    Apply rotary embedding to the first rotary_dim of x.\n+    \"\"\"\n+    return ApplyRotaryEmbUnpad.apply(qkv, cos, sin, cu_seqlens, max_seqlen)\n+\n+\n+class ModernBertUnpaddedRotaryEmbedding(RotaryEmbedding):\n+    \"\"\"\n+    The rotary position embeddings applied directly to unpadded sequences.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        dim: int,\n+        base: float = 10000.0,\n+        max_seqlen: Optional[int] = None,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        \"\"\"\n+        max_seqlen: if max_seqlen, device, and dtype are provided, we precompute the cos_sin_cache\n+            up to max_seqlen. If the max_seqlen, device, or dtype during training/inference differ,\n+            the cos_sin_cache wll be recomputed during the forward pass.\n+        \"\"\"\n+        super().__init__(dim=dim, base=base, pos_idx_in_fp32=True, device=device, interleaved=False)\n+        self.max_seqlen = max_seqlen\n+\n+        if max_seqlen is not None and device is not None and dtype is not None:\n+            self._update_cos_sin_cache(max_seqlen, device=device, dtype=dtype)\n+\n+    def forward(\n+        self,\n+        qkv: torch.Tensor,\n+        cu_seqlens: torch.Tensor,\n+        max_seqlen: Optional[int] = None,\n+    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n+        \"\"\"\n+        Apply rotary embedding *inplace* to qkv.\n+        qkv: (total_nnz, 3, nheads, headdim)\n+        cu_seqlens: (batch + 1,) cumulative sequence lengths\n+        max_seqlen: int max seq length in the batch\n+        \"\"\"\n+        if max_seqlen is not None:\n+            self._update_cos_sin_cache(max_seqlen, device=qkv.device, dtype=qkv.dtype)\n+\n+        qkv = apply_rotary_unpadded(\n+            qkv,\n+            self._cos_cached,\n+            self._sin_cached,\n+            cu_seqlens=cu_seqlens,\n+            max_seqlen=max_seqlen,\n+        )\n+\n+        return qkv\n+\n+    def extra_repr(self) -> str:\n+        return f\"dim={self.dim}, base={self.base}, scale_base={self.scale_base}\"\n+\n+\n+class ModernBertEmbeddings(nn.Module):\n+    \"\"\"\n+    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\n+    \"\"\"\n+\n+    def __init__(self, config: ModernBertConfig):\n+        super().__init__()\n+        self.config = config\n+        self.tok_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n+        self.norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)\n+        self.drop = nn.Dropout(config.embedding_dropout)\n+\n+    @torch.compile(dynamic=True)\n+    def compiled_embeddings(self, input_ids: torch.LongTensor) -> torch.Tensor:\n+        return self.drop(self.norm(self.tok_embeddings(input_ids)))\n+\n+    def forward(self, input_ids: torch.LongTensor, position_ids: Optional[torch.LongTensor] = None) -> torch.Tensor:\n+        hidden_states = (\n+            self.compiled_embeddings(input_ids)\n+            if self.config.reference_compile\n+            else self.drop(self.norm(self.tok_embeddings(input_ids)))\n+        )\n+        return hidden_states\n+\n+\n+class ModernBertMLP(nn.Module):\n+    \"\"\"Applies the GLU at the end of each ModernBERT layer.\n+\n+    Compared to the default BERT architecture, this block replaces :class:`~transformers.model.bert.modeling_bert.BertIntermediate`\n+    and :class:`~transformers.model.bert.modeling_bert.SelfOutput` with a single module that has similar functionality.\n+    \"\"\"\n+\n+    def __init__(self, config: ModernBertConfig):\n+        super().__init__()\n+        self.config = config\n+        self.Wi = nn.Linear(config.hidden_size, int(config.intermediate_size) * 2, bias=config.mlp_bias)\n+        self.act = ACT2FN[config.hidden_activation]\n+        self.drop = nn.Dropout(config.mlp_dropout)\n+        self.Wo = nn.Linear(config.intermediate_size, config.hidden_size, bias=config.mlp_bias)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        input, gate = self.Wi(hidden_states).chunk(2, dim=-1)\n+        return self.Wo(self.drop(self.act(input) * gate))\n+\n+\n+class ModernBertRotaryEmbedding(nn.Module):\n+    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n+        super().__init__()\n+\n+        self.dim = dim\n+        self.max_position_embeddings = max_position_embeddings\n+        self.base = base\n+        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))\n+        self.register_buffer(\"inv_freq\", tensor=inv_freq, persistent=False)\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids, seq_len=None):\n+        # x: [bs, num_attention_heads, seq_len, head_size]\n+        self.inv_freq.to(x.device)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 since bfloat16 loses precision on long contexts\n+        # See https://github.com/huggingface/transformers/pull/29285\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def eager_attention_forward(\n+    module: \"ModernBertAttention\",\n+    qkv: torch.Tensor,\n+    attention_mask: torch.Tensor,\n+    sliding_window_mask: torch.Tensor,\n+    position_ids: Optional[torch.LongTensor],\n+    local_attention: Tuple[int, int],\n+    bs: int,\n+    dim: int,\n+    output_attentions: Optional[bool] = False,\n+    **_kwargs,\n+) -> Tuple[torch.Tensor, torch.Tensor] | Tuple[torch.Tensor]:\n+    # qkv: [batch_size, seqlen, 3, nheads, headdim]\n+    cos, sin = module.rotary_emb(qkv, position_ids=position_ids)\n+    query, key, value = qkv.transpose(3, 1).unbind(dim=2)\n+    # query, key, value: [batch_size, heads, seq_len, head_dim]\n+    query, key = apply_rotary_pos_emb(query, key, cos, sin)\n+\n+    scale = module.head_dim**-0.5\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scale\n+\n+    if local_attention != (-1, -1):\n+        attention_mask = sliding_window_mask\n+\n+    attn_weights = attn_weights + attention_mask\n+\n+    # upcast attention to fp32\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=module.attention_dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+    attn_output = attn_output.view(bs, -1, dim)\n+    if output_attentions:\n+        return (attn_output, attn_weights)\n+    return (attn_output,)\n+\n+\n+def flash_attention_forward(\n+    module: \"ModernBertAttention\",\n+    qkv: torch.Tensor,\n+    rotary_emb: ModernBertUnpaddedRotaryEmbedding,\n+    cu_seqlens: torch.Tensor,\n+    max_seqlen: int,\n+    local_attention: Tuple[int, int],\n+    bs: int,\n+    dim: int,\n+    target_dtype: torch.dtype = torch.bfloat16,\n+    **_kwargs,\n+) -> Tuple[torch.Tensor]:\n+    # (total_seqlen, 3, nheads, headdim)\n+    qkv = rotary_emb(qkv, cu_seqlens=cu_seqlens, max_seqlen=max_seqlen)\n+\n+    convert_dtype = qkv.dtype not in (torch.float16, torch.bfloat16)\n+    if convert_dtype:\n+        # FA2 implementation only supports fp16 and bf16. If FA2 is supported,\n+        # bfloat16 must be supported as of FA2 2.5.7. (Turing GPUs not supported)\n+        orig_dtype = qkv.dtype\n+        qkv = qkv.to(target_dtype)\n+\n+        attn = flash_attn_varlen_qkvpacked_func(\n+            qkv,\n+            cu_seqlens=cu_seqlens,\n+            max_seqlen=max_seqlen,\n+            dropout_p=module.attention_dropout if module.training else 0.0,\n+            deterministic=module.deterministic_flash_attn,\n+            window_size=local_attention,\n+        )\n+        attn = attn.to(orig_dtype)  # type: ignore\n+    else:\n+        attn = flash_attn_varlen_qkvpacked_func(\n+            qkv,\n+            cu_seqlens=cu_seqlens,\n+            max_seqlen=max_seqlen,\n+            dropout_p=module.attention_dropout if module.training else 0.0,\n+            deterministic=module.deterministic_flash_attn,\n+            window_size=local_attention,\n+        )\n+    return (attn.view(bs, dim),)\n+\n+\n+def sdpa_attention_forward(\n+    module: \"ModernBertAttention\",\n+    qkv: torch.Tensor,\n+    attention_mask: torch.Tensor,\n+    sliding_window_mask: torch.Tensor,\n+    position_ids: Optional[torch.LongTensor],\n+    local_attention: Tuple[int, int],\n+    bs: int,\n+    dim: int,\n+    **_kwargs,\n+) -> Tuple[torch.Tensor]:\n+    # qkv: [batch_size, seqlen, 3, nheads, headdim]\n+    cos, sin = module.rotary_emb(qkv, position_ids=position_ids)\n+    query, key, value = qkv.transpose(3, 1).unbind(dim=2)\n+    # query, key, value: [batch_size, heads, seq_len, head_dim]\n+    query, key = apply_rotary_pos_emb(query, key, cos, sin)\n+\n+    if local_attention != (-1, -1):\n+        attention_mask = sliding_window_mask\n+\n+    attn_output = (\n+        F.scaled_dot_product_attention(\n+            query,\n+            key,\n+            value,\n+            dropout_p=module.attention_dropout if module.training else 0.0,\n+            attn_mask=attention_mask,\n+        )\n+        .transpose(1, 2)\n+        .contiguous()\n+    )\n+    attn_output = attn_output.view(bs, -1, dim)\n+    return (attn_output,)\n+\n+\n+MODERNBERT_ATTENTION_FUNCTION = {\n+    \"flash_attention_2\": flash_attention_forward,\n+    \"eager\": eager_attention_forward,\n+    \"sdpa\": sdpa_attention_forward,\n+}\n+\n+\n+class ModernBertAttention(nn.Module):\n+    \"\"\"Performs multi-headed self attention on a batch of unpadded sequences.\n+\n+    If Flash Attention 2 is installed, this module uses Flash Attention to improve throughput.\n+    If Flash Attention 2 is not installed, the implementation will use PyTorch's SDPA kernel,\n+    which requires padding and unpadding inputs, adding some overhead.\n+\n+    See `forward` method for additional details.\n+    \"\"\"\n+\n+    def __init__(self, config: ModernBertConfig, layer_id: Optional[int] = None):\n+        super().__init__()\n+        self.config = config\n+        self.layer_id = layer_id\n+\n+        if config.hidden_size % config.num_attention_heads != 0:\n+            raise ValueError(\n+                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention heads ({config.num_attention_heads})\"\n+            )\n+\n+        self.attention_dropout = config.attention_dropout\n+        self.deterministic_flash_attn = config.deterministic_flash_attn\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = config.hidden_size // config.num_attention_heads\n+        self.all_head_size = self.head_dim * self.num_heads\n+        self.Wqkv = nn.Linear(config.hidden_size, 3 * self.all_head_size, bias=config.attention_bias)\n+\n+        if layer_id % config.global_attn_every_n_layers != 0:\n+            self.local_attention = (config.local_attention // 2, config.local_attention // 2)\n+        else:\n+            self.local_attention = (-1, -1)\n+\n+        rope_theta = config.global_rope_theta\n+        max_position_embeddings = config.max_position_embeddings\n+        if self.local_attention != (-1, -1):\n+            if config.local_rope_theta is not None:\n+                rope_theta = config.local_rope_theta\n+            max_position_embeddings = config.local_attention\n+\n+        if config._attn_implementation == \"flash_attention_2\":\n+            self.rotary_emb = ModernBertUnpaddedRotaryEmbedding(\n+                dim=self.head_dim, max_seqlen=max_position_embeddings, base=rope_theta\n+            )\n+        else:\n+            self.rotary_emb = ModernBertRotaryEmbedding(\n+                dim=self.head_dim, max_position_embeddings=max_position_embeddings, base=rope_theta\n+            )\n+\n+        self.Wo = nn.Linear(config.hidden_size, config.hidden_size, bias=config.attention_bias)\n+        self.out_drop = nn.Dropout(config.attention_dropout) if config.attention_dropout > 0.0 else nn.Identity()\n+        self.pruned_heads = set()\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        output_attentions: Optional[bool] = False,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        qkv = self.Wqkv(hidden_states)\n+\n+        bs = hidden_states.shape[0]\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            qkv = qkv.view(-1, 3, self.num_heads, self.head_dim)\n+        else:\n+            qkv = qkv.view(bs, -1, 3, self.num_heads, self.head_dim)\n+\n+        attn_outputs = MODERNBERT_ATTENTION_FUNCTION[self.config._attn_implementation](\n+            self,\n+            qkv=qkv,\n+            rotary_emb=self.rotary_emb,\n+            local_attention=self.local_attention,\n+            bs=bs,\n+            dim=self.all_head_size,\n+            output_attentions=output_attentions,\n+            **kwargs,\n+        )\n+        hidden_states = attn_outputs[0]\n+        hidden_states = self.out_drop(self.Wo(hidden_states))\n+\n+        return (hidden_states,) + attn_outputs[1:]  # add attentions if outputted\n+\n+\n+class ModernBertEncoderLayer(nn.Module):\n+    def __init__(self, config: ModernBertConfig, layer_id: Optional[int] = None):\n+        super().__init__()\n+        self.config = config\n+        if layer_id == 0:\n+            self.attn_norm = nn.Identity()\n+        else:\n+            self.attn_norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)\n+        self.attn = ModernBertAttention(config=config, layer_id=layer_id)\n+        self.mlp_norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)\n+        self.mlp = ModernBertMLP(config)\n+\n+    @torch.compile(dynamic=True)\n+    def compiled_mlp(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        return self.mlp(self.mlp_norm(hidden_states))\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        sliding_window_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        cu_seqlens: Optional[torch.Tensor] = None,\n+        max_seqlen: Optional[int] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> torch.Tensor:\n+        attn_outputs = self.attn(\n+            self.attn_norm(hidden_states),\n+            attention_mask=attention_mask,\n+            sliding_window_mask=sliding_window_mask,\n+            position_ids=position_ids,\n+            cu_seqlens=cu_seqlens,\n+            max_seqlen=max_seqlen,\n+            output_attentions=output_attentions,\n+        )\n+        hidden_states = hidden_states + attn_outputs[0]\n+        mlp_output = (\n+            self.compiled_mlp(hidden_states)\n+            if self.config.reference_compile\n+            else self.mlp(self.mlp_norm(hidden_states))\n+        )\n+        hidden_states = hidden_states + mlp_output\n+\n+        return (hidden_states,) + attn_outputs[1:]  # add attentions if outputted\n+\n+\n+MODERNBERT_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`ModernBertConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare ModernBert Model outputting raw hidden-states without any specific head on top.\",\n+    MODERNBERT_START_DOCSTRING,\n+)\n+class ModernBertPreTrainedModel(PreTrainedModel):\n+    config_class = ModernBertConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"ModernBertEmbeddings\", \"ModernBertEncoderLayer\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = False\n+\n+    def _init_weights(self, module: nn.Module):\n+        cutoff_factor = self.config.initializer_cutoff_factor\n+        if cutoff_factor is None:\n+            cutoff_factor = 3\n+\n+        def init_weight(module: nn.Module, std: float):\n+            nn.init.trunc_normal_(\n+                module.weight,\n+                mean=0.0,\n+                std=std,\n+                a=-cutoff_factor * std,\n+                b=cutoff_factor * std,\n+            )\n+\n+            if isinstance(module, nn.Linear):\n+                if module.bias is not None:\n+                    nn.init.zeros_(module.bias)\n+\n+        stds = {\n+            \"in\": self.config.initializer_range,\n+            \"out\": self.config.initializer_range / math.sqrt(2.0 * self.config.num_hidden_layers),\n+            \"embedding\": self.config.initializer_range,\n+            \"final_out\": self.config.hidden_size**-0.5,\n+        }\n+\n+        if isinstance(module, ModernBertEmbeddings):\n+            init_weight(module.tok_embeddings, stds[\"embedding\"])\n+        elif isinstance(module, ModernBertMLP):\n+            init_weight(module.Wi, stds[\"in\"])\n+            init_weight(module.Wo, stds[\"out\"])\n+        elif isinstance(module, ModernBertAttention):\n+            init_weight(module.Wqkv, stds[\"in\"])\n+            init_weight(module.Wo, stds[\"out\"])\n+        elif isinstance(module, ModernBertPredictionHead):\n+            init_weight(module.dense, stds[\"in\"])\n+        elif isinstance(module, ModernBertPoolingHead):\n+            init_weight(module.dense, stds[\"out\"])\n+        elif isinstance(module, ModernBertForMaskedLM):\n+            init_weight(module.decoder, stds[\"out\"])\n+        elif isinstance(module, (ModernBertForSequenceClassification, ModernBertForTokenClassification)):\n+            init_weight(module.classifier, stds[\"final_out\"])\n+\n+    @classmethod\n+    def _autoset_attn_implementation(\n+        cls,\n+        config,\n+        use_flash_attention_2: bool = False,\n+        torch_dtype: Optional[torch.dtype] = None,\n+        device_map: Optional[Union[str, Dict[str, int]]] = None,\n+        check_device_map: bool = True,\n+    ):\n+        # If the user didn't specify anything, try to use flash_attention_2 if available.\n+        # Otherwise we fall back to the default SDPA -> Eager from the super() method.\n+        if config._attn_implementation_internal is None:\n+            config._attn_implementation_internal = \"flash_attention_2\"\n+            try:\n+                return cls._check_and_enable_flash_attn_2(\n+                    config,\n+                    torch_dtype=torch_dtype,\n+                    device_map=device_map,\n+                    hard_check_only=False,\n+                    check_device_map=check_device_map,\n+                )\n+            except (ValueError, ImportError):\n+                config._attn_implementation_internal = None\n+        return super()._autoset_attn_implementation(\n+            config,\n+            use_flash_attention_2=use_flash_attention_2,\n+            torch_dtype=torch_dtype,\n+            device_map=device_map,\n+            check_device_map=check_device_map,\n+        )\n+\n+    def _maybe_set_compile(self):\n+        if self.config.reference_compile is False:\n+            return\n+\n+        if hasattr(self, \"hf_device_map\") and len(self.hf_device_map) > 1:\n+            if self.config.reference_compile:\n+                logger.warning_once(\n+                    \"If `accelerate` split the model across devices, `torch.compile` will not work. \"\n+                    \"Falling back to non-compiled mode.\"\n+                )\n+            self.config.reference_compile = False\n+\n+        if self.device.type == \"mps\":\n+            if self.config.reference_compile:\n+                logger.warning_once(\n+                    \"Compiling the model with `torch.compile` and using a `torch.mps` device is not supported. \"\n+                    \"Falling back to non-compiled mode.\"\n+                )\n+            self.config.reference_compile = False\n+\n+        if self.config.reference_compile is None:\n+            self.config.reference_compile = is_triton_available()\n+\n+    def resize_token_embeddings(self, *args, **kwargs):\n+        model_embeds = super().resize_token_embeddings(*args, **kwargs)\n+\n+        if self.config.reference_compile in {True, None}:\n+            if self.config.reference_compile:\n+                logger.warning_once(\n+                    \"Resizing token embeddings with `torch.compile` is not supported. Falling back to non-compiled mode.\"\n+                )\n+            self.config.reference_compile = False\n+\n+        return model_embeds\n+\n+\n+def _unpad_modernbert_input(\n+    inputs: torch.Tensor,\n+    attention_mask: torch.Tensor,\n+    position_ids: Optional[torch.Tensor] = None,\n+    labels: Optional[torch.Tensor] = None,\n+) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int, Optional[torch.Tensor], Optional[torch.Tensor]]:\n+    \"\"\"\n+    Remove padding from input sequences.\n+\n+    Args:\n+        inputs: (batch, seqlen, ...) or (batch, seqlen)\n+        attention_mask: (batch, seqlen), bool / int, 1 means valid and 0 means not valid.\n+        position_ids: (batch, seqlen), int, position ids\n+        labels: (batch, seqlen), int, labels\n+\n+    Returns:\n+        unpadded_inputs: (total_nnz, ...), where total_nnz = number of tokens selected in attention_mask.\n+        indices: (total_nnz)\n+        cu_seqlens: (batch + 1), the cumulative sequence lengths\n+        max_seqlen_in_batch: int\n+        unpadded_position_ids: (total_nnz) or None\n+        unpadded_labels: (total_nnz) or None\n+    \"\"\"\n+    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n+    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()\n+    max_seqlen_in_batch = int(seqlens_in_batch.max().item())\n+    cu_seqlens = torch.nn.functional.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))\n+\n+    if inputs.dim() == 2:\n+        unpadded_inputs = inputs.flatten()[indices]\n+    else:\n+        batch, seqlen, *rest = inputs.shape\n+        shape = batch * seqlen\n+        unpadded_inputs = inputs.view(shape, *rest)[indices]\n+\n+    unpadded_position_ids = position_ids.flatten()[indices] if position_ids is not None else None\n+    unpadded_labels = labels.flatten()[indices] if labels is not None else None\n+\n+    return unpadded_inputs, indices, cu_seqlens, max_seqlen_in_batch, unpadded_position_ids, unpadded_labels\n+\n+\n+def _pad_modernbert_output(\n+    inputs: torch.Tensor,\n+    indices: torch.Tensor,\n+    batch: int,\n+    seqlen: int,\n+) -> torch.Tensor:\n+    \"\"\"\n+    Add padding to sequences.\n+\n+    Args:\n+        inputs: (total_nnz, ...) or (total_nnz,), where total_nnz = number of tokens selected in attention_mask.\n+        indices: (total_nnz)\n+        batch: int, batch size\n+        seqlen: int, max sequence length\n+\n+    Returns:\n+        padded_inputs: (batch, seqlen, ...) or (batch, seqlen)\n+    \"\"\"\n+    if inputs.dim() == 1:\n+        output = torch.zeros(batch * seqlen, dtype=inputs.dtype, device=inputs.device)\n+        output[indices] = inputs\n+        padded_inputs = output.view(batch, seqlen)\n+    else:\n+        _, *rest = inputs.shape\n+        output = torch.zeros(batch * seqlen, *rest, dtype=inputs.dtype, device=inputs.device)\n+        output[indices] = inputs\n+        padded_inputs = output.view(batch, seqlen, *rest)\n+\n+    return padded_inputs\n+\n+\n+MODERNBERT_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        sliding_window_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding or far-away tokens. In ModernBert, only every few layers\n+            perform global attention, while the rest perform local attention. This mask is used to avoid attending to\n+            far-away tokens in the local attention layers.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        indices (`torch.Tensor` of shape `(total_unpadded_tokens,)`, *optional*):\n+            Indices of the non-padding tokens in the input sequence. Used for unpadding the output.\n+        cu_seqlens (`torch.Tensor` of shape `(batch + 1,)`, *optional*):\n+            Cumulative sequence lengths of the input sequences. Used to index the unpadded tensors.\n+        max_seqlen (`int`, *optional*):\n+            Maximum sequence length in the batch. Used to pad the output tensors.\n+        batch_size (`int`, *optional*):\n+            Batch size of the input sequences. Used to pad the output tensors.\n+        seq_len (`int`, *optional*):\n+            Sequence length of the input sequences. Used to pad the output tensors.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare ModernBert Model outputting raw hidden-states without any specific head on top.\",\n+    MODERNBERT_START_DOCSTRING,\n+)\n+class ModernBertModel(ModernBertPreTrainedModel):\n+    def __init__(self, config: ModernBertConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.embeddings = ModernBertEmbeddings(config)\n+        self.layers = nn.ModuleList(\n+            [ModernBertEncoderLayer(config, layer_id) for layer_id in range(config.num_hidden_layers)]\n+        )\n+        self.final_norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)\n+        self.gradient_checkpointing = False\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embeddings.tok_embeddings\n+\n+    def set_input_embeddings(self, value):\n+        self.embeddings.tok_embeddings = value\n+\n+    @add_start_docstrings_to_model_forward(MODERNBERT_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=BaseModelOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        sliding_window_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        indices: Optional[torch.Tensor] = None,\n+        cu_seqlens: Optional[torch.Tensor] = None,\n+        max_seqlen: Optional[int] = None,\n+        batch_size: Optional[int] = None,\n+        seq_len: Optional[int] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple[torch.Tensor, ...], BaseModelOutput]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attentions = () if output_attentions else None\n+\n+        self._maybe_set_compile()\n+        self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n+\n+        if batch_size is None and seq_len is None:\n+            batch_size, seq_len = input_ids.shape[:2]\n+\n+        if attention_mask is None:\n+            attention_mask = torch.ones((batch_size, seq_len), device=input_ids.device, dtype=torch.bool)\n+\n+        repad = False\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if indices is None and cu_seqlens is None and max_seqlen is None:\n+                repad = True\n+                with torch.no_grad():\n+                    input_ids, indices, cu_seqlens, max_seqlen, *_ = _unpad_modernbert_input(\n+                        inputs=input_ids, attention_mask=attention_mask\n+                    )\n+        else:\n+            if position_ids is None:\n+                position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n+\n+            attention_mask, sliding_window_mask = self._update_attention_mask(\n+                attention_mask, output_attentions=output_attentions\n+            )\n+\n+        hidden_states = self.embeddings(input_ids)\n+\n+        for encoder_layer in self.layers:\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    encoder_layer.__call__,\n+                    hidden_states,\n+                    attention_mask,\n+                    sliding_window_mask,\n+                    position_ids,\n+                    cu_seqlens,\n+                    max_seqlen,\n+                    output_attentions,\n+                )\n+            else:\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask=attention_mask,\n+                    sliding_window_mask=sliding_window_mask,\n+                    position_ids=position_ids,\n+                    cu_seqlens=cu_seqlens,\n+                    max_seqlen=max_seqlen,\n+                    output_attentions=output_attentions,\n+                )\n+            hidden_states = layer_outputs[0]\n+            if output_attentions and len(layer_outputs) > 1:\n+                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+        hidden_states = self.final_norm(hidden_states)\n+\n+        if repad:\n+            hidden_states = _pad_modernbert_output(\n+                inputs=hidden_states, indices=indices, batch=batch_size, seqlen=seq_len\n+            )\n+            if all_hidden_states is not None:\n+                all_hidden_states = tuple(\n+                    _pad_modernbert_output(inputs=hs, indices=indices, batch=batch_size, seqlen=seq_len)\n+                    for hs in all_hidden_states\n+                )\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attentions,\n+        )\n+\n+    def _update_attention_mask(self, attention_mask: torch.Tensor, output_attentions: bool) -> torch.Tensor:\n+        if output_attentions:\n+            if self.config._attn_implementation == \"sdpa\":\n+                logger.warning_once(\n+                    \"Outputting attentions is only supported with the 'eager' attention implementation, \"\n+                    'not with \"sdpa\". Falling back to `attn_implementation=\"eager\"`.'\n+                )\n+                self.config._attn_implementation = \"eager\"\n+            elif self.config._attn_implementation != \"eager\":\n+                logger.warning_once(\n+                    \"Outputting attentions is only supported with the eager attention implementation, \"\n+                    f'not with {self.config._attn_implementation}. Consider setting `attn_implementation=\"eager\"`.'\n+                    \" Setting `output_attentions=False`.\"\n+                )\n+\n+        global_attention_mask = _prepare_4d_attention_mask(attention_mask, self.dtype)\n+\n+        # Create position indices\n+        rows = torch.arange(global_attention_mask.shape[2]).unsqueeze(0)\n+        # Calculate distance between positions\n+        distance = torch.abs(rows - rows.T)\n+\n+        # Create sliding window mask (1 for positions within window, 0 outside)\n+        window_mask = (\n+            (distance <= self.config.local_attention // 2).unsqueeze(0).unsqueeze(0).to(attention_mask.device)\n+        )\n+        # Combine with existing mask\n+        sliding_window_mask = global_attention_mask.masked_fill(window_mask.logical_not(), torch.finfo(self.dtype).min)\n+\n+        return global_attention_mask, sliding_window_mask\n+\n+\n+class ModernBertPredictionHead(nn.Module):\n+    def __init__(self, config: ModernBertConfig):\n+        super().__init__()\n+        self.config = config\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size, config.classifier_bias)\n+        self.act = ACT2FN[config.classifier_activation]\n+        self.norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        return self.norm(self.act(self.dense(hidden_states)))\n+\n+\n+@add_start_docstrings(\n+    \"The ModernBert Model with a decoder head on top that is used for masked language modeling.\",\n+    MODERNBERT_START_DOCSTRING,\n+)\n+class ModernBertForMaskedLM(ModernBertPreTrainedModel):\n+    _tied_weights_keys = [\"decoder.weight\"]\n+\n+    def __init__(self, config: ModernBertConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.model = ModernBertModel(config)\n+        self.head = ModernBertPredictionHead(config)\n+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=config.decoder_bias)\n+\n+        self.sparse_prediction = self.config.sparse_prediction\n+        self.sparse_pred_ignore_index = self.config.sparse_pred_ignore_index\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_output_embeddings(self):\n+        return self.decoder\n+\n+    def set_output_embeddings(self, new_embeddings: nn.Linear):\n+        self.decoder = new_embeddings\n+\n+    @torch.compile(dynamic=True)\n+    def compiled_head(self, output: torch.Tensor) -> torch.Tensor:\n+        return self.decoder(self.head(output))\n+\n+    @add_start_docstrings_to_model_forward(MODERNBERT_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=MaskedLMOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        sliding_window_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        indices: Optional[torch.Tensor] = None,\n+        cu_seqlens: Optional[torch.Tensor] = None,\n+        max_seqlen: Optional[int] = None,\n+        batch_size: Optional[int] = None,\n+        seq_len: Optional[int] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        **kwargs,\n+    ) -> Union[Tuple[torch.Tensor], MaskedLMOutput]:\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        self._maybe_set_compile()\n+\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if indices is None and cu_seqlens is None and max_seqlen is None:\n+                batch_size, seq_len = input_ids.shape[:2]\n+                if attention_mask is None:\n+                    attention_mask = torch.ones((batch_size, seq_len), device=input_ids.device, dtype=torch.bool)\n+                with torch.no_grad():\n+                    input_ids, indices, cu_seqlens, max_seqlen, position_ids, labels = _unpad_modernbert_input(\n+                        inputs=input_ids, attention_mask=attention_mask, position_ids=position_ids, labels=labels\n+                    )\n+\n+        outputs = self.model(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            sliding_window_mask=sliding_window_mask,\n+            position_ids=position_ids,\n+            indices=indices,\n+            cu_seqlens=cu_seqlens,\n+            max_seqlen=max_seqlen,\n+            batch_size=batch_size,\n+            seq_len=seq_len,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        last_hidden_state = outputs[0]\n+\n+        if self.sparse_prediction and labels is not None:\n+            # flatten labels and output first\n+            labels = labels.view(-1)\n+            last_hidden_state = last_hidden_state.view(labels.shape[0], -1)\n+\n+            # then filter out the non-masked tokens\n+            mask_tokens = labels != self.sparse_pred_ignore_index\n+            last_hidden_state = last_hidden_state[mask_tokens]\n+            labels = labels[mask_tokens]\n+\n+        logits = (\n+            self.compiled_head(last_hidden_state)\n+            if self.config.reference_compile\n+            else self.decoder(self.head(last_hidden_state))\n+        )\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits, labels, vocab_size=self.config.vocab_size)\n+\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            with torch.no_grad():\n+                logits = _pad_modernbert_output(inputs=logits, indices=indices, batch=batch_size, seqlen=seq_len)\n+        if not return_dict:\n+            output = (logits,)\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return MaskedLMOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+class ModernBertPoolingHead(nn.Module):\n+    def __init__(self, config: ModernBertConfig):\n+        super().__init__()\n+        self.config = config\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size, config.classifier_bias)\n+        self.act = ACT2FN[config.classifier_activation]\n+        self.norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)\n+        self.drop = torch.nn.Dropout(config.classifier_dropout)\n+\n+    def forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n+        if self.config.classifier_pooling == \"cls\":\n+            hidden_states = hidden_states[:, 0]\n+        elif self.config.classifier_pooling == \"mean\":\n+            hidden_states = (hidden_states * attention_mask.unsqueeze(-1)).sum(dim=1) / attention_mask.sum(\n+                dim=1, keepdim=True\n+            )\n+\n+        return self.drop(self.norm(self.act(self.dense(hidden_states))))\n+\n+\n+@add_start_docstrings(\n+    \"The ModernBert Model with a sequence classification head on top that performs pooling.\",\n+    MODERNBERT_START_DOCSTRING,\n+)\n+class ModernBertForSequenceClassification(ModernBertPreTrainedModel):\n+    def __init__(self, config: ModernBertConfig):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+        self.config = config\n+\n+        self.model = ModernBertModel(config)\n+        self.head = ModernBertPoolingHead(config)\n+        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(MODERNBERT_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=SequenceClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        sliding_window_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        indices: Optional[torch.Tensor] = None,\n+        cu_seqlens: Optional[torch.Tensor] = None,\n+        max_seqlen: Optional[int] = None,\n+        batch_size: Optional[int] = None,\n+        seq_len: Optional[int] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        **kwargs,\n+    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        self._maybe_set_compile()\n+\n+        outputs = self.model(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            sliding_window_mask=sliding_window_mask,\n+            position_ids=position_ids,\n+            indices=indices,\n+            cu_seqlens=cu_seqlens,\n+            max_seqlen=max_seqlen,\n+            batch_size=batch_size,\n+            seq_len=seq_len,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        last_hidden_state = outputs[0]\n+\n+        pooled_output = self.head(last_hidden_state, attention_mask)\n+        logits = self.classifier(pooled_output)\n+\n+        loss = None\n+        if labels is not None:\n+            if self.config.problem_type is None:\n+                if self.num_labels == 1:\n+                    self.config.problem_type = \"regression\"\n+                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n+                    self.config.problem_type = \"single_label_classification\"\n+                else:\n+                    self.config.problem_type = \"multi_label_classification\"\n+\n+            if self.config.problem_type == \"regression\":\n+                loss_fct = MSELoss()\n+                if self.num_labels == 1:\n+                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n+                else:\n+                    loss = loss_fct(logits, labels)\n+            elif self.config.problem_type == \"single_label_classification\":\n+                loss_fct = CrossEntropyLoss()\n+                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            elif self.config.problem_type == \"multi_label_classification\":\n+                loss_fct = BCEWithLogitsLoss()\n+                loss = loss_fct(logits, labels)\n+\n+        if not return_dict:\n+            output = (logits,)\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return SequenceClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+@add_start_docstrings(\n+    \"The ModernBert Model with a token classification head on top, e.g. for Named Entity Recognition (NER) tasks.\",\n+    MODERNBERT_START_DOCSTRING,\n+)\n+class ModernBertForTokenClassification(ModernBertPreTrainedModel):\n+    def __init__(self, config: ModernBertConfig):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+\n+        self.model = ModernBertModel(config)\n+        self.drop = nn.Dropout(config.classifier_dropout)\n+        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(MODERNBERT_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=TokenClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        sliding_window_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        indices: Optional[torch.Tensor] = None,\n+        cu_seqlens: Optional[torch.Tensor] = None,\n+        max_seqlen: Optional[int] = None,\n+        batch_size: Optional[int] = None,\n+        seq_len: Optional[int] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        self._maybe_set_compile()\n+\n+        outputs = self.model(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            sliding_window_mask=sliding_window_mask,\n+            position_ids=position_ids,\n+            indices=indices,\n+            cu_seqlens=cu_seqlens,\n+            max_seqlen=max_seqlen,\n+            batch_size=batch_size,\n+            seq_len=seq_len,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        last_hidden_state = outputs[0]\n+\n+        last_hidden_state = self.drop(last_hidden_state)\n+        logits = self.classifier(last_hidden_state)\n+\n+        loss = None\n+        if labels is not None:\n+            loss_fct = CrossEntropyLoss()\n+            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return TokenClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\n+    \"ModernBertModel\",\n+    \"ModernBertPreTrainedModel\",\n+    \"ModernBertForMaskedLM\",\n+    \"ModernBertForSequenceClassification\",\n+    \"ModernBertForTokenClassification\",\n+]"
        },
        {
            "sha": "3c23f9178b1b51e5f5404df4069fa9bb591910d3",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "added",
            "additions": 1452,
            "deletions": 0,
            "changes": 1452,
            "blob_url": "https://github.com/huggingface/transformers/blob/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=667ed5635e6fd7e2df4fc23012746b1c0cbb7575",
            "patch": "@@ -0,0 +1,1452 @@\n+# Copyright 2024 Answer.AI, LightOn, and contributors, and the HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from typing import Dict, Literal, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn.functional as F\n+import torch.utils.checkpoint\n+from torch import nn\n+from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+\n+from ...activations import ACT2FN\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_outputs import (\n+    BaseModelOutput,\n+    MaskedLMOutput,\n+    SequenceClassifierOutput,\n+    TokenClassifierOutput,\n+)\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    add_code_sample_docstrings,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    is_flash_attn_2_available,\n+    logging,\n+)\n+from ...utils.import_utils import is_triton_available\n+from ..gemma.modeling_gemma import GemmaRotaryEmbedding, apply_rotary_pos_emb\n+\n+\n+if is_flash_attn_2_available():\n+    from flash_attn.flash_attn_interface import flash_attn_varlen_qkvpacked_func\n+    from flash_attn.layers.rotary import RotaryEmbedding\n+    from flash_attn.ops.triton.rotary import apply_rotary\n+else:\n+    RotaryEmbedding = object\n+\n+_CHECKPOINT_FOR_DOC = \"answerdotai/ModernBERT-base\"\n+_CONFIG_FOR_DOC = \"ModernBertConfig\"\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class ModernBertConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`ModernBertModel`]. It is used to instantiate an ModernBert\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the ModernBERT-base.\n+    e.g. [answerdotai/ModernBERT-base](https://huggingface.co/answerdotai/ModernBERT-base)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 50368):\n+            Vocabulary size of the ModernBert model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`ModernBertModel`]\n+        hidden_size (`int`, *optional*, defaults to 768):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 1152):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 22):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 12):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        hidden_activation (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the decoder. Will default to `\"gelu\"`\n+            if not specified.\n+        max_position_embeddings (`int`, *optional*, defaults to 8192):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        initializer_cutoff_factor (`float`, *optional*, defaults to 2.0):\n+            The cutoff factor for the truncated_normal_initializer for initializing all weight matrices.\n+        norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the rms normalization layers.\n+        norm_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use bias in the normalization layers.\n+        pad_token_id (`int`, *optional*, defaults to 50283):\n+            Padding token id.\n+        eos_token_id (`int`, *optional*, defaults to 50282):\n+            End of stream token id.\n+        bos_token_id (`int`, *optional*, defaults to 50281):\n+            Beginning of stream token id.\n+        cls_token_id (`int`, *optional*, defaults to 50281):\n+            Classification token id.\n+        sep_token_id (`int`, *optional*, defaults to 50282):\n+            Separation token id.\n+        global_rope_theta (`float`, *optional*, defaults to 160000.0):\n+            The base period of the global RoPE embeddings.\n+        attention_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        global_attn_every_n_layers (`int`, *optional*, defaults to 3):\n+            The number of layers between global attention layers.\n+        local_attention (`int`, *optional*, defaults to 128):\n+            The window size for local attention.\n+        local_rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the local RoPE embeddings.\n+        embedding_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the embeddings.\n+        mlp_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use bias in the MLP layers.\n+        mlp_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the MLP layers.\n+        decoder_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use bias in the decoder layers.\n+        classifier_pooling (`str`, *optional*, defaults to `\"cls\"`):\n+            The pooling method for the classifier. Should be either `\"cls\"` or `\"mean\"`. In local attention layers, the\n+            CLS token doesn't attend to all tokens on long sequences.\n+        classifier_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the classifier.\n+        classifier_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use bias in the classifier.\n+        classifier_activation (`str`, *optional*, defaults to `\"gelu\"`):\n+            The activation function for the classifier.\n+        deterministic_flash_attn (`bool`, *optional*, defaults to `False`):\n+            Whether to use deterministic flash attention. If `False`, inference will be faster but not deterministic.\n+        sparse_prediction (`bool`, *optional*, defaults to `False`):\n+            Whether to use sparse prediction for the masked language model instead of returning the full dense logits.\n+        sparse_pred_ignore_index (`int`, *optional*, defaults to -100):\n+            The index to ignore for the sparse prediction.\n+        reference_compile (`bool`, *optional*):\n+            Whether to compile the layers of the model which were compiled during pretraining. If `None`, then parts of\n+            the model will be compiled if 1) `triton` is installed, 2) the model is not on MPS, 3) the model is not\n+            shared between devices, and 4) the model is not resized after initialization. If `True`, then the model may\n+            be faster in some scenarios.\n+\n+    Examples:\n+\n+    ```python\n+    >>> from transformers import ModernBertModel, ModernBertConfig\n+\n+    >>> # Initializing a ModernBert style configuration\n+    >>> configuration = ModernBertConfig()\n+\n+    >>> # Initializing a model from the modernbert-base style configuration\n+    >>> model = ModernBertModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"modernbert\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    def __init__(\n+        self,\n+        vocab_size=50368,\n+        hidden_size=768,\n+        intermediate_size=1152,\n+        num_hidden_layers=22,\n+        num_attention_heads=12,\n+        hidden_activation=\"gelu\",\n+        max_position_embeddings=8192,\n+        initializer_range=0.02,\n+        initializer_cutoff_factor=2.0,\n+        norm_eps=1e-5,\n+        norm_bias=False,\n+        pad_token_id=50283,\n+        eos_token_id=50282,\n+        bos_token_id=50281,\n+        cls_token_id=50281,\n+        sep_token_id=50282,\n+        global_rope_theta=160000.0,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        global_attn_every_n_layers=3,\n+        local_attention=128,\n+        local_rope_theta=10000.0,\n+        embedding_dropout=0.0,\n+        mlp_bias=False,\n+        mlp_dropout=0.0,\n+        decoder_bias=True,\n+        classifier_pooling: Literal[\"cls\", \"mean\"] = \"cls\",\n+        classifier_dropout=0.0,\n+        classifier_bias=False,\n+        classifier_activation=\"gelu\",\n+        deterministic_flash_attn=False,\n+        sparse_prediction=False,\n+        sparse_pred_ignore_index=-100,\n+        reference_compile=None,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            cls_token_id=cls_token_id,\n+            sep_token_id=sep_token_id,\n+            **kwargs,\n+        )\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.initializer_range = initializer_range\n+        self.initializer_cutoff_factor = initializer_cutoff_factor\n+        self.norm_eps = norm_eps\n+        self.norm_bias = norm_bias\n+        self.global_rope_theta = global_rope_theta\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.hidden_activation = hidden_activation\n+        self.global_attn_every_n_layers = global_attn_every_n_layers\n+        self.local_attention = local_attention\n+        self.local_rope_theta = local_rope_theta\n+        self.embedding_dropout = embedding_dropout\n+        self.mlp_bias = mlp_bias\n+        self.mlp_dropout = mlp_dropout\n+        self.decoder_bias = decoder_bias\n+        self.classifier_pooling = classifier_pooling\n+        self.classifier_dropout = classifier_dropout\n+        self.classifier_bias = classifier_bias\n+        self.classifier_activation = classifier_activation\n+        self.deterministic_flash_attn = deterministic_flash_attn\n+        self.sparse_prediction = sparse_prediction\n+        self.sparse_pred_ignore_index = sparse_pred_ignore_index\n+        self.reference_compile = reference_compile\n+\n+        if self.classifier_pooling not in [\"cls\", \"mean\"]:\n+            raise ValueError(\n+                f'Invalid value for `classifier_pooling`, should be either \"cls\" or \"mean\", but is {self.classifier_pooling}.'\n+            )\n+\n+\n+def _unpad_modernbert_input(\n+    inputs: torch.Tensor,\n+    attention_mask: torch.Tensor,\n+    position_ids: Optional[torch.Tensor] = None,\n+    labels: Optional[torch.Tensor] = None,\n+) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int, Optional[torch.Tensor], Optional[torch.Tensor]]:\n+    \"\"\"\n+    Remove padding from input sequences.\n+\n+    Args:\n+        inputs: (batch, seqlen, ...) or (batch, seqlen)\n+        attention_mask: (batch, seqlen), bool / int, 1 means valid and 0 means not valid.\n+        position_ids: (batch, seqlen), int, position ids\n+        labels: (batch, seqlen), int, labels\n+\n+    Returns:\n+        unpadded_inputs: (total_nnz, ...), where total_nnz = number of tokens selected in attention_mask.\n+        indices: (total_nnz)\n+        cu_seqlens: (batch + 1), the cumulative sequence lengths\n+        max_seqlen_in_batch: int\n+        unpadded_position_ids: (total_nnz) or None\n+        unpadded_labels: (total_nnz) or None\n+    \"\"\"\n+    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n+    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()\n+    max_seqlen_in_batch = int(seqlens_in_batch.max().item())\n+    cu_seqlens = torch.nn.functional.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))\n+\n+    if inputs.dim() == 2:\n+        unpadded_inputs = inputs.flatten()[indices]\n+    else:\n+        batch, seqlen, *rest = inputs.shape\n+        shape = batch * seqlen\n+        unpadded_inputs = inputs.view(shape, *rest)[indices]\n+\n+    unpadded_position_ids = position_ids.flatten()[indices] if position_ids is not None else None\n+    unpadded_labels = labels.flatten()[indices] if labels is not None else None\n+\n+    return unpadded_inputs, indices, cu_seqlens, max_seqlen_in_batch, unpadded_position_ids, unpadded_labels\n+\n+\n+def _pad_modernbert_output(\n+    inputs: torch.Tensor,\n+    indices: torch.Tensor,\n+    batch: int,\n+    seqlen: int,\n+) -> torch.Tensor:\n+    \"\"\"\n+    Add padding to sequences.\n+\n+    Args:\n+        inputs: (total_nnz, ...) or (total_nnz,), where total_nnz = number of tokens selected in attention_mask.\n+        indices: (total_nnz)\n+        batch: int, batch size\n+        seqlen: int, max sequence length\n+\n+    Returns:\n+        padded_inputs: (batch, seqlen, ...) or (batch, seqlen)\n+    \"\"\"\n+    if inputs.dim() == 1:\n+        output = torch.zeros(batch * seqlen, dtype=inputs.dtype, device=inputs.device)\n+        output[indices] = inputs\n+        padded_inputs = output.view(batch, seqlen)\n+    else:\n+        _, *rest = inputs.shape\n+        output = torch.zeros(batch * seqlen, *rest, dtype=inputs.dtype, device=inputs.device)\n+        output[indices] = inputs\n+        padded_inputs = output.view(batch, seqlen, *rest)\n+\n+    return padded_inputs\n+\n+\n+class ApplyRotaryEmbUnpad(torch.autograd.Function):\n+    @staticmethod\n+    def forward(\n+        ctx,\n+        qkv,\n+        cos,\n+        sin,\n+        cu_seqlens: Optional[torch.Tensor] = None,\n+        max_seqlen: Optional[int] = None,\n+    ):\n+        # (total_nnz, 3, nheads, headdim)\n+        qkv = qkv.contiguous()\n+        total_nnz, _three, _nheads, headdim = qkv.shape\n+        # We need qkv to be contiguous so that when we reshape to combine (3, nheads) dimensions,\n+        # we get the same tensor\n+        # qk = rearrange(qkv[:, :2], \"b_s t h d -> b_s (t h) d\")\n+        qk = qkv[:, :2].view(total_nnz, -1, headdim)\n+        apply_rotary(\n+            qk,\n+            cos,\n+            sin,\n+            seqlen_offsets=0,\n+            cu_seqlens=cu_seqlens,\n+            max_seqlen=max_seqlen,\n+            interleaved=False,\n+            inplace=True,\n+        )\n+\n+        ctx.save_for_backward(cos, sin, cu_seqlens)\n+        ctx.max_seqlen = max_seqlen\n+        return qkv\n+\n+    @staticmethod\n+    def backward(ctx, do):\n+        cos, sin, cu_seqlens = ctx.saved_tensors\n+        do = do.contiguous()\n+        total_nnz, _three, _nheads, headdim = do.shape\n+        # We need dqkv to be contiguous so that when we reshape to combine (3, nheads) dimensions,\n+        # we get the same tensor\n+        dqk = do[:, :2].view(total_nnz, -1, headdim)\n+        apply_rotary(\n+            dqk,\n+            cos,\n+            sin,\n+            seqlen_offsets=0,\n+            cu_seqlens=cu_seqlens,\n+            max_seqlen=ctx.max_seqlen,\n+            interleaved=False,\n+            inplace=True,\n+            conjugate=True,\n+        )\n+\n+        return do, None, None, None, None, None, None\n+\n+\n+def apply_rotary_unpadded(\n+    qkv,\n+    cos,\n+    sin,\n+    cu_seqlens: Optional[torch.Tensor] = None,\n+    max_seqlen: Optional[int] = None,\n+):\n+    \"\"\"\n+    Arguments:\n+        qkv: (total_nnz, 3, nheads, headdim) - input tensor for packed QKV.\n+        cos, sin: (seqlen_rotary, rotary_dim / 2)\n+        interleaved: if True, rotate pairs of even and odd dimensions (GPT-J style) instead\n+            of 1st half and 2nd half (GPT-NeoX style).\n+        inplace: if True, apply rotary embedding in-place.\n+        seqlen_offsets: (batch_size,) or int. Each sequence in x is shifted by this amount.\n+            Most commonly used in inference when we have KV cache.\n+        cu_seqlens: (batch + 1,) or None\n+        max_seqlen: int\n+    Return:\n+        out: (total_nnz, dim)\n+    rotary_dim must be <= headdim\n+    Apply rotary embedding to the first rotary_dim of x.\n+    \"\"\"\n+    return ApplyRotaryEmbUnpad.apply(qkv, cos, sin, cu_seqlens, max_seqlen)\n+\n+\n+class ModernBertUnpaddedRotaryEmbedding(RotaryEmbedding):\n+    \"\"\"\n+    The rotary position embeddings applied directly to unpadded sequences.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        dim: int,\n+        base: float = 10000.0,\n+        max_seqlen: Optional[int] = None,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        \"\"\"\n+        max_seqlen: if max_seqlen, device, and dtype are provided, we precompute the cos_sin_cache\n+            up to max_seqlen. If the max_seqlen, device, or dtype during training/inference differ,\n+            the cos_sin_cache wll be recomputed during the forward pass.\n+        \"\"\"\n+        super().__init__(dim=dim, base=base, pos_idx_in_fp32=True, device=device, interleaved=False)\n+        self.max_seqlen = max_seqlen\n+\n+        if max_seqlen is not None and device is not None and dtype is not None:\n+            self._update_cos_sin_cache(max_seqlen, device=device, dtype=dtype)\n+\n+    def forward(\n+        self,\n+        qkv: torch.Tensor,\n+        cu_seqlens: torch.Tensor,\n+        max_seqlen: Optional[int] = None,\n+    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n+        \"\"\"\n+        Apply rotary embedding *inplace* to qkv.\n+        qkv: (total_nnz, 3, nheads, headdim)\n+        cu_seqlens: (batch + 1,) cumulative sequence lengths\n+        max_seqlen: int max seq length in the batch\n+        \"\"\"\n+        if max_seqlen is not None:\n+            self._update_cos_sin_cache(max_seqlen, device=qkv.device, dtype=qkv.dtype)\n+\n+        qkv = apply_rotary_unpadded(\n+            qkv,\n+            self._cos_cached,\n+            self._sin_cached,\n+            cu_seqlens=cu_seqlens,\n+            max_seqlen=max_seqlen,\n+        )\n+\n+        return qkv\n+\n+    def extra_repr(self) -> str:\n+        return f\"dim={self.dim}, base={self.base}, scale_base={self.scale_base}\"\n+\n+\n+class ModernBertEmbeddings(nn.Module):\n+    \"\"\"\n+    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\n+    \"\"\"\n+\n+    def __init__(self, config: ModernBertConfig):\n+        super().__init__()\n+        self.config = config\n+        self.tok_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n+        self.norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)\n+        self.drop = nn.Dropout(config.embedding_dropout)\n+\n+    @torch.compile(dynamic=True)\n+    def compiled_embeddings(self, input_ids: torch.LongTensor) -> torch.Tensor:\n+        return self.drop(self.norm(self.tok_embeddings(input_ids)))\n+\n+    def forward(self, input_ids: torch.LongTensor, position_ids: Optional[torch.LongTensor] = None) -> torch.Tensor:\n+        hidden_states = (\n+            self.compiled_embeddings(input_ids)\n+            if self.config.reference_compile\n+            else self.drop(self.norm(self.tok_embeddings(input_ids)))\n+        )\n+        return hidden_states\n+\n+\n+class ModernBertMLP(nn.Module):\n+    \"\"\"Applies the GLU at the end of each ModernBERT layer.\n+\n+    Compared to the default BERT architecture, this block replaces :class:`~transformers.model.bert.modeling_bert.BertIntermediate`\n+    and :class:`~transformers.model.bert.modeling_bert.SelfOutput` with a single module that has similar functionality.\n+    \"\"\"\n+\n+    def __init__(self, config: ModernBertConfig):\n+        super().__init__()\n+        self.config = config\n+        self.Wi = nn.Linear(config.hidden_size, int(config.intermediate_size) * 2, bias=config.mlp_bias)\n+        self.act = ACT2FN[config.hidden_activation]\n+        self.drop = nn.Dropout(config.mlp_dropout)\n+        self.Wo = nn.Linear(config.intermediate_size, config.hidden_size, bias=config.mlp_bias)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        input, gate = self.Wi(hidden_states).chunk(2, dim=-1)\n+        return self.Wo(self.drop(self.act(input) * gate))\n+\n+\n+class ModernBertRotaryEmbedding(GemmaRotaryEmbedding):\n+    pass\n+\n+\n+def eager_attention_forward(\n+    module: \"ModernBertAttention\",\n+    qkv: torch.Tensor,\n+    attention_mask: torch.Tensor,\n+    sliding_window_mask: torch.Tensor,\n+    position_ids: Optional[torch.LongTensor],\n+    local_attention: Tuple[int, int],\n+    bs: int,\n+    dim: int,\n+    output_attentions: Optional[bool] = False,\n+    **_kwargs,\n+) -> Tuple[torch.Tensor, torch.Tensor] | Tuple[torch.Tensor]:\n+    # qkv: [batch_size, seqlen, 3, nheads, headdim]\n+    cos, sin = module.rotary_emb(qkv, position_ids=position_ids)\n+    query, key, value = qkv.transpose(3, 1).unbind(dim=2)\n+    # query, key, value: [batch_size, heads, seq_len, head_dim]\n+    query, key = apply_rotary_pos_emb(query, key, cos, sin)\n+\n+    scale = module.head_dim**-0.5\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scale\n+\n+    if local_attention != (-1, -1):\n+        attention_mask = sliding_window_mask\n+\n+    attn_weights = attn_weights + attention_mask\n+\n+    # upcast attention to fp32\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=module.attention_dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+    attn_output = attn_output.view(bs, -1, dim)\n+    if output_attentions:\n+        return (attn_output, attn_weights)\n+    return (attn_output,)\n+\n+\n+def flash_attention_forward(\n+    module: \"ModernBertAttention\",\n+    qkv: torch.Tensor,\n+    rotary_emb: ModernBertUnpaddedRotaryEmbedding,\n+    cu_seqlens: torch.Tensor,\n+    max_seqlen: int,\n+    local_attention: Tuple[int, int],\n+    bs: int,\n+    dim: int,\n+    target_dtype: torch.dtype = torch.bfloat16,\n+    **_kwargs,\n+) -> Tuple[torch.Tensor]:\n+    # (total_seqlen, 3, nheads, headdim)\n+    qkv = rotary_emb(qkv, cu_seqlens=cu_seqlens, max_seqlen=max_seqlen)\n+\n+    convert_dtype = qkv.dtype not in (torch.float16, torch.bfloat16)\n+    if convert_dtype:\n+        # FA2 implementation only supports fp16 and bf16. If FA2 is supported,\n+        # bfloat16 must be supported as of FA2 2.5.7. (Turing GPUs not supported)\n+        orig_dtype = qkv.dtype\n+        qkv = qkv.to(target_dtype)\n+\n+        attn = flash_attn_varlen_qkvpacked_func(\n+            qkv,\n+            cu_seqlens=cu_seqlens,\n+            max_seqlen=max_seqlen,\n+            dropout_p=module.attention_dropout if module.training else 0.0,\n+            deterministic=module.deterministic_flash_attn,\n+            window_size=local_attention,\n+        )\n+        attn = attn.to(orig_dtype)  # type: ignore\n+    else:\n+        attn = flash_attn_varlen_qkvpacked_func(\n+            qkv,\n+            cu_seqlens=cu_seqlens,\n+            max_seqlen=max_seqlen,\n+            dropout_p=module.attention_dropout if module.training else 0.0,\n+            deterministic=module.deterministic_flash_attn,\n+            window_size=local_attention,\n+        )\n+    return (attn.view(bs, dim),)\n+\n+\n+def sdpa_attention_forward(\n+    module: \"ModernBertAttention\",\n+    qkv: torch.Tensor,\n+    attention_mask: torch.Tensor,\n+    sliding_window_mask: torch.Tensor,\n+    position_ids: Optional[torch.LongTensor],\n+    local_attention: Tuple[int, int],\n+    bs: int,\n+    dim: int,\n+    **_kwargs,\n+) -> Tuple[torch.Tensor]:\n+    # qkv: [batch_size, seqlen, 3, nheads, headdim]\n+    cos, sin = module.rotary_emb(qkv, position_ids=position_ids)\n+    query, key, value = qkv.transpose(3, 1).unbind(dim=2)\n+    # query, key, value: [batch_size, heads, seq_len, head_dim]\n+    query, key = apply_rotary_pos_emb(query, key, cos, sin)\n+\n+    if local_attention != (-1, -1):\n+        attention_mask = sliding_window_mask\n+\n+    attn_output = (\n+        F.scaled_dot_product_attention(\n+            query,\n+            key,\n+            value,\n+            dropout_p=module.attention_dropout if module.training else 0.0,\n+            attn_mask=attention_mask,\n+        )\n+        .transpose(1, 2)\n+        .contiguous()\n+    )\n+    attn_output = attn_output.view(bs, -1, dim)\n+    return (attn_output,)\n+\n+\n+MODERNBERT_ATTENTION_FUNCTION = {\n+    \"flash_attention_2\": flash_attention_forward,\n+    \"eager\": eager_attention_forward,\n+    \"sdpa\": sdpa_attention_forward,\n+}\n+\n+\n+class ModernBertAttention(nn.Module):\n+    \"\"\"Performs multi-headed self attention on a batch of unpadded sequences.\n+\n+    If Flash Attention 2 is installed, this module uses Flash Attention to improve throughput.\n+    If Flash Attention 2 is not installed, the implementation will use PyTorch's SDPA kernel,\n+    which requires padding and unpadding inputs, adding some overhead.\n+\n+    See `forward` method for additional details.\n+    \"\"\"\n+\n+    def __init__(self, config: ModernBertConfig, layer_id: Optional[int] = None):\n+        super().__init__()\n+        self.config = config\n+        self.layer_id = layer_id\n+\n+        if config.hidden_size % config.num_attention_heads != 0:\n+            raise ValueError(\n+                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention heads ({config.num_attention_heads})\"\n+            )\n+\n+        self.attention_dropout = config.attention_dropout\n+        self.deterministic_flash_attn = config.deterministic_flash_attn\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = config.hidden_size // config.num_attention_heads\n+        self.all_head_size = self.head_dim * self.num_heads\n+        self.Wqkv = nn.Linear(config.hidden_size, 3 * self.all_head_size, bias=config.attention_bias)\n+\n+        if layer_id % config.global_attn_every_n_layers != 0:\n+            self.local_attention = (config.local_attention // 2, config.local_attention // 2)\n+        else:\n+            self.local_attention = (-1, -1)\n+\n+        rope_theta = config.global_rope_theta\n+        max_position_embeddings = config.max_position_embeddings\n+        if self.local_attention != (-1, -1):\n+            if config.local_rope_theta is not None:\n+                rope_theta = config.local_rope_theta\n+            max_position_embeddings = config.local_attention\n+\n+        if config._attn_implementation == \"flash_attention_2\":\n+            self.rotary_emb = ModernBertUnpaddedRotaryEmbedding(\n+                dim=self.head_dim, max_seqlen=max_position_embeddings, base=rope_theta\n+            )\n+        else:\n+            self.rotary_emb = ModernBertRotaryEmbedding(\n+                dim=self.head_dim, max_position_embeddings=max_position_embeddings, base=rope_theta\n+            )\n+\n+        self.Wo = nn.Linear(config.hidden_size, config.hidden_size, bias=config.attention_bias)\n+        self.out_drop = nn.Dropout(config.attention_dropout) if config.attention_dropout > 0.0 else nn.Identity()\n+        self.pruned_heads = set()\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        output_attentions: Optional[bool] = False,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        qkv = self.Wqkv(hidden_states)\n+\n+        bs = hidden_states.shape[0]\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            qkv = qkv.view(-1, 3, self.num_heads, self.head_dim)\n+        else:\n+            qkv = qkv.view(bs, -1, 3, self.num_heads, self.head_dim)\n+\n+        attn_outputs = MODERNBERT_ATTENTION_FUNCTION[self.config._attn_implementation](\n+            self,\n+            qkv=qkv,\n+            rotary_emb=self.rotary_emb,\n+            local_attention=self.local_attention,\n+            bs=bs,\n+            dim=self.all_head_size,\n+            output_attentions=output_attentions,\n+            **kwargs,\n+        )\n+        hidden_states = attn_outputs[0]\n+        hidden_states = self.out_drop(self.Wo(hidden_states))\n+\n+        return (hidden_states,) + attn_outputs[1:]  # add attentions if outputted\n+\n+\n+class ModernBertEncoderLayer(nn.Module):\n+    def __init__(self, config: ModernBertConfig, layer_id: Optional[int] = None):\n+        super().__init__()\n+        self.config = config\n+        if layer_id == 0:\n+            self.attn_norm = nn.Identity()\n+        else:\n+            self.attn_norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)\n+        self.attn = ModernBertAttention(config=config, layer_id=layer_id)\n+        self.mlp_norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)\n+        self.mlp = ModernBertMLP(config)\n+\n+    @torch.compile(dynamic=True)\n+    def compiled_mlp(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        return self.mlp(self.mlp_norm(hidden_states))\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        sliding_window_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        cu_seqlens: Optional[torch.Tensor] = None,\n+        max_seqlen: Optional[int] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> torch.Tensor:\n+        attn_outputs = self.attn(\n+            self.attn_norm(hidden_states),\n+            attention_mask=attention_mask,\n+            sliding_window_mask=sliding_window_mask,\n+            position_ids=position_ids,\n+            cu_seqlens=cu_seqlens,\n+            max_seqlen=max_seqlen,\n+            output_attentions=output_attentions,\n+        )\n+        hidden_states = hidden_states + attn_outputs[0]\n+        mlp_output = (\n+            self.compiled_mlp(hidden_states)\n+            if self.config.reference_compile\n+            else self.mlp(self.mlp_norm(hidden_states))\n+        )\n+        hidden_states = hidden_states + mlp_output\n+\n+        return (hidden_states,) + attn_outputs[1:]  # add attentions if outputted\n+\n+\n+MODERNBERT_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`ModernBertConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare ModernBert Model outputting raw hidden-states without any specific head on top.\",\n+    MODERNBERT_START_DOCSTRING,\n+)\n+class ModernBertPreTrainedModel(PreTrainedModel):\n+    config_class = ModernBertConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"ModernBertEmbeddings\", \"ModernBertEncoderLayer\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = False\n+\n+    def _init_weights(self, module: nn.Module):\n+        cutoff_factor = self.config.initializer_cutoff_factor\n+        if cutoff_factor is None:\n+            cutoff_factor = 3\n+\n+        def init_weight(module: nn.Module, std: float):\n+            nn.init.trunc_normal_(\n+                module.weight,\n+                mean=0.0,\n+                std=std,\n+                a=-cutoff_factor * std,\n+                b=cutoff_factor * std,\n+            )\n+\n+            if isinstance(module, nn.Linear):\n+                if module.bias is not None:\n+                    nn.init.zeros_(module.bias)\n+\n+        stds = {\n+            \"in\": self.config.initializer_range,\n+            \"out\": self.config.initializer_range / math.sqrt(2.0 * self.config.num_hidden_layers),\n+            \"embedding\": self.config.initializer_range,\n+            \"final_out\": self.config.hidden_size**-0.5,\n+        }\n+\n+        if isinstance(module, ModernBertEmbeddings):\n+            init_weight(module.tok_embeddings, stds[\"embedding\"])\n+        elif isinstance(module, ModernBertMLP):\n+            init_weight(module.Wi, stds[\"in\"])\n+            init_weight(module.Wo, stds[\"out\"])\n+        elif isinstance(module, ModernBertAttention):\n+            init_weight(module.Wqkv, stds[\"in\"])\n+            init_weight(module.Wo, stds[\"out\"])\n+        elif isinstance(module, ModernBertPredictionHead):\n+            init_weight(module.dense, stds[\"in\"])\n+        elif isinstance(module, ModernBertPoolingHead):\n+            init_weight(module.dense, stds[\"out\"])\n+        elif isinstance(module, ModernBertForMaskedLM):\n+            init_weight(module.decoder, stds[\"out\"])\n+        elif isinstance(module, (ModernBertForSequenceClassification, ModernBertForTokenClassification)):\n+            init_weight(module.classifier, stds[\"final_out\"])\n+\n+    @classmethod\n+    def _autoset_attn_implementation(\n+        cls,\n+        config,\n+        use_flash_attention_2: bool = False,\n+        torch_dtype: Optional[torch.dtype] = None,\n+        device_map: Optional[Union[str, Dict[str, int]]] = None,\n+        check_device_map: bool = True,\n+    ):\n+        # If the user didn't specify anything, try to use flash_attention_2 if available.\n+        # Otherwise we fall back to the default SDPA -> Eager from the super() method.\n+        if config._attn_implementation_internal is None:\n+            config._attn_implementation_internal = \"flash_attention_2\"\n+            try:\n+                return cls._check_and_enable_flash_attn_2(\n+                    config,\n+                    torch_dtype=torch_dtype,\n+                    device_map=device_map,\n+                    hard_check_only=False,\n+                    check_device_map=check_device_map,\n+                )\n+            except (ValueError, ImportError):\n+                config._attn_implementation_internal = None\n+        return super()._autoset_attn_implementation(\n+            config,\n+            use_flash_attention_2=use_flash_attention_2,\n+            torch_dtype=torch_dtype,\n+            device_map=device_map,\n+            check_device_map=check_device_map,\n+        )\n+\n+    def _maybe_set_compile(self):\n+        if self.config.reference_compile is False:\n+            return\n+\n+        if hasattr(self, \"hf_device_map\") and len(self.hf_device_map) > 1:\n+            if self.config.reference_compile:\n+                logger.warning_once(\n+                    \"If `accelerate` split the model across devices, `torch.compile` will not work. \"\n+                    \"Falling back to non-compiled mode.\"\n+                )\n+            self.config.reference_compile = False\n+\n+        if self.device.type == \"mps\":\n+            if self.config.reference_compile:\n+                logger.warning_once(\n+                    \"Compiling the model with `torch.compile` and using a `torch.mps` device is not supported. \"\n+                    \"Falling back to non-compiled mode.\"\n+                )\n+            self.config.reference_compile = False\n+\n+        if self.config.reference_compile is None:\n+            self.config.reference_compile = is_triton_available()\n+\n+    def resize_token_embeddings(self, *args, **kwargs):\n+        model_embeds = super().resize_token_embeddings(*args, **kwargs)\n+\n+        if self.config.reference_compile in {True, None}:\n+            if self.config.reference_compile:\n+                logger.warning_once(\n+                    \"Resizing token embeddings with `torch.compile` is not supported. Falling back to non-compiled mode.\"\n+                )\n+            self.config.reference_compile = False\n+\n+        return model_embeds\n+\n+\n+MODERNBERT_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        sliding_window_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding or far-away tokens. In ModernBert, only every few layers\n+            perform global attention, while the rest perform local attention. This mask is used to avoid attending to\n+            far-away tokens in the local attention layers.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        indices (`torch.Tensor` of shape `(total_unpadded_tokens,)`, *optional*):\n+            Indices of the non-padding tokens in the input sequence. Used for unpadding the output.\n+        cu_seqlens (`torch.Tensor` of shape `(batch + 1,)`, *optional*):\n+            Cumulative sequence lengths of the input sequences. Used to index the unpadded tensors.\n+        max_seqlen (`int`, *optional*):\n+            Maximum sequence length in the batch. Used to pad the output tensors.\n+        batch_size (`int`, *optional*):\n+            Batch size of the input sequences. Used to pad the output tensors.\n+        seq_len (`int`, *optional*):\n+            Sequence length of the input sequences. Used to pad the output tensors.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare ModernBert Model outputting raw hidden-states without any specific head on top.\",\n+    MODERNBERT_START_DOCSTRING,\n+)\n+class ModernBertModel(ModernBertPreTrainedModel):\n+    def __init__(self, config: ModernBertConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.embeddings = ModernBertEmbeddings(config)\n+        self.layers = nn.ModuleList(\n+            [ModernBertEncoderLayer(config, layer_id) for layer_id in range(config.num_hidden_layers)]\n+        )\n+        self.final_norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)\n+        self.gradient_checkpointing = False\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embeddings.tok_embeddings\n+\n+    def set_input_embeddings(self, value):\n+        self.embeddings.tok_embeddings = value\n+\n+    @add_start_docstrings_to_model_forward(MODERNBERT_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=BaseModelOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        sliding_window_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        indices: Optional[torch.Tensor] = None,\n+        cu_seqlens: Optional[torch.Tensor] = None,\n+        max_seqlen: Optional[int] = None,\n+        batch_size: Optional[int] = None,\n+        seq_len: Optional[int] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple[torch.Tensor, ...], BaseModelOutput]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attentions = () if output_attentions else None\n+\n+        self._maybe_set_compile()\n+        self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n+\n+        if batch_size is None and seq_len is None:\n+            batch_size, seq_len = input_ids.shape[:2]\n+\n+        if attention_mask is None:\n+            attention_mask = torch.ones((batch_size, seq_len), device=input_ids.device, dtype=torch.bool)\n+\n+        repad = False\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if indices is None and cu_seqlens is None and max_seqlen is None:\n+                repad = True\n+                with torch.no_grad():\n+                    input_ids, indices, cu_seqlens, max_seqlen, *_ = _unpad_modernbert_input(\n+                        inputs=input_ids, attention_mask=attention_mask\n+                    )\n+        else:\n+            if position_ids is None:\n+                position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n+\n+            attention_mask, sliding_window_mask = self._update_attention_mask(\n+                attention_mask, output_attentions=output_attentions\n+            )\n+\n+        hidden_states = self.embeddings(input_ids)\n+\n+        for encoder_layer in self.layers:\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    encoder_layer.__call__,\n+                    hidden_states,\n+                    attention_mask,\n+                    sliding_window_mask,\n+                    position_ids,\n+                    cu_seqlens,\n+                    max_seqlen,\n+                    output_attentions,\n+                )\n+            else:\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask=attention_mask,\n+                    sliding_window_mask=sliding_window_mask,\n+                    position_ids=position_ids,\n+                    cu_seqlens=cu_seqlens,\n+                    max_seqlen=max_seqlen,\n+                    output_attentions=output_attentions,\n+                )\n+            hidden_states = layer_outputs[0]\n+            if output_attentions and len(layer_outputs) > 1:\n+                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+        hidden_states = self.final_norm(hidden_states)\n+\n+        if repad:\n+            hidden_states = _pad_modernbert_output(\n+                inputs=hidden_states, indices=indices, batch=batch_size, seqlen=seq_len\n+            )\n+            if all_hidden_states is not None:\n+                all_hidden_states = tuple(\n+                    _pad_modernbert_output(inputs=hs, indices=indices, batch=batch_size, seqlen=seq_len)\n+                    for hs in all_hidden_states\n+                )\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attentions,\n+        )\n+\n+    def _update_attention_mask(self, attention_mask: torch.Tensor, output_attentions: bool) -> torch.Tensor:\n+        if output_attentions:\n+            if self.config._attn_implementation == \"sdpa\":\n+                logger.warning_once(\n+                    \"Outputting attentions is only supported with the 'eager' attention implementation, \"\n+                    'not with \"sdpa\". Falling back to `attn_implementation=\"eager\"`.'\n+                )\n+                self.config._attn_implementation = \"eager\"\n+            elif self.config._attn_implementation != \"eager\":\n+                logger.warning_once(\n+                    \"Outputting attentions is only supported with the eager attention implementation, \"\n+                    f'not with {self.config._attn_implementation}. Consider setting `attn_implementation=\"eager\"`.'\n+                    \" Setting `output_attentions=False`.\"\n+                )\n+\n+        global_attention_mask = _prepare_4d_attention_mask(attention_mask, self.dtype)\n+\n+        # Create position indices\n+        rows = torch.arange(global_attention_mask.shape[2]).unsqueeze(0)\n+        # Calculate distance between positions\n+        distance = torch.abs(rows - rows.T)\n+\n+        # Create sliding window mask (1 for positions within window, 0 outside)\n+        window_mask = (\n+            (distance <= self.config.local_attention // 2).unsqueeze(0).unsqueeze(0).to(attention_mask.device)\n+        )\n+        # Combine with existing mask\n+        sliding_window_mask = global_attention_mask.masked_fill(window_mask.logical_not(), torch.finfo(self.dtype).min)\n+\n+        return global_attention_mask, sliding_window_mask\n+\n+\n+class ModernBertPredictionHead(nn.Module):\n+    def __init__(self, config: ModernBertConfig):\n+        super().__init__()\n+        self.config = config\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size, config.classifier_bias)\n+        self.act = ACT2FN[config.classifier_activation]\n+        self.norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        return self.norm(self.act(self.dense(hidden_states)))\n+\n+\n+@add_start_docstrings(\n+    \"The ModernBert Model with a decoder head on top that is used for masked language modeling.\",\n+    MODERNBERT_START_DOCSTRING,\n+)\n+class ModernBertForMaskedLM(ModernBertPreTrainedModel):\n+    _tied_weights_keys = [\"decoder.weight\"]\n+\n+    def __init__(self, config: ModernBertConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.model = ModernBertModel(config)\n+        self.head = ModernBertPredictionHead(config)\n+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=config.decoder_bias)\n+\n+        self.sparse_prediction = self.config.sparse_prediction\n+        self.sparse_pred_ignore_index = self.config.sparse_pred_ignore_index\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_output_embeddings(self):\n+        return self.decoder\n+\n+    def set_output_embeddings(self, new_embeddings: nn.Linear):\n+        self.decoder = new_embeddings\n+\n+    @torch.compile(dynamic=True)\n+    def compiled_head(self, output: torch.Tensor) -> torch.Tensor:\n+        return self.decoder(self.head(output))\n+\n+    @add_start_docstrings_to_model_forward(MODERNBERT_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=MaskedLMOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        sliding_window_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        indices: Optional[torch.Tensor] = None,\n+        cu_seqlens: Optional[torch.Tensor] = None,\n+        max_seqlen: Optional[int] = None,\n+        batch_size: Optional[int] = None,\n+        seq_len: Optional[int] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        **kwargs,\n+    ) -> Union[Tuple[torch.Tensor], MaskedLMOutput]:\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        self._maybe_set_compile()\n+\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if indices is None and cu_seqlens is None and max_seqlen is None:\n+                batch_size, seq_len = input_ids.shape[:2]\n+                if attention_mask is None:\n+                    attention_mask = torch.ones((batch_size, seq_len), device=input_ids.device, dtype=torch.bool)\n+                with torch.no_grad():\n+                    input_ids, indices, cu_seqlens, max_seqlen, position_ids, labels = _unpad_modernbert_input(\n+                        inputs=input_ids, attention_mask=attention_mask, position_ids=position_ids, labels=labels\n+                    )\n+\n+        outputs = self.model(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            sliding_window_mask=sliding_window_mask,\n+            position_ids=position_ids,\n+            indices=indices,\n+            cu_seqlens=cu_seqlens,\n+            max_seqlen=max_seqlen,\n+            batch_size=batch_size,\n+            seq_len=seq_len,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        last_hidden_state = outputs[0]\n+\n+        if self.sparse_prediction and labels is not None:\n+            # flatten labels and output first\n+            labels = labels.view(-1)\n+            last_hidden_state = last_hidden_state.view(labels.shape[0], -1)\n+\n+            # then filter out the non-masked tokens\n+            mask_tokens = labels != self.sparse_pred_ignore_index\n+            last_hidden_state = last_hidden_state[mask_tokens]\n+            labels = labels[mask_tokens]\n+\n+        logits = (\n+            self.compiled_head(last_hidden_state)\n+            if self.config.reference_compile\n+            else self.decoder(self.head(last_hidden_state))\n+        )\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits, labels, vocab_size=self.config.vocab_size)\n+\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            with torch.no_grad():\n+                logits = _pad_modernbert_output(inputs=logits, indices=indices, batch=batch_size, seqlen=seq_len)\n+        if not return_dict:\n+            output = (logits,)\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return MaskedLMOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+class ModernBertPoolingHead(nn.Module):\n+    def __init__(self, config: ModernBertConfig):\n+        super().__init__()\n+        self.config = config\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size, config.classifier_bias)\n+        self.act = ACT2FN[config.classifier_activation]\n+        self.norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)\n+        self.drop = torch.nn.Dropout(config.classifier_dropout)\n+\n+    def forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n+        if self.config.classifier_pooling == \"cls\":\n+            hidden_states = hidden_states[:, 0]\n+        elif self.config.classifier_pooling == \"mean\":\n+            hidden_states = (hidden_states * attention_mask.unsqueeze(-1)).sum(dim=1) / attention_mask.sum(\n+                dim=1, keepdim=True\n+            )\n+\n+        return self.drop(self.norm(self.act(self.dense(hidden_states))))\n+\n+\n+@add_start_docstrings(\n+    \"The ModernBert Model with a sequence classification head on top that performs pooling.\",\n+    MODERNBERT_START_DOCSTRING,\n+)\n+class ModernBertForSequenceClassification(ModernBertPreTrainedModel):\n+    def __init__(self, config: ModernBertConfig):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+        self.config = config\n+\n+        self.model = ModernBertModel(config)\n+        self.head = ModernBertPoolingHead(config)\n+        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(MODERNBERT_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=SequenceClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        sliding_window_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        indices: Optional[torch.Tensor] = None,\n+        cu_seqlens: Optional[torch.Tensor] = None,\n+        max_seqlen: Optional[int] = None,\n+        batch_size: Optional[int] = None,\n+        seq_len: Optional[int] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        **kwargs,\n+    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        self._maybe_set_compile()\n+\n+        outputs = self.model(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            sliding_window_mask=sliding_window_mask,\n+            position_ids=position_ids,\n+            indices=indices,\n+            cu_seqlens=cu_seqlens,\n+            max_seqlen=max_seqlen,\n+            batch_size=batch_size,\n+            seq_len=seq_len,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        last_hidden_state = outputs[0]\n+\n+        pooled_output = self.head(last_hidden_state, attention_mask)\n+        logits = self.classifier(pooled_output)\n+\n+        loss = None\n+        if labels is not None:\n+            if self.config.problem_type is None:\n+                if self.num_labels == 1:\n+                    self.config.problem_type = \"regression\"\n+                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n+                    self.config.problem_type = \"single_label_classification\"\n+                else:\n+                    self.config.problem_type = \"multi_label_classification\"\n+\n+            if self.config.problem_type == \"regression\":\n+                loss_fct = MSELoss()\n+                if self.num_labels == 1:\n+                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n+                else:\n+                    loss = loss_fct(logits, labels)\n+            elif self.config.problem_type == \"single_label_classification\":\n+                loss_fct = CrossEntropyLoss()\n+                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            elif self.config.problem_type == \"multi_label_classification\":\n+                loss_fct = BCEWithLogitsLoss()\n+                loss = loss_fct(logits, labels)\n+\n+        if not return_dict:\n+            output = (logits,)\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return SequenceClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+@add_start_docstrings(\n+    \"The ModernBert Model with a token classification head on top, e.g. for Named Entity Recognition (NER) tasks.\",\n+    MODERNBERT_START_DOCSTRING,\n+)\n+class ModernBertForTokenClassification(ModernBertPreTrainedModel):\n+    def __init__(self, config: ModernBertConfig):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+\n+        self.model = ModernBertModel(config)\n+        self.drop = nn.Dropout(config.classifier_dropout)\n+        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(MODERNBERT_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=TokenClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        sliding_window_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        indices: Optional[torch.Tensor] = None,\n+        cu_seqlens: Optional[torch.Tensor] = None,\n+        max_seqlen: Optional[int] = None,\n+        batch_size: Optional[int] = None,\n+        seq_len: Optional[int] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        self._maybe_set_compile()\n+\n+        outputs = self.model(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            sliding_window_mask=sliding_window_mask,\n+            position_ids=position_ids,\n+            indices=indices,\n+            cu_seqlens=cu_seqlens,\n+            max_seqlen=max_seqlen,\n+            batch_size=batch_size,\n+            seq_len=seq_len,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        last_hidden_state = outputs[0]\n+\n+        last_hidden_state = self.drop(last_hidden_state)\n+        logits = self.classifier(last_hidden_state)\n+\n+        loss = None\n+        if labels is not None:\n+            loss_fct = CrossEntropyLoss()\n+            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return TokenClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\n+    \"ModernBertConfig\",\n+    \"ModernBertModel\",\n+    \"ModernBertPreTrainedModel\",\n+    \"ModernBertForMaskedLM\",\n+    \"ModernBertForSequenceClassification\",\n+    \"ModernBertForTokenClassification\",\n+]"
        },
        {
            "sha": "e3463461ea07e5589229d7510c948f2998d932f2",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=667ed5635e6fd7e2df4fc23012746b1c0cbb7575",
            "patch": "@@ -6418,6 +6418,41 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class ModernBertForMaskedLM(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class ModernBertForSequenceClassification(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class ModernBertForTokenClassification(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class ModernBertModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class ModernBertPreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class MoshiForCausalLM(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "92823a4ee016c3c09670fa10906fac3380b3bec3",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=667ed5635e6fd7e2df4fc23012746b1c0cbb7575",
            "patch": "@@ -192,7 +192,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n _tiktoken_available = _is_package_available(\"tiktoken\")\n _blobfile_available = _is_package_available(\"blobfile\")\n _liger_kernel_available = _is_package_available(\"liger_kernel\")\n-\n+_triton_available = _is_package_available(\"triton\")\n \n _torch_version = \"N/A\"\n _torch_available = False\n@@ -1243,6 +1243,10 @@ def is_liger_kernel_available():\n     return version.parse(importlib.metadata.version(\"liger_kernel\")) >= version.parse(\"0.3.0\")\n \n \n+def is_triton_available():\n+    return _triton_available\n+\n+\n # docstyle-ignore\n AV_IMPORT_ERROR = \"\"\"\n {0} requires the PyAv library but it was not found in your environment. You can install it with:"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/modernbert/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/tests%2Fmodels%2Fmodernbert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/tests%2Fmodels%2Fmodernbert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmodernbert%2F__init__.py?ref=667ed5635e6fd7e2df4fc23012746b1c0cbb7575"
        },
        {
            "sha": "4fce0cd86352f09755854fd5b82ccca2d5ee0828",
            "filename": "tests/models/modernbert/test_modeling_modernbert.py",
            "status": "added",
            "additions": 367,
            "deletions": 0,
            "changes": 367,
            "blob_url": "https://github.com/huggingface/transformers/blob/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py?ref=667ed5635e6fd7e2df4fc23012746b1c0cbb7575",
            "patch": "@@ -0,0 +1,367 @@\n+# coding=utf-8\n+# Copyright 2020 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import os\n+import unittest\n+\n+import pytest\n+\n+from transformers import ModernBertConfig, is_torch_available\n+from transformers.models.auto import get_values\n+from transformers.testing_utils import (\n+    CaptureLogger,\n+    require_flash_attn,\n+    require_torch,\n+    require_torch_gpu,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init, ids_tensor, random_attention_mask\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        MODEL_FOR_PRETRAINING_MAPPING,\n+        ModernBertForMaskedLM,\n+        ModernBertForSequenceClassification,\n+        ModernBertForTokenClassification,\n+        ModernBertModel,\n+        logging,\n+    )\n+\n+\n+class ModernBertModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=13,\n+        seq_length=7,\n+        is_training=True,\n+        use_input_mask=True,\n+        use_labels=True,\n+        vocab_size=99,\n+        pad_token_id=0,\n+        hidden_size=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        intermediate_size=37,\n+        hidden_activation=\"gelu\",\n+        mlp_dropout=0.0,\n+        attention_dropout=0.0,\n+        embedding_dropout=0.0,\n+        classifier_dropout=0.0,\n+        max_position_embeddings=512,\n+        type_vocab_size=16,\n+        type_sequence_label_size=2,\n+        initializer_range=0.02,\n+        num_labels=3,\n+        num_choices=4,\n+        scope=None,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.seq_length = seq_length\n+        self.is_training = is_training\n+        self.use_input_mask = use_input_mask\n+        self.use_labels = use_labels\n+        self.vocab_size = vocab_size\n+        self.pad_token_id = pad_token_id\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.hidden_activation = hidden_activation\n+        self.mlp_dropout = mlp_dropout\n+        self.attention_dropout = attention_dropout\n+        self.embedding_dropout = embedding_dropout\n+        self.classifier_dropout = classifier_dropout\n+        self.max_position_embeddings = max_position_embeddings\n+        self.type_vocab_size = type_vocab_size\n+        self.type_sequence_label_size = type_sequence_label_size\n+        self.initializer_range = initializer_range\n+        self.num_labels = num_labels\n+        self.num_choices = num_choices\n+        self.scope = scope\n+\n+    def prepare_config_and_inputs(self):\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+\n+        input_mask = None\n+        if self.use_input_mask:\n+            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n+\n+        sequence_labels = None\n+        token_labels = None\n+        choice_labels = None\n+        if self.use_labels:\n+            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n+            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n+            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n+\n+        config = self.get_config()\n+\n+        return config, input_ids, input_mask, sequence_labels, token_labels, choice_labels\n+\n+    def get_config(self):\n+        \"\"\"\n+        Returns a tiny configuration by default.\n+        \"\"\"\n+        config = ModernBertConfig(\n+            vocab_size=self.vocab_size,\n+            pad_token_id=self.pad_token_id,\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            hidden_activation=self.hidden_activation,\n+            mlp_dropout=self.mlp_dropout,\n+            attention_dropout=self.attention_dropout,\n+            embedding_dropout=self.embedding_dropout,\n+            classifier_dropout=self.classifier_dropout,\n+            max_position_embeddings=self.max_position_embeddings,\n+            type_vocab_size=self.type_vocab_size,\n+            is_decoder=False,\n+            initializer_range=self.initializer_range,\n+        )\n+        if test := os.environ.get(\"PYTEST_CURRENT_TEST\", False):\n+            test_name = test.split(\":\")[-1].split(\" \")[0]\n+\n+            # If we're testing `test_retain_grad_hidden_states_attentions`, we normally get an error\n+            # that compilation doesn't work. Users can then set compile=False when loading the model,\n+            # much like here. We're testing whether it works once they've done that.\n+            if test_name == \"test_retain_grad_hidden_states_attentions\":\n+                config.reference_compile = False\n+            # Some tests require attentions to be outputted, in that case we'll set the attention implementation to eager\n+            # as the others don't support outputted attentions\n+            if test_name in (\n+                \"test_attention_outputs\",\n+                \"test_hidden_states_output\",\n+                \"test_retain_grad_hidden_states_attentions\",\n+            ):\n+                config._attn_implementation = \"eager\"\n+        return config\n+\n+    def create_and_check_model(self, config, input_ids, input_mask, sequence_labels, token_labels, choice_labels):\n+        model = ModernBertModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(input_ids, attention_mask=input_mask)\n+        result = model(input_ids)\n+        result = model(input_ids)\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n+\n+    def create_and_check_for_masked_lm(\n+        self, config, input_ids, input_mask, sequence_labels, token_labels, choice_labels\n+    ):\n+        model = ModernBertForMaskedLM(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n+        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n+\n+    def create_and_check_for_sequence_classification(\n+        self, config, input_ids, input_mask, sequence_labels, token_labels, choice_labels\n+    ):\n+        config.num_labels = self.num_labels\n+        model = ModernBertForSequenceClassification(config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(input_ids, attention_mask=input_mask, labels=sequence_labels)\n+        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n+\n+    def create_and_check_for_token_classification(\n+        self, config, input_ids, input_mask, sequence_labels, token_labels, choice_labels\n+    ):\n+        config.num_labels = self.num_labels\n+        model = ModernBertForTokenClassification(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n+        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.num_labels))\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        (\n+            config,\n+            input_ids,\n+            input_mask,\n+            sequence_labels,\n+            token_labels,\n+            choice_labels,\n+        ) = config_and_inputs\n+        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class ModernBertModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    test_torchscript = False\n+\n+    all_model_classes = (\n+        (\n+            ModernBertModel,\n+            ModernBertForMaskedLM,\n+            ModernBertForSequenceClassification,\n+            ModernBertForTokenClassification,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+    all_generative_model_classes = ()\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": ModernBertModel,\n+            \"fill-mask\": ModernBertForMaskedLM,\n+            \"text-classification\": ModernBertForSequenceClassification,\n+            \"token-classification\": ModernBertForTokenClassification,\n+            \"zero-shot\": ModernBertForSequenceClassification,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    fx_compatible = False\n+    test_head_masking = False\n+    test_pruning = False\n+    model_split_percents = [0.5, 0.8, 0.9]\n+\n+    # special case for ForPreTraining model\n+    def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n+        inputs_dict = super()._prepare_for_class(inputs_dict, model_class, return_labels=return_labels)\n+\n+        if inputs_dict.get(\"output_attentions\", False):\n+            inputs_dict[\"output_attentions\"] = True\n+\n+        if return_labels:\n+            if model_class in get_values(MODEL_FOR_PRETRAINING_MAPPING):\n+                inputs_dict[\"labels\"] = torch.zeros(\n+                    (self.model_tester.batch_size, self.model_tester.seq_length), dtype=torch.long, device=torch_device\n+                )\n+                inputs_dict[\"next_sentence_label\"] = torch.zeros(\n+                    self.model_tester.batch_size, dtype=torch.long, device=torch_device\n+                )\n+        return inputs_dict\n+\n+    def setUp(self):\n+        self.model_tester = ModernBertModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=ModernBertConfig, hidden_size=37)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_model_various_embeddings(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n+            config_and_inputs[0].position_embedding_type = type\n+            self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_initialization(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        configs_no_init = _config_zero_init(config)\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            for name, param in model.named_parameters():\n+                # The classifier.weight from ModernBertForSequenceClassification and ModernBertForTokenClassification\n+                # are initialized without `initializer_range`, so they're not set to ~0 via the _config_zero_init\n+                if param.requires_grad and not (\n+                    name == \"classifier.weight\"\n+                    and model_class in [ModernBertForSequenceClassification, ModernBertForTokenClassification]\n+                ):\n+                    self.assertIn(\n+                        ((param.data.mean() * 1e9).round() / 1e9).item(),\n+                        [0.0, 1.0],\n+                        msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                    )\n+\n+    @unittest.skip(\"ModernBert doesn't use `inputs_embeds` as input.\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    def test_for_masked_lm(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)\n+\n+    def test_for_sequence_classification(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_for_sequence_classification(*config_and_inputs)\n+\n+    def test_for_token_classification(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_for_token_classification(*config_and_inputs)\n+\n+    def test_for_warning_if_padding_and_no_attention_mask(self):\n+        (\n+            config,\n+            input_ids,\n+            input_mask,\n+            sequence_labels,\n+            token_labels,\n+            choice_labels,\n+        ) = self.model_tester.prepare_config_and_inputs()\n+\n+        # Set pad tokens in the input_ids\n+        input_ids[0, 0] = config.pad_token_id\n+\n+        # Check for warnings if the attention_mask is missing.\n+        logger = logging.get_logger(\"transformers.modeling_utils\")\n+        # clear cache so we can test the warning is emitted (from `warning_once`).\n+        logger.warning_once.cache_clear()\n+\n+        with CaptureLogger(logger) as cl:\n+            model = ModernBertModel(config=config)\n+            model.to(torch_device)\n+            model.eval()\n+            model(input_ids, attention_mask=None)\n+        self.assertIn(\"We strongly recommend passing in an `attention_mask`\", cl.out)\n+\n+    @unittest.skip(\"ModernBert doesn't use separate classes for SDPA, but a function instead.\")\n+    def test_sdpa_can_dispatch_non_composite_models(self):\n+        pass\n+\n+    @slow\n+    def test_model_from_pretrained(self):\n+        model_name = \"google-bert/bert-base-uncased\"\n+        model = ModernBertModel.from_pretrained(model_name)\n+        self.assertIsNotNone(model)\n+\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @pytest.mark.flash_attn_test\n+    @slow\n+    def test_flash_attn_2_inference_equivalence_right_padding(self):\n+        self.skipTest(reason=\"ModernBert flash attention does not support right padding\")\n+\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @pytest.mark.flash_attn_test\n+    @slow\n+    def test_flash_attn_2_conversion(self):\n+        self.skipTest(reason=\"ModernBert doesn't use the ModernBertFlashAttention2 class method.\")\n+\n+\n+@require_torch\n+class ModernBertModelIntegrationTest(unittest.TestCase):\n+    \"\"\"\n+    These still need to be written, once public models are available.\n+    \"\"\""
        },
        {
            "sha": "5f053c20ff793842d9a873cdad53239bf1fdbc7f",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/667ed5635e6fd7e2df4fc23012746b1c0cbb7575/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=667ed5635e6fd7e2df4fc23012746b1c0cbb7575",
            "patch": "@@ -3457,6 +3457,8 @@ def test_mismatched_shapes_have_properly_initialized_weights(self):\n                 \"Data2VecAudioForSequenceClassification\",\n                 \"UniSpeechForSequenceClassification\",\n                 \"PvtForImageClassification\",\n+                \"ModernBertForSequenceClassification\",\n+                \"ModernBertForTokenClassification\",\n                 \"TimmWrapperForImageClassification\",\n             ]\n             special_param_names = [\n@@ -4042,7 +4044,12 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n \n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n-                model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)\n+                try:\n+                    model_sdpa = model_class.from_pretrained(\n+                        tmpdirname, torch_dtype=torch_dtype, attn_implementation=\"sdpa\"\n+                    )\n+                except ValueError:\n+                    model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)\n                 model_sdpa = model_sdpa.eval().to(torch_device, dtype=torch_dtype)\n \n                 model_eager = model_class.from_pretrained("
        }
    ],
    "stats": {
        "total": 3570,
        "additions": 3568,
        "deletions": 2
    }
}