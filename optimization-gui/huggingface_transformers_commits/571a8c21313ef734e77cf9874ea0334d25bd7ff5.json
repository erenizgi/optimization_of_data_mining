{
    "author": "Cyrilvallez",
    "message": "Add a default value for `position_ids` in masking_utils (#39310)\n\n* set default\n\n* Update masking_utils.py\n\n* add small test",
    "sha": "571a8c21313ef734e77cf9874ea0334d25bd7ff5",
    "files": [
        {
            "sha": "10f1a394d5a8a6b34c6813605f2ad9f7394044db",
            "filename": "src/transformers/masking_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/571a8c21313ef734e77cf9874ea0334d25bd7ff5/src%2Ftransformers%2Fmasking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/571a8c21313ef734e77cf9874ea0334d25bd7ff5/src%2Ftransformers%2Fmasking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmasking_utils.py?ref=571a8c21313ef734e77cf9874ea0334d25bd7ff5",
            "patch": "@@ -607,7 +607,7 @@ class AttentionMaskInterface(GeneralInterface):\n ALL_MASK_ATTENTION_FUNCTIONS: AttentionMaskInterface = AttentionMaskInterface()\n \n \n-def find_packed_sequence_indices(position_ids: torch.Tensor) -> Optional[torch.Tensor]:\n+def find_packed_sequence_indices(position_ids: torch.Tensor) -> torch.Tensor:\n     \"\"\"\n     Find the indices of the sequence to which each new query token in the sequence belongs when using packed\n     tensor format (i.e. several sequences packed in the same batch dimension).\n@@ -721,7 +721,7 @@ def create_causal_mask(\n     attention_mask: Optional[torch.Tensor],\n     cache_position: torch.Tensor,\n     past_key_values: Optional[Cache],\n-    position_ids: Optional[torch.Tensor],\n+    position_ids: Optional[torch.Tensor] = None,\n     or_mask_function: Optional[Callable] = None,\n     and_mask_function: Optional[Callable] = None,\n ) -> Optional[Union[torch.Tensor, BlockMask]]:\n@@ -810,7 +810,7 @@ def create_sliding_window_causal_mask(\n     attention_mask: Optional[torch.Tensor],\n     cache_position: torch.Tensor,\n     past_key_values: Optional[Cache],\n-    position_ids: Optional[torch.Tensor],\n+    position_ids: Optional[torch.Tensor] = None,\n     or_mask_function: Optional[Callable] = None,\n     and_mask_function: Optional[Callable] = None,\n ) -> Optional[Union[torch.Tensor, BlockMask]]:\n@@ -905,7 +905,7 @@ def create_chunked_causal_mask(\n     attention_mask: Optional[torch.Tensor],\n     cache_position: torch.Tensor,\n     past_key_values: Optional[Cache],\n-    position_ids: Optional[torch.Tensor],\n+    position_ids: Optional[torch.Tensor] = None,\n     or_mask_function: Optional[Callable] = None,\n     and_mask_function: Optional[Callable] = None,\n ) -> Optional[Union[torch.Tensor, BlockMask]]:\n@@ -1014,7 +1014,7 @@ def create_masks_for_generate(\n     attention_mask: Optional[torch.Tensor],\n     cache_position: torch.Tensor,\n     past_key_values: Optional[Cache],\n-    position_ids: Optional[torch.Tensor],\n+    position_ids: Optional[torch.Tensor] = None,\n     or_mask_function: Optional[Callable] = None,\n     and_mask_function: Optional[Callable] = None,\n     **kwargs,"
        },
        {
            "sha": "11d7e7e72b89a4b7b41d714bbc50412688b616f3",
            "filename": "tests/utils/test_masking_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/571a8c21313ef734e77cf9874ea0334d25bd7ff5/tests%2Futils%2Ftest_masking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/571a8c21313ef734e77cf9874ea0334d25bd7ff5/tests%2Futils%2Ftest_masking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_masking_utils.py?ref=571a8c21313ef734e77cf9874ea0334d25bd7ff5",
            "patch": "@@ -22,7 +22,7 @@\n     from torch.nn.attention.flex_attention import create_block_mask\n \n     from transformers import LlamaConfig\n-    from transformers.masking_utils import create_causal_mask\n+    from transformers.masking_utils import create_causal_mask, find_packed_sequence_indices\n \n \n # fmt: off\n@@ -130,3 +130,8 @@ def dummy_mask_mod(b, h, q, kv):\n \n         # We compatre the str representations, as the BlockMask objects themselves cannot easily be compared\n         self.assertEqual(causal_mask.to_string(), EXPECTED_BLOCK_MASK.to_string())\n+\n+    def test_find_packed_sequence_indices(self):\n+        position_ids = torch.tensor([[0, 1, 2, 3, 0, 1, 0, 1, 2, 3], [0, 1, 2, 3, 4, 5, 0, 1, 2, 3]])\n+        EXPECTED_SEQUENCE_INDICES = torch.tensor([[0, 0, 0, 0, 1, 1, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]])\n+        self.assertTrue((find_packed_sequence_indices(position_ids) == EXPECTED_SEQUENCE_INDICES).all())"
        }
    ],
    "stats": {
        "total": 17,
        "additions": 11,
        "deletions": 6
    }
}