{
    "author": "Rocketknight1",
    "message": "Purge unused ModelTester code (#37085)\n\n* Purge correctly this time\n\n* Remove more methods from recent PRs\n\n* make fixup",
    "sha": "2d46a08b637588147b4ae515ec51d9bfccc27814",
    "files": [
        {
            "sha": "5665345f7d561b77c9ee49b53458bbeac9d7a2d5",
            "filename": "tests/models/albert/test_modeling_albert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -198,16 +198,6 @@ def create_and_check_for_sequence_classification(\n         result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids, labels=sequence_labels)\n         self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n \n-    def create_and_check_for_token_classification(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_labels = self.num_labels\n-        model = AlbertForTokenClassification(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.num_labels))\n-\n     def create_and_check_for_multiple_choice(\n         self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n     ):"
        },
        {
            "sha": "0545aa77b2f794ee4268c4c6ab90c50add9a3726",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -168,19 +168,6 @@ def prepare_config_and_inputs_for_common(self):\n         }\n         return config, inputs_dict\n \n-    def create_and_check_aria_model_fp16_forward(self, config, input_ids, pixel_values, attention_mask):\n-        model = AriaForConditionalGeneration(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n-            logits = model(\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n-                pixel_values=pixel_values.to(torch.bfloat16),\n-                return_dict=True,\n-            )[\"logits\"]\n-        self.parent.assertFalse(torch.isnan(logits).any().item())\n-\n \n @require_torch\n class AriaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):"
        },
        {
            "sha": "688086fa0f83b446428046139c9f77b336dcc7b4",
            "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -154,33 +154,6 @@ def prepare_config_and_inputs_for_common(self):\n         }\n         return config, inputs_dict\n \n-    def create_and_check_model_fp16_forward(self, config, input_ids, pixel_values, attention_mask):\n-        model = AyaVisionForConditionalGeneration(config=config)\n-        model.to(torch_device)\n-        model.half()\n-        model.eval()\n-        logits = model(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            pixel_values=pixel_values,\n-            return_dict=True,\n-        )[\"logits\"]\n-        self.parent.assertFalse(torch.isnan(logits).any().item())\n-\n-    def create_and_check_model_fp16_autocast_forward(self, config, input_ids, pixel_values, attention_mask):\n-        config.torch_dtype = torch.float16\n-        model = AyaVisionForConditionalGeneration(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n-            logits = model(\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n-                pixel_values=pixel_values,\n-                return_dict=True,\n-            )[\"logits\"]\n-        self.parent.assertFalse(torch.isnan(logits).any().item())\n-\n \n @require_torch\n class AyaVisionModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):"
        },
        {
            "sha": "762d6531b2c2afd0f71c3713a3a3cfbab0f044b1",
            "filename": "tests/models/big_bird/test_modeling_big_bird.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -235,24 +235,6 @@ def create_and_check_model_as_decoder(\n         result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = BigBirdForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n     def create_and_check_for_masked_lm(\n         self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n     ):"
        },
        {
            "sha": "8d92f0a24774ab31d86fe4ea799e5c4b52aa623c",
            "filename": "tests/models/biogpt/test_modeling_biogpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -136,24 +136,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = BioGptForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n     def create_and_check_biogpt_model_attention_mask_past(\n         self, config, input_ids, input_mask, head_mask, token_type_ids, *args\n     ):"
        },
        {
            "sha": "62c45f65b19b431aef16ba419014960b9fdd37c2",
            "filename": "tests/models/bloom/test_modeling_bloom.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -275,14 +275,6 @@ def create_and_check_token_classification_model(self, config, input_ids, input_m\n         result = model(input_ids, attention_mask=input_mask)\n         self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.num_labels))\n \n-    def create_and_check_question_answering_model(self, config, input_ids, input_mask, *args):\n-        model = BloomForQuestionAnswering(config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.num_labels))\n-\n     def create_and_check_forward_and_backwards(\n         self, config, input_ids, input_mask, *args, gradient_checkpointing=False\n     ):"
        },
        {
            "sha": "9bcbdbf8a9cd17b4c823e9cc63ad4a0fa446232b",
            "filename": "tests/models/chameleon/test_modeling_chameleon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 77,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -180,83 +180,6 @@ def create_and_check_model(self, config, input_ids, input_mask, sequence_labels,\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = ChameleonForConditionalGeneration(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        model = ChameleonForConditionalGeneration(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         ("
        },
        {
            "sha": "24adee07df2216f1573407c9692260194fdbee6c",
            "filename": "tests/models/cohere/test_modeling_cohere.py",
            "status": "modified",
            "additions": 0,
            "deletions": 110,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -146,116 +146,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = self.model_class(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = self.for_causal_lm_class(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = self.for_causal_lm_class(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         ("
        },
        {
            "sha": "341f738fddfe107b7fba072d199c692fcf003ae9",
            "filename": "tests/models/convnextv2/test_modeling_convnextv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fconvnextv2%2Ftest_modeling_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fconvnextv2%2Ftest_modeling_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconvnextv2%2Ftest_modeling_convnextv2.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -119,35 +119,6 @@ def create_and_check_for_image_classification(self, config, pixel_values, labels\n         result = model(pixel_values, labels=labels)\n         self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n \n-    def create_and_check_backbone(self, config, pixel_values, labels):\n-        model = ConvNextV2Backbone(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(pixel_values)\n-\n-        # verify hidden states\n-        self.parent.assertEqual(len(result.feature_maps), len(config.out_features))\n-        self.parent.assertListEqual(list(result.feature_maps[0].shape), [self.batch_size, self.hidden_sizes[1], 4, 4])\n-\n-        # verify channels\n-        self.parent.assertEqual(len(model.channels), len(config.out_features))\n-        self.parent.assertListEqual(model.channels, config.hidden_sizes[1:])\n-\n-        # verify backbone works with out_features=None\n-        config.out_features = None\n-        model = ConvNextV2Backbone(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(pixel_values)\n-\n-        # verify feature maps\n-        self.parent.assertEqual(len(result.feature_maps), 1)\n-        self.parent.assertListEqual(list(result.feature_maps[0].shape), [self.batch_size, self.hidden_sizes[-1], 1, 1])\n-\n-        # verify channels\n-        self.parent.assertEqual(len(model.channels), 1)\n-        self.parent.assertListEqual(model.channels, [config.hidden_sizes[-1]])\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         config, pixel_values, labels = config_and_inputs"
        },
        {
            "sha": "1e3a40f833ab40dacbea25e3299904d3dd3a543c",
            "filename": "tests/models/ctrl/test_modeling_ctrl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fctrl%2Ftest_modeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fctrl%2Ftest_modeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fctrl%2Ftest_modeling_ctrl.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -180,15 +180,6 @@ def prepare_config_and_inputs_for_common(self):\n \n         return config, inputs_dict\n \n-    def create_and_check_ctrl_for_sequence_classification(self, config, input_ids, head_mask, token_type_ids, *args):\n-        config.num_labels = self.num_labels\n-        model = CTRLForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-        result = model(input_ids, token_type_ids=token_type_ids, labels=sequence_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n-\n \n @require_torch\n class CTRLModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):"
        },
        {
            "sha": "7aec53a6e7a9b29cdeff2e1f13d45cc7470203b1",
            "filename": "tests/models/data2vec/test_modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_audio.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -184,32 +184,6 @@ def create_and_check_model_with_adapter_proj_dim(self, config, input_values, att\n             (self.batch_size, self.adapter_output_seq_length, config.output_hidden_size),\n         )\n \n-    def create_and_check_batch_inference(self, config, input_values, *args):\n-        # test does not pass for models making use of `group_norm`\n-        # check: https://github.com/pytorch/fairseq/issues/3227\n-        model = Data2VecAudioModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        input_values = input_values[:3]\n-        attention_mask = torch.ones(input_values.shape, device=torch_device, dtype=torch.bool)\n-\n-        input_lengths = [input_values.shape[-1] // i for i in [4, 2, 1]]\n-\n-        # pad input\n-        for i in range(len(input_lengths)):\n-            input_values[i, input_lengths[i] :] = 0.0\n-            attention_mask[i, input_lengths[i] :] = 0.0\n-\n-        batch_outputs = model(input_values, attention_mask=attention_mask).last_hidden_state\n-\n-        for i in range(input_values.shape[0]):\n-            input_slice = input_values[i : i + 1, : input_lengths[i]]\n-            output = model(input_slice).last_hidden_state\n-\n-            batch_output = batch_outputs[i : i + 1, : output.shape[1]]\n-            self.parent.assertTrue(torch.allclose(output, batch_output, atol=1e-3))\n-\n     def check_ctc_loss(self, config, input_values, *args):\n         model = Data2VecAudioForCTC(config=config)\n         model.to(torch_device)"
        },
        {
            "sha": "dbe0cb8b271bc51848e5794a0c6d46703b2edc59",
            "filename": "tests/models/dbrx/test_modeling_dbrx.py",
            "status": "modified",
            "additions": 0,
            "deletions": 112,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -191,118 +191,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_model_as_decoder with Llama->Dbrx\n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = DbrxModel(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_for_causal_lm with Llama->Dbrx\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = DbrxForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = DbrxForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common with Llama->Dbrx\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()"
        },
        {
            "sha": "f6394fe354a074e489ce71ad1e060bce4fa8fb77",
            "filename": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 110,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -193,116 +193,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = DeepseekV3Model(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = DeepseekV3ForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = DeepseekV3ForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         ("
        },
        {
            "sha": "f56b0f764b84edff47565a5cc6ea87c25e189963",
            "filename": "tests/models/diffllama/test_modeling_diffllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 110,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -159,116 +159,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = DiffLlamaModel(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = DiffLlamaForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = DiffLlamaForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         ("
        },
        {
            "sha": "a40b9fa26b109934354f72179b62036cf10db88b",
            "filename": "tests/models/falcon/test_modeling_falcon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 110,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -152,116 +152,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = FalconModel(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = FalconForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = FalconForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         ("
        },
        {
            "sha": "eab0f2598e66d1a88e762a129d3ebf6dfe3f92f0",
            "filename": "tests/models/fnet/test_modeling_fnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 38,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Ffnet%2Ftest_modeling_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Ffnet%2Ftest_modeling_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffnet%2Ftest_modeling_fnet.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -22,7 +22,7 @@\n from transformers.testing_utils import require_tokenizers, require_torch, slow, torch_device\n \n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+from ...test_modeling_common import ModelTesterMixin, ids_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -41,10 +41,6 @@\n         FNetModel,\n         FNetTokenizerFast,\n     )\n-    from transformers.models.fnet.modeling_fnet import (\n-        FNetBasicFourierTransform,\n-        is_scipy_available,\n-    )\n \n \n # Override ConfigTester\n@@ -133,26 +129,6 @@ def get_config(self):\n             tpu_short_seq_length=self.seq_length,\n         )\n \n-    @require_torch\n-    def create_and_check_fourier_transform(self, config):\n-        hidden_states = floats_tensor([self.batch_size, self.seq_length, config.hidden_size])\n-        transform = FNetBasicFourierTransform(config)\n-        fftn_output = transform(hidden_states)\n-\n-        config.use_tpu_fourier_optimizations = True\n-        if is_scipy_available():\n-            transform = FNetBasicFourierTransform(config)\n-            dft_output = transform(hidden_states)\n-\n-        config.max_position_embeddings = 4097\n-        transform = FNetBasicFourierTransform(config)\n-        fft_output = transform(hidden_states)\n-\n-        if is_scipy_available():\n-            self.parent.assertTrue(torch.allclose(fftn_output[0][0], dft_output[0][0], atol=1e-4))\n-            self.parent.assertTrue(torch.allclose(fft_output[0][0], dft_output[0][0], atol=1e-4))\n-        self.parent.assertTrue(torch.allclose(fftn_output[0][0], fft_output[0][0], atol=1e-4))\n-\n     def create_and_check_model(self, config, input_ids, token_type_ids, sequence_labels, token_labels, choice_labels):\n         model = FNetModel(config=config)\n         model.to(torch_device)\n@@ -185,19 +161,6 @@ def create_and_check_for_masked_lm(\n         result = model(input_ids, token_type_ids=token_type_ids, labels=token_labels)\n         self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n \n-    def create_and_check_for_next_sentence_prediction(\n-        self, config, input_ids, token_type_ids, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = FNetForNextSentencePrediction(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            token_type_ids=token_type_ids,\n-            next_sentence_label=sequence_labels,\n-        )\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, 2))\n-\n     def create_and_check_for_question_answering(\n         self, config, input_ids, token_type_ids, sequence_labels, token_labels, choice_labels\n     ):"
        },
        {
            "sha": "513486766c689d6e8587ff30ca6e612fafb18276",
            "filename": "tests/models/fuyu/test_modeling_fuyu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 122,
            "changes": 123,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -22,7 +22,7 @@\n from parameterized import parameterized\n \n from transformers import FuyuConfig, is_torch_available, is_vision_available\n-from transformers.testing_utils import require_torch, require_torch_accelerator, slow, torch_device\n+from transformers.testing_utils import require_torch, require_torch_accelerator, slow\n from transformers.utils import cached_property\n \n from ...generation.test_utils import GenerationTesterMixin\n@@ -39,8 +39,6 @@\n \n \n if is_torch_available():\n-    import torch\n-\n     from transformers import FuyuForCausalLM\n \n \n@@ -133,125 +131,6 @@ def get_config(self):\n             pad_token_id=self.pad_token_id,\n         )\n \n-    def create_and_check_model(\n-        self,\n-        config,\n-        input_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-    ):\n-        model = FuyuForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = FuyuForCausalLM(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = FuyuForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = FuyuForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         ("
        },
        {
            "sha": "9cea971a97ff0c826b813339ffc7f42de56234e6",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 110,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -168,116 +168,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = self.model_class(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = self.for_causal_lm_class(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = self.for_causal_lm_class(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common with Llama->Gemma\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()"
        },
        {
            "sha": "ce9ed841b458dda4df2c2d4b18c6e25daf1a9e0d",
            "filename": "tests/models/glm/test_modeling_glm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 107,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -156,113 +156,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = self.model_class(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = self.for_causal_lm_class(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = self.for_causal_lm_class(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common with Llama->Glm\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()"
        },
        {
            "sha": "29c69dc586f2e399bb812a48b0aaaaf9359edba1",
            "filename": "tests/models/got_ocr2/test_modeling_got_ocr2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -138,33 +138,6 @@ def prepare_config_and_inputs_for_common(self):\n         }\n         return config, inputs_dict\n \n-    def create_and_check_model_fp16_forward(self, config, input_ids, pixel_values, attention_mask):\n-        model = GotOcr2ForConditionalGeneration(config=config)\n-        model.to(torch_device)\n-        model.half()\n-        model.eval()\n-        logits = model(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            pixel_values=pixel_values.to(torch.bfloat16),\n-            return_dict=True,\n-        )[\"logits\"]\n-        self.parent.assertFalse(torch.isnan(logits).any().item())\n-\n-    def create_and_check_model_fp16_autocast_forward(self, config, input_ids, pixel_values, attention_mask):\n-        config.torch_dtype = torch.float16\n-        model = GotOcr2ForConditionalGeneration(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n-            logits = model(\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n-                pixel_values=pixel_values.to(torch.bfloat16),\n-                return_dict=True,\n-            )[\"logits\"]\n-        self.parent.assertFalse(torch.isnan(logits).any().item())\n-\n \n @require_torch\n class GotOcr2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):"
        },
        {
            "sha": "8a377239582b0ebf314c04cf4e5b39026819dac7",
            "filename": "tests/models/granite/test_modeling_granite.py",
            "status": "modified",
            "additions": 0,
            "deletions": 110,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -146,116 +146,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = GraniteModel(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = GraniteForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = GraniteForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         ("
        },
        {
            "sha": "7ed6f764f386866e344c08a4ad1c48edb3041452",
            "filename": "tests/models/granitemoe/test_modeling_granitemoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 110,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -145,116 +145,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = GraniteMoeModel(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = GraniteMoeForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = GraniteMoeForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         ("
        },
        {
            "sha": "cc71b17850ff739068cf6d5aaf2d836b64f94060",
            "filename": "tests/models/granitemoeshared/test_modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 0,
            "deletions": 110,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fgranitemoeshared%2Ftest_modeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fgranitemoeshared%2Ftest_modeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoeshared%2Ftest_modeling_granitemoeshared.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -148,116 +148,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = GraniteMoeSharedModel(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = GraniteMoeSharedForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = GraniteMoeSharedForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         ("
        },
        {
            "sha": "e2c1851333146b8d215d0ecf14ff2ca563993677",
            "filename": "tests/models/jetmoe/test_modeling_jetmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 110,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -150,116 +150,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = JetMoeModel(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = JetMoeForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = JetMoeForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         ("
        },
        {
            "sha": "b5ce6ecf7c09ce3745a52a917b7648ceb15cb564",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 110,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -151,116 +151,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = LlamaModel(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = LlamaForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = LlamaForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         ("
        },
        {
            "sha": "cf709a280700ceb03e16cdbb8bf663ebfdd8c262",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -160,19 +160,6 @@ def prepare_config_and_inputs_for_common(self):\n         }\n         return config, inputs_dict\n \n-    def create_and_check_llava_model_fp16_forward(self, config, input_ids, pixel_values, attention_mask):\n-        model = LlavaForConditionalGeneration(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n-            logits = model(\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n-                pixel_values=pixel_values.to(torch.bfloat16),\n-                return_dict=True,\n-            )[\"logits\"]\n-        self.parent.assertFalse(torch.isnan(logits).any().item())\n-\n \n @require_torch\n class LlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):"
        },
        {
            "sha": "99bfdb632d6c875300223b89613179ad1148e223",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 0,
            "deletions": 33,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -174,39 +174,6 @@ def prepare_config_and_inputs_for_common(self):\n         }\n         return config, inputs_dict\n \n-    def create_and_check_llava_next_model_fp16_forward(\n-        self, config, input_ids, pixel_values, attention_mask, image_sizes\n-    ):\n-        model = LlavaNextForConditionalGeneration(config=config)\n-        model.to(torch_device)\n-        model.half()\n-        model.eval()\n-        logits = model(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            image_sizes=image_sizes,\n-            pixel_values=pixel_values.to(torch.bfloat16),\n-            return_dict=True,\n-        )[\"logits\"]\n-        self.parent.assertFalse(torch.isnan(logits).any().item())\n-\n-    def create_and_check_llava_next_model_fp16_autocast_forward(\n-        self, config, input_ids, pixel_values, attention_mask, image_sizes\n-    ):\n-        config.torch_dtype = torch.float16\n-        model = LlavaNextForConditionalGeneration(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n-            logits = model(\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n-                image_sizes=image_sizes,\n-                pixel_values=pixel_values.to(torch.bfloat16),\n-                return_dict=True,\n-            )[\"logits\"]\n-        self.parent.assertFalse(torch.isnan(logits).any().item())\n-\n \n @require_torch\n class LlavaNextForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):"
        },
        {
            "sha": "6e17836f5c3c41b24521bce96b8213043a28b2a0",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 35,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -188,41 +188,6 @@ def prepare_config_and_inputs_for_common(self):\n         }\n         return config, inputs_dict\n \n-    def create_and_check_llava_next_video_model_fp16_forward(\n-        self, config, input_ids, pixel_values, pixel_values_videos, attention_mask, image_sizes\n-    ):\n-        model = LlavaNextVideoForConditionalGeneration(config=config)\n-        model.to(torch_device)\n-        model.half()\n-        model.eval()\n-        logits = model(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            image_sizes=image_sizes,\n-            pixel_values=pixel_values.to(torch.bfloat16),\n-            pixel_values_videos=pixel_values_videos.to(torch.bfloat16),\n-            return_dict=True,\n-        )[\"logits\"]\n-        self.parent.assertFalse(torch.isnan(logits).any().item())\n-\n-    def create_and_check_llava_next_video_model_fp16_autocast_forward(\n-        self, config, input_ids, pixel_values, pixel_values_videos, attention_mask, image_sizes\n-    ):\n-        config.torch_dtype = torch.float16\n-        model = LlavaNextVideoForConditionalGeneration(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n-            logits = model(\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n-                image_sizes=image_sizes,\n-                pixel_values=pixel_values.to(torch.bfloat16),\n-                pixel_values_videos=pixel_values_videos.to(torch.bfloat16),\n-                return_dict=True,\n-            )[\"logits\"]\n-        self.parent.assertFalse(torch.isnan(logits).any().item())\n-\n \n @require_torch\n class LlavaNextVideoForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):"
        },
        {
            "sha": "b6eb87f95f8e18e4c7eab2eeff6f7196662ea369",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 33,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -174,39 +174,6 @@ def prepare_config_and_inputs_for_common(self):\n         }\n         return config, inputs_dict\n \n-    def create_and_check_llava_onevision_model_fp16_forward(\n-        self, config, input_ids, pixel_values, attention_mask, image_sizes\n-    ):\n-        model = LlavaOnevisionForConditionalGeneration(config=config)\n-        model.to(torch_device)\n-        model.half()\n-        model.eval()\n-        logits = model(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            image_sizes=image_sizes,\n-            pixel_values=pixel_values.to(torch.bfloat16),\n-            return_dict=True,\n-        )[\"logits\"]\n-        self.parent.assertFalse(torch.isnan(logits).any().item())\n-\n-    def create_and_check_llava_onevision_model_fp16_autocast_forward(\n-        self, config, input_ids, pixel_values, attention_mask, image_sizes\n-    ):\n-        config.torch_dtype = torch.float16\n-        model = LlavaOnevisionForConditionalGeneration(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n-            logits = model(\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n-                image_sizes=image_sizes,\n-                pixel_values=pixel_values.to(torch.bfloat16),\n-                return_dict=True,\n-            )[\"logits\"]\n-        self.parent.assertFalse(torch.isnan(logits).any().item())\n-\n \n @require_torch\n class LlavaOnevisionForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):"
        },
        {
            "sha": "f201c0ea934c8fb21e0e6921c10bc650833db940",
            "filename": "tests/models/megatron_bert/test_modeling_megatron_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fmegatron_bert%2Ftest_modeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fmegatron_bert%2Ftest_modeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmegatron_bert%2Ftest_modeling_megatron_bert.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -157,15 +157,6 @@ def create_and_check_megatron_bert_for_masked_lm(\n         result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids, labels=token_labels)\n         self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n \n-    def create_and_check_for_causal_lm(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = MegatronBertForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n     def create_and_check_megatron_bert_for_next_sequence_prediction(\n         self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n     ):"
        },
        {
            "sha": "be82cbc796b70726fb5898654c44f3eb34d5ecd8",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 113,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -159,119 +159,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_model_as_decoder with Llama->Mistral\n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = MistralModel(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_for_causal_lm with Llama->Mistral\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = MistralForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_decoder_model_past_large_inputs with Llama->Mistral\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = MistralForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()"
        },
        {
            "sha": "58096b5db7af8014beb250fe48342d81878a5a92",
            "filename": "tests/models/mistral3/test_modeling_mistral3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -160,33 +160,6 @@ def prepare_config_and_inputs_for_common(self):\n         }\n         return config, inputs_dict\n \n-    def create_and_check_model_fp16_forward(self, config, input_ids, pixel_values, attention_mask):\n-        model = Mistral3ForConditionalGeneration(config=config)\n-        model.to(torch_device)\n-        model.half()\n-        model.eval()\n-        logits = model(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            pixel_values=pixel_values.to(torch.bfloat16),\n-            return_dict=True,\n-        )[\"logits\"]\n-        self.parent.assertFalse(torch.isnan(logits).any().item())\n-\n-    def create_and_check_model_fp16_autocast_forward(self, config, input_ids, pixel_values, attention_mask):\n-        config.torch_dtype = torch.float16\n-        model = Mistral3ForConditionalGeneration(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n-            logits = model(\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n-                pixel_values=pixel_values.to(torch.bfloat16),\n-                return_dict=True,\n-            )[\"logits\"]\n-        self.parent.assertFalse(torch.isnan(logits).any().item())\n-\n \n @require_torch\n class Mistral3ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):"
        },
        {
            "sha": "1c12cbc2505f068fda28dba9f486b4c449b69ddf",
            "filename": "tests/models/mixtral/test_modeling_mixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 113,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -157,119 +157,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_model_as_decoder with Llama->Mixtral\n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = MixtralModel(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_for_causal_lm with Llama->Mixtral\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = MixtralForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_decoder_model_past_large_inputs with Llama->Mixtral\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = MixtralForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common with Llama->Mixtral\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()"
        },
        {
            "sha": "d5f6ed639632c54a6307bd59b76ad1375fe5ba2d",
            "filename": "tests/models/moonshine/test_modeling_moonshine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 35,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -109,41 +109,6 @@ def get_config(self):\n             eos_token_id=self.eos_token_id,\n         )\n \n-    def create_and_check_model(self, config, input_values, attention_mask):\n-        model = MoonshineModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_values, attention_mask=attention_mask)\n-        self.parent.assertEqual(\n-            result.last_hidden_state.shape, (self.batch_size, self.output_seq_length, self.hidden_size)\n-        )\n-\n-    def create_and_check_batch_inference(self, config, input_values, *args):\n-        # test does not pass for models making use of `group_norm`\n-        # check: https://github.com/pytorch/fairseq/issues/3227\n-        model = MoonshineModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        input_values = input_values[:3]\n-        attention_mask = torch.ones(input_values.shape, device=torch_device, dtype=torch.bool)\n-\n-        input_lengths = [input_values.shape[-1] // i for i in [4, 2, 1]]\n-\n-        # pad input\n-        for i in range(len(input_lengths)):\n-            input_values[i, input_lengths[i] :] = 0.0\n-            attention_mask[i, input_lengths[i] :] = 0.0\n-\n-        batch_outputs = model(input_values, attention_mask=attention_mask).last_hidden_state\n-\n-        for i in range(input_values.shape[0]):\n-            input_slice = input_values[i : i + 1, : input_lengths[i]]\n-            output = model(input_slice).last_hidden_state\n-\n-            batch_output = batch_outputs[i : i + 1, : output.shape[1]]\n-            self.parent.assertTrue(torch.allclose(output, batch_output, atol=1e-3))\n-\n     def check_output_attentions(self, config, input_values, attention_mask):\n         model = MoonshineModel(config=config)\n         model.config.layerdrop = 1.0"
        },
        {
            "sha": "cad2f88e2387b22d93f67d8dfaae24d8718e4f73",
            "filename": "tests/models/mpt/test_modeling_mpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -287,14 +287,6 @@ def create_and_check_token_classification_model(self, config, input_ids, input_m\n         result = model(input_ids, attention_mask=input_mask)\n         self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.num_labels))\n \n-    def create_and_check_question_answering_model(self, config, input_ids, input_mask, *args):\n-        model = MptForQuestionAnswering(config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.num_labels))\n-\n     def create_and_check_forward_and_backwards(\n         self, config, input_ids, input_mask, *args, gradient_checkpointing=False\n     ):"
        },
        {
            "sha": "bf87d39085aa936fb26cbdefd309d2905a2d84ef",
            "filename": "tests/models/mra/test_modeling_mra.py",
            "status": "modified",
            "additions": 0,
            "deletions": 32,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fmra%2Ftest_modeling_mra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fmra%2Ftest_modeling_mra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmra%2Ftest_modeling_mra.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -169,38 +169,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = MraModel(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            token_type_ids=token_type_ids,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            token_type_ids=token_type_ids,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n     def create_and_check_for_masked_lm(\n         self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n     ):"
        },
        {
            "sha": "fb4b10e910d74073d45efe054a36a3028a8b7c27",
            "filename": "tests/models/olmo/test_modeling_olmo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 110,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -146,116 +146,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = OlmoModel(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = OlmoForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = OlmoForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         ("
        },
        {
            "sha": "222aeb5b39281b44f5047ef3f5bf62c7668b6c59",
            "filename": "tests/models/olmo2/test_modeling_olmo2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 110,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -145,116 +145,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = Olmo2Model(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = Olmo2ForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = Olmo2ForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         ("
        },
        {
            "sha": "9c72225edca1ecb30113cfb9c962b14012326cf8",
            "filename": "tests/models/olmoe/test_modeling_olmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 110,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -159,116 +159,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = OlmoeModel(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = OlmoeForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = OlmoeForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         ("
        },
        {
            "sha": "5269f7183b1be8d47c3842c278284717c9dbb2cd",
            "filename": "tests/models/persimmon/test_modeling_persimmon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 110,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -151,116 +151,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = PersimmonModel(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = PersimmonForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = PersimmonForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         ("
        },
        {
            "sha": "1812f2b61eb47585b98ed167de505196c6440b71",
            "filename": "tests/models/phi/test_modeling_phi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 110,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -146,116 +146,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = PhiModel(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = PhiForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = PhiForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         ("
        },
        {
            "sha": "200a34ff2574c609ffe3a8bbe053b0a597e80197",
            "filename": "tests/models/phi3/test_modeling_phi3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 113,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -198,119 +198,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_model_as_decoder with Llama->Phi3\n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = Phi3Model(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_for_causal_lm with Llama->Phi3\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = Phi3ForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_decoder_model_past_large_inputs with Llama->Phi3\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = Phi3ForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()"
        },
        {
            "sha": "3268d29ddf16110bd3131f6e50fb98b6931cc91d",
            "filename": "tests/models/phi4_multimodal/test_modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -190,19 +190,6 @@ def prepare_config_and_inputs_for_common(self):\n         }\n         return config, inputs_dict\n \n-    def create_and_check_model(self, config, input_ids, attention_mask):\n-        model = Phi4MultimodalForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n-            logits = model(\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n-                return_dict=True,\n-            )[\"logits\"]\n-        self.parent.assertEqual(logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-        self.parent.assertFalse(torch.isnan(logits).any().item())\n-\n \n @require_torch\n class Phi4MultimodalModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):"
        },
        {
            "sha": "a61091522b85c4d163ebad50bb24b080d1d2a56e",
            "filename": "tests/models/phimoe/test_modeling_phimoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 113,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -205,119 +205,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_model_as_decoder with Llama->Phimoe\n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = PhimoeModel(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_for_causal_lm with Llama->Phimoe\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = PhimoeForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_decoder_model_past_large_inputs with Llama->Phimoe\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = PhimoeForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()"
        },
        {
            "sha": "6ea6fa33c80e7148ccbc5228a98824eabe9f41b2",
            "filename": "tests/models/pixtral/test_modeling_pixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -96,32 +96,6 @@ def get_config(self):\n             initializer_range=self.initializer_range,\n         )\n \n-    def create_and_check_model(self, config, pixel_values):\n-        model = PixtralVisionModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        with torch.no_grad():\n-            result = model(pixel_values)\n-        # expected sequence length = num_patches + 1 (we add 1 for the [CLS] token)\n-        image_size = (self.image_size, self.image_size)\n-        patch_size = (self.patch_size, self.patch_size)\n-        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, num_patches + 1, self.hidden_size))\n-        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n-\n-    def create_and_check_model_with_projection(self, config, pixel_values):\n-        model = PixtralVisionModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        with torch.no_grad():\n-            result = model(pixel_values)\n-        # expected sequence length = num_patches + 1 (we add 1 for the [CLS] token)\n-        image_size = (self.image_size, self.image_size)\n-        patch_size = (self.patch_size, self.patch_size)\n-        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, num_patches + 1, self.hidden_size))\n-        self.parent.assertEqual(result.image_embeds.shape, (self.batch_size, self.projection_dim))\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         config, pixel_values, image_sizes = config_and_inputs"
        },
        {
            "sha": "baa79e729da44e8db3efea8479ea7bdabfc34c3d",
            "filename": "tests/models/pop2piano/test_modeling_pop2piano.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -358,24 +358,6 @@ def create_and_check_decoder_model_past_large_inputs(\n         # test that outputs are equal for slice\n         self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n \n-    def create_and_check_generate_with_past_key_values(\n-        self,\n-        config,\n-        input_ids,\n-        decoder_input_ids,\n-        attention_mask,\n-        decoder_attention_mask,\n-        lm_labels,\n-    ):\n-        model = Pop2PianoForConditionalGeneration(config=config).to(torch_device).eval()\n-        torch.manual_seed(0)\n-        output_without_past_cache = model.generate(\n-            input_ids[:1], num_beams=2, max_length=5, do_sample=True, use_cache=False\n-        )\n-        torch.manual_seed(0)\n-        output_with_past_cache = model.generate(input_ids[:1], num_beams=2, max_length=5, do_sample=True)\n-        self.parent.assertTrue(torch.all(output_with_past_cache == output_without_past_cache))\n-\n     def create_and_check_model_fp16_forward(\n         self,\n         config,"
        },
        {
            "sha": "ea5a1b69d9cacf4c5f5d2eb01c853de8a799c9c7",
            "filename": "tests/models/pvt/test_modeling_pvt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fpvt%2Ftest_modeling_pvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fpvt%2Ftest_modeling_pvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpvt%2Ftest_modeling_pvt.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -121,24 +121,6 @@ def create_and_check_model(self, config, pixel_values, labels):\n         result = model(pixel_values)\n         self.parent.assertIsNotNone(result.last_hidden_state)\n \n-    def create_and_check_for_image_classification(self, config, pixel_values, labels):\n-        config.num_labels = self.type_sequence_label_size\n-        model = PvtForImageClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(pixel_values, labels=labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.type_sequence_label_size))\n-\n-        # test greyscale images\n-        config.num_channels = 1\n-        model = PvtForImageClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        pixel_values = floats_tensor([self.batch_size, 1, self.image_size, self.image_size])\n-        result = model(pixel_values)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.type_sequence_label_size))\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         config, pixel_values, labels = config_and_inputs"
        },
        {
            "sha": "381ab1c0004aff7e7b6fdb578498d26c9745654c",
            "filename": "tests/models/pvt_v2/test_modeling_pvt_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 47,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fpvt_v2%2Ftest_modeling_pvt_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fpvt_v2%2Ftest_modeling_pvt_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpvt_v2%2Ftest_modeling_pvt_v2.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -128,53 +128,6 @@ def create_and_check_model(self, config, pixel_values, labels):\n         result = model(pixel_values)\n         self.parent.assertIsNotNone(result.last_hidden_state)\n \n-    def create_and_check_backbone(self, config, pixel_values, labels):\n-        model = PvtV2Backbone(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(pixel_values)\n-\n-        # verify feature maps\n-        self.parent.assertEqual(len(result.feature_maps), len(config.out_features))\n-        self.parent.assertListEqual(list(result.feature_maps[0].shape), [self.batch_size, self.hidden_sizes[1], 4, 4])\n-\n-        # verify channels\n-        self.parent.assertEqual(len(model.channels), len(config.out_features))\n-        self.parent.assertListEqual(model.channels, config.hidden_sizes[1:])\n-\n-        # verify backbone works with out_features=None\n-        config.out_features = None\n-        model = PvtV2Backbone(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(pixel_values)\n-\n-        # verify feature maps\n-        self.parent.assertEqual(len(result.feature_maps), 1)\n-        self.parent.assertListEqual(list(result.feature_maps[0].shape), [self.batch_size, self.hidden_sizes[-1], 1, 1])\n-\n-        # verify channels\n-        self.parent.assertEqual(len(model.channels), 1)\n-        self.parent.assertListEqual(model.channels, [config.hidden_sizes[-1]])\n-\n-    def create_and_check_for_image_classification(self, config, pixel_values, labels):\n-        config.num_labels = self.num_labels\n-        model = PvtV2ForImageClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(pixel_values, labels=labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n-\n-        # test greyscale images\n-        config.num_channels = 1\n-        model = PvtV2ForImageClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        pixel_values = floats_tensor([self.batch_size, 1, self.image_size, self.image_size])\n-        result = model(pixel_values)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.type_sequence_label_size))\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         config, pixel_values, labels = config_and_inputs"
        },
        {
            "sha": "943fa3a91bdace2c0c8601764534c964e8effb4c",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 113,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -169,119 +169,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_model_as_decoder with Llama->Qwen2\n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = Qwen2Model(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_for_causal_lm with Llama->Qwen2\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = Qwen2ForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_decoder_model_past_large_inputs with Llama->Qwen2\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = Qwen2ForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()"
        },
        {
            "sha": "5bbd21d6e960da825ef41aa465839af621b87ebc",
            "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 33,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -189,39 +189,6 @@ def prepare_config_and_inputs_for_common(self):\n         }\n         return config, inputs_dict\n \n-    def create_and_check_qwen2_5_vl_model_fp16_forward(\n-        self, config, input_ids, pixel_values, attention_mask, image_grid_thw\n-    ):\n-        model = Qwen2_5_VLForConditionalGeneration(config=config)\n-        model.to(torch_device)\n-        model.half()\n-        model.eval()\n-        logits = model(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            image_grid_thw=image_grid_thw,\n-            pixel_values=pixel_values.to(torch.bfloat16),\n-            return_dict=True,\n-        )[\"logits\"]\n-        self.parent.assertFalse(torch.isnan(logits).any().item())\n-\n-    def create_and_check_qwen2_5_vl_model_fp16_autocast_forward(\n-        self, config, input_ids, pixel_values, attention_mask, image_grid_thw\n-    ):\n-        config.torch_dtype = torch.float16\n-        model = Qwen2_5_VLForConditionalGeneration(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n-            logits = model(\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n-                image_grid_thw=image_grid_thw,\n-                pixel_values=pixel_values.to(torch.bfloat16),\n-                return_dict=True,\n-            )[\"logits\"]\n-        self.parent.assertFalse(torch.isnan(logits).any().item())\n-\n \n @require_torch\n class Qwen2_5_VLModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):"
        },
        {
            "sha": "e4b16d834413ad7d9581b2b20068c4fc3d0bd106",
            "filename": "tests/models/qwen2_audio/test_modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -131,19 +131,6 @@ def prepare_config_and_inputs_for_common(self):\n         }\n         return config, inputs_dict\n \n-    def create_and_check_qwen2audio_model_fp16_forward(self, config, input_ids, pixel_values, attention_mask):\n-        model = Qwen2AudioForConditionalGeneration(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n-            logits = model(\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n-                pixel_values=pixel_values.to(torch.bfloat16),\n-                return_dict=True,\n-            )[\"logits\"]\n-        self.parent.assertFalse(torch.isnan(logits).any().item())\n-\n \n @require_torch\n class Qwen2AudioForConditionalGenerationModelTest(ModelTesterMixin, unittest.TestCase):"
        },
        {
            "sha": "b7b8fbab11b78193b79f3501a8a973f8fec0fbcd",
            "filename": "tests/models/qwen2_moe/test_modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 113,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -197,119 +197,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_model_as_decoder with Llama->Qwen2Moe\n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = Qwen2MoeModel(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_for_causal_lm with Llama->Qwen2Moe\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = Qwen2MoeForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_decoder_model_past_large_inputs with Llama->Qwen2Moe\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = Qwen2MoeForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()"
        },
        {
            "sha": "13c58fae7422d6a56a3abd56dc95afb29ec7f11e",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 33,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -184,39 +184,6 @@ def prepare_config_and_inputs_for_common(self):\n         }\n         return config, inputs_dict\n \n-    def create_and_check_qwen2_vl_model_fp16_forward(\n-        self, config, input_ids, pixel_values, attention_mask, image_grid_thw\n-    ):\n-        model = Qwen2VLForConditionalGeneration(config=config)\n-        model.to(torch_device)\n-        model.half()\n-        model.eval()\n-        logits = model(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            image_grid_thw=image_grid_thw,\n-            pixel_values=pixel_values.to(torch.bfloat16),\n-            return_dict=True,\n-        )[\"logits\"]\n-        self.parent.assertFalse(torch.isnan(logits).any().item())\n-\n-    def create_and_check_qwen2_vl_model_fp16_autocast_forward(\n-        self, config, input_ids, pixel_values, attention_mask, image_grid_thw\n-    ):\n-        config.torch_dtype = torch.float16\n-        model = Qwen2VLForConditionalGeneration(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n-            logits = model(\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n-                image_grid_thw=image_grid_thw,\n-                pixel_values=pixel_values.to(torch.bfloat16),\n-                return_dict=True,\n-            )[\"logits\"]\n-        self.parent.assertFalse(torch.isnan(logits).any().item())\n-\n \n @require_torch\n class Qwen2VLModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):"
        },
        {
            "sha": "bd46f11ce3f1b4ec02b9dd41148377b7622597e9",
            "filename": "tests/models/qwen3/test_modeling_qwen3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 113,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -172,119 +172,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_model_as_decoder with Llama->Qwen3\n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = Qwen3Model(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_for_causal_lm with Llama->Qwen3\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = Qwen3ForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_decoder_model_past_large_inputs with Llama->Qwen3\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = Qwen3ForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()"
        },
        {
            "sha": "e548d14ba7e37059dfb755fb0b0afbf35bdc295b",
            "filename": "tests/models/qwen3_moe/test_modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 113,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -191,119 +191,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_model_as_decoder with Llama->Qwen3Moe\n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = Qwen3MoeModel(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_for_causal_lm with Llama->Qwen3Moe\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = Qwen3MoeForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_decoder_model_past_large_inputs with Llama->Qwen3Moe\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = Qwen3MoeForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()"
        },
        {
            "sha": "36ccd04cf6d042ae91888dd0e2847a57807dafb0",
            "filename": "tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 113,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -152,119 +152,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_model_as_decoder with Llama->RecurrentGemma\n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = RecurrentGemmaModel(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_for_causal_lm with Llama->RecurrentGemma\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = RecurrentGemmaForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_decoder_model_past_large_inputs with Llama->RecurrentGemma\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = RecurrentGemmaForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common with Llama->RecurrentGemma\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()"
        },
        {
            "sha": "956f1a2b80cb47753886f6da5901ca0872d3afd6",
            "filename": "tests/models/rembert/test_modeling_rembert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Frembert%2Ftest_modeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Frembert%2Ftest_modeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frembert%2Ftest_modeling_rembert.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -200,24 +200,6 @@ def create_and_check_model_as_decoder(\n         result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = RemBertForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n     def create_and_check_for_masked_lm(\n         self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n     ):"
        },
        {
            "sha": "95de5613a24336f4e469bb5277315f10c94e554d",
            "filename": "tests/models/roc_bert/test_modeling_roc_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Froc_bert%2Ftest_modeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Froc_bert%2Ftest_modeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froc_bert%2Ftest_modeling_roc_bert.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -259,33 +259,6 @@ def create_and_check_model_as_decoder(\n         )\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        input_shape_ids,\n-        input_pronunciation_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = RoCBertForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            input_shape_ids=input_shape_ids,\n-            input_pronunciation_ids=input_pronunciation_ids,\n-            attention_mask=input_mask,\n-            token_type_ids=token_type_ids,\n-            labels=token_labels,\n-        )\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n     def create_and_check_for_masked_lm(\n         self,\n         config,"
        },
        {
            "sha": "22c1e8af8ad28cf88c83a9343d853a521fad2c90",
            "filename": "tests/models/roformer/test_modeling_roformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -200,24 +200,6 @@ def create_and_check_model_as_decoder(\n         result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = RoFormerForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n     def create_and_check_for_generate_causal_lm(\n         self,\n         config,"
        },
        {
            "sha": "d7bfadcecade7d5d03ea0c7862c097eb4502fefa",
            "filename": "tests/models/rt_detr/test_modeling_rt_detr_resnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 30,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Frt_detr%2Ftest_modeling_rt_detr_resnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Frt_detr%2Ftest_modeling_rt_detr_resnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frt_detr%2Ftest_modeling_rt_detr_resnet.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -16,7 +16,7 @@\n import unittest\n \n from transformers import RTDetrResNetConfig\n-from transformers.testing_utils import require_torch, torch_device\n+from transformers.testing_utils import require_torch\n from transformers.utils.import_utils import is_torch_available\n \n from ...test_backbone_common import BackboneTesterMixin\n@@ -84,35 +84,6 @@ def get_config(self):\n             out_indices=self.out_indices,\n         )\n \n-    def create_and_check_backbone(self, config, pixel_values, labels):\n-        model = RTDetrResNetBackbone(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(pixel_values)\n-\n-        # verify feature maps\n-        self.parent.assertEqual(len(result.feature_maps), len(config.out_features))\n-        self.parent.assertListEqual(list(result.feature_maps[0].shape), [self.batch_size, self.hidden_sizes[1], 4, 4])\n-\n-        # verify channels\n-        self.parent.assertEqual(len(model.channels), len(config.out_features))\n-        self.parent.assertListEqual(model.channels, config.hidden_sizes[1:])\n-\n-        # verify backbone works with out_features=None\n-        config.out_features = None\n-        model = RTDetrResNetBackbone(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(pixel_values)\n-\n-        # verify feature maps\n-        self.parent.assertEqual(len(result.feature_maps), 1)\n-        self.parent.assertListEqual(list(result.feature_maps[0].shape), [self.batch_size, self.hidden_sizes[-1], 1, 1])\n-\n-        # verify channels\n-        self.parent.assertEqual(len(model.channels), 1)\n-        self.parent.assertListEqual(model.channels, [config.hidden_sizes[-1]])\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         config, pixel_values, labels = config_and_inputs"
        },
        {
            "sha": "7915be09eb47806f0ef92fe9c7f98fcc89795768",
            "filename": "tests/models/rwkv/test_modeling_rwkv.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -195,19 +195,6 @@ def create_and_check_state_equivalency(self, config, input_ids, input_mask, head\n \n         self.parent.assertTrue(torch.allclose(torch.cat([output_one, output_two], dim=1), output_whole, atol=1e-5))\n \n-    def create_and_check_forward_and_backwards(\n-        self, config, input_ids, input_mask, head_mask, token_type_ids, *args, gradient_checkpointing=False\n-    ):\n-        model = RwkvForCausalLM(config)\n-        model.to(torch_device)\n-        if gradient_checkpointing:\n-            model.gradient_checkpointing_enable()\n-\n-        result = model(input_ids, labels=input_ids)\n-        self.parent.assertEqual(result.loss.shape, ())\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-        result.loss.backward()\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n "
        },
        {
            "sha": "76b0dc2a45259887cefd12e54591f39d9b5cf4cb",
            "filename": "tests/models/sew/test_modeling_sew.py",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fsew%2Ftest_modeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fsew%2Ftest_modeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsew%2Ftest_modeling_sew.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -147,32 +147,6 @@ def create_and_check_model(self, config, input_values, attention_mask):\n             result.last_hidden_state.shape, (self.batch_size, self.output_seq_length, self.hidden_size)\n         )\n \n-    def create_and_check_batch_inference(self, config, input_values, *args):\n-        # test does not pass for models making use of `group_norm`\n-        # check: https://github.com/pytorch/fairseq/issues/3227\n-        model = SEWModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        input_values = input_values[:3]\n-        attention_mask = torch.ones(input_values.shape, device=torch_device, dtype=torch.bool)\n-\n-        input_lengths = [input_values.shape[-1] // i for i in [4, 2, 1]]\n-\n-        # pad input\n-        for i in range(len(input_lengths)):\n-            input_values[i, input_lengths[i] :] = 0.0\n-            attention_mask[i, input_lengths[i] :] = 0.0\n-\n-        batch_outputs = model(input_values, attention_mask=attention_mask).last_hidden_state\n-\n-        for i in range(input_values.shape[0]):\n-            input_slice = input_values[i : i + 1, : input_lengths[i]]\n-            output = model(input_slice).last_hidden_state\n-\n-            batch_output = batch_outputs[i : i + 1, : output.shape[1]]\n-            self.parent.assertTrue(torch.allclose(output, batch_output, atol=1e-3))\n-\n     def check_ctc_loss(self, config, input_values, *args):\n         model = SEWForCTC(config=config)\n         model.to(torch_device)"
        },
        {
            "sha": "c6b28b2474531151daf66550c52f8b78e9400905",
            "filename": "tests/models/sew_d/test_modeling_sew_d.py",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fsew_d%2Ftest_modeling_sew_d.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fsew_d%2Ftest_modeling_sew_d.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsew_d%2Ftest_modeling_sew_d.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -168,32 +168,6 @@ def create_and_check_model(self, config, input_values, attention_mask):\n             result.last_hidden_state.shape, (self.batch_size, self.output_seq_length, self.hidden_size)\n         )\n \n-    def create_and_check_batch_inference(self, config, input_values, *args):\n-        # test does not pass for models making use of `group_norm`\n-        # check: https://github.com/pytorch/fairseq/issues/3227\n-        model = SEWDModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        input_values = input_values[:3]\n-        attention_mask = torch.ones(input_values.shape, device=torch_device, dtype=torch.bool)\n-\n-        input_lengths = [input_values.shape[-1] // i for i in [4, 2, 1]]\n-\n-        # pad input\n-        for i in range(len(input_lengths)):\n-            input_values[i, input_lengths[i] :] = 0.0\n-            attention_mask[i, input_lengths[i] :] = 0.0\n-\n-        batch_outputs = model(input_values, attention_mask=attention_mask).last_hidden_state\n-\n-        for i in range(input_values.shape[0]):\n-            input_slice = input_values[i : i + 1, : input_lengths[i]]\n-            output = model(input_slice).last_hidden_state\n-\n-            batch_output = batch_outputs[i : i + 1, : output.shape[1]]\n-            self.parent.assertTrue(torch.allclose(output, batch_output, atol=1e-3))\n-\n     def check_ctc_loss(self, config, input_values, *args):\n         model = SEWDForCTC(config=config)\n         model.to(torch_device)"
        },
        {
            "sha": "5cc21b8e3d71ee8e4a5cd2b314eba2848e74f938",
            "filename": "tests/models/stablelm/test_modeling_stablelm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 110,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -153,116 +153,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = StableLmModel(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = StableLmForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = StableLmForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         ("
        },
        {
            "sha": "e27b9303229ccdc4bbe0d213b9968786c7af437b",
            "filename": "tests/models/starcoder2/test_modeling_starcoder2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 113,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -157,119 +157,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_model_as_decoder with Llama->Starcoder2\n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = Starcoder2Model(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_for_causal_lm with Llama->Starcoder2\n-    def create_and_check_for_causal_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        model = Starcoder2ForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_decoder_model_past_large_inputs with Llama->Starcoder2\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        model = Starcoder2ForCausalLM(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical multiple next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        )[\"hidden_states\"][0]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs_for_common\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()"
        },
        {
            "sha": "06e9aebb99c49355da61398cab094a8fbb2edca7",
            "filename": "tests/models/timm_backbone/test_modeling_timm_backbone.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -27,8 +27,6 @@\n \n \n if is_torch_available():\n-    import torch\n-\n     from transformers import TimmBackbone, TimmBackboneConfig\n \n from ...test_pipeline_mixin import PipelineTesterMixin\n@@ -76,17 +74,6 @@ def get_config(self):\n             backbone=self.backbone,\n         )\n \n-    def create_and_check_model(self, config, pixel_values):\n-        model = TimmBackbone(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        with torch.no_grad():\n-            result = model(pixel_values)\n-        self.parent.assertEqual(\n-            result.feature_map[-1].shape,\n-            (self.batch_size, model.channels[-1], 14, 14),\n-        )\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         config, pixel_values = config_and_inputs"
        },
        {
            "sha": "2628106512bd841e9203e6ddab43f7d72a47c653",
            "filename": "tests/models/timm_wrapper/test_modeling_timm_wrapper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Ftimm_wrapper%2Ftest_modeling_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Ftimm_wrapper%2Ftest_modeling_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimm_wrapper%2Ftest_modeling_timm_wrapper.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -75,17 +75,6 @@ def prepare_config_and_inputs(self):\n     def get_config(self):\n         return TimmWrapperConfig.from_pretrained(self.model_name)\n \n-    def create_and_check_model(self, config, pixel_values):\n-        model = TimmWrapperModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        with torch.no_grad():\n-            result = model(pixel_values)\n-        self.parent.assertEqual(\n-            result.feature_map[-1].shape,\n-            (self.batch_size, model.channels[-1], 14, 14),\n-        )\n-\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         config, pixel_values = config_and_inputs"
        },
        {
            "sha": "f3ffa8051eebfdd4fce218418e6024487e7f41ff",
            "filename": "tests/models/umt5/test_modeling_umt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -229,43 +229,6 @@ def create_and_check_model(\n         # There should be a self attn key, a self attn value, a cross attn key and a cross attn value stored in each decoder_past tuple\n         self.parent.assertEqual(len(decoder_past[0]), 4)\n \n-    def create_and_check_decoder_model_past(\n-        self,\n-        config,\n-        input_ids,\n-        decoder_input_ids,\n-        attention_mask,\n-        decoder_attention_mask,\n-        lm_labels,\n-    ):\n-        model = UMT5Model(config=config).get_decoder().to(torch_device).eval()\n-        # first forward pass\n-        outputs = model(input_ids, use_cache=True)\n-        outputs_use_cache_conf = model(input_ids)\n-        outputs_no_past = model(input_ids, use_cache=False)\n-\n-        self.parent.assertTrue(len(outputs) == len(outputs_use_cache_conf))\n-        self.parent.assertTrue(len(outputs) == len(outputs_no_past) + 1)\n-\n-        output, past_key_values = outputs.to_tuple()\n-\n-        # create hypothetical next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 1), config.vocab_size)\n-\n-        # append to next input_ids and\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-\n-        output_from_no_past = model(next_input_ids)[\"last_hidden_state\"]\n-        output_from_past = model(next_tokens, past_key_values=past_key_values)[\"last_hidden_state\"]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -1, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, 0, random_slice_idx].detach()\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n     def create_and_check_model_fp16_forward(\n         self,\n         config,"
        },
        {
            "sha": "4cce51d7ace91831a3a3c1e6e23e0375ac118419",
            "filename": "tests/models/wav2vec2_bert/test_modeling_wav2vec2_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_modeling_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_modeling_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_modeling_wav2vec2_bert.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -246,32 +246,6 @@ def create_and_check_model_float16(self, config, input_features, attention_mask)\n             result.last_hidden_state.shape, (self.batch_size, self.output_seq_length, self.hidden_size)\n         )\n \n-    def create_and_check_batch_inference(self, config, input_features, *args):\n-        # test does not pass for models making use of `group_norm`\n-        # check: https://github.com/pytorch/fairseq/issues/3227\n-        model = Wav2Vec2BertModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        input_features = input_features[:3]\n-        attention_mask = torch.ones(input_features.shape, device=torch_device, dtype=torch.bool)\n-\n-        input_lengths = [input_features.shape[-1] // i for i in [4, 2, 1]]\n-\n-        # pad input\n-        for i in range(len(input_lengths)):\n-            input_features[i, input_lengths[i] :] = 0.0\n-            attention_mask[i, input_lengths[i] :] = 0.0\n-\n-        batch_outputs = model(input_features, attention_mask=attention_mask).last_hidden_state\n-\n-        for i in range(input_features.shape[0]):\n-            input_slice = input_features[i : i + 1, : input_lengths[i]]\n-            output = model(input_slice).last_hidden_state\n-\n-            batch_output = batch_outputs[i : i + 1, : output.shape[1]]\n-            self.parent.assertTrue(torch.allclose(output, batch_output, atol=1e-3))\n-\n     def check_ctc_loss(self, config, input_features, *args):\n         model = Wav2Vec2BertForCTC(config=config)\n         model.to(torch_device)"
        },
        {
            "sha": "f276b13d7bef3ef571b5cac9cfaba893e29b9545",
            "filename": "tests/models/wav2vec2_conformer/test_modeling_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fwav2vec2_conformer%2Ftest_modeling_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fwav2vec2_conformer%2Ftest_modeling_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2_conformer%2Ftest_modeling_wav2vec2_conformer.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -240,32 +240,6 @@ def create_and_check_model_float16(self, config, input_values, attention_mask):\n             result.last_hidden_state.shape, (self.batch_size, self.output_seq_length, self.hidden_size)\n         )\n \n-    def create_and_check_batch_inference(self, config, input_values, *args):\n-        # test does not pass for models making use of `group_norm`\n-        # check: https://github.com/pytorch/fairseq/issues/3227\n-        model = Wav2Vec2ConformerModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        input_values = input_values[:3]\n-        attention_mask = torch.ones(input_values.shape, device=torch_device, dtype=torch.bool)\n-\n-        input_lengths = [input_values.shape[-1] // i for i in [4, 2, 1]]\n-\n-        # pad input\n-        for i in range(len(input_lengths)):\n-            input_values[i, input_lengths[i] :] = 0.0\n-            attention_mask[i, input_lengths[i] :] = 0.0\n-\n-        batch_outputs = model(input_values, attention_mask=attention_mask).last_hidden_state\n-\n-        for i in range(input_values.shape[0]):\n-            input_slice = input_values[i : i + 1, : input_lengths[i]]\n-            output = model(input_slice).last_hidden_state\n-\n-            batch_output = batch_outputs[i : i + 1, : output.shape[1]]\n-            self.parent.assertTrue(torch.allclose(output, batch_output, atol=1e-3))\n-\n     def check_ctc_loss(self, config, input_values, *args):\n         model = Wav2Vec2ConformerForCTC(config=config)\n         model.to(torch_device)"
        },
        {
            "sha": "228587c2be9835d23a8563cdbcede976b46d9078",
            "filename": "tests/models/wavlm/test_modeling_wavlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fwavlm%2Ftest_modeling_wavlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fwavlm%2Ftest_modeling_wavlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwavlm%2Ftest_modeling_wavlm.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -157,32 +157,6 @@ def create_and_check_model(self, config, input_values, attention_mask):\n             result.last_hidden_state.shape, (self.batch_size, self.output_seq_length, self.hidden_size)\n         )\n \n-    def create_and_check_batch_inference(self, config, input_values, *args):\n-        # test does not pass for models making use of `group_norm`\n-        # check: https://github.com/pytorch/fairseq/issues/3227\n-        model = WavLMModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        input_values = input_values[:3]\n-        attention_mask = torch.ones(input_values.shape, device=torch_device, dtype=torch.bool)\n-\n-        input_lengths = [input_values.shape[-1] // i for i in [4, 2, 1]]\n-\n-        # pad input\n-        for i in range(len(input_lengths)):\n-            input_values[i, input_lengths[i] :] = 0.0\n-            attention_mask[i, input_lengths[i] :] = 0.0\n-\n-        batch_outputs = model(input_values, attention_mask=attention_mask).last_hidden_state\n-\n-        for i in range(input_values.shape[0]):\n-            input_slice = input_values[i : i + 1, : input_lengths[i]]\n-            output = model(input_slice).last_hidden_state\n-\n-            batch_output = batch_outputs[i : i + 1, : output.shape[1]]\n-            self.parent.assertTrue(torch.allclose(output, batch_output, atol=1e-3))\n-\n     def check_ctc_loss(self, config, input_values, *args):\n         model = WavLMForCTC(config=config)\n         model.to(torch_device)"
        },
        {
            "sha": "cb31efcd5759c9ce5c847cc4a9a07888a281ce92",
            "filename": "tests/models/yoso/test_modeling_yoso.py",
            "status": "modified",
            "additions": 0,
            "deletions": 32,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fyoso%2Ftest_modeling_yoso.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d46a08b637588147b4ae515ec51d9bfccc27814/tests%2Fmodels%2Fyoso%2Ftest_modeling_yoso.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fyoso%2Ftest_modeling_yoso.py?ref=2d46a08b637588147b4ae515ec51d9bfccc27814",
            "patch": "@@ -168,38 +168,6 @@ def create_and_check_model(\n         result = model(input_ids)\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-        model = YosoModel(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            token_type_ids=token_type_ids,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-        )\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            token_type_ids=token_type_ids,\n-            encoder_hidden_states=encoder_hidden_states,\n-        )\n-        result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n     def create_and_check_for_masked_lm(\n         self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n     ):"
        }
    ],
    "stats": {
        "total": 4289,
        "additions": 3,
        "deletions": 4286
    }
}