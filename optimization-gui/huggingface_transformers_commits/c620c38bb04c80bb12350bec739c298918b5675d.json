{
    "author": "danielquintas8",
    "message": "[Qwen3VLMoe] Fixed: Expected self.dtype to be equal to src.dtype - routing_weights casting (#41420)\n\n* Fixed Expected self.dtype to be equal to src.dtype on eval\n\n* Fixed Expected self.dtype to be equal to src.dtype on eval\n\n* Fixed Expected self.dtype to be equal to src.dtype on eval\n\n* generated modeling_qwen3_vl_moe.py file\n\n* Fixed Ernie_4_5_MoE router casting\n\n* Fixed routing_weights dtype casting (ernie4_5_moe, hunyuan_v1_moe, qwen2_moe, qwen3_moe, qwen3_next,qwen3_omni_moe)\n\n* rollback hunyuan_v1_moe changes\n\n---------\n\nCo-authored-by: Daniel Oliveira <daniel-oliveira-11@hotmail.com>\nCo-authored-by: Daniel Oliveira <36623265+daniel3303@users.noreply.github.com>",
    "sha": "c620c38bb04c80bb12350bec739c298918b5675d",
    "files": [
        {
            "sha": "5b8d28fef82d69aa370f1993e24fb4ecbace7049",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=c620c38bb04c80bb12350bec739c298918b5675d",
            "patch": "@@ -307,7 +307,7 @@ def forward(\n             routing_weights = routing_weights / torch.clamp(\n                 routing_weights.sum(dim=-1, keepdim=True), min=self.norm_min\n             )\n-        routing_weights = routing_weights.to(hidden_states.dtype)\n+        routing_weights = routing_weights.to(router_logits.dtype)\n         return router_logits, selected_experts, routing_weights\n \n \n@@ -364,7 +364,7 @@ def route_tokens_to_experts(self, hidden_states, router_logits):\n             routing_weights = routing_weights / torch.clamp(\n                 routing_weights.sum(dim=-1, keepdim=True), min=self.norm_min\n             )\n-        routing_weights = routing_weights.to(hidden_states.dtype)\n+        routing_weights = routing_weights.to(router_logits.dtype)\n         return selected_experts, routing_weights\n \n     def forward("
        },
        {
            "sha": "45ebe54927ebacea8ac316ad97b7befcde8f3e83",
            "filename": "src/transformers/models/ernie4_5_moe/modular_ernie4_5_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py?ref=c620c38bb04c80bb12350bec739c298918b5675d",
            "patch": "@@ -117,7 +117,7 @@ def forward(\n             routing_weights = routing_weights / torch.clamp(\n                 routing_weights.sum(dim=-1, keepdim=True), min=self.norm_min\n             )\n-        routing_weights = routing_weights.to(hidden_states.dtype)\n+        routing_weights = routing_weights.to(router_logits.dtype)\n         return router_logits, selected_experts, routing_weights\n \n \n@@ -174,7 +174,7 @@ def route_tokens_to_experts(self, hidden_states, router_logits):\n             routing_weights = routing_weights / torch.clamp(\n                 routing_weights.sum(dim=-1, keepdim=True), min=self.norm_min\n             )\n-        routing_weights = routing_weights.to(hidden_states.dtype)\n+        routing_weights = routing_weights.to(router_logits.dtype)\n         return selected_experts, routing_weights\n \n     def forward("
        },
        {
            "sha": "24a86fbf21d5b3e1b7c83fc261fb816c81100633",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=c620c38bb04c80bb12350bec739c298918b5675d",
            "patch": "@@ -311,7 +311,7 @@ def route_tokens_to_experts(self, hidden_states, router_logits):\n         routing_weights, selected_experts = torch.topk(routing_weights, self.num_experts_per_tok, dim=-1)\n         if self.norm_topk_prob:\n             routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n-        routing_weights = routing_weights.to(hidden_states.dtype)\n+        routing_weights = routing_weights.to(router_logits.dtype)\n         return selected_experts, routing_weights\n \n     def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:"
        },
        {
            "sha": "9b82be47635f63824102a3263f01e71fdf2fae51",
            "filename": "src/transformers/models/qwen2_moe/modular_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodular_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodular_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodular_qwen2_moe.py?ref=c620c38bb04c80bb12350bec739c298918b5675d",
            "patch": "@@ -106,7 +106,7 @@ def route_tokens_to_experts(self, hidden_states, router_logits):\n         routing_weights, selected_experts = torch.topk(routing_weights, self.num_experts_per_tok, dim=-1)\n         if self.norm_topk_prob:\n             routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n-        routing_weights = routing_weights.to(hidden_states.dtype)\n+        routing_weights = routing_weights.to(router_logits.dtype)\n         return selected_experts, routing_weights\n \n     def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:"
        },
        {
            "sha": "6385f6f9bddf023ffd009c21cef9d20ea842a935",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=c620c38bb04c80bb12350bec739c298918b5675d",
            "patch": "@@ -256,7 +256,7 @@ def route_tokens_to_experts(self, hidden_states, router_logits):\n         routing_weights, selected_experts = torch.topk(routing_weights, self.num_experts_per_tok, dim=-1)\n         if self.norm_topk_prob:\n             routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n-        routing_weights = routing_weights.to(hidden_states.dtype)\n+        routing_weights = routing_weights.to(router_logits.dtype)\n         return selected_experts, routing_weights\n \n     def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:"
        },
        {
            "sha": "1f07a8e20e5ebeecfdb5f345abd5b1b9988eae3d",
            "filename": "src/transformers/models/qwen3_moe/modular_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py?ref=c620c38bb04c80bb12350bec739c298918b5675d",
            "patch": "@@ -77,7 +77,7 @@ def route_tokens_to_experts(self, hidden_states, router_logits):\n         routing_weights, selected_experts = torch.topk(routing_weights, self.num_experts_per_tok, dim=-1)\n         if self.norm_topk_prob:\n             routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n-        routing_weights = routing_weights.to(hidden_states.dtype)\n+        routing_weights = routing_weights.to(router_logits.dtype)\n         return selected_experts, routing_weights\n \n     def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:"
        },
        {
            "sha": "282327b4f96df3def3414953b4faafab0c106ee7",
            "filename": "src/transformers/models/qwen3_next/modeling_qwen3_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py?ref=c620c38bb04c80bb12350bec739c298918b5675d",
            "patch": "@@ -838,7 +838,7 @@ def route_tokens_to_experts(self, hidden_states, router_logits):\n         routing_weights, selected_experts = torch.topk(routing_weights, self.num_experts_per_tok, dim=-1)\n         if self.norm_topk_prob:\n             routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n-        routing_weights = routing_weights.to(hidden_states.dtype)\n+        routing_weights = routing_weights.to(router_logits.dtype)\n         return selected_experts, routing_weights\n \n     def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:"
        },
        {
            "sha": "60bf314c00bfc3d69ceae1f2cdfa010df9b6d2bd",
            "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py?ref=c620c38bb04c80bb12350bec739c298918b5675d",
            "patch": "@@ -1336,7 +1336,7 @@ def route_tokens_to_experts(self, hidden_states, router_logits):\n         routing_weights, selected_experts = torch.topk(routing_weights, self.num_experts_per_tok, dim=-1)\n         if self.norm_topk_prob:\n             routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n-        routing_weights = routing_weights.to(hidden_states.dtype)\n+        routing_weights = routing_weights.to(router_logits.dtype)\n         return selected_experts, routing_weights\n \n     def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -2690,7 +2690,7 @@ def route_tokens_to_experts(self, hidden_states, router_logits):\n         routing_weights, selected_experts = torch.topk(routing_weights, self.num_experts_per_tok, dim=-1)\n         if self.norm_topk_prob:\n             routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n-        routing_weights = routing_weights.to(hidden_states.dtype)\n+        routing_weights = routing_weights.to(router_logits.dtype)\n         return selected_experts, routing_weights\n \n     def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:"
        },
        {
            "sha": "fa6446241d33a4ef33bb5328bb08b54cbacd14f7",
            "filename": "src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py?ref=c620c38bb04c80bb12350bec739c298918b5675d",
            "patch": "@@ -145,7 +145,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         routing_weights = torch.nn.functional.softmax(router_logits, dim=-1, dtype=torch.float)\n         routing_weights, router_indices = torch.topk(routing_weights, self.top_k, dim=-1)\n         routing_weights = routing_weights / routing_weights.sum(dim=-1, keepdim=True)\n-        routing_weights = routing_weights.to(hidden_states.dtype)\n+        routing_weights = routing_weights.to(router_logits.dtype)\n         router_weights = torch.zeros_like(router_logits).scatter_(1, router_indices, routing_weights)\n         hidden_states = hidden_states.reshape(batch_size, -1, self.hidden_size)\n         routed_out = self.experts(hidden_states, router_weights, router_indices)"
        },
        {
            "sha": "1cd833cdfeae3d5175a2f364b92e2d477e1be452",
            "filename": "src/transformers/models/qwen3_vl_moe/modular_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py?ref=c620c38bb04c80bb12350bec739c298918b5675d",
            "patch": "@@ -379,7 +379,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         routing_weights = torch.nn.functional.softmax(router_logits, dim=-1, dtype=torch.float)\n         routing_weights, router_indices = torch.topk(routing_weights, self.top_k, dim=-1)\n         routing_weights = routing_weights / routing_weights.sum(dim=-1, keepdim=True)\n-        routing_weights = routing_weights.to(hidden_states.dtype)\n+        routing_weights = routing_weights.to(router_logits.dtype)\n         router_weights = torch.zeros_like(router_logits).scatter_(1, router_indices, routing_weights)\n         hidden_states = hidden_states.reshape(batch_size, -1, self.hidden_size)\n         routed_out = self.experts(hidden_states, router_weights, router_indices)"
        }
    ],
    "stats": {
        "total": 26,
        "additions": 13,
        "deletions": 13
    }
}