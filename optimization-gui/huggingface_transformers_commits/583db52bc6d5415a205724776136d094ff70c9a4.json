{
    "author": "buttercrab",
    "message": "Add Dia model (#38405)\n\n* add dia model\n\n* add tokenizer files\n\n* cleanup some stuff\n\n* brut copy paste code\n\n* rough cleanup of the modeling code\n\n* nuke some stuff\n\n* more nuking\n\n* more cleanups\n\n* updates\n\n* add mulitLayerEmbedding vectorization\n\n* nits\n\n* more modeling simplifications\n\n* updates\n\n* update rope\n\n* update rope\n\n* just fixup\n\n* update configuration files\n\n* more cleanup!\n\n* default config values\n\n* update\n\n* forgotten comma\n\n* another comma!\n\n* update, more cleanups\n\n* just more nits\n\n* more config cleanups\n\n* time for the encoder\n\n* fix\n\n* sa=mall nit\n\n* nits\n\n* n\n\n* refacto a bit\n\n* cleanup\n\n* update cv scipt\n\n* fix last issues\n\n* fix last nits\n\n* styling\n\n* small fixes\n\n* just run 1 generation\n\n* fixes\n\n* nits\n\n* fix conversion\n\n* fix\n\n* more fixes\n\n* full generate\n\n* ouf!\n\n* fixes!\n\n* updates\n\n* fix\n\n* fix cvrt\n\n* fixup\n\n* nits\n\n* delete wrong test\n\n* update\n\n* update\n\n* test tokenization\n\n* let's start changing things bit by bit - fix encoder step\n\n* removing custom generation, moving to GenerationMixin\n\n* add encoder decoder attention masks for generation\n\n* mask changes, correctness checked against ad29837 in dia repo\n\n* refactor a bit already --> next cache\n\n* too important not to push :)\n\n* minimal cleanup + more todos\n\n* make main overwrite modeling utils\n\n* add cfg filter & eos filter\n\n* add eos countdown & delay pattern\n\n* update eos countdown\n\n* add max step eos countdown\n\n* fix tests\n\n* fix some things\n\n* fix generation with testing\n\n* move cfg & eos stuff to logits processor\n\n* make RepetitionPenaltyLogitsProcessor flexible\n\n- can accept 3D scores like (batch_size, channel, vocab)\n\n* fix input_ids concatenation dimension in GenerationMixin for flexibility\n\n* Add DiaHangoverLogitsProcessor and DiaExponentialDecayLengthPenalty classes; refactor logits processing in DiaForConditionalGeneration to utilize new configurations and improve flexibility.\n\n* Add stopping criteria\n\n* refactor\n\n* move delay pattern from processor to modeling like musicgen.\n\n- add docs\n- change eos countdown to eos delay pattern\n\n* fix processor & fix tests\n\n* refactor types\n\n* refactor imports\n\n* format code\n\n* fix docstring to pass ci\n\n* add docstring to DiaConfig & add DiaModel to test\n\n* fix docstring\n\n* add docstring\n\n* fix some bugs\n\n* check\n\n* porting / merging results from other branch - IMPORTANT: it very likely breaks generation, the goal is to have a proper forward path first\n\n* experimental testing of left padding for first channel\n\n* whoops\n\n* Fix merge to make generation work\n\n* fix cfg filter\n\n* add position ids\n\n* add todos, break things\n\n* revert changes to generation --> we will force 2d but go 3d on custom stuff\n\n* refactor a lot, change prepare decoder ids to work with left padding (needs testing), add todos\n\n* some first fixes to get to 10. in generation\n\n* some more generation fixes / adjustment\n\n* style + rope fixes\n\n* move cfg out, simplify a few things, more todos\n\n* nit\n\n* start working on custom logit processors\n\n* nit\n\n* quick fixes\n\n* cfg top k\n\n* more refactor of logits processing, needs a decision if gen config gets the new attributes or if we move it to config or similar\n\n* lets keep changes to core code minimal, only eos scaling is questionable atm\n\n* simpler eos delay logits processor\n\n* that was for debugging :D\n\n* proof of concept rope\n\n* small fix on device mismatch\n\n* cfg fixes + delay logits max len\n\n* transformers rope\n\n* modular dia\n\n* more cleanup\n\n* keep modeling consistently 3D, generate handles 2D internally\n\n* decoder starts with bos if nothing\n\n* post processing prototype\n\n* style\n\n* lol\n\n* force sample / greedy + fixes on padding\n\n* style\n\n* fixup tokenization\n\n* nits\n\n* revert\n\n* start working on dia tests\n\n* fix a lot of tests\n\n* more test fixes\n\n* nit\n\n* more test fixes + some features to simplify code more\n\n* more cleanup\n\n* forgot that one\n\n* autodocs\n\n* small consistency fixes\n\n* fix regression\n\n* small fixes\n\n* dia feature extraction\n\n* docs\n\n* wip processor\n\n* fix processor order\n\n* processing goes brrr\n\n* transpose before\n\n* small fix\n\n* fix major bug but needs now a closer look into the custom processors esp cfg\n\n* small thing on logits\n\n* nits\n\n* simplify indices and shifts\n\n* add simpler version of padding tests back (temporarily)\n\n* add logit processor tests\n\n* starting tests on processor\n\n* fix mask application during generation\n\n* some fixes on the weights conversion\n\n* style + fixup logits order\n\n* simplify conversion\n\n* nit\n\n* remove padding tests\n\n* nits on modeling\n\n* hmm\n\n* fix tests\n\n* trigger\n\n* probably gonna be reverted, just a quick design around audio tokenizer\n\n* fixup typing\n\n* post merge + more typing\n\n* initial design for audio tokenizer\n\n* more design changes\n\n* nit\n\n* more processor tests and style related things\n\n* add to init\n\n* protect import\n\n* not sure why tbh\n\n* add another protect\n\n* more fixes\n\n* wow\n\n* it aint stopping :D\n\n* another missed type issue\n\n* ...\n\n* change design around audio tokenizer to prioritize init and go for auto - in regards to the review\n\n* change to new causal mask function + docstrings\n\n* change ternary\n\n* docs\n\n* remove todo, i dont think its essential tbh\n\n* remove pipeline as current pipelines do not fit in the current scheme, same as csm\n\n* closer to wrapping up the processor\n\n* text to audio, just for demo purposes (will likely be reverted)\n\n* check if it's this\n\n* save audio function\n\n* ensure no grad\n\n* fixes on prefixed audio, hop length is used via preprocess dac, device fixes\n\n* integration tests (tested locally on a100) + some processor utils / fixes\n\n* style\n\n* nits\n\n* another round of smaller things\n\n* docs + some fixes (generate one might be big)\n\n* msytery solved\n\n* small fix on conversion\n\n* add abstract audio tokenizer, change init check to abstract class\n\n* nits\n\n* update docs + fix some processing :D\n\n* change inheritance scheme for audio tokenizer\n\n* delete dead / unnecessary code in copied generate loop\n\n* last nits on new pipeline behavior (+ todo on tests) + style\n\n* trigger\n\n---------\n\nCo-authored-by: Arthur Zucker <arthur.zucker@gmail.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: Vasqu <antonprogamer@gmail.com>",
    "sha": "583db52bc6d5415a205724776136d094ff70c9a4",
    "files": [
        {
            "sha": "9ed80cfb0b78e1c4d9adbe7cb551884cd145f493",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -839,6 +839,8 @@\n         title: CSM\n       - local: model_doc/dac\n         title: dac\n+      - local: model_doc/dia\n+        title: Dia\n       - local: model_doc/encodec\n         title: EnCodec\n       - local: model_doc/fastspeech2_conformer"
        },
        {
            "sha": "0a36c7c0a1e160316f5229482ae216e65a423812",
            "filename": "docs/source/en/model_doc/auto.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fauto.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fauto.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fauto.md?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -350,6 +350,10 @@ The following auto classes are available for the following audio tasks.\n \n [[autodoc]] AutoModelForTextToWaveform\n \n+### AutoModelForAudioTokenization\n+\n+[[autodoc]] AutoModelForAudioTokenization\n+\n ## Multimodal\n \n The following auto classes are available for the following multimodal tasks."
        },
        {
            "sha": "67c4a3be0b6a5593377b34ab27c6cc2350323136",
            "filename": "docs/source/en/model_doc/dia.md",
            "status": "added",
            "additions": 162,
            "deletions": 0,
            "changes": 162,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fdia.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/docs%2Fsource%2Fen%2Fmodel_doc%2Fdia.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdia.md?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -0,0 +1,162 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Dia\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n+## Overview\n+\n+Dia is an opensource text-to-speech (TTS) model (1.6B parameters) developed by [Nari Labs](https://huggingface.co/nari-labs).\n+It can generate highly realistic dialogue from transcript including nonverbal communications such as laughter and coughing.\n+Furthermore, emotion and tone control is also possible via audio conditioning (voice cloning).\n+\n+**Model Architecture:**\n+Dia is an encoder-decoder transformer based on the original transformer architecture. However, some more modern features such as\n+rotational positional embeddings (RoPE) are also included. For its text portion (encoder), a byte tokenizer is utilized while\n+for the audio portion (decoder), a pretrained codec model [DAC](./dac.md) is used - DAC encodes speech into discrete codebook\n+tokens and decodes them back into audio.\n+\n+## Usage Tips\n+\n+### Generation with Text\n+\n+```python\n+from transformers import AutoProcessor, DiaForConditionalGeneration\n+\n+torch_device = \"cuda\"\n+model_checkpoint = \"buttercrab/dia-v1-1.6b\"\n+\n+text = [\"[S1] Dia is an open weights text to dialogue model.\"]\n+processor = AutoProcessor.from_pretrained(model_checkpoint)\n+inputs = processor(text=text, padding=True, return_tensors=\"pt\").to(torch_device)\n+\n+model = DiaForConditionalGeneration.from_pretrained(model_checkpoint).to(torch_device)\n+outputs = model.generate(**inputs, max_new_tokens=256)  # corresponds to around ~2s\n+\n+# save audio to a file\n+outputs = processor.batch_decode(outputs)\n+processor.save_audio(outputs, \"example.wav\")\n+\n+```\n+\n+### Generation with Text and Audio (Voice Cloning)\n+\n+```python\n+from datasets import load_dataset, Audio\n+from transformers import AutoProcessor, DiaForConditionalGeneration\n+\n+torch_device = \"cuda\"\n+model_checkpoint = \"buttercrab/dia-v1-1.6b\"\n+\n+ds = load_dataset(\"hf-internal-testing/dailytalk-dummy\", split=\"train\")\n+ds = ds.cast_column(\"audio\", Audio(sampling_rate=44100))\n+audio = ds[-1][\"audio\"][\"array\"]\n+# text is a transcript of the audio + additional text you want as new audio\n+text = [\"[S1] I know. It's going to save me a lot of money, I hope. [S2] I sure hope so for you.\"]\n+\n+processor = AutoProcessor.from_pretrained(model_checkpoint)\n+inputs = processor(text=text, audio=audio, padding=True, return_tensors=\"pt\").to(torch_device)\n+prompt_len = processor.get_audio_prompt_len(inputs[\"decoder_attention_mask\"])\n+\n+model = DiaForConditionalGeneration.from_pretrained(model_checkpoint).to(torch_device)\n+outputs = model.generate(**inputs, max_new_tokens=256)  # corresponds to around ~2s\n+\n+# retrieve actually generated audio and save to a file\n+outputs = processor.batch_decode(outputs, audio_prompt_len=prompt_len)\n+processor.save_audio(outputs, \"example_with_audio.wav\")\n+```\n+\n+### Training\n+\n+```python\n+from datasets import load_dataset, Audio\n+from transformers import AutoProcessor, DiaForConditionalGeneration\n+\n+torch_device = \"cuda\"\n+model_checkpoint = \"buttercrab/dia-v1-1.6b\"\n+\n+ds = load_dataset(\"hf-internal-testing/dailytalk-dummy\", split=\"train\")\n+ds = ds.cast_column(\"audio\", Audio(sampling_rate=44100))\n+audio = ds[-1][\"audio\"][\"array\"]\n+# text is a transcript of the audio\n+text = [\"[S1] I know. It's going to save me a lot of money, I hope.\"]\n+\n+processor = AutoProcessor.from_pretrained(model_checkpoint)\n+inputs = processor(\n+    text=text,\n+    audio=audio,\n+    generation=False,\n+    output_labels=True,\n+    padding=True,\n+    return_tensors=\"pt\"\n+).to(torch_device)\n+\n+model = DiaForConditionalGeneration.from_pretrained(model_checkpoint).to(torch_device)\n+out = model(**inputs)\n+out.loss.backward()\n+```\n+\n+\n+This model was contributed by [Jaeyong Sung](https://huggingface.co/buttercrab), [Arthur Zucker](https://huggingface.co/ArthurZ),\n+and [Anton Vlasjuk](https://huggingface.co/AntonV). The original code can be found [here](https://github.com/nari-labs/dia/).\n+\n+\n+## DiaConfig\n+\n+[[autodoc]] DiaConfig\n+\n+## DiaDecoderConfig\n+\n+[[autodoc]] DiaDecoderConfig\n+\n+## DiaEncoderConfig\n+\n+[[autodoc]] DiaEncoderConfig\n+\n+## DiaTokenizer\n+\n+[[autodoc]] DiaTokenizer\n+    - __call__\n+\n+## DiaFeatureExtractor\n+\n+[[autodoc]] DiaFeatureExtractor\n+    - __call__\n+\n+## DiaProcessor\n+\n+[[autodoc]] DiaProcessor\n+    - __call__\n+    - batch_decode\n+    - decode\n+\n+## DiaModel\n+\n+[[autodoc]] DiaModel\n+    - forward\n+\n+## DiaForConditionalGeneration\n+\n+[[autodoc]] DiaForConditionalGeneration\n+    - forward\n+    - generate"
        },
        {
            "sha": "54fa7c7e267d4de59c0c3cbb056e060de962ff55",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -271,7 +271,6 @@ def __init__(self, **kwargs):\n         self.pad_token_id = kwargs.pop(\"pad_token_id\", None)\n         self.eos_token_id = kwargs.pop(\"eos_token_id\", None)\n         self.sep_token_id = kwargs.pop(\"sep_token_id\", None)\n-\n         self.decoder_start_token_id = kwargs.pop(\"decoder_start_token_id\", None)\n \n         # task specific arguments"
        },
        {
            "sha": "d4c08e270bbcbb4c561bbb23d871d25650aa5b86",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 221,
            "deletions": 0,
            "changes": 221,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -2975,3 +2975,224 @@ def expected_mean_g_value(self, vocab_size: int, coinflip_prob: float = 0.5) ->\n             The expected mean g-value for watermarked text.\n         \"\"\"\n         return coinflip_prob + coinflip_prob * (1 - coinflip_prob) * (1 - (1 / vocab_size))\n+\n+\n+class DiaClassifierFreeGuidanceLogitsProcessor(LogitsProcessor):\n+    r\"\"\"\n+    [`LogitsProcessor`] for classifier free guidance (CFG). Similar to the original\n+    `ClassifierFreeGuidanceLogitsProcessor` with some modifications on the overall\n+    calculation, e.g. conditioned logits centered, and an additional top k selection\n+    option.\n+\n+    <Tip warning={true}>\n+\n+    This logits processor is exclusively compatible with\n+    [Dia](https://huggingface.co/docs/transformers/main/en/model_doc/dia)\n+\n+    </Tip>\n+\n+    Args:\n+        guidance_scale (float):\n+            The guidance scale for classifier free guidance (CFG). CFG is enabled by setting `guidance_scale > 1`.\n+            Higher guidance scale encourages the model to generate samples that are more closely linked to the input\n+            prompt, usually at the expense of poorer quality.\n+        guidance_top_k (int, *optional*):\n+            The number of highest probability vocabulary tokens to keep for top-k-filtering. However, we do not keep\n+            the logits of the combined CFG output, but the conditioned output only.\n+    \"\"\"\n+\n+    def __init__(self, guidance_scale: float, guidance_top_k: Optional[int] = None):\n+        if guidance_scale > 1:\n+            self.guidance_scale = guidance_scale\n+        else:\n+            raise ValueError(\n+                \"Require guidance scale >1 to use the classifier free guidance processor, got guidance scale \"\n+                f\"{guidance_scale}.\"\n+            )\n+\n+        self.guidance_top_k = guidance_top_k\n+        if self.guidance_top_k is not None and self.guidance_top_k < 1:\n+            raise ValueError(\n+                f\"`guidance_top_k` has to be a strictly positive integer if given, but is {self.guidance_top_k}\"\n+            )\n+\n+    @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n+    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n+        # simple check to make sure we have compatible batch sizes between our\n+        # logits scores (cond + uncond) and input ids (cond only)\n+        if scores.shape[0] != 2 * input_ids.shape[0]:\n+            raise ValueError(\n+                f\"Logits should have twice the batch size of the input ids, the first half of batches corresponding to \"\n+                f\"the conditional inputs, and the second half of batches corresponding to the unconditional inputs. Got \"\n+                f\"batch size {scores.shape[0]} for the logits and {input_ids.shape[0]} for the input ids.\"\n+            )\n+        # Base CFG with center on cond_logits\n+        unguided_bsz = scores.shape[0] // 2\n+        cond_logits, uncond_logits = scores.split(unguided_bsz, dim=0)\n+        scores_processed = cond_logits + (cond_logits - uncond_logits) * self.guidance_scale\n+\n+        # Optional CFG top k filtering\n+        if self.guidance_top_k is not None:\n+            # Create top k based on the combined CFG output\n+            _, top_k_indices = torch.topk(scores_processed, k=self.guidance_top_k, dim=-1)\n+            top_k_mask = torch.ones_like(scores_processed, dtype=torch.bool)\n+            top_k_mask = top_k_mask.scatter(dim=-1, index=top_k_indices, value=False)\n+            # Only return conditioned logits with top k\n+            scores_processed = cond_logits.masked_fill(top_k_mask, -float(\"inf\"))\n+\n+        return scores_processed\n+\n+\n+class DiaEOSChannelFilterLogitsProcessor(LogitsProcessor):\n+    r\"\"\"Specialized processor that ensures certain properties around EOS sampling:\n+        1. Only channel 0 can generate EOS\n+        2. If channel 0 has EOS with highest logit, it will be the only candidate\n+        3. If channel 0 has EOS not with highest logit, it will be suppressed\n+\n+    2. and 3. are especially important in contexts where we allow sampling to guarantee the\n+    respective tokens to be (not) sampled.\n+\n+    <Tip warning={true}>\n+\n+    This logits processor is exclusively compatible with\n+    [Dia](https://huggingface.co/docs/transformers/en/model_doc/dia).\n+\n+    </Tip>\n+\n+    Args:\n+        num_channels (`int`):\n+            Number of audio codebooks. Simplifies access to the first channel on the logits.\n+        eos_token_id (`int`):\n+            The id of *end-of-sequence* token.\n+    \"\"\"\n+\n+    def __init__(self, num_channels: int, eos_token_id: int):\n+        if num_channels < 1:\n+            raise ValueError(f\"Audio codebooks need at least one channel, but found {num_channels} channels.\")\n+        if eos_token_id < 1:\n+            raise ValueError(f\"Expected `eos_token_id` to be a positive integer, found {eos_token_id} instead.\")\n+\n+        self.num_channels = num_channels\n+        self.eos_id = eos_token_id\n+\n+    @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n+    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n+        # Reshape for easier channel indexing [B, C, V]\n+        scores = scores.reshape(-1, self.num_channels, scores.shape[-1])\n+\n+        # EOS filter\n+        # 1. Condition: Only the first channel can generate the EOS token\n+        # Side condition of disabling generation of special tokens (e.g. audio pad, bos, ...)\n+        # (Assumes them to be greater than audio eos token position)\n+        scores[:, 1:, self.eos_id :] = torch.full_like(\n+            scores[:, 1:, self.eos_id :],\n+            fill_value=-float(\"inf\"),\n+        )\n+        scores[:, 0, self.eos_id + 1 :] = torch.full_like(\n+            scores[:, 0, self.eos_id + 1 :],\n+            fill_value=-float(\"inf\"),\n+        )\n+\n+        # 2+3 Conditions: Force/Suppress EOS if (not) highest logit\n+        # Reshape back to original shape\n+        scores = scores.view(-1, scores.shape[-1])\n+\n+        # Sample highest tokens\n+        top_logit_indices = torch.argmax(scores, dim=-1)\n+\n+        # 2. Force EOS\n+        eos_highest_mask = top_logit_indices == self.eos_id\n+        mask_eos_highest = torch.zeros_like(scores, dtype=torch.bool)\n+        mask_eos_highest[eos_highest_mask, : self.eos_id] = True\n+        scores = scores.masked_fill(mask_eos_highest, -float(\"inf\"))\n+\n+        # 3. Suppress EOS\n+        eos_not_highest_mask = top_logit_indices != self.eos_id\n+        mask_eos_unless_highest = torch.zeros_like(scores, dtype=torch.bool)\n+        mask_eos_unless_highest[eos_not_highest_mask, self.eos_id] = True\n+        scores = scores.masked_fill(mask_eos_unless_highest, -float(\"inf\"))\n+\n+        return scores\n+\n+\n+class DiaEOSDelayPatternLogitsProcessor(LogitsProcessor):\n+    r\"\"\"Special logits processor to handle the generation of the EOS token in Dia.\n+    This is due to the fact that Dia does not allow the generation of EOS in all\n+    channels except the first channel (C0).\n+\n+    Hence, based on the delay pattern, an EOS is forced after the respective delays\n+    in the channels. For example, if the delay pattern is [0, 2, 3, 4]:\n+\n+            s   s+1 s+2 s+3 s+4 s+5 ...\n+            |   |   |   |   |   |\n+        C0: EOS PAD PAD PAD PAD PAD ...\n+        C1: x   x   EOS PAD PAD PAD ...\n+        C2: x   x   x   EOS PAD PAD ...\n+        C3: x   x   x   x   EOS PAD ...\n+\n+    If the first channel generated EOS at step s, channels Cx are forced to generate\n+    theirs at the respective delays (s+2, s+3, s+4). Subsequent padding tokens are\n+    handled by the `EosTokenCriteria` when an EOS has been detected.\n+\n+    <Tip warning={true}>\n+\n+    This logits processor is exclusively compatible with\n+    [Dia](https://huggingface.co/docs/transformers/en/model_doc/dia).\n+\n+    </Tip>\n+\n+    Args:\n+        delay_pattern (`List[int]`):\n+            The delays per channel in the audio codebooks.\n+        eos_token_id (`int`):\n+            The id of *end-of-sequence* token.\n+        max_generation_len (`int`):\n+            The max sequence length that can be generated.\n+        device (`str`, *optional*, defaults to `\"cpu\"`):\n+            The device to allocate the tensors on.\n+    \"\"\"\n+\n+    def __init__(self, delay_pattern: list[int], eos_token_id: int, max_generation_len: int, device: str = \"cpu\"):\n+        self.num_channels = len(delay_pattern)\n+        # Update during first iteration\n+        self.active_batches = None\n+        self.delay_pattern = torch.tensor(delay_pattern, device=device, dtype=torch.int)[None, :]\n+        self.eos_token_id = eos_token_id\n+        self.max_generation_len = max_generation_len - max(delay_pattern) - 1\n+        self.device = device\n+\n+    @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n+    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n+        # Reshape for easier channel indexing [B, C, V]\n+        scores = scores.reshape(-1, self.num_channels, scores.shape[-1])\n+\n+        # Initialize / expand values on first iteration\n+        if self.active_batches is None:\n+            self.delay_pattern = self.delay_pattern.repeat(scores.shape[0], 1)\n+            self.active_batches = torch.zeros(size=(scores.shape[0],), device=self.device, dtype=torch.bool)\n+\n+        # Check if eos has been generated in any batch\n+        channel_generated_eos = torch.argmax(scores, dim=-1)[:, 0] == self.eos_token_id\n+        # Check if max len has been reached\n+        reached_max_len = input_ids.shape[1] == self.max_generation_len\n+\n+        # Update active batches\n+        self.active_batches |= channel_generated_eos\n+        self.active_batches |= reached_max_len\n+\n+        # Find channels that need to force eos\n+        forced_eos_channels = self.active_batches[:, None] & (self.delay_pattern == 0)\n+        # Use indexing to avoid issues on all `False` by having empty tensors in that case\n+        idx_bsz, idx_channel = forced_eos_channels.nonzero(as_tuple=True)\n+\n+        # Force eos if delay is kicking in\n+        scores[idx_bsz, idx_channel, :] = -float(\"inf\")\n+        scores[idx_bsz, idx_channel, self.eos_token_id] = 0.0\n+\n+        # Reshape back to [B * C, V]\n+        scores = scores.reshape(-1, scores.shape[-1])\n+\n+        # Update amount of delay left for each channel\n+        self.delay_pattern -= self.active_batches[:, None].int()\n+\n+        return scores"
        },
        {
            "sha": "ea2bd32aa3e63528936e89bd1dc8a3f6670198e3",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 24,
            "deletions": 0,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -26,6 +26,7 @@\n import shutil\n import tempfile\n import warnings\n+from abc import abstractmethod\n from collections import defaultdict\n from concurrent.futures import ThreadPoolExecutor, as_completed\n from contextlib import contextmanager\n@@ -5884,3 +5885,26 @@ class AttentionInterface(GeneralInterface):\n \n # Global AttentionInterface shared by all models which do not need to overwrite any of the existing ones\n ALL_ATTENTION_FUNCTIONS: AttentionInterface = AttentionInterface()\n+\n+\n+class PreTrainedAudioTokenizerBase(PreTrainedModel):\n+    \"\"\"\n+    Class that additionally defines the behavior of any `audio_tokenizer` to be added.\n+    Characteristic for any of them:\n+        1. Encode raw audio into discrete audio codebooks (with x channels)\n+        2. Decode from discrete audio codebooks back to raw audio\n+    It is possible that they can decode in different ways given a different representation\n+    but they are forced to support 2. nonetheless, e.g. see `DAC`.\n+    \"\"\"\n+\n+    @abstractmethod\n+    def encode(self, input_values: torch.Tensor, *args, **kwargs):\n+        \"\"\"\n+        Encode raw audio retrieved from a respective `FeatureExtractor` into discrete audio codebooks (with x channels)\n+        \"\"\"\n+        pass\n+\n+    @abstractmethod\n+    def decode(self, audio_codes: torch.Tensor, *args, **kwargs):\n+        \"\"\"Decode from discrete audio codebooks back to raw audio\"\"\"\n+        pass"
        },
        {
            "sha": "7b2332d89f40dc45d7dfa77a7b2f19eeaa40f6d2",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -88,6 +88,7 @@\n     from .depth_anything import *\n     from .depth_pro import *\n     from .detr import *\n+    from .dia import *\n     from .dialogpt import *\n     from .diffllama import *\n     from .dinat import *"
        },
        {
            "sha": "71ad6eaadeb52ff2f6056a6471c7eb34f84f5a2d",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -106,6 +106,7 @@\n         (\"depth_pro\", \"DepthProConfig\"),\n         (\"deta\", \"DetaConfig\"),\n         (\"detr\", \"DetrConfig\"),\n+        (\"dia\", \"DiaConfig\"),\n         (\"diffllama\", \"DiffLlamaConfig\"),\n         (\"dinat\", \"DinatConfig\"),\n         (\"dinov2\", \"Dinov2Config\"),\n@@ -478,6 +479,7 @@\n         (\"depth_pro\", \"DepthPro\"),\n         (\"deta\", \"DETA\"),\n         (\"detr\", \"DETR\"),\n+        (\"dia\", \"Dia\"),\n         (\"dialogpt\", \"DialoGPT\"),\n         (\"diffllama\", \"DiffLlama\"),\n         (\"dinat\", \"DiNAT\"),"
        },
        {
            "sha": "d54ca4b0f5aed0945a313c7da9475332612fe422",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -55,6 +55,7 @@\n         (\"deformable_detr\", \"DeformableDetrFeatureExtractor\"),\n         (\"deit\", \"DeiTFeatureExtractor\"),\n         (\"detr\", \"DetrFeatureExtractor\"),\n+        (\"dia\", \"DiaFeatureExtractor\"),\n         (\"dinat\", \"ViTFeatureExtractor\"),\n         (\"donut-swin\", \"DonutFeatureExtractor\"),\n         (\"dpt\", \"DPTFeatureExtractor\"),"
        },
        {
            "sha": "add9d09b0e2dd30851d9332da784cde12a9ed75a",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -99,6 +99,7 @@\n         (\"depth_pro\", \"DepthProModel\"),\n         (\"deta\", \"DetaModel\"),\n         (\"detr\", \"DetrModel\"),\n+        (\"dia\", \"DiaModel\"),\n         (\"diffllama\", \"DiffLlamaModel\"),\n         (\"dinat\", \"DinatModel\"),\n         (\"dinov2\", \"Dinov2Model\"),\n@@ -472,6 +473,7 @@\n         (\"data2vec-text\", \"Data2VecTextForMaskedLM\"),\n         (\"deberta\", \"DebertaForMaskedLM\"),\n         (\"deberta-v2\", \"DebertaV2ForMaskedLM\"),\n+        (\"dia\", \"DiaForConditionalGeneration\"),\n         (\"distilbert\", \"DistilBertForMaskedLM\"),\n         (\"electra\", \"ElectraForMaskedLM\"),\n         (\"encoder-decoder\", \"EncoderDecoderModel\"),\n@@ -1059,6 +1061,7 @@\n \n MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES = OrderedDict(\n     [\n+        (\"dia\", \"DiaForConditionalGeneration\"),\n         (\"granite_speech\", \"GraniteSpeechForConditionalGeneration\"),\n         (\"kyutai_speech_to_text\", \"KyutaiSpeechToTextForConditionalGeneration\"),\n         (\"moonshine\", \"MoonshineForConditionalGeneration\"),\n@@ -1629,6 +1632,12 @@\n     ]\n )\n \n+MODEL_FOR_AUDIO_TOKENIZATION_NAMES = OrderedDict(\n+    [\n+        (\"dac\", \"DacModel\"),\n+    ]\n+)\n+\n MODEL_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, MODEL_MAPPING_NAMES)\n MODEL_FOR_PRETRAINING_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, MODEL_FOR_PRETRAINING_MAPPING_NAMES)\n MODEL_WITH_LM_HEAD_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, MODEL_WITH_LM_HEAD_MAPPING_NAMES)\n@@ -1737,6 +1746,8 @@\n \n MODEL_FOR_IMAGE_TO_IMAGE_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, MODEL_FOR_IMAGE_TO_IMAGE_MAPPING_NAMES)\n \n+MODEL_FOR_AUDIO_TOKENIZATION_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, MODEL_FOR_AUDIO_TOKENIZATION_NAMES)\n+\n \n class AutoModelForMaskGeneration(_BaseAutoModelClass):\n     _model_mapping = MODEL_FOR_MASK_GENERATION_MAPPING\n@@ -2034,6 +2045,15 @@ class AutoModelForMaskedImageModeling(_BaseAutoModelClass):\n AutoModelForMaskedImageModeling = auto_class_update(AutoModelForMaskedImageModeling, head_doc=\"masked image modeling\")\n \n \n+class AutoModelForAudioTokenization(_BaseAutoModelClass):\n+    _model_mapping = MODEL_FOR_AUDIO_TOKENIZATION_MAPPING\n+\n+\n+AutoModelForAudioTokenization = auto_class_update(\n+    AutoModelForAudioTokenization, head_doc=\"audio tokenization through codebooks\"\n+)\n+\n+\n class AutoModelWithLMHead(_AutoModelWithLMHead):\n     @classmethod\n     def from_config(cls, config):\n@@ -2059,6 +2079,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n __all__ = [\n     \"MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING\",\n     \"MODEL_FOR_AUDIO_FRAME_CLASSIFICATION_MAPPING\",\n+    \"MODEL_FOR_AUDIO_TOKENIZATION_MAPPING\",\n     \"MODEL_FOR_AUDIO_XVECTOR_MAPPING\",\n     \"MODEL_FOR_BACKBONE_MAPPING\",\n     \"MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING\",\n@@ -2106,6 +2127,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n     \"AutoBackbone\",\n     \"AutoModelForAudioClassification\",\n     \"AutoModelForAudioFrameClassification\",\n+    \"AutoModelForAudioTokenization\",\n     \"AutoModelForAudioXVector\",\n     \"AutoModelForCausalLM\",\n     \"AutoModelForCTC\","
        },
        {
            "sha": "bccfe3e6d57f777c3f677abf6c6dcce90a80be03",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -61,6 +61,7 @@\n         (\"clvp\", \"ClvpProcessor\"),\n         (\"colpali\", \"ColPaliProcessor\"),\n         (\"colqwen2\", \"ColQwen2Processor\"),\n+        (\"dia\", \"DiaProcessor\"),\n         (\"emu3\", \"Emu3Processor\"),\n         (\"flava\", \"FlavaProcessor\"),\n         (\"fuyu\", \"FuyuProcessor\"),"
        },
        {
            "sha": "0456e1945ca4c99027b83783643a7a6d85937d37",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -177,6 +177,7 @@\n                 \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n             ),\n         ),\n+        (\"dia\", (\"DiaTokenizer\", None)),\n         (\n             \"diffllama\",\n             ("
        },
        {
            "sha": "191e7af89e3fa9e496d6a12686e0890d5fc32ac6",
            "filename": "src/transformers/models/dac/modeling_dac.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -23,7 +23,7 @@\n import torch.nn as nn\n import torch.nn.functional as F\n \n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import PreTrainedAudioTokenizerBase\n from ...utils import ModelOutput, auto_docstring\n from .configuration_dac import DacConfig\n \n@@ -471,7 +471,7 @@ def forward(self, hidden_state):\n \n \n @auto_docstring\n-class DacPreTrainedModel(PreTrainedModel):\n+class DacPreTrainedModel(PreTrainedAudioTokenizerBase):\n     config_class = DacConfig\n     base_model_prefix = \"dac\"\n     main_input_name = \"input_values\""
        },
        {
            "sha": "d738fbc087888597da19735271366d4e35ab708c",
            "filename": "src/transformers/models/dia/__init__.py",
            "status": "added",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fdia%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fdia%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2F__init__.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -0,0 +1,31 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_dia import *\n+    from .feature_extraction_dia import *\n+    from .generation_dia import *\n+    from .modeling_dia import *\n+    from .processing_dia import *\n+    from .tokenization_dia import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "90ace73b3c96e9c4ed333696e72e0900a24a502b",
            "filename": "src/transformers/models/dia/configuration_dia.py",
            "status": "added",
            "additions": 376,
            "deletions": 0,
            "changes": 376,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fdia%2Fconfiguration_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fdia%2Fconfiguration_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fconfiguration_dia.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -0,0 +1,376 @@\n+# coding=utf-8\n+# Copyright 2025 The Nari Labs and HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Dia model configuration\"\"\"\n+\n+from typing import Optional\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class DiaEncoderConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`DiaEncoder`]. It is used to instantiate a Dia\n+    encoder according to the specified arguments, defining the encoder architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        max_position_embeddings (`int`, *optional*, defaults to 1024):\n+            The maximum sequence length that this model might ever be used with.\n+        num_hidden_layers (`int`, *optional*, defaults to 12):\n+            Number of hidden layers in the Transformer encoder.\n+        hidden_size (`int`, *optional*, defaults to 1024):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 16):\n+            Number of key and value heads for each attention layer in the Transformer encoder.\n+        head_dim (`int`, *optional*, defaults to 128):\n+            Dimensionality of the attention head.\n+        intermediate_size (`int`, *optional*, defaults to 4096):\n+            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n+        norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the normalization layers.\n+        vocab_size (`int`, *optional*, defaults to 256):\n+            Vocabulary size of the Dia model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`DiaModel`].\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"swish\"` and `\"gelu_new\"` are supported.\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+    \"\"\"\n+\n+    model_type = \"dia_encoder\"\n+\n+    def __init__(\n+        self,\n+        max_position_embeddings: int = 1024,\n+        num_hidden_layers: int = 12,\n+        hidden_size: int = 1024,\n+        num_attention_heads: int = 16,\n+        num_key_value_heads: int = 16,\n+        head_dim: int = 128,\n+        intermediate_size: int = 4096,\n+        norm_eps: float = 1e-5,\n+        vocab_size: int = 256,\n+        hidden_act: str = \"silu\",\n+        rope_theta: float = 10000.0,\n+        rope_scaling: Optional[dict] = None,\n+        initializer_range: float = 0.02,\n+        **kwargs,\n+    ):\n+        self.max_position_embeddings = max_position_embeddings\n+        self.num_hidden_layers = num_hidden_layers\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_attention_heads = num_attention_heads\n+        self.head_dim = head_dim\n+        self.norm_eps = norm_eps\n+        self.vocab_size = vocab_size\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n+        self.initializer_range = initializer_range\n+        super().__init__(**kwargs)\n+\n+\n+class DiaDecoderConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`DiaDecoder`]. It is used to instantiate a Dia\n+    decoder according to the specified arguments, defining the decoder architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        max_position_embeddings (`int`, *optional*, defaults to 3072):\n+            The maximum sequence length that this model might ever be used with.\n+        num_hidden_layers (`int`, *optional*, defaults to 18):\n+            Number of hidden layers in the Transformer decoder.\n+        hidden_size (`int`, *optional*, defaults to 2048):\n+            Dimensionality of the decoder layers and the pooler layer.\n+        intermediate_size (`int`, *optional*, defaults to 8192):\n+            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 4):\n+            Number of key and value heads for each attention layer in the Transformer decoder.\n+        head_dim (`int`, *optional*, defaults to 128):\n+            Dimensionality of the attention head.\n+        cross_num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each cross-attention layer in the Transformer decoder.\n+        cross_head_dim (`int`, *optional*, defaults to 128):\n+            Dimensionality of the cross-attention head.\n+        cross_num_key_value_heads (`int`, *optional*, defaults to 16):\n+            Number of key and value heads for each cross-attention layer in the Transformer decoder.\n+        cross_hidden_size (`int`, *optional*, defaults to 1024):\n+            Dimensionality of the cross-attention layers.\n+        norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the normalization layers.\n+        vocab_size (`int`, *optional*, defaults to 1028):\n+            Vocabulary size of the Dia model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`DiaModel`].\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder. If string, `\"gelu\"`, `\"relu\"`,\n+            `\"swish\"` and `\"gelu_new\"` are supported.\n+        num_channels (`int`, *optional*, defaults to 9):\n+            Number of channels for the Dia decoder.\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models).\n+        is_encoder_decoder (`bool`, *optional*, defaults to `True`):\n+            Indicating that this model is part of an encoder-decoder architecture.\n+    \"\"\"\n+\n+    model_type = \"dia_decoder\"\n+\n+    def __init__(\n+        self,\n+        max_position_embeddings: int = 3072,\n+        num_hidden_layers: int = 18,\n+        hidden_size: int = 2048,\n+        intermediate_size: int = 8192,\n+        num_attention_heads: int = 16,\n+        num_key_value_heads: int = 4,\n+        head_dim: int = 128,\n+        cross_num_attention_heads: int = 16,\n+        cross_head_dim: int = 128,\n+        cross_num_key_value_heads: int = 16,\n+        cross_hidden_size: int = 1024,\n+        norm_eps: float = 1e-5,\n+        vocab_size: int = 1028,\n+        hidden_act: str = \"silu\",\n+        num_channels: int = 9,\n+        rope_theta: float = 10000.0,\n+        rope_scaling: Optional[dict] = None,\n+        initializer_range: float = 0.02,\n+        use_cache: bool = True,\n+        is_encoder_decoder: bool = True,\n+        **kwargs,\n+    ):\n+        self.max_position_embeddings = max_position_embeddings\n+        self.num_hidden_layers = num_hidden_layers\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        self.head_dim = head_dim\n+        self.cross_num_key_value_heads = cross_num_key_value_heads\n+        self.cross_num_attention_heads = cross_num_attention_heads\n+        self.cross_head_dim = cross_head_dim\n+        self.cross_hidden_size = cross_hidden_size\n+        self.norm_eps = norm_eps\n+        self.vocab_size = vocab_size\n+        self.hidden_act = hidden_act\n+        self.num_channels = num_channels\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n+        self.initializer_range = initializer_range\n+        self.use_cache = use_cache\n+        super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n+\n+\n+class DiaConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`DiaModel`]. It is used to instantiate a\n+    Dia model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the\n+    [nari-labs/Dia-1.6B](https://huggingface.co/nari-labs/Dia-1.6B) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        encoder_config (`DiaEncoderConfig`, *optional*):\n+            Configuration for the encoder part of the model. If not provided, a default `DiaEncoderConfig` will be used.\n+        decoder_config (`DiaDecoderConfig`, *optional*):\n+            Configuration for the decoder part of the model. If not provided, a default `DiaDecoderConfig` will be used.\n+        norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the normalization layers.\n+        is_encoder_decoder (`bool`, *optional*, defaults to `True`):\n+            Indicating that this model uses an encoder-decoder architecture.\n+        pad_token_id (`int`, *optional*, defaults to 1025):\n+            Padding token id.\n+        eos_token_id (`int`, *optional*, defaults to 1024):\n+            End of stream token id.\n+        bos_token_id (`int`, *optional*, defaults to 1026):\n+            Beginning of stream token id.\n+        delay_pattern (`list[int]`, *optional*, defaults to `[0, 8, 9, 10, 11, 12, 13, 14, 15]`):\n+            The delay pattern for the decoder. The length of this list must match `decoder_config.num_channels`.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models).\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import DiaConfig, DiaModel\n+\n+    >>> # Initializing a DiaConfig with default values\n+    >>> configuration = DiaConfig()\n+\n+    >>> # Initializing a DiaModel (with random weights) from the configuration\n+    >>> model = DiaModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\n+    \"\"\"\n+\n+    model_type = \"dia\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    sub_configs = {\"encoder_config\": DiaEncoderConfig, \"decoder_config\": DiaDecoderConfig}\n+\n+    def __init__(\n+        self,\n+        encoder_config: Optional[DiaEncoderConfig] = None,\n+        decoder_config: Optional[DiaDecoderConfig] = None,\n+        norm_eps: float = 1e-5,\n+        is_encoder_decoder: bool = True,\n+        pad_token_id: int = 1025,\n+        eos_token_id: int = 1024,\n+        bos_token_id: int = 1026,\n+        delay_pattern: Optional[list[int]] = None,\n+        initializer_range: float = 0.02,\n+        use_cache: bool = True,\n+        **kwargs,\n+    ):\n+        if isinstance(encoder_config, dict):\n+            encoder_config = DiaEncoderConfig(**encoder_config)\n+        if isinstance(decoder_config, dict):\n+            decoder_config = DiaDecoderConfig(**decoder_config)\n+        self.encoder_config = encoder_config if encoder_config is not None else DiaEncoderConfig()\n+        self.decoder_config = decoder_config if decoder_config is not None else DiaDecoderConfig()\n+        self.norm_eps = norm_eps\n+        self.delay_pattern = delay_pattern if delay_pattern is not None else [0, 8, 9, 10, 11, 12, 13, 14, 15]\n+        self.initializer_range = initializer_range\n+        self.use_cache = use_cache\n+\n+        assert self.decoder_config.num_channels == len(self.delay_pattern), (\n+            \"Number of channels must match delay pattern length.\"\n+        )\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            eos_token_id=eos_token_id,\n+            bos_token_id=bos_token_id,\n+            is_encoder_decoder=is_encoder_decoder,\n+            **kwargs,\n+        )\n+\n+    def get_text_config(self, decoder=False):\n+        \"\"\"Defaulting to audio config as it's the decoder in this case which is usually the text backbone\"\"\"\n+        return self.decoder_config\n+\n+\n+__all__ = [\"DiaConfig\", \"DiaEncoderConfig\", \"DiaDecoderConfig\"]"
        },
        {
            "sha": "3a33860f6be908cdc85d43c2f9fcd0fcc39a2d35",
            "filename": "src/transformers/models/dia/convert_dia_to_hf.py",
            "status": "added",
            "additions": 199,
            "deletions": 0,
            "changes": 199,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fdia%2Fconvert_dia_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fdia%2Fconvert_dia_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fconvert_dia_to_hf.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -0,0 +1,199 @@\n+# coding=utf-8\n+# Copyright 2025 The Nari Labs and HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Converts a Dia model in Nari Labs format to Hugging Face format.\"\"\"\n+\n+import argparse\n+import os\n+import re\n+\n+import torch\n+from huggingface_hub import snapshot_download\n+from safetensors.torch import load_file\n+\n+from transformers import (\n+    DacModel,\n+    DiaConfig,\n+    DiaFeatureExtractor,\n+    DiaForConditionalGeneration,\n+    DiaProcessor,\n+    DiaTokenizer,\n+    GenerationConfig,\n+)\n+from transformers.utils.import_utils import _is_package_available\n+\n+\n+# Provide just the list of layer keys you want to fix\n+shape_mappings = [\n+    \"encoder.layers.*.mlp.gate_up_proj.weight\",\n+    \"encoder.layers.*.mlp.down_proj.weight\",\n+    \"encoder.layers.*.self_attention.q_proj.weight\",\n+    \"encoder.layers.*.self_attention.k_proj.weight\",\n+    \"encoder.layers.*.self_attention.v_proj.weight\",\n+    \"encoder.layers.*.self_attention.o_proj.weight\",\n+    \"decoder.layers.*.mlp.gate_up_proj.weight\",\n+    \"decoder.layers.*.mlp.down_proj.weight\",\n+    \"decoder.layers.*.self_attention.q_proj.weight\",\n+    \"decoder.layers.*.self_attention.k_proj.weight\",\n+    \"decoder.layers.*.self_attention.v_proj.weight\",\n+    \"decoder.layers.*.self_attention.o_proj.weight\",\n+    \"decoder.layers.*.cross_attention.q_proj.weight\",\n+    \"decoder.layers.*.cross_attention.k_proj.weight\",\n+    \"decoder.layers.*.cross_attention.v_proj.weight\",\n+    \"decoder.layers.*.cross_attention.o_proj.weight\",\n+    \"decoder.logits_dense.weight\",\n+]\n+\n+# Provide renamings here\n+rename_mapping = {\n+    \"mlp.wo\": \"mlp.down_proj\",\n+    \"mlp.wi_fused\": \"mlp.gate_up_proj\",\n+}\n+\n+\n+def get_generation_config(config):\n+    model_generation_config = GenerationConfig.from_model_config(config)\n+    model_generation_config._from_model_config = False\n+    model_generation_config.do_sample = True\n+    model_generation_config.top_k = 45\n+    model_generation_config.top_p = 0.95\n+    model_generation_config.temperature = 1.2\n+    model_generation_config.guidance_scale = 3.0\n+    model_generation_config.max_length = 3072  # Decoder max length\n+\n+    return model_generation_config\n+\n+\n+def convert_dia_model_to_hf(checkpoint_path, verbose=False):\n+    \"\"\"\n+    Converts a Dia model in Nari Labs format to Hugging Face format.\n+    Args:\n+        checkpoint_path (`str`):\n+            Path to the downloaded checkpoints.\n+        verbose (`bool`, *optional*)\n+            Whether to print information during conversion.\n+    \"\"\"\n+    # Download from HF Hub if checkpoint_path is None\n+    checkpoint_path = snapshot_download(repo_id=checkpoint_path, allow_patterns=[\"*.pth\", \"*.safetensors\"])\n+    print(f\"Downloaded checkpoint from Hugging Face Hub: {checkpoint_path}\")\n+\n+    # Initialize base model with default config == 1.6B model\n+    with torch.device(\"meta\"):\n+        hf_model = DiaForConditionalGeneration(config=DiaConfig())\n+    hf_model_dict = hf_model.state_dict()\n+    hf_model_keys = hf_model_dict.keys()\n+\n+    # Iterate through dir to catch all respective files - prefers safetensors but allows pt\n+    files = os.listdir(checkpoint_path)\n+    for file in files:\n+        if file.endswith(\".safetensors\"):\n+            load_function = load_file\n+        elif file.endswith(\".pth\"):\n+            load_function = torch.load\n+    checkpoint_path = os.path.join(checkpoint_path, files[0])\n+    nari_state_dict = load_function(checkpoint_path, \"cpu\")\n+\n+    # Conversion starts here\n+    converted_state_dict = {}\n+    embeddings = {}\n+    for key, tensor in nari_state_dict.items():\n+        # add prefix\n+        key = \"model.\" + key\n+\n+        # rename some weights\n+        for original, rename in rename_mapping.items():\n+            if original in key:\n+                key = re.sub(original, rename, key)\n+\n+        # decoder multi channel\n+        if \"embeddings\" in key:\n+            embeddings_key = key.rsplit(\".\", 2)[0] + \".embed.weight\"\n+            if embeddings_key in embeddings:\n+                embeddings[embeddings_key] += [tensor]\n+            else:\n+                embeddings[embeddings_key] = [tensor]\n+            continue\n+        elif re.sub(r\"\\d+\", \"*\", key).removeprefix(\"model.\") in shape_mappings:\n+            # add exception to the head\n+            if \"logits_dense\" in key:\n+                key = re.sub(\"decoder.logits_dense\", \"logits_dense\", key).removeprefix(\"model.\")\n+\n+            # dense general\n+            if key in hf_model_keys:\n+                tensor_shape = tensor.shape\n+                target_shape = hf_model_dict[key].shape\n+                try:\n+                    tensor = tensor.reshape(target_shape[1], target_shape[0]).T\n+                    if verbose:\n+                        print(f\"{key}: transpose reshaped from {tensor_shape} to {target_shape}\")\n+                except Exception as e:\n+                    print(f\"WARNING: Could not reshape {key}: {e}\")\n+\n+        converted_state_dict[key] = tensor\n+\n+    # Combining the embeddings as last step\n+    embeddings = {k: torch.cat(v, dim=0) for k, v in embeddings.items()}\n+    converted_state_dict.update(embeddings)\n+\n+    # Load converted weights into HF model\n+    hf_model.load_state_dict(converted_state_dict, assign=True)\n+\n+    # Overwrite generation config\n+    hf_model.generation_config = get_generation_config(DiaConfig())\n+\n+    return hf_model\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    # # Required parameters\n+    parser.add_argument(\n+        \"--checkpoint_path\", type=str, default=\"nari-labs/Dia-1.6B\", help=\"Path to the downloaded checkpoints\"\n+    )\n+    parser.add_argument(\n+        \"--pytorch_dump_folder_path\", default=\"AntonV/Dia-1.6B\", type=str, help=\"Path to the output PyTorch model.\"\n+    )\n+    parser.add_argument(\n+        \"--convert_preprocessor\",\n+        type=bool,\n+        default=True,\n+        help=\"Whether or not the preprocessor (tokenizer + feature extractor) should be converted along with the model.\",\n+    )\n+    parser.add_argument(\n+        \"--verbose\",\n+        type=bool,\n+        default=True,\n+        help=\"Whether or not to log information during conversion.\",\n+    )\n+    args = parser.parse_args()\n+\n+    model = convert_dia_model_to_hf(args.checkpoint_path, args.verbose)\n+    if args.convert_preprocessor:\n+        try:\n+            if not _is_package_available(\"tiktoken\"):\n+                raise ModuleNotFoundError(\n+                    \"\"\"`tiktoken` is not installed, use `pip install tiktoken` to convert the tokenizer\"\"\"\n+                )\n+        except Exception as e:\n+            print(e)\n+        else:\n+            processor = DiaProcessor(\n+                DiaFeatureExtractor(sampling_rate=44100, hop_length=512),\n+                DiaTokenizer(),\n+                DacModel.from_pretrained(\"descript/dac_44khz\"),\n+            )\n+            processor.save_pretrained(args.pytorch_dump_folder_path)\n+\n+    model.save_pretrained(args.pytorch_dump_folder_path)\n+    print(f\"Saved converted checkpoint to {args.pytorch_dump_folder_path}\")"
        },
        {
            "sha": "0d03ceff37f96a2fb1d5963c237147fa6975e2b8",
            "filename": "src/transformers/models/dia/feature_extraction_dia.py",
            "status": "added",
            "additions": 183,
            "deletions": 0,
            "changes": 183,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fdia%2Ffeature_extraction_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fdia%2Ffeature_extraction_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Ffeature_extraction_dia.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -0,0 +1,183 @@\n+# coding=utf-8\n+# Copyright 2025 The Nari Labs and HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Feature extractor class for Dia\"\"\"\n+\n+from typing import Optional, Union\n+\n+import numpy as np\n+\n+from ...feature_extraction_sequence_utils import SequenceFeatureExtractor\n+from ...feature_extraction_utils import BatchFeature\n+from ...utils import PaddingStrategy, TensorType, logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class DiaFeatureExtractor(SequenceFeatureExtractor):\n+    r\"\"\"\n+    Constructs an Dia feature extractor.\n+\n+    This feature extractor inherits from [`~feature_extraction_sequence_utils.SequenceFeatureExtractor`] which contains\n+    most of the main methods. Users should refer to this superclass for more information regarding those methods.\n+\n+    Args:\n+        feature_size (`int`, *optional*, defaults to 1):\n+            The feature dimension of the extracted features. Use 1 for mono, 2 for stereo.\n+        sampling_rate (`int`, *optional*, defaults to 16000):\n+            The sampling rate at which the audio waveform should be digitalized, expressed in hertz (Hz).\n+        padding_value (`float`, *optional*, defaults to 0.0):\n+            The value that is used for padding.\n+        hop_length (`int`, *optional*, defaults to 512):\n+            Overlap length between successive windows.\n+    \"\"\"\n+\n+    model_input_names = [\"input_values\", \"n_quantizers\"]\n+\n+    def __init__(\n+        self,\n+        feature_size: int = 1,\n+        sampling_rate: int = 16000,\n+        padding_value: float = 0.0,\n+        hop_length: int = 512,\n+        **kwargs,\n+    ):\n+        super().__init__(feature_size=feature_size, sampling_rate=sampling_rate, padding_value=padding_value, **kwargs)\n+        self.hop_length = hop_length\n+\n+    def __call__(\n+        self,\n+        raw_audio: Union[np.ndarray, list[float], list[np.ndarray], list[list[float]]],\n+        padding: Optional[Union[bool, str, PaddingStrategy]] = None,\n+        truncation: Optional[bool] = False,\n+        max_length: Optional[int] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        sampling_rate: Optional[int] = None,\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to featurize and prepare for the model one or several sequence(s).\n+\n+        Args:\n+            raw_audio (`np.ndarray`, `list[float]`, `list[np.ndarray]`, `list[list[float]]`):\n+                The sequence or batch of sequences to be processed. Each sequence can be a numpy array, a list of float\n+                values, a list of numpy arrays or a list of list of float values. The numpy array must be of shape\n+                `(num_samples,)` for mono audio (`feature_size = 1`), or `(2, num_samples)` for stereo audio\n+                (`feature_size = 2`).\n+            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n+                Select a strategy to pad the returned sequences (according to the model's padding side and padding\n+                index) among:\n+\n+                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n+                  sequence if provided).\n+                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n+                  acceptable input length for the model if that argument is not provided.\n+                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n+                  lengths).\n+            truncation (`bool`, *optional*, defaults to `False`):\n+                Activates truncation to cut input sequences longer than `max_length` to `max_length`.\n+            max_length (`int`, *optional*):\n+                Maximum length of the returned list and optionally padding length (see above).\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*, default to 'pt'):\n+                If set, will return tensors instead of list of python integers. Acceptable values are:\n+\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return Numpy `np.ndarray` objects.\n+            sampling_rate (`int`, *optional*):\n+                The sampling rate at which the `audio` input was sampled. It is strongly recommended to pass\n+                `sampling_rate` at the forward call to prevent silent errors.\n+        \"\"\"\n+        if sampling_rate is not None:\n+            if sampling_rate != self.sampling_rate:\n+                raise ValueError(\n+                    f\"The model corresponding to this feature extractor: {self} was trained using a sampling rate of\"\n+                    f\" {self.sampling_rate}. Please make sure that the provided audio input was sampled with\"\n+                    f\" {self.sampling_rate} and not {sampling_rate}.\"\n+                )\n+        else:\n+            logger.warning(\n+                f\"It is strongly recommended to pass the `sampling_rate` argument to `{self.__class__.__name__}()`. \"\n+                \"Failing to do so can result in silent errors that might be hard to debug.\"\n+            )\n+\n+        if padding and truncation:\n+            raise ValueError(\"Both padding and truncation were set. Make sure you only set one.\")\n+        elif padding is None:\n+            # by default let's pad the inputs\n+            padding = True\n+\n+        is_batched = bool(\n+            isinstance(raw_audio, (list, tuple)) and (isinstance(raw_audio[0], (np.ndarray, tuple, list)))\n+        )\n+\n+        if is_batched:\n+            raw_audio = [np.asarray(audio, dtype=np.float32).T for audio in raw_audio]\n+        elif not is_batched and not isinstance(raw_audio, np.ndarray):\n+            raw_audio = np.asarray(raw_audio, dtype=np.float32)\n+        elif isinstance(raw_audio, np.ndarray) and raw_audio.dtype is np.dtype(np.float64):\n+            raw_audio = raw_audio.astype(np.float32)\n+\n+        # always return batch\n+        if not is_batched:\n+            raw_audio = [np.asarray(raw_audio).T]\n+\n+        # convert stereo to mono if necessary, unique to Dia\n+        for idx, example in enumerate(raw_audio):\n+            if self.feature_size == 2 and example.ndim == 2:\n+                raw_audio[idx] = np.mean(example, -1)\n+\n+        # verify inputs are valid\n+        for idx, example in enumerate(raw_audio):\n+            if example.ndim > 2:\n+                raise ValueError(f\"Expected input shape (channels, length) but got shape {example.shape}\")\n+            if self.feature_size == 1 and example.ndim != 1:\n+                raise ValueError(f\"Expected mono audio but example has {example.shape[-1]} channels\")\n+            if self.feature_size == 2 and example.ndim != 1:  # note the conversion before\n+                raise ValueError(f\"Expected stereo audio but example has {example.shape[-1]} channels\")\n+\n+        input_values = BatchFeature({\"input_values\": raw_audio})\n+\n+        # temporarily treat it as if we were mono as we also convert stereo to mono\n+        origingal_feature_size = self.feature_size\n+        self.feature_size = 1\n+\n+        # normal padding on batch\n+        padded_inputs = self.pad(\n+            input_values,\n+            max_length=max_length,\n+            truncation=truncation,\n+            padding=padding,\n+            return_attention_mask=True,\n+            pad_to_multiple_of=self.hop_length,\n+        )\n+        padded_inputs[\"padding_mask\"] = padded_inputs.pop(\"attention_mask\")\n+\n+        input_values = []\n+        for example in padded_inputs.pop(\"input_values\"):\n+            if self.feature_size == 1:\n+                example = example[..., None]\n+            input_values.append(example.T)\n+\n+        padded_inputs[\"input_values\"] = input_values\n+        if return_tensors is not None:\n+            padded_inputs = padded_inputs.convert_to_tensors(return_tensors)\n+\n+        # rewrite back to original feature size\n+        self.feature_size = origingal_feature_size\n+\n+        return padded_inputs\n+\n+\n+__all__ = [\"DiaFeatureExtractor\"]"
        },
        {
            "sha": "0ca5998bf2d64a6c792361ef3068ca07515f282f",
            "filename": "src/transformers/models/dia/generation_dia.py",
            "status": "added",
            "additions": 464,
            "deletions": 0,
            "changes": 464,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -0,0 +1,464 @@\n+# coding=utf-8\n+# Copyright 2025 The Nari Labs and HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+import torch.distributed as dist\n+\n+from ...generation.logits_process import (\n+    DiaClassifierFreeGuidanceLogitsProcessor,\n+    DiaEOSChannelFilterLogitsProcessor,\n+    DiaEOSDelayPatternLogitsProcessor,\n+    LogitsProcessorList,\n+    TemperatureLogitsWarper,\n+)\n+from ...generation.stopping_criteria import StoppingCriteriaList\n+from ...generation.streamers import BaseStreamer\n+from ...generation.utils import GenerateOutput, GenerationConfig, GenerationMixin, GenerationMode\n+from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ...integrations.fsdp import is_fsdp_managed_module\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class DiaGenerationMixin(GenerationMixin):\n+    # Indicates CFG which needs preparation to be properly handled by repeats\n+    _uses_cfg = None\n+\n+    def _get_logits_processor(\n+        self,\n+        generation_config: GenerationConfig,\n+        input_ids_seq_length: Optional[int] = None,\n+        encoder_input_ids: torch.LongTensor = None,\n+        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], list[int]]] = None,\n+        logits_processor: Optional[LogitsProcessorList] = None,\n+        device: Optional[str] = None,\n+        model_kwargs: Optional[dict[str, Any]] = None,\n+        negative_prompt_ids: Optional[torch.Tensor] = None,\n+        negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n+    ) -> LogitsProcessorList:\n+        # Need either custom order or custom processor instead\n+        # (Temporarily disabling those for the super function)\n+        original_guidance_scale = generation_config.guidance_scale\n+        original_temperature = generation_config.temperature\n+        generation_config.guidance_scale = None\n+        generation_config.temperature = None\n+\n+        # Get base processors and those we can integrate easily\n+        custom_processors = LogitsProcessorList()\n+\n+        if original_temperature is not None and original_temperature != 1.0:\n+            custom_processors.append(TemperatureLogitsWarper(original_temperature))\n+\n+        custom_processors.append(\n+            DiaEOSChannelFilterLogitsProcessor(\n+                num_channels=len(self.config.delay_pattern),\n+                eos_token_id=self.config.eos_token_id,\n+            )\n+        )\n+\n+        merged_processors = super()._get_logits_processor(\n+            generation_config=generation_config,\n+            input_ids_seq_length=input_ids_seq_length,\n+            encoder_input_ids=encoder_input_ids,\n+            prefix_allowed_tokens_fn=None,\n+            logits_processor=custom_processors,\n+            device=device,\n+            model_kwargs=model_kwargs,\n+            negative_prompt_ids=negative_prompt_ids,\n+            negative_prompt_attention_mask=negative_prompt_attention_mask,\n+        )\n+\n+        # Custom processors we need at specific positions\n+        if original_guidance_scale is not None and original_guidance_scale != 1:\n+            cfg_processor = DiaClassifierFreeGuidanceLogitsProcessor(\n+                guidance_scale=original_guidance_scale,\n+                guidance_top_k=generation_config.top_k,\n+            )\n+            merged_processors.insert(0, cfg_processor)\n+\n+        merged_processors.append(\n+            DiaEOSDelayPatternLogitsProcessor(\n+                delay_pattern=self.config.delay_pattern,\n+                eos_token_id=self.config.eos_token_id,\n+                max_generation_len=generation_config.max_length,\n+                device=device,\n+            )\n+        )\n+\n+        # Enable temporarily disabled values back\n+        generation_config.guidance_scale = original_guidance_scale\n+        generation_config.temperature = original_temperature\n+\n+        return merged_processors\n+\n+    def _prepare_generation_config(\n+        self, generation_config: Optional[GenerationConfig], use_model_defaults: Optional[bool] = None, **kwargs: dict\n+    ) -> tuple[GenerationConfig, dict]:\n+        generation_config, model_kwargs = super()._prepare_generation_config(\n+            generation_config, use_model_defaults, **kwargs\n+        )\n+\n+        # We allow generation up to max length + max delay pattern\n+        # (will revert back to max length after generation)\n+        generation_config.max_length += max(self.config.delay_pattern)\n+\n+        # Internal flag to indicate CFG that needs to prepare unconditioned input\n+        self._uses_cfg = generation_config.guidance_scale is not None and generation_config.guidance_scale != 1\n+\n+        return generation_config, model_kwargs\n+\n+    def _prepare_model_inputs(\n+        self,\n+        inputs: Optional[torch.Tensor] = None,\n+        bos_token_id: Optional[torch.Tensor] = None,\n+        model_kwargs: Optional[dict[str, torch.Tensor]] = None,\n+    ) -> tuple[torch.Tensor, Optional[str], dict[str, torch.Tensor]]:\n+        inputs, input_name, model_kwargs = super()._prepare_model_inputs(\n+            inputs=inputs,\n+            bos_token_id=bos_token_id,\n+            model_kwargs=model_kwargs,\n+        )\n+\n+        # If CFG is requested we fill in the unconditioned parts\n+        if self._uses_cfg:\n+            unconditioned_inputs = torch.zeros_like(inputs)\n+            inputs = torch.cat([inputs, unconditioned_inputs], dim=0)\n+\n+            if model_kwargs.get(\"attention_mask\", None) is not None:\n+                model_kwargs[\"attention_mask\"] = model_kwargs[\"attention_mask\"].repeat(2, 1)\n+\n+        return inputs, input_name, model_kwargs\n+\n+    def _prepare_decoder_input_ids_for_generation(\n+        self,\n+        batch_size: int,\n+        model_input_name: str,\n+        model_kwargs: dict[str, torch.Tensor],\n+        decoder_start_token_id: torch.Tensor,\n+        device: Optional[torch.device] = None,\n+    ) -> tuple[torch.LongTensor, dict[str, torch.Tensor]]:\n+        \"\"\"Prepares `decoder_input_ids` for generation with encoder-decoder models\"\"\"\n+        # 1. Check whether the user has defined `decoder_input_ids` and `decoder_attention_mask`; if not error out\n+        decoder_input_ids = decoder_attention_mask = None\n+        if model_kwargs is not None and \"decoder_input_ids\" in model_kwargs:\n+            decoder_input_ids = model_kwargs.pop(\"decoder_input_ids\")\n+        if model_kwargs is not None and \"decoder_attention_mask\" in model_kwargs:\n+            decoder_attention_mask = model_kwargs.pop(\"decoder_attention_mask\")\n+\n+        # We allow generating without preparation (no proper delay) but discourage it\n+        if decoder_input_ids is None or decoder_attention_mask is None:\n+            logger.warning_once(\n+                \"In order to generate with Dia, we need the processed audio input: Got `decoder_input_ids`:\"\n+                f\" {decoder_input_ids is not None} and got `decoder_attention_mask`={decoder_attention_mask is not None}.\"\n+                f\" This can be achieved via the [`DiaProcessor`] but now defaulting to non-delayed generation.\"\n+            )\n+\n+            num_channels = self.config.decoder_config.num_channels\n+            real_batch_size = batch_size // 2 if self._uses_cfg else batch_size\n+\n+            if decoder_input_ids is None:\n+                decoder_input_ids = torch.full(\n+                    (real_batch_size, 1, num_channels), decoder_start_token_id, dtype=torch.long, device=device\n+                )\n+\n+            decoder_attention_mask = torch.ones(\n+                size=(real_batch_size, decoder_input_ids.shape[1]), dtype=torch.long, device=device\n+            )\n+\n+        # 2. Determine the valid input and what works as mask within the input\n+        delay_mask = decoder_input_ids.long()\n+        valid_input_size = (\n+            decoder_input_ids.shape[1] - (decoder_input_ids[:, :, 0] == self.config.pad_token_id).sum(dim=-1).max()\n+        )\n+        decoder_input_ids = delay_mask[:, :valid_input_size].transpose(1, 2).long()\n+        decoder_attention_mask = decoder_attention_mask[:, :valid_input_size].long()\n+\n+        # 3. Overwrite into model kwargs\n+        model_kwargs[\"decoder_attention_mask\"] = decoder_attention_mask\n+        model_kwargs[\"decoder_delay_mask\"] = delay_mask\n+\n+        return decoder_input_ids, model_kwargs\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        encoder_outputs=None,  # Using this to easily get the batch size\n+        decoder_delay_mask=None,\n+        **kwargs,\n+    ):\n+        # Reshape decoder input_ids to 3D to be compile friendly and to fit the expected model input shape\n+        batch_size = encoder_outputs[0].shape[0] // 2 if self._uses_cfg else encoder_outputs[0].shape[0]\n+        input_ids = input_ids.reshape(batch_size, self.config.decoder_config.num_channels, -1).transpose(1, 2)\n+\n+        # Base method handles most things except CFG and the delay pattern mask\n+        model_inputs = super().prepare_inputs_for_generation(input_ids, encoder_outputs=encoder_outputs, **kwargs)\n+\n+        # Post processing for CFG and overwriting via delay pattern mask\n+        # 1. Delay pattern mask -- force tokens if not allowed to predict (!= pad_token in mask)\n+        model_inputs[\"decoder_input_ids\"] = self.apply_delay_mask(\n+            input_ids, self.config.pad_token_id, decoder_delay_mask\n+        )\n+\n+        # Depending on cache usage we need to pass all or just one\n+        if model_inputs.get(\"use_cache\", False) and model_inputs[\"cache_position\"][0] > 0:\n+            model_inputs[\"decoder_input_ids\"] = model_inputs[\"decoder_input_ids\"][:, -1, :][:, None, :]\n+\n+        # Be compile friendly\n+        model_inputs[\"decoder_input_ids\"] = model_inputs[\"decoder_input_ids\"].contiguous()\n+\n+        # 2. Apply CFG duplication if needed\n+        if self._uses_cfg:\n+            for key in [\"decoder_input_ids\", \"decoder_attention_mask\", \"decoder_position_ids\"]:\n+                if model_inputs.get(key, None) is not None:\n+                    # double first dimension and keep everything else the same\n+                    repeat_pattern = tuple([2] + [1] * (model_inputs[key].ndim - 1))\n+                    model_inputs[key] = model_inputs[key].repeat(*repeat_pattern)\n+\n+        return model_inputs\n+\n+    @staticmethod\n+    def apply_delay_mask(input_ids: torch.Tensor, pad_id: int, delay_mask: Optional[torch.Tensor]) -> torch.Tensor:\n+        if delay_mask is None:\n+            return input_ids\n+\n+        mask_len = min(input_ids.shape[1], delay_mask.shape[1])\n+        valid_mask = delay_mask[:, :mask_len, :]\n+        valid_input = input_ids[:, :mask_len, :]\n+\n+        # Overwrite the respective parts of the input\n+        input_ids[:, :mask_len, :] = torch.where(valid_mask == pad_id, valid_input, valid_mask)\n+\n+        return input_ids\n+\n+    def _main_generate_loop(\n+        self,\n+        inputs: Optional[torch.Tensor] = None,\n+        generation_config: Optional[GenerationConfig] = None,\n+        logits_processor: Optional[LogitsProcessorList] = None,\n+        stopping_criteria: Optional[StoppingCriteriaList] = None,\n+        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], list[int]]] = None,\n+        synced_gpus: Optional[bool] = None,\n+        assistant_model: Optional[\"PreTrainedModel\"] = None,\n+        streamer: Optional[\"BaseStreamer\"] = None,\n+        negative_prompt_ids: Optional[torch.Tensor] = None,\n+        negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n+        use_model_defaults: Optional[bool] = None,\n+        custom_generate: Optional[str] = None,\n+        **kwargs,\n+    ):\n+        # ********** mostly taken from main generate function up to calling the different methods (see NOTE) **********\n+        # 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\n+        tokenizer = kwargs.pop(\"tokenizer\", None)  # Pull this out first, we only use it for stopping criteria\n+        assistant_tokenizer = kwargs.pop(\"assistant_tokenizer\", None)  # only used for assisted generation\n+\n+        generation_config, model_kwargs = self._prepare_generation_config(\n+            generation_config, use_model_defaults, **kwargs\n+        )\n+        self._validate_model_kwargs(model_kwargs.copy())\n+        self._validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n+\n+        # 2. Set generation parameters if not already defined\n+        if synced_gpus is None:\n+            synced_gpus = (is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)) and dist.get_world_size() > 1\n+\n+        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n+        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n+\n+        # 3. Define model inputs\n+        kwargs_has_attention_mask = model_kwargs.get(\"attention_mask\", None) is not None\n+        inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(\n+            inputs, generation_config.bos_token_id, model_kwargs\n+        )\n+        batch_size = inputs_tensor.shape[0]\n+\n+        device = inputs_tensor.device\n+        self._prepare_special_tokens(generation_config, kwargs_has_attention_mask, device=device)\n+\n+        # 4. Define other model kwargs\n+        if \"encoder_outputs\" not in model_kwargs:\n+            # if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\n+            model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n+                inputs_tensor, model_kwargs, model_input_name, generation_config\n+            )\n+\n+        # 5. Prepare `input_ids` which will be used for auto-regressive generation\n+        input_ids, model_kwargs = self._prepare_decoder_input_ids_for_generation(\n+            batch_size=batch_size,\n+            model_input_name=model_input_name,\n+            model_kwargs=model_kwargs,\n+            decoder_start_token_id=generation_config._decoder_start_token_tensor,\n+            device=inputs_tensor.device,\n+        )\n+\n+        if generation_config.token_healing:\n+            input_ids = self.heal_tokens(input_ids, tokenizer)\n+\n+        if streamer is not None:\n+            streamer.put(input_ids.cpu())\n+\n+        # 6. Prepare `max_length` depending on other stopping criteria.\n+        # NOTE: incorrect `input_ids.shape[1]` previously\n+        input_ids_length = input_ids.shape[-1]\n+        has_default_max_length = kwargs.get(\"max_length\") is None and generation_config.max_length is not None\n+        has_default_min_length = kwargs.get(\"min_length\") is None and generation_config.min_length is not None\n+        generation_config = self._prepare_generated_length(\n+            generation_config=generation_config,\n+            has_default_max_length=has_default_max_length,\n+            has_default_min_length=has_default_min_length,\n+            model_input_name=model_input_name,\n+            inputs_tensor=inputs_tensor,\n+            input_ids_length=input_ids_length,\n+        )\n+\n+        # If the model supports `logits_to_keep` in forward(), set it to 1 to avoid computing the whole\n+        # logit matrix. This can save a lot of memory during the first forward pass. Note that assisted decoding\n+        # dynamically overrides this value as it can need more than the last token logits\n+        if self._supports_logits_to_keep() and \"logits_to_keep\" not in model_kwargs:\n+            model_kwargs[\"logits_to_keep\"] = 1\n+\n+        self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n+\n+        # 7. Prepare the cache.\n+        # - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.\n+        # - different models have a different cache name expected by the model (default = \"past_key_values\")\n+        # - `max_length`, prepared above, is used to determine the maximum cache length\n+        max_cache_length = generation_config.max_length - 1\n+        if (\n+            inputs_tensor.shape[1] != input_ids_length\n+            and model_input_name == \"inputs_embeds\"\n+            and not self.config.is_encoder_decoder\n+        ):\n+            max_cache_length += inputs_tensor.shape[1]\n+        self._prepare_cache_for_generation(\n+            generation_config, model_kwargs, assistant_model, batch_size, max_cache_length, device\n+        )\n+\n+        # 8. determine generation mode\n+        generation_mode = generation_config.get_generation_mode(assistant_model)\n+\n+        if streamer is not None and (generation_config.num_beams > 1):\n+            raise ValueError(\n+                \"`streamer` cannot be used with beam search (yet!). Make sure that `num_beams` is set to 1.\"\n+            )\n+\n+        # 9. prepare logits processors and stopping criteria\n+        prepared_logits_processor = self._get_logits_processor(\n+            generation_config=generation_config,\n+            input_ids_seq_length=input_ids_length,\n+            encoder_input_ids=inputs_tensor,\n+            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n+            logits_processor=logits_processor,\n+            device=inputs_tensor.device,\n+            model_kwargs=model_kwargs,\n+            negative_prompt_ids=negative_prompt_ids,\n+            negative_prompt_attention_mask=negative_prompt_attention_mask,\n+        )\n+        prepared_stopping_criteria = self._get_stopping_criteria(\n+            generation_config=generation_config, stopping_criteria=stopping_criteria, tokenizer=tokenizer, **kwargs\n+        )\n+\n+        # Set model_kwargs `use_cache` so we can use it later in forward runs\n+        model_kwargs[\"use_cache\"] = generation_config.use_cache\n+        # ******************* taken from main generate function up to calling the different methods *******************\n+\n+        # Prepare inner 2D logic in generation loop\n+        input_ids = input_ids.reshape(-1, input_ids.shape[-1])\n+\n+        # 10. go into different generation modes\n+        if generation_mode in (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n+            # 11. expand input_ids with `num_return_sequences` additional sequences per batch\n+            if generation_config.num_return_sequences > 1:\n+                raise ValueError(\"`num_return_sequences>1` is incompatible with Dia.\")\n+\n+            # 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\n+            return self._sample(\n+                input_ids,\n+                logits_processor=prepared_logits_processor,\n+                stopping_criteria=prepared_stopping_criteria,\n+                generation_config=generation_config,\n+                synced_gpus=synced_gpus,\n+                streamer=streamer,\n+                **model_kwargs,\n+            )\n+        else:\n+            raise ValueError(\n+                \"Got incompatible mode for generation, should be one of greedy or sampling. \"\n+                \"Ensure that beam search is de-activated by setting `num_beams=1` and `num_beam_groups=1`.\"\n+            )\n+\n+    @torch.no_grad()\n+    def generate(\n+        self,\n+        inputs: Optional[torch.Tensor] = None,\n+        generation_config: Optional[GenerationConfig] = None,\n+        logits_processor: Optional[LogitsProcessorList] = None,\n+        stopping_criteria: Optional[StoppingCriteriaList] = None,\n+        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], list[int]]] = None,\n+        synced_gpus: Optional[bool] = None,\n+        assistant_model: Optional[\"PreTrainedModel\"] = None,\n+        streamer: Optional[\"BaseStreamer\"] = None,\n+        negative_prompt_ids: Optional[torch.Tensor] = None,\n+        negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n+        use_model_defaults: Optional[bool] = None,\n+        custom_generate: Optional[str] = None,\n+        **kwargs,\n+    ) -> Union[GenerateOutput, torch.LongTensor]:\n+        # We expect the initial input ids to be the complete mask (delayed input)\n+        delay_mask = kwargs.get(\"decoder_input_ids\", None)\n+        if delay_mask is not None:\n+            delay_mask = delay_mask.clone()\n+\n+        output = self._main_generate_loop(\n+            inputs=inputs,\n+            generation_config=generation_config,\n+            logits_processor=logits_processor,\n+            stopping_criteria=stopping_criteria,\n+            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n+            synced_gpus=synced_gpus,\n+            assistant_model=assistant_model,\n+            streamer=streamer,\n+            negative_prompt_ids=negative_prompt_ids,\n+            negative_prompt_attention_mask=negative_prompt_attention_mask,\n+            use_model_defaults=use_model_defaults,\n+            custom_generate=custom_generate,\n+            **kwargs,\n+        )\n+\n+        return_dict_in_generate = not isinstance(output, torch.Tensor)\n+\n+        if return_dict_in_generate:\n+            output_sequences = output.sequences\n+        else:\n+            output_sequences = output\n+\n+        # Reshape from 2D (bsz * channels, seq_len) to 3D (bsz, seq_len, channels)\n+        num_channels = self.config.decoder_config.num_channels\n+        bsz = output_sequences.shape[0] // num_channels\n+        output_sequences = output_sequences.reshape(bsz, num_channels, -1).transpose(1, 2)\n+\n+        # Apply delay mask\n+        output_sequences = self.apply_delay_mask(output_sequences, self.config.pad_token_id, delay_mask)\n+\n+        if return_dict_in_generate:\n+            output.sequences = output_sequences\n+        else:\n+            output = output_sequences\n+\n+        return output"
        },
        {
            "sha": "19cac3e8c3acb68d89a85e06ecaeb3e3d4493604",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "added",
            "additions": 963,
            "deletions": 0,
            "changes": 963,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -0,0 +1,963 @@\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+#           This file was automatically generated from src/transformers/models/dia/modular_dia.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_dia.py file directly. One of our CI enforces this.\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+# coding=utf-8\n+# Copyright 2025 The Nari Labs and HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Callable, Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import (\n+    BaseModelOutput,\n+    BaseModelOutputWithPastAndCrossAttentions,\n+    Seq2SeqLMOutput,\n+    Seq2SeqModelOutput,\n+)\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, is_torchdynamo_compiling, logging\n+from .configuration_dia import DiaConfig, DiaDecoderConfig, DiaEncoderConfig\n+from .generation_dia import DiaGenerationMixin\n+\n+\n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+@auto_docstring\n+class DiaPreTrainedModel(PreTrainedModel):\n+    config_class = DiaConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n+    main_input_name = \"input_ids\"\n+    _no_split_modules = [\"DiaEncoderLayer\", \"DiaDecoderLayer\"]\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, DiaRMSNorm):\n+            module.weight.data.fill_(1.0)\n+\n+\n+class DiaMultiChannelEmbedding(nn.Module):\n+    \"\"\"In order to efficiently compute the audio embedding from the 9 different channels,\n+    we vectorize the embedding process by using a single embedding layer and an offset.\n+    Example:\n+    - num_embeds = 4\n+    - vocab_size = 8\n+    - num_channels = 3\n+    We would have offsets = [0, 8, 16]\n+    If audio_codes = [0, 1, 2, 3], [1, 3, 4, 7], [5, 6, 7, 8],\n+    then tokens = audio_codes + offsets\n+                = [0, 1, 2, 3, 9, 11, 12, 15, 21, 22, 23, 24]\n+    This allows us to use a single embedding layer for all channels.\n+    \"\"\"\n+\n+    def __init__(self, config: DiaDecoderConfig):\n+        super().__init__()\n+        self.embed = nn.Embedding(config.vocab_size * config.num_channels, config.hidden_size)\n+        self.hidden_size = config.hidden_size\n+        self.num_channels = config.num_channels\n+        offsets = torch.arange(config.num_channels, dtype=torch.long) * config.vocab_size  # (C,)\n+        self.register_buffer(\"offsets\", offsets, persistent=False)\n+\n+    def forward(self, audio_codes: torch.Tensor) -> torch.Tensor:\n+        tokens = (audio_codes + self.offsets.to(audio_codes.device)).squeeze(1)\n+        embeds = self.embed(tokens).view(tokens.shape[0], audio_codes.shape[1], -1, self.hidden_size)\n+        return embeds.sum(dim=2)\n+\n+\n+class DiaMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+\n+        self.config = config\n+        self.gate_up_proj = nn.Linear(config.hidden_size, 2 * config.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n+        self.activation_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, hidden_states: torch.FloatTensor) -> torch.FloatTensor:\n+        up_states = self.gate_up_proj(hidden_states)\n+\n+        gate, up_states = up_states.chunk(2, dim=-1)\n+        up_states = up_states * self.activation_fn(gate)\n+\n+        return self.down_proj(up_states)\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class DiaRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        DiaRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class DiaRotaryEmbedding(nn.Module):\n+    def __init__(self, config: DiaConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class DiaSelfAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: Union[DiaEncoderConfig, DiaDecoderConfig], layer_idx: int, is_causal: bool = False):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.hidden_size = config.hidden_size\n+        self.num_heads = self.config.num_attention_heads\n+        self.num_key_value_heads = self.config.num_key_value_heads or self.num_heads\n+        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // self.num_heads)\n+        self.scaling = 1\n+        self.attention_dropout = 0.0\n+        self.is_causal = is_causal\n+\n+        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n+        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n+        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n+        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class DiaCrossAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: DiaDecoderConfig, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.hidden_size = config.hidden_size\n+        self.cross_hidden_size = config.cross_hidden_size\n+        self.num_heads = self.config.cross_num_attention_heads\n+        self.num_key_value_heads = self.config.cross_num_key_value_heads\n+        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n+        self.head_dim = config.cross_head_dim\n+        self.scaling = 1\n+        self.attention_dropout = 0.0\n+        self.is_causal = False\n+\n+        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n+        self.k_proj = nn.Linear(self.cross_hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n+        self.v_proj = nn.Linear(self.cross_hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n+        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        cross_attention_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+        cross_shape = (*cross_attention_states.shape[:-1], -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        is_updated = past_key_values.is_updated.get(self.layer_idx) if past_key_values is not None else False\n+        if past_key_values is not None and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_states = past_key_values.cross_attention_cache.key_cache[self.layer_idx]\n+            value_states = past_key_values.cross_attention_cache.value_cache[self.layer_idx]\n+        else:\n+            key_states = self.k_proj(cross_attention_states).view(cross_shape).transpose(1, 2)\n+            value_states = self.v_proj(cross_attention_states).view(cross_shape).transpose(1, 2)\n+\n+            if past_key_values is not None:\n+                # save all states to the cache\n+                key_states, value_states = past_key_values.cross_attention_cache.update(\n+                    key_states,\n+                    value_states,\n+                    self.layer_idx,\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                past_key_values.is_updated[self.layer_idx] = True\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape((*input_shape, -1)).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class DiaEncoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: DiaEncoderConfig, layer_idx: int):\n+        super().__init__()\n+        self.pre_sa_norm = DiaRMSNorm(config.hidden_size, eps=config.norm_eps)\n+        self.self_attention = DiaSelfAttention(config, layer_idx, is_causal=False)\n+        self.post_sa_norm = DiaRMSNorm(config.hidden_size, eps=config.norm_eps)\n+        self.mlp = DiaMLP(config)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        residual = hidden_states\n+        normed_states = self.pre_sa_norm(hidden_states)\n+        self_attn_output, self_attn_weights = self.self_attention(\n+            normed_states,\n+            position_embeddings=position_embeddings,\n+            attention_mask=attention_mask,\n+            **kwargs,\n+        )\n+        hidden_states = residual + self_attn_output\n+\n+        residual = hidden_states\n+        normed_states = self.post_sa_norm(hidden_states)\n+        mlp_out = self.mlp(normed_states)\n+        hidden_states = residual + mlp_out\n+\n+        return hidden_states, self_attn_weights\n+\n+\n+class DiaEncoder(DiaPreTrainedModel):\n+    def __init__(self, config: DiaEncoderConfig):\n+        super().__init__(config)\n+        self.config = config\n+\n+        self.embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n+        self.layers = nn.ModuleList(\n+            [DiaEncoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = DiaRMSNorm(config.hidden_size, eps=config.norm_eps)\n+        self.rotary_embeddings = DiaRotaryEmbedding(config)\n+\n+    @auto_docstring\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_ids: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = False,\n+        output_hidden_states: Optional[bool] = False,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[BaseModelOutput, tuple]:\n+        hidden_states = self.embedding(input_ids)\n+\n+        # RoPE\n+        # Note: We expect right padding and hence always generate\n+        # the position ids on the fly to reduce preparation overhead\n+        position_ids = torch.arange(input_ids.shape[-1], device=input_ids.device)[None, :]\n+        position_embeddings = self.rotary_embeddings(hidden_states, position_ids)\n+\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            hidden_states,\n+        )\n+\n+        encoder_states = () if output_hidden_states else None\n+        all_attentions = () if output_attentions else None\n+\n+        for encoder_layer in self.layers:\n+            if output_hidden_states:\n+                encoder_states = encoder_states + (hidden_states,)\n+\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                position_embeddings=position_embeddings,\n+                attention_mask=attention_mask,\n+                **kwargs,\n+            )\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_attentions = all_attentions + (layer_outputs[1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        if output_hidden_states:\n+            encoder_states += (hidden_states,)\n+\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n+        )\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+\n+class DiaDecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: DiaDecoderConfig, layer_idx: int):\n+        super().__init__()\n+        self.embed_dim = config.hidden_size\n+        self.self_attention = DiaSelfAttention(config, layer_idx, is_causal=True)\n+        self.cross_attention = DiaCrossAttention(config, layer_idx)\n+        self.pre_sa_norm = DiaRMSNorm(config.hidden_size, eps=config.norm_eps)\n+        self.pre_ca_norm = DiaRMSNorm(config.hidden_size, eps=config.norm_eps)\n+        self.pre_mlp_norm = DiaRMSNorm(config.hidden_size, eps=config.norm_eps)\n+        self.mlp = DiaMLP(config)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        attention_mask: Optional[torch.Tensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:\n+        self_attn_cache = past_key_values\n+        if isinstance(self_attn_cache, EncoderDecoderCache):\n+            self_attn_cache = self_attn_cache.self_attention_cache\n+\n+        residual = hidden_states\n+        normed_states = self.pre_sa_norm(hidden_states)\n+        self_attn_output, self_attn_weights = self.self_attention(\n+            normed_states,\n+            position_embeddings,\n+            attention_mask,\n+            # Needs to be an arg in order to function properly\n+            # on inplace operations to be carried (e.g. compile)\n+            self_attn_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+        hidden_states = residual + self_attn_output\n+\n+        residual = hidden_states\n+        normed_states = self.pre_ca_norm(hidden_states)\n+        cross_states, cross_attn_weights = self.cross_attention(\n+            normed_states,\n+            encoder_hidden_states,\n+            attention_mask=encoder_attention_mask,\n+            past_key_values=past_key_values,\n+            **kwargs,\n+        )\n+        hidden_states = residual + cross_states\n+\n+        residual = hidden_states\n+        normed_states = self.pre_mlp_norm(hidden_states)\n+        mlp_out = self.mlp(normed_states)\n+        hidden_states = residual + mlp_out\n+\n+        return hidden_states, self_attn_weights, cross_attn_weights\n+\n+\n+class DiaDecoder(DiaPreTrainedModel):\n+    \"\"\"Transformer Decoder Stack using DenseGeneral.\"\"\"\n+\n+    def __init__(self, config: DiaDecoderConfig):\n+        super().__init__(config)\n+        self.num_channels = config.num_channels\n+        self.vocab_size = config.vocab_size\n+        self.embeddings = DiaMultiChannelEmbedding(config)\n+        self.rotary_embeddings = DiaRotaryEmbedding(config)\n+        self.layers = nn.ModuleList(\n+            [DiaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = DiaRMSNorm(config.hidden_size, eps=config.norm_eps)\n+\n+    @auto_docstring\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_ids: torch.Tensor,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        output_attentions: Optional[bool] = False,\n+        output_hidden_states: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> Union[BaseModelOutputWithPastAndCrossAttentions, tuple]:\n+        r\"\"\"\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length, num_codebooks)`):\n+            The original `decoder_input_ids` in 3D shape to facilitate more efficient computations.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        \"\"\"\n+\n+        batch_size, seq_length = input_ids.size()[:-1]\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_length, device=input_ids.device\n+            )\n+        if position_ids is None:\n+            position_ids = cache_position[None, :]\n+\n+        # RoPE\n+        hidden_states = self.embeddings(input_ids)\n+        position_embeddings = self.rotary_embeddings(hidden_states, position_ids)\n+\n+        if attention_mask is None and not is_torchdynamo_compiling():\n+            # required mask seq length can be calculated via length of past cache\n+            mask_seq_length = past_key_values_length + seq_length\n+            attention_mask = torch.ones(batch_size, mask_seq_length, device=input_ids.device)\n+\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+        )\n+        encoder_attention_mask = self._update_cross_attn_mask(\n+            encoder_hidden_states,\n+            encoder_attention_mask,\n+            hidden_states.shape[:2],\n+            hidden_states,\n+        )\n+\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n+\n+        for layer in self.layers:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            layer_outputs = layer(\n+                hidden_states,\n+                position_embeddings,\n+                attention_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns = all_self_attns + (layer_outputs[1],)\n+\n+                if encoder_hidden_states is not None:\n+                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        return BaseModelOutputWithPastAndCrossAttentions(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+            cross_attentions=all_cross_attentions,\n+        )\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The bare Dia model outputting raw hidden-states without any specific head on top.\n+    \"\"\"\n+)\n+class DiaModel(DiaPreTrainedModel):\n+    def __init__(self, config: DiaConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.encoder = DiaEncoder(config.encoder_config)\n+        self.decoder = DiaDecoder(config.decoder_config)\n+        self.post_init()\n+\n+    def get_encoder(self):\n+        return self.encoder\n+\n+    def get_decoder(self):\n+        return self.decoder\n+\n+    @auto_docstring\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_position_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.LongTensor] = None,\n+        encoder_outputs: Optional[Union[BaseModelOutput, tuple]] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> Union[tuple, Seq2SeqModelOutput]:\n+        r\"\"\"\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size * num_codebooks, target_sequence_length)\n+        or (batch_size, target_sequence_length, num_codebooks)`, *optional*):\n+            1. (batch_size * num_codebooks, target_sequence_length): corresponds to the general use case where\n+            the audio input codebooks are flattened into the batch dimension. This also aligns with the flat-\n+            tened audio logits which are used to calculate the loss.\n+\n+            2. (batch_size, sequence_length, num_codebooks): corresponds to the internally used shape of\n+            Dia to calculate embeddings and subsequent steps more efficiently.\n+\n+            If no `decoder_input_ids` are provided, it will create a tensor of `bos_token_id` with shape\n+            `(batch_size, 1, num_codebooks)`. Indices can be obtained using the [`DiaProcessor`]. See\n+            [`DiaProcessor.__call__`] for more details.\n+\n+            [What are decoder input IDs?](../glossary#decoder-input-ids)\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`):\n+            Indices of positions of each input sequence tokens in the position embeddings.\n+            Used to calculate the position embeddings up to `config.decoder_config.max_position_embeddings`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        \"\"\"\n+\n+        if input_ids is None and encoder_outputs is None:\n+            raise ValueError(\n+                \"You should either provide text ids or the cached text encodings. Neither has been found.\"\n+            )\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+\n+        if self.is_gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+\n+        if encoder_outputs is None:\n+            encoder_outputs = self.encoder(\n+                input_ids=input_ids,\n+                attention_mask=attention_mask,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+                **kwargs,\n+            )\n+        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput\n+        elif not isinstance(encoder_outputs, BaseModelOutput):\n+            encoder_outputs = BaseModelOutput(\n+                last_hidden_state=encoder_outputs[0],\n+                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n+                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n+            )\n+\n+        # On default we initialize the decoder with bos tokens if nothing has been provided\n+        bsz, seq_len, channels = (encoder_outputs[0].shape[0], -1, self.config.decoder_config.num_channels)\n+        if decoder_input_ids is None:\n+            decoder_input_ids = torch.full(\n+                size=(bsz, 1, channels), fill_value=self.config.bos_token_id, device=self.device\n+            )\n+        # Ensure 3D\n+        if decoder_input_ids.ndim == 2:\n+            decoder_input_ids = decoder_input_ids.reshape(bsz, channels, seq_len).transpose(1, 2)\n+\n+        decoder_outputs = self.decoder(\n+            input_ids=decoder_input_ids,\n+            position_ids=decoder_position_ids,\n+            attention_mask=decoder_attention_mask,\n+            encoder_hidden_states=encoder_outputs[0],\n+            encoder_attention_mask=attention_mask,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        return Seq2SeqModelOutput(\n+            last_hidden_state=decoder_outputs.last_hidden_state,\n+            past_key_values=decoder_outputs.past_key_values,\n+            decoder_hidden_states=decoder_outputs.hidden_states,\n+            decoder_attentions=decoder_outputs.attentions,\n+            cross_attentions=decoder_outputs.cross_attentions,\n+            encoder_last_hidden_state=encoder_outputs[0],\n+            encoder_hidden_states=encoder_outputs.hidden_states,\n+            encoder_attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The Dia model consisting of a (byte) text encoder and audio decoder with a prediction head on top.\n+    \"\"\"\n+)\n+class DiaForConditionalGeneration(DiaPreTrainedModel, DiaGenerationMixin):\n+    base_model_prefix = \"model\"\n+\n+    def __init__(self, config: DiaConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.model = DiaModel(config)\n+\n+        self.num_channels = config.decoder_config.num_channels\n+        self.vocab_size = config.decoder_config.vocab_size\n+        self.logits_dense = nn.Linear(\n+            config.decoder_config.hidden_size, (self.num_channels * self.vocab_size), bias=False\n+        )\n+        self.loss_type = \"ForMaskedLM\"\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_encoder(self):\n+        return self.model.get_encoder()\n+\n+    def get_decoder(self):\n+        return self.model.get_decoder()\n+\n+    @auto_docstring\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_position_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.LongTensor] = None,\n+        encoder_outputs: Optional[Union[BaseModelOutput, tuple]] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> Union[tuple, Seq2SeqLMOutput]:\n+        r\"\"\"\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size * num_codebooks, target_sequence_length)\n+        or (batch_size, target_sequence_length, num_codebooks)`, *optional*):\n+            1. (batch_size * num_codebooks, target_sequence_length): corresponds to the general use case where\n+            the audio input codebooks are flattened into the batch dimension. This also aligns with the flat-\n+            tened audio logits which are used to calculate the loss.\n+\n+            2. (batch_size, sequence_length, num_codebooks): corresponds to the internally used shape of\n+            Dia to calculate embeddings and subsequent steps more efficiently.\n+\n+            If no `decoder_input_ids` are provided, it will create a tensor of `bos_token_id` with shape\n+            `(batch_size, 1, num_codebooks)`. Indices can be obtained using the [`DiaProcessor`]. See\n+            [`DiaProcessor.__call__`] for more details.\n+\n+            [What are decoder input IDs?](../glossary#decoder-input-ids)\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`):\n+            Indices of positions of each input sequence tokens in the position embeddings.\n+            Used to calculate the position embeddings up to `config.decoder_config.max_position_embeddings`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        labels (`torch.LongTensor` of shape `(batch_size * num_codebooks,)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in\n+            `[0, ..., config.decoder_config.vocab_size - 1]` or -100. Tokens with indices set to `-100`\n+            are ignored (masked).\n+        \"\"\"\n+\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            decoder_input_ids=decoder_input_ids,\n+            decoder_position_ids=decoder_position_ids,\n+            decoder_attention_mask=decoder_attention_mask,\n+            encoder_outputs=encoder_outputs,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        last_hidden_state = outputs[0]\n+        batch_size = last_hidden_state.shape[0]\n+        # 3D <-> 2D makes it necessary to prioritize channel dim\n+        audio_logits = (\n+            self.logits_dense(last_hidden_state)\n+            .view((batch_size, -1, self.num_channels, self.vocab_size))\n+            .transpose(1, 2)\n+            .contiguous()\n+            .view(batch_size * self.num_channels, -1, self.vocab_size)\n+        )\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=audio_logits, labels=labels, vocab_size=self.vocab_size, **kwargs)\n+\n+        return Seq2SeqLMOutput(\n+            loss=loss,\n+            logits=audio_logits,\n+            past_key_values=outputs.past_key_values,\n+            decoder_hidden_states=outputs.decoder_hidden_states,\n+            decoder_attentions=outputs.decoder_attentions,\n+            cross_attentions=outputs.cross_attentions,\n+            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n+            encoder_hidden_states=outputs.encoder_hidden_states,\n+            encoder_attentions=outputs.encoder_attentions,\n+        )\n+\n+\n+__all__ = [\"DiaModel\", \"DiaPreTrainedModel\", \"DiaForConditionalGeneration\"]"
        },
        {
            "sha": "fe437fde84eda183298dc38516c231b1abc6ba72",
            "filename": "src/transformers/models/dia/modular_dia.py",
            "status": "added",
            "additions": 789,
            "deletions": 0,
            "changes": 789,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -0,0 +1,789 @@\n+# coding=utf-8\n+# Copyright 2025 The Nari Labs and HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch Dia model.\"\"\"\n+\n+from typing import Callable, Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...cache_utils import DynamicCache, EncoderDecoderCache\n+from ...masking_utils import create_causal_mask\n+from ...modeling_attn_mask_utils import (\n+    _prepare_4d_attention_mask,\n+    _prepare_4d_attention_mask_for_sdpa,\n+)\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import (\n+    BaseModelOutput,\n+    BaseModelOutputWithPastAndCrossAttentions,\n+    Seq2SeqLMOutput,\n+    Seq2SeqModelOutput,\n+)\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, is_torchdynamo_compiling, logging\n+from ..llama.modeling_llama import (\n+    LlamaAttention,\n+    LlamaRMSNorm,\n+    LlamaRotaryEmbedding,\n+    eager_attention_forward,\n+)\n+from ..phi3.modeling_phi3 import Phi3MLP\n+from .configuration_dia import DiaConfig, DiaDecoderConfig, DiaEncoderConfig\n+from .generation_dia import DiaGenerationMixin\n+\n+\n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+@auto_docstring\n+class DiaPreTrainedModel(PreTrainedModel):\n+    config_class = DiaConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n+    main_input_name = \"input_ids\"\n+    _no_split_modules = [\"DiaEncoderLayer\", \"DiaDecoderLayer\"]\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, DiaRMSNorm):\n+            module.weight.data.fill_(1.0)\n+\n+\n+class DiaMultiChannelEmbedding(nn.Module):\n+    \"\"\"In order to efficiently compute the audio embedding from the 9 different channels,\n+    we vectorize the embedding process by using a single embedding layer and an offset.\n+    Example:\n+    - num_embeds = 4\n+    - vocab_size = 8\n+    - num_channels = 3\n+    We would have offsets = [0, 8, 16]\n+    If audio_codes = [0, 1, 2, 3], [1, 3, 4, 7], [5, 6, 7, 8],\n+    then tokens = audio_codes + offsets\n+                = [0, 1, 2, 3, 9, 11, 12, 15, 21, 22, 23, 24]\n+    This allows us to use a single embedding layer for all channels.\n+    \"\"\"\n+\n+    def __init__(self, config: DiaDecoderConfig):\n+        super().__init__()\n+        self.embed = nn.Embedding(config.vocab_size * config.num_channels, config.hidden_size)\n+        self.hidden_size = config.hidden_size\n+        self.num_channels = config.num_channels\n+        offsets = torch.arange(config.num_channels, dtype=torch.long) * config.vocab_size  # (C,)\n+        self.register_buffer(\"offsets\", offsets, persistent=False)\n+\n+    def forward(self, audio_codes: torch.Tensor) -> torch.Tensor:\n+        tokens = (audio_codes + self.offsets.to(audio_codes.device)).squeeze(1)\n+        embeds = self.embed(tokens).view(tokens.shape[0], audio_codes.shape[1], -1, self.hidden_size)\n+        return embeds.sum(dim=2)\n+\n+\n+class DiaMLP(Phi3MLP):\n+    pass\n+\n+\n+class DiaRMSNorm(LlamaRMSNorm):\n+    pass\n+\n+\n+class DiaRotaryEmbedding(LlamaRotaryEmbedding):\n+    pass\n+\n+\n+class DiaSelfAttention(LlamaAttention, nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: Union[DiaEncoderConfig, DiaDecoderConfig], layer_idx: int, is_causal: bool = False):\n+        nn.Module.__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.hidden_size = config.hidden_size\n+        self.num_heads = self.config.num_attention_heads\n+        self.num_key_value_heads = self.config.num_key_value_heads or self.num_heads\n+        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // self.num_heads)\n+        self.scaling = 1\n+        self.attention_dropout = 0.0\n+        self.is_causal = is_causal\n+\n+        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n+        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n+        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n+        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n+\n+\n+class DiaCrossAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: DiaDecoderConfig, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.hidden_size = config.hidden_size\n+        self.cross_hidden_size = config.cross_hidden_size\n+        self.num_heads = self.config.cross_num_attention_heads\n+        self.num_key_value_heads = self.config.cross_num_key_value_heads\n+        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n+        self.head_dim = config.cross_head_dim\n+        self.scaling = 1\n+        self.attention_dropout = 0.0\n+        self.is_causal = False\n+\n+        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n+        self.k_proj = nn.Linear(self.cross_hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n+        self.v_proj = nn.Linear(self.cross_hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n+        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        cross_attention_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+        cross_shape = (*cross_attention_states.shape[:-1], -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        is_updated = past_key_values.is_updated.get(self.layer_idx) if past_key_values is not None else False\n+        if past_key_values is not None and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_states = past_key_values.cross_attention_cache.key_cache[self.layer_idx]\n+            value_states = past_key_values.cross_attention_cache.value_cache[self.layer_idx]\n+        else:\n+            key_states = self.k_proj(cross_attention_states).view(cross_shape).transpose(1, 2)\n+            value_states = self.v_proj(cross_attention_states).view(cross_shape).transpose(1, 2)\n+\n+            if past_key_values is not None:\n+                # save all states to the cache\n+                key_states, value_states = past_key_values.cross_attention_cache.update(\n+                    key_states,\n+                    value_states,\n+                    self.layer_idx,\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                past_key_values.is_updated[self.layer_idx] = True\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape((*input_shape, -1)).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class DiaEncoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: DiaEncoderConfig, layer_idx: int):\n+        super().__init__()\n+        self.pre_sa_norm = DiaRMSNorm(config.hidden_size, eps=config.norm_eps)\n+        self.self_attention = DiaSelfAttention(config, layer_idx, is_causal=False)\n+        self.post_sa_norm = DiaRMSNorm(config.hidden_size, eps=config.norm_eps)\n+        self.mlp = DiaMLP(config)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        residual = hidden_states\n+        normed_states = self.pre_sa_norm(hidden_states)\n+        self_attn_output, self_attn_weights = self.self_attention(\n+            normed_states,\n+            position_embeddings=position_embeddings,\n+            attention_mask=attention_mask,\n+            **kwargs,\n+        )\n+        hidden_states = residual + self_attn_output\n+\n+        residual = hidden_states\n+        normed_states = self.post_sa_norm(hidden_states)\n+        mlp_out = self.mlp(normed_states)\n+        hidden_states = residual + mlp_out\n+\n+        return hidden_states, self_attn_weights\n+\n+\n+class DiaEncoder(DiaPreTrainedModel):\n+    def __init__(self, config: DiaEncoderConfig):\n+        super().__init__(config)\n+        self.config = config\n+\n+        self.embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n+        self.layers = nn.ModuleList(\n+            [DiaEncoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = DiaRMSNorm(config.hidden_size, eps=config.norm_eps)\n+        self.rotary_embeddings = DiaRotaryEmbedding(config)\n+\n+    @auto_docstring\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_ids: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = False,\n+        output_hidden_states: Optional[bool] = False,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[BaseModelOutput, tuple]:\n+        hidden_states = self.embedding(input_ids)\n+\n+        # RoPE\n+        # Note: We expect right padding and hence always generate\n+        # the position ids on the fly to reduce preparation overhead\n+        position_ids = torch.arange(input_ids.shape[-1], device=input_ids.device)[None, :]\n+        position_embeddings = self.rotary_embeddings(hidden_states, position_ids)\n+\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            hidden_states,\n+        )\n+\n+        encoder_states = () if output_hidden_states else None\n+        all_attentions = () if output_attentions else None\n+\n+        for encoder_layer in self.layers:\n+            if output_hidden_states:\n+                encoder_states = encoder_states + (hidden_states,)\n+\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                position_embeddings=position_embeddings,\n+                attention_mask=attention_mask,\n+                **kwargs,\n+            )\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_attentions = all_attentions + (layer_outputs[1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        if output_hidden_states:\n+            encoder_states += (hidden_states,)\n+\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n+        )\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+\n+class DiaDecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: DiaDecoderConfig, layer_idx: int):\n+        super().__init__()\n+        self.embed_dim = config.hidden_size\n+        self.self_attention = DiaSelfAttention(config, layer_idx, is_causal=True)\n+        self.cross_attention = DiaCrossAttention(config, layer_idx)\n+        self.pre_sa_norm = DiaRMSNorm(config.hidden_size, eps=config.norm_eps)\n+        self.pre_ca_norm = DiaRMSNorm(config.hidden_size, eps=config.norm_eps)\n+        self.pre_mlp_norm = DiaRMSNorm(config.hidden_size, eps=config.norm_eps)\n+        self.mlp = DiaMLP(config)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        attention_mask: Optional[torch.Tensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:\n+        self_attn_cache = past_key_values\n+        if isinstance(self_attn_cache, EncoderDecoderCache):\n+            self_attn_cache = self_attn_cache.self_attention_cache\n+\n+        residual = hidden_states\n+        normed_states = self.pre_sa_norm(hidden_states)\n+        self_attn_output, self_attn_weights = self.self_attention(\n+            normed_states,\n+            position_embeddings,\n+            attention_mask,\n+            # Needs to be an arg in order to function properly\n+            # on inplace operations to be carried (e.g. compile)\n+            self_attn_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+        hidden_states = residual + self_attn_output\n+\n+        residual = hidden_states\n+        normed_states = self.pre_ca_norm(hidden_states)\n+        cross_states, cross_attn_weights = self.cross_attention(\n+            normed_states,\n+            encoder_hidden_states,\n+            attention_mask=encoder_attention_mask,\n+            past_key_values=past_key_values,\n+            **kwargs,\n+        )\n+        hidden_states = residual + cross_states\n+\n+        residual = hidden_states\n+        normed_states = self.pre_mlp_norm(hidden_states)\n+        mlp_out = self.mlp(normed_states)\n+        hidden_states = residual + mlp_out\n+\n+        return hidden_states, self_attn_weights, cross_attn_weights\n+\n+\n+class DiaDecoder(DiaPreTrainedModel):\n+    \"\"\"Transformer Decoder Stack using DenseGeneral.\"\"\"\n+\n+    def __init__(self, config: DiaDecoderConfig):\n+        super().__init__(config)\n+        self.num_channels = config.num_channels\n+        self.vocab_size = config.vocab_size\n+        self.embeddings = DiaMultiChannelEmbedding(config)\n+        self.rotary_embeddings = DiaRotaryEmbedding(config)\n+        self.layers = nn.ModuleList(\n+            [DiaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = DiaRMSNorm(config.hidden_size, eps=config.norm_eps)\n+\n+    @auto_docstring\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_ids: torch.Tensor,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        output_attentions: Optional[bool] = False,\n+        output_hidden_states: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> Union[BaseModelOutputWithPastAndCrossAttentions, tuple]:\n+        r\"\"\"\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length, num_codebooks)`):\n+            The original `decoder_input_ids` in 3D shape to facilitate more efficient computations.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        \"\"\"\n+\n+        batch_size, seq_length = input_ids.size()[:-1]\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_length, device=input_ids.device\n+            )\n+        if position_ids is None:\n+            position_ids = cache_position[None, :]\n+\n+        # RoPE\n+        hidden_states = self.embeddings(input_ids)\n+        position_embeddings = self.rotary_embeddings(hidden_states, position_ids)\n+\n+        if attention_mask is None and not is_torchdynamo_compiling():\n+            # required mask seq length can be calculated via length of past cache\n+            mask_seq_length = past_key_values_length + seq_length\n+            attention_mask = torch.ones(batch_size, mask_seq_length, device=input_ids.device)\n+\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+        )\n+        encoder_attention_mask = self._update_cross_attn_mask(\n+            encoder_hidden_states,\n+            encoder_attention_mask,\n+            hidden_states.shape[:2],\n+            hidden_states,\n+        )\n+\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n+\n+        for layer in self.layers:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            layer_outputs = layer(\n+                hidden_states,\n+                position_embeddings,\n+                attention_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns = all_self_attns + (layer_outputs[1],)\n+\n+                if encoder_hidden_states is not None:\n+                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        return BaseModelOutputWithPastAndCrossAttentions(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+            cross_attentions=all_cross_attentions,\n+        )\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The bare Dia model outputting raw hidden-states without any specific head on top.\n+    \"\"\"\n+)\n+class DiaModel(DiaPreTrainedModel):\n+    def __init__(self, config: DiaConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.encoder = DiaEncoder(config.encoder_config)\n+        self.decoder = DiaDecoder(config.decoder_config)\n+        self.post_init()\n+\n+    def get_encoder(self):\n+        return self.encoder\n+\n+    def get_decoder(self):\n+        return self.decoder\n+\n+    @auto_docstring\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_position_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.LongTensor] = None,\n+        encoder_outputs: Optional[Union[BaseModelOutput, tuple]] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> Union[tuple, Seq2SeqModelOutput]:\n+        r\"\"\"\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size * num_codebooks, target_sequence_length)\n+        or (batch_size, target_sequence_length, num_codebooks)`, *optional*):\n+            1. (batch_size * num_codebooks, target_sequence_length): corresponds to the general use case where\n+            the audio input codebooks are flattened into the batch dimension. This also aligns with the flat-\n+            tened audio logits which are used to calculate the loss.\n+\n+            2. (batch_size, sequence_length, num_codebooks): corresponds to the internally used shape of\n+            Dia to calculate embeddings and subsequent steps more efficiently.\n+\n+            If no `decoder_input_ids` are provided, it will create a tensor of `bos_token_id` with shape\n+            `(batch_size, 1, num_codebooks)`. Indices can be obtained using the [`DiaProcessor`]. See\n+            [`DiaProcessor.__call__`] for more details.\n+\n+            [What are decoder input IDs?](../glossary#decoder-input-ids)\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`):\n+            Indices of positions of each input sequence tokens in the position embeddings.\n+            Used to calculate the position embeddings up to `config.decoder_config.max_position_embeddings`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        \"\"\"\n+\n+        if input_ids is None and encoder_outputs is None:\n+            raise ValueError(\n+                \"You should either provide text ids or the cached text encodings. Neither has been found.\"\n+            )\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+\n+        if self.is_gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+\n+        if encoder_outputs is None:\n+            encoder_outputs = self.encoder(\n+                input_ids=input_ids,\n+                attention_mask=attention_mask,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+                **kwargs,\n+            )\n+        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput\n+        elif not isinstance(encoder_outputs, BaseModelOutput):\n+            encoder_outputs = BaseModelOutput(\n+                last_hidden_state=encoder_outputs[0],\n+                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n+                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n+            )\n+\n+        # On default we initialize the decoder with bos tokens if nothing has been provided\n+        bsz, seq_len, channels = (encoder_outputs[0].shape[0], -1, self.config.decoder_config.num_channels)\n+        if decoder_input_ids is None:\n+            decoder_input_ids = torch.full(\n+                size=(bsz, 1, channels), fill_value=self.config.bos_token_id, device=self.device\n+            )\n+        # Ensure 3D\n+        if decoder_input_ids.ndim == 2:\n+            decoder_input_ids = decoder_input_ids.reshape(bsz, channels, seq_len).transpose(1, 2)\n+\n+        decoder_outputs = self.decoder(\n+            input_ids=decoder_input_ids,\n+            position_ids=decoder_position_ids,\n+            attention_mask=decoder_attention_mask,\n+            encoder_hidden_states=encoder_outputs[0],\n+            encoder_attention_mask=attention_mask,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        return Seq2SeqModelOutput(\n+            last_hidden_state=decoder_outputs.last_hidden_state,\n+            past_key_values=decoder_outputs.past_key_values,\n+            decoder_hidden_states=decoder_outputs.hidden_states,\n+            decoder_attentions=decoder_outputs.attentions,\n+            cross_attentions=decoder_outputs.cross_attentions,\n+            encoder_last_hidden_state=encoder_outputs[0],\n+            encoder_hidden_states=encoder_outputs.hidden_states,\n+            encoder_attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The Dia model consisting of a (byte) text encoder and audio decoder with a prediction head on top.\n+    \"\"\"\n+)\n+class DiaForConditionalGeneration(DiaPreTrainedModel, DiaGenerationMixin):\n+    base_model_prefix = \"model\"\n+\n+    def __init__(self, config: DiaConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.model = DiaModel(config)\n+\n+        self.num_channels = config.decoder_config.num_channels\n+        self.vocab_size = config.decoder_config.vocab_size\n+        self.logits_dense = nn.Linear(\n+            config.decoder_config.hidden_size, (self.num_channels * self.vocab_size), bias=False\n+        )\n+        self.loss_type = \"ForMaskedLM\"\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_encoder(self):\n+        return self.model.get_encoder()\n+\n+    def get_decoder(self):\n+        return self.model.get_decoder()\n+\n+    @auto_docstring\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_position_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.LongTensor] = None,\n+        encoder_outputs: Optional[Union[BaseModelOutput, tuple]] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> Union[tuple, Seq2SeqLMOutput]:\n+        r\"\"\"\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size * num_codebooks, target_sequence_length)\n+        or (batch_size, target_sequence_length, num_codebooks)`, *optional*):\n+            1. (batch_size * num_codebooks, target_sequence_length): corresponds to the general use case where\n+            the audio input codebooks are flattened into the batch dimension. This also aligns with the flat-\n+            tened audio logits which are used to calculate the loss.\n+\n+            2. (batch_size, sequence_length, num_codebooks): corresponds to the internally used shape of\n+            Dia to calculate embeddings and subsequent steps more efficiently.\n+\n+            If no `decoder_input_ids` are provided, it will create a tensor of `bos_token_id` with shape\n+            `(batch_size, 1, num_codebooks)`. Indices can be obtained using the [`DiaProcessor`]. See\n+            [`DiaProcessor.__call__`] for more details.\n+\n+            [What are decoder input IDs?](../glossary#decoder-input-ids)\n+        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`):\n+            Indices of positions of each input sequence tokens in the position embeddings.\n+            Used to calculate the position embeddings up to `config.decoder_config.max_position_embeddings`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        labels (`torch.LongTensor` of shape `(batch_size * num_codebooks,)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in\n+            `[0, ..., config.decoder_config.vocab_size - 1]` or -100. Tokens with indices set to `-100`\n+            are ignored (masked).\n+        \"\"\"\n+\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            decoder_input_ids=decoder_input_ids,\n+            decoder_position_ids=decoder_position_ids,\n+            decoder_attention_mask=decoder_attention_mask,\n+            encoder_outputs=encoder_outputs,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        last_hidden_state = outputs[0]\n+        batch_size = last_hidden_state.shape[0]\n+        # 3D <-> 2D makes it necessary to prioritize channel dim\n+        audio_logits = (\n+            self.logits_dense(last_hidden_state)\n+            .view((batch_size, -1, self.num_channels, self.vocab_size))\n+            .transpose(1, 2)\n+            .contiguous()\n+            .view(batch_size * self.num_channels, -1, self.vocab_size)\n+        )\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=audio_logits, labels=labels, vocab_size=self.vocab_size, **kwargs)\n+\n+        return Seq2SeqLMOutput(\n+            loss=loss,\n+            logits=audio_logits,\n+            past_key_values=outputs.past_key_values,\n+            decoder_hidden_states=outputs.decoder_hidden_states,\n+            decoder_attentions=outputs.decoder_attentions,\n+            cross_attentions=outputs.cross_attentions,\n+            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n+            encoder_hidden_states=outputs.encoder_hidden_states,\n+            encoder_attentions=outputs.encoder_attentions,\n+        )\n+\n+\n+__all__ = [\"DiaModel\", \"DiaPreTrainedModel\", \"DiaForConditionalGeneration\"]"
        },
        {
            "sha": "e50ef5de67f8735de6095915a2f2a52ae42c6211",
            "filename": "src/transformers/models/dia/processing_dia.py",
            "status": "added",
            "additions": 484,
            "deletions": 0,
            "changes": 484,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fdia%2Fprocessing_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fdia%2Fprocessing_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fprocessing_dia.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -0,0 +1,484 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Processor class for Dia\"\"\"\n+\n+import math\n+from pathlib import Path\n+from typing import Optional, Union\n+\n+from ...audio_utils import AudioInput, make_list_of_audio\n+from ...feature_extraction_utils import BatchFeature\n+from ...processing_utils import AudioKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...utils import is_soundfile_available, is_torch_available\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_soundfile_available():\n+    import soundfile as sf\n+\n+\n+class DiaAudioKwargs(AudioKwargs, total=False):\n+    bos_token_id: int\n+    eos_token_id: int\n+    pad_token_id: int\n+    delay_pattern: list[int]\n+    generation: bool\n+\n+\n+class DiaProcessorKwargs(ProcessingKwargs, total=False):\n+    audio_kwargs: DiaAudioKwargs\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": True,\n+            \"padding_side\": \"right\",\n+            \"add_special_tokens\": False,\n+        },\n+        \"audio_kwargs\": {\n+            \"eos_token_id\": 1024,\n+            \"pad_token_id\": 1025,\n+            \"bos_token_id\": 1026,\n+            \"delay_pattern\": [0, 8, 9, 10, 11, 12, 13, 14, 15],\n+            \"generation\": True,\n+            \"sampling_rate\": 44100,\n+        },\n+        \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+    }\n+\n+\n+class DiaProcessor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a Dia processor which wraps a [`DiaFeatureExtractor`], [`DiaTokenizer`], and a [`DacModel`] into\n+    a single processor. It inherits, the audio feature extraction, tokenizer, and audio encode/decode functio-\n+    nalities. See [`~DiaProcessor.__call__`], [`~DiaProcessor.encode`], and [`~DiaProcessor.decode`] for more\n+    information.\n+\n+    Args:\n+        feature_extractor (`DiaFeatureExtractor`):\n+            An instance of [`DiaFeatureExtractor`]. The feature extractor is a required input.\n+        tokenizer (`DiaTokenizer`):\n+            An instance of [`DiaTokenizer`]. The tokenizer is a required input.\n+        audio_tokenizer (`DacModel`):\n+            An instance of [`DacModel`] used to encode/decode audio into/from codebooks. It is is a required input.\n+    \"\"\"\n+\n+    feature_extractor_class = \"DiaFeatureExtractor\"\n+    tokenizer_class = \"DiaTokenizer\"\n+    audio_tokenizer_class = \"DacModel\"\n+\n+    def __init__(self, feature_extractor, tokenizer, audio_tokenizer):\n+        super().__init__(feature_extractor, tokenizer, audio_tokenizer=audio_tokenizer)\n+\n+    @property\n+    def model_input_names(self):\n+        \"\"\"\n+        We no longer pass the raw audio values but the codebooks encoded by the `audio_tokenizer`.\n+        Conventions may differ between audio models due to architectural choices.\n+        \"\"\"\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        audio_tokenizer_input_names = [\"decoder_input_ids\", \"decoder_attention_mask\"]\n+        return list(dict.fromkeys(tokenizer_input_names + audio_tokenizer_input_names))\n+\n+    def __call__(\n+        self,\n+        text: Union[str, list[str]],\n+        audio: Optional[AudioInput] = None,\n+        output_labels: Optional[bool] = False,\n+        **kwargs: Unpack[DiaProcessorKwargs],\n+    ):\n+        \"\"\"\n+        Main method to prepare text(s) and audio to be fed as input to the model. The `audio` argument is\n+        forwarded to the DiaFeatureExtractor's [`~DiaFeatureExtractor.__call__`] and subsequently to the\n+        DacModel's [`~DacModel.encode`]. The `text` argument to [`~DiaTokenizer.__call__`]. Please refer\n+        to the docstring of the above methods for more information.\n+        \"\"\"\n+        if not is_torch_available():\n+            raise ValueError(\n+                \"The `DiaProcessor` relies on the `audio_tokenizer` which requires `torch` but we couldn't \"\n+                \"find it in your environment. You can install torch via `pip install torch`.\"\n+            )\n+\n+        if text is None:\n+            raise ValueError(\"You need to specify the `text` input to process.\")\n+\n+        output_kwargs = self._merge_kwargs(\n+            DiaProcessorKwargs,\n+            **kwargs,\n+        )\n+\n+        text_kwargs = output_kwargs[\"text_kwargs\"]\n+        audio_kwargs = output_kwargs[\"audio_kwargs\"]\n+        common_kwargs = output_kwargs[\"common_kwargs\"]\n+\n+        return_tensors = common_kwargs.pop(\"return_tensors\", None)\n+        if return_tensors != \"pt\":\n+            raise ValueError(f\"{self.__class__.__name__} only supports `return_tensors='pt'`.\")\n+\n+        data = {}\n+\n+        # Text\n+        if isinstance(text, str):\n+            text = [text]\n+        elif not (isinstance(text, (list, tuple)) and all(isinstance(t, str) for t in text)):\n+            raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+\n+        encodings = self.tokenizer(text, **text_kwargs)\n+        data.update(encodings)\n+\n+        # Audio\n+        delay_pattern = audio_kwargs.pop(\"delay_pattern\", None)\n+        audio_bos_token_id = audio_kwargs.pop(\"bos_token_id\", None)\n+        audio_eos_token_id = audio_kwargs.pop(\"eos_token_id\", None)\n+        audio_pad_token_id = audio_kwargs.pop(\"pad_token_id\", None)\n+        generation = audio_kwargs.pop(\"generation\", True)\n+        if (\n+            audio_bos_token_id is None\n+            or audio_eos_token_id is None\n+            or audio_pad_token_id is None\n+            or delay_pattern is None\n+        ):\n+            raise ValueError(\n+                \"To enable processing for Dia, we need the `bos_token_id`, `eos_token_id`, \"\n+                \"`pad_token_id`, and `delay_pattern`. You may have accidentally overwritten one of those.\"\n+            )\n+\n+        if generation and output_labels:\n+            raise ValueError(\n+                f\"Labels with `generation` is incompatible, got generation={generation}, output_labels={output_labels}.\"\n+            )\n+\n+        batch_size = data[\"input_ids\"].shape[0]\n+        num_channels = len(delay_pattern)\n+        max_delay = max(delay_pattern)\n+\n+        # Voice cloning generation / general training\n+        if audio is not None:\n+            audio = make_list_of_audio(audio)\n+            input_audios = self.feature_extractor(audio, **audio_kwargs)\n+\n+            compression_rate = math.prod(self.audio_tokenizer.config.downsampling_ratios)\n+            max_encoded_sequence_len = input_audios[\"padding_mask\"][0].shape[-1] // compression_rate\n+\n+            decoder_input_ids = []\n+            decoder_attention_mask = []\n+            # TODO: dac with batching is currently broken, but non-batch is working\n+            # refer to https://gist.github.com/vasqu/643a45b680cf39fd7467271ee2eb6f80 for a validation script\n+            for padding_mask, audio in zip(input_audios[\"padding_mask\"], input_audios[\"input_values\"]):\n+                # get current length with hop length in mind (as if it were sampled as a single audio)\n+                base_pad_len = self.feature_extractor.hop_length\n+                current_audio_len = math.ceil(padding_mask.sum(dim=-1) / base_pad_len) * base_pad_len\n+\n+                encoded_sequence_len = current_audio_len // compression_rate\n+                padding_len = max_encoded_sequence_len - encoded_sequence_len\n+\n+                # compute non-padded forward pass; one extra bos (and eos if training) is added\n+                with torch.no_grad():\n+                    audio = audio[None, ..., :current_audio_len].to(self.audio_tokenizer.device)\n+                    input_ids = self.audio_tokenizer.encode(audio).audio_codes.transpose(1, 2)\n+\n+                if not generation:\n+                    input_ids = torch.nn.functional.pad(\n+                        input_ids, pad=(0, 0, 0, 1, 0, 0), mode=\"constant\", value=audio_eos_token_id\n+                    )\n+\n+                # apply padding\n+                # +1 for the bos within the real sequence\n+                input_ids = torch.nn.functional.pad(\n+                    input_ids, pad=(0, 0, padding_len + 1, 0, 0, 0), mode=\"constant\", value=audio_bos_token_id\n+                )\n+                num_valid_inputs = encoded_sequence_len + 1 + max_delay  # sequence + bos + delay\n+                num_valid_inputs += 0 if generation else 1  # eos if training\n+                attention_mask = torch.tensor([0] * padding_len + [1] * num_valid_inputs, dtype=torch.long)[None, :]\n+\n+                decoder_input_ids.append(input_ids)\n+                decoder_attention_mask.append(attention_mask)\n+\n+            decoder_input_ids = torch.cat(decoder_input_ids, dim=0)\n+            decoder_attention_mask = torch.cat(decoder_attention_mask, dim=0)\n+        # TTS generation\n+        elif generation:\n+            # all bos to start with TTS\n+            decoder_input_ids = torch.full((batch_size, 1, num_channels), audio_bos_token_id, dtype=torch.long)\n+\n+            # we preemptively add the delay\n+            decoder_attention_mask = torch.ones(size=(batch_size, 1 + max_delay), dtype=torch.long)\n+        else:\n+            raise ValueError(\"If you try to train, you should provide audio data as well.\")\n+\n+        if batch_size != decoder_input_ids.shape[0]:\n+            raise ValueError(\n+                f\"Need the same amount of samples for both text and audio, but got text samples={batch_size} and \"\n+                f\"audio samples = {decoder_input_ids.shape[0]} instead.\"\n+            )\n+\n+        # prepare shift indices per delay\n+        max_seq_len = decoder_attention_mask.shape[-1]\n+        max_audio_len = max_seq_len - max_delay\n+        precomputed_idx = self.build_indices(\n+            bsz=batch_size,\n+            seq_len=max_seq_len,\n+            num_channels=num_channels,\n+            delay_pattern=delay_pattern,\n+            revert=False,\n+        )\n+\n+        # create delay pattern input\n+        # the pad token will be used for masking which input is valid for prediction during generation\n+        prefill = torch.full(\n+            (batch_size, max_seq_len, num_channels),\n+            fill_value=audio_pad_token_id,\n+            dtype=torch.int,\n+        )\n+        prefill[:, :max_audio_len] = decoder_input_ids\n+\n+        delayed_decoder_input_ids = self.apply_audio_delay(\n+            audio=prefill,\n+            pad_token_id=audio_pad_token_id,\n+            bos_token_id=audio_bos_token_id,\n+            precomputed_idx=precomputed_idx,\n+        )\n+\n+        data.update({\"decoder_input_ids\": delayed_decoder_input_ids, \"decoder_attention_mask\": decoder_attention_mask})\n+\n+        if output_labels:\n+            # Base idea is to shift on the sequence dim\n+            labels = data[\"decoder_input_ids\"].clone()[:, 1:]\n+            labels[labels == audio_pad_token_id] = -100\n+            labels[labels == audio_bos_token_id] = -100\n+\n+            data[\"labels\"] = labels.transpose(1, 2).reshape(batch_size * num_channels, -1).contiguous().long()\n+            data[\"decoder_input_ids\"] = data[\"decoder_input_ids\"][:, :-1]\n+            data[\"decoder_attention_mask\"] = data[\"decoder_attention_mask\"][:, :-1]\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+    def batch_decode(\n+        self,\n+        decoder_input_ids: \"torch.Tensor\",\n+        audio_prompt_len: Optional[int] = None,\n+        **kwargs: Unpack[DiaProcessorKwargs],\n+    ) -> list[\"torch.Tensor\"]:\n+        \"\"\"\n+        Decodes a batch of audio codebook sequences into their respective audio waveforms via the\n+        `audio_tokenizer`. See [`~DacModel.decode`] for more information.\n+\n+        Args:\n+            decoder_input_ids (`torch.Tensor`): The complete output sequence of the decoder.\n+            audio_prompt_len (`int`): The audio prefix length (e.g. when using voice cloning).\n+        \"\"\"\n+        output_kwargs = self._merge_kwargs(\n+            DiaProcessorKwargs,\n+            **kwargs,\n+        )\n+        audio_kwargs = output_kwargs[\"audio_kwargs\"]\n+\n+        delay_pattern = audio_kwargs.pop(\"delay_pattern\", None)\n+        audio_bos_token_id = audio_kwargs.pop(\"bos_token_id\", None)\n+        audio_pad_token_id = audio_kwargs.pop(\"pad_token_id\", None)\n+        if audio_bos_token_id is None or audio_pad_token_id is None or delay_pattern is None:\n+            raise ValueError(\n+                \"To enable decoding for Dia, we need the `bos_token_id`, `pad_token_id`, \"\n+                \"and `delay_pattern`. You may have accidentally overwritten one of those.\"\n+            )\n+\n+        # either decode the whole audio sequence or only the generated parts\n+        if audio_prompt_len is not None:\n+            audio_prompt_len = torch.tensor(audio_prompt_len, device=decoder_input_ids.device, dtype=torch.long)\n+            start_of_generation_idx = audio_prompt_len[None].expand(decoder_input_ids.shape[0])\n+        else:\n+            start_of_generation_idx = (decoder_input_ids[:, :, 0] == audio_bos_token_id).sum(dim=-1)\n+        # -1 for the eos token\n+        end_of_generation_idx = (\n+            decoder_input_ids.shape[1] - (decoder_input_ids[:, :, 0] == audio_pad_token_id).sum(dim=-1) - 1\n+        )\n+\n+        # revert delay\n+        bsz, seq_len, num_channels = decoder_input_ids.shape\n+        precomputed_idx = self.build_indices(\n+            bsz=bsz,\n+            seq_len=seq_len,\n+            num_channels=num_channels,\n+            delay_pattern=delay_pattern,\n+            revert=True,\n+        )\n+\n+        output_sequences = self.apply_audio_delay(\n+            audio=decoder_input_ids,\n+            # We do not care about these values as we cut them out\n+            # with `start_of_generation_idx` and `end_of_generation_idx`\n+            pad_token_id=-1,\n+            bos_token_id=-1,\n+            precomputed_idx=precomputed_idx,\n+        ).transpose(1, 2)\n+\n+        # retrieve the correct sequences each\n+        audios = []\n+        # TODO: see above, dac doesn't work in batches yet\n+        with torch.no_grad():\n+            for i in range(start_of_generation_idx.shape[0]):\n+                output_i = output_sequences[i, :, start_of_generation_idx[i] : end_of_generation_idx[i]][None, ...]\n+                output_i = output_i.to(self.audio_tokenizer.device)\n+                audio_i = self.audio_tokenizer.decode(audio_codes=output_i).audio_values.cpu().squeeze()\n+                audios.append(audio_i)\n+\n+        return audios\n+\n+    def decode(\n+        self,\n+        decoder_input_ids: \"torch.Tensor\",\n+        audio_prompt_len: Optional[int] = None,\n+        **kwargs: Unpack[DiaProcessorKwargs],\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Decodes a single sequence of audio codebooks into the respective audio waveform via the\n+        `audio_tokenizer`. See [`~DacModel.decode`] and [`~DiaProcessor.batch_decode`] for more information.\n+        \"\"\"\n+        if decoder_input_ids.shape[0] != 1:\n+            raise ValueError(\n+                f\"Expecting a single output to be decoded but received {decoder_input_ids.shape[0]} samples instead.\"\n+            )\n+\n+        return self.batch_decode(decoder_input_ids, audio_prompt_len, **kwargs)[0]\n+\n+    def get_audio_prompt_len(\n+        self,\n+        decoder_attention_mask: \"torch.Tensor\",\n+        **kwargs: Unpack[DiaProcessorKwargs],\n+    ) -> int:\n+        \"\"\"Utility function to get the audio prompt length.\"\"\"\n+        output_kwargs = self._merge_kwargs(\n+            DiaProcessorKwargs,\n+            **kwargs,\n+        )\n+        audio_kwargs = output_kwargs[\"audio_kwargs\"]\n+\n+        delay_pattern = audio_kwargs.pop(\"delay_pattern\", None)\n+        if delay_pattern is None:\n+            raise ValueError(\n+                \"To enable the utility of retrieving the prompt length for Dia, we need the \"\n+                \"`delay_pattern`. You may have accidentally overwritten this.\"\n+            )\n+        return decoder_attention_mask.shape[1] - max(delay_pattern)\n+\n+    # Copied from transformers.models.csm.processing_csm.CsmProcessor.save_audio with Csm->Dia\n+    def save_audio(\n+        self,\n+        audio: AudioInput,\n+        saving_path: Union[str, Path, list[Union[str, Path]]],\n+        **kwargs: Unpack[DiaProcessorKwargs],\n+    ):\n+        # TODO: @eustlb, this should be in AudioProcessor\n+        if not is_soundfile_available():\n+            raise ImportError(\"Please install `soundfile` to save audio files.\")\n+\n+        # ensure correct audio input\n+        audio = make_list_of_audio(audio)\n+\n+        # ensure correct saving path\n+        if isinstance(saving_path, (str, Path)):\n+            saving_path = [saving_path]\n+        elif not (isinstance(saving_path, (list, tuple)) and all(isinstance(p, (str, Path)) for p in saving_path)):\n+            raise ValueError(\"Invalid input path. Please provide a string, or a list of strings\")\n+\n+        if len(audio) != len(saving_path):\n+            raise ValueError(\"The number of audio and saving paths must be the same\")\n+\n+        output_kwargs = self._merge_kwargs(\n+            DiaProcessorKwargs,\n+            **kwargs,\n+        )\n+        audio_kwargs = output_kwargs[\"audio_kwargs\"]\n+        sampling_rate = audio_kwargs[\"sampling_rate\"]\n+\n+        for audio_value, p in zip(audio, saving_path):\n+            if isinstance(audio_value, torch.Tensor):\n+                audio_value = audio_value.cpu().float().numpy()\n+            sf.write(p, audio_value, sampling_rate)\n+\n+    @staticmethod\n+    def build_indices(\n+        bsz: int,\n+        seq_len: int,\n+        num_channels: int,\n+        delay_pattern: list[int],\n+        revert: bool = False,\n+    ) -> tuple[\"torch.Tensor\", \"torch.Tensor\"]:\n+        \"\"\"\n+        Precompute (sequence_idx, all_idx) so that out[seq, channel] = in[seq - delay[channel], channel]\n+        or in[seq, channel] = out[seq + delay[channel], channel] if `revert`.\n+        Negative sequence_idx => BOS; sequence_idx >= seq_len => PAD.\n+        \"\"\"\n+        delay_array = torch.tensor(delay_pattern, dtype=torch.int32)\n+\n+        # (0..seq_len-1)\n+        sequence_idx = torch.arange(seq_len, dtype=torch.int32)[None, :].expand(bsz, seq_len)[..., None]\n+        # + or - delay depending if we delay or revert the delay\n+        if not revert:\n+            sequence_idx = sequence_idx - delay_array[None, None, :]\n+        else:\n+            sequence_idx = sequence_idx + delay_array[None, None, :]\n+        # if delay goes over the range we clamp back to valid values\n+        valid_sequence_idx = torch.clamp(sequence_idx, 0, seq_len - 1)\n+\n+        batch_idx = torch.arange(bsz, dtype=torch.int32)[:, None, None].expand(bsz, seq_len, num_channels)\n+        channel_idx = torch.arange(num_channels, dtype=torch.int32)[None, None, :].expand(bsz, seq_len, num_channels)\n+\n+        all_idx = torch.stack(\n+            [batch_idx.reshape(-1), valid_sequence_idx.reshape(-1), channel_idx.reshape(-1)],\n+            dim=1,\n+        ).long()\n+\n+        return sequence_idx, all_idx\n+\n+    @staticmethod\n+    def apply_audio_delay(\n+        audio: \"torch.Tensor\",\n+        pad_token_id: int,\n+        bos_token_id: int,\n+        precomputed_idx: tuple[\"torch.Tensor\", \"torch.Tensor\"],\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Applies or reverts the delay pattern to batched audio tokens using precomputed indices,\n+        inserting BOS where sequence_idx < 0 and PAD where sequence_idx >= seq_len.\n+\n+        Args:\n+            audio: audio tokens of shape [bsz, seq_len, num_channels]\n+            pad_token_id: the PAD token\n+            bos_token_id: the BOS token\n+            precomputed_idx: from `build_indices`\n+\n+        Returns:\n+            final_audio: delayed or reverted audio tokens of shape [bsz, seq_len, num_channels]\n+        \"\"\"\n+        # Move everything to the same device\n+        device = audio.device\n+        sequence_idx, all_idx = precomputed_idx\n+        sequence_idx = sequence_idx.to(device)\n+        all_idx = all_idx.to(device)\n+\n+        # Gather per precomputed indices\n+        batch_idx, valid_sequence_idx, channel_idx = torch.unbind(all_idx, dim=-1)\n+        gathered_audio = audio[batch_idx, valid_sequence_idx, channel_idx].view(audio.size())\n+\n+        # Mask according to negative sequence_idx => BOS; sequence_idx >= seq_len => PAD\n+        mask_bos = sequence_idx < 0\n+        mask_pad = sequence_idx >= audio.shape[1]\n+        final_audio = torch.where(mask_bos, bos_token_id, torch.where(mask_pad, pad_token_id, gathered_audio))\n+\n+        return final_audio\n+\n+\n+__all__ = [\"DiaProcessor\"]"
        },
        {
            "sha": "4e205906ea709ee2c20f25b0bf6f4fa66ab1f4a4",
            "filename": "src/transformers/models/dia/tokenization_dia.py",
            "status": "added",
            "additions": 118,
            "deletions": 0,
            "changes": 118,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fdia%2Ftokenization_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fmodels%2Fdia%2Ftokenization_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Ftokenization_dia.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -0,0 +1,118 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Tokenization class for Dia.\"\"\"\n+\n+from typing import Optional\n+\n+from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class DiaTokenizer(PreTrainedTokenizer):\n+    \"\"\"\n+    Construct a Dia tokenizer. Dia simply uses raw bytes utf-8 encoding except for special tokens `[S1]` and `[S2]`.\n+\n+    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n+    refer to this superclass for more information regarding those methods.\n+\n+    Args:\n+        pad_token (`str`, *optional*, defaults to `\"<pad>\"`):\n+            The token used for padding, for example when batching sequences of different lengths.\n+        unk_token (`str`, *optional*, defaults to `\"<pad>\"`):\n+            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n+            token instead.\n+        max_length (`int`, *optional*, defaults to 1024):\n+            The maximum length of the sequences when encoding. Sequences longer than this will be truncated.\n+        offset (`int`, *optional*, defaults to 0):\n+            The offset of the tokenizer.\n+    \"\"\"\n+\n+    model_input_names = [\"input_ids\", \"attention_mask\"]\n+\n+    def __init__(\n+        self,\n+        pad_token: Optional[str] = \"<pad>\",\n+        unk_token: Optional[str] = \"<pad>\",\n+        max_length: Optional[int] = 1024,\n+        offset: int = 0,\n+        **kwargs,\n+    ):\n+        # We have no eos/bos tokens but allow padding -- no l/r strip as we treat them as tokens as well\n+        pad_token = AddedToken(pad_token) if isinstance(pad_token, str) else pad_token\n+        unk_token = AddedToken(unk_token) if isinstance(unk_token, str) else unk_token\n+\n+        self._utf_vocab_size = 2**8  # utf is 8 bits\n+        self._added_tokens_decoder = {0: pad_token, 1: AddedToken(\"[S1]\"), 2: AddedToken(\"[S2]\")}\n+        self.offset = offset\n+        super().__init__(\n+            unk_token=unk_token,\n+            pad_token=pad_token,\n+            max_length=max_length,\n+            **kwargs,\n+        )\n+\n+    @property\n+    def vocab_size(self):\n+        return self._utf_vocab_size\n+\n+    def get_vocab(self):\n+        vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size + self.offset)}\n+        vocab.update(self.added_tokens_encoder)\n+        return vocab\n+\n+    def _tokenize(self, text: str) -> list[str]:\n+        \"\"\"Take as input a string and return a list of strings (tokens) for words/sub-words\"\"\"\n+        tokens = [chr(i) for i in text.encode(\"utf-8\")]\n+        return tokens\n+\n+    def _convert_token_to_id(self, token):\n+        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n+\n+        if len(token) != 1:\n+            token_id = None\n+        else:\n+            token_id = ord(token) + self.offset\n+\n+        return token_id\n+\n+    def _convert_id_to_token(self, index):\n+        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n+        token = chr(index - self.offset)\n+        return token\n+\n+    def convert_tokens_to_string(self, tokens: list[str]) -> str:\n+        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n+        bstring = b\"\"\n+        for token in tokens:\n+            if token in self.added_tokens_decoder:\n+                added_token_obj = self.added_tokens_decoder[token]\n+                tok_string = str(added_token_obj).encode(\"utf-8\")\n+            elif token in self.added_tokens_encoder:\n+                tok_string = token.encode(\"utf-8\")\n+            else:\n+                tok_string = token.encode(\"utf-8\")  # Assume general string token\n+            bstring += tok_string\n+        string = bstring.decode(\"utf-8\", errors=\"ignore\")\n+        return string\n+\n+    # No vocab file\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n+        return ()\n+\n+\n+__all__ = [\"DiaTokenizer\"]"
        },
        {
            "sha": "afeae13ae762c5f9e42c2f343671d84cc30b6a39",
            "filename": "src/transformers/pipelines/text_to_audio.py",
            "status": "modified",
            "additions": 27,
            "deletions": 7,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fpipelines%2Ftext_to_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fpipelines%2Ftext_to_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext_to_audio.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -80,15 +80,21 @@ class TextToAudioPipeline(Pipeline):\n     See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=text-to-speech).\n     \"\"\"\n \n+    # Introducing the processor at load time for new behaviour\n+    _load_processor = True\n+\n     _pipeline_calls_generate = True\n     # Make sure the docstring is updated when the default generation config is changed\n     _default_generation_config = GenerationConfig(\n         max_new_tokens=256,\n     )\n \n-    def __init__(self, *args, vocoder=None, sampling_rate=None, **kwargs):\n+    def __init__(self, *args, vocoder=None, sampling_rate=None, no_processor=True, **kwargs):\n         super().__init__(*args, **kwargs)\n \n+        # Legacy behaviour just uses the tokenizer while new models use the processor as a whole at any given time\n+        self.no_processor = no_processor\n+\n         if self.framework == \"tf\":\n             raise ValueError(\"The TextToAudioPipeline is only available in PyTorch.\")\n \n@@ -117,6 +123,10 @@ def __init__(self, *args, vocoder=None, sampling_rate=None, **kwargs):\n                 if sampling_rate is not None:\n                     self.sampling_rate = sampling_rate\n \n+        # last fallback to get the sampling rate based on processor\n+        if self.sampling_rate is None and not self.no_processor and hasattr(self.processor, \"feature_extractor\"):\n+            self.sampling_rate = self.processor.feature_extractor.sampling_rate\n+\n     def preprocess(self, text, **kwargs):\n         if isinstance(text, str):\n             text = [text]\n@@ -136,7 +146,8 @@ def preprocess(self, text, **kwargs):\n \n             kwargs = new_kwargs\n \n-        output = self.tokenizer(text, **kwargs, return_tensors=\"pt\")\n+        preprocessor = self.tokenizer if self.no_processor else self.processor\n+        output = preprocessor(text, **kwargs, return_tensors=\"pt\")\n \n         return output\n \n@@ -228,12 +239,21 @@ def _sanitize_parameters(\n \n         return preprocess_params, params, postprocess_params\n \n-    def postprocess(self, waveform):\n+    def postprocess(self, audio):\n         output_dict = {}\n-        if isinstance(waveform, dict):\n-            waveform = waveform[\"waveform\"]\n-        elif isinstance(waveform, tuple):\n-            waveform = waveform[0]\n+\n+        # We directly get the waveform\n+        if self.no_processor:\n+            if isinstance(audio, dict):\n+                waveform = audio[\"waveform\"]\n+            elif isinstance(audio, tuple):\n+                waveform = audio[0]\n+            else:\n+                waveform = audio\n+        # Or we need to postprocess to get the waveform\n+        else:\n+            waveform = self.processor.decode(audio)\n+\n         output_dict[\"audio\"] = waveform.to(device=\"cpu\", dtype=torch.float).numpy()\n         output_dict[\"sampling_rate\"] = self.sampling_rate\n "
        },
        {
            "sha": "2a97cde3ccf5841a785529000f30220c1982f8a4",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 102,
            "deletions": 17,
            "changes": 119,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -49,6 +49,7 @@\n     TruncationStrategy,\n )\n from .utils import (\n+    AUDIO_TOKENIZER_NAME,\n     CHAT_TEMPLATE_DIR,\n     CHAT_TEMPLATE_FILE,\n     LEGACY_PROCESSOR_CHAT_TEMPLATE_FILE,\n@@ -61,12 +62,17 @@\n     download_url,\n     is_offline_mode,\n     is_remote_url,\n+    is_torch_available,\n     list_repo_templates,\n     logging,\n )\n from .utils.deprecation import deprecate_kwarg\n \n \n+if is_torch_available():\n+    from .modeling_utils import PreTrainedAudioTokenizerBase\n+\n+\n logger = logging.get_logger(__name__)\n \n # Dynamically import the Transformers module to grab the attribute classes of the processor from their names.\n@@ -499,7 +505,7 @@ class ProcessorMixin(PushToHubMixin):\n     \"\"\"\n \n     attributes = [\"feature_extractor\", \"tokenizer\"]\n-    optional_attributes = [\"chat_template\"]\n+    optional_attributes = [\"chat_template\", \"audio_tokenizer\"]\n     optional_call_args: list[str] = []\n     # Names need to be attr_class for attr in attributes\n     feature_extractor_class = None\n@@ -511,7 +517,19 @@ def __init__(self, *args, **kwargs):\n         # First, extract optional attributes from kwargs if present\n         # Optional attributes can never be positional arguments\n         for optional_attribute in self.optional_attributes:\n-            setattr(self, optional_attribute, kwargs.pop(optional_attribute, None))\n+            optional_attribute_value = kwargs.pop(optional_attribute, None)\n+            setattr(self, optional_attribute, optional_attribute_value)\n+\n+            # Check audio tokenizer for its class but do not treat it as attr to avoid saving weights\n+            if optional_attribute == \"audio_tokenizer\" and optional_attribute_value is not None:\n+                proper_class = self.check_argument_for_proper_class(optional_attribute, optional_attribute_value)\n+\n+                if not (is_torch_available() and isinstance(optional_attribute_value, PreTrainedAudioTokenizerBase)):\n+                    raise ValueError(\n+                        f\"Tried to use `{proper_class}` for audio tokenization. However, this class is not\"\n+                        \" registered for audio tokenization.\"\n+                    )\n+\n         # Sanitize args and kwargs\n         for key in kwargs:\n             if key not in self.attributes:\n@@ -530,20 +548,29 @@ def __init__(self, *args, **kwargs):\n \n         # Check each arg is of the proper class (this will also catch a user initializing in the wrong order)\n         for attribute_name, arg in kwargs.items():\n-            class_name = getattr(self, f\"{attribute_name}_class\")\n-            # Nothing is ever going to be an instance of \"AutoXxx\", in that case we check the base class.\n-            class_name = AUTO_TO_BASE_CLASS_MAPPING.get(class_name, class_name)\n-            if isinstance(class_name, tuple):\n-                proper_class = tuple(self.get_possibly_dynamic_module(n) for n in class_name if n is not None)\n-            else:\n-                proper_class = self.get_possibly_dynamic_module(class_name)\n+            self.check_argument_for_proper_class(attribute_name, arg)\n+            setattr(self, attribute_name, arg)\n \n-            if not isinstance(arg, proper_class):\n-                raise TypeError(\n-                    f\"Received a {type(arg).__name__} for argument {attribute_name}, but a {class_name} was expected.\"\n-                )\n+    def check_argument_for_proper_class(self, argument_name, argument):\n+        \"\"\"\n+        Checks the passed argument's class against the expected transformers class. In case of an unexpected\n+        mismatch between expected and actual class, an error is raise. Otherwise, the proper retrieved class\n+        is returned.\n+        \"\"\"\n+        class_name = getattr(self, f\"{argument_name}_class\")\n+        # Nothing is ever going to be an instance of \"AutoXxx\", in that case we check the base class.\n+        class_name = AUTO_TO_BASE_CLASS_MAPPING.get(class_name, class_name)\n+        if isinstance(class_name, tuple):\n+            proper_class = tuple(self.get_possibly_dynamic_module(n) for n in class_name if n is not None)\n+        else:\n+            proper_class = self.get_possibly_dynamic_module(class_name)\n \n-            setattr(self, attribute_name, arg)\n+        if not isinstance(argument, proper_class):\n+            raise TypeError(\n+                f\"Received a {type(argument).__name__} for argument {argument_name}, but a {class_name} was expected.\"\n+            )\n+\n+        return proper_class\n \n     def to_dict(self) -> dict[str, Any]:\n         \"\"\"\n@@ -577,6 +604,8 @@ def to_dict(self) -> dict[str, Any]:\n             del output[\"feature_extractor\"]\n         if \"chat_template\" in output:\n             del output[\"chat_template\"]\n+        if \"audio_tokenizer\" in output:\n+            del output[\"audio_tokenizer\"]\n \n         # Some attributes have different names but containing objects that are not simple strings\n         output = {\n@@ -695,6 +724,7 @@ def save_pretrained(self, save_directory, push_to_hub: bool = False, **kwargs):\n             save_directory, LEGACY_PROCESSOR_CHAT_TEMPLATE_FILE\n         )  # Legacy filename\n         chat_template_dir = os.path.join(save_directory, CHAT_TEMPLATE_DIR)\n+        output_audio_tokenizer_file = os.path.join(save_directory, AUDIO_TOKENIZER_NAME)\n \n         processor_dict = self.to_dict()\n         # Save `chat_template` in its own file. We can't get it from `processor_dict` as we popped it in `to_dict`\n@@ -737,6 +767,19 @@ def save_pretrained(self, save_directory, push_to_hub: bool = False, **kwargs):\n                     \"separate files using the `save_jinja_files` argument.\"\n                 )\n \n+        if self.audio_tokenizer is not None:\n+            audio_tokenizer_class = self.audio_tokenizer.__class__.__name__\n+            audio_tokenizer_name_or_path = self.audio_tokenizer.name_or_path\n+\n+            audio_tokenizer_dict = {\n+                \"audio_tokenizer_class\": audio_tokenizer_class,\n+                \"audio_tokenizer_name_or_path\": audio_tokenizer_name_or_path,\n+            }\n+            audio_tokenizer_json = json.dumps(audio_tokenizer_dict, indent=2, sort_keys=True) + \"\\n\"\n+\n+            with open(output_audio_tokenizer_file, \"w\", encoding=\"utf-8\") as writer:\n+                writer.write(audio_tokenizer_json)\n+\n         # For now, let's not save to `processor_config.json` if the processor doesn't have extra attributes and\n         # `auto_map` is not specified.\n         if set(processor_dict.keys()) != {\"processor_class\"}:\n@@ -774,6 +817,9 @@ def get_processor_dict(\n         Returns:\n             `tuple[Dict, Dict]`: The dictionary(ies) that will be used to instantiate the processor object.\n         \"\"\"\n+        # holding a copy for optionally loading the audio tokenizer (if available)\n+        audio_tokenizer_kwargs = copy.deepcopy(kwargs)\n+\n         cache_dir = kwargs.pop(\"cache_dir\", None)\n         force_download = kwargs.pop(\"force_download\", False)\n         resume_download = kwargs.pop(\"resume_download\", None)\n@@ -803,16 +849,18 @@ def get_processor_dict(\n         resolved_additional_chat_template_files = {}\n         if os.path.isfile(pretrained_model_name_or_path):\n             resolved_processor_file = pretrained_model_name_or_path\n-            # can't load chat-template when given a file as pretrained_model_name_or_path\n+            # can't load chat-template and audio tokenizer when given a file as pretrained_model_name_or_path\n             resolved_chat_template_file = None\n             resolved_raw_chat_template_file = None\n+            resolved_audio_tokenizer_file = None\n             is_local = True\n         elif is_remote_url(pretrained_model_name_or_path):\n             processor_file = pretrained_model_name_or_path\n             resolved_processor_file = download_url(pretrained_model_name_or_path)\n-            # can't load chat-template when given a file url as pretrained_model_name_or_path\n+            # can't load chat-template and audio tokenizer when given a file url as pretrained_model_name_or_path\n             resolved_chat_template_file = None\n             resolved_raw_chat_template_file = None\n+            resolved_audio_tokenizer_file = None\n         else:\n             if is_local:\n                 template_dir = Path(pretrained_model_name_or_path, CHAT_TEMPLATE_DIR)\n@@ -899,6 +947,21 @@ def get_processor_dict(\n                     )\n                     for template_name, template_file in additional_chat_template_files.items()\n                 }\n+\n+                resolved_audio_tokenizer_file = cached_file(\n+                    pretrained_model_name_or_path,\n+                    AUDIO_TOKENIZER_NAME,\n+                    cache_dir=cache_dir,\n+                    force_download=force_download,\n+                    proxies=proxies,\n+                    resume_download=resume_download,\n+                    local_files_only=local_files_only,\n+                    token=token,\n+                    user_agent=user_agent,\n+                    revision=revision,\n+                    subfolder=subfolder,\n+                    _raise_exceptions_for_missing_entries=False,\n+                )\n             except OSError:\n                 # Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\n                 # the original exception.\n@@ -939,6 +1002,22 @@ def get_processor_dict(\n         if chat_templates:\n             kwargs[\"chat_template\"] = chat_templates\n \n+        # Same as chat template, adding as kwarg after loading the model\n+        audio_tokenizer = None\n+        if resolved_audio_tokenizer_file is not None:\n+            with open(resolved_audio_tokenizer_file, \"r\", encoding=\"utf-8\") as reader:\n+                # The json contains the references we need to init the correct model\n+                audio_tokenizer_references = json.load(reader)\n+                audio_tokenizer_class = cls.get_possibly_dynamic_module(\n+                    audio_tokenizer_references[\"audio_tokenizer_class\"]\n+                )\n+                audio_tokenizer_path = audio_tokenizer_references[\"audio_tokenizer_name_or_path\"]\n+\n+            audio_tokenizer = audio_tokenizer_class.from_pretrained(audio_tokenizer_path, **audio_tokenizer_kwargs)\n+\n+        if audio_tokenizer is not None:\n+            kwargs[\"audio_tokenizer\"] = audio_tokenizer\n+\n         # Existing processors on the Hub created before #27761 being merged don't have `processor_config.json` (if not\n         # updated afterward), and we need to keep `from_pretrained` work. So here it fallbacks to the empty dict.\n         # (`cached_file` called using `_raise_exceptions_for_missing_entries=False` to avoid exception)\n@@ -947,7 +1026,9 @@ def get_processor_dict(\n             # In any case we need to pass `chat_template` if it is available\n             processor_dict = {}\n             if \"chat_template\" in kwargs:\n-                processor_dict = {\"chat_template\": kwargs.pop(\"chat_template\")}\n+                processor_dict[\"chat_template\"] = kwargs.pop(\"chat_template\")\n+            if \"audio_tokenizer\" in kwargs:\n+                processor_dict[\"audio_tokenizer\"] = kwargs.pop(\"audio_tokenizer\")\n             return processor_dict, kwargs\n \n         try:\n@@ -972,6 +1053,8 @@ def get_processor_dict(\n \n         if \"chat_template\" in kwargs:\n             processor_dict[\"chat_template\"] = kwargs.pop(\"chat_template\")\n+        if \"audio_tokenizer\" in kwargs:\n+            processor_dict[\"audio_tokenizer\"] = kwargs.pop(\"audio_tokenizer\")\n \n         return processor_dict, kwargs\n \n@@ -1276,6 +1359,7 @@ def _get_arguments_from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n                 attribute_class = cls.get_possibly_dynamic_module(class_name)\n \n             args.append(attribute_class.from_pretrained(pretrained_model_name_or_path, **kwargs))\n+\n         return args\n \n     @staticmethod\n@@ -1287,6 +1371,7 @@ def get_possibly_dynamic_module(module_name):\n             transformers_module.VIDEO_PROCESSOR_MAPPING,\n             transformers_module.TOKENIZER_MAPPING,\n             transformers_module.FEATURE_EXTRACTOR_MAPPING,\n+            transformers_module.MODEL_FOR_AUDIO_TOKENIZATION_MAPPING,\n         ]\n         for lookup_location in lookup_locations:\n             for custom_class in lookup_location._extra_content.values():"
        },
        {
            "sha": "4943e91e73e272f024cb32ad4107e1b6d49d9f4c",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -292,6 +292,7 @@\n FEATURE_EXTRACTOR_NAME = \"preprocessor_config.json\"\n IMAGE_PROCESSOR_NAME = \"preprocessor_config.json\"\n VIDEO_PROCESSOR_NAME = \"video_preprocessor_config.json\"\n+AUDIO_TOKENIZER_NAME = \"audio_tokenizer_config.json\"\n PROCESSOR_NAME = \"processor_config.json\"\n GENERATION_CONFIG_NAME = \"generation_config.json\"\n MODEL_CARD_NAME = \"modelcard.json\""
        },
        {
            "sha": "834c502b1a35d675faedfdbf417eb6abca9a0e6d",
            "filename": "tests/generation/test_logits_process.py",
            "status": "modified",
            "additions": 148,
            "deletions": 1,
            "changes": 149,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/tests%2Fgeneration%2Ftest_logits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/tests%2Fgeneration%2Ftest_logits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_logits_process.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -56,7 +56,12 @@\n         UnbatchedClassifierFreeGuidanceLogitsProcessor,\n         WatermarkLogitsProcessor,\n     )\n-    from transformers.generation.logits_process import BarkEosPrioritizerLogitsProcessor\n+    from transformers.generation.logits_process import (\n+        BarkEosPrioritizerLogitsProcessor,\n+        DiaClassifierFreeGuidanceLogitsProcessor,\n+        DiaEOSChannelFilterLogitsProcessor,\n+        DiaEOSDelayPatternLogitsProcessor,\n+    )\n \n \n @require_torch\n@@ -1211,3 +1216,145 @@ def test_synthidtext_watermark_processor_bias_test(self, vocab_size, ngram_len,\n             )\n         )\n         self.assertTrue(is_close)\n+\n+    def test_dia_classifier_free_guidance(self):\n+        input_ids = torch.LongTensor([[0]])\n+        logits_uncond = torch.tensor([[1.0, 0, 1.5]])\n+        logits_cond = torch.tensor([[1.0, 1.0, 1.0]])\n+\n+        # base cfg with conditioned as center\n+        cfg = DiaClassifierFreeGuidanceLogitsProcessor(guidance_scale=1.5)\n+        out = cfg(input_ids, torch.cat([logits_cond, logits_uncond], dim=0))\n+\n+        res = logits_cond + 1.5 * (logits_cond - logits_uncond)\n+\n+        self.assertAlmostEqual(out[0, 0].item(), res[0, 0].item())\n+        self.assertAlmostEqual(out[0, 1].item(), res[0, 1].item())\n+        self.assertAlmostEqual(out[0, 2].item(), res[0, 2].item())\n+\n+        # additional top k (on cond logits)\n+        cfg = DiaClassifierFreeGuidanceLogitsProcessor(guidance_scale=1.5, guidance_top_k=1)\n+        out = cfg(input_ids, torch.cat([logits_cond, logits_uncond], dim=0))\n+\n+        res = logits_cond + 1.5 * (logits_cond - logits_uncond)\n+        mask = res == res.max()\n+        res = logits_cond.clone()\n+        res[~mask.bool()] = -float(\"inf\")\n+\n+        self.assertAlmostEqual(out[0, 0].item(), res[0, 0].item())\n+        self.assertAlmostEqual(out[0, 1].item(), res[0, 1].item())\n+        self.assertAlmostEqual(out[0, 2].item(), res[0, 2].item())\n+\n+    def test_dia_channel_filter(self):\n+        eos = 2\n+        bsz, channels, vocab = 2, 2, 4\n+\n+        input_ids = torch.LongTensor([[0]])\n+        logits = torch.zeros(size=(bsz, channels, vocab)).view(bsz * channels, vocab)\n+        logits[0, eos] = 1  # Eos max (forced)\n+        logits[1, eos] = 1  # Eos max (forced) but not channel 0\n+\n+        channel_filter = DiaEOSChannelFilterLogitsProcessor(num_channels=channels, eos_token_id=eos)\n+        out = channel_filter(input_ids, logits).view(bsz, channels, vocab)\n+\n+        for i in range(vocab):\n+            if i > eos:\n+                # special tokens are not to be predicted\n+                self.assertTrue((out[:, :, i] == -float(\"inf\")).all())\n+            elif i == eos:\n+                # Eos forced on channel 0\n+                self.assertTrue(out[0, 0, i] == 1)\n+                # Eos suppressed on everything else (even if max before)\n+                self.assertTrue(out[0, 1, i] == -float(\"inf\"))\n+                self.assertTrue((out[1, :, i] == -float(\"inf\")).all())\n+            else:\n+                # Eos forced on channel 0\n+                self.assertTrue(out[0, 0, i] == -float(\"inf\"))\n+                # previous values\n+                self.assertTrue(out[0, 1, i] == 0)\n+                self.assertTrue((out[1, :, i] == 0).all())\n+\n+    def test_dia_delay_pattern(self):\n+        def check_eos_logits(out, logits, batch, channel, eos):\n+            for i in range(vocab):\n+                if i == eos:\n+                    self.assertTrue(out[batch, channel, i] == 0)\n+                else:\n+                    self.assertTrue(out[batch, channel, i] == -float(\"inf\"))\n+\n+            for c in range(channel):\n+                if c != channel:\n+                    self.assertTrue((out[batch, c] == logits[batch, c]).all())\n+\n+        eos = 2\n+        delay_pattern = [0, 2, 3]\n+        max_generation_len = 10\n+        bsz, channels, vocab = 2, 3, 4\n+\n+        input_ids = torch.LongTensor([[0]])\n+        logits = torch.zeros(size=(bsz, channels, vocab))\n+        # Ensure that argmax can not result in eos\n+        logits[:, :, eos] = -1\n+\n+        delay_pattern_processor = DiaEOSDelayPatternLogitsProcessor(\n+            delay_pattern=delay_pattern, eos_token_id=eos, max_generation_len=max_generation_len\n+        )\n+        out = delay_pattern_processor(input_ids, logits.clone()).view(bsz, channels, vocab)\n+\n+        # Nothing should happen except for init of some attributes\n+        self.assertTrue((out == logits).all())\n+        self.assertTrue((~delay_pattern_processor.active_batches).all())\n+        self.assertTrue(\n+            (delay_pattern_processor.delay_pattern == torch.tensor([delay_pattern for _ in range(bsz)])).all()\n+        )\n+\n+        # Make first batch end\n+        logits[0, 0, eos] = 1\n+\n+        # Go through the complete delay pattern\n+        for i in range(max(delay_pattern) + 1):\n+            out = delay_pattern_processor(input_ids, logits.clone()).view(bsz, channels, vocab)\n+\n+            # no delay should kick in\n+            if i == 1:\n+                self.assertTrue((out == logits).all())\n+            else:\n+                j = i if i == 0 else i - 1\n+                check_eos_logits(out=out, logits=logits, batch=0, channel=j, eos=eos)\n+                self.assertTrue((out[1] == logits[1]).all())\n+                self.assertTrue(delay_pattern_processor.active_batches[0])\n+                self.assertFalse(delay_pattern_processor.active_batches[1])\n+                self.assertTrue(\n+                    (\n+                        delay_pattern_processor.delay_pattern[0]\n+                        == torch.tensor([delay - (i + 1) for delay in delay_pattern])\n+                    ).all()\n+                )\n+                self.assertTrue((delay_pattern_processor.delay_pattern[1] == torch.tensor(delay_pattern)).all())\n+\n+        # Make second batch end\n+        logits[1, 0, eos] = 1\n+\n+        # Just to check if other batches could work\n+        out = delay_pattern_processor(input_ids, logits.clone()).view(bsz, channels, vocab)\n+\n+        self.assertTrue((out[0] == logits[0]).all())\n+        self.assertTrue(delay_pattern_processor.active_batches.all())\n+        self.assertTrue(\n+            (delay_pattern_processor.delay_pattern[0] == torch.tensor([delay - 5 for delay in delay_pattern])).all()\n+        )\n+        self.assertTrue(\n+            (delay_pattern_processor.delay_pattern[1] == torch.tensor([delay - 1 for delay in delay_pattern])).all()\n+        )\n+\n+        # Last check on max generation length reached (with delay in mind until last channel produces eos)\n+        input_ids = torch.LongTensor([[0] * (max_generation_len - max(delay_pattern) - 1)])\n+        delay_pattern_processor = DiaEOSDelayPatternLogitsProcessor(\n+            delay_pattern=delay_pattern, eos_token_id=eos, max_generation_len=max_generation_len\n+        )\n+        out = delay_pattern_processor(input_ids, logits.clone()).view(bsz, channels, vocab)\n+\n+        check_eos_logits(out=out, logits=logits, batch=0, channel=0, eos=eos)\n+        check_eos_logits(out=out, logits=logits, batch=1, channel=0, eos=eos)\n+        self.assertTrue(delay_pattern_processor.active_batches.all())\n+        self.assertTrue((delay_pattern_processor.delay_pattern == torch.tensor(delay_pattern) - 1).all())"
        },
        {
            "sha": "60500001a3b633d461331f05e6a241c8c23556fe",
            "filename": "tests/models/auto/test_processor_auto.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -26,6 +26,7 @@\n from transformers import (\n     CONFIG_MAPPING,\n     FEATURE_EXTRACTOR_MAPPING,\n+    MODEL_FOR_AUDIO_TOKENIZATION_MAPPING,\n     PROCESSOR_MAPPING,\n     TOKENIZER_MAPPING,\n     AutoConfig,\n@@ -265,6 +266,8 @@ def test_new_processor_registration(self):\n                 del TOKENIZER_MAPPING._extra_content[CustomConfig]\n             if CustomConfig in PROCESSOR_MAPPING._extra_content:\n                 del PROCESSOR_MAPPING._extra_content[CustomConfig]\n+            if CustomConfig in MODEL_FOR_AUDIO_TOKENIZATION_MAPPING._extra_content:\n+                del MODEL_FOR_AUDIO_TOKENIZATION_MAPPING._extra_content[CustomConfig]\n \n     def test_from_pretrained_dynamic_processor_conflict(self):\n         class NewFeatureExtractor(Wav2Vec2FeatureExtractor):\n@@ -317,6 +320,8 @@ class NewProcessor(ProcessorMixin):\n                 del TOKENIZER_MAPPING._extra_content[CustomConfig]\n             if CustomConfig in PROCESSOR_MAPPING._extra_content:\n                 del PROCESSOR_MAPPING._extra_content[CustomConfig]\n+            if CustomConfig in MODEL_FOR_AUDIO_TOKENIZATION_MAPPING._extra_content:\n+                del MODEL_FOR_AUDIO_TOKENIZATION_MAPPING._extra_content[CustomConfig]\n \n     def test_from_pretrained_dynamic_processor_with_extra_attributes(self):\n         class NewFeatureExtractor(Wav2Vec2FeatureExtractor):\n@@ -356,6 +361,8 @@ def __init__(self, feature_extractor, tokenizer, processor_attr_1=1, processor_a\n                 del TOKENIZER_MAPPING._extra_content[CustomConfig]\n             if CustomConfig in PROCESSOR_MAPPING._extra_content:\n                 del PROCESSOR_MAPPING._extra_content[CustomConfig]\n+            if CustomConfig in MODEL_FOR_AUDIO_TOKENIZATION_MAPPING._extra_content:\n+                del MODEL_FOR_AUDIO_TOKENIZATION_MAPPING._extra_content[CustomConfig]\n \n     def test_dynamic_processor_with_specific_dynamic_subcomponents(self):\n         class NewFeatureExtractor(Wav2Vec2FeatureExtractor):\n@@ -390,6 +397,8 @@ def __init__(self, feature_extractor, tokenizer):\n                 del TOKENIZER_MAPPING._extra_content[CustomConfig]\n             if CustomConfig in PROCESSOR_MAPPING._extra_content:\n                 del PROCESSOR_MAPPING._extra_content[CustomConfig]\n+            if CustomConfig in MODEL_FOR_AUDIO_TOKENIZATION_MAPPING._extra_content:\n+                del MODEL_FOR_AUDIO_TOKENIZATION_MAPPING._extra_content[CustomConfig]\n \n     def test_auto_processor_creates_tokenizer(self):\n         processor = AutoProcessor.from_pretrained(\"hf-internal-testing/tiny-random-bert\")"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/dia/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/tests%2Fmodels%2Fdia%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/tests%2Fmodels%2Fdia%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdia%2F__init__.py?ref=583db52bc6d5415a205724776136d094ff70c9a4"
        },
        {
            "sha": "6243dc479191e1ba3beeb1e153fe6c7e699ec227",
            "filename": "tests/models/dia/test_feature_extraction_dia.py",
            "status": "added",
            "additions": 231,
            "deletions": 0,
            "changes": 231,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/tests%2Fmodels%2Fdia%2Ftest_feature_extraction_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/tests%2Fmodels%2Fdia%2Ftest_feature_extraction_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdia%2Ftest_feature_extraction_dia.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -0,0 +1,231 @@\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Tests for the Dia feature extractor.\"\"\"\n+\n+import itertools\n+import random\n+import unittest\n+\n+import numpy as np\n+\n+from transformers import DiaFeatureExtractor\n+from transformers.testing_utils import require_torch\n+from transformers.utils.import_utils import is_torch_available\n+\n+from ...test_sequence_feature_extraction_common import SequenceFeatureExtractionTestMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+global_rng = random.Random()\n+\n+\n+# Copied from tests.models.whisper.test_feature_extraction_whisper.floats_list\n+def floats_list(shape, scale=1.0, rng=None, name=None):\n+    \"\"\"Creates a random float32 tensor\"\"\"\n+    if rng is None:\n+        rng = global_rng\n+\n+    values = []\n+    for batch_idx in range(shape[0]):\n+        values.append([])\n+        for _ in range(shape[1]):\n+            values[-1].append(rng.random() * scale)\n+\n+    return values\n+\n+\n+@require_torch\n+class DiaFeatureExtractionTester:\n+    # Copied from tests.models.dac.test_feature_extraction_dac.DacFeatureExtractionTester.__init__\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        min_seq_length=400,\n+        max_seq_length=2000,\n+        feature_size=1,\n+        padding_value=0.0,\n+        sampling_rate=16000,\n+        hop_length=512,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.min_seq_length = min_seq_length\n+        self.max_seq_length = max_seq_length\n+        self.hop_length = hop_length\n+        self.seq_length_diff = (self.max_seq_length - self.min_seq_length) // (self.batch_size - 1)\n+        self.feature_size = feature_size\n+        self.padding_value = padding_value\n+        self.sampling_rate = sampling_rate\n+\n+    # Copied from tests.models.dac.test_feature_extraction_dac.DacFeatureExtractionTester.prepare_feat_extract_dict\n+    def prepare_feat_extract_dict(self):\n+        return {\n+            \"feature_size\": self.feature_size,\n+            \"padding_value\": self.padding_value,\n+            \"sampling_rate\": self.sampling_rate,\n+            \"hop_length\": self.hop_length,\n+        }\n+\n+    # Copied from tests.models.encodec.test_feature_extraction_encodec.EnCodecFeatureExtractionTester.prepare_inputs_for_common\n+    def prepare_inputs_for_common(self, equal_length=False, numpify=False):\n+        def _flatten(list_of_lists):\n+            return list(itertools.chain(*list_of_lists))\n+\n+        if equal_length:\n+            audio_inputs = floats_list((self.batch_size, self.max_seq_length))\n+        else:\n+            # make sure that inputs increase in size\n+            audio_inputs = [\n+                _flatten(floats_list((x, self.feature_size)))\n+                for x in range(self.min_seq_length, self.max_seq_length, self.seq_length_diff)\n+            ]\n+\n+        if numpify:\n+            audio_inputs = [np.asarray(x) for x in audio_inputs]\n+\n+        return audio_inputs\n+\n+\n+@require_torch\n+class DiaFeatureExtractionTest(SequenceFeatureExtractionTestMixin, unittest.TestCase):\n+    feature_extraction_class = DiaFeatureExtractor\n+\n+    def setUp(self):\n+        self.feat_extract_tester = DiaFeatureExtractionTester(self)\n+\n+    # Copied from tests.models.dac.test_feature_extraction_dac.DacFeatureExtractionTest.test_call\n+    def test_call(self):\n+        # Tests that all call wrap to encode_plus and batch_encode_plus\n+        feat_extract = self.feature_extraction_class(**self.feat_extract_tester.prepare_feat_extract_dict())\n+        # create three inputs of length 800, 1000, and 1200\n+        audio_inputs = [floats_list((1, x))[0] for x in range(800, 1400, 200)]\n+        np_audio_inputs = [np.asarray(audio_input) for audio_input in audio_inputs]\n+\n+        # Test not batched input\n+        encoded_sequences_1 = feat_extract(audio_inputs[0], return_tensors=\"np\").input_values\n+        encoded_sequences_2 = feat_extract(np_audio_inputs[0], return_tensors=\"np\").input_values\n+        self.assertTrue(np.allclose(encoded_sequences_1, encoded_sequences_2, atol=1e-3))\n+\n+        # Test batched\n+        encoded_sequences_1 = feat_extract(audio_inputs, padding=True, return_tensors=\"np\").input_values\n+        encoded_sequences_2 = feat_extract(np_audio_inputs, padding=True, return_tensors=\"np\").input_values\n+        for enc_seq_1, enc_seq_2 in zip(encoded_sequences_1, encoded_sequences_2):\n+            self.assertTrue(np.allclose(enc_seq_1, enc_seq_2, atol=1e-3))\n+\n+    # Copied from tests.models.dac.test_feature_extraction_dac.DacFeatureExtractionTest.test_double_precision_pad\n+    def test_double_precision_pad(self):\n+        feature_extractor = self.feature_extraction_class(**self.feat_extract_tester.prepare_feat_extract_dict())\n+        np_audio_inputs = np.random.rand(100).astype(np.float64)\n+        py_audio_inputs = np_audio_inputs.tolist()\n+\n+        for inputs in [py_audio_inputs, np_audio_inputs]:\n+            np_processed = feature_extractor.pad([{\"input_values\": inputs}], return_tensors=\"np\")\n+            self.assertTrue(np_processed.input_values.dtype == np.float32)\n+            pt_processed = feature_extractor.pad([{\"input_values\": inputs}], return_tensors=\"pt\")\n+            self.assertTrue(pt_processed.input_values.dtype == torch.float32)\n+\n+    # Copied from tests.models.dac.test_feature_extraction_dac.DacFeatureExtractionTest._load_datasamples\n+    def _load_datasamples(self, num_samples):\n+        from datasets import load_dataset\n+\n+        ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+        # automatic decoding with librispeech\n+        audio_samples = ds.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n+\n+        return [x[\"array\"] for x in audio_samples]\n+\n+    # Copied from tests.models.dac.test_feature_extraction_dac.DacFeatureExtractionTest.test_integration with Dac->Dia\n+    def test_integration(self):\n+        # fmt: off\n+        EXPECTED_INPUT_VALUES = torch.tensor(\n+            [ 2.3803711e-03,  2.0751953e-03,  1.9836426e-03,  2.1057129e-03,\n+            1.6174316e-03,  3.0517578e-04,  9.1552734e-05,  3.3569336e-04,\n+            9.7656250e-04,  1.8310547e-03,  2.0141602e-03,  2.1057129e-03,\n+            1.7395020e-03,  4.5776367e-04, -3.9672852e-04,  4.5776367e-04,\n+            1.0070801e-03,  9.1552734e-05,  4.8828125e-04,  1.1596680e-03,\n+            7.3242188e-04,  9.4604492e-04,  1.8005371e-03,  1.8310547e-03,\n+            8.8500977e-04,  4.2724609e-04,  4.8828125e-04,  7.3242188e-04,\n+            1.0986328e-03,  2.1057129e-03]\n+        )\n+        # fmt: on\n+        input_audio = self._load_datasamples(1)\n+        feature_extractor = DiaFeatureExtractor()\n+        input_values = feature_extractor(input_audio, return_tensors=\"pt\")[\"input_values\"]\n+        self.assertEqual(input_values.shape, (1, 1, 93696))\n+        torch.testing.assert_close(input_values[0, 0, :30], EXPECTED_INPUT_VALUES, rtol=1e-4, atol=1e-4)\n+        audio_input_end = torch.tensor(input_audio[0][-30:], dtype=torch.float32)\n+        torch.testing.assert_close(input_values[0, 0, -46:-16], audio_input_end, rtol=1e-4, atol=1e-4)\n+\n+    def test_integration_stereo(self):\n+        # fmt: off\n+        EXPECTED_INPUT_VALUES = torch.tensor(\n+            [2.3804e-03, 2.0752e-03, 1.9836e-03, 2.1057e-03, 1.6174e-03,\n+             3.0518e-04, 9.1553e-05, 3.3569e-04, 9.7656e-04, 1.8311e-03,\n+             2.0142e-03, 2.1057e-03, 1.7395e-03, 4.5776e-04, -3.9673e-04,\n+             4.5776e-04, 1.0071e-03, 9.1553e-05, 4.8828e-04, 1.1597e-03,\n+             7.3242e-04, 9.4604e-04, 1.8005e-03, 1.8311e-03, 8.8501e-04,\n+             4.2725e-04, 4.8828e-04, 7.3242e-04, 1.0986e-03, 2.1057e-03]\n+        )\n+        # fmt: on\n+        input_audio = self._load_datasamples(1)\n+        input_audio = [np.tile(input_audio[0][None], reps=(2, 1))]\n+        feature_extractor = DiaFeatureExtractor(feature_size=2)\n+        input_values = feature_extractor(input_audio, return_tensors=\"pt\").input_values\n+        self.assertEqual(input_values.shape, (1, 1, 93696))\n+        torch.testing.assert_close(input_values[0, 0, :30], EXPECTED_INPUT_VALUES, rtol=1e-4, atol=1e-4)\n+\n+    # Copied from tests.models.dac.test_feature_extraction_dac.DacFeatureExtractionTest.test_truncation_and_padding with Dac->Dia\n+    def test_truncation_and_padding(self):\n+        input_audio = self._load_datasamples(2)\n+        # would be easier if the stride was like\n+        feature_extractor = DiaFeatureExtractor()\n+\n+        # pad and trunc raise an error ?\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"^Both padding and truncation were set. Make sure you only set one.$\",\n+        ):\n+            truncated_outputs = feature_extractor(\n+                input_audio, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n+            ).input_values\n+\n+        # force truncate to max_length\n+        truncated_outputs = feature_extractor(\n+            input_audio, truncation=True, max_length=48000, return_tensors=\"pt\"\n+        ).input_values\n+        self.assertEqual(truncated_outputs.shape, (2, 1, 48128))\n+\n+        # pad:\n+        padded_outputs = feature_extractor(input_audio, padding=True, return_tensors=\"pt\").input_values\n+        self.assertEqual(padded_outputs.shape, (2, 1, 93696))\n+\n+        # force pad to max length\n+        truncated_outputs = feature_extractor(\n+            input_audio, padding=\"max_length\", max_length=100000, return_tensors=\"pt\"\n+        ).input_values\n+        self.assertEqual(truncated_outputs.shape, (2, 1, 100352))\n+\n+        # force no pad\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"^Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.$\",\n+        ):\n+            truncated_outputs = feature_extractor(input_audio, padding=False, return_tensors=\"pt\").input_values\n+\n+        truncated_outputs = feature_extractor(input_audio[0], padding=False, return_tensors=\"pt\").input_values\n+        self.assertEqual(truncated_outputs.shape, (1, 1, 93680))"
        },
        {
            "sha": "f9427160c254f8da875a28f890e1f6cc464f2e5e",
            "filename": "tests/models/dia/test_modeling_dia.py",
            "status": "added",
            "additions": 752,
            "deletions": 0,
            "changes": 752,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -0,0 +1,752 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Dia model.\"\"\"\n+\n+import copy\n+import pathlib\n+import tempfile\n+import unittest\n+\n+import pytest\n+\n+from transformers.models.dia import DiaConfig, DiaDecoderConfig, DiaEncoderConfig\n+from transformers.testing_utils import (\n+    cleanup,\n+    is_flaky,\n+    require_torch,\n+    require_torch_accelerator,\n+    require_torch_sdpa,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import is_soundfile_available, is_torch_available, is_torchaudio_available\n+from transformers.utils.import_utils import is_datasets_available\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_datasets_available():\n+    from datasets import Audio, load_dataset\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        DiaForConditionalGeneration,\n+        DiaModel,\n+        DiaProcessor,\n+        PretrainedConfig,\n+        PreTrainedModel,\n+    )\n+    from transformers.cache_utils import (\n+        Cache,\n+        StaticCache,\n+    )\n+    from transformers.models.dia.modeling_dia import DiaDecoder, DiaEncoder\n+\n+if is_torchaudio_available():\n+    import torchaudio\n+\n+if is_soundfile_available():\n+    import soundfile as sf\n+\n+\n+@require_torch\n+class DiaModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=3,  # need batch_size != num_hidden_layers\n+        seq_length=7,\n+        max_length=50,\n+        is_training=True,\n+        vocab_size=100,\n+        hidden_size=16,\n+        intermediate_size=37,\n+        num_hidden_layers=2,\n+        num_attention_heads=2,\n+        head_dim=8,\n+        decoder_hidden_size=32,  # typically larger than encoder\n+        hidden_act=\"silu\",\n+        eos_token_id=97,  # special tokens all occur after eos\n+        pad_token_id=98,\n+        bos_token_id=99,\n+        delay_pattern=None,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.seq_length = seq_length\n+        self.max_length = max_length\n+        self.is_training = is_training\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.head_dim = head_dim\n+        self.decoder_hidden_size = decoder_hidden_size\n+        self.hidden_act = hidden_act\n+        self.eos_token_id = eos_token_id\n+        self.pad_token_id = pad_token_id\n+        self.bos_token_id = bos_token_id\n+        # Set default delay pattern if not provided\n+        self.delay_pattern = delay_pattern if delay_pattern is not None else [0, 1, 2]\n+        self.num_channels = len(self.delay_pattern)\n+\n+    def get_config(self):\n+        encoder_config = DiaEncoderConfig(\n+            max_position_embeddings=self.max_length,\n+            num_hidden_layers=self.num_hidden_layers,\n+            hidden_size=self.hidden_size,\n+            num_attention_heads=self.num_attention_heads,\n+            num_key_value_heads=self.num_attention_heads,  # same as num_attention_heads for testing\n+            head_dim=self.head_dim,\n+            intermediate_size=self.intermediate_size,\n+            vocab_size=self.vocab_size,\n+            hidden_act=self.hidden_act,\n+        )\n+\n+        decoder_config = DiaDecoderConfig(\n+            max_position_embeddings=self.max_length,\n+            num_hidden_layers=self.num_hidden_layers,\n+            hidden_size=self.decoder_hidden_size,\n+            intermediate_size=self.intermediate_size,\n+            num_attention_heads=self.num_attention_heads,\n+            num_key_value_heads=1,  # GQA\n+            head_dim=self.head_dim,\n+            cross_num_attention_heads=self.num_attention_heads,\n+            cross_head_dim=self.head_dim,\n+            cross_num_key_value_heads=1,  # GQA\n+            cross_hidden_size=self.hidden_size,  # match encoder hidden size\n+            vocab_size=self.vocab_size,\n+            hidden_act=self.hidden_act,\n+            num_channels=self.num_channels,\n+        )\n+\n+        config = DiaConfig(\n+            encoder_config=encoder_config,\n+            decoder_config=decoder_config,\n+            eos_token_id=self.eos_token_id,\n+            pad_token_id=self.pad_token_id,\n+            bos_token_id=self.bos_token_id,\n+            delay_pattern=self.delay_pattern,\n+        )\n+\n+        return config\n+\n+    def prepare_config_and_inputs(self) -> tuple[DiaConfig, dict]:\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+        attention_mask = input_ids.ne(self.pad_token_id)\n+\n+        decoder_input_ids = ids_tensor([self.batch_size, self.seq_length, self.num_channels], self.vocab_size)\n+        decoder_attention_mask = decoder_input_ids[..., 0].ne(self.pad_token_id)\n+\n+        config = self.get_config()\n+        inputs_dict = {\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+            \"decoder_input_ids\": decoder_input_ids,\n+            \"decoder_attention_mask\": decoder_attention_mask,\n+        }\n+        return config, inputs_dict\n+\n+    def prepare_config_and_inputs_for_common(self) -> tuple[DiaConfig, dict]:\n+        config, inputs_dict = self.prepare_config_and_inputs()\n+        return config, inputs_dict\n+\n+    def create_and_check_model_forward(self, config, inputs_dict):\n+        model = DiaModel(config=config).to(torch_device).eval()\n+\n+        input_ids = inputs_dict[\"input_ids\"]\n+        decoder_input_ids = inputs_dict[\"decoder_input_ids\"]\n+\n+        # first forward pass\n+        last_hidden_state = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids).last_hidden_state\n+\n+        self.parent.assertTrue(\n+            last_hidden_state.shape, (self.batch_size, self.seq_length, config.decoder_config.hidden_size)\n+        )\n+\n+    def check_encoder_decoder_model_standalone(self, config, inputs_dict):\n+        model = DiaModel(config=config).to(torch_device).eval()\n+        outputs = model(**inputs_dict)\n+\n+        encoder_last_hidden_state = outputs.encoder_last_hidden_state\n+        last_hidden_state = outputs.last_hidden_state\n+\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            encoder = model.get_encoder()\n+            encoder.save_pretrained(tmpdirname)\n+            encoder = DiaEncoder.from_pretrained(tmpdirname).to(torch_device)\n+\n+        encoder_last_hidden_state_2 = encoder(\n+            input_ids=inputs_dict[\"input_ids\"], attention_mask=inputs_dict[\"attention_mask\"]\n+        )[0]\n+\n+        self.parent.assertTrue((encoder_last_hidden_state_2 - encoder_last_hidden_state).abs().max().item() < 3e-3)\n+\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            decoder = model.get_decoder()\n+            decoder.save_pretrained(tmpdirname)\n+            decoder = DiaDecoder.from_pretrained(tmpdirname).to(torch_device)\n+\n+        last_hidden_state_2 = decoder(\n+            input_ids=inputs_dict[\"decoder_input_ids\"],\n+            attention_mask=inputs_dict[\"decoder_attention_mask\"],\n+            encoder_hidden_states=encoder_last_hidden_state,\n+        )[0]\n+\n+        self.parent.assertTrue((last_hidden_state_2 - last_hidden_state).abs().max().item() < 3e-3)\n+\n+\n+@require_torch\n+class DiaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (DiaModel, DiaForConditionalGeneration) if is_torch_available() else ()\n+    # We only allow greedy search / sampling with one sequence; see `skip_non_greedy_generate`\n+    all_generative_model_classes = (DiaForConditionalGeneration,)\n+    # TODO: support new pipeline behavior in tests\n+    pipeline_model_mapping = {}\n+    # pipeline_model_mapping = {\"text-to-audio\": DiaForConditionalGeneration} if is_torch_available() else {}\n+    test_pruning = False\n+    test_head_masking = False\n+    test_resize_embeddings = False\n+    is_encoder_decoder = True\n+    # Indicates VLMs usually but there are many audio models which are also composite\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = DiaModelTester(self)\n+        # Skipping `has_text_modality` but manually testing down below\n+        self.config_tester = ConfigTester(self, has_text_modality=False, config_class=DiaConfig)\n+        self.skip_non_greedy_generate()\n+\n+    def skip_non_greedy_generate(self):\n+        skippable_tests = [\n+            \"test_sample_generate_dict_output\",  # return sequences > 1\n+            \"test_beam\",\n+            \"test_group_beam\",\n+            \"test_constrained_beam\",\n+            \"test_contrastive\",\n+            \"test_assisted\",\n+            \"test_dola\",\n+            \"test_prompt_lookup\",\n+            \"test_model_parallel_beam_search\",\n+            \"test_generate_without_input_ids\",\n+            \"test_generate_with_head_masking\",\n+        ]\n+\n+        for test in skippable_tests:\n+            if self._testMethodName.startswith(test):\n+                self.skipTest(reason=\"Dia only supports greedy search / sampling with one sequence.\")\n+\n+    def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n+        \"\"\"Overriden to account for the 2D flattened structure\"\"\"\n+        inputs_dict = copy.deepcopy(inputs_dict)\n+\n+        if return_labels:\n+            inputs_dict[\"labels\"] = torch.ones(\n+                (\n+                    self.model_tester.batch_size * self.model_tester.num_channels,\n+                    self.model_tester.seq_length,\n+                ),\n+                dtype=torch.long,\n+                device=torch_device,\n+            )\n+\n+        return inputs_dict\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+        # Manual testing because of composite configs\n+        config = self.model_tester.prepare_config_and_inputs()[0]\n+        self.assertTrue(hasattr(config.encoder_config, \"vocab_size\"), msg=\"Encoder `vocab_size` does not exist\")\n+        self.assertTrue(hasattr(config.decoder_config, \"vocab_size\"), msg=\"Decoder `vocab_size` does not exist\")\n+\n+    def test_model_forward(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model_forward(*config_and_inputs)\n+\n+    @is_flaky\n+    def test_encoder_decoder_model_standalone(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+        self.model_tester.check_encoder_decoder_model_standalone(*config_and_inputs)\n+\n+    # Overriding shape checks as Dia has different shapes on encoder/decoder using a composite config\n+    # + additional special cases where 3D x 2D meshes confuse the expected shape\n+    def _check_logits(self, batch_size, logits, config):\n+        batch_size *= len(config.delay_pattern)  # Account for flattening\n+        vocab_size = config.decoder_config.vocab_size\n+        self.assertIsInstance(logits, tuple)\n+        self.assertListEqual([iter_logits.shape[0] for iter_logits in logits], [batch_size] * len(logits))\n+        # vocabulary difference equal to one (imagegptmodel?) or zero (all other models)\n+        vocab_diff = vocab_size - logits[0].shape[-1]\n+        self.assertTrue(vocab_diff in [0, 1])\n+        self.assertListEqual([vocab_size - score.shape[-1] for score in logits], [vocab_diff] * len(logits))\n+\n+    def _check_attentions_for_generate(\n+        self, batch_size, attentions, prompt_length, output_length, config, decoder_past_key_values\n+    ):\n+        self.assertIsInstance(attentions, tuple)\n+        self.assertListEqual(\n+            [isinstance(iter_attentions, tuple) for iter_attentions in attentions], [True] * len(attentions)\n+        )\n+        self.assertEqual(len(attentions), (output_length - prompt_length))\n+\n+        use_cache = decoder_past_key_values is not None\n+        has_static_cache = isinstance(decoder_past_key_values, StaticCache)\n+\n+        # When `output_attentions=True`, each iteration of generate appends the attentions corresponding to the new\n+        # token(s)\n+        for generated_length, iter_attentions in enumerate(attentions):\n+            # regardless of using cache, the first forward pass will have the full prompt as input\n+            if use_cache and generated_length > 0:\n+                model_input_length = 1\n+            else:\n+                model_input_length = prompt_length + generated_length\n+            query_length = (\n+                prompt_length + generated_length\n+                if not has_static_cache\n+                else decoder_past_key_values.get_max_cache_shape()\n+            )\n+\n+            expected_shape = (\n+                batch_size,\n+                config.decoder_config.num_attention_heads,  # Decoder config\n+                model_input_length,\n+                query_length,\n+            )\n+            # check attn size\n+            self.assertListEqual(\n+                [layer_attention.shape for layer_attention in iter_attentions], [expected_shape] * len(iter_attentions)\n+            )\n+\n+    def _check_encoder_attention_for_generate(self, attentions, batch_size, config, prompt_length):\n+        # Encoder config\n+        encoder_expected_shape = (batch_size, config.encoder_config.num_attention_heads, prompt_length, prompt_length)\n+        self.assertIsInstance(attentions, tuple)\n+        self.assertListEqual(\n+            [layer_attentions.shape for layer_attentions in attentions],\n+            [encoder_expected_shape] * len(attentions),\n+        )\n+\n+    def _check_hidden_states_for_generate(\n+        self, batch_size, hidden_states, prompt_length, output_length, config, use_cache=False\n+    ):\n+        self.assertIsInstance(hidden_states, tuple)\n+        self.assertListEqual(\n+            [isinstance(iter_hidden_states, tuple) for iter_hidden_states in hidden_states],\n+            [True] * len(hidden_states),\n+        )\n+        self.assertEqual(len(hidden_states), (output_length - prompt_length))\n+\n+        # When `output_hidden_states=True`, each iteration of generate appends the hidden states corresponding to the\n+        # new token(s)\n+        for generated_length, iter_hidden_states in enumerate(hidden_states):\n+            # regardless of using cache, the first forward pass will have the full prompt as input\n+            if use_cache and generated_length > 0:\n+                model_input_length = 1\n+            else:\n+                model_input_length = prompt_length + generated_length\n+\n+            # check hidden size\n+            # we can have different hidden sizes between encoder and decoder --> check both\n+            expected_shape_encoder = (batch_size, model_input_length, config.encoder_config.hidden_size)\n+            expected_shape_decoder = (batch_size, model_input_length, config.decoder_config.hidden_size)\n+            self.assertTrue(\n+                [layer_hidden_states.shape for layer_hidden_states in iter_hidden_states]\n+                == [expected_shape_encoder] * len(iter_hidden_states)\n+                or [layer_hidden_states.shape for layer_hidden_states in iter_hidden_states]\n+                == [expected_shape_decoder] * len(iter_hidden_states)\n+            )\n+\n+    def _check_encoder_hidden_states_for_generate(self, hidden_states, batch_size, config, prompt_length):\n+        # Encoder config\n+        encoder_expected_shape = (batch_size, prompt_length, config.encoder_config.hidden_size)\n+        self.assertIsInstance(hidden_states, tuple)\n+        self.assertListEqual(\n+            [layer_hidden_states.shape for layer_hidden_states in hidden_states],\n+            [encoder_expected_shape] * len(hidden_states),\n+        )\n+\n+    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n+        self.assertIsInstance(decoder_past_key_values, (tuple, Cache))\n+\n+        # we need the decoder config here\n+        config = config.decoder_config\n+\n+        # (batch, head, seq_length, head_features)\n+        expected_shape = (\n+            batch_size,\n+            config.num_key_value_heads if hasattr(config, \"num_key_value_heads\") else config.num_attention_heads,\n+            cache_length,\n+            config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads,\n+        )\n+\n+        if isinstance(decoder_past_key_values, Cache):\n+            self.assertListEqual(\n+                [key_tensor.shape for key_tensor in decoder_past_key_values.key_cache],\n+                [expected_shape] * len(decoder_past_key_values.key_cache),\n+            )\n+            self.assertListEqual(\n+                [value_tensor.shape for value_tensor in decoder_past_key_values.value_cache],\n+                [expected_shape] * len(decoder_past_key_values.value_cache),\n+            )\n+\n+    def _check_scores(self, batch_size, scores, generated_length, config):\n+        # Special case where Dia keeps score in a 2D mesh of (bsz * channels, vocab)\n+        vocab_size = config.decoder_config.vocab_size\n+        expected_shape = (batch_size * len(config.delay_pattern), vocab_size)\n+        self.assertIsInstance(scores, tuple)\n+        self.assertEqual(len(scores), generated_length)\n+        self.assertListEqual([iter_scores.shape for iter_scores in scores], [expected_shape] * len(scores))\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        \"\"\"\n+        Overwritten as it relies on hardcoded namings atm - checking for our case here specifically\n+        \"\"\"\n+        for model_class in self.all_model_classes:\n+            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model = model_class.from_pretrained(tmpdirname)\n+\n+                sub_models_supporting_sdpa = [\n+                    (module._supports_sdpa or module._supports_attention_backend)\n+                    for name, module in model.named_modules()\n+                    if isinstance(module, PreTrainedModel) and name != \"\"\n+                ]\n+                supports_sdpa_all_modules = (\n+                    all(sub_models_supporting_sdpa)\n+                    if len(sub_models_supporting_sdpa) > 0\n+                    else (model._supports_sdpa or model._supports_attention_backend)\n+                )\n+\n+                if not supports_sdpa_all_modules:\n+                    with self.assertRaises(ValueError):\n+                        model_sdpa = model_class.from_pretrained(tmpdirname, attn_implementation=\"sdpa\")\n+                else:\n+                    model_sdpa = model_class.from_pretrained(tmpdirname, attn_implementation=\"sdpa\")\n+                    for key in model_sdpa.config:\n+                        if isinstance(getattr(model_sdpa.config, key), PretrainedConfig):\n+                            sub_config = getattr(model_sdpa.config, key)\n+                            self.assertTrue(sub_config._attn_implementation == \"sdpa\")\n+\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"Custom processor `DiaEOSDelayPatternLogitsProcessor` forces eos token.\")\n+    def test_generate_continue_from_past_key_values(self):\n+        \"\"\"Only a small change due to the expected shapes\"\"\"\n+        # Tests that we can continue generating from past key values, returned from a previous `generate` call\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            # Let's make it always:\n+            # 1. use cache (for obvious reasons)\n+            # 2. generate to max length (which can be achieved by setting the eos token to an invalid value), which\n+            #    would make the test flaky (e.g. EOS is generated on iteration 1 on both generations, but the\n+            #    continuation would force it to generate beyond an EOS token)\n+            # 3. ignore `token_type_ids` for simplicity\n+            # 4. ignore `forced_eos_token_id`, which requires further manipulation of the continuation inputs and is\n+            #    active by default on some models\n+            # 5. ignore `encoder_no_repeat_ngram_size`, which is set by default in some encoder-decoder models. When\n+            #    we use their decoder as a stand-alone model, `encoder_no_repeat_ngram_size` actually prevents\n+            #    repetition exclusively from the prompt. This test relies on comparing one call vs 2 calls\n+            #    with cache, what is considered a prompt is different in the two cases.\n+\n+            if \"token_type_ids\" in inputs:\n+                del inputs[\"token_type_ids\"]\n+\n+            model = model_class(config).to(torch_device)\n+            model.eval()\n+\n+            generate_kwargs = {\n+                \"pad_token_id\": -1,\n+                \"eos_token_id\": -1,\n+                \"forced_eos_token_id\": None,\n+                \"encoder_no_repeat_ngram_size\": 0,\n+                \"use_cache\": True,\n+                \"do_sample\": False,\n+                \"return_dict_in_generate\": True,\n+                \"output_scores\": True,\n+            }\n+\n+            # Traditional way of generating text, with `return_dict_in_generate` to return the past key values\n+            outputs = model.generate(**inputs, **generate_kwargs, max_new_tokens=4)\n+\n+            # Let's generate again, but passing the past key values in between (3 + 1 = 4 tokens). Note that the\n+            # inputs may need to be tweaked across `generate` calls (like the attention mask).\n+            outputs_cached = model.generate(**inputs, **generate_kwargs, max_new_tokens=3)\n+\n+            # Continue from the tokens generated above, preparing the inputs accordingly\n+            inputs[\"past_key_values\"] = outputs_cached.past_key_values\n+            new_attention_len = outputs_cached.sequences.shape[1]  # the only real modification in this test\n+            inputs[\"decoder_input_ids\"] = outputs_cached.sequences\n+            if \"decoder_attention_mask\" in inputs:\n+                inputs[\"decoder_attention_mask\"] = torch.nn.functional.pad(\n+                    inputs[\"decoder_attention_mask\"],\n+                    (0, new_attention_len - inputs[\"decoder_attention_mask\"].shape[1]),\n+                    mode=\"constant\",\n+                    value=1,\n+                )\n+\n+            first_caches_scores = outputs_cached.scores\n+            outputs_cached = model.generate(**inputs, **generate_kwargs, max_new_tokens=1)\n+            full_cached_scores = first_caches_scores + outputs_cached.scores\n+            outputs_cached.scores = full_cached_scores\n+\n+            # The two sets of generated text and past kv should be equal to each other\n+            self._check_similar_generate_outputs(outputs, outputs_cached)\n+            for layer_idx in range(len(outputs_cached.past_key_values)):\n+                for kv_idx in range(len(outputs_cached.past_key_values[layer_idx])):\n+                    self.assertTrue(\n+                        torch.allclose(\n+                            outputs.past_key_values[layer_idx][kv_idx],\n+                            outputs_cached.past_key_values[layer_idx][kv_idx],\n+                        )\n+                    )\n+\n+    @unittest.skip(reason=\"Indirectly checked in Dia through the generate methods.\")\n+    def test_past_key_values_format(self, custom_all_cache_shapes=None):\n+        pass\n+\n+    @unittest.skip(reason=\"Indirectly checked in Dia through the generate methods.\")\n+    def test_hidden_states_output(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"Dia has too many mixed embedding types which would cause unintentional side effects, e.g. attempts at tying embeddings\"\n+    )\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Theoretically works but kernel library causes issues.\")\n+    def test_torchscript_output_hidden_state(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Theoretically works but kernel library causes issues.\")\n+    def test_torchscript_simple(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Encoder-Decoder cache can not be initialized.\")\n+    def test_multi_gpu_data_parallel_forward(self):\n+        pass\n+\n+\n+class DiaForConditionalGenerationIntegrationTest(unittest.TestCase):\n+    \"\"\"\n+    See https://gist.github.com/vasqu/0e3b06360373a4e612aa3b9a7c09185e for generating the integration tests\n+\n+    NOTE: We add a single `eos` line for the last channel which is skipped in the original Dia\n+    (It doesn't change the behaviour as we cut by the eos token position)\n+    \"\"\"\n+\n+    def setUp(self):\n+        # it's a dummy ckpt but should suffice for testing purposes\n+        self.model_checkpoint = \"AntonV/Dia-1.6B\"\n+        self.sampling_rate = 44100\n+\n+        # prepare audio\n+        librispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+        librispeech_dummy = librispeech_dummy.cast_column(\"audio\", Audio(sampling_rate=self.sampling_rate))\n+        audio_sample_1 = librispeech_dummy[-1][\"audio\"][\"array\"]\n+        audio_sample_2 = librispeech_dummy[-2][\"audio\"][\"array\"]\n+        # 10 and 5 codebooks as prefix - saved as files as we need wav files for the original Dia\n+        dac_chunk_len = 512\n+        self.audio_prompt_1_path = \"/tmp/dia_test_sample_1.mp3\"\n+        self.audio_prompt_2_path = \"/tmp/dia_test_sample_2.mp3\"\n+        sf.write(self.audio_prompt_1_path, audio_sample_1[: (dac_chunk_len * 10)], self.sampling_rate)\n+        sf.write(self.audio_prompt_2_path, audio_sample_2[: (dac_chunk_len * 5)], self.sampling_rate)\n+\n+    def tearDown(self):\n+        pathlib.Path(self.audio_prompt_1_path).unlink()\n+        pathlib.Path(self.audio_prompt_2_path).unlink()\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @slow\n+    @require_torch_accelerator\n+    def test_dia_model_integration_generate_tts(self):\n+        text = [\"[S1] Dia is an open weights text to dialogue model.\", \"This is a test\"]\n+        processor = DiaProcessor.from_pretrained(self.model_checkpoint)\n+        inputs = processor(text=text, padding=True, return_tensors=\"pt\").to(torch_device)\n+\n+        model = DiaForConditionalGeneration.from_pretrained(self.model_checkpoint).to(torch_device)\n+        outputs = model.generate(**inputs, max_new_tokens=32, do_sample=False)\n+\n+        # fmt: off\n+        EXPECTED_OUTPUT_TOKENS = torch.tensor([[[1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 568, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 568, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 568, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 568, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 568, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 568, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 568, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 568, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 568,  778, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 568,  778,  338, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 568,  804,   10,  524, 1026, 1026, 1026, 1026, 1026],\n+         [ 568,  804,   10,  674,  967, 1026, 1026, 1026, 1026],\n+         [ 568,  804,   10,  674,  364,  360, 1026, 1026, 1026],\n+         [ 568,  804,   10,  674,  364,  981,  728, 1026, 1026],\n+         [ 568,  804,   10,  674,  364,  981,  741,  550, 1026],\n+         [ 568,  804,   10,  674,  364,  981,  568,  378,   90],\n+         [1024,  804,   10,  674,  364,  981,  568,  378,  731],\n+         [1025,  804,   10,  674,  364,  981,  568,  378,  731],\n+         [1025,  804,   10,  674,  364,  981,  568,  378,  731],\n+         [1025,  804,   10,  674,  364,  981,  568,  378,  731],\n+         [1025,  804,   10,  674,  364,  981,  568,  378,  731],\n+         [1025,  804,   10,  674,  364,  981,  568,  378,  731],\n+         [1025,  804,   10,  674,  364,  981,  568,  378,  731],\n+         [1025,  804,   10,  674,  364,  981,  568,  378,  731],\n+         [1025, 1024,   10,  674,  364,  981,  568,  378,  731],\n+         [1025, 1025, 1024,  674,  364,  981,  568,  378,  731],\n+         [1025, 1025, 1025, 1024,  364,  981,  568,  378,  731],\n+         [1025, 1025, 1025, 1025, 1024,  981,  568,  378,  731],\n+         [1025, 1025, 1025, 1025, 1025, 1024,  568,  378,  731],\n+         [1025, 1025, 1025, 1025, 1025, 1025, 1024,  378,  731],\n+         [1025, 1025, 1025, 1025, 1025, 1025, 1025, 1024,  731],\n+         [1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1024]],\n+\n+        [[1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 568, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 568, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 698, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 592, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 592, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 592, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 592, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 592, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 592,  778, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 592,  778,  338, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 592,  697,   10,  524, 1026, 1026, 1026, 1026, 1026],\n+         [ 592,  288,  476,  649,  967, 1026, 1026, 1026, 1026],\n+         [ 592,  740,  386,  674,  364,  360, 1026, 1026, 1026],\n+         [ 592,  402,  386,  347,  362,  981,  728, 1026, 1026],\n+         [ 592,  402,  721,  728,  327,  981,  741,  550, 1026],\n+         [ 592,  402,  721,  728,  460,   62,  676,  378,   90],\n+         [1024,  402,  721,  728,  837,  595,  195,  982,  784],\n+         [1025,  402,  721,  677,  497,  102,  692,   24,  330],\n+         [1025,  402,  721,  677,  511,  102,  503,  871,  609],\n+         [1025,  402,  721,  677,  511,   96,  801,  871,  894],\n+         [1025,  402,  721,  677,  511,  745,  314,  498,  775],\n+         [1025,  402,  721,  677,  511,  745,  314,  498,  105],\n+         [1025,  402,  721,  677,  511,  745,  314,  861,  889],\n+         [1025,  893,  721,  677,  511,  744,  314,  871,  353],\n+         [1025, 1024,  888,  677,  511,  744,  314,  871,  332],\n+         [1025, 1025, 1024,  518,  511,  744,  314,  871,  366],\n+         [1025, 1025, 1025, 1024,  611,  744,  314,  871,  366],\n+         [1025, 1025, 1025, 1025, 1024,  980,  314,  871,  366],\n+         [1025, 1025, 1025, 1025, 1025, 1024,   45,  124,  366],\n+         [1025, 1025, 1025, 1025, 1025, 1025, 1024,  871,  366],\n+         [1025, 1025, 1025, 1025, 1025, 1025, 1025, 1024,  719],\n+         [1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1024]]])\n+        # fmt: on\n+\n+        torch.testing.assert_close(outputs.cpu(), EXPECTED_OUTPUT_TOKENS)\n+\n+    @slow\n+    @require_torch_accelerator\n+    def test_dia_model_integration_generate_audio_context(self):\n+        text = [\"[S1] Dia is an open weights text to dialogue model.\", \"This is a test\"]\n+        audio_sample_1 = torchaudio.load(self.audio_prompt_1_path, channels_first=True)[0].squeeze().numpy()\n+        audio_sample_2 = torchaudio.load(self.audio_prompt_2_path, channels_first=True)[0].squeeze().numpy()\n+        audio = [audio_sample_1, audio_sample_2]\n+\n+        processor = DiaProcessor.from_pretrained(self.model_checkpoint)\n+        inputs = processor(text=text, audio=audio, padding=True, return_tensors=\"pt\").to(torch_device)\n+\n+        model = DiaForConditionalGeneration.from_pretrained(self.model_checkpoint).to(torch_device)\n+        # dia has right padding while we have left padding (for faster prefill)\n+        # additionally we have new tokens vs dia's max tokens (hence we compare each in the respective settings)\n+        outputs_1 = model.generate(**inputs, max_new_tokens=22, do_sample=False)\n+        outputs_2 = model.generate(**inputs, max_new_tokens=27, do_sample=False)\n+\n+        # fmt: off\n+        EXPECTED_OUTPUT_TOKENS_1 = torch.tensor([[1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 578, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 592, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 494, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 330, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 330, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 330, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 330, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 330, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 330,  501, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 330,  204,   34, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 330,  254,  915,  863, 1026, 1026, 1026, 1026, 1026],\n+         [ 330,  215,  458,  313,   50, 1026, 1026, 1026, 1026],\n+         [ 330,  615,  529,  216,  801,  237, 1026, 1026, 1026],\n+         [ 330,  580,  563,  233,  337,   37, 1018, 1026, 1026],\n+         [ 330,  567,  530,  753,  607,  179,  954,  242, 1026],\n+         [ 330,  627,    6, 1010,  500,  189,  598,  858,  247],\n+         [1024,  432,  480,  530,  122,    3,  788,  149,  814],\n+         [1025,  875,  826,  458,   98,  540,  181,  122,  608],\n+         [1025,  495,  840,  413,  337,  784,  591,  150, 1017],\n+         [1025,  808,  189,  137,  445,    0,  227,  658,  345],\n+         [1025,  397,   89,  753, 1016,  173,  984,    0,  910],\n+         [1025,  875,  460,  934,   50,  335,  670,  818,  722],\n+         [1025,  875,  460,  762,  119,  372,  503,  858,  584],\n+         [1025,  348,  555,  475,  469,  458,  963,   41,  664],\n+         [1025, 1024,  852,  683,  761,  193,  595,  895,  885],\n+         [1025, 1025, 1024,  135,  761,  902,  163,  623,  385],\n+         [1025, 1025, 1025, 1024,  852,  282,  581,  623,   70],\n+         [1025, 1025, 1025, 1025, 1024,   41,  661,  790,  977],\n+         [1025, 1025, 1025, 1025, 1025, 1024,  580,  401,  464],\n+         [1025, 1025, 1025, 1025, 1025, 1025, 1024,  756,   61],\n+         [1025, 1025, 1025, 1025, 1025, 1025, 1025, 1024,  752],\n+         [1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1024]])\n+\n+        EXPECTED_OUTPUT_TOKENS_2 = torch.tensor([[1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 619, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 315, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 315, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 315, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 315, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 315, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 315, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 315, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 315,  968, 1026, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 315, 1007,  458, 1026, 1026, 1026, 1026, 1026, 1026],\n+         [ 315,   35,  266,   68, 1026, 1026, 1026, 1026, 1026],\n+         [ 315,  359,  285,  811,  154, 1026, 1026, 1026, 1026],\n+         [ 315,  906,  407,  297,  785,  649, 1026, 1026, 1026],\n+         [ 315,  249,  678,  868,  899,  257,  950, 1026, 1026],\n+         [ 315,  249,  217,  471,  292,  908,  196,  469, 1026],\n+         [ 315,  249,  825,  771,  839,  802,  633,  590,  531],\n+         [1024,  249,  150,   53,  126,   76,  794,  626,  442],\n+         [1025,  249,  825,  218,  359,  864,  526,  626,  770],\n+         [1025,  249,  150,  137,  530,  845,  877,  600,  111],\n+         [1025,  249,  150,  287,  730,  991,  135,  259,   39],\n+         [1025,  249,  825,  104,  198, 1020,  719,  625,  208],\n+         [1025,  249,  825,  997,  602,  256,  859,  322,  518],\n+         [1025,  668,  825,  979,  584,  256,   98,  665,  589],\n+         [1025,  954,  458,   54,  206,   52,  244,  822,  599],\n+         [1025, 1024,  104,  914,  435,  579,  860,   92,  661],\n+         [1025, 1025, 1024,  848,  126,   74,  304,   92,  753],\n+         [1025, 1025, 1025, 1024,  362,  376,  304,  586,  753],\n+         [1025, 1025, 1025, 1025, 1024,  633,  996,  586,   83],\n+         [1025, 1025, 1025, 1025, 1025, 1024,  179,  898,  928],\n+         [1025, 1025, 1025, 1025, 1025, 1025, 1024,  506,  102],\n+         [1025, 1025, 1025, 1025, 1025, 1025, 1025, 1024,   79],\n+         [1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1024]])\n+        # fmt: on\n+\n+        torch.testing.assert_close(outputs_1[0].cpu(), EXPECTED_OUTPUT_TOKENS_1)\n+        torch.testing.assert_close(outputs_2[1, 5:].cpu(), EXPECTED_OUTPUT_TOKENS_2)  # left padding"
        },
        {
            "sha": "8ce15f4330d5456f76650d472a544e286b917ba1",
            "filename": "tests/models/dia/test_processor_dia.py",
            "status": "added",
            "additions": 269,
            "deletions": 0,
            "changes": 269,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/tests%2Fmodels%2Fdia%2Ftest_processor_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/tests%2Fmodels%2Fdia%2Ftest_processor_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdia%2Ftest_processor_dia.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -0,0 +1,269 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import shutil\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+from parameterized import parameterized\n+\n+from transformers import DacModel, DiaFeatureExtractor, DiaProcessor, DiaTokenizer\n+from transformers.testing_utils import require_torch\n+from transformers.utils import is_torch_available\n+\n+\n+if is_torch_available:\n+    import torch\n+\n+\n+# Copied from tests.utils.test_modeling_utils.check_models_equal\n+def check_models_equal(model1, model2):\n+    models_are_equal = True\n+    for model1_p, model2_p in zip(model1.parameters(), model2.parameters()):\n+        if model1_p.data.ne(model2_p.data).sum() > 0:\n+            models_are_equal = False\n+\n+    return models_are_equal\n+\n+\n+@require_torch\n+class DiaProcessorTest(unittest.TestCase):\n+    def setUp(self):\n+        self.checkpoint = \"AntonV/Dia-1.6B\"\n+        self.audio_tokenizer_checkpoint = \"descript/dac_44khz\"\n+        self.tmpdirname = tempfile.mkdtemp()\n+\n+        # Audio tokenizer is a bigger model so we will reuse this if possible\n+        self.processor = DiaProcessor(\n+            tokenizer=self.get_tokenizer(),\n+            feature_extractor=self.get_feature_extractor(),\n+            audio_tokenizer=self.get_audio_tokenizer(),\n+        )\n+\n+        # Default audio values based on Dia and Dac\n+        self.pad_id = 1025\n+        self.bos_id = 1026\n+        self.dac_chunk_len = 512\n+        self.delay_pattern = [0, 8, 9, 10, 11, 12, 13, 14, 15]\n+\n+    def get_tokenizer(self, **kwargs):\n+        return DiaTokenizer.from_pretrained(self.checkpoint, **kwargs)\n+\n+    def get_feature_extractor(self, **kwargs):\n+        return DiaFeatureExtractor.from_pretrained(self.checkpoint, **kwargs)\n+\n+    def get_audio_tokenizer(self, **kwargs):\n+        return DacModel.from_pretrained(self.audio_tokenizer_checkpoint, **kwargs)\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n+        del self.processor\n+\n+    def test_save_load_pretrained_default(self):\n+        tokenizer = self.get_tokenizer()\n+        feature_extractor = self.get_feature_extractor()\n+        audio_tokenizer = self.get_audio_tokenizer()\n+\n+        processor = DiaProcessor(\n+            tokenizer=tokenizer, feature_extractor=feature_extractor, audio_tokenizer=audio_tokenizer\n+        )\n+\n+        processor.save_pretrained(self.tmpdirname)\n+        processor = DiaProcessor.from_pretrained(self.tmpdirname)\n+\n+        self.assertEqual(processor.tokenizer.get_vocab(), tokenizer.get_vocab())\n+        self.assertIsInstance(processor.tokenizer, DiaTokenizer)\n+\n+        self.assertEqual(processor.feature_extractor.to_json_string(), feature_extractor.to_json_string())\n+        self.assertIsInstance(processor.feature_extractor, DiaFeatureExtractor)\n+\n+        self.assertEqual(processor.audio_tokenizer.__class__.__name__, audio_tokenizer.__class__.__name__)\n+        self.assertEqual(processor.audio_tokenizer.name_or_path, audio_tokenizer.name_or_path)\n+        self.assertTrue(check_models_equal(processor.audio_tokenizer, audio_tokenizer))\n+        self.assertIsInstance(processor.audio_tokenizer, DacModel)\n+\n+    def test_save_load_pretrained_additional_features(self):\n+        processor = DiaProcessor(\n+            tokenizer=self.get_tokenizer(),\n+            feature_extractor=self.get_feature_extractor(),\n+            audio_tokenizer=self.get_audio_tokenizer(),\n+        )\n+        processor.save_pretrained(self.tmpdirname)\n+\n+        tokenizer_add_kwargs = self.get_tokenizer()\n+        feature_extractor_add_kwargs = self.get_feature_extractor()\n+        audio_tokenizer_add_kwargs = self.get_audio_tokenizer()\n+\n+        processor = DiaProcessor.from_pretrained(self.tmpdirname)\n+\n+        self.assertEqual(processor.tokenizer.get_vocab(), tokenizer_add_kwargs.get_vocab())\n+        self.assertIsInstance(processor.tokenizer, DiaTokenizer)\n+\n+        self.assertEqual(processor.feature_extractor.to_json_string(), feature_extractor_add_kwargs.to_json_string())\n+        self.assertIsInstance(processor.feature_extractor, DiaFeatureExtractor)\n+\n+        self.assertEqual(processor.audio_tokenizer.__class__.__name__, audio_tokenizer_add_kwargs.__class__.__name__)\n+        self.assertEqual(processor.audio_tokenizer.name_or_path, audio_tokenizer_add_kwargs.name_or_path)\n+        self.assertTrue(check_models_equal(processor.audio_tokenizer, audio_tokenizer_add_kwargs))\n+        self.assertIsInstance(processor.audio_tokenizer, DacModel)\n+\n+    def test_model_input_names(self):\n+        tokenizer = self.get_tokenizer()\n+\n+        self.assertListEqual(\n+            self.processor.model_input_names,\n+            list(dict.fromkeys(tokenizer.model_input_names + [\"decoder_input_ids\", \"decoder_attention_mask\"])),\n+            msg=\"`processor` model input names do not match the expected names.\",\n+        )\n+\n+    def test_tokenize(self):\n+        tokenizer = self.get_tokenizer()\n+        random_text = [\"This is a processing test for tokenization\", \"[S1] Dia template style [S2] Nice\"]\n+\n+        input_tokenizer = tokenizer(random_text, padding=True, return_tensors=\"pt\")\n+        input_processor = self.processor(random_text)\n+\n+        for key in input_tokenizer.keys():\n+            self.assertTrue((input_tokenizer[key] == input_processor[key]).all())\n+\n+    def test_no_audio(self):\n+        random_text = [\"Dummy Input\"] * 2\n+        input_processor = self.processor(random_text)\n+        audio_tokens, audio_mask = input_processor[\"decoder_input_ids\"], input_processor[\"decoder_attention_mask\"]\n+\n+        # full mask with +1 for bos\n+        self.assertTrue(audio_mask.sum() == (max(self.delay_pattern) + 1) * len(random_text))\n+        self.assertTrue(\n+            audio_tokens.shape\n+            == (\n+                len(random_text),\n+                max(self.delay_pattern) + 1,\n+                len(self.delay_pattern),\n+            )\n+        )\n+\n+        for channel_idx, delay in enumerate(self.delay_pattern):\n+            expected_sequence = torch.ones(size=(audio_tokens.shape[:-1])) * self.pad_id\n+            expected_sequence[:, : delay + 1] = self.bos_id\n+            self.assertTrue((audio_tokens[..., channel_idx] == expected_sequence).all())\n+\n+    def test_audio(self):\n+        audio_tokenizer = self.get_audio_tokenizer()\n+        feature_extractor = self.get_feature_extractor()\n+\n+        random_text = [\"Dummy Input\"] * 2\n+        # Dac only starts accepting audio from a certain length (ensured via >=1024)\n+        raw_speeches = [np.random.rand(2048).astype(np.float32), np.random.rand(1024).astype(np.float32)]\n+        input_processor = self.processor(random_text, raw_speeches)\n+        audio_tokens, audio_mask = input_processor[\"decoder_input_ids\"], input_processor[\"decoder_attention_mask\"]\n+\n+        sequence_len = audio_mask.shape[1]\n+        for batch_idx, speech in enumerate(raw_speeches):\n+            raw_audio = feature_extractor(speech, return_tensors=\"pt\")[\"input_values\"]\n+            codebooks = audio_tokenizer(raw_audio).audio_codes.transpose(1, 2)\n+\n+            pad_len = sequence_len - audio_mask.sum(dim=-1)[batch_idx]\n+            for channel_idx, delay in enumerate(self.delay_pattern):\n+                # Left padding filled bos, right padding (delay) are pad\n+                start_idx = pad_len + delay + 1\n+                end_idx = start_idx + codebooks.shape[1]\n+\n+                encoded_sequence = audio_tokens[batch_idx, :, channel_idx]\n+                expected_sequence = torch.ones(size=(sequence_len,)) * self.pad_id\n+                expected_sequence[:start_idx] = self.bos_id\n+                expected_sequence[start_idx:end_idx] = codebooks[0, :, channel_idx]\n+\n+                self.assertTrue((encoded_sequence == expected_sequence).all())\n+\n+        # Just to make sure the masking correctly only ignores bos tokens\n+        self.assertTrue((audio_tokens[~audio_mask.bool()] == self.bos_id).all())\n+\n+    @parameterized.expand([([1, 1],), ([1, 5],), ([2, 4, 6],)])\n+    def test_decode_audio(self, audio_lens):\n+        feature_extractor = self.get_feature_extractor()\n+        audio_tokenizer = self.get_audio_tokenizer()\n+\n+        random_text = [\"Dummy Input\"] * len(audio_lens)\n+        raw_speeches = [np.random.rand(self.dac_chunk_len * l).astype(np.float32) for l in audio_lens]\n+        # we need eos (given if training) to decode properly, also enforced via custom logits processor\n+        input_processor = self.processor(random_text, raw_speeches, generation=False)\n+        audio_tokens = input_processor[\"decoder_input_ids\"]\n+\n+        decoded_speeches = self.processor.batch_decode(audio_tokens)\n+        for batch_idx, speech in enumerate(raw_speeches):\n+            raw_audio = feature_extractor(speech, return_tensors=\"pt\")[\"input_values\"]\n+            codebooks = audio_tokenizer(raw_audio).audio_codes\n+\n+            decoded_audio = decoded_speeches[batch_idx]\n+            expected_audio = audio_tokenizer.decode(audio_codes=codebooks).audio_values\n+\n+            self.assertTrue((expected_audio == decoded_audio).all())\n+            self.assertTrue(decoded_speeches[batch_idx].shape[-1] == audio_lens[batch_idx] * self.dac_chunk_len)\n+\n+    @parameterized.expand([(1, 2, [0, 1, 4]), (2, 4, [1, 3, 2]), (4, 8, [0, 5, 7])])\n+    def test_delay_in_audio(self, bsz, seq_len, delay_pattern):\n+        # static functions which are crucial, hence we also test them here\n+        build_indices_fn = DiaProcessor.build_indices\n+        delay_fn = DiaProcessor.apply_audio_delay\n+\n+        bos, pad = -2, -1\n+        num_channels = len(delay_pattern)\n+\n+        audio_input = torch.arange(bsz * seq_len * num_channels).view(bsz, seq_len, num_channels)\n+        # imitate a delay mask with zeroes\n+        audio_input = torch.cat([audio_input, torch.zeros(size=(bsz, max(delay_pattern), num_channels))], dim=1)\n+\n+        precomputed_idx = build_indices_fn(\n+            bsz=bsz,\n+            seq_len=seq_len + max(delay_pattern),\n+            num_channels=num_channels,\n+            delay_pattern=delay_pattern,\n+            revert=False,\n+        )\n+        delayed_audio_out = delay_fn(\n+            audio=audio_input,\n+            pad_token_id=pad,\n+            bos_token_id=bos,\n+            precomputed_idx=precomputed_idx,\n+        )\n+\n+        # every channel idx is shifted by delay_pattern[idx]\n+        delayed_audio_res = audio_input.clone()\n+        for idx, delay in enumerate(delay_pattern):\n+            delayed_audio_res[:, :delay, idx] = bos\n+            remaining_input = seq_len + max(delay_pattern) - delay\n+            delayed_audio_res[:, delay:, idx] = audio_input[:, :remaining_input, idx]\n+\n+        self.assertTrue((delayed_audio_out == delayed_audio_res).all())\n+\n+        # we should get back to the original audio we had (when removing the delay pad)\n+        bsz, new_seq_len, num_channels = delayed_audio_out.shape\n+        precomputed_idx = build_indices_fn(\n+            bsz=bsz,\n+            seq_len=new_seq_len,\n+            num_channels=num_channels,\n+            delay_pattern=delay_pattern,\n+            revert=True,\n+        )\n+        reverted_audio_out = delay_fn(\n+            audio=delayed_audio_out,\n+            pad_token_id=pad,\n+            bos_token_id=bos,\n+            precomputed_idx=precomputed_idx,\n+        )\n+\n+        reverted_audio_res = audio_input.clone()[:, :seq_len]\n+\n+        self.assertTrue((reverted_audio_out[:, :seq_len] == reverted_audio_res).all())"
        },
        {
            "sha": "4ade611f68e83e207b0c85cd53ac45c74d6b5073",
            "filename": "tests/models/dia/test_tokenization_dia.py",
            "status": "added",
            "additions": 123,
            "deletions": 0,
            "changes": 123,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/tests%2Fmodels%2Fdia%2Ftest_tokenization_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/tests%2Fmodels%2Fdia%2Ftest_tokenization_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdia%2Ftest_tokenization_dia.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -0,0 +1,123 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+from transformers.models.dia import DiaTokenizer\n+from transformers.testing_utils import slow\n+\n+from ...test_tokenization_common import TokenizerTesterMixin\n+\n+\n+# Special tokens\n+PAD = 0\n+S1 = 1\n+S2 = 2\n+\n+\n+class DiaTokenizerTest(TokenizerTesterMixin, unittest.TestCase):\n+    tokenizer_class = DiaTokenizer\n+    test_rust_tokenizer = False\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n+        tokenizer = DiaTokenizer()\n+        tokenizer.save_pretrained(cls.tmpdirname)\n+\n+    def test_convert_token_and_id(self):\n+        \"\"\"Test ``_convert_token_to_id`` and ``_convert_id_to_token``.\"\"\"\n+        token = \"i\"\n+        token_id = 105\n+\n+        self.assertEqual(self.get_tokenizer()._convert_token_to_id(token), token_id)\n+        self.assertEqual(self.get_tokenizer()._convert_id_to_token(token_id), token)\n+\n+    def test_get_vocab(self):\n+        vocab_keys = list(self.get_tokenizer().get_vocab().keys())\n+\n+        self.assertEqual(vocab_keys[PAD], \"<pad>\")\n+        self.assertEqual(vocab_keys[S1], \"[S1]\")\n+        self.assertEqual(vocab_keys[S2], \"[S2]\")\n+        self.assertEqual(len(vocab_keys), 256)\n+\n+    def test_vocab_size(self):\n+        # utf-8 == 2**8 == 256\n+        self.assertEqual(self.get_tokenizer().vocab_size, 256)\n+\n+    def test_full_tokenizer(self):\n+        tokenizer = DiaTokenizer.from_pretrained(self.tmpdirname)\n+\n+        tokens = tokenizer.tokenize(\"Hello, world!\")\n+        self.assertListEqual(tokens, [\"H\", \"e\", \"l\", \"l\", \"o\", \",\", \" \", \"w\", \"o\", \"r\", \"l\", \"d\", \"!\"])\n+        ids = tokenizer.convert_tokens_to_ids(tokens)\n+        self.assertListEqual(ids, [72, 101, 108, 108, 111, 44, 32, 119, 111, 114, 108, 100, 33])\n+        back_tokens = tokenizer.convert_ids_to_tokens(ids)\n+        self.assertListEqual(back_tokens, [\"H\", \"e\", \"l\", \"l\", \"o\", \",\", \" \", \"w\", \"o\", \"r\", \"l\", \"d\", \"!\"])\n+\n+        tokens = tokenizer.tokenize(\"[S1] Hello [S2] Hello<pad>\")\n+        self.assertListEqual(\n+            tokens,\n+            [\"[S1]\", \" \", \"H\", \"e\", \"l\", \"l\", \"o\", \" \", \"[S2]\", \" \", \"H\", \"e\", \"l\", \"l\", \"o\", \"<pad>\"],\n+        )\n+        ids = tokenizer.convert_tokens_to_ids(tokens)\n+        self.assertListEqual(ids, [S1, 32, 72, 101, 108, 108, 111, 32, S2, 32, 72, 101, 108, 108, 111, PAD])\n+        back_tokens = tokenizer.convert_ids_to_tokens(ids)\n+        self.assertListEqual(\n+            back_tokens, [\"[S1]\", \" \", \"H\", \"e\", \"l\", \"l\", \"o\", \" \", \"[S2]\", \" \", \"H\", \"e\", \"l\", \"l\", \"o\", \"<pad>\"]\n+        )\n+\n+    @slow\n+    def test_tokenizer_integration(self):\n+        # Overwritten as decoding will lead to all single bytes (i.e. characters) while usually the string format is expected\n+        expected_encoding = {'input_ids': [[84, 114, 97, 110, 115, 102, 111, 114, 109, 101, 114, 115, 32, 40, 102, 111, 114, 109, 101, 114, 108, 121, 32, 107, 110, 111, 119, 110, 32, 97, 115, 32, 112, 121, 116, 111, 114, 99, 104, 45, 116, 114, 97, 110, 115, 102, 111, 114, 109, 101, 114, 115, 32, 97, 110, 100, 32, 112, 121, 116, 111, 114, 99, 104, 45, 112, 114, 101, 116, 114, 97, 105, 110, 101, 100, 45, 98, 101, 114, 116, 41, 32, 112, 114, 111, 118, 105, 100, 101, 115, 32, 103, 101, 110, 101, 114, 97, 108, 45, 112, 117, 114, 112, 111, 115, 101, 32, 97, 114, 99, 104, 105, 116, 101, 99, 116, 117, 114, 101, 115, 32, 40, 66, 69, 82, 84, 44, 32, 71, 80, 84, 45, 50, 44, 32, 82, 111, 66, 69, 82, 84, 97, 44, 32, 88, 76, 77, 44, 32, 68, 105, 115, 116, 105, 108, 66, 101, 114, 116, 44, 32, 88, 76, 78, 101, 116, 46, 46, 46, 41, 32, 102, 111, 114, 32, 78, 97, 116, 117, 114, 97, 108, 32, 76, 97, 110, 103, 117, 97, 103, 101, 32, 85, 110, 100, 101, 114, 115, 116, 97, 110, 100, 105, 110, 103, 32, 40, 78, 76, 85, 41, 32, 97, 110, 100, 32, 78, 97, 116, 117, 114, 97, 108, 32, 76, 97, 110, 103, 117, 97, 103, 101, 32, 71, 101, 110, 101, 114, 97, 116, 105, 111, 110, 32, 40, 78, 76, 71, 41, 32, 119, 105, 116, 104, 32, 111, 118, 101, 114, 32, 51, 50, 43, 32, 112, 114, 101, 116, 114, 97, 105, 110, 101, 100, 32, 109, 111, 100, 101, 108, 115, 32, 105, 110, 32, 49, 48, 48, 43, 32, 108, 97, 110, 103, 117, 97, 103, 101, 115, 32, 97, 110, 100, 32, 100, 101, 101, 112, 32, 105, 110, 116, 101, 114, 111, 112, 101, 114, 97, 98, 105, 108, 105, 116, 121, 32, 98, 101, 116, 119, 101, 101, 110, 32, 74, 97, 120, 44, 32, 80, 121, 84, 111, 114, 99, 104, 32, 97, 110, 100, 32, 84, 101, 110, 115, 111, 114, 70, 108, 111, 119, 46], [66, 69, 82, 84, 32, 105, 115, 32, 100, 101, 115, 105, 103, 110, 101, 100, 32, 116, 111, 32, 112, 114, 101, 45, 116, 114, 97, 105, 110, 32, 100, 101, 101, 112, 32, 98, 105, 100, 105, 114, 101, 99, 116, 105, 111, 110, 97, 108, 32, 114, 101, 112, 114, 101, 115, 101, 110, 116, 97, 116, 105, 111, 110, 115, 32, 102, 114, 111, 109, 32, 117, 110, 108, 97, 98, 101, 108, 101, 100, 32, 116, 101, 120, 116, 32, 98, 121, 32, 106, 111, 105, 110, 116, 108, 121, 32, 99, 111, 110, 100, 105, 116, 105, 111, 110, 105, 110, 103, 32, 111, 110, 32, 98, 111, 116, 104, 32, 108, 101, 102, 116, 32, 97, 110, 100, 32, 114, 105, 103, 104, 116, 32, 99, 111, 110, 116, 101, 120, 116, 32, 105, 110, 32, 97, 108, 108, 32, 108, 97, 121, 101, 114, 115, 46], [84, 104, 101, 32, 113, 117, 105, 99, 107, 32, 98, 114, 111, 119, 110, 32, 102, 111, 120, 32, 106, 117, 109, 112, 115, 32, 111, 118, 101, 114, 32, 116, 104, 101, 32, 108, 97, 122, 121, 32, 100, 111, 103, 46]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}  # fmt: skip\n+\n+        sequences = [\n+            \"Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides \"\n+            \"general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet...) for Natural \"\n+            \"Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained \"\n+            \"models in 100+ languages and deep interoperability between Jax, PyTorch and TensorFlow.\",\n+            \"BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly \"\n+            \"conditioning on both left and right context in all layers.\",\n+            \"The quick brown fox jumps over the lazy dog.\",\n+        ]\n+\n+        tokenizer_classes = [self.tokenizer_class]\n+        if self.test_rust_tokenizer:\n+            tokenizer_classes.append(self.rust_tokenizer_class)\n+\n+        for tokenizer_class in tokenizer_classes:\n+            tokenizer = tokenizer_class.from_pretrained(\"AntonV/Dia-1.6B\")\n+\n+            encoding = tokenizer(sequences)\n+            encoding_data = encoding.data\n+            self.assertDictEqual(encoding_data, expected_encoding)\n+\n+            # Byte decoding leads to characters so we need to join them\n+            decoded_sequences = [\n+                \"\".join(tokenizer.decode(seq, skip_special_tokens=True)) for seq in encoding[\"input_ids\"]\n+            ]\n+\n+            for expected, decoded in zip(sequences, decoded_sequences):\n+                if self.test_sentencepiece_ignore_case:\n+                    expected = expected.lower()\n+                self.assertEqual(expected, decoded)\n+\n+    @unittest.skip(reason=\"Dia relies on whole input string due to the byte-level nature.\")\n+    def test_pretokenized_inputs(self):\n+        pass\n+\n+    @unittest.skip\n+    def test_tokenizer_slow_store_full_signature(self):\n+        pass"
        },
        {
            "sha": "d3f8456f544b6cc54f5cfca411f44b4fd705d118",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -4574,6 +4574,11 @@ def update_config_for_flex(config):\n                     head_dim = config.head_dim\n                     config.head_dim = max(16, config.head_dim)\n \n+                cross_head_dim = None\n+                if hasattr(config, \"cross_head_dim\") and config.cross_head_dim is not None:\n+                    cross_head_dim = config.cross_head_dim\n+                    config.cross_head_dim = max(16, config.cross_head_dim)\n+\n                 if (\n                     getattr(config, \"hidden_size\", None) is not None\n                     and getattr(config, \"num_attention_heads\", None) is not None\n@@ -4588,6 +4593,17 @@ def update_config_for_flex(config):\n                     decoder_head_dim = config.decoder_hidden_size // config.decoder_num_attention_heads\n                     config.decoder_hidden_size *= max(16 // decoder_head_dim, 1)\n \n+                if (\n+                    getattr(config, \"cross_hidden_size\", None) is not None\n+                    and getattr(config, \"cross_num_attention_heads\", None) is not None\n+                ):\n+                    cross_head_dim = (\n+                        cross_head_dim\n+                        if cross_head_dim is not None\n+                        else config.cross_hidden_size // config.cross_num_attention_heads\n+                    )\n+                    config.cross_hidden_size *= max(16 // cross_head_dim, 1)\n+\n             # Set default attention to flex and update config values\n             update_config_for_flex(config)\n             for key in config.sub_configs:"
        },
        {
            "sha": "22d6b033afbc0b6bbd19634856218f68b2a88c44",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/583db52bc6d5415a205724776136d094ff70c9a4/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/583db52bc6d5415a205724776136d094ff70c9a4/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=583db52bc6d5415a205724776136d094ff70c9a4",
            "patch": "@@ -32,6 +32,10 @@\n CONFIG_MAPPING = transformers.models.auto.configuration_auto.CONFIG_MAPPING\n \n SPECIAL_CASES_TO_ALLOW = {\n+    # used internally during generation to provide the custom logit processors with their necessary information\n+    \"DiaConfig\": [\n+        \"delay_pattern\",\n+    ],\n     # 'max_position_embeddings' is not used in modeling file, but needed for eval frameworks like Huggingface's lighteval (https://github.com/huggingface/lighteval/blob/af24080ea4f16eaf1683e353042a2dfc9099f038/src/lighteval/models/base_model.py#L264).\n     # periods and offsets are not used in modeling file, but used in the configuration file to define `layers_block_type` and `layers_num_experts`.\n     \"BambaConfig\": ["
        }
    ],
    "stats": {
        "total": 5760,
        "additions": 5732,
        "deletions": 28
    }
}