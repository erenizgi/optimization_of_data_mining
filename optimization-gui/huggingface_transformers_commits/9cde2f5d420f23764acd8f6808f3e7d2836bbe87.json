{
    "author": "molbap",
    "message": "Minor llama4 fixes (#38123)\n\n* fix wrong scaling value/default Cache init\n\n* style\n\n* fix various issues on integration tests\n\n* change expected outputs\n\n* fixup\n\n* fix config access\n\n* protect default scaling",
    "sha": "9cde2f5d420f23764acd8f6808f3e7d2836bbe87",
    "files": [
        {
            "sha": "5bf5f1488c570d50426d2bb312bcfa30a99ba544",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 31,
            "deletions": 4,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/9cde2f5d420f23764acd8f6808f3e7d2836bbe87/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9cde2f5d420f23764acd8f6808f3e7d2836bbe87/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=9cde2f5d420f23764acd8f6808f3e7d2836bbe87",
            "patch": "@@ -258,6 +258,33 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+# Adapted from transformers.models.llama.modeling_llama.eager_attention_forward -> llama4 doesn't cast attn weights to fp32\n+def vision_eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * module.head_dim**-0.5\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class Llama4TextAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -534,10 +561,10 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids.to(self.embed_tokens.weight.device))\n \n         if use_cache and past_key_values is None:\n-            if self.config.get_text_config().get(\"attention_chunk_size\") is not None:\n+            if self.config.get_text_config().attention_chunk_size is not None:\n                 past_key_values = HybridChunkedCache(self.config, inputs_embeds.shape[0], inputs_embeds.shape[1])\n             else:\n-                past_key_values = DynamicCache(self.config, inputs_embeds.shape[0], inputs_embeds.shape[1])\n+                past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -1099,7 +1126,7 @@ def forward(\n         key_states = key_states.transpose(1, 2)\n         value_states = value_states.transpose(1, 2)\n \n-        attention_interface: Callable = eager_attention_forward\n+        attention_interface: Callable = vision_eager_attention_forward\n         # flex disable because breaks on TP 8, embed is 88 not power of 2\n         if self.config._attn_implementation not in [\"eager\", \"flex_attention\"]:\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n@@ -1117,7 +1144,7 @@ def forward(\n             value_states,\n             None,\n             dropout=0.0 if not self.training else self.attention_dropout,\n-            scaling=None,\n+            scaling=None,  # TODO Might be enforced here for TP compatibility as scaling is not just sqrt(head_dim)\n             is_causal=False,  # HAS TO BE ENFORCED\n             **kwargs,\n         )"
        },
        {
            "sha": "b349c47e3c4ff362c44b317fce8256cabdbe94c7",
            "filename": "tests/models/llama4/test_modeling_llama4.py",
            "status": "modified",
            "additions": 27,
            "deletions": 29,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/9cde2f5d420f23764acd8f6808f3e7d2836bbe87/tests%2Fmodels%2Fllama4%2Ftest_modeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9cde2f5d420f23764acd8f6808f3e7d2836bbe87/tests%2Fmodels%2Fllama4%2Ftest_modeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama4%2Ftest_modeling_llama4.py?ref=9cde2f5d420f23764acd8f6808f3e7d2836bbe87",
            "patch": "@@ -37,7 +37,7 @@\n @require_torch_large_gpu\n @require_read_token\n class Llama4IntegrationTest(unittest.TestCase):\n-    model_id = \"ll-re/Llama-4-17B-Omni-Instruct\"\n+    model_id = \"meta-llama/Llama-4-Scout-17B-16E\"\n     # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n     # Depending on the hardware we get different logits / generations\n     cuda_compute_capability_major_version = None\n@@ -48,14 +48,17 @@ def setUpClass(cls):\n             # 8 is for A100 / A10 and 7 for T4\n             cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n         cls.model = Llama4ForConditionalGeneration.from_pretrained(\n-            \"ll-re/Llama-4-17B-Omni-Instruct\", device_map=\"auto\", torch_dtype=torch.float32\n+            \"meta-llama/Llama-4-Scout-17B-16E\",\n+            device_map=\"auto\",\n+            torch_dtype=torch.float32,\n+            attn_implementation=\"eager\",\n         )\n \n     def setUp(self):\n-        self.processor = Llama4Processor.from_pretrained(\"ll-re/Llama-4-17B-Omni-Instruct\", padding_side=\"left\")\n+        self.processor = Llama4Processor.from_pretrained(\"meta-llama/Llama-4-Scout-17B-16E\", padding_side=\"left\")\n \n         url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/cow_beach_1.png\"\n-        self.messages = [\n+        self.messages_1 = [\n             {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n             {\n                 \"role\": \"user\",\n@@ -66,27 +69,7 @@ def setUp(self):\n             },\n         ]\n \n-    def test_model_17b_16e_fp16(self):\n-        EXPECTED_TEXT = [\n-            \"The capital of France is Paris, which is located in the north-central part of the country. Paris is known for its iconic landmarks such as the\",\n-            \"Roses are red, violets are blue, and this poem is about you. Roses are red, violets are blue, and I love\",\n-        ]\n-\n-        messages = [\n-            {\"role\": \"user\", \"content\": \"Who are you?\"},\n-        ]\n-        inputs = self.processor.apply_chat_template(\n-            messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True\n-        ).to(torch_device)\n-\n-        output = self.model.generate(**inputs, max_new_tokens=100)\n-        output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n-\n-        print(output_text)\n-        self.assertEqual(output_text, EXPECTED_TEXT)\n-\n-    def test_model_17b_16e_batch(self):\n-        messages_2 = [\n+        self.messages_2 = [\n             {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n             {\n                 \"role\": \"user\",\n@@ -101,20 +84,35 @@ def test_model_17b_16e_batch(self):\n             },\n         ]\n \n+    def test_model_17b_16e_fp16(self):\n+        EXPECTED_TEXT = [\n+                'system\\n\\nYou are a helpful assistant.user\\n\\nWhat is shown in this image?assistant\\n\\nThe image shows a cow standing on a beach, with a blue sky and a body of water in the background. The cow is brown with a white'\n+        ]  # fmt: skip\n+\n+        inputs = self.processor.apply_chat_template(\n+            self.messages_1, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True\n+        ).to(device=torch_device, dtype=self.model.dtype)\n+        output = self.model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+        output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n+\n+        print(output_text)\n+        self.assertEqual(output_text, EXPECTED_TEXT)\n+\n+    def test_model_17b_16e_batch(self):\n         inputs = self.processor.apply_chat_template(\n-            [self.messages, messages_2],\n+            [self.messages_1, self.messages_2],\n             tokenize=True,\n             return_dict=True,\n             return_tensors=\"pt\",\n             padding=True,\n             add_generation_prompt=True,\n-        ).to(torch_device)\n+        ).to(device=torch_device, dtype=torch.float32)\n \n         output = self.model.generate(**inputs, max_new_tokens=30, do_sample=False)\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n \n         EXPECTED_TEXTS = [\n-            'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with clear turquoise water and a blue sky in the background. It looks like',\n-            \"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, these images are not identical. \\n\\nHere's a breakdown of the differences:\\n\\n*   **Image 1:** Shows a cow\"\n+            'system\\n\\nYou are a helpful assistant.user\\n\\nWhat is shown in this image?assistant\\n\\nThe image shows a cow standing on a beach, with a blue sky and a body of water in the background. The cow is brown with a white',\n+            'system\\n\\nYou are a helpful assistant.user\\n\\nAre these images identical?assistant\\n\\nNo, these images are not identical. The first image shows a cow standing on a beach with a blue sky and a white cloud in the background.'\n         ]  # fmt: skip\n         self.assertEqual(output_text, EXPECTED_TEXTS)"
        }
    ],
    "stats": {
        "total": 91,
        "additions": 58,
        "deletions": 33
    }
}