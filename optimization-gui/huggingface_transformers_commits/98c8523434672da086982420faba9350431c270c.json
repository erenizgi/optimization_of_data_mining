{
    "author": "yonigozlan",
    "message": "Fix more dates in model cards and wrong modalities in _toctree.yml (#40955)\n\n* Fix model cards and modalities in toctree\n\n* fix new models",
    "sha": "98c8523434672da086982420faba9350431c270c",
    "files": [
        {
            "sha": "be97cf6d7c3626bb60e00f607668d92f6d4fc4d9",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=98c8523434672da086982420faba9350431c270c",
            "patch": "@@ -439,6 +439,8 @@\n         title: DeBERTa\n       - local: model_doc/deberta-v2\n         title: DeBERTa-v2\n+      - local: model_doc/deepseek_v2\n+        title: DeepSeek-V2\n       - local: model_doc/deepseek_v3\n         title: DeepSeek-V3\n       - local: model_doc/dialogpt\n@@ -763,12 +765,6 @@\n         title: D-FINE\n       - local: model_doc/dab-detr\n         title: DAB-DETR\n-      - local: model_doc/deepseek_v2\n-        title: DeepSeek-V2\n-      - local: model_doc/deepseek_vl\n-        title: DeepseekVL\n-      - local: model_doc/deepseek_vl_hybrid\n-        title: DeepseekVLHybrid\n       - local: model_doc/deformable_detr\n         title: Deformable DETR\n       - local: model_doc/deit\n@@ -851,10 +847,16 @@\n         title: RT-DETR\n       - local: model_doc/rt_detr_v2\n         title: RT-DETRv2\n+      - local: model_doc/sam2\n+        title: SAM2\n       - local: model_doc/segformer\n         title: SegFormer\n       - local: model_doc/seggpt\n         title: SegGpt\n+      - local: model_doc/sam\n+        title: Segment Anything\n+      - local: model_doc/sam_hq\n+        title: Segment Anything High Quality\n       - local: model_doc/superglue\n         title: SuperGlue\n       - local: model_doc/superpoint\n@@ -977,6 +979,8 @@\n         title: XLSR-Wav2Vec2\n       title: Audio models\n     - sections:\n+      - local: model_doc/sam2_video\n+        title: SAM2 Video\n       - local: model_doc/timesformer\n         title: TimeSformer\n       - local: model_doc/vjepa2\n@@ -1021,6 +1025,10 @@\n         title: ColQwen2\n       - local: model_doc/data2vec\n         title: Data2Vec\n+      - local: model_doc/deepseek_vl\n+        title: DeepseekVL\n+      - local: model_doc/deepseek_vl_hybrid\n+        title: DeepseekVLHybrid\n       - local: model_doc/deplot\n         title: DePlot\n       - local: model_doc/donut\n@@ -1139,14 +1147,6 @@\n         title: Qwen3VL\n       - local: model_doc/qwen3_vl_moe\n         title: Qwen3VLMoe\n-      - local: model_doc/sam2\n-        title: SAM2\n-      - local: model_doc/sam2_video\n-        title: SAM2 Video\n-      - local: model_doc/sam\n-        title: Segment Anything\n-      - local: model_doc/sam_hq\n-        title: Segment Anything High Quality\n       - local: model_doc/shieldgemma2\n         title: ShieldGemma2\n       - local: model_doc/siglip"
        },
        {
            "sha": "b5be3458db7dfa2cd36e40c744774f3d7901883d",
            "filename": "docs/source/en/model_doc/bert-generation.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert-generation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert-generation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert-generation.md?ref=98c8523434672da086982420faba9350431c270c",
            "patch": "@@ -13,6 +13,7 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n \n -->\n+*This model was released on 2019-07-29 and added to Hugging Face Transformers on 2020-11-16.*\n \n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">"
        },
        {
            "sha": "418a660b6d23b607d2e968463873fb4592203684",
            "filename": "docs/source/en/model_doc/flex_olmo.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Fflex_olmo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Fflex_olmo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fflex_olmo.md?ref=98c8523434672da086982420faba9350431c270c",
            "patch": "@@ -16,7 +16,7 @@ limitations under the License.\n ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n \n -->\n-*This model was released on 2025-07-09 and added to Hugging Face Transformers on 2025-09-15.*\n+*This model was released on 2025-07-09 and added to Hugging Face Transformers on 2025-09-18.*\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">"
        },
        {
            "sha": "520c68b7fd9d00ddf6feebf0952be108516fbbcf",
            "filename": "docs/source/en/model_doc/hunyuan_v1_dense.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Fhunyuan_v1_dense.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Fhunyuan_v1_dense.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhunyuan_v1_dense.md?ref=98c8523434672da086982420faba9350431c270c",
            "patch": "@@ -13,6 +13,7 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n \n -->\n+*This model was released on {release_date} and added to Hugging Face Transformers on 2025-08-22.*\n \n # HunYuanDenseV1\n "
        },
        {
            "sha": "36a53742715de1178c806240c61c510aeaf26dc8",
            "filename": "docs/source/en/model_doc/hunyuan_v1_moe.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Fhunyuan_v1_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Fhunyuan_v1_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhunyuan_v1_moe.md?ref=98c8523434672da086982420faba9350431c270c",
            "patch": "@@ -13,6 +13,7 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n \n -->\n+*This model was released on {release_date} and added to Hugging Face Transformers on 2025-08-22.*\n \n # HunYuanMoEV1\n "
        },
        {
            "sha": "3a93a8189a70dff85747720f2c7dc36c08d3024b",
            "filename": "docs/source/en/model_doc/lfm2_vl.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2_vl.md?ref=98c8523434672da086982420faba9350431c270c",
            "patch": "@@ -13,6 +13,7 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n \n -->\n+*This model was released on {release_date} and added to Hugging Face Transformers on 2025-09-18.*\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">"
        },
        {
            "sha": "d9a9a4a7f603deb5b14ef91b412f740e7efa8cf8",
            "filename": "docs/source/en/model_doc/longcat_flash.md",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Flongcat_flash.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Flongcat_flash.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flongcat_flash.md?ref=98c8523434672da086982420faba9350431c270c",
            "patch": "@@ -16,8 +16,7 @@ limitations under the License.\n ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n \n -->\n-*This model was released on 2025-09-01 and added to Hugging Face Transformers on 2025-09-15.*\n-\n+*This model was released on 2025-09-01 and added to Hugging Face Transformers on 2025-09-17.*\n \n # LongCatFlash\n \n@@ -70,7 +69,7 @@ outputs = model.generate(inputs, max_new_tokens=30)\n print(tokenizer.batch_decode(outputs))\n ```\n \n-To run with TP, you will need torchrun: \n+To run with TP, you will need torchrun:\n \n ```bash\n torchrun  --nproc_per_node=8 --nnodes=2 --node_rank=0 | 1  --rdzv-id <an_id> --rdzv-backend c10d --rdzv-endpoint $NODE_ID:$NODE_PORT  --log-dir ./logs_longcat launch_longcat.py"
        },
        {
            "sha": "13b6f3d6c04b2785ef483a847874250d3b911c32",
            "filename": "docs/source/en/model_doc/ministral.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Fministral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Fministral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fministral.md?ref=98c8523434672da086982420faba9350431c270c",
            "patch": "@@ -13,6 +13,7 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n \n -->\n+*This model was released on {release_date} and added to Hugging Face Transformers on 2025-09-11.*\n \n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">"
        },
        {
            "sha": "8e88a175d463e736480d52a75940d09decb47ff7",
            "filename": "docs/source/en/model_doc/olmo3.md",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo3.md?ref=98c8523434672da086982420faba9350431c270c",
            "patch": "@@ -16,7 +16,8 @@ limitations under the License.\n ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n \n -->\n-*This model was released on {release_date} and added to Hugging Face Transformers on 2025-09-08.*\n+*This model was released on {release_date} and added to Hugging Face Transformers on 2025-09-16.*\n+\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n@@ -46,7 +47,7 @@ pipe = pipeline(\n     dtype=torch.bfloat16,\n     device=0,\n )\n-    \n+\n result = pipe(\"Plants create energy through a process known as\")\n print(result)\n ```\n@@ -119,11 +120,11 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n \n ## Notes\n \n-- Load specific intermediate checkpoints by adding the `revision` parameter to [`~PreTrainedModel.from_pretrained`]. \n+- Load specific intermediate checkpoints by adding the `revision` parameter to [`~PreTrainedModel.from_pretrained`].\n \n     ```py\n     from transformers import AutoModelForCausalLM\n-    \n+\n     model = AutoModelForCausalLM.from_pretrained(\"allenai/TBA\", revision=\"stage1-step140000-tokens294B\")\n     ```\n \n@@ -144,4 +145,4 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n ## Olmo3PreTrainedModel\n \n [[autodoc]] Olmo3PreTrainedModel\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "342e34ef7a1bb9e96418f0477a5d8174d8dc9409",
            "filename": "docs/source/en/model_doc/ovis2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md?ref=98c8523434672da086982420faba9350431c270c",
            "patch": "@@ -13,6 +13,7 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n \n -->\n+*This model was released on 2024-05-31 and added to Hugging Face Transformers on 2025-08-18.*\n \n # Ovis2\n "
        },
        {
            "sha": "737934136099bd8cacffeaca65763ec875d0235e",
            "filename": "docs/source/en/model_doc/qwen3_next.md",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_next.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_next.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_next.md?ref=98c8523434672da086982420faba9350431c270c",
            "patch": "@@ -13,18 +13,20 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n \n -->\n+*This model was released on {release_date} and added to Hugging Face Transformers on 2025-09-10.*\n+\n ## Overview\n \n-The Qwen3-Next series represents our next-generation foundation models, optimized for extreme context length and large-scale parameter efficiency. \n+The Qwen3-Next series represents our next-generation foundation models, optimized for extreme context length and large-scale parameter efficiency.\n The series introduces a suite of architectural innovations designed to maximize performance while minimizing computational cost:\n-- **Hybrid Attention**: Replaces standard attention with the combination of **Gated DeltaNet** and **Gated Attention**, enabling efficient context modeling.  \n+- **Hybrid Attention**: Replaces standard attention with the combination of **Gated DeltaNet** and **Gated Attention**, enabling efficient context modeling.\n - **High-Sparsity MoE**: Achieves an extreme low activation ratio as 1:50 in MoE layers — drastically reducing FLOPs per token while preserving model capacity.\n - **Multi-Token Prediction(MTP)**: Boosts pretraining model performance, and accelerates inference.\n-- **Other Optimizations**: Includes techniques such as **zero-centered and weight-decayed layernorm**, **Gated Attention**, and other stabilizing enhancements for robust training.  \n+- **Other Optimizations**: Includes techniques such as **zero-centered and weight-decayed layernorm**, **Gated Attention**, and other stabilizing enhancements for robust training.\n \n Built on this architecture, we trained and open-sourced Qwen3-Next-80B-A3B — 80B total parameters, only 3B active — achieving extreme sparsity and efficiency.\n \n-Despite its ultra-efficiency, it outperforms Qwen3-32B on downstream tasks — while requiring **less than 1/10 of the training cost**. \n+Despite its ultra-efficiency, it outperforms Qwen3-32B on downstream tasks — while requiring **less than 1/10 of the training cost**.\n Moreover, it delivers over **10x higher inference throughput** than Qwen3-32B when handling contexts longer than 32K tokens.\n \n For more details, please visit our blog [Qwen3-Next](qwen3_next) ([blog post](https://qwenlm.github.io/blog/qwen3_next/)).\n@@ -60,7 +62,7 @@ generated_ids = model.generate(\n     **model_inputs,\n     max_new_tokens=512\n )\n-output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n+output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n \n content = tokenizer.decode(output_ids, skip_special_tokens=True)\n "
        },
        {
            "sha": "c939d5da3cd9ad6173cc44fd5ab7fa4e7d35c939",
            "filename": "docs/source/en/model_doc/qwen3_vl.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl.md?ref=98c8523434672da086982420faba9350431c270c",
            "patch": "@@ -13,7 +13,7 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n \n -->\n-*This model was released on None and added to Hugging Face Transformers on 2025-08-16.*\n+*This model was released on None and added to Hugging Face Transformers on 2025-09-15.*\n \n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">"
        },
        {
            "sha": "6e27adf915d31df2fd22a7c6e2aef53dcb03563d",
            "filename": "docs/source/en/model_doc/qwen3_vl_moe.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl_moe.md?ref=98c8523434672da086982420faba9350431c270c",
            "patch": "@@ -13,7 +13,7 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n \n -->\n-*This model was released on None and added to Hugging Face Transformers on 2025-08-17.*\n+*This model was released on None and added to Hugging Face Transformers on 2025-09-15.*\n \n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">"
        },
        {
            "sha": "dbcddcb5f2c745ebb52417fc224351eade79db86",
            "filename": "docs/source/en/model_doc/seed_oss.md",
            "status": "modified",
            "additions": 18,
            "deletions": 15,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Fseed_oss.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Fseed_oss.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fseed_oss.md?ref=98c8523434672da086982420faba9350431c270c",
            "patch": "@@ -1,17 +1,20 @@\n-<!-- \n-# Copyright 2025 Bytedance-Seed Ltd and the HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License. -->\n+<!--\n+ Copyright 2025 Bytedance-Seed Ltd and the HuggingFace Inc. team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+*This model was released on {release_date} and added to Hugging Face Transformers on 2025-08-22.*\n \n # SeedOss\n \n@@ -54,4 +57,4 @@ To be released with the official model launch.\n ## SeedOssForQuestionAnswering\n \n [[autodoc]] SeedOssForQuestionAnswering\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "94d28cc8afe20885fa95d5d9c9a76c75566116c9",
            "filename": "docs/source/en/model_doc/vaultgemma.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Fvaultgemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/98c8523434672da086982420faba9350431c270c/docs%2Fsource%2Fen%2Fmodel_doc%2Fvaultgemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvaultgemma.md?ref=98c8523434672da086982420faba9350431c270c",
            "patch": "@@ -16,6 +16,7 @@ limitations under the License.\n ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n \n -->\n+*This model was released on {release_date} and added to Hugging Face Transformers on 2025-09-12.*\n \n # VaultGemma\n \n@@ -30,7 +31,7 @@ sequence length.\n VaultGemma was trained from scratch with sequence-level differential privacy (DP). Its training data includes the same\n mixture as the [Gemma 2 models](https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315),\n consisting of a number of documents of varying lengths. Additionally, it is trained using\n-[DP stochastic gradient descent (DP-SGD)](https://arxiv.org/abs/1607.00133) and provides a\n+[DP stochastic gradient descent (DP-SGD)](https://huggingface.co/papers/1607.00133) and provides a\n (ε ≤ 2.0, δ ≤ 1.1e-10)-sequence-level DP guarantee, where a sequence consists of 1024 consecutive tokens extracted from\n heterogeneous data sources. Specifically, the privacy unit of the guarantee is for the sequences after sampling and\n packing of the mixture."
        }
    ],
    "stats": {
        "total": 104,
        "additions": 58,
        "deletions": 46
    }
}