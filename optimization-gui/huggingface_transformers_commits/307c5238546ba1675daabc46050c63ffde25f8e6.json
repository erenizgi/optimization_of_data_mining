{
    "author": "ydshieh",
    "message": "further improve `utils/check_bad_commit.py` (#41658) (#41690)\n\n* fix\n\n* Update utils/check_bad_commit.py\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>",
    "sha": "307c5238546ba1675daabc46050c63ffde25f8e6",
    "files": [
        {
            "sha": "aa60275b588fc23dfcffba7db408df79d6af1097",
            "filename": ".github/workflows/check_failed_tests.yml",
            "status": "modified",
            "additions": 59,
            "deletions": 11,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/307c5238546ba1675daabc46050c63ffde25f8e6/.github%2Fworkflows%2Fcheck_failed_tests.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/307c5238546ba1675daabc46050c63ffde25f8e6/.github%2Fworkflows%2Fcheck_failed_tests.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fcheck_failed_tests.yml?ref=307c5238546ba1675daabc46050c63ffde25f8e6",
            "patch": "@@ -41,9 +41,14 @@ env:\n \n jobs:\n   check_new_failures:\n-    name: \" \"\n+    name: \"Find commits for new failing tests\"\n+    strategy:\n+      matrix:\n+        run_idx: [1]\n     runs-on:\n       group: aws-g5-4xlarge-cache\n+    outputs:\n+      process: ${{ steps.check_file.outputs.process }}\n     container:\n       image: ${{ inputs.docker }}\n       options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n@@ -54,14 +59,17 @@ jobs:\n           path: /transformers/ci_results_${{ inputs.job }}\n \n       - name: Check file\n+        id: check_file\n         working-directory: /transformers\n         run: |\n           if [ -f ci_results_${{ inputs.job }}/new_failures.json ]; then\n             echo \"`ci_results_${{ inputs.job }}/new_failures.json` exists, continue ...\"\n             echo \"process=true\" >> $GITHUB_ENV\n+            echo \"process=true\" >> $GITHUB_OUTPUT\n           else\n             echo \"`ci_results_${{ inputs.job }}/new_failures.json` doesn't exist, abort.\"\n             echo \"process=false\" >> $GITHUB_ENV\n+            echo \"process=false\" >> $GITHUB_OUTPUT\n           fi\n \n       - uses: actions/download-artifact@v4\n@@ -118,6 +126,10 @@ jobs:\n         run: |\n           python3 utils/print_env.py\n \n+      - name: Install pytest-flakefinder\n+        if: ${{ env.process == 'true' }}\n+        run: python3 -m pip install pytest-flakefinder\n+\n       - name: Show installed libraries and their versions\n         working-directory: /transformers\n         if: ${{ env.process == 'true' }}\n@@ -126,25 +138,63 @@ jobs:\n       - name: Check failed tests\n         working-directory: /transformers\n         if: ${{ env.process == 'true' }}\n-        run: python3 utils/check_bad_commit.py --start_commit ${{ inputs.start_sha }} --end_commit ${{ env.END_SHA }} --file ci_results_${{ inputs.job }}/new_failures.json --output_file new_failures_with_bad_commit.json\n+        run: python3 utils/check_bad_commit.py --start_commit ${{ inputs.start_sha }} --end_commit ${{ env.END_SHA }} --file ci_results_${{ inputs.job }}/new_failures.json --output_file new_failures_with_bad_commit_${{ inputs.job }}_${{ matrix.run_idx }}.json\n \n       - name: Show results\n         working-directory: /transformers\n         if: ${{ env.process == 'true' }}\n         run: |\n-          ls -l new_failures_with_bad_commit.json\n-          cat new_failures_with_bad_commit.json\n+          ls -l new_failures_with_bad_commit_${{ inputs.job }}_${{ matrix.run_idx }}.json\n+          cat new_failures_with_bad_commit_${{ inputs.job }}_${{ matrix.run_idx }}.json\n+\n+      - name: Upload artifacts\n+        uses: actions/upload-artifact@v4\n+        with:\n+          name: new_failures_with_bad_commit_${{ inputs.job }}_${{ matrix.run_idx }}\n+          path: /transformers/new_failures_with_bad_commit_${{ inputs.job }}_${{ matrix.run_idx }}.json\n+\n+  process_new_failures_with_commit_info:\n+    name: \"process bad commit reports\"\n+    needs: check_new_failures\n+    if: needs.check_new_failures.outputs.process == 'true'\n+    runs-on:\n+      group: aws-g5-4xlarge-cache\n+    container:\n+      image: ${{ inputs.docker }}\n+      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n+    steps:\n+      - uses: actions/download-artifact@v4\n+        with:\n+          name: ci_results_${{ inputs.job }}\n+          path: /transformers/ci_results_${{ inputs.job }}\n \n-      - name: Checkout back\n+      - uses: actions/download-artifact@v4\n+        with:\n+          pattern: new_failures_with_bad_commit_${{ inputs.job }}*\n+          path: /transformers/new_failures_with_bad_commit_${{ inputs.job }}\n+          merge-multiple: true\n+\n+      - name: Check files\n+        working-directory: /transformers\n+        run: |\n+          ls -la /transformers\n+          ls -la /transformers/new_failures_with_bad_commit_${{ inputs.job }}\n+\n+      # Currently, we only run with a single runner by using `run_idx: [1]`. We might try to run with multiple runners\n+      # to further reduce the false positive caused by flaky tests, which requires further processing to merge reports.\n+      - name: Merge files\n+        shell: bash\n         working-directory: /transformers\n-        if: ${{ env.process == 'true' }}\n         run: |\n-          git checkout ${{ inputs.start_sha }}\n+          cp /transformers/new_failures_with_bad_commit_${{ inputs.job }}/new_failures_with_bad_commit_${{ inputs.job }}_1.json new_failures_with_bad_commit.json\n+\n+      - name: Update clone\n+        working-directory: /transformers\n+        run: git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n \n       - name: Process report\n         shell: bash\n         working-directory: /transformers\n-        if: ${{ env.process == 'true' }}\n         env:\n           ACCESS_REPO_INFO_TOKEN: ${{ secrets.ACCESS_REPO_INFO_TOKEN }}\n           TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN: ${{ secrets.TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN }}\n@@ -156,7 +206,6 @@ jobs:\n       - name: Process report\n         shell: bash\n         working-directory: /transformers\n-        if: ${{ env.process == 'true' }}\n         env:\n           ACCESS_REPO_INFO_TOKEN: ${{ secrets.ACCESS_REPO_INFO_TOKEN }}\n           TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN: ${{ secrets.TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN }}\n@@ -171,13 +220,12 @@ jobs:\n \n       - name: Prepare Slack report title\n         working-directory: /transformers\n-        if: ${{ env.process == 'true' }}\n         run: |\n           pip install slack_sdk\n           echo \"title=$(python3 -c 'import sys; sys.path.append(\"utils\"); from utils.notification_service import job_to_test_map; ci_event = \"${{ inputs.ci_event }}\"; job = \"${{ inputs.job }}\"; test_name = job_to_test_map[job]; title = f\"New failed tests of {ci_event}\" + \":\" + f\" {test_name}\"; print(title)')\" >> $GITHUB_ENV\n \n       - name: Send processed report\n-        if: ${{ env.process == 'true' && !endsWith(env.REPORT_TEXT, '{}') }}\n+        if: ${{ !endsWith(env.REPORT_TEXT, '{}') }}\n         uses: slackapi/slack-github-action@6c661ce58804a1a20f6dc5fbee7f0381b469e001\n         with:\n           # Slack channel id, channel name, or user id to post message."
        },
        {
            "sha": "8c053c578d9188d914ff9ef682e17023c921c3ec",
            "filename": "utils/check_bad_commit.py",
            "status": "modified",
            "additions": 70,
            "deletions": 13,
            "changes": 83,
            "blob_url": "https://github.com/huggingface/transformers/blob/307c5238546ba1675daabc46050c63ffde25f8e6/utils%2Fcheck_bad_commit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/307c5238546ba1675daabc46050c63ffde25f8e6/utils%2Fcheck_bad_commit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_bad_commit.py?ref=307c5238546ba1675daabc46050c63ffde25f8e6",
            "patch": "@@ -20,6 +20,7 @@\n import re\n import subprocess\n \n+import git\n import requests\n \n \n@@ -38,26 +39,35 @@ def create_script(target_test):\n import os\n import subprocess\n \n+_ = subprocess.run(\n+    [\"python3\", \"-m\", \"pip\", \"install\", \"-e\", \".\"],\n+    capture_output = True,\n+    text=True,\n+)\n+\n result = subprocess.run(\n-    [\"python3\", \"-m\", \"pytest\", \"-v\", \"-rfEp\", f\"{target_test}\"],\n+    [\"python3\", \"-m\", \"pytest\", \"-v\", \"--flake-finder\", \"--flake-runs=4\", \"-rfEp\", f\"{target_test}\"],\n     capture_output = True,\n     text=True,\n )\n print(result.stdout)\n \n if f\"FAILED {target_test}\" in result.stdout:\n     print(\"test failed\")\n-    exit(2)\n+    exit(1)\n elif result.returncode != 0:\n     if \"ERROR: file or directory not found: \" in result.stderr:\n         print(\"test file or directory not found in this commit\")\n+        # git bisect treats exit code 125 as `test not found`. But this causes it not be able to make the conclusion\n+        # if a test is added between the `good commit` (exclusive) and `bad commit` (inclusive) (in git bisect terminology).\n+        # So we return 0 here in order to allow the process being able to identify the first commit that fails the test.\n         exit(0)\n     elif \"ERROR: not found: \" in result.stderr:\n         print(\"test not found in this commit\")\n         exit(0)\n     else:\n         print(f\"pytest gets unknown error: {{result.stderr}}\")\n-        exit(-1)\n+        exit(1)\n \n print(f\"pytest runs successfully.\")\n exit(0)\n@@ -67,20 +77,63 @@ def create_script(target_test):\n         fp.write(script.strip())\n \n \n+def is_bad_commit(target_test, commit):\n+    repo = git.Repo(\".\")  # or specify path to your repo\n+\n+    # Save the current HEAD reference\n+    original_head = repo.head.commit\n+\n+    # Checkout to the commit\n+    repo.git.checkout(commit)\n+\n+    create_script(target_test=target_test)\n+\n+    result = subprocess.run(\n+        [\"python3\", \"target_script.py\"],\n+        capture_output=True,\n+        text=True,\n+    )\n+\n+    # Restore to original commit\n+    repo.git.checkout(original_head)\n+\n+    return result.returncode != 0\n+\n+\n def find_bad_commit(target_test, start_commit, end_commit):\n-    \"\"\"Find (backward) the earliest commit between `start_commit` and `end_commit` at which `target_test` fails.\n+    \"\"\"Find (backward) the earliest commit between `start_commit` (inclusive) and `end_commit` (exclusive) at which `target_test` fails.\n \n     Args:\n         target_test (`str`): The test to check.\n-        start_commit (`str`): The latest commit.\n-        end_commit (`str`): The earliest commit.\n+        start_commit (`str`): The latest commit (inclusive).\n+        end_commit (`str`): The earliest commit (exclusive).\n \n     Returns:\n         `str`: The earliest commit at which `target_test` fails.\n     \"\"\"\n \n+    # check if `end_commit` fails the test\n+    failed_before = is_bad_commit(target_test, end_commit)\n+    if failed_before:\n+        return (\n+            None,\n+            f\"flaky: test passed in the previous run (commit: {end_commit}) but failed (on the same commit) during the check of the current run.\",\n+        )\n+\n+    # if there is no new commit (e.g. 2 different CI runs on the same commit):\n+    #   - failed once on `start_commit` but passed on `end_commit`, which are the same commit --> flaky (or something change externally) --> don't report\n     if start_commit == end_commit:\n-        return start_commit\n+        return (\n+            None,\n+            f\"flaky: test fails on the current CI run but passed in the previous run which is running on the same commit {end_commit}.\",\n+        )\n+\n+    # Now, we are (almost) sure `target_test` is not failing at `end_commit`\n+    # check if `start_commit` fail the test\n+    failed_now = is_bad_commit(target_test, start_commit)\n+    if not failed_now:\n+        # failed on CI run, but not reproducible here --> don't report\n+        return None, f\"flaky: test fails on the current CI run (commit: {start_commit}) but passes during the check.\"\n \n     create_script(target_test=target_test)\n \n@@ -105,7 +158,7 @@ def find_bad_commit(target_test, start_commit, end_commit):\n     if \"error: bisect run failed\" in result.stderr:\n         error_msg = f\"Error when running git bisect:\\nbash error: {result.stderr}\\nbash output:\\n{result.stdout}\\nset `bad_commit` to `None`.\"\n         print(error_msg)\n-        return None\n+        return None, \"git bisect failed\"\n \n     pattern = r\"(.+) is the first bad commit\"\n     commits = re.findall(pattern, result.stdout)\n@@ -117,7 +170,7 @@ def find_bad_commit(target_test, start_commit, end_commit):\n     print(f\"Between `start_commit` {start_commit} and `end_commit` {end_commit}\")\n     print(f\"bad_commit: {bad_commit}\\n\")\n \n-    return bad_commit\n+    return bad_commit, \"git bisect found the bad commit.\"\n \n \n def get_commit_info(commit):\n@@ -171,9 +224,11 @@ def get_commit_info(commit):\n         raise ValueError(\"Exactly one argument `test` or `file` must be specified.\")\n \n     if args.test is not None:\n-        commit = find_bad_commit(target_test=args.test, start_commit=args.start_commit, end_commit=args.end_commit)\n+        commit, status = find_bad_commit(\n+            target_test=args.test, start_commit=args.start_commit, end_commit=args.end_commit\n+        )\n         with open(args.output_file, \"w\", encoding=\"UTF-8\") as fp:\n-            fp.write(f\"{args.test}\\n{commit}\")\n+            fp.write(f\"{args.test}\\n{commit}\\n{status}\")\n     elif os.path.isfile(args.file):\n         with open(args.file, \"r\", encoding=\"UTF-8\") as fp:\n             reports = json.load(fp)\n@@ -185,8 +240,10 @@ def get_commit_info(commit):\n \n             failed_tests_with_bad_commits = []\n             for test in failed_tests:\n-                commit = find_bad_commit(target_test=test, start_commit=args.start_commit, end_commit=args.end_commit)\n-                info = {\"test\": test, \"commit\": commit}\n+                commit, status = find_bad_commit(\n+                    target_test=test, start_commit=args.start_commit, end_commit=args.end_commit\n+                )\n+                info = {\"test\": test, \"commit\": commit, \"status\": status}\n \n                 if commit in commit_info_cache:\n                     commit_info = commit_info_cache[commit]"
        },
        {
            "sha": "9bf0982501317acd379d76dd0502be99bf2351c0",
            "filename": "utils/process_bad_commit_report.py",
            "status": "modified",
            "additions": 27,
            "deletions": 19,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/307c5238546ba1675daabc46050c63ffde25f8e6/utils%2Fprocess_bad_commit_report.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/307c5238546ba1675daabc46050c63ffde25f8e6/utils%2Fprocess_bad_commit_report.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fprocess_bad_commit_report.py?ref=307c5238546ba1675daabc46050c63ffde25f8e6",
            "patch": "@@ -26,6 +26,33 @@\n \n     job_name = os.environ.get(\"JOB_NAME\")\n \n+    # Upload to Hub and get the url\n+    # if it is not a scheduled run, upload the reports to a subfolder under `report_repo_folder`\n+    report_repo_subfolder = \"\"\n+    if os.getenv(\"GITHUB_EVENT_NAME\") != \"schedule\":\n+        report_repo_subfolder = f\"{os.getenv('GITHUB_RUN_NUMBER')}-{os.getenv('GITHUB_RUN_ID')}\"\n+        report_repo_subfolder = f\"runs/{report_repo_subfolder}\"\n+\n+    workflow_run = get_last_daily_ci_run(\n+        token=os.environ[\"ACCESS_REPO_INFO_TOKEN\"], workflow_run_id=os.getenv(\"GITHUB_RUN_ID\")\n+    )\n+    workflow_run_created_time = workflow_run[\"created_at\"]\n+\n+    report_repo_folder = workflow_run_created_time.split(\"T\")[0]\n+\n+    if report_repo_subfolder:\n+        report_repo_folder = f\"{report_repo_folder}/{report_repo_subfolder}\"\n+\n+    report_repo_id = os.getenv(\"REPORT_REPO_ID\")\n+\n+    commit_info = api.upload_file(\n+        path_or_fileobj=\"new_failures_with_bad_commit.json\",\n+        path_in_repo=f\"{report_repo_folder}/ci_results_{job_name}/new_failures_with_bad_commit.json\",\n+        repo_id=report_repo_id,\n+        repo_type=\"dataset\",\n+        token=os.environ.get(\"TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN\", None),\n+    )\n+\n     with open(\"new_failures_with_bad_commit.json\") as fp:\n         data = json.load(fp)\n \n@@ -88,25 +115,6 @@\n             _data[model] = {k: v for k, v in model_result.items() if len(v) > 0}\n         new_data_full[author] = {k: v for k, v in _data.items() if len(v) > 0}\n \n-    # Upload to Hub and get the url\n-    # if it is not a scheduled run, upload the reports to a subfolder under `report_repo_folder`\n-    report_repo_subfolder = \"\"\n-    if os.getenv(\"GITHUB_EVENT_NAME\") != \"schedule\":\n-        report_repo_subfolder = f\"{os.getenv('GITHUB_RUN_NUMBER')}-{os.getenv('GITHUB_RUN_ID')}\"\n-        report_repo_subfolder = f\"runs/{report_repo_subfolder}\"\n-\n-    workflow_run = get_last_daily_ci_run(\n-        token=os.environ[\"ACCESS_REPO_INFO_TOKEN\"], workflow_run_id=os.getenv(\"GITHUB_RUN_ID\")\n-    )\n-    workflow_run_created_time = workflow_run[\"created_at\"]\n-\n-    report_repo_folder = workflow_run_created_time.split(\"T\")[0]\n-\n-    if report_repo_subfolder:\n-        report_repo_folder = f\"{report_repo_folder}/{report_repo_subfolder}\"\n-\n-    report_repo_id = os.getenv(\"REPORT_REPO_ID\")\n-\n     with open(\"new_failures_with_bad_commit_grouped_by_authors.json\", \"w\") as fp:\n         json.dump(new_data_full, fp, ensure_ascii=False, indent=4)\n     commit_info = api.upload_file("
        }
    ],
    "stats": {
        "total": 199,
        "additions": 156,
        "deletions": 43
    }
}