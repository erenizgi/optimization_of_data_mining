{
    "author": "alex-jw-brooks",
    "message": "Fix Bad Outputs in Fast Path for GraniteMoeHybrid (#39033)\n\nFix bug in previous state setting",
    "sha": "5995cfa0a07de86e3c53fe1f57378c956a5d03db",
    "files": [
        {
            "sha": "12c6e52c65b3231d7fe59f929f8f506a9c5971af",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5995cfa0a07de86e3c53fe1f57378c956a5d03db/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5995cfa0a07de86e3c53fe1f57378c956a5d03db/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=5995cfa0a07de86e3c53fe1f57378c956a5d03db",
            "patch": "@@ -867,7 +867,6 @@ def torch_forward(\n             # Init cache\n             if ssm_state is not None and cache_params is not None:\n                 cache_params.ssm_states[self.layer_idx].copy_(ssm_state)\n-                cache_params.has_previous_state = True\n \n         scan_output = self.norm(y, gate)\n "
        },
        {
            "sha": "f5f6d8be8713b297984a53d6b460c6c2a81f63a7",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5995cfa0a07de86e3c53fe1f57378c956a5d03db/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5995cfa0a07de86e3c53fe1f57378c956a5d03db/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=5995cfa0a07de86e3c53fe1f57378c956a5d03db",
            "patch": "@@ -666,7 +666,6 @@ def torch_forward(\n             # Init cache\n             if ssm_state is not None and cache_params is not None:\n                 cache_params.ssm_states[self.layer_idx].copy_(ssm_state)\n-                cache_params.has_previous_state = True\n \n         scan_output = self.norm(y, gate)\n "
        },
        {
            "sha": "ffdb7cf04aff51d2be7c12c95d770f87cb8654f1",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5995cfa0a07de86e3c53fe1f57378c956a5d03db/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5995cfa0a07de86e3c53fe1f57378c956a5d03db/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=5995cfa0a07de86e3c53fe1f57378c956a5d03db",
            "patch": "@@ -794,7 +794,6 @@ def torch_forward(\n             # Init cache\n             if ssm_state is not None and cache_params is not None:\n                 cache_params.ssm_states[self.layer_idx].copy_(ssm_state)\n-                cache_params.has_previous_state = True\n \n         scan_output = self.norm(y, gate)\n \n@@ -1376,6 +1375,9 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n+        if past_key_values and not past_key_values.has_previous_state:\n+            past_key_values.has_previous_state = True\n+\n         next_cache = next_decoder_cache if use_cache else None\n \n         return MoeModelOutputWithPast("
        },
        {
            "sha": "fb49cf29b37a7b622e58bf345631a51fdbfdf89c",
            "filename": "src/transformers/models/granitemoehybrid/modular_granitemoehybrid.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5995cfa0a07de86e3c53fe1f57378c956a5d03db/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5995cfa0a07de86e3c53fe1f57378c956a5d03db/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py?ref=5995cfa0a07de86e3c53fe1f57378c956a5d03db",
            "patch": "@@ -301,6 +301,9 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n+        if past_key_values and not past_key_values.has_previous_state:\n+            past_key_values.has_previous_state = True\n+\n         next_cache = next_decoder_cache if use_cache else None\n \n         return MoeModelOutputWithPast("
        }
    ],
    "stats": {
        "total": 9,
        "additions": 6,
        "deletions": 3
    }
}