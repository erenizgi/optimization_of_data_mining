{
    "author": "Cyrilvallez",
    "message": "[modular] Fix the prefix-based renaming if the old and new model share a common name suffix (#37829)\n\n* first try\n\n* Fix and set examples\n\n* style\n\n* fix\n\n* Update modular_test_detr.py\n\n* Update image_processing_new_imgproc_model.py\n\n* Update modular_model_converter.py",
    "sha": "4602059aaee7b075ca51c21941ab2d27980bef7c",
    "files": [
        {
            "sha": "c8eae17dadcef78d5d97e0981be4f487c01c7988",
            "filename": "examples/modular-transformers/image_processing_new_imgproc_model.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fimage_processing_new_imgproc_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fimage_processing_new_imgproc_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fimage_processing_new_imgproc_model.py?ref=4602059aaee7b075ca51c21941ab2d27980bef7c",
            "patch": "@@ -4,7 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_new_imgproc_model.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-from typing import Optional, Union\n+from typing import Dict, List, Optional, Union\n \n import numpy as np\n import torch\n@@ -74,13 +74,13 @@ class ImgprocModelImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n         do_convert_rgb: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -101,7 +101,7 @@ def __init__(\n     def resize(\n         self,\n         image: np.ndarray,\n-        size: dict[str, int],\n+        size: Dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -151,13 +151,13 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         do_convert_rgb: Optional[bool] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,"
        },
        {
            "sha": "acf140f025d93422bef5e717c2d8c6565641ce2c",
            "filename": "examples/modular-transformers/modeling_add_function.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodeling_add_function.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodeling_add_function.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_add_function.py?ref=4602059aaee7b075ca51c21941ab2d27980bef7c",
            "patch": "@@ -5,7 +5,7 @@\n #                          modular_add_function.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # Note that zamba does not have the `apply_rotary_pos_emb` function!\n-from typing import Optional\n+from typing import Optional, Tuple\n \n import torch\n from torch import nn\n@@ -62,5 +62,5 @@ class TestAttention(nn.Module):\n     def __init__(self):\n         pass\n \n-    def forward(self) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    def forward(self) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         _ = apply_rotary_pos_emb(1, 1, 1, 1)"
        },
        {
            "sha": "e513e274973fa3258e49b95b895cfe92813db20c",
            "filename": "examples/modular-transformers/modeling_dummy.py",
            "status": "modified",
            "additions": 68,
            "deletions": 95,
            "changes": 163,
            "blob_url": "https://github.com/huggingface/transformers/blob/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy.py?ref=4602059aaee7b075ca51c21941ab2d27980bef7c",
            "patch": "@@ -4,27 +4,41 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_dummy.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-from functools import partial\n-from typing import Callable, Optional, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n from torch import nn\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n+    is_torch_flex_attn_available,\n+    logging,\n+)\n from .configuration_dummy import DummyConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class DummyRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -63,45 +77,18 @@ def __init__(self, config: DummyConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-\n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n@@ -223,12 +210,12 @@ def __init__(self, config: DummyConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -245,6 +232,7 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n+\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n                 logger.warning_once(\n@@ -270,7 +258,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class DummyDecoderLayer(nn.Module):\n+class DummyDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: DummyConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -290,11 +278,10 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n@@ -369,6 +356,8 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, DummyRMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n DUMMY_INPUTS_DOCSTRING = r\"\"\"\n@@ -381,12 +370,15 @@ def _init_weights(self, module):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -406,20 +398,12 @@ def _init_weights(self, module):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n@@ -480,27 +464,26 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(DUMMY_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -511,6 +494,10 @@ def forward(\n             )\n             use_cache = False\n \n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n@@ -543,30 +530,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n \n@@ -579,26 +553,29 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n@@ -616,7 +593,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -633,15 +610,14 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n \n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n@@ -658,7 +634,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -678,8 +653,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -691,11 +664,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "5543a60c8d3bab3b51f70c86c825ce0d2ca97250",
            "filename": "examples/modular-transformers/modeling_dummy_bert.py",
            "status": "modified",
            "additions": 53,
            "deletions": 11,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py?ref=4602059aaee7b075ca51c21941ab2d27980bef7c",
            "patch": "@@ -6,7 +6,7 @@\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n import math\n import os\n-from typing import Optional, Union\n+from typing import Optional, Tuple, Union\n \n import torch\n from packaging import version\n@@ -136,9 +136,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor]:\n+    ) -> Tuple[torch.Tensor]:\n         mixed_query_layer = self.query(hidden_states)\n \n         # If this is instantiated as a cross-attention module, the keys\n@@ -245,9 +245,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor]:\n+    ) -> Tuple[torch.Tensor]:\n         if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n             logger.warning_once(\n@@ -386,9 +386,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor]:\n+    ) -> Tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask,\n@@ -454,9 +454,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor]:\n+    ) -> Tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n         self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n@@ -532,12 +532,12 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -626,6 +626,46 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return pooled_output\n \n \n+class DummyBertPredictionHeadTransform(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n+        if isinstance(config.hidden_act, str):\n+            self.transform_act_fn = ACT2FN[config.hidden_act]\n+        else:\n+            self.transform_act_fn = config.hidden_act\n+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.dense(hidden_states)\n+        hidden_states = self.transform_act_fn(hidden_states)\n+        hidden_states = self.LayerNorm(hidden_states)\n+        return hidden_states\n+\n+\n+class DummyBertLMPredictionHead(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.transform = DummyBertPredictionHeadTransform(config)\n+\n+        # The output weights are the same as the input embeddings, but there is\n+        # an output-only bias for each token.\n+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n+\n+        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n+        self.decoder.bias = self.bias\n+\n+    def _tie_weights(self):\n+        self.decoder.bias = self.bias\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.transform(hidden_states)\n+        hidden_states = self.decoder(hidden_states)\n+        return hidden_states\n+\n+\n def load_tf_weights_in_dummy_bert(model, config, tf_checkpoint_path):\n     \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n     try:\n@@ -726,6 +766,8 @@ def _init_weights(self, module):\n         elif isinstance(module, nn.LayerNorm):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n+        elif isinstance(module, DummyBertLMPredictionHead):\n+            module.bias.data.zero_()\n \n \n DUMMY_BERT_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "c7c57ae08ec2fdf3ae2cf5cc73cfc5c25f6b3d26",
            "filename": "examples/modular-transformers/modeling_from_uppercase_model.py",
            "status": "modified",
            "additions": 67,
            "deletions": 236,
            "changes": 303,
            "blob_url": "https://github.com/huggingface/transformers/blob/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodeling_from_uppercase_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodeling_from_uppercase_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_from_uppercase_model.py?ref=4602059aaee7b075ca51c21941ab2d27980bef7c",
            "patch": "@@ -4,28 +4,48 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_from_uppercase_model.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-from typing import Optional\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...pytorch_utils import is_torch_greater_or_equal_than_2_2\n-from ...utils import is_flash_attn_2_available, is_flash_attn_greater_or_equal_2_10, logging\n-from .configuration_from_uppercase_model import FromUppercaseModelConfig\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...utils import logging\n+from .configuration_from_uppercase_model import FromUppercaseModelTextConfig, FromUppercaseModelVisionConfig\n \n \n-if is_flash_attn_2_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n+logger = logging.get_logger(__name__)\n \n \n-logger = logging.get_logger(__name__)\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    output_attentions: bool = True,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+    if not output_attentions:\n+        attn_weights = None\n+    return attn_output, attn_weights\n \n \n class FromUppercaseModelAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config):\n+    def __init__(self, config: Union[FromUppercaseModelVisionConfig, FromUppercaseModelTextConfig]):\n         super().__init__()\n         self.config = config\n         self.embed_dim = config.hidden_size\n@@ -38,253 +58,71 @@ def __init__(self, config):\n             )\n         self.scale = self.head_dim**-0.5\n         self.dropout = config.attention_dropout\n+        self.is_causal = False\n \n         self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n-        bsz, tgt_len, embed_dim = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scale\n-        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+        batch_size, seq_length, embed_dim = hidden_states.shape\n \n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.view(*proj_shape)\n-        value_states = value_states.view(*proj_shape)\n+        queries = self.q_proj(hidden_states)\n+        keys = self.k_proj(hidden_states)\n+        values = self.v_proj(hidden_states)\n \n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        # apply the causal_attention_mask first\n-        if causal_attention_mask is not None:\n-            if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is\"\n-                    f\" {causal_attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if output_attentions:\n-            # this operation is a bit akward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n+        queries = queries.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n+        keys = keys.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n+        values = values.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n+        # FROM_UPPERCASE_MODEL text model uses both `causal_attention_mask` and `attention_mask`\n+        # in case FA2 kernel is called, `is_causal` should be inferred from `causal_attention_mask`\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            self.is_causal = causal_attention_mask is not None\n         else:\n-            attn_weights_reshaped = None\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-\n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights_reshaped\n-\n-\n-class FromUppercaseModelFlashAttention2(FromUppercaseModelAttention):\n-    \"\"\"\n-    FromUppercaseModelAttention flash attention module. This module inherits from `FromUppercaseModelAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n-\n-    # Adapted from transformers.models.llama.modeling_llama.LlamaFlashAttention2.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n-        output_attentions = False\n-\n-        batch_size, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n-        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n-        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n-\n-        dropout_rate = self.dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32.\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+            if attention_mask is not None and causal_attention_mask is not None:\n+                attention_mask = attention_mask + causal_attention_mask\n+            elif causal_attention_mask is not None:\n+                attention_mask = causal_attention_mask\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n-            query_states,\n-            key_states,\n-            value_states,\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            queries,\n+            keys,\n+            values,\n             attention_mask,\n-            q_len,\n-            dropout=dropout_rate,\n-            is_causal=causal_attention_mask is not None,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+            is_causal=self.is_causal,\n+            scaling=self.scale,\n+            dropout=0.0 if not self.training else self.dropout,\n+            output_attentions=output_attentions,\n         )\n \n-        attn_output = attn_output.reshape(batch_size, q_len, self.embed_dim).contiguous()\n+        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n         if not output_attentions:\n             attn_weights = None\n-\n         return attn_output, attn_weights\n \n \n-class FromUppercaseModelSdpaAttention(FromUppercaseModelAttention):\n-    \"\"\"\n-    SDPA attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `FromUppercaseModelAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from FromUppercaseModelAttention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"FromUppercaseModelModel is using FromUppercaseModelSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not \"\n-                \"support `output_attentions=True`. Falling back to the manual attention implementation, but specifying \"\n-                \"the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can \"\n-                'be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                causal_attention_mask=causal_attention_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-        # FROM_UPPERCASE_MODEL text model uses both `causal_attention_mask` and `attention_mask`\n-        if attention_mask is not None and causal_attention_mask is not None:\n-            attn_mask = attention_mask + causal_attention_mask\n-        elif causal_attention_mask is not None:\n-            attn_mask = causal_attention_mask\n-        else:\n-            attn_mask = attention_mask\n-\n-        bsz, tgt_len, embed_dim = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if not is_torch_greater_or_equal_than_2_2 and query_states.device.type == \"cuda\" and attn_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # FROM_UPPERCASE_MODEL text model uses both `causal_attention_mask` and `attention_mask` sequentially.\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=attn_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n-            scale=self.scale,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2)\n-        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, None\n-\n-\n class FromUppercaseModelMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -300,18 +138,11 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-FROM_UPPERCASE_MODEL_ATTENTION_CLASSES = {\n-    \"eager\": FromUppercaseModelAttention,\n-    \"sdpa\": FromUppercaseModelSdpaAttention,\n-    \"flash_attention_2\": FromUppercaseModelFlashAttention2,\n-}\n-\n-\n class FromUppercaseModelEncoderLayer(nn.Module):\n-    def __init__(self, config: FromUppercaseModelConfig):\n+    def __init__(self, config: Union[FromUppercaseModelVisionConfig, FromUppercaseModelTextConfig]):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n-        self.self_attn = FROM_UPPERCASE_MODEL_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.self_attn = FromUppercaseModelAttention(config)\n         self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n         self.mlp = FromUppercaseModelMLP(config)\n         self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n@@ -322,7 +153,7 @@ def forward(\n         attention_mask: torch.Tensor,\n         causal_attention_mask: torch.Tensor,\n         output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.FloatTensor]:\n+    ) -> Tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`"
        },
        {
            "sha": "5f1a083321c10028d6c5fbe8603eb725fc1714a2",
            "filename": "examples/modular-transformers/modeling_multimodal1.py",
            "status": "modified",
            "additions": 68,
            "deletions": 95,
            "changes": 163,
            "blob_url": "https://github.com/huggingface/transformers/blob/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py?ref=4602059aaee7b075ca51c21941ab2d27980bef7c",
            "patch": "@@ -4,27 +4,41 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_multimodal1.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-from functools import partial\n-from typing import Callable, Optional, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n from torch import nn\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n+    is_torch_flex_attn_available,\n+    logging,\n+)\n from .configuration_multimodal1 import Multimodal1TextConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class Multimodal1TextRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -63,45 +77,18 @@ def __init__(self, config: Multimodal1TextConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-\n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n@@ -223,12 +210,12 @@ def __init__(self, config: Multimodal1TextConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -245,6 +232,7 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n+\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n                 logger.warning_once(\n@@ -270,7 +258,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class Multimodal1TextDecoderLayer(nn.Module):\n+class Multimodal1TextDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Multimodal1TextConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -290,11 +278,10 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n@@ -369,6 +356,8 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, Multimodal1TextRMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n MULTIMODAL1_TEXT_INPUTS_DOCSTRING = r\"\"\"\n@@ -381,12 +370,15 @@ def _init_weights(self, module):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -406,20 +398,12 @@ def _init_weights(self, module):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n@@ -480,27 +464,26 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(MULTIMODAL1_TEXT_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -511,6 +494,10 @@ def forward(\n             )\n             use_cache = False\n \n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n@@ -543,30 +530,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n \n@@ -579,26 +553,29 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n@@ -616,7 +593,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -633,15 +610,14 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n \n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n@@ -658,7 +634,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -678,8 +653,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -691,11 +664,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "4ef8b1e6842aa9c7eab7e753169df5824fe16b35",
            "filename": "examples/modular-transformers/modeling_multimodal2.py",
            "status": "modified",
            "additions": 152,
            "deletions": 250,
            "changes": 402,
            "blob_url": "https://github.com/huggingface/transformers/blob/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py?ref=4602059aaee7b075ca51c21941ab2d27980bef7c",
            "patch": "@@ -5,7 +5,7 @@\n #                          modular_multimodal2.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n \n-from typing import Optional, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n from torch import nn\n@@ -14,30 +14,48 @@\n \n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n-from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import is_torch_greater_or_equal_than_2_2\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...utils import (\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n     torch_int,\n )\n-from .configuration_multimodal2 import Multimodal2Config, Multimodal2VisionConfig\n+from .configuration_multimodal2 import Multimodal2Config, Multimodal2TextConfig, Multimodal2VisionConfig\n \n \n-if is_flash_attn_2_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n+logger = logging.get_logger(__name__)\n \n \n-logger = logging.get_logger(__name__)\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    output_attentions: bool = True,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+    if not output_attentions:\n+        attn_weights = None\n+    return attn_output, attn_weights\n \n \n class Multimodal2VisionAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config):\n+    def __init__(self, config: Union[Multimodal2VisionConfig, Multimodal2TextConfig]):\n         super().__init__()\n         self.config = config\n         self.embed_dim = config.hidden_size\n@@ -50,280 +68,172 @@ def __init__(self, config):\n             )\n         self.scale = self.head_dim**-0.5\n         self.dropout = config.attention_dropout\n+        self.is_causal = False\n \n         self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n-        bsz, tgt_len, embed_dim = hidden_states.size()\n+        batch_size, seq_length, embed_dim = hidden_states.shape\n \n-        # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scale\n-        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+        queries = self.q_proj(hidden_states)\n+        keys = self.k_proj(hidden_states)\n+        values = self.v_proj(hidden_states)\n \n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.view(*proj_shape)\n-        value_states = value_states.view(*proj_shape)\n+        queries = queries.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n+        keys = keys.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n+        values = values.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n+        # MULTIMODAL2_VISION text model uses both `causal_attention_mask` and `attention_mask`\n+        # in case FA2 kernel is called, `is_causal` should be inferred from `causal_attention_mask`\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            self.is_causal = causal_attention_mask is not None\n+        else:\n+            if attention_mask is not None and causal_attention_mask is not None:\n+                attention_mask = attention_mask + causal_attention_mask\n+            elif causal_attention_mask is not None:\n+                attention_mask = causal_attention_mask\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            queries,\n+            keys,\n+            values,\n+            attention_mask,\n+            is_causal=self.is_causal,\n+            scaling=self.scale,\n+            dropout=0.0 if not self.training else self.dropout,\n+            output_attentions=output_attentions,\n+        )\n \n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n+        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n+        attn_output = self.out_proj(attn_output)\n \n-        # apply the causal_attention_mask first\n-        if causal_attention_mask is not None:\n-            if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is\"\n-                    f\" {causal_attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n+        if not output_attentions:\n+            attn_weights = None\n+        return attn_output, attn_weights\n \n-        if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n \n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+class Multimodal2VisionMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.activation_fn = ACT2FN[config.hidden_act]\n+        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n+        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n \n-        if output_attentions:\n-            # this operation is a bit akward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states = self.activation_fn(hidden_states)\n+        hidden_states = self.fc2(hidden_states)\n+        return hidden_states\n \n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n \n-        attn_output = torch.bmm(attn_probs, value_states)\n+class Multimodal2Attention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n+    def __init__(self, config: Union[Multimodal2VisionConfig, Multimodal2TextConfig]):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = self.embed_dim // self.num_heads\n+        if self.head_dim * self.num_heads != self.embed_dim:\n             raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n+                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n+                f\" {self.num_heads}).\"\n             )\n+        self.scale = self.head_dim**-0.5\n+        self.dropout = config.attention_dropout\n+        self.is_causal = False\n \n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n-        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights_reshaped\n-\n-\n-class Multimodal2VisionSdpaAttention(Multimodal2VisionAttention):\n-    \"\"\"\n-    SDPA attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `Multimodal2VisionAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n+        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n \n-    # Adapted from Multimodal2VisionAttention.forward\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"Multimodal2VisionModel is using Multimodal2VisionSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not \"\n-                \"support `output_attentions=True`. Falling back to the manual attention implementation, but specifying \"\n-                \"the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can \"\n-                'be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                causal_attention_mask=causal_attention_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-        # MULTIMODAL2_VISION text model uses both `causal_attention_mask` and `attention_mask`\n-        if attention_mask is not None and causal_attention_mask is not None:\n-            attn_mask = attention_mask + causal_attention_mask\n-        elif causal_attention_mask is not None:\n-            attn_mask = causal_attention_mask\n-        else:\n-            attn_mask = attention_mask\n-\n-        bsz, tgt_len, embed_dim = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if not is_torch_greater_or_equal_than_2_2 and query_states.device.type == \"cuda\" and attn_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # MULTIMODAL2_VISION text model uses both `causal_attention_mask` and `attention_mask` sequentially.\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=attn_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n-            scale=self.scale,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2)\n-        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, None\n-\n-\n-class Multimodal2VisionFlashAttention2(Multimodal2VisionAttention):\n-    \"\"\"\n-    Multimodal2VisionAttention flash attention module. This module inherits from `Multimodal2VisionAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n+        batch_size, seq_length, embed_dim = hidden_states.shape\n \n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        queries = self.q_proj(hidden_states)\n+        keys = self.k_proj(hidden_states)\n+        values = self.v_proj(hidden_states)\n \n-    # Adapted from transformers.models.llama.modeling_llama.LlamaFlashAttention2.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n-        output_attentions = False\n-\n-        batch_size, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n-        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n-        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n-\n-        dropout_rate = self.dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32.\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+        queries = queries.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n+        keys = keys.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n+        values = values.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n+        # MULTIMODAL2 text model uses both `causal_attention_mask` and `attention_mask`\n+        # in case FA2 kernel is called, `is_causal` should be inferred from `causal_attention_mask`\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            self.is_causal = causal_attention_mask is not None\n+        else:\n+            if attention_mask is not None and causal_attention_mask is not None:\n+                attention_mask = attention_mask + causal_attention_mask\n+            elif causal_attention_mask is not None:\n+                attention_mask = causal_attention_mask\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n-            query_states,\n-            key_states,\n-            value_states,\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            queries,\n+            keys,\n+            values,\n             attention_mask,\n-            q_len,\n-            dropout=dropout_rate,\n-            is_causal=causal_attention_mask is not None,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+            is_causal=self.is_causal,\n+            scaling=self.scale,\n+            dropout=0.0 if not self.training else self.dropout,\n+            output_attentions=output_attentions,\n         )\n \n-        attn_output = attn_output.reshape(batch_size, q_len, self.embed_dim).contiguous()\n+        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n         if not output_attentions:\n             attn_weights = None\n-\n         return attn_output, attn_weights\n \n \n-class Multimodal2VisionMLP(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.config = config\n-        self.activation_fn = ACT2FN[config.hidden_act]\n-        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n-        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n-\n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        hidden_states = self.fc1(hidden_states)\n-        hidden_states = self.activation_fn(hidden_states)\n-        hidden_states = self.fc2(hidden_states)\n-        return hidden_states\n-\n-\n-MULTIMODAL2_VISION_ATTENTION_CLASSES = {\n-    \"eager\": Multimodal2VisionAttention,\n-    \"sdpa\": Multimodal2VisionSdpaAttention,\n-    \"flash_attention_2\": Multimodal2VisionFlashAttention2,\n-}\n-\n-\n class Multimodal2VisionEncoderLayer(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n-        self.self_attn = MULTIMODAL2_VISION_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.self_attn = Multimodal2Attention(config)\n         self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n         self.mlp = Multimodal2VisionMLP(config)\n         self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n@@ -334,7 +244,7 @@ def forward(\n         attention_mask: torch.Tensor,\n         causal_attention_mask: torch.Tensor,\n         output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.FloatTensor]:\n+    ) -> Tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -384,15 +294,15 @@ def __init__(self, config):\n         self.layers = nn.ModuleList([Multimodal2VisionEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n+    @can_return_tuple\n     def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, BaseModelOutput]:\n+    ) -> BaseModelOutput:\n         r\"\"\"\n         Args:\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n@@ -426,7 +336,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n@@ -459,10 +368,10 @@ def forward(\n         if output_hidden_states:\n             encoder_states = encoder_states + (hidden_states,)\n \n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n         return BaseModelOutput(\n-            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n+            last_hidden_state=hidden_states,\n+            hidden_states=encoder_states,\n+            attentions=all_attentions,\n         )\n \n \n@@ -578,16 +487,16 @@ def __init__(self, config):\n         self.encoder = Multimodal2VisionEncoder(config)\n         self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(MULTIMODAL2_VISION_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=Multimodal2VisionConfig)\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = False,\n-    ) -> Union[tuple, BaseModelOutputWithPooling]:\n+    ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Returns:\n \n@@ -596,28 +505,23 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n         hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n         hidden_states = self.pre_layrnorm(hidden_states)\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = encoder_outputs.last_hidden_state\n         pooled_output = last_hidden_state[:, 0, :]\n         pooled_output = self.post_layernorm(pooled_output)\n \n-        if not return_dict:\n-            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n-\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n@@ -662,6 +566,7 @@ def __init__(self, config: Multimodal2VisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(MULTIMODAL2_VISION_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=Multimodal2VisionConfig)\n     def forward(\n@@ -670,8 +575,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, BaseModelOutputWithPooling]:\n+    ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Returns:\n \n@@ -694,12 +598,10 @@ def forward(\n         >>> last_hidden_state = outputs.last_hidden_state\n         >>> pooled_output = outputs.pooler_output  # pooled CLS states\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         return self.vision_model(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n         )"
        },
        {
            "sha": "df794c7873f3d97e8e00738b1fa8c0c65ba52eed",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 68,
            "deletions": 106,
            "changes": 174,
            "blob_url": "https://github.com/huggingface/transformers/blob/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=4602059aaee7b075ca51c21941ab2d27980bef7c",
            "patch": "@@ -4,7 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_my_new_model2.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-from typing import Callable, Optional, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n from torch import nn\n@@ -13,14 +13,27 @@\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, SequenceClassifierOutputWithPast\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n+    is_torch_flex_attn_available,\n+    logging,\n+)\n from .configuration_my_new_model2 import MyNewModel2Config\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -78,45 +91,18 @@ def __init__(self, config: MyNewModel2Config, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-\n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n@@ -222,12 +208,12 @@ def __init__(self, config: MyNewModel2Config, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -244,6 +230,7 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n+\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n                 logger.warning_once(\n@@ -269,7 +256,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class MyNewModel2DecoderLayer(nn.Module):\n+class MyNewModel2DecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: MyNewModel2Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -289,11 +276,10 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n@@ -368,6 +354,8 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, MyNewModel2RMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n MY_NEW_MODEL2_INPUTS_DOCSTRING = r\"\"\"\n@@ -380,12 +368,15 @@ def _init_weights(self, module):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -405,20 +396,12 @@ def _init_weights(self, module):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n@@ -479,27 +462,26 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(MY_NEW_MODEL2_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,  # NOOP kwarg for now\n-    ) -> Union[tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -549,29 +531,16 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+            )\n \n             hidden_states = layer_outputs[0]\n \n@@ -584,26 +553,29 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n@@ -621,7 +593,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -638,15 +610,14 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n \n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n@@ -663,7 +634,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -683,8 +653,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -696,11 +664,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n@@ -747,29 +715,28 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(MY_NEW_MODEL2_INPUTS_DOCSTRING)\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, SequenceClassifierOutputWithPast]:\n+    ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        transformer_outputs = self.model(\n+        transformer_outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -778,9 +745,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        hidden_states = transformer_outputs[0]\n+        hidden_states = transformer_outputs.last_hidden_state\n         logits = self.score(hidden_states)\n \n         if input_ids is not None:\n@@ -795,7 +761,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1\n@@ -810,10 +776,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        if not return_dict:\n-            output = (pooled_logits,) + transformer_outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,"
        },
        {
            "sha": "8da71ab17095dd5c86233e7c4b9119d2da4ed4c6",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 25,
            "deletions": 31,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=4602059aaee7b075ca51c21941ab2d27980bef7c",
            "patch": "@@ -5,7 +5,7 @@\n #                          modular_new_task_model.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n from dataclasses import dataclass\n-from typing import ClassVar, Optional, Union\n+from typing import ClassVar, List, Optional, Tuple, Union\n \n import torch\n from torch import nn\n@@ -59,10 +59,10 @@ class NewTaskModelCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n-    past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    logits: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n@@ -113,23 +113,12 @@ class NewTaskModelPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         # important: this ported version of NewTaskModelisn't meant for training from scratch - only\n         # inference and fine-tuning\n-        std = (\n-            self.config.initializer_range\n-            if hasattr(self.config, \"initializer_range\")\n-            else self.config.text_config.initializer_range\n-        )\n-\n-        if hasattr(module, \"class_embedding\"):\n-            module.class_embedding.data.normal_(mean=0.0, std=std)\n+        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+        if isinstance(module, nn.Linear):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n \n \n NEW_TASK_MODEL_INPUTS_DOCSTRING = r\"\"\"\n@@ -251,19 +240,22 @@ def get_decoder(self):\n     def _update_causal_mask(\n         self,\n         attention_mask,\n-        token_type_ids,\n-        past_key_values,\n-        cache_position,\n-        input_tensor,\n-        is_training: bool = False,\n+        token_type_ids=None,\n+        past_key_values=None,\n+        cache_position=None,\n+        input_tensor=None,\n+        is_training: Optional[bool] = None,\n     ):\n         if self.config.text_config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n-\n+        is_training = is_training if is_training is not None else self.training\n         using_static_cache = isinstance(past_key_values, StaticCache)\n         min_dtype = torch.finfo(self.dtype).min\n+        if input_tensor is None:\n+            input_tensor = attention_mask\n+\n         inputs_lead_dim, sequence_length = input_tensor.shape[:2]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -298,6 +290,8 @@ def _update_causal_mask(\n \n             # First unmask prefix tokens during training\n             if is_training:\n+                if token_type_ids is None:\n+                    raise ValueError(\"Token type ids must be provided during training\")\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     token_type_ids[:, None, None, :].to(causal_mask.device) == 0, 0\n                 )\n@@ -345,7 +339,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         num_logits_to_keep: int = 0,\n-    ) -> Union[tuple, NewTaskModelCausalLMOutputWithPast]:\n+    ) -> Union[Tuple, NewTaskModelCausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -368,19 +362,19 @@ def forward(\n         >>> import requests\n         >>> from transformers import AutoProcessor, NewTaskModelForNewTask\n \n-        >>> model = NewTaskModelForNewTask.from_pretrained(\"google/NewTaskModel-test-224px-hf\")\n-        >>> processor = AutoProcessor.from_pretrained(\"google/NewTaskModel-test-224px-hf\")\n+        >>> model = NewTaskModelForNewTask.from_pretrained(\"google/new_task_model2-3b-mix-224\")\n+        >>> processor = AutoProcessor.from_pretrained(\"google/new_task_model2-3b-mix-224\")\n \n-        >>> prompt = \"answer en Where is the cow standing?\"\n-        >>> url = \"https://huggingface.co/gv-hf/NewTaskModel-test-224px-hf/resolve/main/cow_beach_1.png\"\n+        >>> prompt = \"Where is the cat standing?\"\n+        >>> url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n         >>> image = Image.open(requests.get(url, stream=True).raw)\n \n         >>> inputs = processor(images=image, text=prompt,  return_tensors=\"pt\")\n \n         >>> # Generate\n-        >>> generate_ids = model.generate(**inputs, max_length=30)\n+        >>> generate_ids = model.generate(**inputs,)\n         >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"answer en Where is the cow standing?\\nbeach\"\n+        \"Where is the cat standing?\\nsnow\"\n         ```\n         Returns:\n         \"\"\""
        },
        {
            "sha": "2aa5a94a6a214ff8b90cbdceba1fa9ff1252b513",
            "filename": "examples/modular-transformers/modeling_roberta.py",
            "status": "modified",
            "additions": 55,
            "deletions": 13,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_roberta.py?ref=4602059aaee7b075ca51c21941ab2d27980bef7c",
            "patch": "@@ -6,7 +6,7 @@\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n import math\n import os\n-from typing import Optional, Union\n+from typing import List, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -139,9 +139,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor]:\n+    ) -> Tuple[torch.Tensor]:\n         mixed_query_layer = self.query(hidden_states)\n \n         # If this is instantiated as a cross-attention module, the keys\n@@ -248,9 +248,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor]:\n+    ) -> Tuple[torch.Tensor]:\n         if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n             logger.warning_once(\n@@ -389,9 +389,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor]:\n+    ) -> Tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask,\n@@ -457,9 +457,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor]:\n+    ) -> Tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n         self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n@@ -535,12 +535,12 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -629,6 +629,46 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return pooled_output\n \n \n+class RobertaPredictionHeadTransform(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n+        if isinstance(config.hidden_act, str):\n+            self.transform_act_fn = ACT2FN[config.hidden_act]\n+        else:\n+            self.transform_act_fn = config.hidden_act\n+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.dense(hidden_states)\n+        hidden_states = self.transform_act_fn(hidden_states)\n+        hidden_states = self.LayerNorm(hidden_states)\n+        return hidden_states\n+\n+\n+class RobertaLMPredictionHead(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.transform = RobertaPredictionHeadTransform(config)\n+\n+        # The output weights are the same as the input embeddings, but there is\n+        # an output-only bias for each token.\n+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n+\n+        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n+        self.decoder.bias = self.bias\n+\n+    def _tie_weights(self):\n+        self.decoder.bias = self.bias\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.transform(hidden_states)\n+        hidden_states = self.decoder(hidden_states)\n+        return hidden_states\n+\n+\n def load_tf_weights_in_roberta(model, config, tf_checkpoint_path):\n     \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n     try:\n@@ -729,6 +769,8 @@ def _init_weights(self, module):\n         elif isinstance(module, nn.LayerNorm):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n+        elif isinstance(module, RobertaLMPredictionHead):\n+            module.bias.data.zero_()\n \n \n ROBERTA_START_DOCSTRING = r\"\"\"\n@@ -861,12 +903,12 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if"
        },
        {
            "sha": "4b23c6cb16831f7b005dfaa0d4e490475d3a095c",
            "filename": "examples/modular-transformers/modeling_super.py",
            "status": "modified",
            "additions": 53,
            "deletions": 66,
            "changes": 119,
            "blob_url": "https://github.com/huggingface/transformers/blob/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_super.py?ref=4602059aaee7b075ca51c21941ab2d27980bef7c",
            "patch": "@@ -4,26 +4,42 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_super.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-from typing import Callable, Optional, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n from torch import nn\n \n+from transformers.modeling_outputs import CausalLMOutputWithPast\n+\n from ...activations import ACT2FN\n from ...cache_utils import Cache, StaticCache\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_outputs import BaseModelOutputWithPast\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n+    is_torch_flex_attn_available,\n+    logging,\n+)\n from .configuration_super import SuperConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class SuperRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -62,45 +78,18 @@ def __init__(self, config: SuperConfig, device=None):\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            # This .to() is needed if the model has been moved to a device after being initialized (because\n-            # the buffer is automatically moved, but not the original copy)\n-            self.original_inv_freq = self.original_inv_freq.to(device)\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n     @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n     def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-\n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n@@ -222,12 +211,12 @@ def __init__(self, config: SuperConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -244,6 +233,7 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n+\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n                 logger.warning_once(\n@@ -269,7 +259,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class SuperDecoderLayer(nn.Module):\n+class SuperDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: SuperConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -289,11 +279,10 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n@@ -368,6 +357,8 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, SuperRMSNorm):\n+            module.weight.data.fill_(1.0)\n \n \n SUPER_INPUTS_DOCSTRING = r\"\"\"\n@@ -380,12 +371,15 @@ def _init_weights(self, module):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -405,20 +399,12 @@ def _init_weights(self, module):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n@@ -479,6 +465,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(SUPER_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -492,7 +479,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[tuple, BaseModelOutputWithPast]:\n+    ) -> Union[tuple, CausalLMOutputWithPast]:\n         out = super().forward(\n             input_ids,\n             attention_mask,\n@@ -510,16 +497,20 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n@@ -537,7 +528,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -554,15 +545,14 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n \n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n@@ -579,7 +569,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -599,8 +588,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -612,11 +599,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        },
        {
            "sha": "f07691a223ded26da82b5699a22ef9c13a44b465",
            "filename": "examples/modular-transformers/modeling_switch_function.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodeling_switch_function.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodeling_switch_function.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_switch_function.py?ref=4602059aaee7b075ca51c21941ab2d27980bef7c",
            "patch": "@@ -5,7 +5,7 @@\n #                          modular_switch_function.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # Note that llama and cohere have different definitions for rotate_half\n-from typing import Callable, Optional\n+from typing import Callable, Optional, Tuple\n \n import torch\n from torch import nn\n@@ -123,12 +123,12 @@ def __init__(self, config: SwitchFunctionConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -145,6 +145,7 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n+\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n                 logger.warning_once("
        },
        {
            "sha": "7a455333304470427104584395f0e3838f98e9fa",
            "filename": "examples/modular-transformers/modeling_test_detr.py",
            "status": "added",
            "additions": 1692,
            "deletions": 0,
            "changes": 1692,
            "blob_url": "https://github.com/huggingface/transformers/blob/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodeling_test_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodeling_test_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_test_detr.py?ref=4602059aaee7b075ca51c21941ab2d27980bef7c",
            "patch": "@@ -0,0 +1,1692 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from examples/modular-transformers/modular_test_detr.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_test_detr.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+import math\n+import warnings\n+from dataclasses import dataclass\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import Tensor, nn\n+\n+from ...activations import ACT2FN\n+from ...integrations import use_kernel_forward_from_hub\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_outputs import BaseModelOutput\n+from ...modeling_utils import PreTrainedModel\n+from ...pytorch_utils import meshgrid\n+from ...utils import (\n+    ModelOutput,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    is_timm_available,\n+    replace_return_docstrings,\n+    requires_backends,\n+)\n+from ...utils.backbone_utils import load_backbone\n+from .configuration_test_detr import TestDetrConfig\n+\n+\n+if is_timm_available():\n+    from timm import create_model\n+\n+_CONFIG_FOR_DOC = \"TestDetrConfig\"\n+\n+\n+@use_kernel_forward_from_hub(\"MultiScaleDeformableAttention\")\n+class MultiScaleDeformableAttention(nn.Module):\n+    def forward(\n+        self,\n+        value: Tensor,\n+        value_spatial_shapes: Tensor,\n+        value_spatial_shapes_list: List[Tuple],\n+        level_start_index: Tensor,\n+        sampling_locations: Tensor,\n+        attention_weights: Tensor,\n+        im2col_step: int,\n+    ):\n+        batch_size, _, num_heads, hidden_dim = value.shape\n+        _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n+        value_list = value.split([height * width for height, width in value_spatial_shapes_list], dim=1)\n+        sampling_grids = 2 * sampling_locations - 1\n+        sampling_value_list = []\n+        for level_id, (height, width) in enumerate(value_spatial_shapes_list):\n+            # batch_size, height*width, num_heads, hidden_dim\n+            # -> batch_size, height*width, num_heads*hidden_dim\n+            # -> batch_size, num_heads*hidden_dim, height*width\n+            # -> batch_size*num_heads, hidden_dim, height, width\n+            value_l_ = (\n+                value_list[level_id]\n+                .flatten(2)\n+                .transpose(1, 2)\n+                .reshape(batch_size * num_heads, hidden_dim, height, width)\n+            )\n+            # batch_size, num_queries, num_heads, num_points, 2\n+            # -> batch_size, num_heads, num_queries, num_points, 2\n+            # -> batch_size*num_heads, num_queries, num_points, 2\n+            sampling_grid_l_ = sampling_grids[:, :, :, level_id].transpose(1, 2).flatten(0, 1)\n+            # batch_size*num_heads, hidden_dim, num_queries, num_points\n+            sampling_value_l_ = nn.functional.grid_sample(\n+                value_l_,\n+                sampling_grid_l_,\n+                mode=\"bilinear\",\n+                padding_mode=\"zeros\",\n+                align_corners=False,\n+            )\n+            sampling_value_list.append(sampling_value_l_)\n+        # (batch_size, num_queries, num_heads, num_levels, num_points)\n+        # -> (batch_size, num_heads, num_queries, num_levels, num_points)\n+        # -> (batch_size, num_heads, 1, num_queries, num_levels*num_points)\n+        attention_weights = attention_weights.transpose(1, 2).reshape(\n+            batch_size * num_heads, 1, num_queries, num_levels * num_points\n+        )\n+        output = (\n+            (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights)\n+            .sum(-1)\n+            .view(batch_size, num_heads * hidden_dim, num_queries)\n+        )\n+        return output.transpose(1, 2).contiguous()\n+\n+\n+@dataclass\n+class TestDetrDecoderOutput(ModelOutput):\n+    \"\"\"\n+    Base class for outputs of the TestDetrDecoder. This class adds two attributes to\n+    BaseModelOutputWithCrossAttentions, namely:\n+    - a stacked tensor of intermediate decoder hidden states (i.e. the output of each decoder layer)\n+    - a stacked tensor of intermediate reference points.\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n+            Stacked intermediate hidden states (output of each layer of the decoder).\n+        intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, hidden_size)`):\n+            Stacked intermediate reference points (reference points of each layer of the decoder).\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n+            plus the initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n+            the self-attention heads.\n+        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n+            used to compute the weighted average in the cross-attention heads.\n+    \"\"\"\n+\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n+    intermediate_reference_points: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n+\n+\n+@dataclass\n+class TestDetrModelOutput(ModelOutput):\n+    \"\"\"\n+    Base class for outputs of the Deformable DETR encoder-decoder model.\n+\n+    Args:\n+        init_reference_points (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`):\n+            Initial reference points sent through the Transformer decoder.\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n+        intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n+            Stacked intermediate hidden states (output of each layer of the decoder).\n+        intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+            Stacked intermediate reference points (reference points of each layer of the decoder).\n+        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+            shape `(batch_size, num_queries, hidden_size)`. Hidden-states of the decoder at the output of each layer\n+            plus the initial embedding outputs.\n+        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, num_queries,\n+            num_queries)`. Attentions weights of the decoder, after the attention softmax, used to compute the weighted\n+            average in the self-attention heads.\n+        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_queries, num_heads, 4, 4)`.\n+            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n+            weighted average in the cross-attention heads.\n+        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n+        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each\n+            layer plus the initial embedding outputs.\n+        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_queries, num_heads, 4, 4)`.\n+            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n+            self-attention heads.\n+        enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+            Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n+            picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n+            foreground and background).\n+        enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+            Logits of predicted bounding boxes coordinates in the first stage.\n+    \"\"\"\n+\n+    init_reference_points: Optional[torch.FloatTensor] = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n+    intermediate_reference_points: Optional[torch.FloatTensor] = None\n+    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n+    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    enc_outputs_class: Optional[torch.FloatTensor] = None\n+    enc_outputs_coord_logits: Optional[torch.FloatTensor] = None\n+\n+\n+class TestDetrFrozenBatchNorm2d(nn.Module):\n+    \"\"\"\n+    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n+\n+    Copy-paste from torchvision.misc.ops with added eps before rqsrt, without which any other models than\n+    torchvision.models.resnet[18,34,50,101] produce nans.\n+    \"\"\"\n+\n+    def __init__(self, n):\n+        super().__init__()\n+        self.register_buffer(\"weight\", torch.ones(n))\n+        self.register_buffer(\"bias\", torch.zeros(n))\n+        self.register_buffer(\"running_mean\", torch.zeros(n))\n+        self.register_buffer(\"running_var\", torch.ones(n))\n+\n+    def _load_from_state_dict(\n+        self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n+    ):\n+        num_batches_tracked_key = prefix + \"num_batches_tracked\"\n+        if num_batches_tracked_key in state_dict:\n+            del state_dict[num_batches_tracked_key]\n+\n+        super()._load_from_state_dict(\n+            state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n+        )\n+\n+    def forward(self, x):\n+        # move reshapes to the beginning\n+        # to make it user-friendly\n+        weight = self.weight.reshape(1, -1, 1, 1)\n+        bias = self.bias.reshape(1, -1, 1, 1)\n+        running_var = self.running_var.reshape(1, -1, 1, 1)\n+        running_mean = self.running_mean.reshape(1, -1, 1, 1)\n+        epsilon = 1e-5\n+        scale = weight * (running_var + epsilon).rsqrt()\n+        bias = bias - running_mean * scale\n+        return x * scale + bias\n+\n+\n+def replace_batch_norm(model):\n+    r\"\"\"\n+    Recursively replace all `torch.nn.BatchNorm2d` with `TestDetrFrozenBatchNorm2d`.\n+\n+    Args:\n+        model (torch.nn.Module):\n+            input model\n+    \"\"\"\n+    for name, module in model.named_children():\n+        if isinstance(module, nn.BatchNorm2d):\n+            new_module = TestDetrFrozenBatchNorm2d(module.num_features)\n+\n+            if not module.weight.device == torch.device(\"meta\"):\n+                new_module.weight.data.copy_(module.weight)\n+                new_module.bias.data.copy_(module.bias)\n+                new_module.running_mean.data.copy_(module.running_mean)\n+                new_module.running_var.data.copy_(module.running_var)\n+\n+            model._modules[name] = new_module\n+\n+        if len(list(module.children())) > 0:\n+            replace_batch_norm(module)\n+\n+\n+class TestDetrConvEncoder(nn.Module):\n+    \"\"\"\n+    Convolutional backbone, using either the AutoBackbone API or one from the timm library.\n+\n+    nn.BatchNorm2d layers are replaced by TestDetrFrozenBatchNorm2d as defined above.\n+\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+\n+        self.config = config\n+\n+        # For backwards compatibility we have to use the timm library directly instead of the AutoBackbone API\n+        if config.use_timm_backbone:\n+            # We default to values which were previously hard-coded. This enables configurability from the config\n+            # using backbone arguments, while keeping the default behavior the same.\n+            requires_backends(self, [\"timm\"])\n+            kwargs = getattr(config, \"backbone_kwargs\", {})\n+            kwargs = {} if kwargs is None else kwargs.copy()\n+            out_indices = kwargs.pop(\"out_indices\", (2, 3, 4) if config.num_feature_levels > 1 else (4,))\n+            num_channels = kwargs.pop(\"in_chans\", config.num_channels)\n+            if config.dilation:\n+                kwargs[\"output_stride\"] = kwargs.get(\"output_stride\", 16)\n+            backbone = create_model(\n+                config.backbone,\n+                pretrained=config.use_pretrained_backbone,\n+                features_only=True,\n+                out_indices=out_indices,\n+                in_chans=num_channels,\n+                **kwargs,\n+            )\n+        else:\n+            backbone = load_backbone(config)\n+\n+        # replace batch norm by frozen batch norm\n+        with torch.no_grad():\n+            replace_batch_norm(backbone)\n+        self.model = backbone\n+        self.intermediate_channel_sizes = (\n+            self.model.feature_info.channels() if config.use_timm_backbone else self.model.channels\n+        )\n+\n+        backbone_model_type = None\n+        if config.backbone is not None:\n+            backbone_model_type = config.backbone\n+        elif config.backbone_config is not None:\n+            backbone_model_type = config.backbone_config.model_type\n+        else:\n+            raise ValueError(\"Either `backbone` or `backbone_config` should be provided in the config\")\n+\n+        if \"resnet\" in backbone_model_type:\n+            for name, parameter in self.model.named_parameters():\n+                if config.use_timm_backbone:\n+                    if \"layer2\" not in name and \"layer3\" not in name and \"layer4\" not in name:\n+                        parameter.requires_grad_(False)\n+                else:\n+                    if \"stage.1\" not in name and \"stage.2\" not in name and \"stage.3\" not in name:\n+                        parameter.requires_grad_(False)\n+\n+    def forward(self, pixel_values: torch.Tensor, pixel_mask: torch.Tensor):\n+        # send pixel_values through the model to get list of feature maps\n+        features = self.model(pixel_values) if self.config.use_timm_backbone else self.model(pixel_values).feature_maps\n+\n+        out = []\n+        for feature_map in features:\n+            # downsample pixel_mask to match shape of corresponding feature_map\n+            mask = nn.functional.interpolate(pixel_mask[None].float(), size=feature_map.shape[-2:]).to(torch.bool)[0]\n+            out.append((feature_map, mask))\n+        return out\n+\n+\n+class TestDetrConvModel(nn.Module):\n+    \"\"\"\n+    This module adds 2D position embeddings to all intermediate feature maps of the convolutional encoder.\n+    \"\"\"\n+\n+    def __init__(self, conv_encoder, position_embedding):\n+        super().__init__()\n+        self.conv_encoder = conv_encoder\n+        self.position_embedding = position_embedding\n+\n+    def forward(self, pixel_values, pixel_mask):\n+        # send pixel_values and pixel_mask through backbone to get list of (feature_map, pixel_mask) tuples\n+        out = self.conv_encoder(pixel_values, pixel_mask)\n+        pos = []\n+        for feature_map, mask in out:\n+            # position encoding\n+            pos.append(self.position_embedding(feature_map, mask).to(feature_map.dtype))\n+\n+        return out, pos\n+\n+\n+class TestDetrSinePositionEmbedding(nn.Module):\n+    \"\"\"\n+    This is a more standard version of the position embedding, very similar to the one used by the Attention is all you\n+    need paper, generalized to work on images.\n+    \"\"\"\n+\n+    def __init__(self, embedding_dim=64, temperature=10000, normalize=False, scale=None):\n+        super().__init__()\n+        self.embedding_dim = embedding_dim\n+        self.temperature = temperature\n+        self.normalize = normalize\n+        if scale is not None and normalize is False:\n+            raise ValueError(\"normalize should be True if scale is passed\")\n+        if scale is None:\n+            scale = 2 * math.pi\n+        self.scale = scale\n+\n+    def forward(self, pixel_values, pixel_mask):\n+        if pixel_mask is None:\n+            raise ValueError(\"No pixel mask provided\")\n+        y_embed = pixel_mask.cumsum(1, dtype=pixel_values.dtype)\n+        x_embed = pixel_mask.cumsum(2, dtype=pixel_values.dtype)\n+        if self.normalize:\n+            eps = 1e-6\n+            y_embed = (y_embed - 0.5) / (y_embed[:, -1:, :] + eps) * self.scale\n+            x_embed = (x_embed - 0.5) / (x_embed[:, :, -1:] + eps) * self.scale\n+\n+        dim_t = torch.arange(self.embedding_dim, dtype=pixel_values.dtype, device=pixel_values.device)\n+        dim_t = self.temperature ** (2 * torch.div(dim_t, 2, rounding_mode=\"floor\") / self.embedding_dim)\n+\n+        pos_x = x_embed[:, :, :, None] / dim_t\n+        pos_y = y_embed[:, :, :, None] / dim_t\n+        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n+        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n+        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n+        return pos\n+\n+\n+class TestDetrLearnedPositionEmbedding(nn.Module):\n+    \"\"\"\n+    This module learns positional embeddings up to a fixed maximum size.\n+    \"\"\"\n+\n+    def __init__(self, embedding_dim=256):\n+        super().__init__()\n+        self.row_embeddings = nn.Embedding(50, embedding_dim)\n+        self.column_embeddings = nn.Embedding(50, embedding_dim)\n+\n+    def forward(self, pixel_values, pixel_mask=None):\n+        height, width = pixel_values.shape[-2:]\n+        width_values = torch.arange(width, device=pixel_values.device)\n+        height_values = torch.arange(height, device=pixel_values.device)\n+        x_emb = self.column_embeddings(width_values)\n+        y_emb = self.row_embeddings(height_values)\n+        pos = torch.cat([x_emb.unsqueeze(0).repeat(height, 1, 1), y_emb.unsqueeze(1).repeat(1, width, 1)], dim=-1)\n+        pos = pos.permute(2, 0, 1)\n+        pos = pos.unsqueeze(0)\n+        pos = pos.repeat(pixel_values.shape[0], 1, 1, 1)\n+        return pos\n+\n+\n+class TestDetrMultiscaleDeformableAttention(nn.Module):\n+    \"\"\"\n+    Multiscale deformable attention as proposed in Deformable DETR.\n+    \"\"\"\n+\n+    def __init__(self, config: TestDetrConfig, num_heads: int, n_points: int):\n+        super().__init__()\n+\n+        self.attn = MultiScaleDeformableAttention()\n+\n+        if config.d_model % num_heads != 0:\n+            raise ValueError(\n+                f\"embed_dim (d_model) must be divisible by num_heads, but got {config.d_model} and {num_heads}\"\n+            )\n+        dim_per_head = config.d_model // num_heads\n+        # check if dim_per_head is power of 2\n+        if not ((dim_per_head & (dim_per_head - 1) == 0) and dim_per_head != 0):\n+            warnings.warn(\n+                \"You'd better set embed_dim (d_model) in TestDetrMultiscaleDeformableAttention to make the\"\n+                \" dimension of each attention head a power of 2 which is more efficient in the authors' CUDA\"\n+                \" implementation.\"\n+            )\n+\n+        self.im2col_step = 64\n+\n+        self.d_model = config.d_model\n+        self.n_levels = config.num_feature_levels\n+        self.n_heads = num_heads\n+        self.n_points = n_points\n+\n+        self.sampling_offsets = nn.Linear(config.d_model, num_heads * self.n_levels * n_points * 2)\n+        self.attention_weights = nn.Linear(config.d_model, num_heads * self.n_levels * n_points)\n+        self.value_proj = nn.Linear(config.d_model, config.d_model)\n+        self.output_proj = nn.Linear(config.d_model, config.d_model)\n+\n+        self.disable_custom_kernels = config.disable_custom_kernels\n+\n+    def with_pos_embed(self, tensor: torch.Tensor, position_embeddings: Optional[Tensor]):\n+        return tensor if position_embeddings is None else tensor + position_embeddings\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        encoder_hidden_states=None,\n+        encoder_attention_mask=None,\n+        position_embeddings: Optional[torch.Tensor] = None,\n+        reference_points=None,\n+        spatial_shapes=None,\n+        spatial_shapes_list=None,\n+        level_start_index=None,\n+        output_attentions: bool = False,\n+    ):\n+        # add position embeddings to the hidden states before projecting to queries and keys\n+        if position_embeddings is not None:\n+            hidden_states = self.with_pos_embed(hidden_states, position_embeddings)\n+\n+        batch_size, num_queries, _ = hidden_states.shape\n+        batch_size, sequence_length, _ = encoder_hidden_states.shape\n+        total_elements = sum(height * width for height, width in spatial_shapes_list)\n+        if total_elements != sequence_length:\n+            raise ValueError(\n+                \"Make sure to align the spatial shapes with the sequence length of the encoder hidden states\"\n+            )\n+\n+        value = self.value_proj(encoder_hidden_states)\n+        if attention_mask is not None:\n+            # we invert the attention_mask\n+            value = value.masked_fill(~attention_mask[..., None], float(0))\n+        value = value.view(batch_size, sequence_length, self.n_heads, self.d_model // self.n_heads)\n+        sampling_offsets = self.sampling_offsets(hidden_states).view(\n+            batch_size, num_queries, self.n_heads, self.n_levels, self.n_points, 2\n+        )\n+        attention_weights = self.attention_weights(hidden_states).view(\n+            batch_size, num_queries, self.n_heads, self.n_levels * self.n_points\n+        )\n+        attention_weights = F.softmax(attention_weights, -1).view(\n+            batch_size, num_queries, self.n_heads, self.n_levels, self.n_points\n+        )\n+        # batch_size, num_queries, n_heads, n_levels, n_points, 2\n+        num_coordinates = reference_points.shape[-1]\n+        if num_coordinates == 2:\n+            offset_normalizer = torch.stack([spatial_shapes[..., 1], spatial_shapes[..., 0]], -1)\n+            sampling_locations = (\n+                reference_points[:, :, None, :, None, :]\n+                + sampling_offsets / offset_normalizer[None, None, None, :, None, :]\n+            )\n+        elif num_coordinates == 4:\n+            sampling_locations = (\n+                reference_points[:, :, None, :, None, :2]\n+                + sampling_offsets / self.n_points * reference_points[:, :, None, :, None, 2:] * 0.5\n+            )\n+        else:\n+            raise ValueError(f\"Last dim of reference_points must be 2 or 4, but got {reference_points.shape[-1]}\")\n+\n+        output = self.attn(\n+            value,\n+            spatial_shapes,\n+            spatial_shapes_list,\n+            level_start_index,\n+            sampling_locations,\n+            attention_weights,\n+            self.im2col_step,\n+        )\n+\n+        output = self.output_proj(output)\n+\n+        return output, attention_weights\n+\n+\n+class TestDetrMultiheadAttention(nn.Module):\n+    \"\"\"\n+    Multi-headed attention from 'Attention Is All You Need' paper.\n+\n+    Here, we add position embeddings to the queries and keys (as explained in the Deformable DETR paper).\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        embed_dim: int,\n+        num_heads: int,\n+        dropout: float = 0.0,\n+        bias: bool = True,\n+    ):\n+        super().__init__()\n+        self.embed_dim = embed_dim\n+        self.num_heads = num_heads\n+        self.dropout = dropout\n+        self.head_dim = embed_dim // num_heads\n+        if self.head_dim * num_heads != self.embed_dim:\n+            raise ValueError(\n+                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n+                f\" {num_heads}).\"\n+            )\n+        self.scaling = self.head_dim**-0.5\n+\n+        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n+        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n+        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n+        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n+\n+    def _shape(self, tensor: torch.Tensor, seq_len: int, batch_size: int):\n+        return tensor.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n+\n+    def with_pos_embed(self, tensor: torch.Tensor, position_embeddings: Optional[Tensor]):\n+        return tensor if position_embeddings is None else tensor + position_embeddings\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+\n+        batch_size, target_len, embed_dim = hidden_states.size()\n+        # add position embeddings to the hidden states before projecting to queries and keys\n+        if position_embeddings is not None:\n+            hidden_states_original = hidden_states\n+            hidden_states = self.with_pos_embed(hidden_states, position_embeddings)\n+\n+        # get queries, keys and values\n+        query_states = self.q_proj(hidden_states) * self.scaling\n+        key_states = self._shape(self.k_proj(hidden_states), -1, batch_size)\n+        value_states = self._shape(self.v_proj(hidden_states_original), -1, batch_size)\n+\n+        proj_shape = (batch_size * self.num_heads, -1, self.head_dim)\n+        query_states = self._shape(query_states, target_len, batch_size).view(*proj_shape)\n+        key_states = key_states.view(*proj_shape)\n+        value_states = value_states.view(*proj_shape)\n+\n+        source_len = key_states.size(1)\n+\n+        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n+\n+        if attn_weights.size() != (batch_size * self.num_heads, target_len, source_len):\n+            raise ValueError(\n+                f\"Attention weights should be of size {(batch_size * self.num_heads, target_len, source_len)}, but is\"\n+                f\" {attn_weights.size()}\"\n+            )\n+\n+        # expand attention_mask\n+        if attention_mask is not None:\n+            # [batch_size, seq_len] -> [batch_size, 1, target_seq_len, source_seq_len]\n+            attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n+\n+        if attention_mask is not None:\n+            if attention_mask.size() != (batch_size, 1, target_len, source_len):\n+                raise ValueError(\n+                    f\"Attention mask should be of size {(batch_size, 1, target_len, source_len)}, but is\"\n+                    f\" {attention_mask.size()}\"\n+                )\n+            attn_weights = attn_weights.view(batch_size, self.num_heads, target_len, source_len) + attention_mask\n+            attn_weights = attn_weights.view(batch_size * self.num_heads, target_len, source_len)\n+\n+        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+        if output_attentions:\n+            # this operation is a bit awkward, but it's required to\n+            # make sure that attn_weights keeps its gradient.\n+            # In order to do so, attn_weights have to reshaped\n+            # twice and have to be reused in the following\n+            attn_weights_reshaped = attn_weights.view(batch_size, self.num_heads, target_len, source_len)\n+            attn_weights = attn_weights_reshaped.view(batch_size * self.num_heads, target_len, source_len)\n+        else:\n+            attn_weights_reshaped = None\n+\n+        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n+\n+        attn_output = torch.bmm(attn_probs, value_states)\n+\n+        if attn_output.size() != (\n+            batch_size * self.num_heads,\n+            target_len,\n+            self.head_dim,\n+        ):\n+            raise ValueError(\n+                f\"`attn_output` should be of size {(batch_size, self.num_heads, target_len, self.head_dim)}, but is\"\n+                f\" {attn_output.size()}\"\n+            )\n+\n+        attn_output = attn_output.view(batch_size, self.num_heads, target_len, self.head_dim)\n+        attn_output = attn_output.transpose(1, 2)\n+        attn_output = attn_output.reshape(batch_size, target_len, embed_dim)\n+\n+        attn_output = self.out_proj(attn_output)\n+\n+        return attn_output, attn_weights_reshaped\n+\n+\n+class TestDetrEncoderLayer(nn.Module):\n+    def __init__(self, config: TestDetrConfig):\n+        super().__init__()\n+        self.embed_dim = config.d_model\n+        self.self_attn = TestDetrMultiscaleDeformableAttention(\n+            config,\n+            num_heads=config.encoder_attention_heads,\n+            n_points=config.encoder_n_points,\n+        )\n+        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n+        self.dropout = config.dropout\n+        self.activation_fn = ACT2FN[config.activation_function]\n+        self.activation_dropout = config.activation_dropout\n+        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n+        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n+        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: torch.Tensor,\n+        position_embeddings: Optional[torch.Tensor] = None,\n+        reference_points=None,\n+        spatial_shapes=None,\n+        spatial_shapes_list=None,\n+        level_start_index=None,\n+        output_attentions: bool = False,\n+    ):\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+                Input to the layer.\n+            attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n+                Attention mask.\n+            position_embeddings (`torch.FloatTensor`, *optional*):\n+                Position embeddings, to be added to `hidden_states`.\n+            reference_points (`torch.FloatTensor`, *optional*):\n+                Reference points.\n+            spatial_shapes (`torch.LongTensor`, *optional*):\n+                Spatial shapes of the backbone feature maps.\n+            level_start_index (`torch.LongTensor`, *optional*):\n+                Level start index.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+        \"\"\"\n+        residual = hidden_states\n+\n+        # Apply Multi-scale Deformable Attention Module on the multi-scale feature maps.\n+        hidden_states, attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            encoder_hidden_states=hidden_states,\n+            encoder_attention_mask=attention_mask,\n+            position_embeddings=position_embeddings,\n+            reference_points=reference_points,\n+            spatial_shapes=spatial_shapes,\n+            spatial_shapes_list=spatial_shapes_list,\n+            level_start_index=level_start_index,\n+            output_attentions=output_attentions,\n+        )\n+\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        hidden_states = residual + hidden_states\n+        hidden_states = self.self_attn_layer_norm(hidden_states)\n+\n+        residual = hidden_states\n+        hidden_states = self.activation_fn(self.fc1(hidden_states))\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n+\n+        hidden_states = self.fc2(hidden_states)\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+\n+        hidden_states = residual + hidden_states\n+        hidden_states = self.final_layer_norm(hidden_states)\n+\n+        if self.training:\n+            if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():\n+                clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n+                hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (attn_weights,)\n+\n+        return outputs\n+\n+\n+class TestDetrDecoderLayer(nn.Module):\n+    def __init__(self, config: TestDetrConfig):\n+        super().__init__()\n+        self.embed_dim = config.d_model\n+\n+        # self-attention\n+        self.self_attn = TestDetrMultiheadAttention(\n+            embed_dim=self.embed_dim,\n+            num_heads=config.decoder_attention_heads,\n+            dropout=config.attention_dropout,\n+        )\n+        self.dropout = config.dropout\n+        self.activation_fn = ACT2FN[config.activation_function]\n+        self.activation_dropout = config.activation_dropout\n+\n+        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n+        # cross-attention\n+        self.encoder_attn = TestDetrMultiscaleDeformableAttention(\n+            config,\n+            num_heads=config.decoder_attention_heads,\n+            n_points=config.decoder_n_points,\n+        )\n+        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n+        # feedforward neural networks\n+        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n+        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n+        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[torch.Tensor] = None,\n+        reference_points=None,\n+        spatial_shapes=None,\n+        spatial_shapes_list=None,\n+        level_start_index=None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = False,\n+    ):\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`):\n+                Input to the layer of shape `(seq_len, batch, embed_dim)`.\n+            position_embeddings (`torch.FloatTensor`, *optional*):\n+                Position embeddings that are added to the queries and keys in the self-attention layer.\n+            reference_points (`torch.FloatTensor`, *optional*):\n+                Reference points.\n+            spatial_shapes (`torch.LongTensor`, *optional*):\n+                Spatial shapes.\n+            level_start_index (`torch.LongTensor`, *optional*):\n+                Level start index.\n+            encoder_hidden_states (`torch.FloatTensor`):\n+                cross attention input to the layer of shape `(seq_len, batch, embed_dim)`\n+            encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n+                `(batch, 1, target_len, source_len)` where padding elements are indicated by very large negative\n+                values.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+        \"\"\"\n+        residual = hidden_states\n+\n+        # Self Attention\n+        hidden_states, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n+            output_attentions=output_attentions,\n+        )\n+\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        hidden_states = residual + hidden_states\n+        hidden_states = self.self_attn_layer_norm(hidden_states)\n+\n+        second_residual = hidden_states\n+\n+        # Cross-Attention\n+        cross_attn_weights = None\n+        hidden_states, cross_attn_weights = self.encoder_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=encoder_attention_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            position_embeddings=position_embeddings,\n+            reference_points=reference_points,\n+            spatial_shapes=spatial_shapes,\n+            spatial_shapes_list=spatial_shapes_list,\n+            level_start_index=level_start_index,\n+            output_attentions=output_attentions,\n+        )\n+\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        hidden_states = second_residual + hidden_states\n+\n+        hidden_states = self.encoder_attn_layer_norm(hidden_states)\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.activation_fn(self.fc1(hidden_states))\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n+        hidden_states = self.fc2(hidden_states)\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        hidden_states = residual + hidden_states\n+        hidden_states = self.final_layer_norm(hidden_states)\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights, cross_attn_weights)\n+\n+        return outputs\n+\n+\n+class TestDetrPreTrainedModel(PreTrainedModel):\n+    config_class = TestDetrConfig\n+    base_model_prefix = \"model\"\n+    main_input_name = \"pixel_values\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\n+        r\"TestDetrConvEncoder\",\n+        r\"TestDetrEncoderLayer\",\n+        r\"TestDetrDecoderLayer\",\n+    ]\n+\n+    def _init_weights(self, module):\n+        std = self.config.init_std\n+\n+        if isinstance(module, TestDetrLearnedPositionEmbedding):\n+            nn.init.uniform_(module.row_embeddings.weight)\n+            nn.init.uniform_(module.column_embeddings.weight)\n+        elif isinstance(module, TestDetrMultiscaleDeformableAttention):\n+            nn.init.constant_(module.sampling_offsets.weight.data, 0.0)\n+            default_dtype = torch.get_default_dtype()\n+            thetas = torch.arange(module.n_heads, dtype=torch.int64).to(default_dtype) * (\n+                2.0 * math.pi / module.n_heads\n+            )\n+            grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n+            grid_init = (\n+                (grid_init / grid_init.abs().max(-1, keepdim=True)[0])\n+                .view(module.n_heads, 1, 1, 2)\n+                .repeat(1, module.n_levels, module.n_points, 1)\n+            )\n+            for i in range(module.n_points):\n+                grid_init[:, :, i, :] *= i + 1\n+            with torch.no_grad():\n+                module.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n+            nn.init.constant_(module.attention_weights.weight.data, 0.0)\n+            nn.init.constant_(module.attention_weights.bias.data, 0.0)\n+            nn.init.xavier_uniform_(module.value_proj.weight.data)\n+            nn.init.constant_(module.value_proj.bias.data, 0.0)\n+            nn.init.xavier_uniform_(module.output_proj.weight.data)\n+            nn.init.constant_(module.output_proj.bias.data, 0.0)\n+        elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n+            # Slightly different from the TF version which uses truncated_normal for initialization\n+            # cf https://github.com/pytorch/pytorch/pull/5617\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        if hasattr(module, \"reference_points\") and not self.config.two_stage:\n+            nn.init.xavier_uniform_(module.reference_points.weight.data, gain=1.0)\n+            nn.init.constant_(module.reference_points.bias.data, 0.0)\n+        if hasattr(module, \"level_embed\"):\n+            nn.init.normal_(module.level_embed)\n+\n+\n+class TestDetrEncoder(TestDetrPreTrainedModel):\n+    \"\"\"\n+    Transformer encoder consisting of *config.encoder_layers* deformable attention layers. Each layer is a\n+    [`TestDetrEncoderLayer`].\n+\n+    The encoder updates the flattened multi-scale feature maps through multiple deformable attention layers.\n+\n+    Args:\n+        config: TestDetrConfig\n+    \"\"\"\n+\n+    def __init__(self, config: TestDetrConfig):\n+        super().__init__(config)\n+        self.gradient_checkpointing = False\n+\n+        self.dropout = config.dropout\n+        self.layers = nn.ModuleList([TestDetrEncoderLayer(config) for _ in range(config.encoder_layers)])\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @staticmethod\n+    def get_reference_points(spatial_shapes, valid_ratios, device):\n+        \"\"\"\n+        Get reference points for each feature map. Used in decoder.\n+\n+        Args:\n+            spatial_shapes (`torch.LongTensor` of shape `(num_feature_levels, 2)`):\n+                Spatial shapes of each feature map.\n+            valid_ratios (`torch.FloatTensor` of shape `(batch_size, num_feature_levels, 2)`):\n+                Valid ratios of each feature map.\n+            device (`torch.device`):\n+                Device on which to create the tensors.\n+        Returns:\n+            `torch.FloatTensor` of shape `(batch_size, num_queries, num_feature_levels, 2)`\n+        \"\"\"\n+        reference_points_list = []\n+        for level, (height, width) in enumerate(spatial_shapes):\n+            ref_y, ref_x = meshgrid(\n+                torch.linspace(0.5, height - 0.5, height, dtype=valid_ratios.dtype, device=device),\n+                torch.linspace(0.5, width - 0.5, width, dtype=valid_ratios.dtype, device=device),\n+                indexing=\"ij\",\n+            )\n+            # TODO: valid_ratios could be useless here. check https://github.com/fundamentalvision/Deformable-DETR/issues/36\n+            ref_y = ref_y.reshape(-1)[None] / (valid_ratios[:, None, level, 1] * height)\n+            ref_x = ref_x.reshape(-1)[None] / (valid_ratios[:, None, level, 0] * width)\n+            ref = torch.stack((ref_x, ref_y), -1)\n+            reference_points_list.append(ref)\n+        reference_points = torch.cat(reference_points_list, 1)\n+        reference_points = reference_points[:, :, None] * valid_ratios[:, None]\n+        return reference_points\n+\n+    def forward(\n+        self,\n+        inputs_embeds=None,\n+        attention_mask=None,\n+        position_embeddings=None,\n+        spatial_shapes=None,\n+        spatial_shapes_list=None,\n+        level_start_index=None,\n+        valid_ratios=None,\n+        output_attentions=None,\n+        output_hidden_states=None,\n+        return_dict=None,\n+    ):\n+        r\"\"\"\n+        Args:\n+            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+                Flattened feature map (output of the backbone + projection layer) that is passed to the encoder.\n+            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Mask to avoid performing attention on padding pixel features. Mask values selected in `[0, 1]`:\n+                - 1 for pixel features that are real (i.e. **not masked**),\n+                - 0 for pixel features that are padding (i.e. **masked**).\n+                [What are attention masks?](../glossary#attention-mask)\n+            position_embeddings (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+                Position embeddings that are added to the queries and keys in each self-attention layer.\n+            spatial_shapes (`torch.LongTensor` of shape `(num_feature_levels, 2)`):\n+                Spatial shapes of each feature map.\n+            level_start_index (`torch.LongTensor` of shape `(num_feature_levels)`):\n+                Starting index of each feature map.\n+            valid_ratios (`torch.FloatTensor` of shape `(batch_size, num_feature_levels, 2)`):\n+                Ratio of valid area in each feature level.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n+                for more detail.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        hidden_states = inputs_embeds\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+\n+        spatial_shapes_tuple = tuple(spatial_shapes_list)\n+        reference_points = self.get_reference_points(spatial_shapes_tuple, valid_ratios, device=inputs_embeds.device)\n+\n+        encoder_states = () if output_hidden_states else None\n+        all_attentions = () if output_attentions else None\n+        for i, encoder_layer in enumerate(self.layers):\n+            if output_hidden_states:\n+                encoder_states = encoder_states + (hidden_states,)\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    encoder_layer.__call__,\n+                    hidden_states,\n+                    attention_mask,\n+                    position_embeddings,\n+                    reference_points,\n+                    spatial_shapes,\n+                    spatial_shapes_list,\n+                    level_start_index,\n+                    output_attentions,\n+                )\n+            else:\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    position_embeddings=position_embeddings,\n+                    reference_points=reference_points,\n+                    spatial_shapes=spatial_shapes,\n+                    spatial_shapes_list=spatial_shapes_list,\n+                    level_start_index=level_start_index,\n+                    output_attentions=output_attentions,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_attentions = all_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            encoder_states = encoder_states + (hidden_states,)\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=encoder_states,\n+            attentions=all_attentions,\n+        )\n+\n+\n+def inverse_sigmoid(x, eps=1e-5):\n+    x = x.clamp(min=0, max=1)\n+    x1 = x.clamp(min=eps)\n+    x2 = (1 - x).clamp(min=eps)\n+    return torch.log(x1 / x2)\n+\n+\n+class TestDetrDecoder(TestDetrPreTrainedModel):\n+    \"\"\"\n+    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a [`TestDetrDecoderLayer`].\n+\n+    The decoder updates the query embeddings through multiple self-attention and cross-attention layers.\n+\n+    Some tweaks for Deformable DETR:\n+\n+    - `position_embeddings`, `reference_points`, `spatial_shapes` and `valid_ratios` are added to the forward pass.\n+    - it also returns a stack of intermediate outputs and reference points from all decoding layers.\n+\n+    Args:\n+        config: TestDetrConfig\n+    \"\"\"\n+\n+    def __init__(self, config: TestDetrConfig):\n+        super().__init__(config)\n+\n+        self.dropout = config.dropout\n+        self.layers = nn.ModuleList([TestDetrDecoderLayer(config) for _ in range(config.decoder_layers)])\n+        self.gradient_checkpointing = False\n+\n+        # hack implementation for iterative bounding box refinement and two-stage Deformable DETR\n+        self.bbox_embed = None\n+        self.class_embed = None\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def forward(\n+        self,\n+        inputs_embeds=None,\n+        encoder_hidden_states=None,\n+        encoder_attention_mask=None,\n+        position_embeddings=None,\n+        reference_points=None,\n+        spatial_shapes=None,\n+        spatial_shapes_list=None,\n+        level_start_index=None,\n+        valid_ratios=None,\n+        output_attentions=None,\n+        output_hidden_states=None,\n+        return_dict=None,\n+    ):\n+        r\"\"\"\n+        Args:\n+            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n+                The query embeddings that are passed into the decoder.\n+            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n+                of the decoder.\n+            encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Mask to avoid performing cross-attention on padding pixel_values of the encoder. Mask values selected\n+                in `[0, 1]`:\n+                - 1 for pixels that are real (i.e. **not masked**),\n+                - 0 for pixels that are padding (i.e. **masked**).\n+            position_embeddings (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n+                Position embeddings that are added to the queries and keys in each self-attention layer.\n+            reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)` is `as_two_stage` else `(batch_size, num_queries, 2)` or , *optional*):\n+                Reference point in range `[0, 1]`, top-left (0,0), bottom-right (1, 1), including padding area.\n+            spatial_shapes (`torch.FloatTensor` of shape `(num_feature_levels, 2)`):\n+                Spatial shapes of the feature maps.\n+            level_start_index (`torch.LongTensor` of shape `(num_feature_levels)`, *optional*):\n+                Indexes for the start of each feature level. In range `[0, sequence_length]`.\n+            valid_ratios (`torch.FloatTensor` of shape `(batch_size, num_feature_levels, 2)`, *optional*):\n+                Ratio of valid area in each feature level.\n+\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n+                for more detail.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if inputs_embeds is not None:\n+            hidden_states = inputs_embeds\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n+        intermediate = ()\n+        intermediate_reference_points = ()\n+\n+        for idx, decoder_layer in enumerate(self.layers):\n+            num_coordinates = reference_points.shape[-1]\n+            if num_coordinates == 4:\n+                reference_points_input = (\n+                    reference_points[:, :, None] * torch.cat([valid_ratios, valid_ratios], -1)[:, None]\n+                )\n+            elif reference_points.shape[-1] == 2:\n+                reference_points_input = reference_points[:, :, None] * valid_ratios[:, None]\n+            else:\n+                raise ValueError(\"Reference points' last dimension must be of size 2\")\n+\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    decoder_layer.__call__,\n+                    hidden_states,\n+                    position_embeddings,\n+                    reference_points_input,\n+                    spatial_shapes,\n+                    spatial_shapes_list,\n+                    level_start_index,\n+                    encoder_hidden_states,\n+                    encoder_attention_mask,\n+                    output_attentions,\n+                )\n+            else:\n+                layer_outputs = decoder_layer(\n+                    hidden_states,\n+                    position_embeddings=position_embeddings,\n+                    encoder_hidden_states=encoder_hidden_states,\n+                    reference_points=reference_points_input,\n+                    spatial_shapes=spatial_shapes,\n+                    spatial_shapes_list=spatial_shapes_list,\n+                    level_start_index=level_start_index,\n+                    encoder_attention_mask=encoder_attention_mask,\n+                    output_attentions=output_attentions,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            # hack implementation for iterative bounding box refinement\n+            if self.bbox_embed is not None:\n+                tmp = self.bbox_embed[idx](hidden_states)\n+                num_coordinates = reference_points.shape[-1]\n+                if num_coordinates == 4:\n+                    new_reference_points = tmp + inverse_sigmoid(reference_points)\n+                    new_reference_points = new_reference_points.sigmoid()\n+                elif num_coordinates == 2:\n+                    new_reference_points = tmp\n+                    new_reference_points[..., :2] = tmp[..., :2] + inverse_sigmoid(reference_points)\n+                    new_reference_points = new_reference_points.sigmoid()\n+                else:\n+                    raise ValueError(\n+                        f\"Last dim of reference_points must be 2 or 4, but got {reference_points.shape[-1]}\"\n+                    )\n+                reference_points = new_reference_points.detach()\n+\n+            intermediate += (hidden_states,)\n+            intermediate_reference_points += (reference_points,)\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+                if encoder_hidden_states is not None:\n+                    all_cross_attentions += (layer_outputs[2],)\n+\n+        # Keep batch_size as first dimension\n+        intermediate = torch.stack(intermediate, dim=1)\n+        intermediate_reference_points = torch.stack(intermediate_reference_points, dim=1)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        if not return_dict:\n+            return tuple(\n+                v\n+                for v in [\n+                    hidden_states,\n+                    intermediate,\n+                    intermediate_reference_points,\n+                    all_hidden_states,\n+                    all_self_attns,\n+                    all_cross_attentions,\n+                ]\n+                if v is not None\n+            )\n+        return TestDetrDecoderOutput(\n+            last_hidden_state=hidden_states,\n+            intermediate_hidden_states=intermediate,\n+            intermediate_reference_points=intermediate_reference_points,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+            cross_attentions=all_cross_attentions,\n+        )\n+\n+\n+def build_position_encoding(config):\n+    n_steps = config.d_model // 2\n+    if config.position_embedding_type == \"sine\":\n+        # TODO find a better way of exposing other arguments\n+        position_embedding = TestDetrSinePositionEmbedding(n_steps, normalize=True)\n+    elif config.position_embedding_type == \"learned\":\n+        position_embedding = TestDetrLearnedPositionEmbedding(n_steps)\n+    else:\n+        raise ValueError(f\"Not supported {config.position_embedding_type}\")\n+\n+    return position_embedding\n+\n+\n+TEST_DETR_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`TestDetrConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+TEST_DETR_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Padding will be ignored by default should you provide it.\n+\n+            Pixel values can be obtained using [`AutoImageProcessor`]. See [`TestDetrImageProcessor.__call__`]\n+            for details.\n+\n+        pixel_mask (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):\n+            Mask to avoid performing attention on padding pixel values. Mask values selected in `[0, 1]`:\n+\n+            - 1 for pixels that are real (i.e. **not masked**),\n+            - 0 for pixels that are padding (i.e. **masked**).\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+        decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n+            Not used by default. Can be used to mask object queries.\n+        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n+            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n+            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n+            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n+            can choose to directly pass a flattened representation of an image.\n+        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n+            Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n+            embedded representation.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    The bare Deformable DETR Model (consisting of a backbone and encoder-decoder Transformer) outputting raw\n+    hidden-states without any specific head on top.\n+    \"\"\",\n+    TEST_DETR_START_DOCSTRING,\n+)\n+class TestDetrModel(TestDetrPreTrainedModel):\n+    def __init__(self, config: TestDetrConfig):\n+        super().__init__(config)\n+\n+        # Create backbone + positional encoding\n+        backbone = TestDetrConvEncoder(config)\n+        position_embeddings = build_position_encoding(config)\n+        self.backbone = TestDetrConvModel(backbone, position_embeddings)\n+\n+        # Create input projection layers\n+        if config.num_feature_levels > 1:\n+            num_backbone_outs = len(backbone.intermediate_channel_sizes)\n+            input_proj_list = []\n+            for _ in range(num_backbone_outs):\n+                in_channels = backbone.intermediate_channel_sizes[_]\n+                input_proj_list.append(\n+                    nn.Sequential(\n+                        nn.Conv2d(in_channels, config.d_model, kernel_size=1),\n+                        nn.GroupNorm(32, config.d_model),\n+                    )\n+                )\n+            for _ in range(config.num_feature_levels - num_backbone_outs):\n+                input_proj_list.append(\n+                    nn.Sequential(\n+                        nn.Conv2d(\n+                            in_channels,\n+                            config.d_model,\n+                            kernel_size=3,\n+                            stride=2,\n+                            padding=1,\n+                        ),\n+                        nn.GroupNorm(32, config.d_model),\n+                    )\n+                )\n+                in_channels = config.d_model\n+            self.input_proj = nn.ModuleList(input_proj_list)\n+        else:\n+            self.input_proj = nn.ModuleList(\n+                [\n+                    nn.Sequential(\n+                        nn.Conv2d(\n+                            backbone.intermediate_channel_sizes[-1],\n+                            config.d_model,\n+                            kernel_size=1,\n+                        ),\n+                        nn.GroupNorm(32, config.d_model),\n+                    )\n+                ]\n+            )\n+\n+        if not config.two_stage:\n+            self.query_position_embeddings = nn.Embedding(config.num_queries, config.d_model * 2)\n+\n+        self.encoder = TestDetrEncoder(config)\n+        self.decoder = TestDetrDecoder(config)\n+\n+        self.level_embed = nn.Parameter(torch.Tensor(config.num_feature_levels, config.d_model))\n+\n+        if config.two_stage:\n+            self.enc_output = nn.Linear(config.d_model, config.d_model)\n+            self.enc_output_norm = nn.LayerNorm(config.d_model)\n+            self.pos_trans = nn.Linear(config.d_model * 2, config.d_model * 2)\n+            self.pos_trans_norm = nn.LayerNorm(config.d_model * 2)\n+        else:\n+            self.reference_points = nn.Linear(config.d_model, 2)\n+\n+        self.post_init()\n+\n+    def get_encoder(self):\n+        return self.encoder\n+\n+    def get_decoder(self):\n+        return self.decoder\n+\n+    def freeze_backbone(self):\n+        for name, param in self.backbone.conv_encoder.model.named_parameters():\n+            param.requires_grad_(False)\n+\n+    def unfreeze_backbone(self):\n+        for name, param in self.backbone.conv_encoder.model.named_parameters():\n+            param.requires_grad_(True)\n+\n+    def get_valid_ratio(self, mask, dtype=torch.float32):\n+        \"\"\"Get the valid ratio of all feature maps.\"\"\"\n+\n+        _, height, width = mask.shape\n+        valid_height = torch.sum(mask[:, :, 0], 1)\n+        valid_width = torch.sum(mask[:, 0, :], 1)\n+        valid_ratio_height = valid_height.to(dtype) / height\n+        valid_ratio_width = valid_width.to(dtype) / width\n+        valid_ratio = torch.stack([valid_ratio_width, valid_ratio_height], -1)\n+        return valid_ratio\n+\n+    def get_proposal_pos_embed(self, proposals):\n+        \"\"\"Get the position embedding of the proposals.\"\"\"\n+\n+        num_pos_feats = self.config.d_model // 2\n+        temperature = 10000\n+        scale = 2 * math.pi\n+\n+        dim_t = torch.arange(num_pos_feats, dtype=proposals.dtype, device=proposals.device)\n+        dim_t = temperature ** (2 * torch.div(dim_t, 2, rounding_mode=\"floor\") / num_pos_feats)\n+        # batch_size, num_queries, 4\n+        proposals = proposals.sigmoid() * scale\n+        # batch_size, num_queries, 4, 128\n+        pos = proposals[:, :, :, None] / dim_t\n+        # batch_size, num_queries, 4, 64, 2 -> batch_size, num_queries, 512\n+        pos = torch.stack((pos[:, :, :, 0::2].sin(), pos[:, :, :, 1::2].cos()), dim=4).flatten(2)\n+        return pos\n+\n+    def gen_encoder_output_proposals(self, enc_output, padding_mask, spatial_shapes):\n+        \"\"\"Generate the encoder output proposals from encoded enc_output.\n+\n+        Args:\n+            enc_output (Tensor[batch_size, sequence_length, hidden_size]): Output of the encoder.\n+            padding_mask (Tensor[batch_size, sequence_length]): Padding mask for `enc_output`.\n+            spatial_shapes (List[Tuple[int, int]]): Spatial shapes of the feature maps.\n+\n+        Returns:\n+            `tuple(torch.FloatTensor)`: A tuple of feature map and bbox prediction.\n+                - object_query (Tensor[batch_size, sequence_length, hidden_size]): Object query features. Later used to\n+                  directly predict a bounding box. (without the need of a decoder)\n+                - output_proposals (Tensor[batch_size, sequence_length, 4]): Normalized proposals, after an inverse\n+                  sigmoid.\n+        \"\"\"\n+        batch_size = enc_output.shape[0]\n+        proposals = []\n+        _cur = 0\n+        for level, (height, width) in enumerate(spatial_shapes):\n+            mask_flatten_ = padding_mask[:, _cur : (_cur + height * width)].view(batch_size, height, width, 1)\n+            valid_height = torch.sum(~mask_flatten_[:, :, 0, 0], 1)\n+            valid_width = torch.sum(~mask_flatten_[:, 0, :, 0], 1)\n+\n+            grid_y, grid_x = meshgrid(\n+                torch.linspace(\n+                    0,\n+                    height - 1,\n+                    height,\n+                    dtype=enc_output.dtype,\n+                    device=enc_output.device,\n+                ),\n+                torch.linspace(\n+                    0,\n+                    width - 1,\n+                    width,\n+                    dtype=enc_output.dtype,\n+                    device=enc_output.device,\n+                ),\n+                indexing=\"ij\",\n+            )\n+            grid = torch.cat([grid_x.unsqueeze(-1), grid_y.unsqueeze(-1)], -1)\n+\n+            scale = torch.cat([valid_width.unsqueeze(-1), valid_height.unsqueeze(-1)], 1).view(batch_size, 1, 1, 2)\n+            grid = (grid.unsqueeze(0).expand(batch_size, -1, -1, -1) + 0.5) / scale\n+            width_height = torch.ones_like(grid) * 0.05 * (2.0**level)\n+            proposal = torch.cat((grid, width_height), -1).view(batch_size, -1, 4)\n+            proposals.append(proposal)\n+            _cur += height * width\n+        output_proposals = torch.cat(proposals, 1)\n+        output_proposals_valid = ((output_proposals > 0.01) & (output_proposals < 0.99)).all(-1, keepdim=True)\n+        output_proposals = torch.log(output_proposals / (1 - output_proposals))  # inverse sigmoid\n+        output_proposals = output_proposals.masked_fill(padding_mask.unsqueeze(-1), float(\"inf\"))\n+        output_proposals = output_proposals.masked_fill(~output_proposals_valid, float(\"inf\"))\n+\n+        # assign each pixel as an object query\n+        object_query = enc_output\n+        object_query = object_query.masked_fill(padding_mask.unsqueeze(-1), float(0))\n+        object_query = object_query.masked_fill(~output_proposals_valid, float(0))\n+        object_query = self.enc_output_norm(self.enc_output(object_query))\n+        return object_query, output_proposals\n+\n+    @add_start_docstrings_to_model_forward(TEST_DETR_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=TestDetrModelOutput, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        pixel_mask: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        encoder_outputs: Optional[torch.FloatTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple[torch.FloatTensor], TestDetrModelOutput]:\n+        r\"\"\"\n+        Returns:\n+\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import AutoImageProcessor, TestDetrModel\n+        >>> from PIL import Image\n+        >>> import requests\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> image_processor = AutoImageProcessor.from_pretrained(\"SenseTime/deformable-detr\")\n+        >>> model = TestDetrModel.from_pretrained(\"SenseTime/deformable-detr\")\n+\n+        >>> inputs = image_processor(images=image, return_tensors=\"pt\")\n+\n+        >>> outputs = model(**inputs)\n+\n+        >>> last_hidden_states = outputs.last_hidden_state\n+        >>> list(last_hidden_states.shape)\n+        [1, 300, 256]\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        batch_size, num_channels, height, width = pixel_values.shape\n+        device = pixel_values.device\n+\n+        if pixel_mask is None:\n+            pixel_mask = torch.ones(((batch_size, height, width)), dtype=torch.long, device=device)\n+\n+        # Extract multi-scale feature maps of same resolution `config.d_model` (cf Figure 4 in paper)\n+        # First, sent pixel_values + pixel_mask through Backbone to obtain the features\n+        # which is a list of tuples\n+        features, position_embeddings_list = self.backbone(pixel_values, pixel_mask)\n+\n+        # Then, apply 1x1 convolution to reduce the channel dimension to d_model (256 by default)\n+        sources = []\n+        masks = []\n+        for level, (source, mask) in enumerate(features):\n+            sources.append(self.input_proj[level](source))\n+            masks.append(mask)\n+            if mask is None:\n+                raise ValueError(\"No attention mask was provided\")\n+\n+        # Lowest resolution feature maps are obtained via 3x3 stride 2 convolutions on the final stage\n+        if self.config.num_feature_levels > len(sources):\n+            _len_sources = len(sources)\n+            for level in range(_len_sources, self.config.num_feature_levels):\n+                if level == _len_sources:\n+                    source = self.input_proj[level](features[-1][0])\n+                else:\n+                    source = self.input_proj[level](sources[-1])\n+                mask = nn.functional.interpolate(pixel_mask[None].to(pixel_values.dtype), size=source.shape[-2:]).to(\n+                    torch.bool\n+                )[0]\n+                pos_l = self.backbone.position_embedding(source, mask).to(source.dtype)\n+                sources.append(source)\n+                masks.append(mask)\n+                position_embeddings_list.append(pos_l)\n+\n+        # Create queries\n+        query_embeds = None\n+        if not self.config.two_stage:\n+            query_embeds = self.query_position_embeddings.weight\n+\n+        # Prepare encoder inputs (by flattening)\n+        source_flatten = []\n+        mask_flatten = []\n+        lvl_pos_embed_flatten = []\n+        spatial_shapes_list = []\n+        for level, (source, mask, pos_embed) in enumerate(zip(sources, masks, position_embeddings_list)):\n+            batch_size, num_channels, height, width = source.shape\n+            spatial_shape = (height, width)\n+            spatial_shapes_list.append(spatial_shape)\n+            source = source.flatten(2).transpose(1, 2)\n+            mask = mask.flatten(1)\n+            pos_embed = pos_embed.flatten(2).transpose(1, 2)\n+            lvl_pos_embed = pos_embed + self.level_embed[level].view(1, 1, -1)\n+            lvl_pos_embed_flatten.append(lvl_pos_embed)\n+            source_flatten.append(source)\n+            mask_flatten.append(mask)\n+        source_flatten = torch.cat(source_flatten, 1)\n+        mask_flatten = torch.cat(mask_flatten, 1)\n+        lvl_pos_embed_flatten = torch.cat(lvl_pos_embed_flatten, 1)\n+        spatial_shapes = torch.as_tensor(spatial_shapes_list, dtype=torch.long, device=source_flatten.device)\n+        level_start_index = torch.cat((spatial_shapes.new_zeros((1,)), spatial_shapes.prod(1).cumsum(0)[:-1]))\n+        valid_ratios = torch.stack([self.get_valid_ratio(m, dtype=source_flatten.dtype) for m in masks], 1)\n+\n+        # Fourth, sent source_flatten + mask_flatten + lvl_pos_embed_flatten (backbone + proj layer output) through encoder\n+        # Also provide spatial_shapes, level_start_index and valid_ratios\n+        if encoder_outputs is None:\n+            encoder_outputs = self.encoder(\n+                inputs_embeds=source_flatten,\n+                attention_mask=mask_flatten,\n+                position_embeddings=lvl_pos_embed_flatten,\n+                spatial_shapes=spatial_shapes,\n+                spatial_shapes_list=spatial_shapes_list,\n+                level_start_index=level_start_index,\n+                valid_ratios=valid_ratios,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+                return_dict=return_dict,\n+            )\n+        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n+        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n+            encoder_outputs = BaseModelOutput(\n+                last_hidden_state=encoder_outputs[0],\n+                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n+                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n+            )\n+\n+        # Fifth, prepare decoder inputs\n+        batch_size, _, num_channels = encoder_outputs[0].shape\n+        enc_outputs_class = None\n+        enc_outputs_coord_logits = None\n+        if self.config.two_stage:\n+            object_query_embedding, output_proposals = self.gen_encoder_output_proposals(\n+                encoder_outputs[0], ~mask_flatten, spatial_shapes_list\n+            )\n+\n+            # hack implementation for two-stage Deformable DETR\n+            # apply a detection head to each pixel (A.4 in paper)\n+            # linear projection for bounding box binary classification (i.e. foreground and background)\n+            enc_outputs_class = self.decoder.class_embed[-1](object_query_embedding)\n+            # 3-layer FFN to predict bounding boxes coordinates (bbox regression branch)\n+            delta_bbox = self.decoder.bbox_embed[-1](object_query_embedding)\n+            enc_outputs_coord_logits = delta_bbox + output_proposals\n+\n+            # only keep top scoring `config.two_stage_num_proposals` proposals\n+            topk = self.config.two_stage_num_proposals\n+            topk_proposals = torch.topk(enc_outputs_class[..., 0], topk, dim=1)[1]\n+            topk_coords_logits = torch.gather(\n+                enc_outputs_coord_logits,\n+                1,\n+                topk_proposals.unsqueeze(-1).repeat(1, 1, 4),\n+            )\n+\n+            topk_coords_logits = topk_coords_logits.detach()\n+            reference_points = topk_coords_logits.sigmoid()\n+            init_reference_points = reference_points\n+            pos_trans_out = self.pos_trans_norm(self.pos_trans(self.get_proposal_pos_embed(topk_coords_logits)))\n+            query_embed, target = torch.split(pos_trans_out, num_channels, dim=2)\n+        else:\n+            query_embed, target = torch.split(query_embeds, num_channels, dim=1)\n+            query_embed = query_embed.unsqueeze(0).expand(batch_size, -1, -1)\n+            target = target.unsqueeze(0).expand(batch_size, -1, -1)\n+            reference_points = self.reference_points(query_embed).sigmoid()\n+            init_reference_points = reference_points\n+\n+        decoder_outputs = self.decoder(\n+            inputs_embeds=target,\n+            position_embeddings=query_embed,\n+            encoder_hidden_states=encoder_outputs[0],\n+            encoder_attention_mask=mask_flatten,\n+            reference_points=reference_points,\n+            spatial_shapes=spatial_shapes,\n+            spatial_shapes_list=spatial_shapes_list,\n+            level_start_index=level_start_index,\n+            valid_ratios=valid_ratios,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        if not return_dict:\n+            enc_outputs = tuple(value for value in [enc_outputs_class, enc_outputs_coord_logits] if value is not None)\n+            tuple_outputs = (init_reference_points,) + decoder_outputs + encoder_outputs + enc_outputs\n+\n+            return tuple_outputs\n+\n+        return TestDetrModelOutput(\n+            init_reference_points=init_reference_points,\n+            last_hidden_state=decoder_outputs.last_hidden_state,\n+            intermediate_hidden_states=decoder_outputs.intermediate_hidden_states,\n+            intermediate_reference_points=decoder_outputs.intermediate_reference_points,\n+            decoder_hidden_states=decoder_outputs.hidden_states,\n+            decoder_attentions=decoder_outputs.attentions,\n+            cross_attentions=decoder_outputs.cross_attentions,\n+            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n+            encoder_hidden_states=encoder_outputs.hidden_states,\n+            encoder_attentions=encoder_outputs.attentions,\n+            enc_outputs_class=enc_outputs_class,\n+            enc_outputs_coord_logits=enc_outputs_coord_logits,\n+        )"
        },
        {
            "sha": "4080520c284a3d975055033090caa4a0cc1c31c3",
            "filename": "examples/modular-transformers/modular_multimodal2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodular_multimodal2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodular_multimodal2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_multimodal2.py?ref=4602059aaee7b075ca51c21941ab2d27980bef7c",
            "patch": "@@ -16,9 +16,7 @@ class Multimodal2VisionModel(CLIPVisionModel):\n     CLIPAttention,\n     CLIPEncoder,\n     CLIPEncoderLayer,\n-    CLIPFlashAttention2,\n     CLIPPreTrainedModel,\n-    CLIPSdpaAttention,\n     CLIPVisionModel,\n     CLIPVisionTransformer,\n )\n@@ -29,31 +27,13 @@ class Multimodal2VisionAttention(CLIPAttention):\n     pass\n \n \n-# Check that adding the second base class correctly set the parent, even though in clip it does not have the \"Vision\" part\n-class Multimodal2VisionSdpaAttention(CLIPSdpaAttention, Multimodal2VisionAttention):\n-    pass\n-\n-\n-# Check that adding the second base class correctly set the parent, even though in clip it does not have the \"Vision\" part\n-class Multimodal2VisionFlashAttention2(CLIPFlashAttention2, Multimodal2VisionAttention):\n-    pass\n-\n-\n-MULTIMODAL2_VISION_ATTENTION_CLASSES = {\n-    \"eager\": Multimodal2VisionAttention,\n-    \"sdpa\": Multimodal2VisionSdpaAttention,\n-    \"flash_attention_2\": Multimodal2VisionFlashAttention2,\n-}\n-\n-\n class Multimodal2VisionMLP(CLIPMLP):\n     pass\n \n \n class Multimodal2VisionEncoderLayer(CLIPEncoderLayer):\n     def __init__(self, config):\n         super().__init__()\n-        self.self_attn = MULTIMODAL2_VISION_ATTENTION_CLASSES[config._attn_implementation](config)\n         self.mlp = Multimodal2VisionMLP(config)\n \n "
        },
        {
            "sha": "2546b51279021fc680fa844558ee67c62ed3bff5",
            "filename": "examples/modular-transformers/modular_test_detr.py",
            "status": "added",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodular_test_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4602059aaee7b075ca51c21941ab2d27980bef7c/examples%2Fmodular-transformers%2Fmodular_test_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_test_detr.py?ref=4602059aaee7b075ca51c21941ab2d27980bef7c",
            "patch": "@@ -0,0 +1,7 @@\n+from transformers.models.deformable_detr.modeling_deformable_detr import DeformableDetrModel\n+\n+\n+# Here, the old and new model have by essence a common \"detr\" suffix. Make sure everything is correctly named\n+# in this case (i.e., we do not wrongly detect `Detr` as part of a suffix to remove)\n+class TestDetrModel(DeformableDetrModel):\n+    pass"
        },
        {
            "sha": "2a383123baef5670a5700d762f227c55794af9e5",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/4602059aaee7b075ca51c21941ab2d27980bef7c/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4602059aaee7b075ca51c21941ab2d27980bef7c/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=4602059aaee7b075ca51c21941ab2d27980bef7c",
            "patch": "@@ -1466,6 +1466,10 @@ class NewModelNameTextDecoderLayer(LlamaDecoderLayer):\n                 suffix = common_partial_suffix(class_name, modeling_bases[0])\n                 if len(suffix) > 0 and suffix[0].isupper():\n                     cased_model_name = class_name.replace(suffix, \"\")\n+                    # If both the old model and new model share the last part of their name, is is detected as a common\n+                    # suffix, but it should not be the case -> use the full name in this case\n+                    if len(cased_model_name) < len(cased_default_name) and cased_default_name in class_name:\n+                        cased_model_name = cased_default_name\n                 prefix_model_name_mapping[filename].update([cased_model_name])\n \n         # Check if we found multiple prefixes for some modeling files\n@@ -1761,6 +1765,17 @@ def save_modeling_file(modular_file, converted_file):\n         args.files_to_parse = glob.glob(\"src/transformers/models/**/modular_*.py\", recursive=True)\n     if args.files_to_parse == [\"examples\"]:\n         args.files_to_parse = glob.glob(\"examples/**/modular_*.py\", recursive=True)\n+    else:\n+        for i, model_name in enumerate(args.files_to_parse):\n+            if os.sep not in model_name:\n+                full_path = os.path.join(\"src\", \"transformers\", \"models\", model_name, f\"modular_{model_name}.py\")\n+                # If it does not exist, try in the examples section\n+                if not os.path.isfile(full_path):\n+                    full_path = os.path.join(\"examples\", \"modular-transformers\", f\"modular_{model_name}.py\")\n+                # We did not find it anywhere\n+                if not os.path.isfile(full_path):\n+                    raise ValueError(f\"Cannot find a modular file for {model_name}. Please provide the full path.\")\n+                args.files_to_parse[i] = full_path\n \n     priority_list, _ = find_priority_list(args.files_to_parse)\n     assert len(priority_list) == len(args.files_to_parse), \"Some files will not be converted\""
        }
    ],
    "stats": {
        "total": 3273,
        "additions": 2337,
        "deletions": 936
    }
}