{
    "author": "ArthurZucker",
    "message": "the fix that did not get in (#37370)\n\n* debugging improvements\n\n* add debugging details\n\n* add more debugging details\n\n* debug more\n\n* the fix that did not get in\n\n* First fix flex\n\n* fix query offset\n\n* fix flex first\n\n* fix device mask creation for speed\n\n* small mask creation sdpa\n\n* Update flex_attention.py\n\n* remove chunked prefill from HybridChunkedCache\n\n* never seen such a fucked up merged\n\n* clean up layers + output\n\n* add summary json file\n\n* Efficient general cache\n\n* Update cache_utils.py\n\n* cleanup\n\n* fix?\n\n* fix!\n\n* oups typo\n\n* not everywhere\n\n* more fixes\n\n* revert unrelated changes\n\n* Fix but ugly for now -> should use pad instead\n\n* oups\n\n* re-initialize the cache\n\n* Use pad to simplify\n\n* style\n\n* correct slicing\n\n---------\n\nCo-authored-by: Pablo <pablo.montalvo.leroux@gmail.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "e032d12e8a402d3ec1ab7ec26b92d917f447af29",
    "files": [
        {
            "sha": "1e7ed00cf2de94a380fcefc21ee659aa369a1f80",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 32,
            "deletions": 31,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/e032d12e8a402d3ec1ab7ec26b92d917f447af29/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e032d12e8a402d3ec1ab7ec26b92d917f447af29/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=e032d12e8a402d3ec1ab7ec26b92d917f447af29",
            "patch": "@@ -1913,37 +1913,38 @@ def initialise_cache_layer(self, layer_idx, key_states):\n         self.value_cache.append(new_layer_value_cache)\n \n     def _sliding_update(self, cache_position, layer_idx, key_states, value_states, k_out, v_out, max_cache_len):\n-        if cache_position.shape[0] > max_cache_len:\n-            cache_position = cache_position.clamp(0, max_cache_len - 1)\n-            k_out = key_states[:, :, -max_cache_len:, :]\n-            v_out = value_states[:, :, -max_cache_len:, :]\n-            # Assumption: caches are all zeros at this point, `+=` is equivalent to `=` but compile-friendly\n-            self.key_cache[layer_idx].zero_()\n-            self.value_cache[layer_idx].zero_()\n-\n-            self.key_cache[layer_idx] += k_out\n-            self.value_cache[layer_idx] += v_out\n-            # we should return the whole states instead of k_out, v_out to take the whole prompt\n-            # into consideration when building kv cache instead of just throwing away tokens outside of the window\n-            return key_states, value_states\n-\n-        # otherwise we are decoding. Most efficient way to cat 1 token\n-        slicing = torch.ones(max_cache_len, dtype=torch.long, device=value_states.device).cumsum(0)\n-        cache_position = cache_position.clamp(0, max_cache_len - 1)\n-        to_shift = cache_position >= max_cache_len - 1\n-        indices = (slicing + to_shift[-1].int() - 1) % max_cache_len\n-        k_out = k_out[:, :, indices]\n-        v_out = v_out[:, :, indices]\n-\n-        k_out[:, :, cache_position] = key_states\n-        v_out[:, :, cache_position] = value_states\n-        # `_.zero()` followed by `+=` is equivalent `=`, but compile-friendly (without graph breaks due to assignment)\n-        self.key_cache[layer_idx].zero_()\n-        self.value_cache[layer_idx].zero_()\n-\n-        self.key_cache[layer_idx] += k_out\n-        self.value_cache[layer_idx] += v_out\n-        return k_out, v_out\n+        cumulative_length = self.cumulative_length[layer_idx]\n+        # Update it now that we saved the value above\n+        self.cumulative_length[layer_idx] += key_states.shape[-2]\n+        is_full = cumulative_length >= max_cache_len\n+        if is_full:\n+            full_key_states = torch.cat((k_out[:, :, 1:, :], key_states), dim=-2)\n+            full_value_states = torch.cat((v_out[:, :, 1:, :], value_states), dim=-2)\n+            # Fast decoding path -> here as the effective size is still sliding window, it is extremely important\n+            # to return `self.key_cache[layer_idx]` and `self.value_cache[layer_idx]`, as they have the fixed adress\n+            # in memory (the values are the same as the full states, but not the address!!)\n+            if key_states.shape[-2] == 1:\n+                self.key_cache[layer_idx].copy_(full_key_states)\n+                self.value_cache[layer_idx].copy_(full_value_states)\n+                return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+        elif not is_full and cumulative_length + key_states.shape[2] > max_cache_len:\n+            # Fast prefill path, no need to cat() in this case (which creates a copy even if cating from 0 dim)\n+            if cumulative_length == 0:\n+                full_key_states = key_states\n+                full_value_states = value_states\n+            else:\n+                full_key_states = torch.cat((k_out[:, :, :cumulative_length, :], key_states), dim=-2)\n+                full_value_states = torch.cat((v_out[:, :, :cumulative_length, :], value_states), dim=-2)\n+        else:\n+            self.key_cache[layer_idx].index_copy_(2, cache_position, key_states)\n+            self.value_cache[layer_idx].index_copy_(2, cache_position, value_states)\n+            return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n+        self.key_cache[layer_idx].copy_(full_key_states[:, :, -max_cache_len:, :])\n+        self.value_cache[layer_idx].copy_(full_value_states[:, :, -max_cache_len:, :])\n+        # we should return the whole states instead of k_out, v_out to take the whole prompt\n+        # into consideration when building kv cache instead of just throwing away tokens outside of the window\n+        return full_key_states, full_value_states\n \n     def _static_update(self, cache_position, layer_idx, key_states, value_states, k_out, v_out, max_cache_len):\n         k_out[:, :, cache_position] = key_states"
        },
        {
            "sha": "a28058c674ecf3fd0a09116b92b4ae47fc79640d",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e032d12e8a402d3ec1ab7ec26b92d917f447af29/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e032d12e8a402d3ec1ab7ec26b92d917f447af29/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=e032d12e8a402d3ec1ab7ec26b92d917f447af29",
            "patch": "@@ -33,6 +33,7 @@\n     Cache,\n     DynamicCache,\n     EncoderDecoderCache,\n+    HybridChunkedCache,\n     OffloadedCache,\n     QuantizedCacheConfig,\n     StaticCache,\n@@ -1833,6 +1834,7 @@ def _get_cache(\n             not hasattr(self, \"_cache\")\n             or (not isinstance(cache_to_check, cache_cls))\n             or cache_to_check.max_batch_size != batch_size\n+            or isinstance(cache_to_check, HybridChunkedCache)  # due to internal slicing, we always re-init\n         )\n         if cache_implementation != \"mamba\":\n             need_new_cache = need_new_cache or cache_to_check.max_cache_len < max_cache_len"
        },
        {
            "sha": "815eb548b74e7bcffa8c1b13f2a29ce274bdb5d7",
            "filename": "src/transformers/integrations/flex_attention.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e032d12e8a402d3ec1ab7ec26b92d917f447af29/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e032d12e8a402d3ec1ab7ec26b92d917f447af29/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflex_attention.py?ref=e032d12e8a402d3ec1ab7ec26b92d917f447af29",
            "patch": "@@ -61,16 +61,17 @@ def __init__(self, training):\n         \"\"\"\n         Initialize or update the singleton instance.\n         \"\"\"\n-        if not self._is_flex_compiled:\n+        if not self._is_flex_compiled or training != self.training:\n             # In PyTorch 2.6.0, there's a known issue with flex attention compilation which may\n             # cause errors. The suggested fix is to compile with \"max-autotune-no-cudagraphs\"\n             # see https://github.com/pytorch/pytorch/issues/146260 for training\n+            self.training = training\n             if _torch_version == \"2.6.0\" and training:\n                 self._compiled_flex_attention = torch.compile(\n                     flex_attention, dynamic=False, mode=\"max-autotune-no-cudagraphs\"\n                 )\n             else:\n-                self._compiled_flex_attention = torch.compile(flex_attention, dynamic=False)\n+                self._compiled_flex_attention = torch.compile(flex_attention)\n             self._is_flex_compiled = True\n \n     def __call__(self):"
        },
        {
            "sha": "9612a1de89d3781e2bac88de0b0d0cecf769d36e",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 29,
            "deletions": 18,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/e032d12e8a402d3ec1ab7ec26b92d917f447af29/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e032d12e8a402d3ec1ab7ec26b92d917f447af29/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=e032d12e8a402d3ec1ab7ec26b92d917f447af29",
            "patch": "@@ -738,12 +738,10 @@ def _update_causal_mask(\n         else:\n             full_cache_length = attention_mask.shape[-1] if attention_mask is not None else sequence_length\n \n-        # to avoid graph break, we introduce this hack\n         cond1 = first_cache_position >= attention_chunk_size\n         cond2 = (first_cache_position < attention_chunk_size) & (\n             first_cache_position + sequence_length > attention_chunk_size\n         )\n-\n         key_length = (\n             torch.where(\n                 cond1,\n@@ -764,7 +762,7 @@ def _update_causal_mask(\n                     attention_mask,\n                     query_length=sequence_length,\n                     key_length=full_cache_length,\n-                    offsets=None if sequence_length != 1 else (first_cache_position, 0),\n+                    offsets=(first_cache_position, 0),\n                 )\n                 return attention_mask, chunked_attention_mask\n             if isinstance(attention_mask, BlockMask):\n@@ -775,28 +773,42 @@ def _update_causal_mask(\n         causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n             attention_mask,\n             sequence_length=sequence_length,\n-            target_length=full_cache_length,\n+            target_length=max(full_cache_length, attention_chunk_size),\n             dtype=dtype,\n             device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n         if full_cache_length > self.config.attention_chunk_size:\n+            start_idx = max(last_cache_position - key_length, 0)\n+            end_idx = last_cache_position + 1 if sequence_length > 1 else last_cache_position\n+            # We always need a mask of at least attention_chunk_size, so we use the max here\n+            end_idx = max(end_idx, start_idx + attention_chunk_size)\n             chunked_attention_mask = self.create_chunked_attention_mask(\n                 self.config.attention_chunk_size,\n-                start=first_cache_position,\n-                end=first_cache_position + key_length,\n+                start=start_idx,  # same offset as with flex\n+                end=end_idx,\n                 device=device,\n             )\n-            chunked_attention_mask = chunked_attention_mask & attention_mask\n-            if sequence_length == 1:\n-                chunked_attention_mask = chunked_attention_mask[-1:]\n-            if self.config._attn_implementation == \"eager\":\n-                chunked_attention_mask = (\n-                    chunked_attention_mask[None, None, :, :]\n-                    .to(dtype)\n-                    .masked_fill(chunked_attention_mask, torch.finfo(dtype).min)\n+\n+            local_attention_mask = attention_mask[:, start_idx:end_idx]  # offset here as well\n+            # It may be smaller than attention_chunk_size -> pad it\n+            requires_padding = local_attention_mask.shape[-1] < attention_chunk_size\n+            if requires_padding:\n+                local_attention_mask = nn.functional.pad(\n+                    local_attention_mask, (0, attention_chunk_size - local_attention_mask.shape[-1])\n                 )\n+            # Depending on the padding, take the query tokens from the end or the cache_position\n+            if not requires_padding:\n+                chunked_attention_mask = chunked_attention_mask[None, None, -sequence_length:, :]\n+            else:\n+                chunked_attention_mask = chunked_attention_mask[None, None, cache_position, :]\n+\n+            chunked_attention_mask = chunked_attention_mask.expand(input_tensor.shape[0], -1, -1, -1)\n+            chunked_attention_mask = chunked_attention_mask * local_attention_mask[:, None, None, :]\n+            if self.config._attn_implementation == \"eager\":\n+                min_dtype = torch.finfo(dtype).min\n+                chunked_attention_mask = torch.where(chunked_attention_mask == 0, min_dtype, 0.0).to(dtype)\n \n         if (\n             self.config._attn_implementation == \"sdpa\"\n@@ -810,7 +822,6 @@ def _update_causal_mask(\n             # Details: https://github.com/pytorch/pytorch/issues/110213\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-            # chunked_attention_mask = AttentionMaskConverter._unmask_unattended(chunked_attention_mask, min_dtype)\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n         if self.config._attn_implementation == \"sdpa\" and chunked_attention_mask is not None:\n@@ -841,11 +852,11 @@ def create_chunked_attention_mask(\n         If the chunk size is 3.\n         This can just be appplied over the already created attention mask\n         \"\"\"\n+        arange_vector = torch.arange(start, end, device=device)\n         block_pos = torch.abs(\n-            (torch.arange(start, end).unsqueeze(0) // attention_chunk_size)\n-            - (torch.arange(start, end).unsqueeze(1) // attention_chunk_size)\n+            arange_vector.unsqueeze(0) // attention_chunk_size - arange_vector.unsqueeze(1) // attention_chunk_size\n         )\n-        token_pos = torch.arange(start, end).unsqueeze(0) - torch.arange(start, end).unsqueeze(1)\n+        token_pos = arange_vector.unsqueeze(0) - arange_vector.unsqueeze(1)\n         mask = (block_pos == 0) & (token_pos <= 0)\n         return mask.to(device)\n "
        }
    ],
    "stats": {
        "total": 117,
        "additions": 66,
        "deletions": 51
    }
}