{
    "author": "sywangyi",
    "message": "fix patch_attention_mask incorrect setting which leads to the differeâ€¦ (#33499)\n\n* fix patch_attention_mask incorrect setting which leads to the difference in the generated text if batch > 1\r\n\r\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>\r\n\r\n* fix format\r\n\r\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>\r\n\r\n* [run_slow] idefics2\r\n\r\n---------\r\n\r\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>",
    "sha": "454a0f2efdf9f0d94b77ef08efabbdc6418c868d",
    "files": [
        {
            "sha": "6108f0e8a42e8f62922264475dad1827c455310a",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/454a0f2efdf9f0d94b77ef08efabbdc6418c868d/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/454a0f2efdf9f0d94b77ef08efabbdc6418c868d/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=454a0f2efdf9f0d94b77ef08efabbdc6418c868d",
            "patch": "@@ -1388,7 +1388,7 @@ def forward(\n             patch_size = self.config.vision_config.patch_size\n             patches_subgrid = pixel_attention_mask.unfold(dimension=1, size=patch_size, step=patch_size)\n             patches_subgrid = patches_subgrid.unfold(dimension=2, size=patch_size, step=patch_size)\n-            patch_attention_mask = (patches_subgrid.sum(dim=(-1, -2)) > 0).bool()\n+            patch_attention_mask = (patches_subgrid.sum(dim=(-1, -2)) == patch_size * patch_size).bool()\n \n             # Get sequence from the vision encoder\n             image_hidden_states = self.vision_model("
        },
        {
            "sha": "e02c5b4c9f09c668b18d5ce63d72f65e7e503c10",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/454a0f2efdf9f0d94b77ef08efabbdc6418c868d/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/454a0f2efdf9f0d94b77ef08efabbdc6418c868d/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=454a0f2efdf9f0d94b77ef08efabbdc6418c868d",
            "patch": "@@ -540,6 +540,41 @@ def test_integration_test_4bit(self):\n         expected_generated_text = \"In this image, we see the Statue of Liberty, the Hudson River,\"\n         self.assertEqual(generated_texts[0], expected_generated_text)\n \n+    @slow\n+    @require_bitsandbytes\n+    def test_integration_test_4bit_batch2(self):\n+        # Let' s make sure we test the preprocessing to replace what is used\n+\n+        model = Idefics2ForConditionalGeneration.from_pretrained(\n+            \"HuggingFaceM4/idefics2-8b-base\",\n+            load_in_4bit=True,\n+        )\n+\n+        from datasets import load_dataset\n+\n+        dataset = load_dataset(\"nielsr/docvqa_1200_examples\", split=\"test\")\n+\n+        text = [f\"<image>{dataset[40]['query']['en']}\", f\"<image>{dataset[41]['query']['en']}\"]\n+        images = [[dataset[40][\"image\"]], [dataset[41][\"image\"]]]\n+        inputs = self.processor(text=text, images=images, padding=True, return_tensors=\"pt\")\n+        generated_ids = model.generate(**inputs, max_new_tokens=64)\n+        batched_generated_texts = self.processor.batch_decode(generated_ids, skip_special_tokens=True)\n+\n+        text = f\"<image>{dataset[40]['query']['en']}\"\n+        images = dataset[40][\"image\"]\n+        inputs = self.processor(text=text, images=images, padding=True, return_tensors=\"pt\")\n+        generated_ids = model.generate(**inputs, max_new_tokens=64)\n+        generated_text_0 = self.processor.batch_decode(generated_ids, skip_special_tokens=True)\n+\n+        text = f\"<image>{dataset[41]['query']['en']}\"\n+        images = dataset[41][\"image\"]\n+        inputs = self.processor(text=text, images=images, padding=True, return_tensors=\"pt\")\n+        generated_ids = model.generate(**inputs, max_new_tokens=64)\n+        generated_text_1 = self.processor.batch_decode(generated_ids, skip_special_tokens=True)\n+\n+        self.assertEqual(batched_generated_texts[0], generated_text_0[0])\n+        self.assertEqual(batched_generated_texts[1], generated_text_1[0])\n+\n     @require_flash_attn\n     @require_torch_gpu\n     @require_bitsandbytes"
        }
    ],
    "stats": {
        "total": 37,
        "additions": 36,
        "deletions": 1
    }
}