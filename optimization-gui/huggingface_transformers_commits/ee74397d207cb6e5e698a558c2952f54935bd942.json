{
    "author": "ArthurZucker",
    "message": "update cb TP (#39361)\n\n* update cb TP\n\n* safety",
    "sha": "ee74397d207cb6e5e698a558c2952f54935bd942",
    "files": [
        {
            "sha": "c8a953f48c06fc1b23b549c7895d65707b62f981",
            "filename": "src/transformers/generation/continuous_batching.py",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee74397d207cb6e5e698a558c2952f54935bd942/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee74397d207cb6e5e698a558c2952f54935bd942/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py?ref=ee74397d207cb6e5e698a558c2952f54935bd942",
            "patch": "@@ -162,6 +162,7 @@ def __init__(\n         dtype: torch.dtype = torch.float16,\n         layer_device_map: Optional[dict[int, Union[str, torch.device, int]]] = None,\n         initial_prompt_shapes: Optional[list[list[int]]] = None,\n+        tp_size: Optional[int] = None,\n     ) -> None:\n         \"\"\"Initialize a paged attention cache for efficient memory usage.\n \n@@ -196,7 +197,16 @@ def __init__(\n \n         self.block_size = block_size\n         self.num_blocks = num_blocks\n-        self.cache_shape = (self.num_key_value_heads, num_blocks, self.block_size, self.head_dim)\n+        num_key_value_heads = self.num_key_value_heads\n+        if tp_size is not None and tp_size > 1:\n+            if num_key_value_heads % tp_size != 0:\n+                raise ValueError(\n+                    f\"Number of key value heads {num_key_value_heads} must be divisible by tensor parallel size {tp_size}.\"\n+                )\n+            # If the model is using tensor parallelism, we need to adjust the number of heads accordingly.\n+            num_key_value_heads //= tp_size\n+\n+        self.cache_shape = (num_key_value_heads, num_blocks, self.block_size, self.head_dim)\n \n         self.dtype = dtype\n         self.device = device\n@@ -1281,6 +1291,7 @@ def _run_generation_loop(self):\n                 self.generation_config,\n                 self.model.device,\n                 self.model.dtype,\n+                tp_size=getattr(self.model, \"tp_size\"),\n             )\n \n             scheduler = None"
        }
    ],
    "stats": {
        "total": 13,
        "additions": 12,
        "deletions": 1
    }
}