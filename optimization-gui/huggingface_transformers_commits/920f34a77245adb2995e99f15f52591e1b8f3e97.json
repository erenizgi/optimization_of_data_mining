{
    "author": "nikosanto13",
    "message": "modular_model_converter bugfix on assignments (#35642)\n\n* added bugfix in modular converter to keep modular assignments for docstrings, expected outputs etc.\n\n* revert stracoder2 docstring copying, add forward in EMU3 to enable docstring assingment, remove verbatim assignments in modular converter\n\n* added _FOR_DOC in assignments to keep, corrected wrong checkpoint name in ijepa's configuration",
    "sha": "920f34a77245adb2995e99f15f52591e1b8f3e97",
    "files": [
        {
            "sha": "20a3247be2bb94f9701d3a88161df89548ca95b3",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/920f34a77245adb2995e99f15f52591e1b8f3e97/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/920f34a77245adb2995e99f15f52591e1b8f3e97/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=920f34a77245adb2995e99f15f52591e1b8f3e97",
            "patch": "@@ -61,6 +61,7 @@\n \n \n logger = logging.get_logger(__name__)\n+\n _CONFIG_FOR_DOC = \"BambaConfig\"\n \n "
        },
        {
            "sha": "9c7207adfc1f99380bdfa4354f7b5c77f99ca82f",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/920f34a77245adb2995e99f15f52591e1b8f3e97/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/920f34a77245adb2995e99f15f52591e1b8f3e97/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=920f34a77245adb2995e99f15f52591e1b8f3e97",
            "patch": "@@ -52,6 +52,7 @@\n \n \n logger = logging.get_logger(__name__)\n+\n _CONFIG_FOR_DOC = \"CohereConfig\"\n \n "
        },
        {
            "sha": "0b38c89d75a5f311a9a94df6597635568ef3d6b1",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/920f34a77245adb2995e99f15f52591e1b8f3e97/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/920f34a77245adb2995e99f15f52591e1b8f3e97/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=920f34a77245adb2995e99f15f52591e1b8f3e97",
            "patch": "@@ -43,6 +43,7 @@\n \n \n logger = logging.get_logger(__name__)\n+\n _CONFIG_FOR_DOC = \"Cohere2Config\"\n \n "
        },
        {
            "sha": "b42a222f6ce98e837b49b67027f735bdfeb9e59d",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 85,
            "deletions": 81,
            "changes": 166,
            "blob_url": "https://github.com/huggingface/transformers/blob/920f34a77245adb2995e99f15f52591e1b8f3e97/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/920f34a77245adb2995e99f15f52591e1b8f3e97/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=920f34a77245adb2995e99f15f52591e1b8f3e97",
            "patch": "@@ -1257,7 +1257,7 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-EMU3_INPUTS_DOCSTRING = r\"\"\"\n+EMU3_TEXT_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n             Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n@@ -1292,19 +1292,15 @@ def forward(self, x, position_ids):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n+            Has to be an instance of [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n+            The model will output the same cache type that is fed as input. If no `past_key_values` are passed, the\n             legacy cache format will be returned.\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n@@ -1366,7 +1362,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n-    @add_start_docstrings_to_model_forward(EMU3_INPUTS_DOCSTRING)\n+    @add_start_docstrings_to_model_forward(EMU3_TEXT_INPUTS_DOCSTRING)\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -1598,77 +1594,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n \n-EMU3_TEXT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n-            `past_key_values`).\n-\n-            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n-            information on the default strategy.\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.n_positions - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache`, *optional*):\n-            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n-            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n-\n-            Has to be an instance of [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n-\n-            The model will output the same cache type that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n-            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n-            of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n-\"\"\"\n-\n-\n class Emu3ForCausalLM(Emu3PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n@@ -1790,6 +1715,85 @@ def forward(\n         )\n \n \n+EMU3_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, max_num_images, max_num_tiles, channels, image_size, image_size)):\n+            The tensors corresponding to the input images. Pixel values can be obtained using\n+            [`AutoImageProcessor`]. See [`Emu3ImageProcessor.__call__`] for details ([]`Emu3Processor`] uses\n+            [`Emu3ImageProcessor`] for processing images).\n+        image_sizes (`torch.LongTensor` of shape `(batch_size, 2)`):\n+                The sizes of the images in the batch, being (height, width) for each image. Image sizes can be obtained using\n+            [`AutoImageProcessor`]. See [`Emu3ImageProcessor.__call__`] for details ([]`Emu3Processor`] uses\n+            [`Emu3ImageProcessor`] for processing images).\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n+            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+\n+            Has to be an instance of [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n+\n+            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n+            legacy cache format will be returned.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n+            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n+            of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n class Emu3ForConditionalGeneration(Emu3PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"text_model.lm_head.weight\"]\n "
        },
        {
            "sha": "aacf52fe31c6badd867f0275551c5610eb873f40",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/920f34a77245adb2995e99f15f52591e1b8f3e97/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/920f34a77245adb2995e99f15f52591e1b8f3e97/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=920f34a77245adb2995e99f15f52591e1b8f3e97",
            "patch": "@@ -1059,6 +1059,10 @@ def __init__(self, config: Emu3Config):\n             [Emu3DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n \n+    @add_start_docstrings_to_model_forward(EMU3_TEXT_INPUTS_DOCSTRING)\n+    def forward(self, **super_kwargs):\n+        super().forward(**super_kwargs)\n+\n \n class Emu3ForCausalLM(LlamaForCausalLM, Emu3PreTrainedModel, GenerationMixin):\n     config_class = Emu3TextConfig"
        },
        {
            "sha": "e59920338aedff437e28f0b6fa45ecc7695c2c9e",
            "filename": "src/transformers/models/ijepa/configuration_ijepa.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/920f34a77245adb2995e99f15f52591e1b8f3e97/src%2Ftransformers%2Fmodels%2Fijepa%2Fconfiguration_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/920f34a77245adb2995e99f15f52591e1b8f3e97/src%2Ftransformers%2Fmodels%2Fijepa%2Fconfiguration_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fconfiguration_ijepa.py?ref=920f34a77245adb2995e99f15f52591e1b8f3e97",
            "patch": "@@ -22,7 +22,7 @@ class IJepaConfig(PretrainedConfig):\n     This is the configuration class to store the configuration of a [`IJepaModel`]. It is used to instantiate an IJEPA\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the I-JEPA\n-    [google/ijepa-base-patch16-224](https://huggingface.co/google/ijepa-base-patch16-224) architecture.\n+    [facebook/ijepa_vith14_1k](https://huggingface.co/facebook/ijepa_vith14_1k) architecture.\n \n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n     documentation from [`PretrainedConfig`] for more information."
        },
        {
            "sha": "e01290b089fd13939ad179d605acc36337f11a1a",
            "filename": "src/transformers/models/ijepa/modeling_ijepa.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/920f34a77245adb2995e99f15f52591e1b8f3e97/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/920f34a77245adb2995e99f15f52591e1b8f3e97/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py?ref=920f34a77245adb2995e99f15f52591e1b8f3e97",
            "patch": "@@ -527,7 +527,9 @@ def forward(self, hidden_states):\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n-_EXPECTED_OUTPUT_SHAPE = [1, 197, 768]\n+\n+\n+_EXPECTED_OUTPUT_SHAPE = [1, 256, 1280]\n \n \n IJEPA_START_DOCSTRING = r\"\"\"\n@@ -640,8 +642,7 @@ def forward(\n         )\n \n \n-# Image classification docstring\n-_IMAGE_CLASS_CHECKPOINT = \"google/ijepa-base-patch16-224\"\n+_IMAGE_CLASS_CHECKPOINT = \"facebook/ijepa_vith14_1k\"\n _IMAGE_CLASS_EXPECTED_OUTPUT = \"Egyptian cat\"\n \n "
        },
        {
            "sha": "03d2b8d00d05488acea1da213434eb1f2a6cb285",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/920f34a77245adb2995e99f15f52591e1b8f3e97/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/920f34a77245adb2995e99f15f52591e1b8f3e97/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=920f34a77245adb2995e99f15f52591e1b8f3e97",
            "patch": "@@ -49,7 +49,6 @@\n \n \n logger = logging.get_logger(__name__)\n-\n _CONFIG_FOR_DOC = \"MoonshineConfig\"\n \n "
        },
        {
            "sha": "33439dff756eb712ecef30e4d73d3445201f4bb8",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/920f34a77245adb2995e99f15f52591e1b8f3e97/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/920f34a77245adb2995e99f15f52591e1b8f3e97/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=920f34a77245adb2995e99f15f52591e1b8f3e97",
            "patch": "@@ -36,7 +36,7 @@\n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"meta-phi/Phi-2-7b-hf\"\n+_CHECKPOINT_FOR_DOC = \"microsoft/phi-1\"\n _CONFIG_FOR_DOC = \"PhiConfig\"\n \n "
        },
        {
            "sha": "1b98d939bf5506843465d9740a29b2ae5e75a0ee",
            "filename": "src/transformers/models/phi/modular_phi.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/920f34a77245adb2995e99f15f52591e1b8f3e97/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/920f34a77245adb2995e99f15f52591e1b8f3e97/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py?ref=920f34a77245adb2995e99f15f52591e1b8f3e97",
            "patch": "@@ -26,6 +26,9 @@\n \n logger = logging.get_logger(__name__)\n \n+_CHECKPOINT_FOR_DOC = \"microsoft/phi-1\"\n+_CONFIG_FOR_DOC = \"PhiConfig\"\n+\n \n class PhiAttention(LlamaAttention):\n     def __init__(self, config: PhiConfig, layer_idx: int):"
        },
        {
            "sha": "500f96e3e30b95f96771df653b17327c72b3dcfc",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/920f34a77245adb2995e99f15f52591e1b8f3e97/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/920f34a77245adb2995e99f15f52591e1b8f3e97/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=920f34a77245adb2995e99f15f52591e1b8f3e97",
            "patch": "@@ -56,6 +56,7 @@\n \n logger = logging.get_logger(__name__)\n _CHECKPOINT_FOR_DOC = \"bigcode/starcoder2-7b\"\n+\n _CONFIG_FOR_DOC = \"Starcoder2Config\"\n \n "
        },
        {
            "sha": "9730b6d2cb3c921fdb51e960d482654f12d70071",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/920f34a77245adb2995e99f15f52591e1b8f3e97/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/920f34a77245adb2995e99f15f52591e1b8f3e97/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=920f34a77245adb2995e99f15f52591e1b8f3e97",
            "patch": "@@ -268,7 +268,7 @@ def merge_docstrings(original_docstring, updated_docstring):\n             updated_docstring = \"\".join(\n                 [\n                     parts[0].rstrip(\" \\n\") + new_parts[0],\n-                    f\"\\n{original_level*' '}```\",\n+                    f\"\\n{original_level * ' '}```\",\n                     parts[1],\n                     \"```\",\n                     parts[2],\n@@ -515,10 +515,8 @@ def forward(...):\n     return all_dependencies_with_parent\n \n \n-# These top-level variables will always use the value in the `modular_xxx.py` file\n-ASSIGNMENTS_TO_KEEP = {\n-    \"_CHECKPOINT_FOR_DOC\",\n-}\n+# Top-level variables that match the following patterns will always use the value in the `modular_xxx.py` file\n+ASSIGNMENTS_REGEX_TO_KEEP = [r\"_CHECKPOINT\", r\"_EXPECTED\", r\"_FOR_DOC\"]\n \n \n class ClassDependencyMapper(CSTVisitor):\n@@ -828,12 +826,14 @@ def _merge_functions(self, functions: dict[str, cst.CSTNode], object_mapping: di\n     def _merge_assignments(self, assignments: dict[str, cst.CSTNode], object_mapping: dict[str, set]):\n         \"\"\"Update the global nodes with the assignment from the modular file.\n \n-        Merging rule: if any assignment with the same name was redefined in the modular, we use it and its dependencies ONLY if it is\n-        in `ASSIGNMENTS_TO_KEEP`. Otherwise, we use the original value and dependencies. This rule was chosen to avoid having to rewrite the\n+        Merging rule: if any assignment with the same name was redefined in the modular, we use it and its dependencies ONLY if it matches\n+        a pattern in `ASSIGNMENTS_REGEX_TO_KEEP`. Otherwise, we use the original value and dependencies. This rule was chosen to avoid having to rewrite the\n         big docstrings.\n         \"\"\"\n         for assignment, node in assignments.items():\n-            if assignment in ASSIGNMENTS_TO_KEEP or assignment not in self.assignments:\n+            should_keep = any(re.search(pattern, assignment) for pattern in ASSIGNMENTS_REGEX_TO_KEEP)\n+\n+            if should_keep or assignment not in self.assignments:\n                 self.assignments[assignment] = node\n                 if assignment in object_mapping:\n                     self.object_dependency_mapping[assignment] = object_mapping[assignment]\n@@ -1404,7 +1404,7 @@ class NewModelNameTextDecoderLayer(LlamaDecoderLayer):\n             ]\n             if len(modeling_bases) > 1:\n                 raise ValueError(\n-                    f\"{class_name} was defined with more than 1 model-specific super class. This is unsupported. We found {*modeling_bases,}.\"\n+                    f\"{class_name} was defined with more than 1 model-specific super class. This is unsupported. We found {(*modeling_bases,)}.\"\n                 )\n             if len(modeling_bases) == 1:\n                 filename = self.model_specific_imported_objects[modeling_bases[0]]\n@@ -1432,7 +1432,7 @@ class NewModelNameTextDecoderLayer(LlamaDecoderLayer):\n             if final_name != cased_default_name and has_prefix_collision:\n                 if len(prefixes_counter) > 1:\n                     logger.warning(\n-                        f\"We detected multiple prefix names when inheriting from {file}: {*set(prefixes_counter),}. However, the \"\n+                        f\"We detected multiple prefix names when inheriting from {file}: {(*set(prefixes_counter),)}. However, the \"\n                         f\"most used one, '{final_name}', is already present in the source file and will likely cause consistency \"\n                         f\"issues. For this reason we fallback to the default prefix '{cased_default_name}' when grabbing args \"\n                         \"and dependencies. Make sure to subclass the intermediate classes with the prefix you want (if different \"\n@@ -1448,7 +1448,7 @@ class NewModelNameTextDecoderLayer(LlamaDecoderLayer):\n                 final_name = cased_default_name\n             elif len(prefixes_counter) > 1:\n                 logger.warning(\n-                    f\"We detected multiple prefix names when inheriting from {file}: {*set(prefixes_counter),}. We will only \"\n+                    f\"We detected multiple prefix names when inheriting from {file}: {(*set(prefixes_counter),)}. We will only \"\n                     f\"use the most used '{final_name}' prefix when grabbing args and dependencies. Make sure to subclass the \"\n                     f\"intermediate classes with the prefix you want (if different from '{final_name}') or use a single prefix \"\n                     \"in all the modular (best).\""
        }
    ],
    "stats": {
        "total": 211,
        "additions": 113,
        "deletions": 98
    }
}