{
    "author": "ducviet00",
    "message": "Add support for Florence-2 (#38188)\n\n* init\n\n* add modular\n\n* fixup\n\n* update configuration\n\n* add processing file\n\n* update auto files\n\n* update\n\n* update modular\n\n* green setup_and_quality ci\n\n* it works\n\n* fix some tests\n\n* commit florence2\n\n* update test\n\n* make test cases done - 16 left\n\n* style\n\n* fix few test cases\n\n* fix some tests\n\n* fix init test\n\n* update florence2 vision style\n\n* hope is green\n\n* fix init test\n\n* fix init\n\n* update modular\n\n* refactor vision module\n\n* fix: channel attention use dynamic scale\n\n* update modular\n\n* update\n\n* update attention mask\n\n* update\n\n* fix naming\n\n* Update src/transformers/models/florence2/processing_florence2.py\n\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\n\n* spatial block works\n\n* more beautiful\n\n* more more beautiful\n\n* merge main\n\n* merge main and fixup\n\n* fix typing hint\n\n* update modeling\n\n* fix eager matches sdpa\n\n* fix style\n\n* fix compile test - all green\n\n* remove florence2 language\n\n* remove Florence2LanguageModel things\n\n* fix style\n\n* update florence2 model\n\n* override prepare encoder_decoder for generation\n\n* add weight conversion script\n\n* rewrite channel attention to use sdpa\n\n* eleminate 1 tranpose op\n\n* support fa2\n\n* fix quality check\n\n* chore: reformat `test_modeling_florence2.py`\n\n* some refactor for processor\n\n* some refactor for processor\n\n* update naming convention and remove BC\n\n* make it pass the test\n\n* fix: correct Embedding Cosine\n\n* update comments and docstring\n\n* support input_embeds\n\n* support input embeds ideally\n\n* fix style\n\n* fix style\n\n* fix style again :D\n\n* add test prcoessor\n\n* refactor processor and add test for processor\n\n* reformat test processor\n\n* make fixup\n\n* fix schema check\n\n* remove image_token\n\n* ensure image token in tokenizer and fix integration tests\n\n* fix processor test\n\n* add more integration tests for large model and rename test_processor to test_processing\n\n* test_assisted_decoding_sample should pass\n\n* update doc and make model work with image text to text pipeline\n\n* docs: add sdpa bagde\n\n* resolve cyril's comments\n\n* fix import torch error\n\n* add helper get_placeholder_mask\n\n* inherit from llava\n\n* florence2 may not _supports_attention_backend because of bart ...\n\n* move florence2 model card to multimodal\n\n* let base model always return_dict\n\n* fix style\n\n* tiny update doc\n\n* set   _checkpoint_conversion_mapping = {}\n\n* fix code quality\n\n* support flex and compile graph and move external func to internal func\n\n* remove condition because it always true\n\n* remove window funcs\n\n* move post processor config out\n\n* fix ci\n\n* new intro to trigger test\n\n* remove `kernel_size` argument\n\n---------\n\nCo-authored-by: ducviet00-h2 <viet.d.hoang@h2corporation.jp>\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>",
    "sha": "ca543f822f73ebc69b00835c74ae927d9730b6f5",
    "files": [
        {
            "sha": "6628528b81da216e1df6f1f96b23ca9a92b9c7c2",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca543f822f73ebc69b00835c74ae927d9730b6f5/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca543f822f73ebc69b00835c74ae927d9730b6f5/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=ca543f822f73ebc69b00835c74ae927d9730b6f5",
            "patch": "@@ -1005,6 +1005,8 @@\n         title: Evolla\n       - local: model_doc/flava\n         title: FLAVA\n+      - local: model_doc/florence2\n+        title: Florence2\n       - local: model_doc/gemma3\n         title: Gemma3\n       - local: model_doc/gemma3n"
        },
        {
            "sha": "1fe3250f0cbe10755ef5090403cc3e379cea302b",
            "filename": "docs/source/en/model_doc/florence2.md",
            "status": "added",
            "additions": 185,
            "deletions": 0,
            "changes": 185,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca543f822f73ebc69b00835c74ae927d9730b6f5/docs%2Fsource%2Fen%2Fmodel_doc%2Fflorence2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca543f822f73ebc69b00835c74ae927d9730b6f5/docs%2Fsource%2Fen%2Fmodel_doc%2Fflorence2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fflorence2.md?ref=ca543f822f73ebc69b00835c74ae927d9730b6f5",
            "patch": "@@ -0,0 +1,185 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n+# Florence-2\n+\n+[Florence-2](https://arxiv.org/abs/2311.06242) is an advanced vision foundation model that uses a prompt-based approach to handle a wide range of vision and vision-language tasks. Florence-2 can interpret simple text prompts to perform tasks like captioning, object detection, and segmentation. It leverages the FLD-5B dataset, containing 5.4 billion annotations across 126 million images, to master multi-task learning. The model's sequence-to-sequence architecture enables it to excel in both zero-shot and fine-tuned settings, proving to be a competitive vision foundation model.\n+\n+You can find all the original Florence-2 checkpoints under the [Florence-2](https://huggingface.co/models?other=florence-2) collection.\n+\n+> [!TIP]\n+> This model was contributed by [ducviet00](https://huggingface.co/ducviet00).\n+> Click on the Florence-2 models in the right sidebar for more examples of how to apply Florence-2 to different vision and language tasks.\n+\n+The example below demonstrates how to perform object detection with [`Pipeline`] or the [`AutoModel`] class.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+import torch\n+import requests\n+from PIL import Image\n+from transformers import pipeline\n+\n+pipeline = pipeline(\n+    \"image-text-to-text\",\n+    model=\"ducviet00/Florence-2-base-hf\",\n+    device=0,\n+    torch_dtype=torch.bfloat16\n+)\n+\n+pipeline(\n+    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\",\n+    text=\"<OD>\"\n+)\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+import requests\n+from PIL import Image\n+from transformers import AutoProcessor, Florence2ForConditionalGeneration\n+\n+url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\n+image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n+\n+model = Florence2ForConditionalGeneration.from_pretrained(\"microsoft/Florence-2-base\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base\")\n+\n+task_prompt = \"<OD>\"\n+inputs = processor(text=task_prompt, images=image, return_tensors=\"pt\").to(model.device)\n+\n+generated_ids = model.generate(\n+    **inputs,\n+    max_new_tokens=1024,\n+    num_beams=3,\n+)\n+generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n+\n+image_size = image.size\n+parsed_answer = processor.post_process_generation(generated_text, task=task_prompt, image_size=image_size)\n+print(parsed_answer)\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n+\n+The example below uses [bitsandbytes](../quantization/bitsandbytes) to quantize the model to 4-bit.\n+\n+```py\n+# pip install bitsandbytes\n+import torch\n+import requests\n+from PIL import Image\n+from transformers import AutoProcessor, Florence2ForConditionalGeneration, BitsAndBytesConfig\n+\n+quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n+\n+model = Florence2ForConditionalGeneration.from_pretrained(\n+    \"microsoft/Florence-2-large\",\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"auto\",\n+    quantization_config=quantization_config\n+)\n+processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-large\")\n+\n+url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\n+image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n+\n+task_prompt = \"<OD>\"\n+inputs = processor(text=task_prompt, images=image, return_tensors=\"pt\").to(model.device, torch.bfloat16)\n+\n+generated_ids = model.generate(\n+    **inputs,\n+    max_new_tokens=1024,\n+    num_beams=3,\n+)\n+generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n+\n+image_size = image.size\n+parsed_answer = processor.post_process_generation(generated_text, task=task_prompt, image_size=image_size)\n+\n+print(parsed_answer)\n+```\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"\"/>\n+</div>\n+\n+## Notes\n+\n+- Florence-2 is a prompt-based model. You need to provide a task prompt to tell the model what to do. Supported tasks are:\n+    - `<OCR>`\n+    - `<OCR_WITH_REGION>`\n+    - `<CAPTION>`\n+    - `<DETAILED_CAPTION>`\n+    - `<MORE_DETAILED_CAPTION>`\n+    - `<OD>`\n+    - `<DENSE_REGION_CAPTION>`\n+    - `<CAPTION_TO_PHRASE_GROUNDING>`\n+    - `<REFERRING_EXPRESSION_SEGMENTATION>`\n+    - `<REGION_TO_SEGMENTATION>`\n+    - `<OPEN_VOCABULARY_DETECTION>`\n+    - `<REGION_TO_CATEGORY>`\n+    - `<REGION_TO_DESCRIPTION>`\n+    - `<REGION_TO_OCR>`\n+    - `<REGION_PROPOSAL>`\n+- The raw output of the model is a string that needs to be parsed. The [`Florence2Processor`] has a [`~Florence2Processor.post_process_generation`] method that can parse the string into a more usable format, like bounding boxes and labels for object detection.\n+\n+## Resources\n+\n+- [Florence-2 technical report](https://arxiv.org/abs/2311.06242)\n+- [Jupyter Notebook for inference and visualization of Florence-2-large model](https://huggingface.co/microsoft/Florence-2-large/blob/main/sample_inference.ipynb)\n+\n+## Florence2VisionConfig\n+\n+[[autodoc]] Florence2VisionConfig\n+\n+## Florence2Config\n+\n+[[autodoc]] Florence2Config\n+\n+## Florence2Processor\n+\n+[[autodoc]] Florence2Processor\n+\n+## Florence2Model\n+\n+[[autodoc]] Florence2Model\n+    - forward\n+\n+## Florence2ForConditionalGeneration\n+\n+[[autodoc]] Florence2ForConditionalGeneration\n+    - forward\n+\n+## Florence2VisionBackbone\n+\n+[[autodoc]] Florence2VisionBackbone\n+    - forward"
        },
        {
            "sha": "3a2e1fe32823554e26869bee03a3cd4475c05000",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca543f822f73ebc69b00835c74ae927d9730b6f5/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca543f822f73ebc69b00835c74ae927d9730b6f5/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=ca543f822f73ebc69b00835c74ae927d9730b6f5",
            "patch": "@@ -123,6 +123,7 @@\n     from .fastspeech2_conformer import *\n     from .flaubert import *\n     from .flava import *\n+    from .florence2 import *\n     from .fnet import *\n     from .focalnet import *\n     from .fsmt import *"
        },
        {
            "sha": "715da85df740cb1d860d0d07917ce204221a6373",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca543f822f73ebc69b00835c74ae927d9730b6f5/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca543f822f73ebc69b00835c74ae927d9730b6f5/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=ca543f822f73ebc69b00835c74ae927d9730b6f5",
            "patch": "@@ -147,6 +147,7 @@\n         (\"fastspeech2_conformer_with_hifigan\", \"FastSpeech2ConformerWithHifiGanConfig\"),\n         (\"flaubert\", \"FlaubertConfig\"),\n         (\"flava\", \"FlavaConfig\"),\n+        (\"florence2\", \"Florence2Config\"),\n         (\"fnet\", \"FNetConfig\"),\n         (\"focalnet\", \"FocalNetConfig\"),\n         (\"fsmt\", \"FSMTConfig\"),\n@@ -565,6 +566,7 @@\n         (\"flan-ul2\", \"FLAN-UL2\"),\n         (\"flaubert\", \"FlauBERT\"),\n         (\"flava\", \"FLAVA\"),\n+        (\"florence2\", \"Florence2\"),\n         (\"fnet\", \"FNet\"),\n         (\"focalnet\", \"FocalNet\"),\n         (\"fsmt\", \"FairSeq Machine-Translation\"),"
        },
        {
            "sha": "0d52ba7312f67aa9eb82bd243bf6163d4dbda7cc",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca543f822f73ebc69b00835c74ae927d9730b6f5/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca543f822f73ebc69b00835c74ae927d9730b6f5/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=ca543f822f73ebc69b00835c74ae927d9730b6f5",
            "patch": "@@ -149,6 +149,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"fastspeech2_conformer_with_hifigan\", \"FastSpeech2ConformerWithHifiGan\"),\n         (\"flaubert\", \"FlaubertModel\"),\n         (\"flava\", \"FlavaModel\"),\n+        (\"florence2\", \"Florence2Model\"),\n         (\"fnet\", \"FNetModel\"),\n         (\"focalnet\", \"FocalNetModel\"),\n         (\"fsmt\", \"FSMTModel\"),\n@@ -438,6 +439,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"falcon_mamba\", \"FalconMambaForCausalLM\"),\n         (\"flaubert\", \"FlaubertWithLMHeadModel\"),\n         (\"flava\", \"FlavaForPreTraining\"),\n+        (\"florence2\", \"Florence2ForConditionalGeneration\"),\n         (\"fnet\", \"FNetForPreTraining\"),\n         (\"fsmt\", \"FSMTForConditionalGeneration\"),\n         (\"funnel\", \"FunnelForPreTraining\"),\n@@ -982,6 +984,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"deepseek_vl_hybrid\", \"DeepseekVLHybridForConditionalGeneration\"),\n         (\"emu3\", \"Emu3ForConditionalGeneration\"),\n         (\"evolla\", \"EvollaForProteinText2Text\"),\n+        (\"florence2\", \"Florence2ForConditionalGeneration\"),\n         (\"fuyu\", \"FuyuForCausalLM\"),\n         (\"gemma3\", \"Gemma3ForConditionalGeneration\"),\n         (\"gemma3n\", \"Gemma3nForConditionalGeneration\"),"
        },
        {
            "sha": "d8db58cb7b1f630292d0c668d3e2ae6299811a6d",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca543f822f73ebc69b00835c74ae927d9730b6f5/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca543f822f73ebc69b00835c74ae927d9730b6f5/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=ca543f822f73ebc69b00835c74ae927d9730b6f5",
            "patch": "@@ -69,6 +69,7 @@\n         (\"emu3\", \"Emu3Processor\"),\n         (\"evolla\", \"EvollaProcessor\"),\n         (\"flava\", \"FlavaProcessor\"),\n+        (\"florence2\", \"Florence2Processor\"),\n         (\"fuyu\", \"FuyuProcessor\"),\n         (\"gemma3\", \"Gemma3Processor\"),\n         (\"gemma3n\", \"Gemma3nProcessor\"),"
        },
        {
            "sha": "686295d1fbd952e124bab885980ac682261a07fb",
            "filename": "src/transformers/models/florence2/__init__.py",
            "status": "added",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca543f822f73ebc69b00835c74ae927d9730b6f5/src%2Ftransformers%2Fmodels%2Fflorence2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca543f822f73ebc69b00835c74ae927d9730b6f5/src%2Ftransformers%2Fmodels%2Fflorence2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2F__init__.py?ref=ca543f822f73ebc69b00835c74ae927d9730b6f5",
            "patch": "@@ -0,0 +1,28 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_florence2 import *\n+    from .modeling_florence2 import *\n+    from .processing_florence2 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "4bbd4b3a03e9f2c591e77689e1606c60214ab430",
            "filename": "src/transformers/models/florence2/configuration_florence2.py",
            "status": "added",
            "additions": 215,
            "deletions": 0,
            "changes": 215,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca543f822f73ebc69b00835c74ae927d9730b6f5/src%2Ftransformers%2Fmodels%2Fflorence2%2Fconfiguration_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca543f822f73ebc69b00835c74ae927d9730b6f5/src%2Ftransformers%2Fmodels%2Fflorence2%2Fconfiguration_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fconfiguration_florence2.py?ref=ca543f822f73ebc69b00835c74ae927d9730b6f5",
            "patch": "@@ -0,0 +1,215 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/florence2/modular_florence2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_florence2.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 Microsoft and the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Florence2VisionConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Florence2VisionModel`]. It is used to instantiate a Florence2VisionModel\n+    according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the Florence2VisionModel architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        in_channels (`int`, *optional*, defaults to 3):\n+            Number of input image channels.\n+        depths (`Tuple[int]`, *optional*, defaults to `(1, 1, 9, 1)`):\n+            The depth of the model.\n+        patch_size (`Tuple[int]`, *optional*, defaults to `(7, 3, 3, 3)`):\n+            The patch size of the image.\n+        patch_stride (`Tuple[int]`, *optional*, defaults to `(4, 2, 2, 2)`):\n+            The patch stride of the image.\n+        patch_padding (`Tuple[int]`, *optional*, defaults to `(3, 1, 1, 1)`):\n+            The patch padding of the image.\n+        patch_prenorm (`Tuple[bool]`, *optional*, defaults to `(False, True, True, True)`):\n+            Whether to apply layer normalization before the patch embedding layer.\n+        embed_dim (`Tuple[int]`, *optional*, defaults to `(128, 256, 512, 1024)`):\n+            The dimension of the embedding layer.\n+        num_heads (`Tuple[int]`, *optional*, defaults to `(4, 8, 16, 32)`):\n+            The number of attention heads.\n+        num_groups (`Tuple[int]`, *optional*, defaults to `(4, 8, 16, 32)`):\n+            The number of groups.\n+        window_size (`int`, *optional*, defaults to 12):\n+            The window size of the model.\n+        drop_path_rate (`float`, *optional*, defaults to 0.1):\n+            The dropout rate of the drop path layer.\n+        mlp_ratio (`int`, *optional*, defaults to 4.0):\n+            Ratio of mlp hidden dim to embedding dim.\n+        qkv_bias (`bool`, *optional*, defaults to `True`):\n+            If True, add a learnable bias to query, key, value.\n+        activation_function (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n+        projection_dim (`int`, *optional*, defaults to 1024):\n+            The dimension of the projection layer.\n+        max_temporal_embeddings (`int`, *optional*, defaults to 100):\n+            The configuration of the visual temporal embedding.\n+        max_position_embeddings (`int`, *optional*, defaults to 50):\n+            The configuration of the image position embedding.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+    Example:\n+\n+    ```python\n+    >>> from transformers import Florence2VisionConfig, Florence2VisionModel\n+\n+    >>> # Initializing a Florence2 Vision style configuration\n+    >>> configuration = Florence2VisionConfig()\n+\n+    >>> # Initializing a model (with random weights)\n+    >>> model = Florence2VisionModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"florence_vision\"\n+\n+    def __init__(\n+        self,\n+        in_channels=3,\n+        depths=(1, 1, 9, 1),\n+        patch_size=(7, 3, 3, 3),\n+        patch_stride=(4, 2, 2, 2),\n+        patch_padding=(3, 1, 1, 1),\n+        patch_prenorm=(False, True, True, True),\n+        embed_dim=(128, 256, 512, 1024),\n+        num_heads=(4, 8, 16, 32),\n+        num_groups=(4, 8, 16, 32),\n+        window_size=12,\n+        drop_path_rate=0.1,\n+        mlp_ratio=4.0,\n+        qkv_bias=True,\n+        activation_function=\"gelu\",\n+        projection_dim=1024,\n+        max_temporal_embeddings=100,\n+        max_position_embeddings=50,\n+        initializer_range=0.02,\n+        **kwargs,\n+    ):\n+        self.in_channels = in_channels\n+        self.depths = list(depths)\n+        self.patch_size = list(patch_size)\n+        self.patch_stride = list(patch_stride)\n+        self.patch_padding = list(patch_padding)\n+        self.patch_prenorm = list(patch_prenorm)\n+        self.embed_dim = list(embed_dim)\n+        self.num_heads = list(num_heads)\n+        self.num_groups = list(num_groups)\n+        self.window_size = window_size\n+        self.drop_path_rate = drop_path_rate\n+        self.mlp_ratio = mlp_ratio\n+        self.qkv_bias = qkv_bias\n+        self.projection_dim = projection_dim\n+        self.max_temporal_embeddings = max_temporal_embeddings\n+        self.max_position_embeddings = max_position_embeddings\n+        self.initializer_range = initializer_range\n+        self.activation_function = activation_function\n+\n+        super().__init__(**kwargs)\n+\n+\n+class Florence2Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Florence2ForConditionalGeneration`]. It is used to instantiate an\n+    Florence-2 model according to the specified arguments, defining the model architecture.\n+\n+    Instantiating a configuration with the defaults will yield a similar configuration to that of the Florence-2\n+    [microsoft/Florence-2-base](https://huggingface.co/microsoft/Florence-2-base) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        text_config (`dict`, *optional*):\n+            Dictionary of configuration options used to initialize [`AutoConfig`].\n+        vision_config (`dict`, *optional*):\n+            Dictionary of configuration options used to initialize [`Florence2VisionConfig`].\n+        image_token_id (`int`, *optional*, defaults to 51289):\n+            The image token index to encode the image prompt.\n+        is_encoder_decoder (bool, optional, *optional*, defaults to `True`):\n+            Whether the model is used as an encoder/decoder or not.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import Florence2ForConditionalGeneration, Florence2Config, CLIPVisionConfig, BartConfig\n+\n+    >>> # Initializing a clip-like vision config\n+    >>> vision_config = CLIPVisionConfig()\n+\n+    >>> # Initializing a Bart config\n+    >>> text_config = BartConfig()\n+\n+    >>> # Initializing a Florence-2 configuration\n+    >>> configuration = Florence2Config(vision_config, text_config)\n+\n+    >>> # Initializing a model from the florence-2 configuration\n+    >>> model = Florence2ForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"florence2\"\n+    sub_configs = {\n+        \"text_config\": AutoConfig,\n+        \"vision_config\": Florence2VisionConfig,\n+    }\n+\n+    def __init__(\n+        self,\n+        text_config=None,\n+        vision_config=None,\n+        image_token_id=51289,\n+        is_encoder_decoder=True,\n+        **kwargs,\n+    ):\n+        if isinstance(text_config, dict):\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"bart\")\n+            text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n+        elif text_config is None:\n+            text_config = CONFIG_MAPPING[\"bart\"]()\n+\n+        if isinstance(vision_config, dict):\n+            vision_config = Florence2VisionConfig(**vision_config)\n+        elif vision_config is None:\n+            logger.info(\"vision_config is None. Initializing the Florence2VisionConfig with default values.\")\n+            vision_config = Florence2VisionConfig()\n+\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.image_token_id = image_token_id\n+\n+        super().__init__(\n+            is_encoder_decoder=is_encoder_decoder,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"Florence2Config\", \"Florence2VisionConfig\"]"
        },
        {
            "sha": "0a0ff4084e9e1bb63ca4d39c276a3803e96ea5bd",
            "filename": "src/transformers/models/florence2/convert_florence2_original_pytorch_to_hf.py",
            "status": "added",
            "additions": 530,
            "deletions": 0,
            "changes": 530,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca543f822f73ebc69b00835c74ae927d9730b6f5/src%2Ftransformers%2Fmodels%2Fflorence2%2Fconvert_florence2_original_pytorch_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca543f822f73ebc69b00835c74ae927d9730b6f5/src%2Ftransformers%2Fmodels%2Fflorence2%2Fconvert_florence2_original_pytorch_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fconvert_florence2_original_pytorch_to_hf.py?ref=ca543f822f73ebc69b00835c74ae927d9730b6f5",
            "patch": "@@ -0,0 +1,530 @@\n+# coding=utf-8\n+# Copyright 2025 Microsoft and the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import argparse\n+from collections import OrderedDict\n+\n+import torch\n+\n+from transformers import (\n+    AddedToken,\n+    AutoConfig,\n+    AutoModelForCausalLM,\n+    AutoProcessor,\n+    Florence2Config,\n+    Florence2ForConditionalGeneration,\n+    Florence2Processor,\n+    Florence2VisionConfig,\n+)\n+\n+\n+def convert_config(original_config: dict):\n+    new_config = Florence2VisionConfig(\n+        embed_dim=original_config[\"dim_embed\"],\n+        max_temporal_embeddings=original_config[\"visual_temporal_embedding\"][\"max_temporal_embeddings\"],\n+        max_pos_embeddings=original_config[\"image_pos_embed\"][\"max_pos_embeddings\"],\n+        **original_config,\n+    )\n+\n+    return new_config\n+\n+\n+def vision_conv_embeddings(idx):\n+    \"\"\"\n+    The function helps in renaming vision convolution embedding layer weights.\n+\n+    Args:\n+        idx: stage number in original model\n+    \"\"\"\n+    convs = []\n+    convs.append(\n+        (\n+            f\"vision_tower.convs.{idx}.proj.weight\",\n+            f\"model.vision_tower.convs.{idx}.conv.weight\",\n+        )\n+    )\n+    convs.append(\n+        (\n+            f\"vision_tower.convs.{idx}.proj.bias\",\n+            f\"model.vision_tower.convs.{idx}.conv.bias\",\n+        )\n+    )\n+    convs.append(\n+        (\n+            f\"vision_tower.convs.{idx}.norm.weight\",\n+            f\"model.vision_tower.convs.{idx}.norm.weight\",\n+        )\n+    )\n+    convs.append(\n+        (\n+            f\"vision_tower.convs.{idx}.norm.bias\",\n+            f\"model.vision_tower.convs.{idx}.norm.bias\",\n+        )\n+    )\n+    return convs\n+\n+\n+def vision_spatial_block(stage_idx, block_idx):\n+    \"\"\"\n+    The function helps in renaming vision spatial block layers weights.\n+\n+    Args:\n+        idx: stage number in original model\n+        cnt: count of blocks in each stage\n+    \"\"\"\n+    spatial_block = []\n+    spatial_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.conv1.fn.dw.weight\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.conv1.weight\",\n+        )\n+    )\n+    spatial_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.conv1.fn.dw.bias\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.conv1.bias\",\n+        )\n+    )\n+    spatial_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.window_attn.norm.weight\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.norm1.weight\",\n+        )\n+    )\n+    spatial_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.window_attn.norm.bias\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.norm1.bias\",\n+        )\n+    )\n+    spatial_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.window_attn.fn.qkv.weight\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.window_attn.qkv.weight\",\n+        )\n+    )\n+    spatial_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.window_attn.fn.qkv.bias\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.window_attn.qkv.bias\",\n+        )\n+    )\n+    spatial_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.window_attn.fn.proj.weight\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.window_attn.proj.weight\",\n+        )\n+    )\n+    spatial_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.window_attn.fn.proj.bias\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.window_attn.proj.bias\",\n+        )\n+    )\n+    spatial_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.conv2.fn.dw.weight\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.conv2.weight\",\n+        )\n+    )\n+    spatial_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.conv2.fn.dw.bias\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.conv2.bias\",\n+        )\n+    )\n+    spatial_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.ffn.norm.weight\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.norm2.weight\",\n+        )\n+    )\n+    spatial_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.ffn.norm.bias\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.norm2.bias\",\n+        )\n+    )\n+    spatial_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.ffn.fn.net.fc1.weight\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.ffn.fc1.weight\",\n+        )\n+    )\n+    spatial_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.ffn.fn.net.fc1.bias\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.ffn.fc1.bias\",\n+        )\n+    )\n+    spatial_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.ffn.fn.net.fc2.weight\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.ffn.fc2.weight\",\n+        )\n+    )\n+    spatial_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.ffn.fn.net.fc2.bias\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.spatial_block.ffn.fc2.bias\",\n+        )\n+    )\n+    return spatial_block\n+\n+\n+def vision_channel_block(stage_idx, block_idx):\n+    \"\"\"\n+    The function helps in renaming vision channel block layers weights.\n+\n+    Args:\n+        idx: stage number in original model\n+        cnt: count of blocks in each stage\n+    \"\"\"\n+    channel_block = []\n+    channel_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.conv1.fn.dw.weight\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.conv1.weight\",\n+        )\n+    )\n+    channel_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.conv1.fn.dw.bias\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.conv1.bias\",\n+        )\n+    )\n+    channel_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.channel_attn.norm.weight\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.norm1.weight\",\n+        )\n+    )\n+    channel_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.channel_attn.norm.bias\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.norm1.bias\",\n+        )\n+    )\n+    channel_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.channel_attn.fn.qkv.weight\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.channel_attn.qkv.weight\",\n+        )\n+    )\n+    channel_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.channel_attn.fn.qkv.bias\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.channel_attn.qkv.bias\",\n+        )\n+    )\n+    channel_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.channel_attn.fn.proj.weight\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.channel_attn.proj.weight\",\n+        )\n+    )\n+    channel_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.channel_attn.fn.proj.bias\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.channel_attn.proj.bias\",\n+        )\n+    )\n+    channel_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.conv2.fn.dw.weight\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.conv2.weight\",\n+        )\n+    )\n+    channel_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.conv2.fn.dw.bias\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.conv2.bias\",\n+        )\n+    )\n+    channel_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.ffn.norm.weight\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.norm2.weight\",\n+        )\n+    )\n+    channel_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.ffn.norm.bias\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.norm2.bias\",\n+        )\n+    )\n+    channel_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.ffn.fn.net.fc1.weight\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.ffn.fc1.weight\",\n+        )\n+    )\n+    channel_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.ffn.fn.net.fc1.bias\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.ffn.fc1.bias\",\n+        )\n+    )\n+    channel_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.ffn.fn.net.fc2.weight\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.ffn.fc2.weight\",\n+        )\n+    )\n+    channel_block.append(\n+        (\n+            f\"vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.ffn.fn.net.fc2.bias\",\n+            f\"model.vision_tower.blocks.{stage_idx}.{block_idx}.channel_block.ffn.fc2.bias\",\n+        )\n+    )\n+    return channel_block\n+\n+\n+def multi_modal_projector():\n+    \"\"\"\n+    Function helps in renaming final classification layer\n+    \"\"\"\n+    projector = []\n+    projector.append((\"image_projection\", \"model.multi_modal_projector.image_projection.weight\"))\n+    projector.append((\"image_proj_norm.weight\", \"model.multi_modal_projector.image_proj_norm.weight\"))\n+    projector.append((\"image_proj_norm.bias\", \"model.multi_modal_projector.image_proj_norm.bias\"))\n+    projector.append(\n+        (\n+            \"image_pos_embed.row_embeddings.weight\",\n+            \"model.multi_modal_projector.image_position_embed.row_embeddings.weight\",\n+        )\n+    )\n+    projector.append(\n+        (\n+            \"image_pos_embed.column_embeddings.weight\",\n+            \"model.multi_modal_projector.image_position_embed.column_embeddings.weight\",\n+        )\n+    )\n+    projector.append(\n+        (\n+            \"visual_temporal_embed.pos_idx_to_embed\",\n+            \"model.multi_modal_projector.visual_temporal_embed.pos_idx_to_embed\",\n+        )\n+    )\n+    return projector\n+\n+\n+def language_model(state_dict):\n+    language_state_dict_keys = []\n+    for key in state_dict.keys():\n+        if key.startswith(\"language_model.model\") and \"lm_head\" not in key:\n+            new_key = key.replace(\"language_model.model.\", \"model.language_model.\")\n+            language_state_dict_keys.append((key, new_key))\n+    language_state_dict_keys.append((\"language_model.lm_head.weight\", \"lm_head.weight\"))\n+    return language_state_dict_keys\n+\n+\n+def convert_florence2_checkpoint(hf_model_id, pytorch_dump_folder, output_hub_path):\n+    \"\"\"\n+    Function to convert the microsoft florence2 checkpoint to huggingface checkpoint\n+    \"\"\"\n+\n+    hf_config = AutoConfig.from_pretrained(hf_model_id, trust_remote_code=True)\n+    hf_model = AutoModelForCausalLM.from_pretrained(\n+        hf_model_id, trust_remote_code=True, torch_dtype=torch.float16, attn_implementation=\"eager\"\n+    )\n+    hf_processor = AutoProcessor.from_pretrained(hf_model_id, trust_remote_code=True)\n+    huggingface_weights = OrderedDict()\n+    list_of_state_dict = []\n+\n+    image_processor = hf_processor.image_processor\n+\n+    tokenizer = hf_processor.tokenizer\n+    tokenizer.image_token = \"<image>\"\n+    tokenizer.add_tokens(AddedToken(tokenizer.image_token, special=True, normalized=False), special_tokens=True)\n+    tokenizer.image_token_id = tokenizer.encode(tokenizer.image_token, add_special_tokens=False)[0]\n+    tokenizer.extra_special_tokens = {\"image_token\": \"<image>\"}\n+\n+    post_processor_config = {\n+        \"ocr\": {\n+            \"pattern\": r\"(.+?)<loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)>\",\n+            \"area_threshold\": 0.0,\n+        },\n+        \"phrase_grounding\": {\n+            \"banned_grounding_tokens\": [\n+                \"it\",\n+                \"I\",\n+                \"me\",\n+                \"mine\",\n+                \"you\",\n+                \"your\",\n+                \"yours\",\n+                \"he\",\n+                \"him\",\n+                \"his\",\n+                \"she\",\n+                \"her\",\n+                \"hers\",\n+                \"they\",\n+                \"them\",\n+                \"their\",\n+                \"theirs\",\n+                \"one\",\n+                \"oneself\",\n+                \"we\",\n+                \"us\",\n+                \"our\",\n+                \"ours\",\n+                \"you\",\n+                \"your\",\n+                \"yours\",\n+                \"they\",\n+                \"them\",\n+                \"their\",\n+                \"theirs\",\n+                \"mine\",\n+                \"yours\",\n+                \"his\",\n+                \"hers\",\n+                \"its\",\n+                \"ours\",\n+                \"yours\",\n+                \"theirs\",\n+                \"myself\",\n+                \"yourself\",\n+                \"himself\",\n+                \"herself\",\n+                \"itself\",\n+                \"ourselves\",\n+                \"yourselves\",\n+                \"themselves\",\n+                \"this\",\n+                \"that\",\n+                \"these\",\n+                \"those\",\n+                \"who\",\n+                \"whom\",\n+                \"whose\",\n+                \"which\",\n+                \"what\",\n+                \"who\",\n+                \"whom\",\n+                \"whose\",\n+                \"which\",\n+                \"that\",\n+                \"all\",\n+                \"another\",\n+                \"any\",\n+                \"anybody\",\n+                \"anyone\",\n+                \"anything\",\n+                \"each\",\n+                \"everybody\",\n+                \"everyone\",\n+                \"everything\",\n+                \"few\",\n+                \"many\",\n+                \"nobody\",\n+                \"none\",\n+                \"one\",\n+                \"several\",\n+                \"some\",\n+                \"somebody\",\n+                \"someone\",\n+                \"something\",\n+                \"each other\",\n+                \"one another\",\n+                \"myself\",\n+                \"yourself\",\n+                \"himself\",\n+                \"herself\",\n+                \"itself\",\n+                \"ourselves\",\n+                \"yourselves\",\n+                \"themselves\",\n+                \"the image\",\n+                \"image\",\n+                \"images\",\n+                \"the\",\n+                \"a\",\n+                \"an\",\n+                \"a group\",\n+                \"other objects\",\n+                \"lots\",\n+                \"a set\",\n+            ],\n+        },\n+        \"pure_text\": {},\n+        \"description_with_bboxes\": {},\n+        \"description_with_polygons\": {},\n+        \"polygons\": {},\n+        \"bboxes\": {},\n+        \"description_with_bboxes_or_polygons\": {},\n+    }\n+    processor = Florence2Processor(\n+        image_processor=image_processor, tokenizer=tokenizer, post_processor_config=post_processor_config\n+    )\n+\n+    vision_config = convert_config(hf_config.vision_config.__dict__)\n+    text_config = hf_config.text_config.__dict__\n+    config = Florence2Config(\n+        text_config=text_config,\n+        vision_config=vision_config,\n+        image_token_id=tokenizer.image_token_id,\n+        torch_dtype=torch.float16,\n+    )\n+\n+    for stage_idx in range(len(config.vision_config.embed_dim)):\n+        list_of_state_dict = list_of_state_dict + vision_conv_embeddings(stage_idx)\n+        for block_idx in range(config.vision_config.depths[stage_idx]):\n+            list_of_state_dict = list_of_state_dict + vision_spatial_block(stage_idx, block_idx)\n+            list_of_state_dict = list_of_state_dict + vision_channel_block(stage_idx, block_idx)\n+\n+    original_weights = hf_model.state_dict()\n+    list_of_state_dict = list_of_state_dict + multi_modal_projector()\n+    list_of_state_dict = list_of_state_dict + language_model(original_weights)\n+    for i in range(len(list_of_state_dict)):\n+        if list_of_state_dict[i][0] == \"image_projection\":\n+            original_weights[list_of_state_dict[i][0]].transpose_(1, 0)\n+        huggingface_weights[list_of_state_dict[i][1]] = original_weights[list_of_state_dict[i][0]]\n+\n+    model = Florence2ForConditionalGeneration(config)\n+    model.load_state_dict(huggingface_weights, strict=True, assign=True)\n+    model.tie_weights()\n+    # We add an image token so we resize the model and pad to 64 for performance reasons\n+    pad_shape = 64\n+    model.resize_token_embeddings(len(tokenizer), pad_shape)\n+\n+    if pytorch_dump_folder:\n+        model.save_pretrained(pytorch_dump_folder)\n+        processor.save_pretrained(pytorch_dump_folder)\n+\n+    if output_hub_path:\n+        model.push_to_hub(output_hub_path)\n+        processor.push_to_hub(output_hub_path)\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--hf_model_id\",\n+        default=\"microsoft/Florence-2-base\",\n+        type=str,\n+        help=\"Name of the florence2 model you'd like to convert.\",\n+    )\n+    parser.add_argument(\n+        \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the output PyTorch model directory.\"\n+    )\n+    parser.add_argument(\n+        \"--output_hub_path\",\n+        help=\"Location on the hub of the converted model\",\n+    )\n+\n+    args = parser.parse_args()\n+    convert_florence2_checkpoint(args.hf_model_id, args.pytorch_dump_folder_path, args.output_hub_path)"
        },
        {
            "sha": "19379a33046b8b06d33a72c2f65a0de1df367198",
            "filename": "src/transformers/models/florence2/modeling_florence2.py",
            "status": "added",
            "additions": 1028,
            "deletions": 0,
            "changes": 1028,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca543f822f73ebc69b00835c74ae927d9730b6f5/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca543f822f73ebc69b00835c74ae927d9730b6f5/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py?ref=ca543f822f73ebc69b00835c74ae927d9730b6f5",
            "patch": "@@ -0,0 +1,1028 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/florence2/modular_florence2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_florence2.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 Microsoft and the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import math\n+from dataclasses import dataclass\n+from typing import Any, Callable, Optional, Union\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache\n+from ...generation import GenerationMixin\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import Seq2SeqLMOutput, Seq2SeqModelOutput\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TransformersKwargs,\n+    auto_docstring,\n+    can_return_tuple,\n+    is_torch_available,\n+)\n+from ..auto import AutoModel\n+from .configuration_florence2 import Florence2Config, Florence2VisionConfig\n+\n+\n+if is_torch_available():\n+    import torch\n+    import torch.nn as nn\n+    import torch.nn.functional as F\n+\n+\n+def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:\n+    \"\"\"\n+    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n+\n+    Comment by Ross Wightman: This is the same as the DropConnect impl I created for EfficientNet, etc networks,\n+    however, the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n+    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for changing the\n+    layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use 'survival rate' as the\n+    argument.\n+    \"\"\"\n+    if drop_prob == 0.0 or not training:\n+        return input\n+    keep_prob = 1 - drop_prob\n+    shape = (input.shape[0],) + (1,) * (input.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n+    random_tensor = keep_prob + torch.rand(shape, dtype=input.dtype, device=input.device)\n+    random_tensor.floor_()  # binarize\n+    output = input.div(keep_prob) * random_tensor\n+    return output\n+\n+\n+class Florence2VisionDropPath(nn.Module):\n+    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n+\n+    def __init__(self, drop_prob: Optional[float] = None) -> None:\n+        super().__init__()\n+        self.drop_prob = drop_prob\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        return drop_path(hidden_states, self.drop_prob, self.training)\n+\n+    def extra_repr(self) -> str:\n+        return f\"p={self.drop_prob}\"\n+\n+\n+class Florence2VisionLearnedAbsolutePositionEmbedding2D(nn.Module):\n+    \"\"\"\n+    This module learns positional embeddings up to a fixed maximum size.\n+    \"\"\"\n+\n+    def __init__(self, config: Florence2Config):\n+        super().__init__()\n+        num_pos = config.vision_config.max_position_embeddings\n+        embedding_dim = config.vision_config.embed_dim[-1]\n+        self.row_embeddings = nn.Embedding(num_pos, embedding_dim // 2)\n+        self.column_embeddings = nn.Embedding(num_pos, embedding_dim - (embedding_dim // 2))\n+\n+    def forward(self, pixel_values, pixel_mask=None):\n+        height, width = pixel_values.shape[-2:]\n+        width_values = torch.arange(width, device=pixel_values.device)\n+        height_values = torch.arange(height, device=pixel_values.device)\n+        x_emb = self.column_embeddings(width_values)\n+        y_emb = self.row_embeddings(height_values)\n+        pos = torch.cat([x_emb.unsqueeze(0).repeat(height, 1, 1), y_emb.unsqueeze(1).repeat(1, width, 1)], dim=-1)\n+        pos = pos.permute(2, 0, 1)\n+        pos = pos.unsqueeze(0)\n+        pos = pos.repeat(pixel_values.shape[0], 1, 1, 1)\n+        return pos\n+\n+\n+class Florence2VisionPositionalEmbeddingCosine1D(nn.Module):\n+    \"\"\"\n+    This module generates 1D cosine positional embeddings using precomputed sinusoidal functions.\n+    \"\"\"\n+\n+    def __init__(self, config: Florence2Config):\n+        super().__init__()\n+        self.embed_dim = config.vision_config.embed_dim[-1]\n+        self.max_seq_len = config.vision_config.max_temporal_embeddings\n+        pos_idx_to_embed = torch.empty((self.max_seq_len, self.embed_dim))\n+        sine, cosine = self.get_sinusoid_embeddings(\n+            max_positions=self.max_seq_len,\n+            embed_dim=self.embed_dim,\n+        )\n+        pos_idx_to_embed[:, 0::2] = sine\n+        pos_idx_to_embed[:, 1::2] = cosine\n+        # Save the positional embeddings in a constant buffer.\n+        self.register_buffer(\"pos_idx_to_embed\", pos_idx_to_embed)\n+\n+    @staticmethod\n+    def get_sinusoid_embeddings(max_positions: int, embed_dim: int):\n+        half_dim = embed_dim // 2\n+        emb = math.log(10000) / half_dim\n+        emb = torch.exp(torch.arange(half_dim, dtype=torch.int64).float() * -emb)\n+        emb = torch.arange(max_positions, dtype=torch.float).unsqueeze(1) * emb.unsqueeze(0)\n+        return torch.sin(emb), torch.cos(emb)\n+\n+    def forward(self, seq_embeds: torch.Tensor) -> torch.Tensor:\n+        len_seq = seq_embeds.size(1)\n+        if len_seq > self.max_seq_len:\n+            raise ValueError(f\"Maximum sequence length {self.max_seq_len}, got {len_seq}\")\n+        pos_embeds = self.pos_idx_to_embed[0:len_seq, :]\n+        return pos_embeds\n+\n+\n+class Florence2VisionMLP(nn.Module):\n+    def __init__(self, config: Florence2VisionConfig, stage_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.activation_fn = ACT2FN[config.activation_function]\n+        self.fc1 = nn.Linear(config.embed_dim[stage_idx], int(config.embed_dim[stage_idx] * config.mlp_ratio))\n+        self.fc2 = nn.Linear(int(config.embed_dim[stage_idx] * config.mlp_ratio), config.embed_dim[stage_idx])\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states = self.activation_fn(hidden_states)\n+        hidden_states = self.fc2(hidden_states)\n+        return hidden_states\n+\n+\n+class Florence2VisionConvEmbed(nn.Module):\n+    \"\"\"Image to Patch Embedding\"\"\"\n+\n+    def __init__(self, config: Florence2VisionConfig, stage_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.stage_idx = stage_idx\n+        self.patch_size = config.patch_size[stage_idx]\n+        self.in_channels = config.in_channels if stage_idx == 0 else config.embed_dim[stage_idx - 1]\n+        self.embed_dim = config.embed_dim[stage_idx]\n+        self.stride = config.patch_stride[stage_idx]\n+        self.padding = config.patch_padding[stage_idx]\n+        self.pre_norm = config.patch_prenorm[stage_idx]\n+\n+        self.conv = nn.Conv2d(\n+            self.in_channels,\n+            self.embed_dim,\n+            kernel_size=self.patch_size,\n+            stride=self.stride,\n+            padding=self.padding,\n+        )\n+\n+        dim_norm = self.in_channels if self.pre_norm else self.embed_dim\n+        self.norm = nn.LayerNorm(dim_norm)\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        if self.norm and self.pre_norm:\n+            hidden_states = hidden_states.permute(0, 2, 3, 1)\n+            hidden_states = self.norm(hidden_states)\n+            hidden_states = hidden_states.permute(0, 3, 1, 2)\n+\n+        hidden_states = self.conv(hidden_states)\n+\n+        if self.norm and not self.pre_norm:\n+            hidden_states = hidden_states.permute(0, 2, 3, 1)\n+            hidden_states = self.norm(hidden_states)\n+            hidden_states = hidden_states.permute(0, 3, 1, 2)\n+        return hidden_states\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class Florence2VisionChannelAttention(nn.Module):\n+    def __init__(self, config: Florence2VisionConfig, stage_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.dim = config.embed_dim[stage_idx]\n+        self.groups = config.num_groups[stage_idx]\n+        self.qkv = nn.Linear(self.dim, self.dim * 3, bias=config.qkv_bias)\n+        self.proj = nn.Linear(self.dim, self.dim)\n+        self.is_causal = False\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        batch_size, num_tokens, hidden_size = hidden_states.shape\n+\n+        # Reshape for grouped channel attention\n+        qkv = self.qkv(hidden_states).reshape(batch_size, num_tokens, 3, self.groups, hidden_size // self.groups)\n+        qkv = qkv.permute(2, 0, 3, 4, 1)\n+        query, key, value = qkv.unbind(0)\n+\n+        scale = num_tokens**-0.5\n+        # Channel-to-channel attention within groups:\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+        hidden_states, _ = attention_interface(\n+            self,\n+            query,\n+            key,\n+            value,\n+            attention_mask=None,\n+            scaling=scale,\n+        )\n+        hidden_states = hidden_states.permute(0, 3, 2, 1)\n+        hidden_states = hidden_states.reshape(batch_size, num_tokens, hidden_size)\n+\n+        # Final projection\n+        hidden_states = self.proj(hidden_states)\n+        return hidden_states\n+\n+\n+class Florence2VisionChannelBlock(nn.Module):\n+    def __init__(\n+        self,\n+        config: Florence2VisionConfig,\n+        stage_idx: int,\n+        drop_path_rate: float,\n+    ):\n+        super().__init__()\n+\n+        self.config = config\n+        dim_in = config.embed_dim[stage_idx]\n+\n+        self.conv1 = nn.Conv2d(\n+            dim_in,\n+            dim_in,\n+            kernel_size=3,\n+            padding=1,\n+            groups=dim_in,\n+        )\n+        self.norm1 = nn.LayerNorm(config.embed_dim[stage_idx])\n+        self.channel_attn = Florence2VisionChannelAttention(config=config, stage_idx=stage_idx)\n+        self.drop_path1 = Florence2VisionDropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n+\n+        self.conv2 = nn.Conv2d(\n+            dim_in,\n+            dim_in,\n+            kernel_size=3,\n+            padding=1,\n+            groups=dim_in,\n+        )\n+        self.norm2 = nn.LayerNorm(config.embed_dim[stage_idx])\n+        self.ffn = Florence2VisionMLP(config=config, stage_idx=stage_idx)\n+        self.drop_path2 = Florence2VisionDropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        batch_size, embed_dim, height, width = hidden_states.shape\n+\n+        # First channel block: Depthwise Conv + Channel Attention\n+        hidden_states = self.conv1(hidden_states) + hidden_states\n+        hidden_states = hidden_states.flatten(2).transpose(1, 2)\n+        residual = hidden_states\n+\n+        # Channel group attention self-attention mechanism\n+        hidden_states = self.norm1(hidden_states)\n+        hidden_states = self.channel_attn(hidden_states)\n+        hidden_states = residual + self.drop_path1(hidden_states)\n+        hidden_states = hidden_states.transpose(1, 2).view(batch_size, embed_dim, height, width)\n+\n+        # Second channel block: Depthwise Conv + FFN\n+        hidden_states = self.conv2(hidden_states) + hidden_states\n+        hidden_states = hidden_states.flatten(2).transpose(1, 2)\n+        residual = hidden_states\n+\n+        # FFN\n+        hidden_states = self.norm2(hidden_states)\n+        hidden_states = self.ffn(hidden_states)\n+        hidden_states = residual + self.drop_path2(hidden_states)\n+        hidden_states = hidden_states.transpose(1, 2).view(batch_size, embed_dim, height, width)\n+\n+        return hidden_states\n+\n+\n+class Florence2VisionWindowAttention(nn.Module):\n+    def __init__(self, config: Florence2VisionConfig, stage_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.dim = config.embed_dim[stage_idx]\n+        self.window_size = config.window_size\n+        self.num_heads = config.num_heads[stage_idx]\n+        head_dim = self.dim // self.num_heads\n+        self.scale = head_dim**-0.5\n+\n+        self.qkv = nn.Linear(self.dim, self.dim * 3, bias=config.qkv_bias)\n+        self.proj = nn.Linear(self.dim, self.dim)\n+        self.is_causal = False\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        batch_size, height, width, embed_dim = hidden_states.shape\n+\n+        # Pad the input if necessary\n+        pad_left = pad_top = 0\n+        pad_right = (self.window_size - width % self.window_size) % self.window_size\n+        pad_bottom = (self.window_size - height % self.window_size) % self.window_size\n+        hidden_states = F.pad(hidden_states, (0, 0, pad_left, pad_right, pad_top, pad_bottom))\n+        _, padded_height, padded_width, _ = hidden_states.shape\n+\n+        # Partition input into non-overlapping windows (for local spatial attention in DaViT)\n+        hidden_states = hidden_states.view(\n+            batch_size,\n+            padded_height // self.window_size,\n+            self.window_size,\n+            padded_width // self.window_size,\n+            self.window_size,\n+            embed_dim,\n+        )\n+        windowed_hidden_states = hidden_states.permute(0, 1, 3, 2, 4, 5).contiguous()\n+        windowed_hidden_states = windowed_hidden_states.view(-1, self.window_size * self.window_size, embed_dim)\n+\n+        # Generate Q, K, V for each window\n+        num_windows_per_batch, num_tokens_per_window, embed_dim = windowed_hidden_states.shape\n+        qkv = self.qkv(windowed_hidden_states).reshape(\n+            num_windows_per_batch, num_tokens_per_window, 3, self.num_heads, embed_dim // self.num_heads\n+        )\n+        qkv = qkv.permute(2, 0, 3, 1, 4)\n+        query, key, value = qkv.unbind(0)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        windowed_hidden_states, _ = attention_interface(\n+            self,\n+            query,\n+            key,\n+            value,\n+            attention_mask=None,\n+            scaling=self.scale,\n+        )\n+        windowed_hidden_states = windowed_hidden_states.view(num_windows_per_batch, num_tokens_per_window, embed_dim)\n+        windowed_hidden_states = self.proj(windowed_hidden_states)\n+\n+        # Merge windows back to original spatial layout\n+        windowed_hidden_states = windowed_hidden_states.view(-1, self.window_size, self.window_size, embed_dim)\n+        hidden_states = windowed_hidden_states.view(\n+            -1,\n+            padded_height // self.window_size,\n+            padded_width // self.window_size,\n+            self.window_size,\n+            self.window_size,\n+            embed_dim,\n+        )\n+        hidden_states = hidden_states.permute(0, 1, 3, 2, 4, 5).contiguous()\n+        hidden_states = hidden_states.view(-1, padded_height, padded_width, embed_dim)\n+        hidden_states = hidden_states[:, :height, :width, :].contiguous()\n+        hidden_states = hidden_states.view(batch_size, height * width, embed_dim)\n+\n+        return hidden_states\n+\n+\n+class Florence2VisionSpatialBlock(nn.Module):\n+    def __init__(\n+        self,\n+        config: Florence2VisionConfig,\n+        stage_idx: int,\n+        drop_path_rate: float,\n+    ):\n+        super().__init__()\n+\n+        self.conv1 = nn.Conv2d(\n+            config.embed_dim[stage_idx],\n+            config.embed_dim[stage_idx],\n+            kernel_size=3,\n+            padding=1,\n+            groups=config.embed_dim[stage_idx],\n+        )\n+        self.norm1 = nn.LayerNorm(config.embed_dim[stage_idx])\n+        self.window_attn = Florence2VisionWindowAttention(config=config, stage_idx=stage_idx)\n+        self.drop_path1 = Florence2VisionDropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n+\n+        self.conv2 = nn.Conv2d(\n+            config.embed_dim[stage_idx],\n+            config.embed_dim[stage_idx],\n+            kernel_size=3,\n+            padding=1,\n+            groups=config.embed_dim[stage_idx],\n+        )\n+        self.norm2 = nn.LayerNorm(config.embed_dim[stage_idx])\n+        self.ffn = Florence2VisionMLP(config=config, stage_idx=stage_idx)\n+        self.drop_path2 = Florence2VisionDropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        batch_size, embed_dim, height, width = hidden_states.shape\n+\n+        # First spatial mixing block: Conv + Window Attention\n+        hidden_states = self.conv1(hidden_states) + hidden_states\n+        hidden_states = hidden_states.flatten(2).transpose(1, 2)\n+        residual = hidden_states\n+\n+        # Spatial Window-based self-attention mechanism\n+        hidden_states = self.norm1(hidden_states)\n+        hidden_states = hidden_states.view(batch_size, height, width, embed_dim)\n+        hidden_states = self.window_attn(hidden_states)\n+        hidden_states = residual + self.drop_path1(hidden_states)\n+        hidden_states = hidden_states.transpose(1, 2).view(batch_size, embed_dim, height, width)\n+\n+        # Second spatial mixing block: Conv + FFN\n+        hidden_states = self.conv2(hidden_states) + hidden_states\n+        hidden_states = hidden_states.flatten(2).transpose(1, 2)\n+        residual = hidden_states\n+\n+        # FFN\n+        hidden_states = self.norm2(hidden_states)\n+        hidden_states = self.ffn(hidden_states)\n+        hidden_states = residual + self.drop_path2(hidden_states)\n+        hidden_states = hidden_states.transpose(1, 2).view(batch_size, embed_dim, height, width)\n+\n+        return hidden_states\n+\n+\n+class Florence2VisionBlock(nn.Module):\n+    def __init__(\n+        self,\n+        config: Florence2VisionConfig,\n+        stage_idx: int,\n+        spatial_drop_path_rate: float,\n+        channel_drop_path_rate: float,\n+    ):\n+        super().__init__()\n+        self.spatial_block = Florence2VisionSpatialBlock(\n+            config=config,\n+            stage_idx=stage_idx,\n+            drop_path_rate=spatial_drop_path_rate,\n+        )\n+        self.channel_block = Florence2VisionChannelBlock(\n+            config=config,\n+            stage_idx=stage_idx,\n+            drop_path_rate=channel_drop_path_rate,\n+        )\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        hidden_states = self.spatial_block(hidden_states)\n+        hidden_states = self.channel_block(hidden_states)\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class Florence2VisionPreTrainedModel(PreTrainedModel):\n+    config_class = Florence2VisionConfig\n+    main_input_name = \"pixel_values\"\n+    _supports_sdpa = True\n+    _supports_flash_attn = True\n+    _supports_flex_attn = True\n+\n+    _can_compile_fullgraph = True\n+\n+\n+@auto_docstring\n+class Florence2VisionBackbone(Florence2VisionPreTrainedModel):\n+    def __init__(self, config: Florence2VisionConfig):\n+        super().__init__(config)\n+        self.config = config\n+\n+        self.embed_dim = config.embed_dim\n+        self.num_heads = config.num_heads\n+        self.num_groups = config.num_groups\n+        self.num_stages = len(self.embed_dim)\n+\n+        if not (self.num_stages == len(self.num_heads) == len(self.num_groups)):\n+            raise ValueError(\n+                f\"Expected self.num_stages ({self.num_stages}) == \"\n+                f\"len(self.num_heads) ({len(self.num_heads)}) == \"\n+                f\"len(self.num_groups) ({len(self.num_groups)})\"\n+            )\n+\n+        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths) * 2, device=\"cpu\")]\n+        depth_offset = 0\n+\n+        convs = []\n+        blocks = []\n+        for stage_idx in range(self.num_stages):\n+            conv_embed = Florence2VisionConvEmbed(\n+                config=config,\n+                stage_idx=stage_idx,\n+            )\n+            convs.append(conv_embed)\n+\n+            block = nn.ModuleList(\n+                Florence2VisionBlock(\n+                    config=config,\n+                    stage_idx=stage_idx,\n+                    spatial_drop_path_rate=dpr[depth_offset + block_idx * 2],\n+                    channel_drop_path_rate=dpr[depth_offset + block_idx * 2 + 1],\n+                )\n+                for block_idx in range(config.depths[stage_idx])\n+            )\n+            blocks.append(block)\n+            depth_offset += config.depths[stage_idx] * 2\n+\n+        self.convs = nn.ModuleList(convs)\n+        self.blocks = nn.ModuleList(blocks)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        for conv, block in zip(self.convs, self.blocks):\n+            hidden_states = conv(hidden_states)\n+            for layer in block:\n+                hidden_states = layer(hidden_states)\n+        return hidden_states\n+\n+\n+class Florence2MultiModalProjector(nn.Module):\n+    def __init__(self, config: Florence2Config):\n+        super().__init__()\n+        self.vision_embedding_dim = config.vision_config.embed_dim[-1]\n+        self.vision_projection_dim = config.vision_config.projection_dim\n+        self.image_projection = nn.Linear(self.vision_embedding_dim, self.vision_projection_dim, bias=False)\n+        self.image_proj_norm = nn.LayerNorm(self.vision_projection_dim)\n+        self.image_position_embed = Florence2VisionLearnedAbsolutePositionEmbedding2D(config=config)\n+        self.visual_temporal_embed = Florence2VisionPositionalEmbeddingCosine1D(config=config)\n+\n+    def forward(self, image_features):\n+        position_features = image_features + self.image_position_embed(image_features)\n+        position_features = position_features.flatten(2).transpose(1, 2)\n+        temporal_features = self.visual_temporal_embed(position_features[:, :1, :])\n+        temporal_features = temporal_features.unsqueeze(1)\n+        visual_token_features = position_features + temporal_features\n+        visual_token_features = visual_token_features.unsqueeze(1)\n+        spatial_image_features = visual_token_features.mean(dim=2)\n+        temporal_image_features = visual_token_features.mean(dim=1)\n+        image_features = torch.cat([spatial_image_features, temporal_image_features], dim=1)\n+        image_features = self.image_projection(image_features)\n+        image_features = self.image_proj_norm(image_features)\n+        return image_features\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for Florence-2 base model's outputs that also contains : pre-computed hidden states that can speed up sequential\n+    decoding.\n+    \"\"\"\n+)\n+class Florence2Seq2SeqModelOutput(Seq2SeqModelOutput):\n+    r\"\"\"\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_image_tokens, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for Florence-2 model's outputs that also contains : pre-computed hidden states that can speed up sequential\n+    decoding.\n+    \"\"\"\n+)\n+class Florence2Seq2SeqLMOutput(Seq2SeqLMOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_image_tokens, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    image_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+\n+\n+@auto_docstring\n+class Florence2PreTrainedModel(PreTrainedModel):\n+    config: Florence2Config\n+    base_model_prefix = \"\"\n+    supports_gradient_checkpointing = True\n+    _skip_keys_device_placement = \"past_key_values\"\n+\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+\n+    _can_compile_fullgraph = True\n+    _supports_flex_attn = True\n+\n+    _supports_attention_backend = False\n+    config_class = Florence2Config\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Florence-2 is a vision model for captioning, detection, and segmentation.\n+    \"\"\"\n+)\n+class Florence2Model(Florence2PreTrainedModel):\n+    _checkpoint_conversion_mapping = {}\n+    _tied_weights_keys = [\n+        \"language_model.encoder.embed_tokens.weight\",\n+        \"language_model.decoder.embed_tokens.weight\",\n+    ]\n+\n+    def __init__(self, config: Florence2Config):\n+        super().__init__(config)\n+        self.vision_tower = Florence2VisionBackbone(config=config.vision_config)\n+\n+        self.multi_modal_projector = Florence2MultiModalProjector(config)\n+        self.language_model = AutoModel.from_config(config.text_config)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model.get_decoder()\n+\n+    def get_image_features(self, pixel_values: torch.Tensor, **kwargs):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`):\n+               The tensors corresponding to the input images.\n+        Returns:\n+            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n+        \"\"\"\n+        image_features = self.vision_tower(pixel_values, **kwargs)\n+        image_embeds = self.multi_modal_projector(image_features)\n+        return image_embeds\n+\n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        n_image_features = image_features.shape[0] * image_features.shape[1]\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.LongTensor] = None,\n+        decoder_head_mask: Optional[torch.Tensor] = None,\n+        cross_attn_head_mask: Optional[torch.Tensor] = None,\n+        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n+        encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[tuple, Florence2Seq2SeqModelOutput]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if encoder_outputs is None:\n+            if (input_ids is None) ^ (inputs_embeds is not None):\n+                raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+            if inputs_embeds is None:\n+                inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+            if pixel_values is not None:\n+                image_features = self.get_image_features(pixel_values)\n+                image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+                special_image_mask = self.get_placeholder_mask(\n+                    input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+                )\n+                inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+            encoder_outputs = self.language_model.encoder(\n+                attention_mask=attention_mask,\n+                head_mask=head_mask,\n+                inputs_embeds=inputs_embeds,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+                return_dict=True,\n+            )\n+\n+        if decoder_input_ids is None:\n+            decoder_start_token_id = self.config.text_config.decoder_start_token_id\n+            decoder_input_ids = torch.ones((inputs_embeds.size()[0], 1), dtype=torch.long, device=inputs_embeds.device)\n+            decoder_input_ids *= decoder_start_token_id\n+\n+        decoder_outputs = self.language_model.decoder(\n+            input_ids=decoder_input_ids,\n+            attention_mask=decoder_attention_mask,\n+            encoder_hidden_states=encoder_outputs[0],\n+            encoder_attention_mask=attention_mask,\n+            head_mask=decoder_head_mask,\n+            cross_attn_head_mask=cross_attn_head_mask,\n+            past_key_values=past_key_values,\n+            inputs_embeds=decoder_inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+\n+        return Florence2Seq2SeqModelOutput(\n+            last_hidden_state=decoder_outputs.last_hidden_state,\n+            past_key_values=decoder_outputs.past_key_values,\n+            decoder_hidden_states=decoder_outputs.hidden_states,\n+            decoder_attentions=decoder_outputs.attentions,\n+            cross_attentions=decoder_outputs.cross_attentions,\n+            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n+            encoder_hidden_states=encoder_outputs.hidden_states,\n+            encoder_attentions=encoder_outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+\n+    def get_encoder(self):\n+        return self.language_model.get_encoder()\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Florence-2 is a vision model for captioning, detection, and segmentation.\n+    \"\"\"\n+)\n+class Florence2ForConditionalGeneration(Florence2PreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {}\n+    _tied_weights_keys = [\n+        \"model.language_model.encoder.embed_tokens.weight\",\n+        \"model.language_model.decoder.embed_tokens.weight\",\n+        \"lm_head.weight\",\n+    ]\n+\n+    def __init__(self, config: Florence2Config):\n+        super().__init__(config)\n+        self.model = Florence2Model(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self) -> nn.Module:\n+        return self.lm_head\n+\n+    def set_decoder(self, decoder):\n+        self.model.set_decoder(decoder)\n+\n+    def get_decoder(self):\n+        return self.model.get_decoder()\n+\n+    def get_image_features(self, pixel_values: torch.Tensor, **kwargs):\n+        return self.model.get_image_features(pixel_values=pixel_values, **kwargs)\n+\n+    # Make modules available through conditional class for BC\n+    @property\n+    def language_model(self):\n+        return self.model.language_model\n+\n+    @property\n+    def vision_tower(self):\n+        return self.model.vision_tower\n+\n+    @property\n+    def multi_modal_projector(self):\n+        return self.model.multi_modal_projector\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.LongTensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        decoder_head_mask: Optional[torch.Tensor] = None,\n+        cross_attn_head_mask: Optional[torch.Tensor] = None,\n+        encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, Florence2Seq2SeqLMOutput]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, Florence2ForConditionalGeneration\n+\n+        >>> model = Florence2ForConditionalGeneration.from_pretrained(\"microsoft/Florence-2-large\")\n+        >>> processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-large\")\n+\n+        >>> prompt = \"<CAPTION>\"\n+        >>> url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(**inputs, max_length=100)\n+        >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"A green car parked in front of a yellow building.\"\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            attention_mask=attention_mask,\n+            decoder_input_ids=decoder_input_ids,\n+            encoder_outputs=encoder_outputs,\n+            decoder_attention_mask=decoder_attention_mask,\n+            head_mask=head_mask,\n+            decoder_head_mask=decoder_head_mask,\n+            cross_attn_head_mask=cross_attn_head_mask,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            decoder_inputs_embeds=decoder_inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n+\n+        return Florence2Seq2SeqLMOutput(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            decoder_hidden_states=outputs.decoder_hidden_states,\n+            decoder_attentions=outputs.decoder_attentions,\n+            cross_attentions=outputs.cross_attentions,\n+            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n+            encoder_hidden_states=outputs.encoder_hidden_states,\n+            encoder_attentions=outputs.encoder_attentions,\n+            image_hidden_states=outputs.image_hidden_states,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        inputs_embeds=None,\n+        pixel_values=None,\n+        attention_mask=None,\n+        cache_position=None,\n+        logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        if cache_position[0] == 0:\n+            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+            # Otherwise we need pixel values to be passed to model\n+            model_inputs[\"pixel_values\"] = pixel_values\n+\n+        return model_inputs\n+\n+    def get_encoder(self):\n+        return self.model.get_encoder()\n+\n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        return self.model.get_placeholder_mask(\n+            input_ids=input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+        )\n+\n+    def _prepare_encoder_decoder_kwargs_for_generation(\n+        self,\n+        inputs_tensor: torch.Tensor,\n+        model_kwargs,\n+        model_input_name: Optional[str],\n+        generation_config,\n+    ) -> dict[str, Any]:\n+        # override to handle merging image and text embeddings before passing to language encoder\n+        inputs_embeds = model_kwargs.pop(\"inputs_embeds\", None)\n+        pixel_values = model_kwargs.pop(\"pixel_values\", None)\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(inputs_tensor)\n+\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(pixel_values)\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask = self.get_placeholder_mask(\n+                inputs_tensor, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        model_kwargs[\"inputs_embeds\"] = inputs_embeds\n+        model_kwargs = super()._prepare_encoder_decoder_kwargs_for_generation(\n+            None, model_kwargs, model_input_name, generation_config\n+        )\n+        model_kwargs.pop(\"inputs_embeds\", None)\n+        return model_kwargs\n+\n+\n+__all__ = [\n+    \"Florence2Model\",\n+    \"Florence2ForConditionalGeneration\",\n+    \"Florence2PreTrainedModel\",\n+    \"Florence2VisionBackbone\",\n+    \"Florence2VisionPreTrainedModel\",\n+]"
        },
        {
            "sha": "73ba9882eae00c2440c655d8821d19b5545638e1",
            "filename": "src/transformers/models/florence2/modular_florence2.py",
            "status": "added",
            "additions": 1807,
            "deletions": 0,
            "changes": 1807,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca543f822f73ebc69b00835c74ae927d9730b6f5/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca543f822f73ebc69b00835c74ae927d9730b6f5/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py?ref=ca543f822f73ebc69b00835c74ae927d9730b6f5",
            "patch": "@@ -0,0 +1,1807 @@\n+# coding=utf-8\n+# Copyright 2025 Microsoft and the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import math\n+import re\n+from dataclasses import dataclass\n+from typing import Any, Callable, Optional, Union\n+\n+import numpy as np\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache\n+from ...configuration_utils import PretrainedConfig\n+from ...feature_extraction_utils import BatchFeature\n+from ...image_utils import ImageInput\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import Seq2SeqLMOutput, Seq2SeqModelOutput\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import MultiModalData, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import (\n+    TransformersKwargs,\n+    auto_docstring,\n+    can_return_tuple,\n+    is_torch_available,\n+    logging,\n+)\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+from ..bart.modeling_bart import eager_attention_forward\n+from ..beit.modeling_beit import BeitDropPath\n+from ..llama4.modeling_llama4 import Llama4VisionMLP\n+from ..llava.modeling_llava import LlavaForConditionalGeneration, LlavaModel, LlavaPreTrainedModel\n+from ..llava.processing_llava import LlavaProcessorKwargs\n+\n+\n+if is_torch_available():\n+    import torch\n+    import torch.nn as nn\n+    import torch.nn.functional as F\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Florence2VisionConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Florence2VisionModel`]. It is used to instantiate a Florence2VisionModel\n+    according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the Florence2VisionModel architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        in_channels (`int`, *optional*, defaults to 3):\n+            Number of input image channels.\n+        depths (`Tuple[int]`, *optional*, defaults to `(1, 1, 9, 1)`):\n+            The depth of the model.\n+        patch_size (`Tuple[int]`, *optional*, defaults to `(7, 3, 3, 3)`):\n+            The patch size of the image.\n+        patch_stride (`Tuple[int]`, *optional*, defaults to `(4, 2, 2, 2)`):\n+            The patch stride of the image.\n+        patch_padding (`Tuple[int]`, *optional*, defaults to `(3, 1, 1, 1)`):\n+            The patch padding of the image.\n+        patch_prenorm (`Tuple[bool]`, *optional*, defaults to `(False, True, True, True)`):\n+            Whether to apply layer normalization before the patch embedding layer.\n+        embed_dim (`Tuple[int]`, *optional*, defaults to `(128, 256, 512, 1024)`):\n+            The dimension of the embedding layer.\n+        num_heads (`Tuple[int]`, *optional*, defaults to `(4, 8, 16, 32)`):\n+            The number of attention heads.\n+        num_groups (`Tuple[int]`, *optional*, defaults to `(4, 8, 16, 32)`):\n+            The number of groups.\n+        window_size (`int`, *optional*, defaults to 12):\n+            The window size of the model.\n+        drop_path_rate (`float`, *optional*, defaults to 0.1):\n+            The dropout rate of the drop path layer.\n+        mlp_ratio (`int`, *optional*, defaults to 4.0):\n+            Ratio of mlp hidden dim to embedding dim.\n+        qkv_bias (`bool`, *optional*, defaults to `True`):\n+            If True, add a learnable bias to query, key, value.\n+        activation_function (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n+        projection_dim (`int`, *optional*, defaults to 1024):\n+            The dimension of the projection layer.\n+        max_temporal_embeddings (`int`, *optional*, defaults to 100):\n+            The configuration of the visual temporal embedding.\n+        max_position_embeddings (`int`, *optional*, defaults to 50):\n+            The configuration of the image position embedding.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+    Example:\n+\n+    ```python\n+    >>> from transformers import Florence2VisionConfig, Florence2VisionModel\n+\n+    >>> # Initializing a Florence2 Vision style configuration\n+    >>> configuration = Florence2VisionConfig()\n+\n+    >>> # Initializing a model (with random weights)\n+    >>> model = Florence2VisionModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"florence_vision\"\n+\n+    def __init__(\n+        self,\n+        in_channels=3,\n+        depths=(1, 1, 9, 1),\n+        patch_size=(7, 3, 3, 3),\n+        patch_stride=(4, 2, 2, 2),\n+        patch_padding=(3, 1, 1, 1),\n+        patch_prenorm=(False, True, True, True),\n+        embed_dim=(128, 256, 512, 1024),\n+        num_heads=(4, 8, 16, 32),\n+        num_groups=(4, 8, 16, 32),\n+        window_size=12,\n+        drop_path_rate=0.1,\n+        mlp_ratio=4.0,\n+        qkv_bias=True,\n+        activation_function=\"gelu\",\n+        projection_dim=1024,\n+        max_temporal_embeddings=100,\n+        max_position_embeddings=50,\n+        initializer_range=0.02,\n+        **kwargs,\n+    ):\n+        self.in_channels = in_channels\n+        self.depths = list(depths)\n+        self.patch_size = list(patch_size)\n+        self.patch_stride = list(patch_stride)\n+        self.patch_padding = list(patch_padding)\n+        self.patch_prenorm = list(patch_prenorm)\n+        self.embed_dim = list(embed_dim)\n+        self.num_heads = list(num_heads)\n+        self.num_groups = list(num_groups)\n+        self.window_size = window_size\n+        self.drop_path_rate = drop_path_rate\n+        self.mlp_ratio = mlp_ratio\n+        self.qkv_bias = qkv_bias\n+        self.projection_dim = projection_dim\n+        self.max_temporal_embeddings = max_temporal_embeddings\n+        self.max_position_embeddings = max_position_embeddings\n+        self.initializer_range = initializer_range\n+        self.activation_function = activation_function\n+\n+        super().__init__(**kwargs)\n+\n+\n+class Florence2Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Florence2ForConditionalGeneration`]. It is used to instantiate an\n+    Florence-2 model according to the specified arguments, defining the model architecture.\n+\n+    Instantiating a configuration with the defaults will yield a similar configuration to that of the Florence-2\n+    [microsoft/Florence-2-base](https://huggingface.co/microsoft/Florence-2-base) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        text_config (`dict`, *optional*):\n+            Dictionary of configuration options used to initialize [`AutoConfig`].\n+        vision_config (`dict`, *optional*):\n+            Dictionary of configuration options used to initialize [`Florence2VisionConfig`].\n+        image_token_id (`int`, *optional*, defaults to 51289):\n+            The image token index to encode the image prompt.\n+        is_encoder_decoder (bool, optional, *optional*, defaults to `True`):\n+            Whether the model is used as an encoder/decoder or not.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import Florence2ForConditionalGeneration, Florence2Config, CLIPVisionConfig, BartConfig\n+\n+    >>> # Initializing a clip-like vision config\n+    >>> vision_config = CLIPVisionConfig()\n+\n+    >>> # Initializing a Bart config\n+    >>> text_config = BartConfig()\n+\n+    >>> # Initializing a Florence-2 configuration\n+    >>> configuration = Florence2Config(vision_config, text_config)\n+\n+    >>> # Initializing a model from the florence-2 configuration\n+    >>> model = Florence2ForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"florence2\"\n+    sub_configs = {\n+        \"text_config\": AutoConfig,\n+        \"vision_config\": Florence2VisionConfig,\n+    }\n+\n+    def __init__(\n+        self,\n+        text_config=None,\n+        vision_config=None,\n+        image_token_id=51289,\n+        is_encoder_decoder=True,\n+        **kwargs,\n+    ):\n+        if isinstance(text_config, dict):\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"bart\")\n+            text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n+        elif text_config is None:\n+            text_config = CONFIG_MAPPING[\"bart\"]()\n+\n+        if isinstance(vision_config, dict):\n+            vision_config = Florence2VisionConfig(**vision_config)\n+        elif vision_config is None:\n+            logger.info(\"vision_config is None. Initializing the Florence2VisionConfig with default values.\")\n+            vision_config = Florence2VisionConfig()\n+\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.image_token_id = image_token_id\n+\n+        super().__init__(\n+            is_encoder_decoder=is_encoder_decoder,\n+            **kwargs,\n+        )\n+\n+\n+class Florence2ProcessorKwargs(LlavaProcessorKwargs):\n+    pass\n+\n+\n+class Florence2Processor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a Florence2 processor which wraps a Florence2 image processor and a Florence2 tokenizer into a single processor.\n+\n+    [`Florence2Processor`] offers all the functionalities of [`AutoImageProcessor`] and [`BartTokenizerFast`]. See the\n+    [`~Florence2Processor.__call__`] and [`~Florence2Processor.decode`] for more information.\n+\n+    Args:\n+        image_processor (`AutoImageProcessor`, *optional*):\n+            The image processor is a required input.\n+        tokenizer (`Union[BartTokenizer, BartTokenizerFast]`, *optional*):\n+            The tokenizer is a required input.\n+        num_additional_image_tokens (`int`, *optional*, defaults to 0):\n+            Number of additional tokens added to the image embeddings, such as CLS (+1). If the backbone has no CLS or other\n+            extra tokens appended, no need to set this arg.\n+        post_processor_config (`dict`,  *optional*, defaults to 0):\n+            Task-specific parsing rules for [`Florence2PostProcessor`], e.g. regex patterns,\n+            thresholds, or banned tokens.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"tokenizer\"]\n+    image_processor_class = \"AutoImageProcessor\"\n+    tokenizer_class = (\"BartTokenizer\", \"BartTokenizerFast\")\n+\n+    def __init__(\n+        self,\n+        image_processor=None,\n+        tokenizer=None,\n+        num_additional_image_tokens: int = 0,\n+        post_processor_config: Optional[dict] = None,\n+        **kwargs,\n+    ):\n+        self.tasks_answer_post_processing_type = {\n+            \"<OCR>\": \"pure_text\",\n+            \"<OCR_WITH_REGION>\": \"ocr\",\n+            \"<CAPTION>\": \"pure_text\",\n+            \"<DETAILED_CAPTION>\": \"pure_text\",\n+            \"<MORE_DETAILED_CAPTION>\": \"pure_text\",\n+            \"<OD>\": \"description_with_bboxes\",\n+            \"<DENSE_REGION_CAPTION>\": \"description_with_bboxes\",\n+            \"<CAPTION_TO_PHRASE_GROUNDING>\": \"phrase_grounding\",\n+            \"<REFERRING_EXPRESSION_SEGMENTATION>\": \"polygons\",\n+            \"<REGION_TO_SEGMENTATION>\": \"polygons\",\n+            \"<OPEN_VOCABULARY_DETECTION>\": \"description_with_bboxes_or_polygons\",\n+            \"<REGION_TO_CATEGORY>\": \"pure_text\",\n+            \"<REGION_TO_DESCRIPTION>\": \"pure_text\",\n+            \"<REGION_TO_OCR>\": \"pure_text\",\n+            \"<REGION_PROPOSAL>\": \"bboxes\",\n+        }\n+\n+        self.task_prompts_without_inputs = {\n+            \"<OCR>\": \"What is the text in the image?\",\n+            \"<OCR_WITH_REGION>\": \"What is the text in the image, with regions?\",\n+            \"<CAPTION>\": \"What does the image describe?\",\n+            \"<DETAILED_CAPTION>\": \"Describe in detail what is shown in the image.\",\n+            \"<MORE_DETAILED_CAPTION>\": \"Describe with a paragraph what is shown in the image.\",\n+            \"<OD>\": \"Locate the objects with category name in the image.\",\n+            \"<DENSE_REGION_CAPTION>\": \"Locate the objects in the image, with their descriptions.\",\n+            \"<REGION_PROPOSAL>\": \"Locate the region proposals in the image.\",\n+        }\n+\n+        self.task_prompts_with_input = {\n+            \"<CAPTION_TO_PHRASE_GROUNDING>\": \"Locate the phrases in the caption: {input}\",\n+            \"<REFERRING_EXPRESSION_SEGMENTATION>\": \"Locate {input} in the image with mask\",\n+            \"<REGION_TO_SEGMENTATION>\": \"What is the polygon mask of region {input}\",\n+            \"<OPEN_VOCABULARY_DETECTION>\": \"Locate {input} in the image.\",\n+            \"<REGION_TO_CATEGORY>\": \"What is the region {input}?\",\n+            \"<REGION_TO_DESCRIPTION>\": \"What does the region {input} describe?\",\n+            \"<REGION_TO_OCR>\": \"What text is in the region {input}?\",\n+        }\n+\n+        self.num_image_tokens = image_processor.image_seq_length\n+        self.num_additional_image_tokens = num_additional_image_tokens\n+        self.post_processor_config = post_processor_config\n+        self.post_processor = Florence2PostProcessor(config=post_processor_config, tokenizer=tokenizer)\n+        self.image_token = tokenizer.image_token\n+        self.image_token_id = tokenizer.image_token_id\n+\n+        super().__init__(image_processor, tokenizer, **kwargs)\n+\n+    def _construct_prompts(self, text: Union[str, list[str]]) -> list[str]:\n+        \"\"\"\n+        Construct prompts by replacing task tokens with corresponding prompt strings.\n+        \"\"\"\n+        if isinstance(text, str):\n+            text = [text]\n+\n+        prompts = []\n+        for prompt in text:\n+            # Check for tasks without inputs\n+            for task_token, task_prompt in self.task_prompts_without_inputs.items():\n+                if task_token in prompt:\n+                    if prompt != task_token:\n+                        raise ValueError(f\"Task token {task_token} should be the only content in the prompt.\")\n+                    prompt = task_prompt\n+                    break\n+            # Check for tasks with inputs\n+            for task_token, task_prompt in self.task_prompts_with_input.items():\n+                if task_token in prompt:\n+                    input_text = prompt.replace(task_token, \"\").strip()\n+                    prompt = task_prompt.format(input=input_text)\n+                    break\n+            prompts.append(prompt)\n+        return prompts\n+\n+    def __call__(\n+        self,\n+        images: Optional[ImageInput] = None,\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        **kwargs: Unpack[Florence2ProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n+        and `kwargs` arguments to BartTokenizerFast's [`~BartTokenizerFast.__call__`] if `text` is not `None` to encode\n+        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n+        of the above two methods for more information.\n+\n+        Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            text (`str`, `list[str]`, `list[list[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+                - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+              `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+        \"\"\"\n+        if images is None and text is None:\n+            raise ValueError(\"You have to specify at least one of `images` or `text`.\")\n+\n+        output_kwargs = self._merge_kwargs(\n+            Florence2ProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n+        image_inputs = {}\n+        if images is not None:\n+            image_inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n+\n+        if text is None:\n+            logger.warning_once(\"You are using Florence-2 without a text prefix.\")\n+            text = [\"\"] * (1 if not isinstance(images, list) else len(images))\n+        elif isinstance(text, str):\n+            text = [text]\n+\n+        if not isinstance(text, list) or not all(isinstance(token, str) for token in text):\n+            raise ValueError(\"`text` must be a string or list of strings.\")\n+\n+        if isinstance(images, list) and len(images) != len(text):\n+            raise ValueError(f\"Number of images ({len(images)}) must match number of texts ({len(text)}).\")\n+\n+        prompt_strings = self._construct_prompts(text)\n+\n+        # Add image tokens and special tokens if images are provided\n+        if image_inputs.get(\"pixel_values\") is not None:\n+            # Replace the image token with the expanded image token sequence\n+            expanded_image_prompts = []\n+            for sample in prompt_strings:\n+                sample = (\n+                    self.image_token * self.num_image_tokens\n+                    + self.tokenizer.bos_token\n+                    + sample\n+                    + self.tokenizer.eos_token\n+                )\n+                expanded_image_prompts.append(sample)\n+            prompt_strings = expanded_image_prompts\n+\n+        # Construct and tokenize prompts\n+        output_kwargs[\"text_kwargs\"].pop(\"add_special_tokens\", None)\n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", False)\n+        text_inputs = self.tokenizer(\n+            prompt_strings, **output_kwargs[\"text_kwargs\"], add_special_tokens=False, return_tensors=None\n+        )\n+        self._check_special_mm_tokens(prompt_strings, text_inputs, modalities=[\"image\"])\n+\n+        if return_mm_token_type_ids:\n+            array_ids = np.array(text_inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n+            mm_token_type_ids[array_ids == self.image_token_id] = 1\n+            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n+\n+        return BatchFeature(data={**image_inputs, **text_inputs}, tensor_type=return_tensors)\n+\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to BartTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to BartTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+\n+    def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+\n+        Args:\n+            image_sizes (`list[list[int]]`, *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+\n+        Returns:\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n+        \"\"\"\n+\n+        vision_data = {}\n+        if image_sizes is not None:\n+            num_image_tokens = [self.image_seq_length] * len(image_sizes)\n+            num_image_patches = [1] * len(image_sizes)\n+\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+\n+        return MultiModalData(**vision_data)\n+\n+    def post_process_image_text_to_text(self, generated_outputs, skip_special_tokens=False, **kwargs):\n+        \"\"\"\n+        Post-processes the output of `FuyuForConditionalGeneration` to only return the text output.\n+\n+        Args:\n+            generated_outputs (`torch.Tensor` or `np.ndarray`):\n+                The output of the model. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n+                containing the token ids of the generated sequences.\n+            skip_special_tokens (`bool`, *optional*, defaults to `False`):\n+                Whether or not to remove special tokens in the output. Argument passed to the tokenizer's `batch_decode` method.\n+            **kwargs:\n+                Additional arguments to be passed to the tokenizer's `batch_decode method`.\n+\n+        Returns:\n+            `list[str]`: The decoded text output.\n+        \"\"\"\n+        return self.batch_decode(generated_outputs, skip_special_tokens=skip_special_tokens, **kwargs)\n+\n+    def post_process_generation(self, text=None, sequence=None, task=None, image_size=None) -> dict[str, Any]:\n+        \"\"\"\n+        Post-process generation outputs based on the task.\n+\n+        Args:\n+            text (`str`, *optional*):\n+                Generated text.\n+            sequence (`Union[List[int], torch.Tensor]`, *optional*):\n+                Generated token sequence.\n+            task (`str`, *optional*):\n+                The task for post-processing.\n+            image_size (`Tuple[int, int]`, *optional*):\n+                Image size for dequantization.\n+\n+        Returns:\n+            `Dict[str, Any]`: Post-processed results keyed by task.\n+        \"\"\"\n+        if task is None:\n+            raise ValueError(\"`task` must be provided for post-processing.\")\n+\n+        post_proc_type = self.tasks_answer_post_processing_type.get(task, \"pure_text\")\n+        parsed = self.post_processor(\n+            text=text,\n+            sequence=sequence,\n+            image_size=image_size,\n+            parse_tasks=[post_proc_type],\n+        )[post_proc_type]\n+\n+        if post_proc_type == \"pure_text\":\n+            final_answer = parsed.replace(\"<s>\", \"\").replace(\"</s>\", \"\").strip()\n+        elif post_proc_type in [\"description_with_bboxes\", \"bboxes\"]:\n+            bboxes = [inst[\"bbox\"] for inst in parsed]\n+            labels = [inst[\"cat_name\"] for inst in parsed]\n+            final_answer = {\"bboxes\": bboxes, \"labels\": labels}\n+            if parsed and \"score\" in parsed[0]:\n+                final_answer[\"scores\"] = [inst[\"score\"] for inst in parsed]\n+        elif post_proc_type == \"ocr\":\n+            quad_boxes = [inst[\"quad_box\"] for inst in parsed]\n+            labels = [inst[\"text\"] for inst in parsed]\n+            final_answer = {\"quad_boxes\": quad_boxes, \"labels\": labels}\n+        elif post_proc_type == \"phrase_grounding\":\n+            bboxes = []\n+            labels = []\n+            for inst in parsed:\n+                for bbox in inst[\"bbox\"]:\n+                    bboxes.append(bbox)\n+                    labels.append(inst[\"cat_name\"])\n+            final_answer = {\"bboxes\": bboxes, \"labels\": labels}\n+        elif post_proc_type in [\"description_with_polygons\", \"polygons\"]:\n+            polygons = [inst[\"polygons\"] for inst in parsed]\n+            labels = [inst[\"cat_name\"] for inst in parsed]\n+            final_answer = {\"polygons\": polygons, \"labels\": labels}\n+        elif post_proc_type == \"description_with_bboxes_or_polygons\":\n+            bboxes = []\n+            bboxes_labels = []\n+            polygons = []\n+            polygons_labels = []\n+            for inst in parsed:\n+                label = inst[\"cat_name\"]\n+                if \"polygons\" in inst:\n+                    polygons.append(inst[\"polygons\"])\n+                    polygons_labels.append(label)\n+                else:\n+                    bboxes.append(inst[\"bbox\"])\n+                    bboxes_labels.append(label)\n+            final_answer = {\n+                \"bboxes\": bboxes,\n+                \"bboxes_labels\": bboxes_labels,\n+                \"polygons\": polygons,\n+                \"polygons_labels\": polygons_labels,\n+            }\n+        else:\n+            raise ValueError(f\"Unknown post-processing type: {post_proc_type}\")\n+\n+        return {task: final_answer}\n+\n+\n+class Florence2PostProcessor:\n+    \"\"\"\n+    Post-processor for Florence-2 model outputs. Parses generated text into structured results for various tasks\n+    like object detection, OCR, phrase grounding, etc.\n+\n+    Args:\n+        tokenizer (`PreTrainedTokenizer`):\n+            The tokenizer used for decoding model outputs.\n+    \"\"\"\n+\n+    def __init__(self, config, tokenizer):\n+        self.tokenizer = tokenizer\n+        self.parse_task_config = config or {}\n+        self.banned_grounding_tokens = set(\n+            self.parse_task_config.get(\"phrase_grounding\", {}).get(\"banned_grounding_tokens\", [])\n+        )\n+        self.all_special_tokens = set(self.tokenizer.all_special_tokens)\n+        self.quantize_bins = (1000, 1000)\n+\n+    def quantize(self, locations: \"torch.Tensor\", size: tuple[int, int]) -> \"torch.Tensor\":\n+        \"\"\"\n+        Quantize locations.\n+\n+        Args:\n+            locations (`torch.Tensor`):\n+                Tensor of shape (N, 4) for boxes or (N, 2) for points/coordinates.\n+            size (`tuple[int, int]`):\n+                Original image size (width, height).\n+\n+        Returns:\n+            `torch.Tensor`: Quantized locations as integers.\n+        \"\"\"\n+        bins_w, bins_h = self.quantize_bins\n+        size_w, size_h = size\n+        per_bin_w = size_w / bins_w\n+        per_bin_h = size_h / bins_h\n+\n+        if locations.shape[-1] == 4:  # Bounding boxes: [xmin, ymin, xmax, ymax]\n+            xmin, ymin, xmax, ymax = locations.split(1, dim=-1)\n+            q_xmin = (xmin / per_bin_w).floor().clamp(0, bins_w - 1)\n+            q_ymin = (ymin / per_bin_h).floor().clamp(0, bins_h - 1)\n+            q_xmax = (xmax / per_bin_w).floor().clamp(0, bins_w - 1)\n+            q_ymax = (ymax / per_bin_h).floor().clamp(0, bins_h - 1)\n+            return torch.cat([q_xmin, q_ymin, q_xmax, q_ymax], dim=-1).int()\n+\n+        elif locations.shape[-1] == 2:  # Points/coordinates: [x, y]\n+            x, y = locations.split(1, dim=-1)\n+            q_x = (x / per_bin_w).floor().clamp(0, bins_w - 1)\n+            q_y = (y / per_bin_h).floor().clamp(0, bins_h - 1)\n+            return torch.cat([q_x, q_y], dim=-1).int()\n+\n+        else:\n+            raise ValueError(f\"Unsupported location shape: last dim must be 2 or 4, got {locations.shape[-1]}.\")\n+\n+    def dequantize(self, locations: \"torch.Tensor\", size: tuple[int, int]) -> \"torch.Tensor\":\n+        \"\"\"\n+        Dequantize locations back to original scale.\n+\n+        Args:\n+            locations (`torch.Tensor`):\n+                Quantized tensor of shape (N, 4) for boxes or (N, 2) for points/coordinates.\n+            size (`tuple[int, int]`):\n+                Original image size (width, height).\n+\n+        Returns:\n+            `torch.Tensor`: Dequantized locations as floats.\n+        \"\"\"\n+        bins_w, bins_h = self.quantize_bins\n+        size_w, size_h = size\n+        per_bin_w = size_w / bins_w\n+        per_bin_h = size_h / bins_h\n+\n+        # Add 0.5 to use the center position of the bin as the coordinate.\n+        if locations.shape[-1] == 4:  # Bounding boxes\n+            xmin, ymin, xmax, ymax = locations.split(1, dim=-1)\n+            dq_xmin = (xmin + 0.5) * per_bin_w\n+            dq_ymin = (ymin + 0.5) * per_bin_h\n+            dq_xmax = (xmax + 0.5) * per_bin_w\n+            dq_ymax = (ymax + 0.5) * per_bin_h\n+            return torch.cat([dq_xmin, dq_ymin, dq_xmax, dq_ymax], dim=-1).int()\n+\n+        elif locations.shape[-1] == 2:  # Points/coordinates\n+            x, y = locations.split(1, dim=-1)\n+            dq_x = (x + 0.5) * per_bin_w\n+            dq_y = (y + 0.5) * per_bin_h\n+            return torch.cat([dq_x, dq_y], dim=-1).int()\n+\n+        else:\n+            raise ValueError(f\"Unsupported location shape: last dim must be 2 or 4, got {locations.shape[-1]}.\")\n+\n+    def decode_with_spans(self, token_ids: list[int]) -> tuple[str, list[tuple[int, int]]]:\n+        \"\"\"\n+        Decode token IDs to text and compute character spans.\n+\n+        Args:\n+            token_ids (`list[int]`):\n+                list of token IDs to decode.\n+\n+        Returns:\n+            `tuple[str, list[tuple[int, int]]]`: Decoded text and list of spans (start, end) for each token.\n+        \"\"\"\n+        filtered_tokens = self.tokenizer.convert_ids_to_tokens(token_ids, skip_special_tokens=False)\n+        text = \"\"\n+        spans = []\n+        for token in filtered_tokens:\n+            if token in self.all_special_tokens:\n+                sub_text = token\n+            else:\n+                sub_text = self.tokenizer.convert_tokens_to_string([token])\n+            span = (len(text), len(text) + len(sub_text))\n+            text += sub_text\n+            spans.append(span)\n+        return text, spans\n+\n+    def parse_ocr_from_text_and_spans(\n+        self, text: str, pattern: Optional[str], image_size: tuple[int, int], area_threshold: float = 0.0\n+    ) -> list[dict[str, Any]]:\n+        \"\"\"\n+        Parse OCR results with quadrilateral boxes.\n+\n+        Args:\n+            text (`str`):\n+                The generated text.\n+            pattern (`str`):\n+                Regex pattern for matching.\n+            image_size (`tuple[int, int]`):\n+                Image size (width, height).\n+            area_threshold (`float`, *optional*, defaults to 0.0):\n+                Minimum area threshold for filtering boxes.\n+\n+        Returns:\n+            `list[dict[str, Any]]`: list of instances with 'quad_box' and 'text'.\n+        \"\"\"\n+        text = text.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<pad>\", \"\")\n+        if pattern is None:\n+            pattern = r\"(.+?)<loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)>\"\n+\n+        matches = re.findall(pattern, text)\n+        instances = []\n+        width, height = image_size\n+\n+        for content, *quad_str in matches:\n+            quad_bins = [int(i) for i in quad_str]\n+            quad_box = self.dequantize(torch.tensor(quad_bins).reshape(-1, 2), size=image_size).flatten().tolist()\n+\n+            if area_threshold > 0:\n+                x_coords = quad_box[0::2]\n+                y_coords = quad_box[1::2]\n+                # Apply the Shoelace formula\n+                area = 0.5 * abs(\n+                    sum(x_coords[i] * y_coords[i + 1] - x_coords[i + 1] * y_coords[i] for i in range(4 - 1))\n+                )\n+\n+                if area < (width * height) * area_threshold:\n+                    continue\n+\n+            instances.append({\"quad_box\": quad_box, \"text\": content.strip()})\n+        return instances\n+\n+    def parse_phrase_grounding_from_text_and_spans(\n+        self, text: str, image_size: tuple[int, int]\n+    ) -> list[dict[str, Any]]:\n+        \"\"\"\n+        Parse phrase grounding results.\n+\n+        Args:\n+            text (`str`):\n+                The generated text.\n+            image_size (`tuple[int, int]`):\n+                Image size (width, height).\n+\n+        Returns:\n+            `list[dict[str, Any]]`: list of instances with 'bbox' and 'cat_name'.\n+        \"\"\"\n+        text = text.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<pad>\", \"\")\n+        phrase_pattern = r\"([^<]+(?:<loc_\\d+>){4,})\"\n+        phrases = re.findall(phrase_pattern, text)\n+        text_pattern = r\"^\\s*(.*?)(?=<od>|</od>|<box>|</box>|<bbox>|</bbox>|<loc_)\"\n+        box_pattern = r\"<loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)>\"\n+\n+        instances = []\n+        for phrase_text in phrases:\n+            phrase_text = phrase_text.replace(\"<ground>\", \"\", 1).replace(\"<obj>\", \"\", 1)\n+            if not phrase_text:\n+                continue\n+            match = re.search(text_pattern, phrase_text)\n+            if not match:\n+                continue\n+            phrase = match.group().strip()\n+            if phrase in self.banned_grounding_tokens:\n+                continue\n+            boxes_matches = list(re.finditer(box_pattern, phrase_text))\n+            if not boxes_matches:\n+                continue\n+            bbox_bins = [[int(m.group(j)) for j in range(1, 5)] for m in boxes_matches]\n+            bboxes = self.dequantize(torch.tensor(bbox_bins), size=image_size).tolist()\n+            phrase = phrase.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n+            instances.append({\"bbox\": bboxes, \"cat_name\": phrase})\n+        return instances\n+\n+    def _find_matched_token_indices(self, cur_span: tuple[int, int], token_spans: list[tuple[int, int]]) -> list[int]:\n+        return [i for i, span in enumerate(token_spans) if not (span[1] <= cur_span[0] or span[0] >= cur_span[1])]\n+\n+    def parse_description_with_bboxes_from_text_and_spans(\n+        self,\n+        text: str,\n+        image_size: tuple[int, int],\n+        allow_empty_phrase: bool = False,\n+    ) -> list[dict[str, Any]]:\n+        \"\"\"\n+        Parse descriptions with bounding boxes.\n+\n+        Args:\n+            text (`str`):\n+                The generated text.\n+            image_size (`tuple[int, int]`):\n+                Image size (width, height).\n+            allow_empty_phrase (`bool`, *optional*, defaults to `False`):\n+                Allow phrases without text.\n+\n+        Returns:\n+            `list[dict[str, Any]]`: list of instances with 'bbox', 'cat_name', and optional 'score'.\n+        \"\"\"\n+        text = text.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<pad>\", \"\")\n+\n+        if allow_empty_phrase:\n+            pattern = r\"(?:(?:<loc_\\d+>){4,})\"\n+        else:\n+            pattern = r\"([^<]+(?:<loc_\\d+>){4,})\"\n+        phrases = re.findall(pattern, text)\n+\n+        text_pattern = r\"^\\s*(.*?)(?=<od>|</od>|<box>|</box>|<bbox>|</bbox>|<loc_)\"\n+        box_pattern = r\"<loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)>\"\n+\n+        instances = []\n+        for phrase_text in phrases:\n+            phrase_text = phrase_text.replace(\"<ground>\", \"\", 1).replace(\"<obj>\", \"\", 1)\n+            if not phrase_text and not allow_empty_phrase:\n+                continue\n+            match = re.search(text_pattern, phrase_text)\n+            if not match:\n+                continue\n+            phrase = match.group().strip()\n+            boxes_matches = list(re.finditer(box_pattern, phrase_text))\n+            if not boxes_matches:\n+                continue\n+            bbox_bins = [[int(m.group(j)) for j in range(1, 5)] for m in boxes_matches]\n+            bboxes = self.dequantize(torch.tensor(bbox_bins), size=image_size).tolist()\n+\n+            phrase = phrase.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n+            for bbox in bboxes:\n+                instance = {\"bbox\": bbox, \"cat_name\": phrase}\n+                instances.append(instance)\n+\n+        return instances\n+\n+    def parse_description_with_polygons_from_text_and_spans(\n+        self,\n+        text: str,\n+        image_size: tuple[int, int],\n+        allow_empty_phrase: bool = False,\n+        polygon_sep_token: str = \"<sep>\",\n+        polygon_start_token: str = \"<poly>\",\n+        polygon_end_token: str = \"</poly>\",\n+        with_box_at_start: bool = False,\n+    ) -> list[dict[str, Any]]:\n+        \"\"\"\n+        Parse descriptions with polygons.\n+\n+        Args:\n+            text (`str`):\n+                The generated text.\n+            image_size (`tuple[int, int]`):\n+                Image size (width, height).\n+            allow_empty_phrase (`bool`, *optional*, defaults to `False`):\n+                Allow phrases without text.\n+            polygon_sep_token (`str`, *optional*, defaults to \"<sep>\"):\n+                Token separating polygons.\n+            polygon_start_token (`str`, *optional*, defaults to \"<poly>\"):\n+                Start token for polygons.\n+            polygon_end_token (`str`, *optional*, defaults to \"</poly>\"):\n+                End token for polygons.\n+            with_box_at_start (`bool`, *optional*, defaults to `False`):\n+                Whether a bounding box is at the start of polygons.\n+\n+        Returns:\n+            `list[dict[str, Any]]`: list of instances with 'polygons', 'cat_name', and optional 'bbox'.\n+        \"\"\"\n+        text = text.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<pad>\", \"\")\n+\n+        if allow_empty_phrase:\n+            pattern = rf\"(?:(?:<loc_\\d+>|{re.escape(polygon_sep_token)}|{re.escape(polygon_start_token)}|{re.escape(polygon_end_token)}){{4,}})\"\n+        else:\n+            pattern = rf\"([^<]+(?:<loc_\\d+>|{re.escape(polygon_sep_token)}|{re.escape(polygon_start_token)}|{re.escape(polygon_end_token)}){{4,}})\"\n+        phrases = re.findall(pattern, text)\n+        phrase_pattern = r\"^\\s*(.*?)(?=<od>|</od>|<box>|</box>|<bbox>|</bbox>|<loc_|<poly>)\"\n+        poly_instance_pattern = rf\"{re.escape(polygon_start_token)}(.*?){re.escape(polygon_end_token)}\"\n+        box_pattern = rf\"((?:<loc_\\d+>)+)(?:{re.escape(polygon_sep_token)}|$)\"\n+\n+        instances = []\n+        for phrase_text in phrases:\n+            phrase_text_strip = re.sub(r\"^<loc_\\d+>\", \"\", phrase_text, count=1)\n+            if not phrase_text_strip and not allow_empty_phrase:\n+                continue\n+            match = re.search(phrase_pattern, phrase_text_strip)\n+            if not match:\n+                continue\n+            phrase = match.group().strip()\n+\n+            if polygon_start_token in phrase_text and polygon_end_token in phrase_text:\n+                poly_instances = [m.group(1) for m in re.finditer(poly_instance_pattern, phrase_text)]\n+            else:\n+                poly_instances = [phrase_text]\n+\n+            for poly_inst in poly_instances:\n+                poly_matches = list(re.finditer(box_pattern, poly_inst))\n+                if len(poly_matches) == 0:\n+                    continue\n+                bbox = []\n+                polygons = []\n+                for poly_match in poly_matches:\n+                    poly_str = poly_match.group(1)\n+                    poly_bins = [int(m.group(1)) for m in re.finditer(r\"<loc_(\\d+)>\", poly_str)]\n+                    if with_box_at_start and not bbox:\n+                        if len(poly_bins) > 4:\n+                            bbox = poly_bins[:4]\n+                            poly_bins = poly_bins[4:]\n+                        else:\n+                            bbox = [0, 0, 0, 0]\n+                    if len(poly_bins) % 2 == 1:\n+                        poly_bins = poly_bins[:-1]\n+                    poly_coords = (\n+                        self.dequantize(torch.tensor(poly_bins).reshape(-1, 2), size=image_size).flatten().tolist()\n+                    )\n+                    polygons.append(poly_coords)\n+\n+                instance = {\"cat_name\": phrase, \"polygons\": polygons}\n+                if bbox:\n+                    instance[\"bbox\"] = self.dequantize(torch.tensor([bbox]), size=image_size)[0].tolist()\n+                instances.append(instance)\n+        return instances\n+\n+    def __call__(self, text=None, sequence=None, image_size=None, parse_tasks=None) -> dict[str, Any]:\n+        \"\"\"\n+        Process model output and parse into task-specific results.\n+\n+        Args:\n+            text (`Optional[str]`, *optional*):\n+                Generated text. Either this or `sequence` must be provided.\n+            sequence (`Optional[Union[list[int], torch.Tensor]]`, *optional*):\n+                Token sequence. Either this or `text` must be provided.\n+            image_size (`Optional[tuple[int, int]]`, *optional*):\n+                Image size (width, height) required for dequantization.\n+            parse_tasks (`Optional[Union[str, list[str]]]`, *optional*):\n+                Specific tasks to parse. If None, parse all supported tasks.\n+\n+        Returns:\n+            `dict[str, Any]`: Parsed results for each task, including the raw 'text'.\n+        \"\"\"\n+        if parse_tasks is not None:\n+            parse_tasks = [parse_tasks] if isinstance(parse_tasks, str) else parse_tasks\n+            for task in parse_tasks:\n+                if task not in self.parse_task_config.keys():\n+                    raise ValueError(f\"Unsupported parse task: {task}\")\n+\n+        if (text is None and sequence is None) or (text is not None and sequence is not None):\n+            raise ValueError(\"Exactly one of 'text' or 'sequence' must be provided.\")\n+\n+        if sequence is not None:\n+            if isinstance(sequence, torch.Tensor):\n+                sequence = sequence.tolist()\n+            sequence = sequence[1:] if sequence[0] == self.tokenizer.bos_token_id else sequence  # Skip BOS if present\n+            text, _ = self.decode_with_spans(sequence)\n+\n+        parsed_dict = {\"text\": text}\n+\n+        tasks_to_parse = parse_tasks or self.parse_task_config.keys()\n+        for task in tasks_to_parse:\n+            config = self.parse_task_config[task]\n+            pattern = config.get(\"PATTERN\")\n+\n+            if task == \"ocr\":\n+                parsed_dict[\"ocr\"] = self.parse_ocr_from_text_and_spans(\n+                    text, pattern=pattern, image_size=image_size, area_threshold=config.get(\"AREA_THRESHOLD\", 0.0)\n+                )\n+            elif task == \"phrase_grounding\":\n+                parsed_dict[\"phrase_grounding\"] = self.parse_phrase_grounding_from_text_and_spans(\n+                    text, image_size=image_size\n+                )\n+            elif task == \"pure_text\":\n+                parsed_dict[\"pure_text\"] = text\n+            elif task == \"description_with_bboxes\":\n+                parsed_dict[\"description_with_bboxes\"] = self.parse_description_with_bboxes_from_text_and_spans(\n+                    text, image_size=image_size\n+                )\n+            elif task == \"description_with_polygons\":\n+                parsed_dict[\"description_with_polygons\"] = self.parse_description_with_polygons_from_text_and_spans(\n+                    text, image_size=image_size\n+                )\n+            elif task == \"polygons\":\n+                parsed_dict[\"polygons\"] = self.parse_description_with_polygons_from_text_and_spans(\n+                    text, image_size=image_size, allow_empty_phrase=True\n+                )\n+            elif task == \"bboxes\":\n+                parsed_dict[\"bboxes\"] = self.parse_description_with_bboxes_from_text_and_spans(\n+                    text, image_size=image_size, allow_empty_phrase=True\n+                )\n+            elif task == \"description_with_bboxes_or_polygons\":\n+                if \"<poly>\" in text:\n+                    instances = self.parse_description_with_polygons_from_text_and_spans(text, image_size=image_size)\n+                else:\n+                    instances = self.parse_description_with_bboxes_from_text_and_spans(text, image_size=image_size)\n+                parsed_dict[\"description_with_bboxes_or_polygons\"] = instances\n+            else:\n+                raise ValueError(\"task {} is not supported\".format(task))\n+\n+        return parsed_dict\n+\n+\n+class Florence2VisionDropPath(BeitDropPath):\n+    pass\n+\n+\n+class Florence2VisionLearnedAbsolutePositionEmbedding2D(nn.Module):\n+    \"\"\"\n+    This module learns positional embeddings up to a fixed maximum size.\n+    \"\"\"\n+\n+    def __init__(self, config: Florence2Config):\n+        super().__init__()\n+        num_pos = config.vision_config.max_position_embeddings\n+        embedding_dim = config.vision_config.embed_dim[-1]\n+        self.row_embeddings = nn.Embedding(num_pos, embedding_dim // 2)\n+        self.column_embeddings = nn.Embedding(num_pos, embedding_dim - (embedding_dim // 2))\n+\n+    def forward(self, pixel_values, pixel_mask=None):\n+        height, width = pixel_values.shape[-2:]\n+        width_values = torch.arange(width, device=pixel_values.device)\n+        height_values = torch.arange(height, device=pixel_values.device)\n+        x_emb = self.column_embeddings(width_values)\n+        y_emb = self.row_embeddings(height_values)\n+        pos = torch.cat([x_emb.unsqueeze(0).repeat(height, 1, 1), y_emb.unsqueeze(1).repeat(1, width, 1)], dim=-1)\n+        pos = pos.permute(2, 0, 1)\n+        pos = pos.unsqueeze(0)\n+        pos = pos.repeat(pixel_values.shape[0], 1, 1, 1)\n+        return pos\n+\n+\n+class Florence2VisionPositionalEmbeddingCosine1D(nn.Module):\n+    \"\"\"\n+    This module generates 1D cosine positional embeddings using precomputed sinusoidal functions.\n+    \"\"\"\n+\n+    def __init__(self, config: Florence2Config):\n+        super().__init__()\n+        self.embed_dim = config.vision_config.embed_dim[-1]\n+        self.max_seq_len = config.vision_config.max_temporal_embeddings\n+        pos_idx_to_embed = torch.empty((self.max_seq_len, self.embed_dim))\n+        sine, cosine = self.get_sinusoid_embeddings(\n+            max_positions=self.max_seq_len,\n+            embed_dim=self.embed_dim,\n+        )\n+        pos_idx_to_embed[:, 0::2] = sine\n+        pos_idx_to_embed[:, 1::2] = cosine\n+        # Save the positional embeddings in a constant buffer.\n+        self.register_buffer(\"pos_idx_to_embed\", pos_idx_to_embed)\n+\n+    @staticmethod\n+    def get_sinusoid_embeddings(max_positions: int, embed_dim: int):\n+        half_dim = embed_dim // 2\n+        emb = math.log(10000) / half_dim\n+        emb = torch.exp(torch.arange(half_dim, dtype=torch.int64).float() * -emb)\n+        emb = torch.arange(max_positions, dtype=torch.float).unsqueeze(1) * emb.unsqueeze(0)\n+        return torch.sin(emb), torch.cos(emb)\n+\n+    def forward(self, seq_embeds: torch.Tensor) -> torch.Tensor:\n+        len_seq = seq_embeds.size(1)\n+        if len_seq > self.max_seq_len:\n+            raise ValueError(f\"Maximum sequence length {self.max_seq_len}, got {len_seq}\")\n+        pos_embeds = self.pos_idx_to_embed[0:len_seq, :]\n+        return pos_embeds\n+\n+\n+class Florence2VisionMLP(Llama4VisionMLP):\n+    def __init__(self, config: Florence2VisionConfig, stage_idx: int):\n+        super().__init__()\n+        self.fc1 = nn.Linear(config.embed_dim[stage_idx], int(config.embed_dim[stage_idx] * config.mlp_ratio))\n+        self.activation_fn = ACT2FN[config.activation_function]\n+        self.fc2 = nn.Linear(int(config.embed_dim[stage_idx] * config.mlp_ratio), config.embed_dim[stage_idx])\n+\n+\n+class Florence2VisionConvEmbed(nn.Module):\n+    \"\"\"Image to Patch Embedding\"\"\"\n+\n+    def __init__(self, config: Florence2VisionConfig, stage_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.stage_idx = stage_idx\n+        self.patch_size = config.patch_size[stage_idx]\n+        self.in_channels = config.in_channels if stage_idx == 0 else config.embed_dim[stage_idx - 1]\n+        self.embed_dim = config.embed_dim[stage_idx]\n+        self.stride = config.patch_stride[stage_idx]\n+        self.padding = config.patch_padding[stage_idx]\n+        self.pre_norm = config.patch_prenorm[stage_idx]\n+\n+        self.conv = nn.Conv2d(\n+            self.in_channels,\n+            self.embed_dim,\n+            kernel_size=self.patch_size,\n+            stride=self.stride,\n+            padding=self.padding,\n+        )\n+\n+        dim_norm = self.in_channels if self.pre_norm else self.embed_dim\n+        self.norm = nn.LayerNorm(dim_norm)\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        if self.norm and self.pre_norm:\n+            hidden_states = hidden_states.permute(0, 2, 3, 1)\n+            hidden_states = self.norm(hidden_states)\n+            hidden_states = hidden_states.permute(0, 3, 1, 2)\n+\n+        hidden_states = self.conv(hidden_states)\n+\n+        if self.norm and not self.pre_norm:\n+            hidden_states = hidden_states.permute(0, 2, 3, 1)\n+            hidden_states = self.norm(hidden_states)\n+            hidden_states = hidden_states.permute(0, 3, 1, 2)\n+        return hidden_states\n+\n+\n+class Florence2VisionChannelAttention(nn.Module):\n+    def __init__(self, config: Florence2VisionConfig, stage_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.dim = config.embed_dim[stage_idx]\n+        self.groups = config.num_groups[stage_idx]\n+        self.qkv = nn.Linear(self.dim, self.dim * 3, bias=config.qkv_bias)\n+        self.proj = nn.Linear(self.dim, self.dim)\n+        self.is_causal = False\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        batch_size, num_tokens, hidden_size = hidden_states.shape\n+\n+        # Reshape for grouped channel attention\n+        qkv = self.qkv(hidden_states).reshape(batch_size, num_tokens, 3, self.groups, hidden_size // self.groups)\n+        qkv = qkv.permute(2, 0, 3, 4, 1)\n+        query, key, value = qkv.unbind(0)\n+\n+        scale = num_tokens**-0.5\n+        # Channel-to-channel attention within groups:\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+        hidden_states, _ = attention_interface(\n+            self,\n+            query,\n+            key,\n+            value,\n+            attention_mask=None,\n+            scaling=scale,\n+        )\n+        hidden_states = hidden_states.permute(0, 3, 2, 1)\n+        hidden_states = hidden_states.reshape(batch_size, num_tokens, hidden_size)\n+\n+        # Final projection\n+        hidden_states = self.proj(hidden_states)\n+        return hidden_states\n+\n+\n+class Florence2VisionChannelBlock(nn.Module):\n+    def __init__(\n+        self,\n+        config: Florence2VisionConfig,\n+        stage_idx: int,\n+        drop_path_rate: float,\n+    ):\n+        super().__init__()\n+\n+        self.config = config\n+        dim_in = config.embed_dim[stage_idx]\n+\n+        self.conv1 = nn.Conv2d(\n+            dim_in,\n+            dim_in,\n+            kernel_size=3,\n+            padding=1,\n+            groups=dim_in,\n+        )\n+        self.norm1 = nn.LayerNorm(config.embed_dim[stage_idx])\n+        self.channel_attn = Florence2VisionChannelAttention(config=config, stage_idx=stage_idx)\n+        self.drop_path1 = Florence2VisionDropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n+\n+        self.conv2 = nn.Conv2d(\n+            dim_in,\n+            dim_in,\n+            kernel_size=3,\n+            padding=1,\n+            groups=dim_in,\n+        )\n+        self.norm2 = nn.LayerNorm(config.embed_dim[stage_idx])\n+        self.ffn = Florence2VisionMLP(config=config, stage_idx=stage_idx)\n+        self.drop_path2 = Florence2VisionDropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        batch_size, embed_dim, height, width = hidden_states.shape\n+\n+        # First channel block: Depthwise Conv + Channel Attention\n+        hidden_states = self.conv1(hidden_states) + hidden_states\n+        hidden_states = hidden_states.flatten(2).transpose(1, 2)\n+        residual = hidden_states\n+\n+        # Channel group attention self-attention mechanism\n+        hidden_states = self.norm1(hidden_states)\n+        hidden_states = self.channel_attn(hidden_states)\n+        hidden_states = residual + self.drop_path1(hidden_states)\n+        hidden_states = hidden_states.transpose(1, 2).view(batch_size, embed_dim, height, width)\n+\n+        # Second channel block: Depthwise Conv + FFN\n+        hidden_states = self.conv2(hidden_states) + hidden_states\n+        hidden_states = hidden_states.flatten(2).transpose(1, 2)\n+        residual = hidden_states\n+\n+        # FFN\n+        hidden_states = self.norm2(hidden_states)\n+        hidden_states = self.ffn(hidden_states)\n+        hidden_states = residual + self.drop_path2(hidden_states)\n+        hidden_states = hidden_states.transpose(1, 2).view(batch_size, embed_dim, height, width)\n+\n+        return hidden_states\n+\n+\n+class Florence2VisionWindowAttention(nn.Module):\n+    def __init__(self, config: Florence2VisionConfig, stage_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.dim = config.embed_dim[stage_idx]\n+        self.window_size = config.window_size\n+        self.num_heads = config.num_heads[stage_idx]\n+        head_dim = self.dim // self.num_heads\n+        self.scale = head_dim**-0.5\n+\n+        self.qkv = nn.Linear(self.dim, self.dim * 3, bias=config.qkv_bias)\n+        self.proj = nn.Linear(self.dim, self.dim)\n+        self.is_causal = False\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        batch_size, height, width, embed_dim = hidden_states.shape\n+\n+        # Pad the input if necessary\n+        pad_left = pad_top = 0\n+        pad_right = (self.window_size - width % self.window_size) % self.window_size\n+        pad_bottom = (self.window_size - height % self.window_size) % self.window_size\n+        hidden_states = F.pad(hidden_states, (0, 0, pad_left, pad_right, pad_top, pad_bottom))\n+        _, padded_height, padded_width, _ = hidden_states.shape\n+\n+        # Partition input into non-overlapping windows (for local spatial attention in DaViT)\n+        hidden_states = hidden_states.view(\n+            batch_size,\n+            padded_height // self.window_size,\n+            self.window_size,\n+            padded_width // self.window_size,\n+            self.window_size,\n+            embed_dim,\n+        )\n+        windowed_hidden_states = hidden_states.permute(0, 1, 3, 2, 4, 5).contiguous()\n+        windowed_hidden_states = windowed_hidden_states.view(-1, self.window_size * self.window_size, embed_dim)\n+\n+        # Generate Q, K, V for each window\n+        num_windows_per_batch, num_tokens_per_window, embed_dim = windowed_hidden_states.shape\n+        qkv = self.qkv(windowed_hidden_states).reshape(\n+            num_windows_per_batch, num_tokens_per_window, 3, self.num_heads, embed_dim // self.num_heads\n+        )\n+        qkv = qkv.permute(2, 0, 3, 1, 4)\n+        query, key, value = qkv.unbind(0)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        windowed_hidden_states, _ = attention_interface(\n+            self,\n+            query,\n+            key,\n+            value,\n+            attention_mask=None,\n+            scaling=self.scale,\n+        )\n+        windowed_hidden_states = windowed_hidden_states.view(num_windows_per_batch, num_tokens_per_window, embed_dim)\n+        windowed_hidden_states = self.proj(windowed_hidden_states)\n+\n+        # Merge windows back to original spatial layout\n+        windowed_hidden_states = windowed_hidden_states.view(-1, self.window_size, self.window_size, embed_dim)\n+        hidden_states = windowed_hidden_states.view(\n+            -1,\n+            padded_height // self.window_size,\n+            padded_width // self.window_size,\n+            self.window_size,\n+            self.window_size,\n+            embed_dim,\n+        )\n+        hidden_states = hidden_states.permute(0, 1, 3, 2, 4, 5).contiguous()\n+        hidden_states = hidden_states.view(-1, padded_height, padded_width, embed_dim)\n+        hidden_states = hidden_states[:, :height, :width, :].contiguous()\n+        hidden_states = hidden_states.view(batch_size, height * width, embed_dim)\n+\n+        return hidden_states\n+\n+\n+class Florence2VisionSpatialBlock(nn.Module):\n+    def __init__(\n+        self,\n+        config: Florence2VisionConfig,\n+        stage_idx: int,\n+        drop_path_rate: float,\n+    ):\n+        super().__init__()\n+\n+        self.conv1 = nn.Conv2d(\n+            config.embed_dim[stage_idx],\n+            config.embed_dim[stage_idx],\n+            kernel_size=3,\n+            padding=1,\n+            groups=config.embed_dim[stage_idx],\n+        )\n+        self.norm1 = nn.LayerNorm(config.embed_dim[stage_idx])\n+        self.window_attn = Florence2VisionWindowAttention(config=config, stage_idx=stage_idx)\n+        self.drop_path1 = Florence2VisionDropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n+\n+        self.conv2 = nn.Conv2d(\n+            config.embed_dim[stage_idx],\n+            config.embed_dim[stage_idx],\n+            kernel_size=3,\n+            padding=1,\n+            groups=config.embed_dim[stage_idx],\n+        )\n+        self.norm2 = nn.LayerNorm(config.embed_dim[stage_idx])\n+        self.ffn = Florence2VisionMLP(config=config, stage_idx=stage_idx)\n+        self.drop_path2 = Florence2VisionDropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        batch_size, embed_dim, height, width = hidden_states.shape\n+\n+        # First spatial mixing block: Conv + Window Attention\n+        hidden_states = self.conv1(hidden_states) + hidden_states\n+        hidden_states = hidden_states.flatten(2).transpose(1, 2)\n+        residual = hidden_states\n+\n+        # Spatial Window-based self-attention mechanism\n+        hidden_states = self.norm1(hidden_states)\n+        hidden_states = hidden_states.view(batch_size, height, width, embed_dim)\n+        hidden_states = self.window_attn(hidden_states)\n+        hidden_states = residual + self.drop_path1(hidden_states)\n+        hidden_states = hidden_states.transpose(1, 2).view(batch_size, embed_dim, height, width)\n+\n+        # Second spatial mixing block: Conv + FFN\n+        hidden_states = self.conv2(hidden_states) + hidden_states\n+        hidden_states = hidden_states.flatten(2).transpose(1, 2)\n+        residual = hidden_states\n+\n+        # FFN\n+        hidden_states = self.norm2(hidden_states)\n+        hidden_states = self.ffn(hidden_states)\n+        hidden_states = residual + self.drop_path2(hidden_states)\n+        hidden_states = hidden_states.transpose(1, 2).view(batch_size, embed_dim, height, width)\n+\n+        return hidden_states\n+\n+\n+class Florence2VisionBlock(nn.Module):\n+    def __init__(\n+        self,\n+        config: Florence2VisionConfig,\n+        stage_idx: int,\n+        spatial_drop_path_rate: float,\n+        channel_drop_path_rate: float,\n+    ):\n+        super().__init__()\n+        self.spatial_block = Florence2VisionSpatialBlock(\n+            config=config,\n+            stage_idx=stage_idx,\n+            drop_path_rate=spatial_drop_path_rate,\n+        )\n+        self.channel_block = Florence2VisionChannelBlock(\n+            config=config,\n+            stage_idx=stage_idx,\n+            drop_path_rate=channel_drop_path_rate,\n+        )\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        hidden_states = self.spatial_block(hidden_states)\n+        hidden_states = self.channel_block(hidden_states)\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class Florence2VisionPreTrainedModel(PreTrainedModel):\n+    config_class = Florence2VisionConfig\n+    main_input_name = \"pixel_values\"\n+    _supports_sdpa = True\n+    _supports_flash_attn = True\n+    _supports_flex_attn = True\n+\n+    _can_compile_fullgraph = True\n+\n+\n+@auto_docstring\n+class Florence2VisionBackbone(Florence2VisionPreTrainedModel):\n+    def __init__(self, config: Florence2VisionConfig):\n+        super().__init__(config)\n+        self.config = config\n+\n+        self.embed_dim = config.embed_dim\n+        self.num_heads = config.num_heads\n+        self.num_groups = config.num_groups\n+        self.num_stages = len(self.embed_dim)\n+\n+        if not (self.num_stages == len(self.num_heads) == len(self.num_groups)):\n+            raise ValueError(\n+                f\"Expected self.num_stages ({self.num_stages}) == \"\n+                f\"len(self.num_heads) ({len(self.num_heads)}) == \"\n+                f\"len(self.num_groups) ({len(self.num_groups)})\"\n+            )\n+\n+        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths) * 2, device=\"cpu\")]\n+        depth_offset = 0\n+\n+        convs = []\n+        blocks = []\n+        for stage_idx in range(self.num_stages):\n+            conv_embed = Florence2VisionConvEmbed(\n+                config=config,\n+                stage_idx=stage_idx,\n+            )\n+            convs.append(conv_embed)\n+\n+            block = nn.ModuleList(\n+                Florence2VisionBlock(\n+                    config=config,\n+                    stage_idx=stage_idx,\n+                    spatial_drop_path_rate=dpr[depth_offset + block_idx * 2],\n+                    channel_drop_path_rate=dpr[depth_offset + block_idx * 2 + 1],\n+                )\n+                for block_idx in range(config.depths[stage_idx])\n+            )\n+            blocks.append(block)\n+            depth_offset += config.depths[stage_idx] * 2\n+\n+        self.convs = nn.ModuleList(convs)\n+        self.blocks = nn.ModuleList(blocks)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        for conv, block in zip(self.convs, self.blocks):\n+            hidden_states = conv(hidden_states)\n+            for layer in block:\n+                hidden_states = layer(hidden_states)\n+        return hidden_states\n+\n+\n+class Florence2MultiModalProjector(nn.Module):\n+    def __init__(self, config: Florence2Config):\n+        super().__init__()\n+        self.vision_embedding_dim = config.vision_config.embed_dim[-1]\n+        self.vision_projection_dim = config.vision_config.projection_dim\n+        self.image_projection = nn.Linear(self.vision_embedding_dim, self.vision_projection_dim, bias=False)\n+        self.image_proj_norm = nn.LayerNorm(self.vision_projection_dim)\n+        self.image_position_embed = Florence2VisionLearnedAbsolutePositionEmbedding2D(config=config)\n+        self.visual_temporal_embed = Florence2VisionPositionalEmbeddingCosine1D(config=config)\n+\n+    def forward(self, image_features):\n+        position_features = image_features + self.image_position_embed(image_features)\n+        position_features = position_features.flatten(2).transpose(1, 2)\n+        temporal_features = self.visual_temporal_embed(position_features[:, :1, :])\n+        temporal_features = temporal_features.unsqueeze(1)\n+        visual_token_features = position_features + temporal_features\n+        visual_token_features = visual_token_features.unsqueeze(1)\n+        spatial_image_features = visual_token_features.mean(dim=2)\n+        temporal_image_features = visual_token_features.mean(dim=1)\n+        image_features = torch.cat([spatial_image_features, temporal_image_features], dim=1)\n+        image_features = self.image_projection(image_features)\n+        image_features = self.image_proj_norm(image_features)\n+        return image_features\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for Florence-2 base model's outputs that also contains : pre-computed hidden states that can speed up sequential\n+    decoding.\n+    \"\"\"\n+)\n+class Florence2Seq2SeqModelOutput(Seq2SeqModelOutput):\n+    r\"\"\"\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_image_tokens, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for Florence-2 model's outputs that also contains : pre-computed hidden states that can speed up sequential\n+    decoding.\n+    \"\"\"\n+)\n+class Florence2Seq2SeqLMOutput(Seq2SeqLMOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_image_tokens, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    image_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+\n+\n+@auto_docstring\n+class Florence2PreTrainedModel(LlavaPreTrainedModel):\n+    config_class = Florence2Config\n+\n+    _supports_attention_backend = False\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Florence-2 is a vision model for captioning, detection, and segmentation.\n+    \"\"\"\n+)\n+class Florence2Model(LlavaModel):\n+    _checkpoint_conversion_mapping = {}\n+    _tied_weights_keys = [\n+        \"language_model.encoder.embed_tokens.weight\",\n+        \"language_model.decoder.embed_tokens.weight\",\n+    ]\n+\n+    def __init__(self, config: Florence2Config):\n+        super().__init__(config)\n+        self.vision_tower = Florence2VisionBackbone(config=config.vision_config)\n+\n+    def get_encoder(self):\n+        return self.language_model.get_encoder()\n+\n+    def get_decoder(self):\n+        return self.language_model.get_decoder()\n+\n+    def get_image_features(self, pixel_values: torch.Tensor, **kwargs):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`):\n+               The tensors corresponding to the input images.\n+        Returns:\n+            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n+        \"\"\"\n+        image_features = self.vision_tower(pixel_values, **kwargs)\n+        image_embeds = self.multi_modal_projector(image_features)\n+        return image_embeds\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.LongTensor] = None,\n+        decoder_head_mask: Optional[torch.Tensor] = None,\n+        cross_attn_head_mask: Optional[torch.Tensor] = None,\n+        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n+        encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[tuple, Florence2Seq2SeqModelOutput]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if encoder_outputs is None:\n+            if (input_ids is None) ^ (inputs_embeds is not None):\n+                raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+            if inputs_embeds is None:\n+                inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+            if pixel_values is not None:\n+                image_features = self.get_image_features(pixel_values)\n+                image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+                special_image_mask = self.get_placeholder_mask(\n+                    input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+                )\n+                inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+            encoder_outputs = self.language_model.encoder(\n+                attention_mask=attention_mask,\n+                head_mask=head_mask,\n+                inputs_embeds=inputs_embeds,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+                return_dict=True,\n+            )\n+\n+        if decoder_input_ids is None:\n+            decoder_start_token_id = self.config.text_config.decoder_start_token_id\n+            decoder_input_ids = torch.ones((inputs_embeds.size()[0], 1), dtype=torch.long, device=inputs_embeds.device)\n+            decoder_input_ids *= decoder_start_token_id\n+\n+        decoder_outputs = self.language_model.decoder(\n+            input_ids=decoder_input_ids,\n+            attention_mask=decoder_attention_mask,\n+            encoder_hidden_states=encoder_outputs[0],\n+            encoder_attention_mask=attention_mask,\n+            head_mask=decoder_head_mask,\n+            cross_attn_head_mask=cross_attn_head_mask,\n+            past_key_values=past_key_values,\n+            inputs_embeds=decoder_inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+\n+        return Florence2Seq2SeqModelOutput(\n+            last_hidden_state=decoder_outputs.last_hidden_state,\n+            past_key_values=decoder_outputs.past_key_values,\n+            decoder_hidden_states=decoder_outputs.hidden_states,\n+            decoder_attentions=decoder_outputs.attentions,\n+            cross_attentions=decoder_outputs.cross_attentions,\n+            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n+            encoder_hidden_states=encoder_outputs.hidden_states,\n+            encoder_attentions=encoder_outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Florence-2 is a vision model for captioning, detection, and segmentation.\n+    \"\"\"\n+)\n+class Florence2ForConditionalGeneration(LlavaForConditionalGeneration):\n+    _checkpoint_conversion_mapping = {}\n+    _tied_weights_keys = [\n+        \"model.language_model.encoder.embed_tokens.weight\",\n+        \"model.language_model.decoder.embed_tokens.weight\",\n+        \"lm_head.weight\",\n+    ]\n+\n+    def get_encoder(self):\n+        return self.model.get_encoder()\n+\n+    def get_image_features(self, pixel_values: torch.Tensor, **kwargs):\n+        return self.model.get_image_features(pixel_values=pixel_values, **kwargs)\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.LongTensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        decoder_head_mask: Optional[torch.Tensor] = None,\n+        cross_attn_head_mask: Optional[torch.Tensor] = None,\n+        encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, Florence2Seq2SeqLMOutput]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, Florence2ForConditionalGeneration\n+\n+        >>> model = Florence2ForConditionalGeneration.from_pretrained(\"microsoft/Florence-2-large\")\n+        >>> processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-large\")\n+\n+        >>> prompt = \"<CAPTION>\"\n+        >>> url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(**inputs, max_length=100)\n+        >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"A green car parked in front of a yellow building.\"\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            attention_mask=attention_mask,\n+            decoder_input_ids=decoder_input_ids,\n+            encoder_outputs=encoder_outputs,\n+            decoder_attention_mask=decoder_attention_mask,\n+            head_mask=head_mask,\n+            decoder_head_mask=decoder_head_mask,\n+            cross_attn_head_mask=cross_attn_head_mask,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            decoder_inputs_embeds=decoder_inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n+\n+        return Florence2Seq2SeqLMOutput(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            decoder_hidden_states=outputs.decoder_hidden_states,\n+            decoder_attentions=outputs.decoder_attentions,\n+            cross_attentions=outputs.cross_attentions,\n+            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n+            encoder_hidden_states=outputs.encoder_hidden_states,\n+            encoder_attentions=outputs.encoder_attentions,\n+            image_hidden_states=outputs.image_hidden_states,\n+        )\n+\n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        return self.model.get_placeholder_mask(\n+            input_ids=input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+        )\n+\n+    def _prepare_encoder_decoder_kwargs_for_generation(\n+        self,\n+        inputs_tensor: torch.Tensor,\n+        model_kwargs,\n+        model_input_name: Optional[str],\n+        generation_config,\n+    ) -> dict[str, Any]:\n+        # override to handle merging image and text embeddings before passing to language encoder\n+        inputs_embeds = model_kwargs.pop(\"inputs_embeds\", None)\n+        pixel_values = model_kwargs.pop(\"pixel_values\", None)\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(inputs_tensor)\n+\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(pixel_values)\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask = self.get_placeholder_mask(\n+                inputs_tensor, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        model_kwargs[\"inputs_embeds\"] = inputs_embeds\n+        model_kwargs = super()._prepare_encoder_decoder_kwargs_for_generation(\n+            None, model_kwargs, model_input_name, generation_config\n+        )\n+        model_kwargs.pop(\"inputs_embeds\", None)\n+        return model_kwargs\n+\n+\n+__all__ = [\n+    \"Florence2Config\",\n+    \"Florence2Processor\",\n+    \"Florence2VisionConfig\",\n+    \"Florence2Model\",\n+    \"Florence2ForConditionalGeneration\",\n+    \"Florence2PreTrainedModel\",\n+    \"Florence2VisionBackbone\",\n+    \"Florence2VisionPreTrainedModel\",\n+]"
        },
        {
            "sha": "96dd81a68ab6db4b8f58aab94383be998298bc90",
            "filename": "src/transformers/models/florence2/processing_florence2.py",
            "status": "added",
            "additions": 803,
            "deletions": 0,
            "changes": 803,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca543f822f73ebc69b00835c74ae927d9730b6f5/src%2Ftransformers%2Fmodels%2Fflorence2%2Fprocessing_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca543f822f73ebc69b00835c74ae927d9730b6f5/src%2Ftransformers%2Fmodels%2Fflorence2%2Fprocessing_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fprocessing_florence2.py?ref=ca543f822f73ebc69b00835c74ae927d9730b6f5",
            "patch": "@@ -0,0 +1,803 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/florence2/modular_florence2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_florence2.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 Microsoft and the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import re\n+from typing import Any, Optional, Union\n+\n+import numpy as np\n+\n+from ...feature_extraction_utils import BatchFeature\n+from ...image_utils import ImageInput\n+from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import is_torch_available, logging\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Florence2ProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\"padding\": False, \"return_mm_token_type_ids\": False},\n+        \"images_kwargs\": {},\n+    }\n+\n+\n+class Florence2Processor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a Florence2 processor which wraps a Florence2 image processor and a Florence2 tokenizer into a single processor.\n+\n+    [`Florence2Processor`] offers all the functionalities of [`AutoImageProcessor`] and [`BartTokenizerFast`]. See the\n+    [`~Florence2Processor.__call__`] and [`~Florence2Processor.decode`] for more information.\n+\n+    Args:\n+        image_processor (`AutoImageProcessor`, *optional*):\n+            The image processor is a required input.\n+        tokenizer (`Union[BartTokenizer, BartTokenizerFast]`, *optional*):\n+            The tokenizer is a required input.\n+        num_additional_image_tokens (`int`, *optional*, defaults to 0):\n+            Number of additional tokens added to the image embeddings, such as CLS (+1). If the backbone has no CLS or other\n+            extra tokens appended, no need to set this arg.\n+        post_processor_config (`dict`,  *optional*, defaults to 0):\n+            Task-specific parsing rules for [`Florence2PostProcessor`], e.g. regex patterns,\n+            thresholds, or banned tokens.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"tokenizer\"]\n+    image_processor_class = \"AutoImageProcessor\"\n+    tokenizer_class = (\"BartTokenizer\", \"BartTokenizerFast\")\n+\n+    def __init__(\n+        self,\n+        image_processor=None,\n+        tokenizer=None,\n+        num_additional_image_tokens: int = 0,\n+        post_processor_config: Optional[dict] = None,\n+        **kwargs,\n+    ):\n+        self.tasks_answer_post_processing_type = {\n+            \"<OCR>\": \"pure_text\",\n+            \"<OCR_WITH_REGION>\": \"ocr\",\n+            \"<CAPTION>\": \"pure_text\",\n+            \"<DETAILED_CAPTION>\": \"pure_text\",\n+            \"<MORE_DETAILED_CAPTION>\": \"pure_text\",\n+            \"<OD>\": \"description_with_bboxes\",\n+            \"<DENSE_REGION_CAPTION>\": \"description_with_bboxes\",\n+            \"<CAPTION_TO_PHRASE_GROUNDING>\": \"phrase_grounding\",\n+            \"<REFERRING_EXPRESSION_SEGMENTATION>\": \"polygons\",\n+            \"<REGION_TO_SEGMENTATION>\": \"polygons\",\n+            \"<OPEN_VOCABULARY_DETECTION>\": \"description_with_bboxes_or_polygons\",\n+            \"<REGION_TO_CATEGORY>\": \"pure_text\",\n+            \"<REGION_TO_DESCRIPTION>\": \"pure_text\",\n+            \"<REGION_TO_OCR>\": \"pure_text\",\n+            \"<REGION_PROPOSAL>\": \"bboxes\",\n+        }\n+\n+        self.task_prompts_without_inputs = {\n+            \"<OCR>\": \"What is the text in the image?\",\n+            \"<OCR_WITH_REGION>\": \"What is the text in the image, with regions?\",\n+            \"<CAPTION>\": \"What does the image describe?\",\n+            \"<DETAILED_CAPTION>\": \"Describe in detail what is shown in the image.\",\n+            \"<MORE_DETAILED_CAPTION>\": \"Describe with a paragraph what is shown in the image.\",\n+            \"<OD>\": \"Locate the objects with category name in the image.\",\n+            \"<DENSE_REGION_CAPTION>\": \"Locate the objects in the image, with their descriptions.\",\n+            \"<REGION_PROPOSAL>\": \"Locate the region proposals in the image.\",\n+        }\n+\n+        self.task_prompts_with_input = {\n+            \"<CAPTION_TO_PHRASE_GROUNDING>\": \"Locate the phrases in the caption: {input}\",\n+            \"<REFERRING_EXPRESSION_SEGMENTATION>\": \"Locate {input} in the image with mask\",\n+            \"<REGION_TO_SEGMENTATION>\": \"What is the polygon mask of region {input}\",\n+            \"<OPEN_VOCABULARY_DETECTION>\": \"Locate {input} in the image.\",\n+            \"<REGION_TO_CATEGORY>\": \"What is the region {input}?\",\n+            \"<REGION_TO_DESCRIPTION>\": \"What does the region {input} describe?\",\n+            \"<REGION_TO_OCR>\": \"What text is in the region {input}?\",\n+        }\n+\n+        self.num_image_tokens = image_processor.image_seq_length\n+        self.num_additional_image_tokens = num_additional_image_tokens\n+        self.post_processor_config = post_processor_config\n+        self.post_processor = Florence2PostProcessor(config=post_processor_config, tokenizer=tokenizer)\n+        self.image_token = tokenizer.image_token\n+        self.image_token_id = tokenizer.image_token_id\n+\n+        super().__init__(image_processor, tokenizer, **kwargs)\n+\n+    def _construct_prompts(self, text: Union[str, list[str]]) -> list[str]:\n+        \"\"\"\n+        Construct prompts by replacing task tokens with corresponding prompt strings.\n+        \"\"\"\n+        if isinstance(text, str):\n+            text = [text]\n+\n+        prompts = []\n+        for prompt in text:\n+            # Check for tasks without inputs\n+            for task_token, task_prompt in self.task_prompts_without_inputs.items():\n+                if task_token in prompt:\n+                    if prompt != task_token:\n+                        raise ValueError(f\"Task token {task_token} should be the only content in the prompt.\")\n+                    prompt = task_prompt\n+                    break\n+            # Check for tasks with inputs\n+            for task_token, task_prompt in self.task_prompts_with_input.items():\n+                if task_token in prompt:\n+                    input_text = prompt.replace(task_token, \"\").strip()\n+                    prompt = task_prompt.format(input=input_text)\n+                    break\n+            prompts.append(prompt)\n+        return prompts\n+\n+    def __call__(\n+        self,\n+        images: Optional[ImageInput] = None,\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        **kwargs: Unpack[Florence2ProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n+        and `kwargs` arguments to BartTokenizerFast's [`~BartTokenizerFast.__call__`] if `text` is not `None` to encode\n+        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n+        of the above two methods for more information.\n+\n+        Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            text (`str`, `list[str]`, `list[list[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+                - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+              `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+        \"\"\"\n+        if images is None and text is None:\n+            raise ValueError(\"You have to specify at least one of `images` or `text`.\")\n+\n+        output_kwargs = self._merge_kwargs(\n+            Florence2ProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n+        image_inputs = {}\n+        if images is not None:\n+            image_inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n+\n+        if text is None:\n+            logger.warning_once(\"You are using Florence-2 without a text prefix.\")\n+            text = [\"\"] * (1 if not isinstance(images, list) else len(images))\n+        elif isinstance(text, str):\n+            text = [text]\n+\n+        if not isinstance(text, list) or not all(isinstance(token, str) for token in text):\n+            raise ValueError(\"`text` must be a string or list of strings.\")\n+\n+        if isinstance(images, list) and len(images) != len(text):\n+            raise ValueError(f\"Number of images ({len(images)}) must match number of texts ({len(text)}).\")\n+\n+        prompt_strings = self._construct_prompts(text)\n+\n+        # Add image tokens and special tokens if images are provided\n+        if image_inputs.get(\"pixel_values\") is not None:\n+            # Replace the image token with the expanded image token sequence\n+            expanded_image_prompts = []\n+            for sample in prompt_strings:\n+                sample = (\n+                    self.image_token * self.num_image_tokens\n+                    + self.tokenizer.bos_token\n+                    + sample\n+                    + self.tokenizer.eos_token\n+                )\n+                expanded_image_prompts.append(sample)\n+            prompt_strings = expanded_image_prompts\n+\n+        # Construct and tokenize prompts\n+        output_kwargs[\"text_kwargs\"].pop(\"add_special_tokens\", None)\n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", False)\n+        text_inputs = self.tokenizer(\n+            prompt_strings, **output_kwargs[\"text_kwargs\"], add_special_tokens=False, return_tensors=None\n+        )\n+        self._check_special_mm_tokens(prompt_strings, text_inputs, modalities=[\"image\"])\n+\n+        if return_mm_token_type_ids:\n+            array_ids = np.array(text_inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n+            mm_token_type_ids[array_ids == self.image_token_id] = 1\n+            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n+\n+        return BatchFeature(data={**image_inputs, **text_inputs}, tensor_type=return_tensors)\n+\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to BartTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to BartTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+\n+    def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+\n+        Args:\n+            image_sizes (`list[list[int]]`, *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+\n+        Returns:\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n+        \"\"\"\n+\n+        vision_data = {}\n+        if image_sizes is not None:\n+            num_image_tokens = [self.image_seq_length] * len(image_sizes)\n+            num_image_patches = [1] * len(image_sizes)\n+\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+\n+        return MultiModalData(**vision_data)\n+\n+    def post_process_image_text_to_text(self, generated_outputs, skip_special_tokens=False, **kwargs):\n+        \"\"\"\n+        Post-processes the output of `FuyuForConditionalGeneration` to only return the text output.\n+\n+        Args:\n+            generated_outputs (`torch.Tensor` or `np.ndarray`):\n+                The output of the model. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n+                containing the token ids of the generated sequences.\n+            skip_special_tokens (`bool`, *optional*, defaults to `False`):\n+                Whether or not to remove special tokens in the output. Argument passed to the tokenizer's `batch_decode` method.\n+            **kwargs:\n+                Additional arguments to be passed to the tokenizer's `batch_decode method`.\n+\n+        Returns:\n+            `list[str]`: The decoded text output.\n+        \"\"\"\n+        return self.batch_decode(generated_outputs, skip_special_tokens=skip_special_tokens, **kwargs)\n+\n+    def post_process_generation(self, text=None, sequence=None, task=None, image_size=None) -> dict[str, Any]:\n+        \"\"\"\n+        Post-process generation outputs based on the task.\n+\n+        Args:\n+            text (`str`, *optional*):\n+                Generated text.\n+            sequence (`Union[List[int], torch.Tensor]`, *optional*):\n+                Generated token sequence.\n+            task (`str`, *optional*):\n+                The task for post-processing.\n+            image_size (`Tuple[int, int]`, *optional*):\n+                Image size for dequantization.\n+\n+        Returns:\n+            `Dict[str, Any]`: Post-processed results keyed by task.\n+        \"\"\"\n+        if task is None:\n+            raise ValueError(\"`task` must be provided for post-processing.\")\n+\n+        post_proc_type = self.tasks_answer_post_processing_type.get(task, \"pure_text\")\n+        parsed = self.post_processor(\n+            text=text,\n+            sequence=sequence,\n+            image_size=image_size,\n+            parse_tasks=[post_proc_type],\n+        )[post_proc_type]\n+\n+        if post_proc_type == \"pure_text\":\n+            final_answer = parsed.replace(\"<s>\", \"\").replace(\"</s>\", \"\").strip()\n+        elif post_proc_type in [\"description_with_bboxes\", \"bboxes\"]:\n+            bboxes = [inst[\"bbox\"] for inst in parsed]\n+            labels = [inst[\"cat_name\"] for inst in parsed]\n+            final_answer = {\"bboxes\": bboxes, \"labels\": labels}\n+            if parsed and \"score\" in parsed[0]:\n+                final_answer[\"scores\"] = [inst[\"score\"] for inst in parsed]\n+        elif post_proc_type == \"ocr\":\n+            quad_boxes = [inst[\"quad_box\"] for inst in parsed]\n+            labels = [inst[\"text\"] for inst in parsed]\n+            final_answer = {\"quad_boxes\": quad_boxes, \"labels\": labels}\n+        elif post_proc_type == \"phrase_grounding\":\n+            bboxes = []\n+            labels = []\n+            for inst in parsed:\n+                for bbox in inst[\"bbox\"]:\n+                    bboxes.append(bbox)\n+                    labels.append(inst[\"cat_name\"])\n+            final_answer = {\"bboxes\": bboxes, \"labels\": labels}\n+        elif post_proc_type in [\"description_with_polygons\", \"polygons\"]:\n+            polygons = [inst[\"polygons\"] for inst in parsed]\n+            labels = [inst[\"cat_name\"] for inst in parsed]\n+            final_answer = {\"polygons\": polygons, \"labels\": labels}\n+        elif post_proc_type == \"description_with_bboxes_or_polygons\":\n+            bboxes = []\n+            bboxes_labels = []\n+            polygons = []\n+            polygons_labels = []\n+            for inst in parsed:\n+                label = inst[\"cat_name\"]\n+                if \"polygons\" in inst:\n+                    polygons.append(inst[\"polygons\"])\n+                    polygons_labels.append(label)\n+                else:\n+                    bboxes.append(inst[\"bbox\"])\n+                    bboxes_labels.append(label)\n+            final_answer = {\n+                \"bboxes\": bboxes,\n+                \"bboxes_labels\": bboxes_labels,\n+                \"polygons\": polygons,\n+                \"polygons_labels\": polygons_labels,\n+            }\n+        else:\n+            raise ValueError(f\"Unknown post-processing type: {post_proc_type}\")\n+\n+        return {task: final_answer}\n+\n+\n+class Florence2PostProcessor:\n+    \"\"\"\n+    Post-processor for Florence-2 model outputs. Parses generated text into structured results for various tasks\n+    like object detection, OCR, phrase grounding, etc.\n+\n+    Args:\n+        tokenizer (`PreTrainedTokenizer`):\n+            The tokenizer used for decoding model outputs.\n+    \"\"\"\n+\n+    def __init__(self, config, tokenizer):\n+        self.tokenizer = tokenizer\n+        self.parse_task_config = config or {}\n+        self.banned_grounding_tokens = set(\n+            self.parse_task_config.get(\"phrase_grounding\", {}).get(\"banned_grounding_tokens\", [])\n+        )\n+        self.all_special_tokens = set(self.tokenizer.all_special_tokens)\n+        self.quantize_bins = (1000, 1000)\n+\n+    def quantize(self, locations: \"torch.Tensor\", size: tuple[int, int]) -> \"torch.Tensor\":\n+        \"\"\"\n+        Quantize locations.\n+\n+        Args:\n+            locations (`torch.Tensor`):\n+                Tensor of shape (N, 4) for boxes or (N, 2) for points/coordinates.\n+            size (`tuple[int, int]`):\n+                Original image size (width, height).\n+\n+        Returns:\n+            `torch.Tensor`: Quantized locations as integers.\n+        \"\"\"\n+        bins_w, bins_h = self.quantize_bins\n+        size_w, size_h = size\n+        per_bin_w = size_w / bins_w\n+        per_bin_h = size_h / bins_h\n+\n+        if locations.shape[-1] == 4:  # Bounding boxes: [xmin, ymin, xmax, ymax]\n+            xmin, ymin, xmax, ymax = locations.split(1, dim=-1)\n+            q_xmin = (xmin / per_bin_w).floor().clamp(0, bins_w - 1)\n+            q_ymin = (ymin / per_bin_h).floor().clamp(0, bins_h - 1)\n+            q_xmax = (xmax / per_bin_w).floor().clamp(0, bins_w - 1)\n+            q_ymax = (ymax / per_bin_h).floor().clamp(0, bins_h - 1)\n+            return torch.cat([q_xmin, q_ymin, q_xmax, q_ymax], dim=-1).int()\n+\n+        elif locations.shape[-1] == 2:  # Points/coordinates: [x, y]\n+            x, y = locations.split(1, dim=-1)\n+            q_x = (x / per_bin_w).floor().clamp(0, bins_w - 1)\n+            q_y = (y / per_bin_h).floor().clamp(0, bins_h - 1)\n+            return torch.cat([q_x, q_y], dim=-1).int()\n+\n+        else:\n+            raise ValueError(f\"Unsupported location shape: last dim must be 2 or 4, got {locations.shape[-1]}.\")\n+\n+    def dequantize(self, locations: \"torch.Tensor\", size: tuple[int, int]) -> \"torch.Tensor\":\n+        \"\"\"\n+        Dequantize locations back to original scale.\n+\n+        Args:\n+            locations (`torch.Tensor`):\n+                Quantized tensor of shape (N, 4) for boxes or (N, 2) for points/coordinates.\n+            size (`tuple[int, int]`):\n+                Original image size (width, height).\n+\n+        Returns:\n+            `torch.Tensor`: Dequantized locations as floats.\n+        \"\"\"\n+        bins_w, bins_h = self.quantize_bins\n+        size_w, size_h = size\n+        per_bin_w = size_w / bins_w\n+        per_bin_h = size_h / bins_h\n+\n+        # Add 0.5 to use the center position of the bin as the coordinate.\n+        if locations.shape[-1] == 4:  # Bounding boxes\n+            xmin, ymin, xmax, ymax = locations.split(1, dim=-1)\n+            dq_xmin = (xmin + 0.5) * per_bin_w\n+            dq_ymin = (ymin + 0.5) * per_bin_h\n+            dq_xmax = (xmax + 0.5) * per_bin_w\n+            dq_ymax = (ymax + 0.5) * per_bin_h\n+            return torch.cat([dq_xmin, dq_ymin, dq_xmax, dq_ymax], dim=-1).int()\n+\n+        elif locations.shape[-1] == 2:  # Points/coordinates\n+            x, y = locations.split(1, dim=-1)\n+            dq_x = (x + 0.5) * per_bin_w\n+            dq_y = (y + 0.5) * per_bin_h\n+            return torch.cat([dq_x, dq_y], dim=-1).int()\n+\n+        else:\n+            raise ValueError(f\"Unsupported location shape: last dim must be 2 or 4, got {locations.shape[-1]}.\")\n+\n+    def decode_with_spans(self, token_ids: list[int]) -> tuple[str, list[tuple[int, int]]]:\n+        \"\"\"\n+        Decode token IDs to text and compute character spans.\n+\n+        Args:\n+            token_ids (`list[int]`):\n+                list of token IDs to decode.\n+\n+        Returns:\n+            `tuple[str, list[tuple[int, int]]]`: Decoded text and list of spans (start, end) for each token.\n+        \"\"\"\n+        filtered_tokens = self.tokenizer.convert_ids_to_tokens(token_ids, skip_special_tokens=False)\n+        text = \"\"\n+        spans = []\n+        for token in filtered_tokens:\n+            if token in self.all_special_tokens:\n+                sub_text = token\n+            else:\n+                sub_text = self.tokenizer.convert_tokens_to_string([token])\n+            span = (len(text), len(text) + len(sub_text))\n+            text += sub_text\n+            spans.append(span)\n+        return text, spans\n+\n+    def parse_ocr_from_text_and_spans(\n+        self, text: str, pattern: Optional[str], image_size: tuple[int, int], area_threshold: float = 0.0\n+    ) -> list[dict[str, Any]]:\n+        \"\"\"\n+        Parse OCR results with quadrilateral boxes.\n+\n+        Args:\n+            text (`str`):\n+                The generated text.\n+            pattern (`str`):\n+                Regex pattern for matching.\n+            image_size (`tuple[int, int]`):\n+                Image size (width, height).\n+            area_threshold (`float`, *optional*, defaults to 0.0):\n+                Minimum area threshold for filtering boxes.\n+\n+        Returns:\n+            `list[dict[str, Any]]`: list of instances with 'quad_box' and 'text'.\n+        \"\"\"\n+        text = text.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<pad>\", \"\")\n+        if pattern is None:\n+            pattern = r\"(.+?)<loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)>\"\n+\n+        matches = re.findall(pattern, text)\n+        instances = []\n+        width, height = image_size\n+\n+        for content, *quad_str in matches:\n+            quad_bins = [int(i) for i in quad_str]\n+            quad_box = self.dequantize(torch.tensor(quad_bins).reshape(-1, 2), size=image_size).flatten().tolist()\n+\n+            if area_threshold > 0:\n+                x_coords = quad_box[0::2]\n+                y_coords = quad_box[1::2]\n+                # Apply the Shoelace formula\n+                area = 0.5 * abs(\n+                    sum(x_coords[i] * y_coords[i + 1] - x_coords[i + 1] * y_coords[i] for i in range(4 - 1))\n+                )\n+\n+                if area < (width * height) * area_threshold:\n+                    continue\n+\n+            instances.append({\"quad_box\": quad_box, \"text\": content.strip()})\n+        return instances\n+\n+    def parse_phrase_grounding_from_text_and_spans(\n+        self, text: str, image_size: tuple[int, int]\n+    ) -> list[dict[str, Any]]:\n+        \"\"\"\n+        Parse phrase grounding results.\n+\n+        Args:\n+            text (`str`):\n+                The generated text.\n+            image_size (`tuple[int, int]`):\n+                Image size (width, height).\n+\n+        Returns:\n+            `list[dict[str, Any]]`: list of instances with 'bbox' and 'cat_name'.\n+        \"\"\"\n+        text = text.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<pad>\", \"\")\n+        phrase_pattern = r\"([^<]+(?:<loc_\\d+>){4,})\"\n+        phrases = re.findall(phrase_pattern, text)\n+        text_pattern = r\"^\\s*(.*?)(?=<od>|</od>|<box>|</box>|<bbox>|</bbox>|<loc_)\"\n+        box_pattern = r\"<loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)>\"\n+\n+        instances = []\n+        for phrase_text in phrases:\n+            phrase_text = phrase_text.replace(\"<ground>\", \"\", 1).replace(\"<obj>\", \"\", 1)\n+            if not phrase_text:\n+                continue\n+            match = re.search(text_pattern, phrase_text)\n+            if not match:\n+                continue\n+            phrase = match.group().strip()\n+            if phrase in self.banned_grounding_tokens:\n+                continue\n+            boxes_matches = list(re.finditer(box_pattern, phrase_text))\n+            if not boxes_matches:\n+                continue\n+            bbox_bins = [[int(m.group(j)) for j in range(1, 5)] for m in boxes_matches]\n+            bboxes = self.dequantize(torch.tensor(bbox_bins), size=image_size).tolist()\n+            phrase = phrase.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n+            instances.append({\"bbox\": bboxes, \"cat_name\": phrase})\n+        return instances\n+\n+    def _find_matched_token_indices(self, cur_span: tuple[int, int], token_spans: list[tuple[int, int]]) -> list[int]:\n+        return [i for i, span in enumerate(token_spans) if not (span[1] <= cur_span[0] or span[0] >= cur_span[1])]\n+\n+    def parse_description_with_bboxes_from_text_and_spans(\n+        self,\n+        text: str,\n+        image_size: tuple[int, int],\n+        allow_empty_phrase: bool = False,\n+    ) -> list[dict[str, Any]]:\n+        \"\"\"\n+        Parse descriptions with bounding boxes.\n+\n+        Args:\n+            text (`str`):\n+                The generated text.\n+            image_size (`tuple[int, int]`):\n+                Image size (width, height).\n+            allow_empty_phrase (`bool`, *optional*, defaults to `False`):\n+                Allow phrases without text.\n+\n+        Returns:\n+            `list[dict[str, Any]]`: list of instances with 'bbox', 'cat_name', and optional 'score'.\n+        \"\"\"\n+        text = text.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<pad>\", \"\")\n+\n+        if allow_empty_phrase:\n+            pattern = r\"(?:(?:<loc_\\d+>){4,})\"\n+        else:\n+            pattern = r\"([^<]+(?:<loc_\\d+>){4,})\"\n+        phrases = re.findall(pattern, text)\n+\n+        text_pattern = r\"^\\s*(.*?)(?=<od>|</od>|<box>|</box>|<bbox>|</bbox>|<loc_)\"\n+        box_pattern = r\"<loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)>\"\n+\n+        instances = []\n+        for phrase_text in phrases:\n+            phrase_text = phrase_text.replace(\"<ground>\", \"\", 1).replace(\"<obj>\", \"\", 1)\n+            if not phrase_text and not allow_empty_phrase:\n+                continue\n+            match = re.search(text_pattern, phrase_text)\n+            if not match:\n+                continue\n+            phrase = match.group().strip()\n+            boxes_matches = list(re.finditer(box_pattern, phrase_text))\n+            if not boxes_matches:\n+                continue\n+            bbox_bins = [[int(m.group(j)) for j in range(1, 5)] for m in boxes_matches]\n+            bboxes = self.dequantize(torch.tensor(bbox_bins), size=image_size).tolist()\n+\n+            phrase = phrase.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n+            for bbox in bboxes:\n+                instance = {\"bbox\": bbox, \"cat_name\": phrase}\n+                instances.append(instance)\n+\n+        return instances\n+\n+    def parse_description_with_polygons_from_text_and_spans(\n+        self,\n+        text: str,\n+        image_size: tuple[int, int],\n+        allow_empty_phrase: bool = False,\n+        polygon_sep_token: str = \"<sep>\",\n+        polygon_start_token: str = \"<poly>\",\n+        polygon_end_token: str = \"</poly>\",\n+        with_box_at_start: bool = False,\n+    ) -> list[dict[str, Any]]:\n+        \"\"\"\n+        Parse descriptions with polygons.\n+\n+        Args:\n+            text (`str`):\n+                The generated text.\n+            image_size (`tuple[int, int]`):\n+                Image size (width, height).\n+            allow_empty_phrase (`bool`, *optional*, defaults to `False`):\n+                Allow phrases without text.\n+            polygon_sep_token (`str`, *optional*, defaults to \"<sep>\"):\n+                Token separating polygons.\n+            polygon_start_token (`str`, *optional*, defaults to \"<poly>\"):\n+                Start token for polygons.\n+            polygon_end_token (`str`, *optional*, defaults to \"</poly>\"):\n+                End token for polygons.\n+            with_box_at_start (`bool`, *optional*, defaults to `False`):\n+                Whether a bounding box is at the start of polygons.\n+\n+        Returns:\n+            `list[dict[str, Any]]`: list of instances with 'polygons', 'cat_name', and optional 'bbox'.\n+        \"\"\"\n+        text = text.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<pad>\", \"\")\n+\n+        if allow_empty_phrase:\n+            pattern = rf\"(?:(?:<loc_\\d+>|{re.escape(polygon_sep_token)}|{re.escape(polygon_start_token)}|{re.escape(polygon_end_token)}){{4,}})\"\n+        else:\n+            pattern = rf\"([^<]+(?:<loc_\\d+>|{re.escape(polygon_sep_token)}|{re.escape(polygon_start_token)}|{re.escape(polygon_end_token)}){{4,}})\"\n+        phrases = re.findall(pattern, text)\n+        phrase_pattern = r\"^\\s*(.*?)(?=<od>|</od>|<box>|</box>|<bbox>|</bbox>|<loc_|<poly>)\"\n+        poly_instance_pattern = rf\"{re.escape(polygon_start_token)}(.*?){re.escape(polygon_end_token)}\"\n+        box_pattern = rf\"((?:<loc_\\d+>)+)(?:{re.escape(polygon_sep_token)}|$)\"\n+\n+        instances = []\n+        for phrase_text in phrases:\n+            phrase_text_strip = re.sub(r\"^<loc_\\d+>\", \"\", phrase_text, count=1)\n+            if not phrase_text_strip and not allow_empty_phrase:\n+                continue\n+            match = re.search(phrase_pattern, phrase_text_strip)\n+            if not match:\n+                continue\n+            phrase = match.group().strip()\n+\n+            if polygon_start_token in phrase_text and polygon_end_token in phrase_text:\n+                poly_instances = [m.group(1) for m in re.finditer(poly_instance_pattern, phrase_text)]\n+            else:\n+                poly_instances = [phrase_text]\n+\n+            for poly_inst in poly_instances:\n+                poly_matches = list(re.finditer(box_pattern, poly_inst))\n+                if len(poly_matches) == 0:\n+                    continue\n+                bbox = []\n+                polygons = []\n+                for poly_match in poly_matches:\n+                    poly_str = poly_match.group(1)\n+                    poly_bins = [int(m.group(1)) for m in re.finditer(r\"<loc_(\\d+)>\", poly_str)]\n+                    if with_box_at_start and not bbox:\n+                        if len(poly_bins) > 4:\n+                            bbox = poly_bins[:4]\n+                            poly_bins = poly_bins[4:]\n+                        else:\n+                            bbox = [0, 0, 0, 0]\n+                    if len(poly_bins) % 2 == 1:\n+                        poly_bins = poly_bins[:-1]\n+                    poly_coords = (\n+                        self.dequantize(torch.tensor(poly_bins).reshape(-1, 2), size=image_size).flatten().tolist()\n+                    )\n+                    polygons.append(poly_coords)\n+\n+                instance = {\"cat_name\": phrase, \"polygons\": polygons}\n+                if bbox:\n+                    instance[\"bbox\"] = self.dequantize(torch.tensor([bbox]), size=image_size)[0].tolist()\n+                instances.append(instance)\n+        return instances\n+\n+    def __call__(self, text=None, sequence=None, image_size=None, parse_tasks=None) -> dict[str, Any]:\n+        \"\"\"\n+        Process model output and parse into task-specific results.\n+\n+        Args:\n+            text (`Optional[str]`, *optional*):\n+                Generated text. Either this or `sequence` must be provided.\n+            sequence (`Optional[Union[list[int], torch.Tensor]]`, *optional*):\n+                Token sequence. Either this or `text` must be provided.\n+            image_size (`Optional[tuple[int, int]]`, *optional*):\n+                Image size (width, height) required for dequantization.\n+            parse_tasks (`Optional[Union[str, list[str]]]`, *optional*):\n+                Specific tasks to parse. If None, parse all supported tasks.\n+\n+        Returns:\n+            `dict[str, Any]`: Parsed results for each task, including the raw 'text'.\n+        \"\"\"\n+        if parse_tasks is not None:\n+            parse_tasks = [parse_tasks] if isinstance(parse_tasks, str) else parse_tasks\n+            for task in parse_tasks:\n+                if task not in self.parse_task_config.keys():\n+                    raise ValueError(f\"Unsupported parse task: {task}\")\n+\n+        if (text is None and sequence is None) or (text is not None and sequence is not None):\n+            raise ValueError(\"Exactly one of 'text' or 'sequence' must be provided.\")\n+\n+        if sequence is not None:\n+            if isinstance(sequence, torch.Tensor):\n+                sequence = sequence.tolist()\n+            sequence = sequence[1:] if sequence[0] == self.tokenizer.bos_token_id else sequence  # Skip BOS if present\n+            text, _ = self.decode_with_spans(sequence)\n+\n+        parsed_dict = {\"text\": text}\n+\n+        tasks_to_parse = parse_tasks or self.parse_task_config.keys()\n+        for task in tasks_to_parse:\n+            config = self.parse_task_config[task]\n+            pattern = config.get(\"PATTERN\")\n+\n+            if task == \"ocr\":\n+                parsed_dict[\"ocr\"] = self.parse_ocr_from_text_and_spans(\n+                    text, pattern=pattern, image_size=image_size, area_threshold=config.get(\"AREA_THRESHOLD\", 0.0)\n+                )\n+            elif task == \"phrase_grounding\":\n+                parsed_dict[\"phrase_grounding\"] = self.parse_phrase_grounding_from_text_and_spans(\n+                    text, image_size=image_size\n+                )\n+            elif task == \"pure_text\":\n+                parsed_dict[\"pure_text\"] = text\n+            elif task == \"description_with_bboxes\":\n+                parsed_dict[\"description_with_bboxes\"] = self.parse_description_with_bboxes_from_text_and_spans(\n+                    text, image_size=image_size\n+                )\n+            elif task == \"description_with_polygons\":\n+                parsed_dict[\"description_with_polygons\"] = self.parse_description_with_polygons_from_text_and_spans(\n+                    text, image_size=image_size\n+                )\n+            elif task == \"polygons\":\n+                parsed_dict[\"polygons\"] = self.parse_description_with_polygons_from_text_and_spans(\n+                    text, image_size=image_size, allow_empty_phrase=True\n+                )\n+            elif task == \"bboxes\":\n+                parsed_dict[\"bboxes\"] = self.parse_description_with_bboxes_from_text_and_spans(\n+                    text, image_size=image_size, allow_empty_phrase=True\n+                )\n+            elif task == \"description_with_bboxes_or_polygons\":\n+                if \"<poly>\" in text:\n+                    instances = self.parse_description_with_polygons_from_text_and_spans(text, image_size=image_size)\n+                else:\n+                    instances = self.parse_description_with_bboxes_from_text_and_spans(text, image_size=image_size)\n+                parsed_dict[\"description_with_bboxes_or_polygons\"] = instances\n+            else:\n+                raise ValueError(\"task {} is not supported\".format(task))\n+\n+        return parsed_dict\n+\n+\n+__all__ = [\"Florence2Processor\"]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/florence2/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca543f822f73ebc69b00835c74ae927d9730b6f5/tests%2Fmodels%2Fflorence2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca543f822f73ebc69b00835c74ae927d9730b6f5/tests%2Fmodels%2Fflorence2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflorence2%2F__init__.py?ref=ca543f822f73ebc69b00835c74ae927d9730b6f5"
        },
        {
            "sha": "d53b527aedae40405237ea45586c20b3bff94145",
            "filename": "tests/models/florence2/test_modeling_florence2.py",
            "status": "added",
            "additions": 557,
            "deletions": 0,
            "changes": 557,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca543f822f73ebc69b00835c74ae927d9730b6f5/tests%2Fmodels%2Fflorence2%2Ftest_modeling_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca543f822f73ebc69b00835c74ae927d9730b6f5/tests%2Fmodels%2Fflorence2%2Ftest_modeling_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflorence2%2Ftest_modeling_florence2.py?ref=ca543f822f73ebc69b00835c74ae927d9730b6f5",
            "patch": "@@ -0,0 +1,557 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Florence2 model.\"\"\"\n+\n+import unittest\n+\n+import requests\n+\n+from transformers import (\n+    AutoProcessor,\n+    Florence2Config,\n+    Florence2ForConditionalGeneration,\n+    Florence2Model,\n+    is_torch_available,\n+    is_vision_available,\n+)\n+from transformers.testing_utils import (\n+    cleanup,\n+    require_torch,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+\n+class Florence2VisionText2TextModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=13,\n+        num_channels=3,\n+        image_size=8,\n+        seq_length=13,\n+        encoder_seq_length=18,\n+        is_training=True,\n+        vocab_size=99,\n+        max_position_embeddings=64,\n+        encoder_layers=1,\n+        encoder_ffn_dim=8,\n+        decoder_layers=1,\n+        decoder_ffn_dim=8,\n+        num_attention_heads=1,\n+        d_model=8,\n+        activation_function=\"gelu\",\n+        dropout=0.1,\n+        eos_token_id=2,\n+        bos_token_id=0,\n+        pad_token_id=1,\n+        image_token_id=4,\n+        depths=[1],\n+        patch_size=[7],\n+        patch_stride=[4],\n+        patch_padding=[3],\n+        patch_prenorm=[False],\n+        embed_dim=[8],\n+        num_heads=[1],\n+        num_groups=[1],\n+        window_size=12,\n+        drop_path_rate=0.1,\n+        projection_dim=8,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.is_training = is_training\n+        self.num_hidden_layers = decoder_layers\n+        self.hidden_size = d_model\n+\n+        # Language model configs\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.encoder_layers = encoder_layers\n+        self.encoder_ffn_dim = encoder_ffn_dim\n+        self.decoder_layers = decoder_layers\n+        self.decoder_ffn_dim = decoder_ffn_dim\n+        self.num_attention_heads = num_attention_heads\n+        self.d_model = d_model\n+        self.activation_function = activation_function\n+        self.dropout = dropout\n+        self.eos_token_id = eos_token_id\n+        self.bos_token_id = bos_token_id\n+        self.pad_token_id = pad_token_id\n+        self.image_token_id = image_token_id\n+\n+        # Vision model configs\n+        self.drop_path_rate = drop_path_rate\n+        self.patch_size = patch_size\n+        self.depths = depths\n+        self.patch_stride = patch_stride\n+        self.patch_padding = patch_padding\n+        self.patch_prenorm = patch_prenorm\n+        self.embed_dim = embed_dim\n+        self.num_heads = num_heads\n+        self.num_groups = num_groups\n+        self.window_size = window_size\n+        self.projection_dim = projection_dim\n+\n+        self.num_channels = 3\n+        self.num_image_tokens = 5\n+        self.seq_length = seq_length + self.num_image_tokens\n+        self.encoder_seq_length = encoder_seq_length\n+\n+    def get_config(self):\n+        text_config = {\n+            \"model_type\": \"bart\",\n+            \"vocab_size\": self.vocab_size,\n+            \"max_position_embeddings\": self.max_position_embeddings,\n+            \"encoder_layers\": self.encoder_layers,\n+            \"encoder_ffn_dim\": self.encoder_ffn_dim,\n+            \"encoder_attention_heads\": self.num_attention_heads,\n+            \"decoder_layers\": self.decoder_layers,\n+            \"decoder_ffn_dim\": self.decoder_ffn_dim,\n+            \"decoder_attention_heads\": self.num_attention_heads,\n+            \"d_model\": self.d_model,\n+            \"activation_function\": self.activation_function,\n+            \"dropout\": self.dropout,\n+            \"attention_dropout\": self.dropout,\n+            \"activation_dropout\": self.dropout,\n+            \"eos_token_id\": self.eos_token_id,\n+            \"bos_token_id\": self.bos_token_id,\n+            \"pad_token_id\": self.pad_token_id,\n+        }\n+\n+        vision_config = {\n+            \"drop_path_rate\": self.drop_path_rate,\n+            \"patch_size\": self.patch_size,\n+            \"depths\": self.depths,\n+            \"patch_stride\": self.patch_stride,\n+            \"patch_padding\": self.patch_padding,\n+            \"patch_prenorm\": self.patch_prenorm,\n+            \"embed_dim\": self.embed_dim,\n+            \"num_heads\": self.num_heads,\n+            \"num_groups\": self.num_groups,\n+            \"window_size\": self.window_size,\n+            \"activation_function\": self.activation_function,\n+            \"projection_dim\": self.projection_dim,\n+        }\n+\n+        return Florence2Config(\n+            text_config=text_config,\n+            vision_config=vision_config,\n+            image_token_id=self.image_token_id,\n+            initializer_range=0.02,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor(\n+            [\n+                self.batch_size,\n+                self.num_channels,\n+                self.image_size,\n+                self.image_size,\n+            ]\n+        )\n+        input_ids = ids_tensor([self.batch_size, self.encoder_seq_length], self.vocab_size - 1) + 1\n+        input_ids[input_ids == self.image_token_id] = self.pad_token_id\n+        input_ids[:, : self.num_image_tokens] = self.image_token_id\n+        input_ids[:, -1] = self.eos_token_id\n+        decoder_input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+        decoder_attention_mask = decoder_input_ids.ne(self.pad_token_id)\n+\n+        inputs_dict = {\n+            \"input_ids\": input_ids,\n+            \"pixel_values\": pixel_values,\n+            \"decoder_input_ids\": decoder_input_ids,\n+            \"decoder_attention_mask\": decoder_attention_mask,\n+        }\n+\n+        config = self.get_config()\n+        return config, inputs_dict\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config, inputs_dict = self.prepare_config_and_inputs()\n+        return config, inputs_dict\n+\n+    def create_and_check_florence2_model_fp16_forward(self, config, input_ids, pixel_values, attention_mask):\n+        model = Florence2ForConditionalGeneration(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n+            logits = model(\n+                input_ids=input_ids,\n+                attention_mask=attention_mask,\n+                pixel_values=pixel_values.to(torch.float16),\n+                return_dict=True,\n+            )[\"logits\"]\n+        self.parent.assertFalse(torch.isnan(logits).any().item())\n+\n+    @unittest.skip(\n+        reason=\"This architecture (bart) has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n+    )\n+    def test_load_save_without_tied_weights(self):\n+        pass\n+\n+\n+@require_torch\n+class Florence2ForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Model tester for `Florence2ForConditionalGeneration`.\n+    \"\"\"\n+\n+    all_model_classes = (Florence2Model, Florence2ForConditionalGeneration) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\n+            \"image-to-text\": Florence2ForConditionalGeneration,\n+            \"image-text-to-text\": Florence2ForConditionalGeneration,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    test_pruning = False\n+    test_head_masking = False\n+    test_attention_outputs = False\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = Florence2VisionText2TextModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=Florence2Config, has_text_modality=False)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+\n+def prepare_img():\n+    url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg?download=true\"\n+    image = Image.open(requests.get(url, stream=True).raw)\n+    return image\n+\n+\n+@slow\n+@require_torch\n+class Florence2ForConditionalGenerationIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        self.image1 = Image.open(\n+            requests.get(\n+                \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg?download=true\",\n+                stream=True,\n+            ).raw\n+        )\n+        self.image2 = Image.open(\n+            requests.get(\n+                \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\",\n+                stream=True,\n+            ).raw\n+        )\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def test_base_model_inference_eager(self):\n+        model_name = \"ducviet00/Florence-2-base-hf\"\n+        processor = AutoProcessor.from_pretrained(model_name)\n+        model = Florence2ForConditionalGeneration.from_pretrained(model_name, attn_implementation=\"eager\").to(\n+            torch_device\n+        )\n+\n+        prompt = \"<DETAILED_CAPTION>\"\n+        inputs = processor(images=self.image1, text=prompt, return_tensors=\"pt\")\n+        inputs.to(device=torch_device)\n+\n+        EXPECTED_INPUT_IDS = [[processor.image_token_id] * processor.num_image_tokens + [0, 47066, 21700, 11, 4617, 99, 16, 2343, 11, 5, 2274, 4, 2]]  # fmt: skip\n+        self.assertEqual(inputs[\"input_ids\"].tolist(), EXPECTED_INPUT_IDS)\n+\n+        predictions = model.generate(**inputs, max_new_tokens=100)\n+\n+        EXPECTED_PREDICTION_IDS = [[2, 0, 133, 2274, 924, 10, 912, 1203, 2828, 15, 5, 526, 9, 10, 2014, 11, 35910, 6, 188, 469, 412, 4, 20, 2014, 16, 9321, 19, 3413, 6, 3980, 6, 8, 19638, 6, 8, 89, 32, 82, 3051, 15, 5, 2767, 22609, 4, 20, 6360, 16, 7097, 11, 5, 3618, 4, 2]]  # fmt: skip\n+        self.assertEqual(predictions.tolist(), EXPECTED_PREDICTION_IDS)\n+\n+        generated_text = processor.batch_decode(predictions, skip_special_tokens=True)[0]\n+\n+        EXPECTED_GENERATED_TEXT = \"The image shows a stop sign sitting on the side of a street in Chinatown, New York City. The street is lined with buildings, trees, and statues, and there are people walking on the footpath. The sky is visible in the background.\"  # fmt: skip\n+        self.assertEqual(generated_text, EXPECTED_GENERATED_TEXT)\n+\n+    def test_base_model_batching_inference_eager(self):\n+        model_name = \"ducviet00/Florence-2-base-hf\"\n+        processor = AutoProcessor.from_pretrained(model_name)\n+        model = Florence2ForConditionalGeneration.from_pretrained(model_name, attn_implementation=\"eager\").to(\n+            torch_device\n+        )\n+\n+        images = [self.image1, self.image2]\n+        prompts = [\"<REGION_PROPOSAL>\", \"<OPEN_VOCABULARY_DETECTION>wheels\"]\n+        inputs = processor(images=images, text=prompts, padding=\"longest\", return_tensors=\"pt\")\n+\n+        EXPECTED_INPUT_IDS = [\n+            [processor.image_token_id] * processor.num_image_tokens + [0, 574, 22486, 5, 976, 5327, 11, 5, 2274, 4, 2],\n+            [processor.image_token_id] * processor.num_image_tokens + [0, 574, 22486, 10562, 11, 5, 2274, 4, 2, 1, 1],\n+        ]\n+        self.assertEqual(inputs[\"input_ids\"].tolist(), EXPECTED_INPUT_IDS)\n+\n+        inputs.to(device=torch_device)\n+        predictions = model.generate(**inputs, do_sample=False, max_new_tokens=100)\n+\n+        EXPECTED_PREDICTION_IDS = [\n+            [2, 0, 50269, 50269, 51267, 50980, 50269, 50269, 50688, 50942, 50269, 50333, 50633, 50941, 51033, 50269, 51267, 50934, 50794, 50814, 51190, 51032, 50432, 50402, 50634, 50692, 50269, 50334, 50340, 50927, 51224, 50417, 51267, 50930, 51076, 50944, 51159, 51028, 50836, 50947, 50915, 51030, 2],\n+            [2, 0, 28884,  2507, 50413, 50839, 51139, 51047, 28884,  2507, 50980, 50842, 51135, 51043, 28884, 2507, 50417, 50848, 50573, 51043, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n+        ]  # fmt: skip\n+        self.assertEqual(predictions.tolist(), EXPECTED_PREDICTION_IDS)\n+\n+        generated_texts = processor.batch_decode(predictions, skip_special_tokens=False)\n+\n+        EXPECTED_GENERATED_TEXTS = [\n+            \"</s><s><loc_0><loc_0><loc_998><loc_711><loc_0><loc_0><loc_419><loc_673><loc_0><loc_64><loc_364><loc_672><loc_764><loc_0><loc_998><loc_665><loc_525><loc_545><loc_921><loc_763><loc_163><loc_133><loc_365><loc_423><loc_0><loc_65><loc_71><loc_658><loc_955><loc_148><loc_998><loc_661><loc_807><loc_675><loc_890><loc_759><loc_567><loc_678><loc_646><loc_761></s>\",\n+            \"</s><s>wheels<loc_144><loc_570><loc_870><loc_778>wheels<loc_711><loc_573><loc_866><loc_774>wheels<loc_148><loc_579><loc_304><loc_774></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\",\n+        ]\n+        self.assertEqual(generated_texts, EXPECTED_GENERATED_TEXTS)\n+\n+        parsed_answer_0 = processor.post_process_generation(\n+            generated_texts[0], task=\"<REGION_PROPOSAL>\", image_size=(images[0].width, images[0].height)\n+        )\n+        EXPECTED_PARSED_ANSWER_0 = {\"<REGION_PROPOSAL>\": {\"bboxes\": [[0, 0, 1298, 623], [0, 0, 545, 589], [0, 56, 473, 589], [993, 0, 1298, 582], [683, 477, 1197, 668], [212, 116, 475, 370], [0, 57, 92, 576], [1242, 130, 1298, 579], [1049, 591, 1157, 665], [737, 594, 840, 667]], \"labels\": [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]}}  # fmt: skip\n+\n+        self.assertEqual(parsed_answer_0, EXPECTED_PARSED_ANSWER_0)\n+\n+        parsed_answer_1 = processor.post_process_generation(\n+            generated_texts[1], task=\"<OPEN_VOCABULARY_DETECTION>\", image_size=(images[1].width, images[1].height)\n+        )\n+        EXPECTED_PARSED_ANSWER_1 = {\"<OPEN_VOCABULARY_DETECTION>\": {\"bboxes\": [[92, 273, 557, 373], [455, 275, 554, 371], [95, 278, 194, 371]], \"bboxes_labels\": [\"wheels\", \"wheels\", \"wheels\"], \"polygons\": [], \"polygons_labels\": []}}  # fmt: skip\n+\n+        self.assertEqual(parsed_answer_1, EXPECTED_PARSED_ANSWER_1)\n+\n+    def test_base_model_inference_sdpa(self):\n+        model_name = \"ducviet00/Florence-2-base-hf\"\n+        processor = AutoProcessor.from_pretrained(model_name)\n+        model = Florence2ForConditionalGeneration.from_pretrained(model_name, attn_implementation=\"sdpa\").to(\n+            torch_device\n+        )\n+\n+        prompt = \"<REFERRING_EXPRESSION_SEGMENTATION>a car\"\n+        inputs = processor(images=self.image2, text=prompt, return_tensors=\"pt\")\n+        inputs.to(device=torch_device)\n+\n+        EXPECTED_INPUT_IDS = [[processor.image_token_id] * processor.num_image_tokens + [0, 574, 22486, 10, 512, 11, 5, 2274, 19, 11445, 2]]  # fmt: skip\n+        self.assertEqual(inputs[\"input_ids\"].tolist(), EXPECTED_INPUT_IDS)\n+\n+        predictions = model.generate(**inputs, do_sample=False, max_new_tokens=100)\n+\n+        EXPECTED_PREDICTION_IDS = [[2, 0, 50548, 50648, 50551, 50648, 50559, 50641, 50562, 50641, 50567, 50637, 50570, 50637, 50575, 50633, 50579, 50631, 50584, 50629, 50589, 50627, 50593, 50624, 50600, 50622, 50606, 50620, 50612, 50618, 50618, 50616, 50625, 50614, 50634, 50612, 50645, 50610, 50659, 50608, 50678, 50606, 50758, 50606, 50783, 50608, 50797, 50610, 50808, 50612, 50816, 50614, 50822, 50616, 50828, 50618, 50835, 50620, 50841, 50622, 50847, 50624, 50853, 50629, 50858, 50635, 50861, 50641, 50864, 50648, 50867, 50654, 50870, 50660, 50872, 50666, 50875, 50670, 50877, 50677, 50880, 50683, 50883, 50689, 50886, 50695, 50889, 50702, 50895, 50710, 50900, 50714, 50905, 50716, 50908, 50720, 50908, 50725, 50911, 50729, 2]]  # fmt: skip\n+        self.assertEqual(predictions.tolist(), EXPECTED_PREDICTION_IDS)\n+\n+        generated_text = processor.batch_decode(predictions, skip_special_tokens=False)[0]\n+\n+        EXPECTED_GENERATED_TEXT = \"</s><s><loc_279><loc_379><loc_282><loc_379><loc_290><loc_372><loc_293><loc_372><loc_298><loc_368><loc_301><loc_368><loc_306><loc_364><loc_310><loc_362><loc_315><loc_360><loc_320><loc_358><loc_324><loc_355><loc_331><loc_353><loc_337><loc_351><loc_343><loc_349><loc_349><loc_347><loc_356><loc_345><loc_365><loc_343><loc_376><loc_341><loc_390><loc_339><loc_409><loc_337><loc_489><loc_337><loc_514><loc_339><loc_528><loc_341><loc_539><loc_343><loc_547><loc_345><loc_553><loc_347><loc_559><loc_349><loc_566><loc_351><loc_572><loc_353><loc_578><loc_355><loc_584><loc_360><loc_589><loc_366><loc_592><loc_372><loc_595><loc_379><loc_598><loc_385><loc_601><loc_391><loc_603><loc_397><loc_606><loc_401><loc_608><loc_408><loc_611><loc_414><loc_614><loc_420><loc_617><loc_426><loc_620><loc_433><loc_626><loc_441><loc_631><loc_445><loc_636><loc_447><loc_639><loc_451><loc_639><loc_456><loc_642><loc_460></s>\"  # fmt: skip\n+        self.assertEqual(generated_text, EXPECTED_GENERATED_TEXT)\n+\n+        parsed_answer = processor.post_process_generation(\n+            generated_text,\n+            task=\"<REFERRING_EXPRESSION_SEGMENTATION>\",\n+            image_size=(self.image2.width, self.image2.height),\n+        )\n+        EXPECTED_PARSED_ANSWER = {'<REFERRING_EXPRESSION_SEGMENTATION>': {'polygons': [[[178, 182, 180, 182, 185, 178, 187, 178, 191, 176, 192, 176, 196, 174, 198, 174, 201, 173, 205, 172, 207, 170, 212, 169, 216, 168, 219, 167, 223, 166, 228, 165, 233, 164, 240, 163, 249, 162, 262, 162, 313, 162, 329, 162, 338, 163, 345, 164, 350, 165, 354, 166, 358, 167, 362, 168, 366, 169, 370, 170, 374, 173, 377, 175, 379, 178, 381, 182, 383, 185, 384, 187, 386, 190, 388, 192, 389, 196, 391, 198, 393, 201, 395, 204, 397, 208, 400, 211, 404, 213, 407, 214, 409, 216, 409, 219, 411, 221]]], 'labels': ['']}}  # fmt: skip\n+        self.assertEqual(parsed_answer, EXPECTED_PARSED_ANSWER)\n+\n+    def test_base_model_batching_inference_sdpa(self):\n+        model_name = \"ducviet00/Florence-2-base-hf\"\n+        processor = AutoProcessor.from_pretrained(model_name)\n+        model = Florence2ForConditionalGeneration.from_pretrained(model_name, attn_implementation=\"sdpa\").to(\n+            torch_device\n+        )\n+\n+        images = [self.image1, self.image2]\n+        prompts = [\"<OCR>\", \"<OD>\"]\n+        inputs = processor(images=images, text=prompts, padding=\"longest\", return_tensors=\"pt\")\n+\n+        EXPECTED_INPUT_IDS = [\n+            [processor.image_token_id] * processor.num_image_tokens + [0, 2264, 16, 5, 2788, 11, 5, 2274, 116, 2, 1, 1, 1],\n+            [processor.image_token_id] * processor.num_image_tokens + [0, 574, 22486, 5, 8720, 19, 4120, 766, 11, 5, 2274, 4, 2],\n+        ]  # fmt: skip\n+        self.assertEqual(inputs[\"input_ids\"].tolist(), EXPECTED_INPUT_IDS)\n+\n+        inputs.to(device=torch_device)\n+        predictions = model.generate(**inputs, do_sample=False, max_new_tokens=100)\n+\n+        EXPECTED_PREDICTION_IDS = [\n+            [2, 0, 47643, 47240, 6382, 47643, 7405, 495, 211, 2571, 4014, 5733, 36714, 11582, 11582, 36714, 18164, 9357, 36714, 6248, 3602, 37127, 27969, 7471, 44636, 23171, 41907, 27, 16948, 45895, 11582, 45262, 18537, 530, 791, 384, 229, 791, 5733, 565, 3048, 673, 10932, 5733, 565, 11120, 673, 2],\n+            [2, 0, 5901, 50322, 50602, 51202, 51043, 11219, 3679, 50694, 50772, 50743, 50784, 13630, 50978, 50845, 51134, 51041, 50419, 50853, 50578, 51042, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n+        ]  # fmt: skip\n+        self.assertEqual(predictions.tolist(), EXPECTED_PREDICTION_IDS)\n+\n+        generated_texts = processor.batch_decode(predictions, skip_special_tokens=False)\n+\n+        EXPECTED_GENERATED_TEXTS = [\n+            \"</s><s>ä¸­æ–‡ä¸­BBD DATSTOPç¬¬ç¦ç§‘æŠ€æœ‰é™å…¬å¸KU O KUOPTUSOyesOPTUSTO</s>\",\n+            \"</s><s>car<loc_53><loc_333><loc_933><loc_774>door handle<loc_425><loc_503><loc_474><loc_515>wheel<loc_709><loc_576><loc_865><loc_772><loc_150><loc_584><loc_309><loc_773></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\",\n+        ]  # fmt: skip\n+        self.assertEqual(generated_texts, EXPECTED_GENERATED_TEXTS)\n+\n+        parsed_answer = processor.post_process_generation(\n+            generated_texts[1], task=\"<OD>\", image_size=(images[1].width, images[1].height)\n+        )\n+        EXPECTED_PARSED_ANSWER = {'<OD>': {'bboxes': [[34, 160, 597, 371], [272, 241, 303, 247], [454, 276, 553, 370], [96, 280, 198, 371]], 'labels': ['car', 'door handle', 'wheel', 'wheel']}}  # fmt: skip\n+        self.assertEqual(parsed_answer, EXPECTED_PARSED_ANSWER)\n+\n+    def test_large_model_inference_eager(self):\n+        model_name = \"ducviet00/Florence-2-large-hf\"\n+        processor = AutoProcessor.from_pretrained(model_name)\n+        model = Florence2ForConditionalGeneration.from_pretrained(model_name, attn_implementation=\"eager\").to(\n+            torch_device\n+        )\n+\n+        prompt = \"<DETAILED_CAPTION>\"\n+        inputs = processor(images=self.image1, text=prompt, return_tensors=\"pt\")\n+        inputs.to(device=torch_device)\n+\n+        EXPECTED_INPUT_IDS = [[processor.image_token_id] * processor.num_image_tokens + [0, 47066, 21700, 11, 4617, 99, 16, 2343, 11, 5, 2274, 4, 2]]  # fmt: skip\n+        self.assertEqual(inputs[\"input_ids\"].tolist(), EXPECTED_INPUT_IDS)\n+\n+        predictions = model.generate(**inputs, do_sample=False, max_new_tokens=100)\n+\n+        EXPECTED_PREDICTION_IDS = [[2, 0, 133, 2274, 924, 10, 909, 512, 1428, 159, 10, 2014, 9321, 19, 6764, 3413, 4, 96, 5, 39299, 6, 89, 16, 10, 1275, 912, 1203, 2828, 15, 5, 526, 9, 5, 921, 6, 8, 11, 5, 3618, 6, 89, 32, 1104, 19638, 6, 3980, 6, 8, 10, 699, 2440, 6360, 4, 2]]  # fmt: skip\n+        self.assertEqual(predictions.tolist(), EXPECTED_PREDICTION_IDS)\n+\n+        generated_text = processor.batch_decode(predictions, skip_special_tokens=True)[0]\n+\n+        EXPECTED_GENERATED_TEXT = \"The image shows a black car driving down a street lined with tall buildings. In the foreground, there is a red stop sign sitting on the side of the road, and in the background, there are white statues, trees, and a clear blue sky.\"  # fmt: skip\n+        self.assertEqual(generated_text, EXPECTED_GENERATED_TEXT)\n+\n+    def test_large_model_batching_inference_eager(self):\n+        model_name = \"ducviet00/Florence-2-large-hf\"\n+        processor = AutoProcessor.from_pretrained(model_name)\n+        model = Florence2ForConditionalGeneration.from_pretrained(model_name, attn_implementation=\"eager\").to(\n+            torch_device\n+        )\n+\n+        images = [self.image1, self.image2]\n+        prompts = [\"<REGION_PROPOSAL>\", \"<OPEN_VOCABULARY_DETECTION>car\"]\n+        inputs = processor(images=images, text=prompts, padding=\"longest\", return_tensors=\"pt\")\n+\n+        EXPECTED_INPUT_IDS = [\n+            [processor.image_token_id] * processor.num_image_tokens + [0, 574, 22486, 5, 976, 5327, 11, 5, 2274, 4, 2],\n+            [processor.image_token_id] * processor.num_image_tokens + [0, 574, 22486, 512, 11, 5, 2274, 4, 2, 1, 1],\n+        ]  # fmt: skip\n+        self.assertEqual(inputs[\"input_ids\"].tolist(), EXPECTED_INPUT_IDS)\n+\n+        inputs.to(device=torch_device)\n+        predictions = model.generate(**inputs, max_new_tokens=100)\n+\n+        EXPECTED_PREDICTION_IDS = [\n+            [2, 0, 0, 0, 50269, 50269, 51268, 50944, 50269, 50269, 50579, 50940, 51032, 50269, 51268, 50932, 50793, 50813, 51190, 51031, 50432, 50401, 50632, 50691, 51071, 50943, 51159, 51027, 50835, 50946, 50915, 51029, 2],\n+            [2, 0, 5901, 50321, 50603, 51201, 51043, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n+        ]  # fmt: skip\n+        self.assertEqual(predictions.tolist(), EXPECTED_PREDICTION_IDS)\n+\n+        generated_texts = processor.batch_decode(predictions, skip_special_tokens=False)\n+\n+        EXPECTED_GENERATED_TEXTS = [\n+            '</s><s><s><s><loc_0><loc_0><loc_999><loc_675><loc_0><loc_0><loc_310><loc_671><loc_763><loc_0><loc_999><loc_663><loc_524><loc_544><loc_921><loc_762><loc_163><loc_132><loc_363><loc_422><loc_802><loc_674><loc_890><loc_758><loc_566><loc_677><loc_646><loc_760></s>',\n+            '</s><s>car<loc_52><loc_334><loc_932><loc_774></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n+        ]  # fmt: skip\n+        self.assertEqual(generated_texts, EXPECTED_GENERATED_TEXTS)\n+\n+        parsed_answer_0 = processor.post_process_generation(\n+            generated_texts[0], task=\"<REGION_PROPOSAL>\", image_size=(images[0].width, images[0].height)\n+        )\n+        EXPECTED_PARSED_ANSWER_0 = {'<REGION_PROPOSAL>': {'bboxes': [[0, 0, 1299, 591], [0, 0, 403, 588], [992, 0, 1299, 581], [681, 476, 1197, 667], [212, 116, 472, 370], [1043, 590, 1157, 664], [736, 593, 840, 666]], 'labels': ['', '', '', '', '', '', '']}}  # fmt: skip\n+        self.assertEqual(parsed_answer_0, EXPECTED_PARSED_ANSWER_0)\n+\n+        parsed_answer_1 = processor.post_process_generation(\n+            generated_texts[1], task=\"<OPEN_VOCABULARY_DETECTION>\", image_size=(images[1].width, images[1].height)\n+        )\n+        EXPECTED_PARSED_ANSWER_1 = {'<OPEN_VOCABULARY_DETECTION>': {'bboxes': [[33, 160, 596, 371]], 'bboxes_labels': ['car'], 'polygons': [], 'polygons_labels': []}}  # fmt: skip\n+        self.assertEqual(parsed_answer_1, EXPECTED_PARSED_ANSWER_1)\n+\n+    def test_large_model_inference_sdpa(self):\n+        model_name = \"ducviet00/Florence-2-large-hf\"\n+        processor = AutoProcessor.from_pretrained(model_name)\n+        model = Florence2ForConditionalGeneration.from_pretrained(model_name, attn_implementation=\"sdpa\").to(\n+            torch_device\n+        )\n+\n+        prompt = \"<REFERRING_EXPRESSION_SEGMENTATION>a car\"\n+        inputs = processor(images=self.image2, text=prompt, return_tensors=\"pt\")\n+        inputs.to(device=torch_device)\n+\n+        EXPECTED_INPUT_IDS = [[processor.image_token_id] * processor.num_image_tokens + [0, 574, 22486, 10, 512, 11, 5, 2274, 19, 11445, 2]]  # fmt: skip\n+        self.assertEqual(inputs[\"input_ids\"].tolist(), EXPECTED_INPUT_IDS)\n+\n+        predictions = model.generate(**inputs, max_new_tokens=100)\n+\n+        EXPECTED_PREDICTION_IDS = [[2, 0, 0, 0, 50548, 50646, 50551, 50644, 50554, 50644, 50562, 50637, 50565, 50637, 50570, 50633, 50573, 50633, 50578, 50629, 50582, 50627, 50587, 50625, 50592, 50623, 50597, 50621, 50603, 50619, 50609, 50616, 50615, 50614, 50622, 50612, 50629, 50610, 50639, 50608, 50651, 50606, 50667, 50604, 50695, 50602, 50750, 50602, 50778, 50604, 50793, 50606, 50805, 50608, 50812, 50610, 50818, 50612, 50825, 50614, 50831, 50616, 50837, 50619, 50844, 50621, 50848, 50623, 50854, 50627, 50857, 50631, 50861, 50637, 50864, 50644, 50867, 50650, 50870, 50656, 50873, 50662, 50875, 50668, 50878, 50673, 50879, 50679, 50883, 50685, 50886, 50691, 50889, 50698, 50892, 50704, 50898, 50712, 50903, 50714, 2]]  # fmt: skip\n+        self.assertEqual(predictions.tolist(), EXPECTED_PREDICTION_IDS)\n+\n+        generated_text = processor.batch_decode(predictions, skip_special_tokens=False)[0]\n+\n+        EXPECTED_GENERATED_TEXT = \"</s><s><s><s><loc_279><loc_377><loc_282><loc_375><loc_285><loc_375><loc_293><loc_368><loc_296><loc_368><loc_301><loc_364><loc_304><loc_364><loc_309><loc_360><loc_313><loc_358><loc_318><loc_356><loc_323><loc_354><loc_328><loc_352><loc_334><loc_350><loc_340><loc_347><loc_346><loc_345><loc_353><loc_343><loc_360><loc_341><loc_370><loc_339><loc_382><loc_337><loc_398><loc_335><loc_426><loc_333><loc_481><loc_333><loc_509><loc_335><loc_524><loc_337><loc_536><loc_339><loc_543><loc_341><loc_549><loc_343><loc_556><loc_345><loc_562><loc_347><loc_568><loc_350><loc_575><loc_352><loc_579><loc_354><loc_585><loc_358><loc_588><loc_362><loc_592><loc_368><loc_595><loc_375><loc_598><loc_381><loc_601><loc_387><loc_604><loc_393><loc_606><loc_399><loc_609><loc_404><loc_610><loc_410><loc_614><loc_416><loc_617><loc_422><loc_620><loc_429><loc_623><loc_435><loc_629><loc_443><loc_634><loc_445></s>\"  # fmt: skip\n+        self.assertEqual(generated_text, EXPECTED_GENERATED_TEXT)\n+\n+        parsed_answer = processor.post_process_generation(\n+            generated_text,\n+            task=\"<REFERRING_EXPRESSION_SEGMENTATION>\",\n+            image_size=(self.image2.width, self.image2.height),\n+        )\n+        EXPECTED_PARSED_ANSWER = {'<REFERRING_EXPRESSION_SEGMENTATION>': {'polygons': [[[178, 181, 180, 180, 182, 180, 187, 176, 189, 176, 192, 174, 194, 174, 198, 173, 200, 172, 203, 171, 207, 170, 210, 169, 214, 168, 217, 166, 221, 165, 226, 164, 230, 163, 237, 162, 244, 162, 255, 161, 272, 160, 308, 160, 326, 161, 335, 162, 343, 162, 347, 163, 351, 164, 356, 165, 360, 166, 363, 168, 368, 169, 370, 170, 374, 172, 376, 174, 379, 176, 381, 180, 383, 183, 384, 186, 386, 188, 388, 191, 390, 194, 390, 197, 393, 199, 395, 202, 397, 206, 399, 209, 402, 212, 406, 213]]], 'labels': ['']}}  # fmt: skip\n+        self.assertEqual(parsed_answer, EXPECTED_PARSED_ANSWER)\n+\n+    def test_large_model_batching_inference_sdpa(self):\n+        model_name = \"ducviet00/Florence-2-large-hf\"\n+        processor = AutoProcessor.from_pretrained(model_name)\n+        model = Florence2ForConditionalGeneration.from_pretrained(model_name, attn_implementation=\"sdpa\").to(\n+            torch_device\n+        )\n+\n+        images = [self.image1, self.image2]\n+        prompts = [\"<OCR_WITH_REGION>\", \"<CAPTION>\"]\n+        inputs = processor(images=images, text=prompts, padding=\"longest\", return_tensors=\"pt\")\n+\n+        EXPECTED_INPUT_IDS = [\n+            [processor.image_token_id] * processor.num_image_tokens + [0, 2264, 16, 5, 2788, 11, 5, 2274, 6, 19, 3806, 116, 2],\n+            [processor.image_token_id] * processor.num_image_tokens + [0, 2264, 473, 5, 2274, 6190, 116, 2, 1, 1, 1, 1, 1],\n+        ]  # fmt: skip\n+        self.assertEqual(inputs[\"input_ids\"].tolist(), EXPECTED_INPUT_IDS)\n+\n+        inputs.to(device=torch_device)\n+        predictions = model.generate(**inputs, max_new_tokens=100)\n+\n+        EXPECTED_PREDICTION_IDS = [\n+            [2, 0, 0, 0, 47643, 47240, 7487, 47643, 50802, 50337, 50922, 50337, 50922, 50397, 50802, 50397, 4652, 50270, 50372, 50288, 50372, 50288, 50394, 50270, 50394, 495, 2571, 50401, 50455, 50446, 50457, 50446, 50483, 50401, 50482, 4014, 5733, 50446, 50495, 50614, 50493, 50614, 50596, 50446, 50600, 530, 791, 673, 51230, 50640, 51261, 50640, 51261, 50666, 51230, 50666, 5733, 565, 3048, 50389, 50683, 50461, 50684, 50461, 50719, 50389, 50717, 7111, 230, 5061, 33893, 50707, 50668, 50755, 50668, 50755, 50682, 50707, 50682, 10932, 50290, 50708, 50333, 50706, 50334, 50751, 50290, 50753, 4652, 51128, 50704, 51149, 50704, 51149, 50729, 51128, 50729, 2],\n+            [2, 0, 102, 2272, 512, 9181, 11, 760, 9, 10, 5718, 745, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n+        ]  # fmt: skip\n+        self.assertEqual(predictions.tolist(), EXPECTED_PREDICTION_IDS)\n+\n+        generated_texts = processor.batch_decode(predictions, skip_special_tokens=False)\n+\n+        EXPECTED_GENERATED_TEXTS = [\n+            \"</s><s><s><s>ä¸­æ–°ä¸­<loc_533><loc_68><loc_653><loc_68><loc_653><loc_128><loc_533><loc_128>88<loc_1><loc_103><loc_19><loc_103><loc_19><loc_125><loc_1><loc_125>DAT<loc_132><loc_186><loc_177><loc_188><loc_177><loc_214><loc_132><loc_213>STOP<loc_177><loc_226><loc_345><loc_224><loc_345><loc_327><loc_177><loc_331>KUO<loc_961><loc_371><loc_992><loc_371><loc_992><loc_397><loc_961><loc_397>OPTUS<loc_120><loc_414><loc_192><loc_415><loc_192><loc_450><loc_120><loc_448>OD COUKT<loc_438><loc_399><loc_486><loc_399><loc_486><loc_413><loc_438><loc_413>yes<loc_21><loc_439><loc_64><loc_437><loc_65><loc_482><loc_21><loc_484>88<loc_859><loc_435><loc_880><loc_435><loc_880><loc_460><loc_859><loc_460></s>\",\n+            \"</s><s>a green car parked in front of a yellow building</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\",\n+        ]  # fmt: skip\n+        self.assertEqual(generated_texts, EXPECTED_GENERATED_TEXTS)\n+\n+        parsed_answer = processor.post_process_generation(\n+            generated_texts[0], task=\"<OCR_WITH_REGION>\", image_size=(images[0].width, images[0].height)\n+        )\n+        EXPECTED_PARSED_ANSWER = {'<OCR_WITH_REGION>': {'quad_boxes': [[693, 60, 849, 60, 849, 112, 693, 112], [1, 90, 25, 90, 25, 109, 1, 109], [172, 163, 230, 165, 230, 187, 172, 187], [230, 198, 449, 196, 449, 286, 230, 290], [1249, 325, 1290, 325, 1290, 348, 1249, 348], [156, 363, 250, 363, 250, 394, 156, 392], [570, 349, 632, 349, 632, 362, 570, 362], [27, 385, 83, 383, 85, 422, 27, 424], [1117, 381, 1144, 381, 1144, 403, 1117, 403]], 'labels': ['ä¸­æ–°ä¸­', '88', 'DAT', 'STOP', 'KUO', 'OPTUS', 'OD COUKT', 'yes', '88']}}  # fmt: skip\n+        self.assertEqual(parsed_answer, EXPECTED_PARSED_ANSWER)"
        },
        {
            "sha": "2f9a72c9c7e785a63da31ac2f6509d74fd870215",
            "filename": "tests/models/florence2/test_processing_florence2.py",
            "status": "added",
            "additions": 246,
            "deletions": 0,
            "changes": 246,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca543f822f73ebc69b00835c74ae927d9730b6f5/tests%2Fmodels%2Fflorence2%2Ftest_processing_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca543f822f73ebc69b00835c74ae927d9730b6f5/tests%2Fmodels%2Fflorence2%2Ftest_processing_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflorence2%2Ftest_processing_florence2.py?ref=ca543f822f73ebc69b00835c74ae927d9730b6f5",
            "patch": "@@ -0,0 +1,246 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import shutil\n+import tempfile\n+import unittest\n+\n+from transformers import AutoProcessor, BartTokenizerFast, Florence2Processor\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_vision_available():\n+    from transformers import CLIPImageProcessor\n+\n+\n+@require_torch\n+@require_vision\n+class Florence2ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = Florence2Processor\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.tmpdirname = tempfile.mkdtemp()\n+\n+        image_processor = CLIPImageProcessor.from_pretrained(\"ducviet00/Florence-2-base-hf\")\n+        image_processor.image_seq_length = 0\n+        tokenizer = BartTokenizerFast.from_pretrained(\"ducviet00/Florence-2-base-hf\")\n+        tokenizer.image_token = \"<image>\"\n+        tokenizer.image_token_id = tokenizer.encode(tokenizer.image_token, add_special_tokens=False)[0]\n+        tokenizer.extra_special_tokens = {\"image_token\": \"<image>\"}\n+        processor_kwargs = cls.prepare_processor_dict()\n+        processor = Florence2Processor(image_processor, tokenizer, **processor_kwargs)\n+        processor.save_pretrained(cls.tmpdirname)\n+        cls.image_token = processor.image_token\n+\n+    @staticmethod\n+    def prepare_processor_dict():\n+        return {\n+            \"post_processor_config\": {\n+                \"ocr\": {\n+                    \"pattern\": r\"(.+?)<loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)><loc_(\\d+)>\",\n+                    \"area_threshold\": 0.0,\n+                },\n+                \"phrase_grounding\": {\"banned_grounding_tokens\": [\"the image\"]},\n+                \"pure_text\": {},\n+                \"description_with_bboxes\": {},\n+                \"description_with_polygons\": {},\n+                \"polygons\": {},\n+                \"bboxes\": {},\n+                \"description_with_bboxes_or_polygons\": {},\n+            }\n+        }\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n+\n+    def test_construct_prompts(self):\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n+\n+        # Test single text without task token\n+        text = \"This is a simple text.\"\n+        prompts = processor._construct_prompts(text)\n+        self.assertEqual(prompts, [text])\n+\n+        # Test list of texts with task without input\n+        texts = [\"<OCR>\", \"<CAPTION>\"]\n+        prompts = processor._construct_prompts(texts)\n+        EXPECTED_PROMPTS_WITHOUT_INPUT = [\"What is the text in the image?\", \"What does the image describe?\"]\n+        self.assertEqual(prompts, EXPECTED_PROMPTS_WITHOUT_INPUT)\n+\n+        # Test task with input\n+        texts = [\"<CAPTION_TO_PHRASE_GROUNDING> a red car\"]\n+        prompts = processor._construct_prompts(texts)\n+        EXPECTED_PROMPTS_WITH_INPUT = [\"Locate the phrases in the caption: a red car\"]\n+        self.assertEqual(prompts, EXPECTED_PROMPTS_WITH_INPUT)\n+\n+        # Test invalid prompt with task token not alone\n+        with self.assertRaises(ValueError):\n+            processor._construct_prompts(\"<OCR> extra text\")\n+\n+    def test_quantizer_quantize_dequantize(self):\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n+\n+        # Test bounding box quantization and dequantization\n+        boxes = torch.tensor([[0, 0, 30, 40], [500, 550, 600, 690], [750, 1121, 851, 1239]], dtype=torch.int32)\n+        size = (800, 1200)\n+        quantized_boxes = processor.post_processor.quantize(boxes, size)\n+        dequantized_boxes = processor.post_processor.dequantize(quantized_boxes, size)\n+        EXPECTED_DEQUANTIZED_BBOX = torch.tensor(\n+            [[0, 0, 30, 40], [500, 550, 600, 690], [750, 1121, 799, 1199]], dtype=torch.int32\n+        )\n+        self.assertTrue(torch.allclose(dequantized_boxes, EXPECTED_DEQUANTIZED_BBOX))\n+\n+        # Test points quantization and dequantization\n+        points = torch.tensor([[0, 0], [300, 400], [850, 1250]], dtype=torch.int32)\n+        quantized_points = processor.post_processor.quantize(points, size)\n+        dequantized_points = processor.post_processor.dequantize(quantized_points, size)\n+        EXPECTED_DEQUANTIZED_POINTS = torch.tensor([[0, 0], [300, 400], [799, 1199]], dtype=torch.int32)\n+        self.assertTrue(torch.allclose(dequantized_points, EXPECTED_DEQUANTIZED_POINTS))\n+\n+        # Test invalid shape\n+        with self.assertRaises(ValueError):\n+            processor.post_processor.quantize(torch.tensor([[1, 2, 3]]), size)\n+\n+    def test_post_process_parse_description_with_bboxes_from_text_and_spans(self):\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n+        text_without_phrase = \"</s><s><loc_53><loc_334><loc_933><loc_775><loc_711><loc_203><loc_906><loc_546><loc_585><loc_309><loc_774><loc_709><loc_577></s><pad>\"\n+        image_size = (1000, 1000)\n+        parsed_text_without_phrase = processor.post_processor.parse_description_with_bboxes_from_text_and_spans(\n+            text_without_phrase, image_size=image_size, allow_empty_phrase=True\n+        )\n+        EXPECTED_PARSED_TEXT_WITHOUT_PHRASE = [\n+            {\"bbox\": [53, 334, 933, 775], \"cat_name\": \"\"},\n+            {\"bbox\": [711, 203, 906, 546], \"cat_name\": \"\"},\n+            {\"bbox\": [585, 309, 774, 709], \"cat_name\": \"\"},\n+        ]\n+        self.assertEqual(parsed_text_without_phrase, EXPECTED_PARSED_TEXT_WITHOUT_PHRASE)\n+\n+        text_with_phrase = (\n+            \"</s><s>car<loc_53><loc_334><loc_933><loc_775>door handle<loc_425><loc_504><loc_474><loc_516></s><pad>\"\n+        )\n+        image_size = (1000, 1000)\n+        parsed_text_with_phrase = processor.post_processor.parse_description_with_bboxes_from_text_and_spans(\n+            text_with_phrase, image_size=image_size, allow_empty_phrase=False\n+        )\n+        EXPECTED_PARSED_TEXT_WITH_PHRASE = [\n+            {\"bbox\": [53, 334, 933, 775], \"cat_name\": \"car\"},\n+            {\"bbox\": [425, 504, 474, 516], \"cat_name\": \"door handle\"},\n+        ]\n+        self.assertEqual(parsed_text_with_phrase, EXPECTED_PARSED_TEXT_WITH_PHRASE)\n+\n+    def test_post_process_parse_description_with_polygons_from_text_and_spans(self):\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n+        text_without_phrase = \"<loc_279><loc_379><loc_282><loc_379><loc_290><loc_373><loc_293><loc_373><loc_298><loc_369><loc_301><loc_369>\"\n+        image_size = (1000, 1000)\n+        parsed_text_without_phrase = processor.post_processor.parse_description_with_polygons_from_text_and_spans(\n+            text_without_phrase, image_size=image_size, allow_empty_phrase=True\n+        )\n+        EXPECTED_PARSED_TEXT_WITHOUT_PHRASE = [\n+            {\n+                \"cat_name\": \"\",\n+                \"polygons\": [[279, 379, 282, 379, 290, 373, 293, 373, 298, 369, 301, 369]],\n+            }\n+        ]\n+        self.assertEqual(parsed_text_without_phrase, EXPECTED_PARSED_TEXT_WITHOUT_PHRASE)\n+\n+        text_with_phrase = (\n+            \"Hello<loc_769><loc_248><loc_771><loc_234><loc_773><loc_206><loc_773><loc_198><loc_771><loc_193>\"\n+        )\n+        image_size = (1000, 1000)\n+        parsed_text_with_phrase = processor.post_processor.parse_description_with_polygons_from_text_and_spans(\n+            text_with_phrase, image_size=image_size, allow_empty_phrase=False\n+        )\n+        EXPECTED_PARSED_TEXT_WITH_PHRASE = [\n+            {\n+                \"cat_name\": \"Hello\",\n+                \"polygons\": [[769, 248, 771, 234, 773, 206, 773, 198, 771, 193]],\n+            }\n+        ]\n+        self.assertEqual(parsed_text_with_phrase, EXPECTED_PARSED_TEXT_WITH_PHRASE)\n+\n+    def test_post_process_parse_ocr_from_text_and_spans(self):\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n+        text = \"</s><s>Hello<loc_100><loc_100><loc_200><loc_100><loc_200><loc_200><loc_100><loc_200>World<loc_300><loc_300><loc_400><loc_300><loc_400><loc_400><loc_300><loc_400></s>\"\n+        image_size = (1000, 1000)\n+        parsed = processor.post_processor.parse_ocr_from_text_and_spans(\n+            text, pattern=None, image_size=image_size, area_threshold=0.0\n+        )\n+        EXPECTED_PARSED_OCR = [\n+            {\"quad_box\": [100, 100, 200, 100, 200, 200, 100, 200], \"text\": \"Hello\"},\n+            {\"quad_box\": [300, 300, 400, 300, 400, 400, 300, 400], \"text\": \"World\"},\n+        ]\n+        self.assertEqual(parsed, EXPECTED_PARSED_OCR)\n+\n+        # Test with area threshold filtering\n+        small_text = \"Small<loc_1><loc_1><loc_2><loc_2><loc_2><loc_2><loc_1><loc_1>\"\n+        parsed_small = processor.post_processor.parse_ocr_from_text_and_spans(\n+            small_text, pattern=None, image_size=image_size, area_threshold=0.01\n+        )\n+        EXPECTED_PARSED_OCR_SMALL = []\n+        self.assertEqual(parsed_small, EXPECTED_PARSED_OCR_SMALL)\n+\n+    def test_post_process_parse_phrase_grounding_from_text_and_spans(self):\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n+        text = \"</s><s>red car<loc_53><loc_334><loc_933><loc_775><loc_711><loc_203><loc_906><loc_546>sky<loc_0><loc_0><loc_1000><loc_300></s>\"\n+        image_size = (1000, 1000)\n+        parsed = processor.post_processor.parse_phrase_grounding_from_text_and_spans(text, image_size=image_size)\n+        EXPECTED_PARSED_PHRASE_GROUNDING = [\n+            {\"bbox\": [[53, 334, 933, 775], [711, 203, 906, 546]], \"cat_name\": \"red car\"},\n+            {\"bbox\": [[0, 0, 1000, 300]], \"cat_name\": \"sky\"},\n+        ]\n+        self.assertEqual(parsed, EXPECTED_PARSED_PHRASE_GROUNDING)\n+\n+        # Test with blacklisted phrase\n+        blacklisted_text = \"the image<loc_100><loc_100><loc_200><loc_200>\"\n+        parsed_blacklisted = processor.post_processor.parse_phrase_grounding_from_text_and_spans(\n+            blacklisted_text, image_size=image_size\n+        )\n+        EXPECTED_PARSED_BLACKLISTED = []\n+        self.assertEqual(parsed_blacklisted, EXPECTED_PARSED_BLACKLISTED)\n+\n+    def test_post_process_generation(self):\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n+\n+        # Test pure_text task\n+        text = \"<s>Hello world</s>\"\n+        cap_result = processor.post_process_generation(text=text, task=\"<CAPTION>\", image_size=None)\n+        EXPECTED_PURE_TEXT_RESULT = {\"<CAPTION>\": \"Hello world\"}\n+        self.assertEqual(cap_result, EXPECTED_PURE_TEXT_RESULT)\n+\n+        # Test description_with_bboxes task\n+        text = \"car<loc_53><loc_334><loc_933><loc_775>\"\n+        od_result = processor.post_process_generation(text=text, task=\"<OD>\", image_size=(1000, 1000))\n+        EXPECTED_BBOXES_RESULT = {\"<OD>\": {\"bboxes\": [[53, 334, 933, 775]], \"labels\": [\"car\"]}}\n+        self.assertEqual(od_result, EXPECTED_BBOXES_RESULT)\n+\n+        # Test OCR task\n+        text = \"Hello<loc_100><loc_100><loc_200><loc_100><loc_200><loc_200><loc_100><loc_200>\"\n+        ocr_result = processor.post_process_generation(text=text, task=\"<OCR_WITH_REGION>\", image_size=(1000, 1000))\n+        EXPECTED_OCR_RESULT = {\n+            \"<OCR_WITH_REGION>\": {\"quad_boxes\": [[100, 100, 200, 100, 200, 200, 100, 200]], \"labels\": [\"Hello\"]}\n+        }\n+        self.assertEqual(ocr_result, EXPECTED_OCR_RESULT)"
        },
        {
            "sha": "53028bdb1f0c95b38161accf3ecbf5a0deb80188",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca543f822f73ebc69b00835c74ae927d9730b6f5/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca543f822f73ebc69b00835c74ae927d9730b6f5/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=ca543f822f73ebc69b00835c74ae927d9730b6f5",
            "patch": "@@ -174,6 +174,7 @@\n         \"CsmDepthDecoderForCausalLM\",  # Building part of bigger (tested) model. Tested implicitly through CsmForConditionalGenerationIntegrationTest.\n         \"CsmDepthDecoderModel\",  # Building part of bigger (tested) model. Tested implicitly through CsmForConditionalGenerationIntegrationTest.\n         \"CsmBackboneModel\",  # Building part of bigger (tested) model. Tested implicitly through CsmForConditionalGenerationIntegrationTest.\n+        \"Florence2VisionBackbone\",  # Building part of bigger (tested) model. Tested implicitly through Florence2ForConditionalGeneration.\n     ]\n )\n \n@@ -395,6 +396,7 @@\n     \"CsmDepthDecoderModel\",  # Building part of a bigger model\n     \"CsmDepthDecoderForCausalLM\",  # Building part of a bigger model\n     \"CsmForConditionalGeneration\",  # Building part of a bigger model\n+    \"Florence2VisionBackbone\",  # Building part of a bigger model\n ]\n \n # DO NOT edit this list!"
        }
    ],
    "stats": {
        "total": 5410,
        "additions": 5410,
        "deletions": 0
    }
}