{
    "author": "JJJYmmm",
    "message": "make Llama4TextMoe forward more readable (#37529)\n\n* update forward of Llama4TextMoe\n\n* remove redudant transpose\n\n* fix formatting\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "565a0052ed2783bbae54b431862253922ecaeb3c",
    "files": [
        {
            "sha": "fe77ea4a58c80c81b5ac1cafa85e4249fb0846cc",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 5,
            "deletions": 18,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/565a0052ed2783bbae54b431862253922ecaeb3c/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/565a0052ed2783bbae54b431862253922ecaeb3c/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=565a0052ed2783bbae54b431862253922ecaeb3c",
            "patch": "@@ -138,36 +138,23 @@ def __init__(self, config):\n         self.shared_expert = Llama4TextMLP(config)\n \n     def forward(self, hidden_states):\n-        batch, seq_len, hidden_dim = hidden_states.shape\n         hidden_states = hidden_states.reshape(-1, self.hidden_dim)\n         router_logits = self.router(hidden_states)\n-        tokens_per_expert = batch * seq_len\n \n         router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=1)\n+\n         router_scores = (\n             torch.full_like(router_logits, float(\"-inf\")).scatter_(1, router_indices, router_top_value).transpose(0, 1)\n         )\n-        # We do this to make sure we have -inf for non topK tokens before going through the !\n-        # Here we are just creating a tensor to index each and every single one of the hidden states. Let s maybe register a buffer for this!\n-        router_indices = (\n-            torch.arange(tokens_per_expert, device=hidden_states.device).view(1, -1).expand(router_scores.size(0), -1)\n-        )\n         router_scores = torch.sigmoid(router_scores.float()).to(hidden_states.dtype)\n \n-        router_indices = router_indices.reshape(-1, 1).expand(-1, hidden_dim)\n-        routed_in = torch.gather(\n-            input=hidden_states,\n-            dim=0,\n-            index=router_indices,\n-        ).to(hidden_states.device)\n-        # we gather inputs corresponding to each expert based on the router indices\n+        routed_in = hidden_states.repeat(self.num_experts, 1)\n         routed_in = routed_in * router_scores.reshape(-1, 1)\n         routed_out = self.experts(routed_in)\n+\n         out = self.shared_expert(hidden_states)\n-        # now that we finished expert computation -> we scatter add because we gathered previously\n-        # we have to do this because we used all experts on all tokens. This is faster than the for loop, tho you are compute bound\n-        # this scales a lot better if you do EP!\n-        out.scatter_add_(dim=0, index=router_indices, src=routed_out.view(-1, hidden_dim))\n+        out.add_(routed_out.reshape(self.num_experts, -1, self.hidden_dim).sum(dim=0))\n+\n         return out, router_scores\n \n "
        }
    ],
    "stats": {
        "total": 23,
        "additions": 5,
        "deletions": 18
    }
}