{
    "author": "MekkCyber",
    "message": "[kernels] Kernel Config  (#41232)\n\n* first config\n\n* add kernel_config\n\n* add import logic\n\n* fixing style\n\n* compare class name\n\n* add comments\n\n* rm import\n\n* adding kernel md files\n\n* add to toctree\n\n* adding to main_classes\n\n* simplify required config\n\n* add to doc\n\n* style\n\n* store the mapping\n\n* remove nested func\n\n* add hub mixin\n\n* fix\n\n* imports\n\n* fix",
    "sha": "da7b8ce11f5e2605301b0d576613bc2f00d32405",
    "files": [
        {
            "sha": "d5cd109227f5039d4a444b440293ae33512ac07a",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/da7b8ce11f5e2605301b0d576613bc2f00d32405/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/da7b8ce11f5e2605301b0d576613bc2f00d32405/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=da7b8ce11f5e2605301b0d576613bc2f00d32405",
            "patch": "@@ -216,6 +216,11 @@\n   - local: quantization/contribute\n     title: Contribute\n   title: Quantization\n+- isExpanded: false\n+  sections:\n+  - local: kernel_doc/overview\n+    title: Kernels in transformers\n+  title: Kernels\n - isExpanded: false\n   sections:\n   - local: serialization\n@@ -368,6 +373,8 @@\n       title: Image Processor\n     - local: main_classes/video_processor\n       title: Video Processor\n+    - local: main_classes/kernels\n+      title: Kernels\n     title: Main Classes\n   - sections:\n     - sections:"
        },
        {
            "sha": "6fdec8608eea1b5e2bf1837d015edc3d8b048913",
            "filename": "docs/source/en/kernel_doc/overview.md",
            "status": "added",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/da7b8ce11f5e2605301b0d576613bc2f00d32405/docs%2Fsource%2Fen%2Fkernel_doc%2Foverview.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/da7b8ce11f5e2605301b0d576613bc2f00d32405/docs%2Fsource%2Fen%2Fkernel_doc%2Foverview.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fkernel_doc%2Foverview.md?ref=da7b8ce11f5e2605301b0d576613bc2f00d32405",
            "patch": "@@ -0,0 +1,3 @@\n+# Overview\n+\n+Kernels in transformers are used to optimize the performance of models with custom layers from the hub and very low effort.\n\\ No newline at end of file"
        },
        {
            "sha": "0ceffe800fee2c894ba42f2e027639492975db72",
            "filename": "docs/source/en/main_classes/kernels.md",
            "status": "added",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/da7b8ce11f5e2605301b0d576613bc2f00d32405/docs%2Fsource%2Fen%2Fmain_classes%2Fkernels.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/da7b8ce11f5e2605301b0d576613bc2f00d32405/docs%2Fsource%2Fen%2Fmain_classes%2Fkernels.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fkernels.md?ref=da7b8ce11f5e2605301b0d576613bc2f00d32405",
            "patch": "@@ -0,0 +1,7 @@\n+## Kernels\n+\n+This page documents the kernels configuration utilities.\n+\n+### KernelConfig\n+\n+[[autodoc]] KernelConfig"
        },
        {
            "sha": "b7e302452b54822c14b0d7b0588e42fbf7f0b155",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/da7b8ce11f5e2605301b0d576613bc2f00d32405/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da7b8ce11f5e2605301b0d576613bc2f00d32405/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=da7b8ce11f5e2605301b0d576613bc2f00d32405",
            "patch": "@@ -265,6 +265,7 @@\n         \"VptqConfig\",\n     ],\n     \"video_utils\": [],\n+    \"utils.kernel_config\": [\"KernelConfig\"],\n }\n \n # tokenizers-backed objects\n@@ -754,6 +755,7 @@\n     from .utils import is_torch_npu_available as is_torch_npu_available\n     from .utils import is_torch_xla_available as is_torch_xla_available\n     from .utils import is_torch_xpu_available as is_torch_xpu_available\n+    from .utils.kernel_config import KernelConfig as KernelConfig\n \n     # bitsandbytes config\n     from .utils.quantization_config import AqlmConfig as AqlmConfig\n@@ -775,7 +777,6 @@\n     from .utils.quantization_config import TorchAoConfig as TorchAoConfig\n     from .utils.quantization_config import VptqConfig as VptqConfig\n     from .video_processing_utils import BaseVideoProcessor as BaseVideoProcessor\n-\n else:\n     import sys\n "
        },
        {
            "sha": "a2c680068cafcb809daef1cb2d94fbb12279f909",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 24,
            "deletions": 3,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/da7b8ce11f5e2605301b0d576613bc2f00d32405/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da7b8ce11f5e2605301b0d576613bc2f00d32405/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=da7b8ce11f5e2605301b0d576613bc2f00d32405",
            "patch": "@@ -84,6 +84,7 @@\n     WEIGHTS_INDEX_NAME,\n     WEIGHTS_NAME,\n     ContextManagers,\n+    KernelConfig,\n     PushToHubMixin,\n     cached_file,\n     check_torch_load_is_safe,\n@@ -4503,6 +4504,7 @@ def from_pretrained(\n         device_mesh = kwargs.pop(\"device_mesh\", None)\n         trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n         use_kernels = kwargs.pop(\"use_kernels\", False)\n+        kernel_config = kwargs.pop(\"kernel_config\", None)\n \n         key_mapping = kwargs.pop(\"key_mapping\", None)\n         # Load models with hardcoded key mapping on class for VLMs only, to keep BC and standardize model\n@@ -4895,7 +4897,26 @@ def _assign_original_dtype(module):\n \n         # check if using kernels\n         if use_kernels:\n-            model.use_kernels = True\n+            if not is_kernels_available():\n+                raise ValueError(\n+                    \"Kernels are not available. To use kernels, please install kernels using `pip install kernels`\"\n+                )\n+            from kernels import use_kernel_mapping\n+\n+            if kernel_config is not None and isinstance(kernel_config, KernelConfig):\n+                # This will make sure the mapping is valid, and the layers are registered in the model\n+                kernel_config.sanitize_kernel_mapping(model)\n+\n+                # This will create a compatible mapping for the model with the kernels library\n+                kernel_config.create_compatible_mapping(model)\n+\n+                # This is a context manager to override the default kernel mapping\n+                # We are calling kernelize inside this context manager using the use_kernels setter\n+                with use_kernel_mapping(kernel_config.kernel_mapping):\n+                    model.use_kernels = True\n+            # We use the default kernel mapping in .integrations.hub_kernels\n+            else:\n+                model.use_kernels = True\n \n         # If it is a model with generation capabilities, attempt to load generation files (generation config,\n         # custom generate function)\n@@ -5506,14 +5527,14 @@ def loss_function(self):\n     def loss_function(self, value):\n         self._loss_function = value\n \n-    def kernelize(self):\n+    def kernelize(self, mode=None):\n         if not is_kernels_available():\n             raise ValueError(\n                 \"Kernels are not available. To use kernels, please install kernels using `pip install kernels`\"\n             )\n         from kernels import Device, Mode, kernelize\n \n-        mode = Mode.INFERENCE if not self.training else Mode.TRAINING\n+        mode = Mode.INFERENCE if not self.training else Mode.TRAINING if mode is None else mode\n         kernelize(self, device=Device(type=self.device.type), mode=mode)\n         self._use_kernels = True\n "
        },
        {
            "sha": "ab4f1c2411380482515e33b77abb02ca1f704e19",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da7b8ce11f5e2605301b0d576613bc2f00d32405/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da7b8ce11f5e2605301b0d576613bc2f00d32405/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=da7b8ce11f5e2605301b0d576613bc2f00d32405",
            "patch": "@@ -252,6 +252,7 @@\n     requires_backends,\n     torch_only_method,\n )\n+from .kernel_config import KernelConfig\n from .peft_utils import (\n     ADAPTER_CONFIG_NAME,\n     ADAPTER_SAFE_WEIGHTS_NAME,"
        },
        {
            "sha": "aa9adab2f29c9f4bd81c28e98e8f17a938b66f03",
            "filename": "src/transformers/utils/kernel_config.py",
            "status": "added",
            "additions": 222,
            "deletions": 0,
            "changes": 222,
            "blob_url": "https://github.com/huggingface/transformers/blob/da7b8ce11f5e2605301b0d576613bc2f00d32405/src%2Ftransformers%2Futils%2Fkernel_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da7b8ce11f5e2605301b0d576613bc2f00d32405/src%2Ftransformers%2Futils%2Fkernel_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fkernel_config.py?ref=da7b8ce11f5e2605301b0d576613bc2f00d32405",
            "patch": "@@ -0,0 +1,222 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ..utils import PushToHubMixin, is_kernels_available, is_torch_available\n+\n+\n+if is_kernels_available():\n+    from kernels import LayerRepository, Mode\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+def infer_device(model):\n+    \"\"\"\n+    Infers the device type from the model parameters.\n+    Args:\n+        model: The model instance.\n+\n+    Returns:\n+        The device type.\n+    \"\"\"\n+    EXAMPLE_MAPPING = \"\"\"\n+    {\n+        \"RMSNorm\": {\n+            \"cuda\":\n+                \"kernels-community/layer_norm:LlamaRMSNorm\",\n+            ...\n+        },\n+        ...\n+    }\n+    \"\"\"\n+    try:\n+        param = next(model.parameters())\n+    except StopIteration:\n+        raise ValueError(\n+            f\"Cannot determine model device, please provide a device to the mapping. Example: {EXAMPLE_MAPPING}\"\n+        )\n+\n+    dev_type = param.device.type\n+    if dev_type == \"cuda\":\n+        # Refine based on actual platform\n+        if torch.version.hip is not None:\n+            return \"rocm\"\n+\n+    return dev_type\n+\n+\n+def add_to_mapping(layer_name, device, repo_name, mode, compatible_mapping):\n+    if device not in [\"cuda\", \"rocm\", \"xpu\"]:\n+        raise ValueError(f\"Only cuda, rocm, and xpu devices supported, got: {device}\")\n+    repo_layer_name = repo_name.split(\":\")[1]\n+    repo_id = repo_name.split(\":\")[0]\n+    compatible_mapping[layer_name] = {\n+        device: {\n+            mode: LayerRepository(\n+                repo_id=repo_id,\n+                layer_name=repo_layer_name,\n+            )\n+        }\n+    }\n+\n+\n+class KernelConfig(PushToHubMixin):\n+    \"\"\"\n+    Kernel configuration class. This class is used to configure the kernel mapping for a model.\n+    \"\"\"\n+\n+    def __init__(self, kernel_mapping={}):\n+        self.kernel_mapping = kernel_mapping\n+        self.registered_layer_names = {}\n+\n+    def update_kernel(self, repo_id, registered_name, layer_name, device, mode, revision=None):\n+        self.kernel_mapping[registered_name] = {\n+            device: {\n+                mode: LayerRepository(\n+                    repo_id=repo_id,\n+                    layer_name=layer_name,\n+                    revision=revision,\n+                )\n+            }\n+        }\n+\n+    def store_registered_layer_names(self, model):\n+        for name, module in model.named_modules():\n+            if hasattr(module, \"kernel_layer_name\"):\n+                self.registered_layer_names[name] = module.kernel_layer_name\n+\n+    def sanitize_kernel_mapping(self, model):\n+        \"\"\"\n+        Validates the kernel_mapping to ensure that:\n+        1. Each layer_name in the mapping is registered in the model (i.e., the model contains a module with a matching kernel_layer_name).\n+        2. Each kernel value is either a string of the form 'org/repo:layer_name' or a dict mapping device types (\"cuda\", \"rocm\", \"xpu\") to such strings.\n+        3. Each device key in a dict is one of \"cuda\", \"rocm\", or \"xpu\".\n+        4. Each repo_name is a valid repository and layer name in the format 'org/repo:layer_name' (i.e., a string containing both a slash and a colon).\n+\n+        Args:\n+            model: The model instance whose modules are checked for registered kernel_layer_name attributes.\n+\n+        Raises:\n+            ValueError: If a layer_name is not registered in the model, if a device is not supported,\n+                        or if a repo_name is not a valid 'org/repo:layer_name' string.\n+        \"\"\"\n+        MAPPING_FORMAT = \"\"\"\n+        {\n+            \"RMSNorm\":\n+                \"kernels-community/layer_norm:LlamaRMSNorm\",\n+            ...\n+        },\n+\n+        or\n+\n+        {\n+            \"RMSNorm\": {\n+                \"cuda\":\n+                    \"kernels-community/layer_norm:LlamaRMSNorm\",\n+                \"rocm\":\n+                    \"kernels-community/layer_norm:LlamaRMSNorm\",\n+                ...\n+            },\n+            ...\n+        }\n+        \"\"\"\n+        self.store_registered_layer_names(model)\n+        # Validate that the kernel mapping is a dict\n+        if not isinstance(self.kernel_mapping, dict):\n+            raise ValueError(\n+                f\"Kernel mapping must be a dict of the following format: {MAPPING_FORMAT}, got: {type(self.kernel_mapping)}\"\n+            )\n+\n+        for layer_name, kernel in self.kernel_mapping.items():\n+            if layer_name not in self.registered_layer_names.values():\n+                raise ValueError(\n+                    f\"Layer {layer_name} is not registered in the model, please register it first using register_kernel_forward_from_hub\"\n+                )\n+\n+            if isinstance(kernel, str):\n+                if \"/\" not in kernel or \":\" not in kernel:\n+                    raise ValueError(\n+                        f\"Kernel mapping for '{layer_name}' must be a valid repo name with a layer name (e.g., 'org/repo:layer_name'), got: {kernel}\"\n+                    )\n+\n+            elif isinstance(kernel, dict):\n+                for device, repo_name in kernel.items():\n+                    if device not in [\"cuda\", \"rocm\", \"xpu\"]:\n+                        raise ValueError(f\"Only cuda, rocm, and xpu devices supported, got: {device}\")\n+\n+                    if not isinstance(repo_name, str) or \"/\" not in repo_name or \":\" not in repo_name:\n+                        raise ValueError(\n+                            f\"Kernel mapping for '{layer_name}' must be a valid repo name with a layer name (e.g., 'org/repo:layer_name'), got: {repo_name}\"\n+                        )\n+\n+            else:\n+                raise ValueError(f\"Kernel mapping must follow the format: {MAPPING_FORMAT}, got: {kernel}\")\n+\n+    def create_compatible_mapping(self, model, compile=False):\n+        \"\"\"\n+        Transforms a simple kernel_mapping of the form:\n+            {\n+                \"RMSNorm\":\n+                    \"kernels-community/layer_norm:LlamaRMSNorm\",\n+                ...\n+            },\n+\n+            or\n+\n+            {\n+                \"RMSNorm\": {\n+                    \"cuda\":\n+                        \"kernels-community/layer_norm:LlamaRMSNorm\",\n+                    \"rocm\":\n+                        \"kernels-community/layer_norm:LlamaRMSNorm\",\n+                    ...\n+                },\n+                ...\n+            }\n+\n+        into a nested mapping:\n+\n+            {\n+                \"RMSNorm\": {\n+                    \"cuda\": {\n+                        Mode.INFERENCE: LayerRepository(\n+                            repo_id=\"kernels-community/layer_norm\",\n+                            layer_name=\"LlamaRMSNorm\",\n+                        )\n+                    }\n+                }\n+            }\n+\n+        that's compatible with the kernels library.\n+\n+        The device is inferred from the model's parameters if not provided.\n+        The Mode is inferred from the model's training state.\n+        \"\"\"\n+        compatible_mapping = {}\n+        for layer_name, kernel in self.kernel_mapping.items():\n+            # Infer Mode: use Mode.TRAINING if model is training, else use Mode.INFERENCE\n+            mode = Mode.TRAINING if model.training else Mode.INFERENCE\n+            if compile:\n+                mode = mode | Mode.TORCH_COMPILE\n+\n+            if isinstance(kernel, str):\n+                repo_name = kernel\n+                device = infer_device(model)\n+                add_to_mapping(layer_name, device, repo_name, mode, compatible_mapping)\n+            elif isinstance(kernel, dict):\n+                for device, repo_name in kernel.items():\n+                    add_to_mapping(layer_name, device, repo_name, mode, compatible_mapping)\n+\n+        self.kernel_mapping = compatible_mapping"
        }
    ],
    "stats": {
        "total": 270,
        "additions": 266,
        "deletions": 4
    }
}