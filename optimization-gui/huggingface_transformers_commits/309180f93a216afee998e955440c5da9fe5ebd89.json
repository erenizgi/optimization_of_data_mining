{
    "author": "vasqu",
    "message": "[`BLT`] Fix cache usage (#42188)\n\n* fix\n\n* properly\n\n* fix tests",
    "sha": "309180f93a216afee998e955440c5da9fe5ebd89",
    "files": [
        {
            "sha": "ae40c5e75ab95e681ec824cd2cfc8d4df57b1f21",
            "filename": "src/transformers/models/blt/modeling_blt.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/309180f93a216afee998e955440c5da9fe5ebd89/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/309180f93a216afee998e955440c5da9fe5ebd89/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py?ref=309180f93a216afee998e955440c5da9fe5ebd89",
            "patch": "@@ -28,7 +28,7 @@\n import torch.nn.functional as F\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -321,7 +321,6 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n         position_embeddings: torch.Tensor,\n-        use_cache: bool = False,\n         past_key_values=None,\n         cache_position=None,\n         **kwargs,\n@@ -393,9 +392,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         cross_attention_states: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -404,27 +401,13 @@ def forward(\n         query_states = self.q_proj(query_states)\n         query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n \n-        if cross_attention_states is not None:\n-            cross_attention_states = self.k_norm(cross_attention_states)\n-            key_states = self.k_proj(cross_attention_states)\n-            value_states = self.v_proj(cross_attention_states)\n-            key_states = key_states.view(bsz, -1, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-            value_states = value_states.view(bsz, -1, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-            if past_key_values is not None:\n-                key_states, value_states = past_key_values.update(\n-                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n-        elif cache_position[0] != 0:\n-            key_states, value_states = (\n-                past_key_values.layers[self.layer_idx].keys,\n-                past_key_values.layers[self.layer_idx].values,\n-            )\n-        else:\n-            raise ValueError(\n-                \"Cross attention layer can't find neither `cross_attn_states` nor cached values for key/values!\"\n-            )\n-        attention_interface: Callable = eager_attention_forward\n+        cross_attention_states = self.k_norm(cross_attention_states)\n+        key_states = self.k_proj(cross_attention_states)\n+        value_states = self.v_proj(cross_attention_states)\n+        key_states = key_states.view(bsz, -1, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, -1, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n+        attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n@@ -1089,6 +1072,9 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+\n         # Extract input embeddings as early as possible\n         if inputs_embeds is not None:\n             encoder_embeds = inputs_embeds\n@@ -1137,7 +1123,7 @@ def forward(\n             input_embeds=encoder_embeds,\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n-            past_key_values=past_key_values,\n+            past_key_values=past_key_values.self_attention_cache if past_key_values is not None else None,\n             position_ids=position_ids,\n         )\n \n@@ -1157,6 +1143,7 @@ def forward(\n             encoder_attention_mask=cross_attn_mask_enc,\n             num_patches=patch_lengths.shape[1],\n             patch_ids=patch_ids,\n+            past_key_values=past_key_values.self_attention_cache if past_key_values is not None else None,\n             **kwargs,\n         )\n         encoder_cross_states = encoder_cross_states.view(batch_size, patch_lengths.shape[1], -1)\n@@ -1192,7 +1179,7 @@ def forward(\n             patch_embeds=global_hidden_states,\n             attention_mask=causal_mask,\n             position_ids=position_ids,\n-            past_key_values=past_key_values,\n+            past_key_values=past_key_values.cross_attention_cache if past_key_values is not None else None,\n             cache_position=cache_position,\n             encoder_attention_mask=cross_attn_mask_dec,\n             **kwargs,"
        },
        {
            "sha": "f9c9bf434005bbca2fc78597389902313adc55f6",
            "filename": "src/transformers/models/blt/modular_blt.py",
            "status": "modified",
            "additions": 13,
            "deletions": 46,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/309180f93a216afee998e955440c5da9fe5ebd89/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/309180f93a216afee998e955440c5da9fe5ebd89/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py?ref=309180f93a216afee998e955440c5da9fe5ebd89",
            "patch": "@@ -22,7 +22,7 @@\n import torch.nn as nn\n import torch.nn.functional as F\n \n-from ...cache_utils import Cache, DynamicCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...masking_utils import create_causal_mask\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import dynamic_rope_update\n@@ -299,27 +299,6 @@ def __init__(self, config, layer_idx: int):\n class BltSelfAttention(MllamaTextSelfAttention):\n     def __init__(self, config: BltConfig, layer_idx: int):\n         super().__init__(config, layer_idx)\n-        self.is_causal = True\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: torch.Tensor,\n-        position_embeddings: torch.Tensor,\n-        use_cache: bool = False,\n-        past_key_values=None,\n-        cache_position=None,\n-        **kwargs,\n-    ):\n-        return super().forward(\n-            hidden_states=hidden_states,\n-            attention_mask=attention_mask,\n-            position_embeddings=position_embeddings,\n-            use_cache=use_cache,\n-            past_key_values=past_key_values,\n-            cache_position=cache_position,\n-            **kwargs,\n-        )\n \n \n class BltCrossAttention(MllamaTextCrossAttention):\n@@ -335,37 +314,21 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         cross_attention_states: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         bsz, q_len, _ = hidden_states.size()\n         query_states = self.q_norm(hidden_states)\n         query_states = self.q_proj(query_states)\n         query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n \n-        if cross_attention_states is not None:\n-            cross_attention_states = self.k_norm(cross_attention_states)\n-            key_states = self.k_proj(cross_attention_states)\n-            value_states = self.v_proj(cross_attention_states)\n-            key_states = key_states.view(bsz, -1, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-            value_states = value_states.view(bsz, -1, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-            if past_key_values is not None:\n-                key_states, value_states = past_key_values.update(\n-                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n-        elif cache_position[0] != 0:\n-            key_states, value_states = (\n-                past_key_values.layers[self.layer_idx].keys,\n-                past_key_values.layers[self.layer_idx].values,\n-            )\n-        else:\n-            raise ValueError(\n-                \"Cross attention layer can't find neither `cross_attn_states` nor cached values for key/values!\"\n-            )\n-        attention_interface: Callable = eager_attention_forward\n+        cross_attention_states = self.k_norm(cross_attention_states)\n+        key_states = self.k_proj(cross_attention_states)\n+        value_states = self.v_proj(cross_attention_states)\n+        key_states = key_states.view(bsz, -1, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, -1, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n+        attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n@@ -828,6 +791,9 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+\n         # Extract input embeddings as early as possible\n         if inputs_embeds is not None:\n             encoder_embeds = inputs_embeds\n@@ -876,7 +842,7 @@ def forward(\n             input_embeds=encoder_embeds,\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n-            past_key_values=past_key_values,\n+            past_key_values=past_key_values.self_attention_cache if past_key_values is not None else None,\n             position_ids=position_ids,\n         )\n \n@@ -896,6 +862,7 @@ def forward(\n             encoder_attention_mask=cross_attn_mask_enc,\n             num_patches=patch_lengths.shape[1],\n             patch_ids=patch_ids,\n+            past_key_values=past_key_values.self_attention_cache if past_key_values is not None else None,\n             **kwargs,\n         )\n         encoder_cross_states = encoder_cross_states.view(batch_size, patch_lengths.shape[1], -1)\n@@ -931,7 +898,7 @@ def forward(\n             patch_embeds=global_hidden_states,\n             attention_mask=causal_mask,\n             position_ids=position_ids,\n-            past_key_values=past_key_values,\n+            past_key_values=past_key_values.cross_attention_cache if past_key_values is not None else None,\n             cache_position=cache_position,\n             encoder_attention_mask=cross_attn_mask_dec,\n             **kwargs,"
        },
        {
            "sha": "1f52d30be45cbdbd481d4709ec27df296f0d10d5",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/309180f93a216afee998e955440c5da9fe5ebd89/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/309180f93a216afee998e955440c5da9fe5ebd89/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=309180f93a216afee998e955440c5da9fe5ebd89",
            "patch": "@@ -534,10 +534,8 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n         position_embeddings: torch.Tensor,\n-        use_cache: bool = False,\n         past_key_values=None,\n         cache_position=None,\n-        position_ids=None,\n         **kwargs,\n     ):\n         bsz, q_len, _ = hidden_states.size()"
        },
        {
            "sha": "7688da9b04c23dfde217b59a5b2239a57c535d9b",
            "filename": "tests/models/blt/test_modeling_blt.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/309180f93a216afee998e955440c5da9fe5ebd89/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/309180f93a216afee998e955440c5da9fe5ebd89/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py?ref=309180f93a216afee998e955440c5da9fe5ebd89",
            "patch": "@@ -224,12 +224,15 @@ def test_eager_matches_sdpa_inference(\n \n @require_torch_accelerator\n class BltIntegrationTest(unittest.TestCase):\n+    def setup(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n     def tearDown(self):\n         # TODO (joao): automatic compilation, i.e. compilation when `cache_implementation=\"static\"` is used, leaves\n         # some memory allocated in the cache, which means some object is not being released properly. This causes some\n         # unoptimal memory usage, e.g. after certain tests a 7B model in FP16 no longer fits in a 24GB GPU.\n         # Investigate the root cause.\n-        cleanup(torch_device, gc_collect=False)\n+        cleanup(torch_device, gc_collect=True)\n \n     @slow\n     @require_read_token\n@@ -339,7 +342,7 @@ def test_model_logits(self):\n     def test_model_bf16(self):\n         \"\"\"Test Blt model with bfloat16 precision.\"\"\"\n         NUM_TOKENS_TO_GENERATE = 200\n-        EXPECTED_TEXT = \"my name is alex and i am a student at the university of michigan. i am a senior majoring in computer science and minoring in mathematics. i am also a member of the michigan math club and the michigan computer s\"\n+        EXPECTED_TEXT = \"my name is alex and i am a student at the university of michigan in the college of arts and sciences. i am a senior majoring in computer science and minoring in mathematics. i am also a member of the michigan m\"\n \n         prompt = \"my name is\"\n \n@@ -472,7 +475,7 @@ def test_model_eager(self):\n     def test_model_bf16_static_cache(self):\n         \"\"\"Test Blt model with bfloat16 precision and static cache.\"\"\"\n         NUM_TOKENS_TO_GENERATE = 200\n-        EXPECTED_TEXT = \"my name is alex and i am a student at the university of michigan. i am a senior majoring in computer science and minoring in mathematics. i am also a member of the michigan math club and the michigan computer s\"\n+        EXPECTED_TEXT = \"my name is alex and i am a student at the university of michigan in the college of arts and sciences. i am a senior majoring in computer science and minoring in mathematics. i am also a member of the michigan m\"\n \n         prompt = \"my name is\"\n "
        }
    ],
    "stats": {
        "total": 109,
        "additions": 32,
        "deletions": 77
    }
}