{
    "author": "Cyrilvallez",
    "message": "Harmonize `past_key_value` to `past_key_valueS` everywhere (#39956)\n\n* all modulars and llama\n\n* apply modular\n\n* bert and gpt2 copies\n\n* fix imports\n\n* do it everywhere\n\n* fix import\n\n* finalize it\n\n* fix\n\n* oups set it in modular\n\n* style\n\n* fix\n\n* Add 1 version to deprecation cycle\n\n* Update modeling_layers.py",
    "sha": "5c3fb7f731743609262d7c77a636b41f69d204fe",
    "files": [
        {
            "sha": "ceade678b8a3849a86bb599cf3d3bf2085750e7e",
            "filename": "docs/source/en/modular_transformers.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodular_transformers.md?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -218,7 +218,7 @@ class Olmo2Attention(OlmoAttention):\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -236,10 +236,10 @@ class Olmo2Attention(OlmoAttention):\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -290,7 +290,7 @@ class Olmo2DecoderLayer(OlmoDecoderLayer):\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -304,7 +304,7 @@ class Olmo2DecoderLayer(OlmoDecoderLayer):\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,"
        },
        {
            "sha": "06cdc5c571112cbaadf1c06db63c33360bdb90e4",
            "filename": "examples/modular-transformers/modeling_add_function.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/examples%2Fmodular-transformers%2Fmodeling_add_function.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/examples%2Fmodular-transformers%2Fmodeling_add_function.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_add_function.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -10,6 +10,8 @@\n import torch\n from torch import nn\n \n+from ...utils.deprecation import deprecate_kwarg\n+\n \n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n@@ -62,5 +64,6 @@ class TestAttention(nn.Module):\n     def __init__(self):\n         pass\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(self) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         _ = apply_rotary_pos_emb(1, 1, 1, 1)"
        },
        {
            "sha": "1065afe80d36296fe1954229b465d5dfb2d914fc",
            "filename": "examples/modular-transformers/modeling_dummy_bert.py",
            "status": "modified",
            "additions": 154,
            "deletions": 138,
            "changes": 292,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -13,12 +13,14 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, BaseModelOutputWithPoolingAndCrossAttentions\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, get_torch_version, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_dummy_bert import DummyBertConfig\n \n \n@@ -90,7 +92,7 @@ def forward(\n \n \n class DummyBertSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -115,66 +117,68 @@ def __init__(self, config, position_embedding_type=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.layer_idx = layer_idx\n \n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        mixed_query_layer = self.query(hidden_states)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = self.query(hidden_states)\n+        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+            1, 2\n+        )\n \n-        # If this is instantiated as a cross-attention module, the keys\n-        # and values come from an encoder; the attention mask needs to be\n-        # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_values.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_values\n \n-        if is_cross_attention and past_key_value is not None:\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value[0]\n-            value_layer = past_key_value[1]\n-            attention_mask = encoder_attention_mask\n-        elif is_cross_attention:\n-            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n-            attention_mask = encoder_attention_mask\n-        elif past_key_value is not None:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        use_cache = past_key_value is not None\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+            key_layer = self.key(current_states)\n+            key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+                1, 2\n+            )\n+            value_layer = self.value(current_states)\n+            value_layer = value_layer.view(\n+                batch_size, -1, self.num_attention_heads, self.attention_head_size\n+            ).transpose(1, 2)\n+\n+            if past_key_values is not None:\n+                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_layer, value_layer = curr_past_key_value.update(\n+                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if use_cache:\n+            if past_key_values is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -216,29 +220,26 @@ def forward(\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n         context_layer = context_layer.view(new_context_layer_shape)\n \n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n-        return outputs\n+        return context_layer, attention_probs\n \n \n class DummyBertSdpaSelfAttention(DummyBertSelfAttention):\n-    def __init__(self, config, position_embedding_type=None):\n-        super().__init__(config, position_embedding_type=position_embedding_type)\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+        super().__init__(config, position_embedding_type=position_embedding_type, layer_idx=layer_idx)\n         self.dropout_prob = config.attention_probs_dropout_prob\n         self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n \n     # Adapted from DummyBertSelfAttention\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n         if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n@@ -254,41 +255,56 @@ def forward(\n                 attention_mask,\n                 head_mask,\n                 encoder_hidden_states,\n-                encoder_attention_mask,\n-                past_key_value,\n+                past_key_values,\n                 output_attentions,\n+                cache_position,\n             )\n \n         bsz, tgt_len, _ = hidden_states.size()\n \n-        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+        query_layer = (\n+            self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n+        )\n \n-        # If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\n-        # mask needs to be such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_values.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_values\n \n-        # Check `seq_length` of `past_key_value` == `len(current_states)` to support prefix tuning\n-        if is_cross_attention and past_key_value and past_key_value[0].shape[2] == current_states.shape[1]:\n-            key_layer, value_layer = past_key_value\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_values is not None and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n-            key_layer = self.transpose_for_scores(self.key(current_states))\n-            value_layer = self.transpose_for_scores(self.value(current_states))\n-            if past_key_value is not None and not is_cross_attention:\n-                key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-                value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+            key_layer = (\n+                self.key(current_states)\n+                .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n+                .transpose(1, 2)\n+            )\n+            value_layer = (\n+                self.value(current_states)\n+                .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n+                .transpose(1, 2)\n+            )\n+\n+            if past_key_values is not None:\n+                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_layer, value_layer = curr_past_key_value.update(\n+                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n         # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n@@ -302,9 +318,7 @@ def forward(\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\n         # a causal mask in case tgt_len == 1.\n-        is_causal = (\n-            True if self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1 else False\n-        )\n+        is_causal = self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1\n \n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_layer,\n@@ -318,10 +332,7 @@ def forward(\n         attn_output = attn_output.transpose(1, 2)\n         attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n \n-        outputs = (attn_output,)\n-        if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n-        return outputs\n+        return attn_output, None\n \n \n class DummyBertSelfOutput(nn.Module):\n@@ -345,10 +356,12 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class DummyBertAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         self.self = DUMMY_BERT_SELF_ATTENTION_CLASSES[config._attn_implementation](\n-            config, position_embedding_type=position_embedding_type\n+            config,\n+            position_embedding_type=position_embedding_type,\n+            layer_idx=layer_idx,\n         )\n         self.output = DummyBertSelfOutput(config)\n         self.pruned_heads = set()\n@@ -371,24 +384,25 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            past_key_value,\n-            output_attentions,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n@@ -425,83 +439,67 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class DummyBertLayer(GradientCheckpointingLayer):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = DummyBertAttention(config)\n+        self.attention = DummyBertAttention(config, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = DummyBertAttention(config, position_embedding_type=\"absolute\")\n+            self.crossattention = DummyBertAttention(config, position_embedding_type=\"absolute\", layer_idx=layer_idx)\n         self.intermediate = DummyBertIntermediate(config)\n         self.output = DummyBertOutput(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_values=past_key_values,\n+            cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n+        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n \n-        # if decoder, the last output is tuple of self-attn cache\n-        if self.is_decoder:\n-            outputs = self_attention_outputs[1:-1]\n-            present_key_value = self_attention_outputs[-1]\n-        else:\n-            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n-\n-        cross_attn_present_key_value = None\n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n                 raise ValueError(\n                     f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n-                attention_mask,\n-                head_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n-                cross_attn_past_key_value,\n-                output_attentions,\n+                attention_mask=encoder_attention_mask,\n+                head_mask=head_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                past_key_values=past_key_values,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n-\n-            # add cross-attn cache to positions 3,4 of present_key_value tuple\n-            cross_attn_present_key_value = cross_attention_outputs[-1]\n-            present_key_value = present_key_value + cross_attn_present_key_value\n+            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n         outputs = (layer_output,) + outputs\n \n-        # if decoder, return the attn key/values as the last output\n-        if self.is_decoder:\n-            outputs = outputs + (present_key_value,)\n-\n         return outputs\n \n     def feed_forward_chunk(self, attention_output):\n@@ -511,10 +509,10 @@ def feed_forward_chunk(self, attention_output):\n \n \n class DummyBertEncoder(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.config = config\n-        self.layer = nn.ModuleList([DummyBertLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.layer = nn.ModuleList([DummyBertLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n     def forward(\n@@ -529,6 +527,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n@@ -541,27 +540,34 @@ def forward(\n                 )\n                 use_cache = False\n \n-        next_decoder_cache = () if use_cache else None\n+        return_legacy_cache = False\n+        if use_cache and self.config.is_decoder and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n         for i, layer_module in enumerate(self.layer):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            past_key_value = past_key_values[i] if past_key_values is not None else None\n \n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n \n             hidden_states = layer_outputs[0]\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[-1],)\n             if output_attentions:\n                 all_self_attentions = all_self_attentions + (layer_outputs[1],)\n                 if self.config.add_cross_attention:\n@@ -570,12 +576,15 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_decoder_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_self_attentions,\n                     all_cross_attentions,\n@@ -584,7 +593,7 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_decoder_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -721,7 +730,7 @@ def load_tf_weights_in_dummy_bert(model, config, tf_checkpoint_path):\n \n @auto_docstring\n class DummyBertPreTrainedModel(PreTrainedModel):\n-    config_class = DummyBertConfig\n+    config: DummyBertConfig\n     load_tf_weights = load_tf_weights_in_dummy_bert\n     base_model_prefix = \"dummy_bert\"\n     supports_gradient_checkpointing = True\n@@ -810,6 +819,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -835,8 +845,13 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        past_key_values_length = 0\n+        if past_key_values is not None:\n+            past_key_values_length = (\n+                past_key_values[0][0].shape[-2]\n+                if not isinstance(past_key_values, Cache)\n+                else past_key_values.get_seq_length()\n+            )\n \n         if token_type_ids is None:\n             if hasattr(self.embeddings, \"token_type_ids\"):\n@@ -921,6 +936,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         sequence_output = encoder_outputs[0]\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None"
        },
        {
            "sha": "50f86222b38eedb66213794300426bb5121178d4",
            "filename": "examples/modular-transformers/modeling_global_indexing.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/examples%2Fmodular-transformers%2Fmodeling_global_indexing.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/examples%2Fmodular-transformers%2Fmodeling_global_indexing.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_global_indexing.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -14,6 +14,7 @@\n from ...cache_utils import Cache\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_global_indexing import GlobalIndexingConfig\n \n \n@@ -125,12 +126,13 @@ def __init__(self, config: GlobalIndexingConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -144,10 +146,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":"
        },
        {
            "sha": "bb011ee126b09cbd2eed328a748337cf4ac4ee1e",
            "filename": "examples/modular-transformers/modeling_multimodal2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -493,7 +493,7 @@ def forward(\n \n @auto_docstring\n class Multimodal2VisionPreTrainedModel(PreTrainedModel):\n-    config_class = Multimodal2Config\n+    config: Multimodal2Config\n     base_model_prefix = \"multimodal2_vision\"\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n@@ -512,7 +512,7 @@ def _init_weights(self, module):\n \n @add_start_docstrings(\"New doc\", MULTIMODAL2_VISION_START_DOCSTRING)\n class Multimodal2VisionModel(Multimodal2VisionPreTrainedModel):\n-    config_class = Multimodal2VisionConfig\n+    config: Multimodal2VisionConfig\n     main_input_name = \"pixel_values\"\n     _no_split_modules = [\"Multimodal2VisionEncoderLayer\"]\n "
        },
        {
            "sha": "4c1c8b0c0cb67bf5f9c2ddd50271bc06add825bc",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 15,
            "deletions": 243,
            "changes": 258,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -10,21 +10,15 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache\n-from ...masking_utils import create_causal_mask\n-from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import BaseModelOutputWithPast, SequenceClassifierOutputWithPast\n-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...cache_utils import Cache\n+from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.generic import check_model_inputs\n+from ...utils import TransformersKwargs, auto_docstring\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_my_new_model2 import MyNewModel2Config\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n class MyNewModel2RMSNorm(nn.Module):\n     def __init__(self, dim: int, eps: float = 1e-6):\n         super().__init__()\n@@ -61,40 +55,6 @@ def forward(self, x):\n         return down_proj\n \n \n-class MyNewModel2RotaryEmbedding(nn.Module):\n-    def __init__(self, config: MyNewModel2Config, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n@@ -193,12 +153,13 @@ def __init__(self, config: MyNewModel2Config, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -212,10 +173,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -248,12 +209,13 @@ def __init__(self, config: MyNewModel2Config, layer_idx: int):\n         self.input_layernorm = MyNewModel2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = MyNewModel2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -266,7 +228,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -284,212 +246,22 @@ def forward(\n \n @auto_docstring\n class MyNewModel2PreTrainedModel(PreTrainedModel):\n-    config_class = MyNewModel2Config\n+    config: MyNewModel2Config\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MyNewModel2DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": MyNewModel2DecoderLayer,\n         \"attentions\": MyNewModel2Attention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, MyNewModel2RMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n-\n-@auto_docstring\n-class MyNewModel2Model(MyNewModel2PreTrainedModel):\n-    def __init__(self, config: MyNewModel2Config):\n-        super().__init__(config)\n-        self.padding_idx = config.pad_token_id\n-        self.vocab_size = config.vocab_size\n-\n-        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n-        self.layers = nn.ModuleList(\n-            [MyNewModel2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n-        )\n-        self.norm = MyNewModel2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.rotary_emb = MyNewModel2RotaryEmbedding(config=config)\n-        self.gradient_checkpointing = False\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @check_model_inputs\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> BaseModelOutputWithPast:\n-        if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input_ids)\n-\n-        if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n-\n-        if cache_position is None:\n-            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n-            )\n-\n-        if position_ids is None:\n-            position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = create_causal_mask(\n-            config=self.config,\n-            input_embeds=inputs_embeds,\n-            attention_mask=attention_mask,\n-            cache_position=cache_position,\n-            past_key_values=past_key_values,\n-            position_ids=position_ids,\n-        )\n-\n-        # embed positions\n-        hidden_states = inputs_embeds\n-\n-        # create position embeddings to be shared across the decoder layers\n-        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n-\n-        # normalized\n-        # MyNewModel2 downcasts the below to float16, causing sqrt(3072)=55.4256 to become 55.5\n-        # See https://github.com/huggingface/transformers/pull/29402\n-        normalizer = torch.tensor(self.config.hidden_size**0.5, dtype=hidden_states.dtype)\n-        hidden_states = hidden_states * normalizer\n-\n-        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n-            hidden_states = decoder_layer(\n-                hidden_states,\n-                attention_mask=causal_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_values,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n-                **kwargs,\n-            )\n-        hidden_states = self.norm(hidden_states)\n-        return BaseModelOutputWithPast(\n-            last_hidden_state=hidden_states,\n-            past_key_values=past_key_values if use_cache else None,\n-        )\n-\n-\n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The MyNewModel2 Model transformer with a sequence classification head on top (linear layer).\n-\n-    [`MyNewModel2ForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n-\n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-class MyNewModel2ForSequenceClassification(MyNewModel2PreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = MyNewModel2Model(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n+class MyNewModel2ForSequenceClassification(GenericForSequenceClassification, MyNewModel2PreTrainedModel):\n+    pass"
        },
        {
            "sha": "eb35c2ade527b8761eb04c41a679432d956d1069",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 32,
            "deletions": 24,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -16,7 +16,7 @@\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import ModelOutput, auto_docstring, can_return_tuple, is_torchdynamo_compiling\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple\n from ..auto import AutoModel\n from .configuration_new_task_model import NewTaskModelConfig\n \n@@ -29,7 +29,7 @@\n )\n class NewTaskModelModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -55,7 +55,7 @@ class NewTaskModelCausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.text_config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -87,13 +87,12 @@ def forward(self, image_features):\n \n @auto_docstring\n class NewTaskModelPreTrainedModel(PreTrainedModel):\n-    config_class = NewTaskModelConfig\n+    config: NewTaskModelConfig\n     base_model_prefix = \"\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"NewTaskModelMultiModalProjector\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _can_compile_fullgraph = True\n     _supports_flash_attn = True\n     _supports_sdpa = True\n@@ -229,6 +228,30 @@ def get_image_features(self, pixel_values: torch.FloatTensor):\n         image_features = image_features / (self.config.text_config.hidden_size**0.5)\n         return image_features\n \n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        n_image_features = image_features.shape[0] * image_features.shape[1]\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -310,25 +333,10 @@ def forward(\n         # Merge text and images\n         if pixel_values is not None:\n             image_features = self.get_image_features(pixel_values)\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                image_tokens_in_text = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n-                raise ValueError(\n-                    f\"Number of images does not match number of special image tokens in the input text. \"\n-                    f\"Got {image_tokens_in_text} image tokens in the text but {image_features.shape[0] * image_features.shape[1]} \"\n-                    \"tokens from image embeddings.\"\n-                )\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         causal_mask = self._update_causal_mask("
        },
        {
            "sha": "68ce50d4529f210a92a70e7028be2e6fa4ce5245",
            "filename": "examples/modular-transformers/modeling_roberta.py",
            "status": "modified",
            "additions": 154,
            "deletions": 138,
            "changes": 292,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_roberta.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -13,12 +13,14 @@\n from packaging import version\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, BaseModelOutputWithPoolingAndCrossAttentions\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, get_torch_version, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_roberta import RobertaConfig\n \n \n@@ -93,7 +95,7 @@ def forward(\n \n \n class RobertaSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -118,66 +120,68 @@ def __init__(self, config, position_embedding_type=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.layer_idx = layer_idx\n \n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        mixed_query_layer = self.query(hidden_states)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = self.query(hidden_states)\n+        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+            1, 2\n+        )\n \n-        # If this is instantiated as a cross-attention module, the keys\n-        # and values come from an encoder; the attention mask needs to be\n-        # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_values.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_values\n \n-        if is_cross_attention and past_key_value is not None:\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value[0]\n-            value_layer = past_key_value[1]\n-            attention_mask = encoder_attention_mask\n-        elif is_cross_attention:\n-            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n-            attention_mask = encoder_attention_mask\n-        elif past_key_value is not None:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        use_cache = past_key_value is not None\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+            key_layer = self.key(current_states)\n+            key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+                1, 2\n+            )\n+            value_layer = self.value(current_states)\n+            value_layer = value_layer.view(\n+                batch_size, -1, self.num_attention_heads, self.attention_head_size\n+            ).transpose(1, 2)\n+\n+            if past_key_values is not None:\n+                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_layer, value_layer = curr_past_key_value.update(\n+                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if use_cache:\n+            if past_key_values is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -219,29 +223,26 @@ def forward(\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n         context_layer = context_layer.view(new_context_layer_shape)\n \n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n-        return outputs\n+        return context_layer, attention_probs\n \n \n class RobertaSdpaSelfAttention(RobertaSelfAttention):\n-    def __init__(self, config, position_embedding_type=None):\n-        super().__init__(config, position_embedding_type=position_embedding_type)\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+        super().__init__(config, position_embedding_type=position_embedding_type, layer_idx=layer_idx)\n         self.dropout_prob = config.attention_probs_dropout_prob\n         self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n \n     # Adapted from RobertaSelfAttention\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n         if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n@@ -257,41 +258,56 @@ def forward(\n                 attention_mask,\n                 head_mask,\n                 encoder_hidden_states,\n-                encoder_attention_mask,\n-                past_key_value,\n+                past_key_values,\n                 output_attentions,\n+                cache_position,\n             )\n \n         bsz, tgt_len, _ = hidden_states.size()\n \n-        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+        query_layer = (\n+            self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n+        )\n \n-        # If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\n-        # mask needs to be such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_values.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_values\n \n-        # Check `seq_length` of `past_key_value` == `len(current_states)` to support prefix tuning\n-        if is_cross_attention and past_key_value and past_key_value[0].shape[2] == current_states.shape[1]:\n-            key_layer, value_layer = past_key_value\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_values is not None and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n-            key_layer = self.transpose_for_scores(self.key(current_states))\n-            value_layer = self.transpose_for_scores(self.value(current_states))\n-            if past_key_value is not None and not is_cross_attention:\n-                key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-                value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+            key_layer = (\n+                self.key(current_states)\n+                .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n+                .transpose(1, 2)\n+            )\n+            value_layer = (\n+                self.value(current_states)\n+                .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n+                .transpose(1, 2)\n+            )\n+\n+            if past_key_values is not None:\n+                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_layer, value_layer = curr_past_key_value.update(\n+                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n         # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n@@ -305,9 +321,7 @@ def forward(\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\n         # a causal mask in case tgt_len == 1.\n-        is_causal = (\n-            True if self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1 else False\n-        )\n+        is_causal = self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1\n \n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_layer,\n@@ -321,10 +335,7 @@ def forward(\n         attn_output = attn_output.transpose(1, 2)\n         attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n \n-        outputs = (attn_output,)\n-        if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n-        return outputs\n+        return attn_output, None\n \n \n class RobertaSelfOutput(nn.Module):\n@@ -348,10 +359,12 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class RobertaAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         self.self = ROBERTA_SELF_ATTENTION_CLASSES[config._attn_implementation](\n-            config, position_embedding_type=position_embedding_type\n+            config,\n+            position_embedding_type=position_embedding_type,\n+            layer_idx=layer_idx,\n         )\n         self.output = RobertaSelfOutput(config)\n         self.pruned_heads = set()\n@@ -374,24 +387,25 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            past_key_value,\n-            output_attentions,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n@@ -428,83 +442,67 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class RobertaLayer(GradientCheckpointingLayer):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = RobertaAttention(config)\n+        self.attention = RobertaAttention(config, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = RobertaAttention(config, position_embedding_type=\"absolute\")\n+            self.crossattention = RobertaAttention(config, position_embedding_type=\"absolute\", layer_idx=layer_idx)\n         self.intermediate = RobertaIntermediate(config)\n         self.output = RobertaOutput(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_values=past_key_values,\n+            cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n+        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n \n-        # if decoder, the last output is tuple of self-attn cache\n-        if self.is_decoder:\n-            outputs = self_attention_outputs[1:-1]\n-            present_key_value = self_attention_outputs[-1]\n-        else:\n-            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n-\n-        cross_attn_present_key_value = None\n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n                 raise ValueError(\n                     f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n-                attention_mask,\n-                head_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n-                cross_attn_past_key_value,\n-                output_attentions,\n+                attention_mask=encoder_attention_mask,\n+                head_mask=head_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                past_key_values=past_key_values,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n-\n-            # add cross-attn cache to positions 3,4 of present_key_value tuple\n-            cross_attn_present_key_value = cross_attention_outputs[-1]\n-            present_key_value = present_key_value + cross_attn_present_key_value\n+            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n         outputs = (layer_output,) + outputs\n \n-        # if decoder, return the attn key/values as the last output\n-        if self.is_decoder:\n-            outputs = outputs + (present_key_value,)\n-\n         return outputs\n \n     def feed_forward_chunk(self, attention_output):\n@@ -514,10 +512,10 @@ def feed_forward_chunk(self, attention_output):\n \n \n class RobertaEncoder(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.config = config\n-        self.layer = nn.ModuleList([RobertaLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.layer = nn.ModuleList([RobertaLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n     def forward(\n@@ -532,6 +530,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n@@ -544,27 +543,34 @@ def forward(\n                 )\n                 use_cache = False\n \n-        next_decoder_cache = () if use_cache else None\n+        return_legacy_cache = False\n+        if use_cache and self.config.is_decoder and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n         for i, layer_module in enumerate(self.layer):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            past_key_value = past_key_values[i] if past_key_values is not None else None\n \n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n \n             hidden_states = layer_outputs[0]\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[-1],)\n             if output_attentions:\n                 all_self_attentions = all_self_attentions + (layer_outputs[1],)\n                 if self.config.add_cross_attention:\n@@ -573,12 +579,15 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_decoder_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_self_attentions,\n                     all_cross_attentions,\n@@ -587,7 +596,7 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_decoder_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -724,7 +733,7 @@ def load_tf_weights_in_roberta(model, config, tf_checkpoint_path):\n \n @auto_docstring\n class RobertaPreTrainedModel(PreTrainedModel):\n-    config_class = RobertaConfig\n+    config: RobertaConfig\n     load_tf_weights = load_tf_weights_in_roberta\n     base_model_prefix = \"roberta\"\n     supports_gradient_checkpointing = True\n@@ -813,6 +822,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -838,8 +848,13 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        past_key_values_length = 0\n+        if past_key_values is not None:\n+            past_key_values_length = (\n+                past_key_values[0][0].shape[-2]\n+                if not isinstance(past_key_values, Cache)\n+                else past_key_values.get_seq_length()\n+            )\n \n         if token_type_ids is None:\n             if hasattr(self.embeddings, \"token_type_ids\"):\n@@ -924,6 +939,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         sequence_output = encoder_outputs[0]\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None"
        },
        {
            "sha": "6927dab86dc1843327199334211a5abb7480c04f",
            "filename": "examples/modular-transformers/modeling_super.py",
            "status": "modified",
            "additions": 10,
            "deletions": 21,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_super.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -19,6 +19,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_super import SuperConfig\n \n@@ -192,12 +193,13 @@ def __init__(self, config: SuperConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -211,10 +213,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -247,12 +249,13 @@ def __init__(self, config: SuperConfig, layer_idx: int):\n         self.input_layernorm = SuperRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = SuperRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -265,7 +268,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -283,36 +286,22 @@ def forward(\n \n @auto_docstring\n class SuperPreTrainedModel(PreTrainedModel):\n-    config_class = SuperConfig\n+    config: SuperConfig\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"SuperDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": SuperDecoderLayer,\n         \"attentions\": SuperAttention,\n     }\n \n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, SuperRMSNorm):\n-            module.weight.data.fill_(1.0)\n-\n \n @auto_docstring\n class SuperModel(SuperPreTrainedModel):"
        },
        {
            "sha": "6e8ffed806b16e76909cefbde3e8cdfcc83bd89b",
            "filename": "examples/modular-transformers/modeling_switch_function.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/examples%2Fmodular-transformers%2Fmodeling_switch_function.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/examples%2Fmodular-transformers%2Fmodeling_switch_function.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_switch_function.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -14,6 +14,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_switch_function import SwitchFunctionConfig\n \n \n@@ -116,12 +117,13 @@ def __init__(self, config: SwitchFunctionConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -135,10 +137,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":"
        },
        {
            "sha": "ac2d45a8635cb986d390395e47adccc515155476",
            "filename": "examples/modular-transformers/modeling_test_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/examples%2Fmodular-transformers%2Fmodeling_test_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/examples%2Fmodular-transformers%2Fmodeling_test_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_test_detr.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -803,7 +803,7 @@ def forward(\n \n @auto_docstring\n class TestDetrPreTrainedModel(PreTrainedModel):\n-    config_class = TestDetrConfig\n+    config: TestDetrConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True"
        },
        {
            "sha": "fb7440228d8cbbcd89a5c3e452385e4886d6f038",
            "filename": "examples/modular-transformers/modular_dummy_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/examples%2Fmodular-transformers%2Fmodular_dummy_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/examples%2Fmodular-transformers%2Fmodular_dummy_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_dummy_bert.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -23,5 +23,6 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         return super().forward(input_ids)"
        },
        {
            "sha": "bb5aac99b33b638729ffd97c33a115dc90425453",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -1112,7 +1112,7 @@ def __init__(\n \n     def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n-        Support for backwards-compatible `past_key_value` indexing, e.g. `past_key_value[0][0].shape[2]` to get the\n+        Support for backwards-compatible `past_key_values` indexing, e.g. `past_key_values[0][0].shape[2]` to get the\n         sequence length.\n         \"\"\"\n         if layer_idx < len(self.layers):\n@@ -1124,15 +1124,15 @@ def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n \n     def __iter__(self):\n         \"\"\"\n-        Support for backwards-compatible `past_key_value` iteration, e.g. `for x in past_key_value:` to iterate over\n+        Support for backwards-compatible `past_key_values` iteration, e.g. `for x in past_key_values:` to iterate over\n         keys and values\n         \"\"\"\n         for layer_idx in range(len(self)):\n             yield (self.layers[layer_idx].keys, self.layers[layer_idx].values)\n \n     def __len__(self):\n         \"\"\"\n-        Support for backwards-compatible `past_key_value` length, e.g. `len(past_key_value)`. This value corresponds\n+        Support for backwards-compatible `past_key_values` length, e.g. `len(past_key_values)`. This value corresponds\n         to the number of layers in the model.\n         \"\"\"\n         # Best effort BC support for old-style caches like Mambas, Falcon, HybridChunked that rely on __len__\n@@ -1742,7 +1742,7 @@ def __init__(self, self_attention_cache: Cache, cross_attention_cache: Cache):\n \n     def __iter__(self):\n         \"\"\"\n-        Support for backwards-compatible `past_key_value` iteration, e.g. `for x in past_key_value:` to iterate over\n+        Support for backwards-compatible `past_key_values` iteration, e.g. `for x in past_key_values:` to iterate over\n         keys and values\n         \"\"\"\n         for layer_idx in range(len(self)):\n@@ -1755,7 +1755,7 @@ def __iter__(self):\n \n     def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n         \"\"\"\n-        Support for backwards-compatible `past_key_value` indexing, e.g. `past_key_value[0][0].shape[2]` to get the\n+        Support for backwards-compatible `past_key_values` indexing, e.g. `past_key_values[0][0].shape[2]` to get the\n         sequence length.\n         \"\"\"\n         if layer_idx < len(self):\n@@ -1770,7 +1770,7 @@ def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor, torch\n \n     def __len__(self):\n         \"\"\"\n-        Support for backwards-compatible `past_key_value` length, e.g. `len(past_key_value)`. This value corresponds\n+        Support for backwards-compatible `past_key_values` length, e.g. `len(past_key_values)`. This value corresponds\n         to the number of layers in the model.\n         \"\"\"\n         return len(self.self_attention_cache)"
        },
        {
            "sha": "7e29ab1886fc4bf8b2d03bc4f07a2c6807ff7d13",
            "filename": "src/transformers/modeling_layers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodeling_layers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodeling_layers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_layers.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -70,6 +70,7 @@ def __call__(self, *args, **kwargs):\n                 do_warn = True\n \n             # different names for the same thing in different layers\n+            # TODO cyril: this one without `S` can be removed after deprection cycle\n             if \"past_key_value\" in kwargs and kwargs[\"past_key_value\"] is not None:\n                 kwargs[\"past_key_value\"] = None\n                 message += \" `past_key_value=None`,\""
        },
        {
            "sha": "661d3a72dc0739c0dfc809514836ae102ac1c00d",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -42,6 +42,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_arcee import ArceeConfig\n \n@@ -215,12 +216,13 @@ def __init__(self, config: ArceeConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -234,10 +236,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -270,12 +272,13 @@ def __init__(self, config: ArceeConfig, layer_idx: int):\n         self.input_layernorm = ArceeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = ArceeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -288,7 +291,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -389,7 +392,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n                 **kwargs,"
        },
        {
            "sha": "bfb2e6b194bc8988c293bfdd765f58ac41cf5eec",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -33,6 +33,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from ...utils.import_utils import is_torch_available\n from ..auto import AutoModel\n@@ -522,12 +523,13 @@ def __init__(self, config: AriaTextConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -541,10 +543,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -588,12 +590,13 @@ def __init__(self, config: AriaTextConfig, layer_idx: int):\n         self.input_layernorm = AriaTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = AriaTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -606,7 +609,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -769,7 +772,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n                 **kwargs,"
        },
        {
            "sha": "bc0e5faf5965a09bfd718323c352646cc4c385ad",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 18,
            "deletions": 15,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -36,6 +36,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_autoformer import AutoformerConfig\n \n \n@@ -451,11 +452,12 @@ def __init__(\n \n         self.autocorrelation_factor = autocorrelation_factor\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n@@ -471,19 +473,19 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states)\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -493,15 +495,15 @@ def forward(\n             key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n             value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n         query_states = query_states.view(bsz, tgt_len, self.num_heads, self.head_dim).transpose(1, 2)\n@@ -751,6 +753,7 @@ def __init__(self, config: AutoformerConfig, layer_idx=None):\n             bias=False,\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -759,7 +762,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -777,7 +780,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -790,7 +793,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n@@ -812,7 +815,7 @@ def forward(\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -1205,7 +1208,7 @@ def forward(\n                 encoder_attention_mask=encoder_attention_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "eaae2133b66ba050887bb4230b071db710909427",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -41,6 +41,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_bamba import BambaConfig\n \n@@ -336,12 +337,13 @@ def __init__(self, config: BambaConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -355,10 +357,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -1003,12 +1005,13 @@ def __init__(self, config: BambaConfig, layer_idx: int, layer_type: str = \"mamba\n         else:\n             raise ValueError(\"Invalid layer_type\")\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[HybridMambaAttentionDynamicCache] = None,\n+        past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -1020,7 +1023,7 @@ def forward(\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n             attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n                 `(batch, sequence_length)` where padding elements are indicated by 0.\n-            past_key_value (`HybridMambaAttentionDynamicCache`, *optional*): cached past key and value projection states\n+            past_key_values (`HybridMambaAttentionDynamicCache`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -1045,7 +1048,7 @@ def forward(\n         if self.layer_type == \"mamba\":\n             hidden_states = self.mamba(\n                 hidden_states=hidden_states,\n-                cache_params=past_key_value,\n+                cache_params=past_key_values,\n                 cache_position=cache_position,\n                 attention_mask=attention_mask,\n                 **kwargs,\n@@ -1056,7 +1059,7 @@ def forward(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -1189,7 +1192,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=layer_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "95c5ce8e36d7db35d0c1c0a9d9ad3f3cdfbbf468",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -52,6 +52,7 @@\n     can_return_tuple,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_bamba import BambaConfig\n \n@@ -725,12 +726,13 @@ def __init__(self, config: BambaConfig, layer_idx: int, layer_type: str = \"mamba\n         else:\n             raise ValueError(\"Invalid layer_type\")\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[HybridMambaAttentionDynamicCache] = None,\n+        past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -742,7 +744,7 @@ def forward(\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n             attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n                 `(batch, sequence_length)` where padding elements are indicated by 0.\n-            past_key_value (`HybridMambaAttentionDynamicCache`, *optional*): cached past key and value projection states\n+            past_key_values (`HybridMambaAttentionDynamicCache`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -767,7 +769,7 @@ def forward(\n         if self.layer_type == \"mamba\":\n             hidden_states = self.mamba(\n                 hidden_states=hidden_states,\n-                cache_params=past_key_value,\n+                cache_params=past_key_values,\n                 cache_position=cache_position,\n                 attention_mask=attention_mask,\n                 **kwargs,\n@@ -778,7 +780,7 @@ def forward(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -911,7 +913,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=layer_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "de20331e829e34ddcf81ff25f85732ff38530ae5",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 19,
            "deletions": 16,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -50,6 +50,7 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_bart import BartConfig\n \n \n@@ -186,11 +187,12 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -215,19 +217,19 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -237,15 +239,15 @@ def forward(\n             key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n             value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -373,6 +375,7 @@ def __init__(self, config: BartConfig, layer_idx: Optional[int] = None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -381,7 +384,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -399,7 +402,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -412,7 +415,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n@@ -432,7 +435,7 @@ def forward(\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -1119,7 +1122,7 @@ def forward(\n                 encoder_attention_mask=encoder_attention_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -1284,7 +1287,7 @@ def forward(\n                 attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n             )\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,"
        },
        {
            "sha": "04323c7ec4ae290768991516d905fad1132c294c",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 33,
            "deletions": 28,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -46,6 +46,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, get_torch_version, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_bert import BertConfig\n \n \n@@ -217,13 +218,14 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -234,19 +236,19 @@ def forward(\n         )\n \n         is_cross_attention = encoder_hidden_states is not None\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_layer = curr_past_key_value.layers[self.layer_idx].keys\n             value_layer = curr_past_key_value.layers[self.layer_idx].values\n@@ -260,22 +262,22 @@ def forward(\n                 batch_size, -1, self.num_attention_heads, self.attention_head_size\n             ).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_layer, value_layer = curr_past_key_value.update(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -327,13 +329,14 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n \n     # Adapted from BertSelfAttention\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -351,7 +354,7 @@ def forward(\n                 attention_mask,\n                 head_mask,\n                 encoder_hidden_states,\n-                past_key_value,\n+                past_key_values,\n                 output_attentions,\n                 cache_position,\n             )\n@@ -364,19 +367,19 @@ def forward(\n \n         is_cross_attention = encoder_hidden_states is not None\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_layer = curr_past_key_value.layers[self.layer_idx].keys\n             value_layer = curr_past_key_value.layers[self.layer_idx].values\n@@ -392,15 +395,15 @@ def forward(\n                 .transpose(1, 2)\n             )\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_layer, value_layer = curr_past_key_value.update(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n         # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n@@ -480,13 +483,14 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -495,7 +499,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -548,14 +552,15 @@ def __init__(self, config, layer_idx=None):\n         self.intermediate = BertIntermediate(config)\n         self.output = BertOutput(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -564,7 +569,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n@@ -582,7 +587,7 @@ def forward(\n                 attention_mask=encoder_attention_mask,\n                 head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -656,7 +661,7 @@ def forward(\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )"
        },
        {
            "sha": "d582a914395a4c23c2bde8cdfeaf823dc5b20fa9",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 21,
            "deletions": 17,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -29,6 +29,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_bert_generation import BertGenerationConfig\n \n \n@@ -79,13 +80,14 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -96,19 +98,19 @@ def forward(\n         )\n \n         is_cross_attention = encoder_hidden_states is not None\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_layer = curr_past_key_value.layers[self.layer_idx].keys\n             value_layer = curr_past_key_value.layers[self.layer_idx].values\n@@ -122,22 +124,22 @@ def forward(\n                 batch_size, -1, self.num_attention_heads, self.attention_head_size\n             ).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_layer, value_layer = curr_past_key_value.update(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -217,13 +219,14 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -232,7 +235,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -290,14 +293,15 @@ def __init__(self, config, layer_idx=None):\n         self.intermediate = BertGenerationIntermediate(config)\n         self.output = BertGenerationOutput(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -306,7 +310,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n@@ -324,7 +328,7 @@ def forward(\n                 attention_mask=encoder_attention_mask,\n                 head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -398,7 +402,7 @@ def forward(\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )"
        },
        {
            "sha": "97f6187a1f98ec08a821a055d2b0e735062f9eb9",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 15,
            "deletions": 11,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -41,6 +41,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import ModelOutput, auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_big_bird import BigBirdConfig\n \n \n@@ -315,14 +316,15 @@ def __init__(self, config, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         output_attentions=False,\n         cache_position=None,\n     ):\n@@ -336,10 +338,10 @@ def forward(\n         is_cross_attention = encoder_hidden_states is not None\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n-        if is_cross_attention and past_key_value is not None and past_key_value.get_seq_length(self.layer_idx) > 0:\n+        if is_cross_attention and past_key_values is not None and past_key_values.get_seq_length(self.layer_idx) > 0:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value.layers[self.layer_idx].keys\n-            value_layer = past_key_value.layers[self.layer_idx].values\n+            key_layer = past_key_values.layers[self.layer_idx].keys\n+            value_layer = past_key_values.layers[self.layer_idx].values\n         else:\n             key_layer = (\n                 self.key(current_states)\n@@ -352,9 +354,9 @@ def forward(\n                 .transpose(1, 2)\n             )\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-                key_layer, value_layer = past_key_value.update(\n+                key_layer, value_layer = past_key_values.update(\n                     key_layer,\n                     value_layer,\n                     self.layer_idx,\n@@ -1338,14 +1340,15 @@ def set_attention_type(self, value: str, layer_idx=None):\n         if not self.training:\n             self.self.eval()\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         output_attentions=False,\n         # block_sparse config\n         band_mask=None,\n@@ -1369,7 +1372,7 @@ def forward(\n                 head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -1447,6 +1450,7 @@ def set_attention_type(self, value: str, layer_idx=None):\n         if self.add_cross_attention:\n             self.crossattention.set_attention_type(value, layer_idx=layer_idx)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -1458,7 +1462,7 @@ def forward(\n         from_mask=None,\n         to_mask=None,\n         blocked_encoder_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         output_attentions=False,\n         cache_position=None,\n     ):\n@@ -1469,7 +1473,7 @@ def forward(\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             band_mask=band_mask,\n             from_mask=from_mask,\n@@ -1493,7 +1497,7 @@ def forward(\n                 attention_mask=encoder_attention_mask,\n                 head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )"
        },
        {
            "sha": "9af25afd797fc6cf468f0a1f43a8f65c8798ea2f",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 25,
            "deletions": 22,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -44,6 +44,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, is_torch_flex_attn_available, is_torchdynamo_compiling, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_bigbird_pegasus import BigBirdPegasusConfig\n \n \n@@ -127,14 +128,15 @@ def __init__(self, config, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         output_attentions=False,\n         cache_position=None,\n     ):\n@@ -148,10 +150,10 @@ def forward(\n         is_cross_attention = encoder_hidden_states is not None\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n-        if is_cross_attention and past_key_value is not None and past_key_value.get_seq_length(self.layer_idx) > 0:\n+        if is_cross_attention and past_key_values is not None and past_key_values.get_seq_length(self.layer_idx) > 0:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value.layers[self.layer_idx].keys\n-            value_layer = past_key_value.layers[self.layer_idx].values\n+            key_layer = past_key_values.layers[self.layer_idx].keys\n+            value_layer = past_key_values.layers[self.layer_idx].values\n         else:\n             key_layer = (\n                 self.key(current_states)\n@@ -164,9 +166,9 @@ def forward(\n                 .transpose(1, 2)\n             )\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-                key_layer, value_layer = past_key_value.update(\n+                key_layer, value_layer = past_key_values.update(\n                     key_layer,\n                     value_layer,\n                     self.layer_idx,\n@@ -1244,11 +1246,12 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -1273,19 +1276,19 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -1295,15 +1298,15 @@ def forward(\n             key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n             value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -1456,7 +1459,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -1474,7 +1477,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -1488,7 +1491,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n@@ -1508,7 +1511,7 @@ def forward(\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n@@ -2275,7 +2278,7 @@ def forward(\n                 encoder_attention_mask=encoder_attention_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -2426,7 +2429,7 @@ def forward(\n                 attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n             )\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,"
        },
        {
            "sha": "bba29c892fce9420e15fb1b9e219759007281c04",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 17,
            "deletions": 14,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -41,6 +41,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_biogpt import BioGptConfig\n \n \n@@ -164,11 +165,12 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -193,19 +195,19 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -215,15 +217,15 @@ def forward(\n             key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n             value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -272,12 +274,13 @@ def __init__(self, config: BioGptConfig, layer_idx: Optional[int] = None):\n         self.fc2 = nn.Linear(config.intermediate_size, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -291,7 +294,7 @@ def forward(\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n             layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n                 `(encoder_attention_heads,)`.\n-            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -309,7 +312,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n@@ -624,7 +627,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 position_ids=position_ids,"
        },
        {
            "sha": "7b29640cd8f2f9a4fe5fd98b67bc39ca467787a6",
            "filename": "src/transformers/models/biogpt/modular_biogpt.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -42,6 +42,7 @@\n     is_torch_flex_attn_available,\n     logger,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ..bart.modeling_bart import (\n     BartAttention,\n     BartDecoderLayer,\n@@ -97,12 +98,13 @@ def __init__(self, config: BioGptConfig, layer_idx: Optional[int] = None):\n         del self.encoder_attn\n         del self.encoder_attn_layer_norm\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -116,7 +118,7 @@ def forward(\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n             layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n                 `(encoder_attention_heads,)`.\n-            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -134,7 +136,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n@@ -449,7 +451,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 position_ids=position_ids,"
        },
        {
            "sha": "6c4d8e21e2f48867110eb4238f1ea236f2710207",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -35,6 +35,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_bitnet import BitNetConfig\n \n@@ -176,12 +177,13 @@ def __init__(self, config: BitNetConfig, layer_idx: int):\n         )\n         self.attn_sub_norm = BitNetRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -195,10 +197,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n \n@@ -233,12 +235,13 @@ def __init__(self, config: BitNetConfig, layer_idx: int):\n         self.input_layernorm = BitNetRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = BitNetRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -251,7 +254,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -388,7 +391,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n                 **kwargs,"
        },
        {
            "sha": "92ad3a32145992eb5b8a6bbac0af97f079ef2352",
            "filename": "src/transformers/models/bitnet/modular_bitnet.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodular_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodular_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodular_bitnet.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -23,6 +23,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..gemma.modeling_gemma import GemmaMLP\n from ..llama.modeling_llama import (\n     LlamaAttention,\n@@ -58,12 +59,13 @@ def __init__(self, config: BitNetConfig, layer_idx: int):\n         super().__init__(config, layer_idx)\n         self.attn_sub_norm = BitNetRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -77,10 +79,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n "
        },
        {
            "sha": "78dd0223bc71c347555fc8223e2100c29ba9f309",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 19,
            "deletions": 16,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -49,6 +49,7 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ..blenderbot_small import BlenderbotSmallForConditionalGeneration, BlenderbotSmallModel\n from .configuration_blenderbot import BlenderbotConfig\n \n@@ -185,11 +186,12 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -214,19 +216,19 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -236,15 +238,15 @@ def forward(\n             key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n             value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -366,6 +368,7 @@ def __init__(self, config: BlenderbotConfig, layer_idx: Optional[int] = None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -374,7 +377,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -392,7 +395,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -406,7 +409,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n@@ -426,7 +429,7 @@ def forward(\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n@@ -1073,7 +1076,7 @@ def forward(\n                 encoder_attention_mask=encoder_attention_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -1237,7 +1240,7 @@ def forward(\n                 attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n             )\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,"
        },
        {
            "sha": "212c7cb135a3734cdde58a665ebd3ba15b4e4060",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 19,
            "deletions": 16,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -47,6 +47,7 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_blenderbot_small import BlenderbotSmallConfig\n \n \n@@ -169,11 +170,12 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -198,19 +200,19 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -220,15 +222,15 @@ def forward(\n             key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n             value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -358,6 +360,7 @@ def __init__(self, config: BlenderbotSmallConfig, layer_idx: Optional[int] = Non\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -366,7 +369,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -384,7 +387,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -397,7 +400,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n@@ -417,7 +420,7 @@ def forward(\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -1061,7 +1064,7 @@ def forward(\n                 encoder_attention_mask=encoder_attention_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -1209,7 +1212,7 @@ def forward(\n                 attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n             )\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,"
        },
        {
            "sha": "4a58d70eb0c772075326de3f8b4784456ecdd3bf",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -438,7 +438,7 @@ class BlipPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"blip\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"BlipEncoderLayer\", \"BlipTextEmbeddings\"]\n-    _skip_keys_device_placement = [\"past_key_value\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "a75582200be4ee52b31636c876bdce71407d6db1",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 19,
            "deletions": 15,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -38,6 +38,7 @@\n     prune_linear_layer,\n )\n from ...utils import logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_blip import BlipTextConfig\n \n \n@@ -138,14 +139,15 @@ def save_attention_map(self, attention_map):\n     def get_attention_map(self):\n         return self.attention_map\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -162,19 +164,19 @@ def forward(\n         is_cross_attention = encoder_hidden_states is not None\n         attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_layer = curr_past_key_value.layers[self.layer_idx].keys\n             value_layer = curr_past_key_value.layers[self.layer_idx].values\n@@ -190,15 +192,15 @@ def forward(\n                 .transpose(1, 2)\n             )\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_layer, value_layer = curr_past_key_value.update(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n@@ -285,13 +287,14 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -300,7 +303,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -355,14 +358,15 @@ def __init__(self, config, layer_num):\n         self.intermediate = BlipTextIntermediate(config)\n         self.output = BlipTextOutput(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -371,7 +375,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n@@ -383,7 +387,7 @@ def forward(\n                 attention_mask=encoder_attention_mask,\n                 head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )"
        },
        {
            "sha": "b547d160ab0ff1d9a01666930e0b949ead023704",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 25,
            "deletions": 20,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -37,6 +37,7 @@\n from ...modeling_utils import PreTrainedModel, apply_chunking_to_forward\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging, torch_int\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_bridgetower import BridgeTowerConfig, BridgeTowerTextConfig, BridgeTowerVisionConfig\n \n \n@@ -429,13 +430,14 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -446,19 +448,19 @@ def forward(\n         )\n \n         is_cross_attention = encoder_hidden_states is not None\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_layer = curr_past_key_value.layers[self.layer_idx].keys\n             value_layer = curr_past_key_value.layers[self.layer_idx].values\n@@ -472,22 +474,22 @@ def forward(\n                 batch_size, -1, self.num_attention_heads, self.attention_head_size\n             ).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_layer, value_layer = curr_past_key_value.update(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -567,13 +569,14 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -582,7 +585,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -603,14 +606,15 @@ def __init__(self, config, layer_idx=None):\n         self.intermediate = BridgeTowerIntermediate(config)\n         self.output = BridgeTowerOutput(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         encoder_hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_attention_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         output_attentions=False,\n         cache_position=None,\n     ):\n@@ -620,7 +624,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=None,\n             output_attentions=output_attentions,\n-            past_key_value=None,\n+            past_key_values=None,\n         )\n         attention_output = self_attention_outputs[0]\n \n@@ -633,7 +637,7 @@ def forward(\n             attention_mask=encoder_attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -669,14 +673,15 @@ def __init__(self, config, layer_idx=None):\n         self.intermediate = BridgeTowerIntermediate(config)\n         self.output = BridgeTowerOutput(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -686,7 +691,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n@@ -709,7 +714,7 @@ def forward(\n                 attention_mask=encoder_attention_mask,\n                 head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -784,7 +789,7 @@ def forward(\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )"
        },
        {
            "sha": "746112afef26ddac841ab3e235bdcdda4cad21df",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 33,
            "deletions": 28,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -42,6 +42,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, get_torch_version, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_camembert import CamembertConfig\n \n \n@@ -167,13 +168,14 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -184,19 +186,19 @@ def forward(\n         )\n \n         is_cross_attention = encoder_hidden_states is not None\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_layer = curr_past_key_value.layers[self.layer_idx].keys\n             value_layer = curr_past_key_value.layers[self.layer_idx].values\n@@ -210,22 +212,22 @@ def forward(\n                 batch_size, -1, self.num_attention_heads, self.attention_head_size\n             ).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_layer, value_layer = curr_past_key_value.update(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -278,13 +280,14 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n \n     # Adapted from CamembertSelfAttention\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -302,7 +305,7 @@ def forward(\n                 attention_mask,\n                 head_mask,\n                 encoder_hidden_states,\n-                past_key_value,\n+                past_key_values,\n                 output_attentions,\n                 cache_position,\n             )\n@@ -315,19 +318,19 @@ def forward(\n \n         is_cross_attention = encoder_hidden_states is not None\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_layer = curr_past_key_value.layers[self.layer_idx].keys\n             value_layer = curr_past_key_value.layers[self.layer_idx].values\n@@ -343,15 +346,15 @@ def forward(\n                 .transpose(1, 2)\n             )\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_layer, value_layer = curr_past_key_value.update(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n         # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n@@ -433,13 +436,14 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -448,7 +452,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -504,14 +508,15 @@ def __init__(self, config, layer_idx=None):\n         self.intermediate = CamembertIntermediate(config)\n         self.output = CamembertOutput(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -520,7 +525,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n@@ -538,7 +543,7 @@ def forward(\n                 attention_mask=encoder_attention_mask,\n                 head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -613,7 +618,7 @@ def forward(\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )"
        },
        {
            "sha": "70818f824c7cfb5023b7166bc6aac61c210c9d7d",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 14,
            "deletions": 10,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -37,6 +37,7 @@\n     can_return_tuple,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_chameleon import ChameleonConfig, ChameleonVQVAEConfig\n \n \n@@ -311,12 +312,13 @@ def _init_rope(self):\n             else:\n                 raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -341,10 +343,10 @@ def forward(\n         cos, sin = self.rotary_emb(value_states, position_ids)\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; position_ids needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -379,12 +381,13 @@ def __init__(self, config: ChameleonConfig, layer_idx: int):\n         self.input_layernorm = ChameleonRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = ChameleonRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -402,7 +405,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             kwargs (`dict`, *optional*):\n@@ -418,7 +421,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -451,12 +454,13 @@ def __init__(self, config: ChameleonConfig, layer_idx: int):\n         self.input_layernorm = ChameleonRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = ChameleonRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -471,7 +475,7 @@ def forward(\n                 query_sequence_length, key_sequence_length)` if default attention is used.\n             position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Indices of positions of each input sequence tokens in the position embeddings\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -489,7 +493,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -992,7 +996,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "314c18c4d03802756029fa4faa4446c48dbdd3bd",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -42,6 +42,7 @@\n     auto_docstring,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_clvp import (\n     ClvpConfig,\n     ClvpDecoderConfig,\n@@ -298,13 +299,14 @@ def __init__(self, config, layer_idx=None):\n     def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.FloatTensor,\n         rotary_pos_emb: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n@@ -322,8 +324,8 @@ def forward(\n         key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n         value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n \n-        if past_key_value is not None:\n-            key_states, value_states = past_key_value.update(\n+        if past_key_values is not None:\n+            key_states, value_states = past_key_values.update(\n                 key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n             )\n \n@@ -607,10 +609,11 @@ def __init__(self, config, layer_idx=None):\n \n         self.mlp = ClvpDecoderMLP(inner_dim, config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.FloatTensor]],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n@@ -622,7 +625,7 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n         attn_outputs = self.attn(\n             hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             head_mask=head_mask,\n@@ -1128,7 +1131,7 @@ def forward(\n             else:\n                 outputs = block(\n                     hidden_states,\n-                    past_key_value=past_key_values,\n+                    past_key_values=past_key_values,\n                     attention_mask=attention_mask,\n                     position_ids=position_ids,\n                     head_mask=head_mask[i],\n@@ -1213,7 +1216,7 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.decoder(\n             input_ids=input_ids,\n             attention_mask=attention_mask,"
        },
        {
            "sha": "79cb07e0b8fc0269aaaade623f13f2eb16430edc",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -43,6 +43,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_cohere import CohereConfig\n \n@@ -227,12 +228,13 @@ def __init__(self, config: CohereConfig, layer_idx: Optional[int] = None):\n                 hidden_size=(config.num_key_value_heads, self.head_dim), eps=config.layer_norm_eps\n             )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -254,10 +256,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; position_ids needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -287,12 +289,13 @@ def __init__(self, config: CohereConfig, layer_idx: int):\n         self.mlp = CohereMLP(config)\n         self.input_layernorm = CohereLayerNorm(hidden_size=(config.hidden_size), eps=config.layer_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -304,7 +307,7 @@ def forward(\n             attention_mask (`torch.FloatTensor`, *optional*):\n                 attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n                 query_sequence_length, key_sequence_length)` if default attention is used.\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -324,7 +327,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -421,7 +424,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n                 **kwargs,"
        },
        {
            "sha": "4f05fedc986e44e043c64cbd8215b56481737dbb",
            "filename": "src/transformers/models/cohere/modular_cohere.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -36,6 +36,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..llama.modeling_llama import (\n     LlamaAttention,\n     LlamaForCausalLM,\n@@ -145,12 +146,13 @@ def __init__(self, config: CohereConfig, layer_idx: Optional[int] = None):\n                 hidden_size=(config.num_key_value_heads, self.head_dim), eps=config.layer_norm_eps\n             )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -172,10 +174,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; position_ids needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -205,12 +207,13 @@ def __init__(self, config: CohereConfig, layer_idx: int):\n         self.mlp = CohereMLP(config)\n         self.input_layernorm = CohereLayerNorm(hidden_size=(config.hidden_size), eps=config.layer_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -222,7 +225,7 @@ def forward(\n             attention_mask (`torch.FloatTensor`, *optional*):\n                 attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n                 query_sequence_length, key_sequence_length)` if default attention is used.\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -242,7 +245,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,"
        },
        {
            "sha": "8ab3952088d4d11930c8b0fb495d22fe0512cb50",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -35,6 +35,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_cohere2 import Cohere2Config\n \n@@ -195,12 +196,13 @@ def __init__(self, config: Cohere2Config, layer_idx: Optional[int] = None):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -215,9 +217,9 @@ def forward(\n         if self.sliding_window is not None:\n             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -265,12 +267,13 @@ def __init__(self, config: Cohere2Config, layer_idx: int):\n         self.input_layernorm = Cohere2LayerNorm(hidden_size=(config.hidden_size), eps=config.layer_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n@@ -281,7 +284,7 @@ def forward(\n             attention_mask (`torch.FloatTensor`, *optional*):\n                 attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n                 query_sequence_length, key_sequence_length)` if default attention is used.\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -300,7 +303,7 @@ def forward(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n@@ -400,7 +403,7 @@ def forward(\n                 hidden_states,\n                 position_embeddings=position_embeddings,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,"
        },
        {
            "sha": "f232862421d10ff9fbf2b3b48e1c5faa6b464ef6",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -28,6 +28,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..cohere.modeling_cohere import (\n     CohereAttention,\n     CohereDecoderLayer,\n@@ -297,12 +298,13 @@ def __init__(self, config: Cohere2Config, layer_idx: Optional[int] = None):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -317,9 +319,9 @@ def forward(\n         if self.sliding_window is not None:\n             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -347,12 +349,13 @@ def __init__(self, config: Cohere2Config, layer_idx: int):\n         super().__init__(config, layer_idx)\n         self.attention_type = config.layer_types[layer_idx]\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n@@ -363,7 +366,7 @@ def forward(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n@@ -434,7 +437,7 @@ def forward(\n                 hidden_states,\n                 position_embeddings=position_embeddings,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,"
        },
        {
            "sha": "8bcc3b028c29157a1f3d0417dd6cbf1107fd2bd2",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -38,6 +38,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel\n from .configuration_csm import CsmConfig, CsmDepthDecoderConfig\n from .generation_csm import CsmGenerationMixin\n@@ -267,12 +268,13 @@ def __init__(self, config: CsmConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -286,10 +288,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -322,12 +324,13 @@ def __init__(self, config: CsmConfig, layer_idx: int):\n         self.input_layernorm = CsmRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = CsmRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -340,7 +343,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -481,7 +484,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n@@ -727,7 +730,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n                 **kwargs,"
        },
        {
            "sha": "bccd3b4d3a15ab74b7e4829121223211c09ba669",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -231,7 +231,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,"
        },
        {
            "sha": "e6754770d0386eba4b0bb9b0868a015de24c3245",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 21,
            "deletions": 17,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -39,6 +39,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_data2vec_text import Data2VecTextConfig\n \n \n@@ -167,13 +168,14 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -184,19 +186,19 @@ def forward(\n         )\n \n         is_cross_attention = encoder_hidden_states is not None\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_layer = curr_past_key_value.layers[self.layer_idx].keys\n             value_layer = curr_past_key_value.layers[self.layer_idx].values\n@@ -210,22 +212,22 @@ def forward(\n                 batch_size, -1, self.num_attention_heads, self.attention_head_size\n             ).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_layer, value_layer = curr_past_key_value.update(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -320,13 +322,14 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -335,7 +338,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -393,14 +396,15 @@ def __init__(self, config, layer_idx=None):\n         self.intermediate = Data2VecTextIntermediate(config)\n         self.output = Data2VecTextOutput(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -409,7 +413,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n@@ -427,7 +431,7 @@ def forward(\n                 attention_mask=encoder_attention_mask,\n                 head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -502,7 +506,7 @@ def forward(\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )"
        },
        {
            "sha": "4bd02e8b8df4ebc0ee2c8e0e671232c5d54de8c2",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 23,
            "deletions": 17,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -30,6 +30,7 @@\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_dbrx import DbrxConfig\n \n \n@@ -239,12 +240,13 @@ def __init__(self, config: DbrxConfig, block_idx: Optional[int] = None):\n             base=self.rope_theta,\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_ids: torch.LongTensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -273,10 +275,10 @@ def forward(\n         cos, sin = self.rotary_emb(value_states, position_ids)\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; position_ids needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.block_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.block_idx, cache_kwargs)\n \n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n         value_states = repeat_kv(value_states, self.num_key_value_groups)\n@@ -324,18 +326,19 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Any,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        if isinstance(past_key_value, StaticCache):\n+        if isinstance(past_key_values, StaticCache):\n             raise ValueError(\n                 \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n                 \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n@@ -368,10 +371,10 @@ def forward(\n         cos, sin = self.rotary_emb(value_states, position_ids)\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.block_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.block_idx, cache_kwargs)\n \n         # TODO: These transpose are quite inefficient but Flash Attention requires the layout\n         # [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n@@ -440,12 +443,13 @@ class DbrxSdpaAttention(DbrxAttention):\n     SDPA API.\n     \"\"\"\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -460,7 +464,7 @@ def forward(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -488,10 +492,10 @@ def forward(\n         cos, sin = self.rotary_emb(value_states, position_ids, seq_len=None)\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, None)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.block_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.block_idx, cache_kwargs)\n \n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n         value_states = repeat_kv(value_states, self.num_key_value_groups)\n@@ -547,12 +551,13 @@ def __init__(self, config: DbrxConfig, block_idx: Optional[int] = None):\n         )\n         self.norm_2 = nn.LayerNorm(config.d_model, bias=False)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_ids: torch.LongTensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -565,7 +570,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -737,12 +742,13 @@ def __init__(self, config: DbrxConfig, block_idx: int):\n         )\n         self.ffn = DbrxFFN(config=config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         output_router_logits: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n@@ -765,7 +771,7 @@ def forward(\n             attention_mask (`torch.Tensor`, *optional*): attention mask of size (batch_size, sequence_length)\n                 if flash attention is used or (batch_size, 1, query_sequence_length, key_sequence_length)\n                 if default attention is used.\n-            past_key_value (`Tuple(torch.Tensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.Tensor)`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*): Whether or not to return the attentions tensors of all\n                 attention layers. See `attentions` under returned tensors for more detail.\n             output_router_logits (`bool`, *optional*): Whether or not to return the router logits.\n@@ -779,7 +785,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -940,7 +946,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 output_router_logits=output_router_logits,\n                 use_cache=use_cache,"
        },
        {
            "sha": "6b3419beb6388dd21732ff16e9f094fdd8b87a42",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 17,
            "deletions": 14,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -34,6 +34,7 @@\n     auto_docstring,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_decision_transformer import DecisionTransformerConfig\n \n \n@@ -255,10 +256,11 @@ def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, hea\n \n         return attn_output, attn_weights\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.FloatTensor]],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n@@ -268,16 +270,16 @@ def forward(\n         **kwargs,\n     ) -> tuple[Union[torch.Tensor, tuple[torch.Tensor]], ...]:\n         is_cross_attention = encoder_hidden_states is not None\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         if is_cross_attention:\n             if not hasattr(self, \"q_attn\"):\n@@ -289,7 +291,7 @@ def forward(\n             attention_mask = encoder_attention_mask\n \n             # Try to get key/value states from cache if possible\n-            if past_key_value is not None and is_updated:\n+            if past_key_values is not None and is_updated:\n                 key_states = curr_past_key_value.layers[self.layer_idx].keys\n                 value_states = curr_past_key_value.layers[self.layer_idx].values\n             else:\n@@ -306,8 +308,8 @@ def forward(\n         shape_q = (*query_states.shape[:-1], -1, self.head_dim)\n         query_states = query_states.view(shape_q).transpose(1, 2)\n \n-        if (past_key_value is not None and not is_cross_attention) or (\n-            past_key_value is not None and is_cross_attention and not is_updated\n+        if (past_key_values is not None and not is_cross_attention) or (\n+            past_key_values is not None and is_cross_attention and not is_updated\n         ):\n             # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n             cache_position = cache_position if not is_cross_attention else None\n@@ -316,7 +318,7 @@ def forward(\n             )\n             # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n             if is_cross_attention:\n-                past_key_value.is_updated[self.layer_idx] = True\n+                past_key_values.is_updated[self.layer_idx] = True\n \n         is_causal = attention_mask is None and query_states.shape[-2] > 1 and not is_cross_attention\n \n@@ -387,10 +389,11 @@ def __init__(self, config, layer_idx=None):\n \n         self.mlp = DecisionTransformerGPT2MLP(inner_dim, config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.FloatTensor]],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n@@ -404,7 +407,7 @@ def forward(\n         hidden_states = self.ln_1(hidden_states)\n         attn_output, self_attn_weights = self.attn(\n             hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n@@ -426,7 +429,7 @@ def forward(\n             hidden_states = self.ln_cross_attn(hidden_states)\n             cross_attn_output, cross_attn_weights = self.crossattention(\n                 hidden_states,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 attention_mask=attention_mask,\n                 head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,"
        },
        {
            "sha": "5da9d687fbb576a1785aa0d6de3cc651ae796730",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -37,6 +37,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_deepseek_v2 import DeepseekV2Config\n \n@@ -334,11 +335,12 @@ def __init__(self, config: DeepseekV2Config, layer_idx: Optional[int] = None):\n \n         self.scaling = self.qk_head_dim ** (-0.5)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         position_ids: Optional[torch.Tensor] = None,\n@@ -371,10 +373,10 @@ def forward(\n         query_states = torch.cat((q_nope, q_pe), dim=-1)\n         key_states = torch.cat((k_nope, k_pe), dim=-1)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         if self.config._attn_implementation == \"flash_attention_2\" and self.qk_head_dim != self.v_head_dim:\n             value_states = F.pad(value_states, [0, self.qk_head_dim - self.v_head_dim])\n@@ -413,12 +415,13 @@ def __init__(self, config: DeepseekV2Config, layer_idx: int):\n         self.input_layernorm = DeepseekV2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = DeepseekV2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -431,7 +434,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -537,7 +540,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n                 **kwargs,"
        },
        {
            "sha": "dad427debc77183abb7f5f03c77519504d30ea69",
            "filename": "src/transformers/models/deepseek_v2/modular_deepseek_v2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -25,6 +25,7 @@\n from ...utils import (\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ..llama.configuration_llama import LlamaConfig\n from ..llama.modeling_llama import (\n     LlamaDecoderLayer,\n@@ -422,11 +423,12 @@ def __init__(self, config: DeepseekV2Config, layer_idx: Optional[int] = None):\n \n         self.scaling = self.qk_head_dim ** (-0.5)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         position_ids: Optional[torch.Tensor] = None,\n@@ -459,10 +461,10 @@ def forward(\n         query_states = torch.cat((q_nope, q_pe), dim=-1)\n         key_states = torch.cat((k_nope, k_pe), dim=-1)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         if self.config._attn_implementation == \"flash_attention_2\" and self.qk_head_dim != self.v_head_dim:\n             value_states = F.pad(value_states, [0, self.qk_head_dim - self.v_head_dim])"
        },
        {
            "sha": "5d879800801131be6ee1cc8df0bd73cac5404baf",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -23,6 +23,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_deepseek_v3 import DeepseekV3Config\n \n@@ -371,12 +372,13 @@ def __init__(self, config: DeepseekV3Config, layer_idx: int):\n                 mscale = yarn_get_mscale(scaling_factor, mscale_all_dim)\n                 self.scaling = self.scaling * mscale * mscale\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -409,10 +411,10 @@ def forward(\n         query_states = torch.cat((q_pass, q_rot), dim=-1)\n         key_states = torch.cat((k_pass, k_rot), dim=-1)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         if self.config._attn_implementation == \"flash_attention_2\" and self.qk_head_dim != self.v_head_dim:\n             value_states = F.pad(value_states, [0, self.qk_head_dim - self.v_head_dim])\n@@ -455,12 +457,13 @@ def __init__(self, config: DeepseekV3Config, layer_idx: int):\n         self.input_layernorm = DeepseekV3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = DeepseekV3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -473,7 +476,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -581,7 +584,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n                 **kwargs,"
        },
        {
            "sha": "791c4a19582b5806349a4eb8500a5bf4cf036600",
            "filename": "src/transformers/models/deepseek_v3/modular_deepseek_v3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -12,6 +12,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..llama.modeling_llama import (\n     LlamaDecoderLayer,\n     LlamaForCausalLM,\n@@ -252,12 +253,13 @@ def __init__(self, config: DeepseekV3Config, layer_idx: int):\n                 mscale = yarn_get_mscale(scaling_factor, mscale_all_dim)\n                 self.scaling = self.scaling * mscale * mscale\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -290,10 +292,10 @@ def forward(\n         query_states = torch.cat((q_pass, q_rot), dim=-1)\n         key_states = torch.cat((k_pass, k_rot), dim=-1)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         if self.config._attn_implementation == \"flash_attention_2\" and self.qk_head_dim != self.v_head_dim:\n             value_states = F.pad(value_states, [0, self.qk_head_dim - self.v_head_dim])"
        },
        {
            "sha": "e2c939b255b510d86642d6c44b383cf52e5080c6",
            "filename": "src/transformers/models/deprecated/ernie_m/modeling_ernie_m.py",
            "status": "modified",
            "additions": 21,
            "deletions": 18,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -34,6 +34,7 @@\n from ....modeling_utils import PreTrainedModel\n from ....pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ....utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n+from ....utils.deprecation import deprecate_kwarg\n from .configuration_ernie_m import ErnieMConfig\n \n \n@@ -118,14 +119,15 @@ def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n         x = x.view(new_x_shape)\n         return x.permute(0, 2, 1, 3)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         mixed_query_layer = self.q_proj(hidden_states)\n@@ -135,36 +137,36 @@ def forward(\n         # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n \n-        if is_cross_attention and past_key_value is not None:\n+        if is_cross_attention and past_key_values is not None:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value[0]\n-            value_layer = past_key_value[1]\n+            key_layer = past_key_values[0]\n+            value_layer = past_key_values[1]\n             attention_mask = encoder_attention_mask\n         elif is_cross_attention:\n             key_layer = self.transpose_for_scores(self.k_proj(encoder_hidden_states))\n             value_layer = self.transpose_for_scores(self.v_proj(encoder_hidden_states))\n             attention_mask = encoder_attention_mask\n-        elif past_key_value is not None:\n+        elif past_key_values is not None:\n             key_layer = self.transpose_for_scores(self.k_proj(hidden_states))\n             value_layer = self.transpose_for_scores(self.v_proj(hidden_states))\n-            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+            key_layer = torch.cat([past_key_values[0], key_layer], dim=2)\n+            value_layer = torch.cat([past_key_values[1], value_layer], dim=2)\n         else:\n             key_layer = self.transpose_for_scores(self.k_proj(hidden_states))\n             value_layer = self.transpose_for_scores(self.v_proj(hidden_states))\n \n         query_layer = self.transpose_for_scores(mixed_query_layer)\n \n-        use_cache = past_key_value is not None\n+        use_cache = past_key_values is not None\n         if self.is_decoder:\n             # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n             # Further calls to cross_attention layer can then reuse all cross-attention\n             # key/value_states (first \"if\" case)\n             # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n             # all previous decoder key/value_states. Further calls to uni-directional self-attention\n             # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+            # if encoder bi-directional self-attention `past_key_values` is always `None`\n+            past_key_values = (key_layer, value_layer)\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n@@ -216,7 +218,7 @@ def forward(\n         outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n \n         if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n+            outputs = outputs + (past_key_values,)\n         return outputs\n \n \n@@ -245,14 +247,15 @@ def prune_heads(self, heads):\n         self.self_attn.all_head_size = self.self_attn.attention_head_size * self.self_attn.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self_attn(\n@@ -261,7 +264,7 @@ def forward(\n             head_mask,\n             encoder_hidden_states,\n             encoder_attention_mask,\n-            past_key_value,\n+            past_key_values,\n             output_attentions,\n         )\n         attention_output = self.out_proj(self_outputs[0])\n@@ -289,12 +292,13 @@ def __init__(self, config):\n         else:\n             self.activation = config.hidden_act\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = True,\n     ):\n         residual = hidden_states\n@@ -303,7 +307,7 @@ def forward(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n                 head_mask=head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n             )\n \n@@ -312,7 +316,7 @@ def forward(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n                 head_mask=head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n             )\n         hidden_states = residual + self.dropout1(hidden_states)\n@@ -356,13 +360,12 @@ def forward(\n             hidden_states = hidden_states + (output,)\n         for i, layer in enumerate(self.layers):\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            past_key_value = past_key_values[i] if past_key_values is not None else None\n \n             output, opt_attn_weights = layer(\n                 hidden_states=output,\n                 attention_mask=attention_mask,\n                 head_mask=layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values[i] if past_key_values is not None else None,\n             )\n \n             if output_hidden_states:"
        },
        {
            "sha": "49b3e1bb05a07043d5657afe24452db5b2b10545",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
            "status": "modified",
            "additions": 24,
            "deletions": 20,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -30,6 +30,7 @@\n     is_torch_fx_proxy,\n     logging,\n )\n+from ....utils.deprecation import deprecate_kwarg\n from .configuration_gptsan_japanese import GPTSanJapaneseConfig\n \n \n@@ -377,11 +378,12 @@ def __init__(\n     def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -397,27 +399,27 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states) * self.scaling\n         # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n+        # `past_key_values[0].shape[2] == key_value_states.shape[1]`\n+        # is checking that the `sequence_length` of the `past_key_values` is the same as\n         # the provided `key_value_states` to support prefix tuning\n         if (\n             is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n+            and past_key_values is not None\n+            and past_key_values[0].shape[2] == key_value_states.shape[1]\n         ):\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n+            key_states = past_key_values[0]\n+            value_states = past_key_values[1]\n         elif is_cross_attention:\n             # cross_attentions\n             key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n             value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n+        elif past_key_values is not None:\n             # reuse k, v, self_attention\n             key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n             value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = torch.cat([past_key_values[0], key_states], dim=2)\n+            value_states = torch.cat([past_key_values[1], value_states], dim=2)\n         else:\n             # self_attention\n             key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n@@ -430,8 +432,8 @@ def forward(\n             # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n             # all previous decoder key/value_states. Further calls to uni-directional self-attention\n             # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+            # if encoder bi-directional self-attention `past_key_values` is always `None`\n+            past_key_values = (key_states, value_states)\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n         query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n@@ -495,7 +497,7 @@ def forward(\n \n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights_reshaped, past_key_value\n+        return attn_output, attn_weights_reshaped, past_key_values\n \n \n class GPTSanJapaneseLayerSelfAttention(nn.Module):\n@@ -513,10 +515,11 @@ def __init__(self, config, has_relative_attention_bias=False):\n         )\n         self.norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.FloatTensor]],\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n@@ -558,11 +561,11 @@ def forward(\n         \"\"\"\n         # Self Attention\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n+        self_attn_past_key_value = past_key_values[:2] if past_key_values is not None else None\n         # add present self-attn cache to positions 1,2 of present_key_value tuple\n         atten_out = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_values=self_attn_past_key_value,\n             attention_mask=(1 - attention_mask) * torch.finfo(hidden_states.dtype).min,\n             layer_head_mask=head_mask,\n             output_attentions=output_attentions,\n@@ -594,10 +597,11 @@ def __init__(self, config, ext_layer=False):\n         self.self_attn = GPTSanJapaneseLayerSelfAttention(config)\n         self.feed_forward = GPTSanJapaneseLayerDenseFF(config) if ext_layer else GPTSanJapaneseLayerSparseFF(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.FloatTensor]],\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n@@ -641,7 +645,7 @@ def forward(\n         \"\"\"\n         atten_out = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             use_cache=use_cache,\n@@ -918,7 +922,7 @@ def forward(\n         elif self.config.d_spout and spout is not None:\n             # `spout` is a special input vector specific to GPTSAN\n             # This controls the output by projecting embedded information such as the class of sentences during learning.\n-            # It should passed instead of the first past_key_value.\n+            # It should passed instead of the first past_key_values.\n             # See the original GPTSAN repository for details\n             num_pasts_contexts += 1\n \n@@ -1032,7 +1036,7 @@ def forward(\n             ) and layer < self.config.num_switch_layers\n             block_output = self.blocks[layer](\n                 hidden_states=hidden_states,\n-                past_key_value=past,\n+                past_key_values=past,\n                 attention_mask=extended_attention_mask,\n                 head_mask=head_mask,\n                 use_cache=self.config.use_cache or use_cache,"
        },
        {
            "sha": "cc77cb2874e684b52841ffd9b23fccf72c6ec441",
            "filename": "src/transformers/models/deprecated/mega/modeling_mega.py",
            "status": "modified",
            "additions": 9,
            "deletions": 7,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -41,6 +41,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ....utils.deprecation import deprecate_kwarg\n from .configuration_mega import MegaConfig\n \n \n@@ -1173,14 +1174,15 @@ def __init__(self, config: MegaConfig):\n         else:\n             self.cross_attn = None\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.LongTensor] = None,\n         causal_mask: Optional[torch.LongTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[torch.FloatTensor]] = None,\n+        past_key_values: Optional[tuple[torch.FloatTensor]] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: bool = False,\n     ) -> tuple[torch.Tensor]:\n@@ -1202,14 +1204,14 @@ def forward(\n             encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, source_sequence_length)`, *optional*):\n                 Indicates which entries in the cross/source sequence are to be ignored (mostly due to padding), where\n                 elements are either 1 for *not masked* or 0 for *masked*.\n-            past_key_value (`tuple(torch.Tensor)`, *optional*):\n+            past_key_values (`tuple(torch.Tensor)`, *optional*):\n                 The hidden states returned from the previous timestep during incremental decoding; expects that\n                 self-attention key, value, and EMA states are the first 3 entries in the tuple, and (if doing\n                 cross-attention) cross-attention key and value are the last 2 entries in the tuple\n             output_attentions (`bool`, default `False`):\n                 Whether to return self-attention weights\n             use_cache (`bool`, default `False`):\n-                Whether to perform incremental decoding; uses `past_key_value` as prior state, and returns the updated\n+                Whether to perform incremental decoding; uses `past_key_values` as prior state, and returns the updated\n                 states for use in the next step\n \n         Returns:\n@@ -1244,7 +1246,7 @@ def forward(\n         # sequence length as the input tensor; if we're caching incremental states, we assume the input\n         # sequence length is 1 (Mega will break otherwise), so we take the padding mask for the final\n         # token in the input (mask is received as [batch X sequence length])\n-        if use_cache and (past_key_value is not None) and (attention_mask is not None):\n+        if use_cache and (past_key_values is not None) and (attention_mask is not None):\n             mega_padding_mask = attention_mask[:, -1].unsqueeze(-1)\n         else:\n             mega_padding_mask = attention_mask\n@@ -1253,7 +1255,7 @@ def forward(\n             input=hidden_states,\n             padding_mask=mega_padding_mask,\n             causal_mask=causal_mask,\n-            past_key_values=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n         )\n@@ -1272,7 +1274,7 @@ def forward(\n                 key=encoder_hidden_states,\n                 value=encoder_hidden_states,\n                 key_padding_mask=encoder_attention_mask,\n-                past_key_values=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n             )\n@@ -1592,7 +1594,7 @@ def forward(\n                 causal_mask=causal_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=current_decoder_cache,\n+                past_key_values=current_decoder_cache,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n             )"
        },
        {
            "sha": "635a078c6a13c3894e1c70e72bdca9dedec4f764",
            "filename": "src/transformers/models/deprecated/nezha/modeling_nezha.py",
            "status": "modified",
            "additions": 22,
            "deletions": 19,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -47,6 +47,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ....utils.deprecation import deprecate_kwarg\n from .configuration_nezha import NezhaConfig\n \n \n@@ -242,14 +243,15 @@ def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n         x = x.view(new_x_shape)\n         return x.permute(0, 2, 1, 3)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         mixed_query_layer = self.query(hidden_states)\n@@ -259,20 +261,20 @@ def forward(\n         # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n \n-        if is_cross_attention and past_key_value is not None:\n+        if is_cross_attention and past_key_values is not None:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value[0]\n-            value_layer = past_key_value[1]\n+            key_layer = past_key_values[0]\n+            value_layer = past_key_values[1]\n             attention_mask = encoder_attention_mask\n         elif is_cross_attention:\n             key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n             value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n             attention_mask = encoder_attention_mask\n-        elif past_key_value is not None:\n+        elif past_key_values is not None:\n             key_layer = self.transpose_for_scores(self.key(hidden_states))\n             value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+            key_layer = torch.cat([past_key_values[0], key_layer], dim=2)\n+            value_layer = torch.cat([past_key_values[1], value_layer], dim=2)\n         else:\n             key_layer = self.transpose_for_scores(self.key(hidden_states))\n             value_layer = self.transpose_for_scores(self.value(hidden_states))\n@@ -286,8 +288,8 @@ def forward(\n             # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n             # all previous decoder key/value_states. Further calls to uni-directional self-attention\n             # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+            # if encoder bi-directional self-attention `past_key_values` is always `None`\n+            past_key_values = (key_layer, value_layer)\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n@@ -343,7 +345,7 @@ def forward(\n         outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n \n         if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n+            outputs = outputs + (past_key_values,)\n         return outputs\n \n \n@@ -386,14 +388,15 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n@@ -402,7 +405,7 @@ def forward(\n             head_mask,\n             encoder_hidden_states,\n             encoder_attention_mask,\n-            past_key_value,\n+            past_key_values,\n             output_attentions,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n@@ -454,24 +457,25 @@ def __init__(self, config):\n         self.intermediate = NezhaIntermediate(config)\n         self.output = NezhaOutput(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n+        self_attn_past_key_value = past_key_values[:2] if past_key_values is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask,\n             head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_values=self_attn_past_key_value,\n         )\n         attention_output = self_attention_outputs[0]\n \n@@ -490,8 +494,8 @@ def forward(\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n+            # cross_attn cached key/values tuple is at positions 3,4 of past_key_values tuple\n+            cross_attn_past_key_value = past_key_values[-2:] if past_key_values is not None else None\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n                 attention_mask,\n@@ -562,15 +566,14 @@ def forward(\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            past_key_value = past_key_values[i] if past_key_values is not None else None\n \n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                past_key_value,\n+                past_key_values[i] if past_key_values is not None else None,\n                 output_attentions,\n             )\n "
        },
        {
            "sha": "fab787f8e7b9f13b317aa9927585d11c63f259ef",
            "filename": "src/transformers/models/deprecated/open_llama/modeling_open_llama.py",
            "status": "modified",
            "additions": 15,
            "deletions": 14,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -33,6 +33,7 @@\n from ....modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n from ....modeling_utils import PreTrainedModel\n from ....utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n+from ....utils.deprecation import deprecate_kwarg\n from .configuration_open_llama import OpenLlamaConfig\n \n \n@@ -267,12 +268,13 @@ def _init_rope(self):\n     def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -283,18 +285,18 @@ def forward(\n         value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n \n         kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += past_key_value[0].shape[-2]\n+        if past_key_values is not None:\n+            kv_seq_len += past_key_values[0].shape[-2]\n         cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n         # [bsz, nh, t, hd]\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # reuse k, v, self_attention\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = torch.cat([past_key_values[0], key_states], dim=2)\n+            value_states = torch.cat([past_key_values[1], value_states], dim=2)\n \n-        past_key_value = (key_states, value_states) if use_cache else None\n+        past_key_values = (key_states, value_states) if use_cache else None\n \n         if self.config.use_memory_efficient_attention and xops is not None and self.training:\n             attn_weights = None\n@@ -341,7 +343,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights, past_key_values\n \n \n class OpenLlamaDecoderLayer(GradientCheckpointingLayer):\n@@ -358,12 +360,13 @@ def __init__(self, config: OpenLlamaConfig):\n         self.input_layernorm = OpenLlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = OpenLlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n@@ -378,7 +381,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n         \"\"\"\n \n         residual = hidden_states\n@@ -390,7 +393,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n         )\n@@ -628,13 +631,11 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n             layer_outputs = decoder_layer(\n                 hidden_states,\n                 attention_mask=attention_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values[idx] if past_key_values is not None else None,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n             )"
        },
        {
            "sha": "cfa66aaf0250d91f8093d74f3c9adc8ae5e3059f",
            "filename": "src/transformers/models/deprecated/qdqbert/modeling_qdqbert.py",
            "status": "modified",
            "additions": 22,
            "deletions": 19,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -49,6 +49,7 @@\n     replace_return_docstrings,\n     requires_backends,\n )\n+from ....utils.deprecation import deprecate_kwarg\n from .configuration_qdqbert import QDQBertConfig\n \n \n@@ -242,14 +243,15 @@ def transpose_for_scores(self, x):\n         x = x.view(*new_x_shape)\n         return x.permute(0, 2, 1, 3)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         output_attentions=False,\n     ):\n         mixed_query_layer = self.query(hidden_states)\n@@ -259,20 +261,20 @@ def forward(\n         # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n \n-        if is_cross_attention and past_key_value is not None:\n+        if is_cross_attention and past_key_values is not None:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value[0]\n-            value_layer = past_key_value[1]\n+            key_layer = past_key_values[0]\n+            value_layer = past_key_values[1]\n             attention_mask = encoder_attention_mask\n         elif is_cross_attention:\n             key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n             value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n             attention_mask = encoder_attention_mask\n-        elif past_key_value is not None:\n+        elif past_key_values is not None:\n             key_layer = self.transpose_for_scores(self.key(hidden_states))\n             value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+            key_layer = torch.cat([past_key_values[0], key_layer], dim=2)\n+            value_layer = torch.cat([past_key_values[1], value_layer], dim=2)\n         else:\n             key_layer = self.transpose_for_scores(self.key(hidden_states))\n             value_layer = self.transpose_for_scores(self.value(hidden_states))\n@@ -286,8 +288,8 @@ def forward(\n             # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n             # all previous decoder key/value_states. Further calls to uni-directional self-attention\n             # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+            # if encoder bi-directional self-attention `past_key_values` is always `None`\n+            past_key_values = (key_layer, value_layer)\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(\n@@ -337,7 +339,7 @@ def forward(\n         outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n \n         if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n+            outputs = outputs + (past_key_values,)\n         return outputs\n \n \n@@ -390,14 +392,15 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         output_attentions=False,\n     ):\n         self_outputs = self.self(\n@@ -406,7 +409,7 @@ def forward(\n             head_mask,\n             encoder_hidden_states,\n             encoder_attention_mask,\n-            past_key_value,\n+            past_key_values,\n             output_attentions,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n@@ -467,24 +470,25 @@ def __init__(self, config):\n         self.intermediate = QDQBertIntermediate(config)\n         self.output = QDQBertOutput(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         output_attentions=False,\n     ):\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n+        self_attn_past_key_value = past_key_values[:2] if past_key_values is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask,\n             head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_values=self_attn_past_key_value,\n         )\n         attention_output = self_attention_outputs[0]\n \n@@ -503,8 +507,8 @@ def forward(\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n+            # cross_attn cached key/values tuple is at positions 3,4 of past_key_values tuple\n+            cross_attn_past_key_value = past_key_values[-2:] if past_key_values is not None else None\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n                 attention_mask,\n@@ -567,15 +571,14 @@ def forward(\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            past_key_value = past_key_values[i] if past_key_values is not None else None\n \n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                past_key_value,\n+                past_key_values[i] if past_key_values is not None else None,\n                 output_attentions,\n             )\n "
        },
        {
            "sha": "8021a142dd80fa5352c8ada4f01d22ef49be4091",
            "filename": "src/transformers/models/deprecated/realm/modeling_realm.py",
            "status": "modified",
            "additions": 23,
            "deletions": 20,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -34,6 +34,7 @@\n from ....modeling_utils import PreTrainedModel\n from ....pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ....utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n+from ....utils.deprecation import deprecate_kwarg\n from .configuration_realm import RealmConfig\n \n \n@@ -247,14 +248,15 @@ def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n         x = x.view(new_x_shape)\n         return x.permute(0, 2, 1, 3)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         mixed_query_layer = self.query(hidden_states)\n@@ -264,36 +266,36 @@ def forward(\n         # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n \n-        if is_cross_attention and past_key_value is not None:\n+        if is_cross_attention and past_key_values is not None:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value[0]\n-            value_layer = past_key_value[1]\n+            key_layer = past_key_values[0]\n+            value_layer = past_key_values[1]\n             attention_mask = encoder_attention_mask\n         elif is_cross_attention:\n             key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n             value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n             attention_mask = encoder_attention_mask\n-        elif past_key_value is not None:\n+        elif past_key_values is not None:\n             key_layer = self.transpose_for_scores(self.key(hidden_states))\n             value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+            key_layer = torch.cat([past_key_values[0], key_layer], dim=2)\n+            value_layer = torch.cat([past_key_values[1], value_layer], dim=2)\n         else:\n             key_layer = self.transpose_for_scores(self.key(hidden_states))\n             value_layer = self.transpose_for_scores(self.value(hidden_states))\n \n         query_layer = self.transpose_for_scores(mixed_query_layer)\n \n-        use_cache = past_key_value is not None\n+        use_cache = past_key_values is not None\n         if self.is_decoder:\n             # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n             # Further calls to cross_attention layer can then reuse all cross-attention\n             # key/value_states (first \"if\" case)\n             # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n             # all previous decoder key/value_states. Further calls to uni-directional self-attention\n             # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+            # if encoder bi-directional self-attention `past_key_values` is always `None`\n+            past_key_values = (key_layer, value_layer)\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n@@ -345,7 +347,7 @@ def forward(\n         outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n \n         if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n+            outputs = outputs + (past_key_values,)\n         return outputs\n \n \n@@ -395,14 +397,15 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n@@ -411,7 +414,7 @@ def forward(\n             head_mask,\n             encoder_hidden_states,\n             encoder_attention_mask,\n-            past_key_value,\n+            past_key_values,\n             output_attentions,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n@@ -463,24 +466,25 @@ def __init__(self, config):\n         self.intermediate = RealmIntermediate(config)\n         self.output = RealmOutput(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n+        self_attn_past_key_value = past_key_values[:2] if past_key_values is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask,\n             head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_values=self_attn_past_key_value,\n         )\n         attention_output = self_attention_outputs[0]\n \n@@ -499,8 +503,8 @@ def forward(\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n+            # cross_attn cached key/values tuple is at positions 3,4 of past_key_values tuple\n+            cross_attn_past_key_value = past_key_values[-2:] if past_key_values is not None else None\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n                 attention_mask,\n@@ -571,15 +575,14 @@ def forward(\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            past_key_value = past_key_values[i] if past_key_values is not None else None\n \n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                past_key_value,\n+                past_key_values[i] if past_key_values is not None else None,\n                 output_attentions,\n             )\n "
        },
        {
            "sha": "2117526b04f836b69cc20c80c70169d012b6764a",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/modeling_speech_to_text_2.py",
            "status": "modified",
            "additions": 23,
            "deletions": 22,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -27,6 +27,7 @@\n from ....modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n from ....modeling_utils import PreTrainedModel\n from ....utils import add_start_docstrings, logging, replace_return_docstrings\n+from ....utils.deprecation import deprecate_kwarg\n from .configuration_speech_to_text_2 import Speech2Text2Config\n \n \n@@ -142,11 +143,12 @@ def __init__(\n     def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -162,27 +164,27 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states) * self.scaling\n         # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n+        # `past_key_values[0].shape[2] == key_value_states.shape[1]`\n+        # is checking that the `sequence_length` of the `past_key_values` is the same as\n         # the provided `key_value_states` to support prefix tuning\n         if (\n             is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n+            and past_key_values is not None\n+            and past_key_values[0].shape[2] == key_value_states.shape[1]\n         ):\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n+            key_states = past_key_values[0]\n+            value_states = past_key_values[1]\n         elif is_cross_attention:\n             # cross_attentions\n             key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n             value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n+        elif past_key_values is not None:\n             # reuse k, v, self_attention\n             key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n             value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = torch.cat([past_key_values[0], key_states], dim=2)\n+            value_states = torch.cat([past_key_values[1], value_states], dim=2)\n         else:\n             # self_attention\n             key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n@@ -195,8 +197,8 @@ def forward(\n             # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n             # all previous decoder key/value_states. Further calls to uni-directional self-attention\n             # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+            # if encoder bi-directional self-attention `past_key_values` is always `None`\n+            past_key_values = (key_states, value_states)\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n         query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n@@ -260,7 +262,7 @@ def forward(\n \n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights_reshaped, past_key_value\n+        return attn_output, attn_weights_reshaped, past_key_values\n \n \n class Speech2Text2DecoderLayer(GradientCheckpointingLayer):\n@@ -293,6 +295,7 @@ def __init__(self, config: Speech2Text2Config):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -301,7 +304,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n     ):\n@@ -318,7 +321,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size *(decoder_attention_heads,)*.\n-            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -327,11 +330,11 @@ def forward(\n \n         # Self Attention\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n+        self_attn_past_key_value = past_key_values[:2] if past_key_values is not None else None\n         # add present self-attn cache to positions 1,2 of present_key_value tuple\n         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_values=self_attn_past_key_value,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n@@ -347,13 +350,13 @@ def forward(\n             residual = hidden_states\n \n             # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n+            cross_attn_past_key_value = past_key_values[-2:] if past_key_values is not None else None\n             hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n+                past_key_values=cross_attn_past_key_value,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n@@ -604,16 +607,14 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n             layer_outputs = decoder_layer(\n                 hidden_states,\n                 attention_mask=attention_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values[idx] if past_key_values is not None else None,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n             )"
        },
        {
            "sha": "35d500731eb767d23edc29960d6e210e6944ac9b",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/modeling_xlm_prophetnet.py",
            "status": "modified",
            "additions": 23,
            "deletions": 21,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -36,6 +36,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ....utils.deprecation import deprecate_kwarg\n from .configuration_xlm_prophetnet import XLMProphetNetConfig\n \n \n@@ -650,13 +651,14 @@ def __init__(\n     def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_attn_heads, self.head_dim).transpose(1, 2).contiguous()\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         key_value_states: Optional[Tensor] = None,\n         attention_mask: Optional[Tensor] = None,\n         layer_head_mask: Optional[Tensor] = None,\n-        past_key_value: Optional[tuple[Tensor]] = None,\n+        past_key_values: Optional[tuple[Tensor]] = None,\n         output_attentions: bool = False,\n     ) -> tuple[Tensor, Optional[Tensor]]:\n         batch_size, tgt_len, hidden_size = hidden_states.size()\n@@ -673,10 +675,10 @@ def forward(\n         # previous time steps are cached - no need to recompute key and value if they are static\n         query_states = self.query_proj(hidden_states) / (self.head_dim**0.5)\n \n-        if is_cross_attention and past_key_value is not None:\n+        if is_cross_attention and past_key_values is not None:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n+            key_states = past_key_values[0]\n+            value_states = past_key_values[1]\n         elif is_cross_attention:\n             # cross_attentions\n             key_states = self._shape(self.key_proj(key_value_states), -1, batch_size)\n@@ -690,8 +692,8 @@ def forward(\n             # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n             # Further calls to cross_attention layer can then reuse all cross-attention\n             # key/value_states (first \"if\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+            # if encoder bi-directional self-attention `past_key_values` is always `None`\n+            past_key_values = (key_states, value_states)\n \n         # project states into the correct shape\n         proj_shape = (batch_size, self.num_attn_heads, -1, self.head_dim)\n@@ -746,7 +748,7 @@ def forward(\n         attn_output = self.out_proj(attn_output)\n \n         attn_output = nn.functional.dropout(attn_output, p=self.dropout, training=self.training)\n-        return attn_output, attn_weights_reshaped, past_key_value\n+        return attn_output, attn_weights_reshaped, past_key_values\n \n \n class XLMProphetNetFeedForward(nn.Module):\n@@ -808,10 +810,11 @@ def _shape(self, tensor, seq_len, batch_size):\n     def prepare_for_onnx_export_(self):\n         self.onnx_trace = True\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n-        past_key_value: Optional[tuple[Tensor]] = None,\n+        past_key_values: Optional[tuple[Tensor]] = None,\n         attention_mask=None,\n         layer_head_mask=None,\n         extended_predict_attention_mask=None,\n@@ -855,14 +858,14 @@ def forward(\n         main_value_states, predict_value_states_list = value_states_list[0], value_states_list[1:]\n \n         # saved states are stored with shape (batch_size, num_attn_heads, seq_len, head_dim)\n-        if past_key_value is not None:\n-            prev_main_key_states = past_key_value[0]\n+        if past_key_values is not None:\n+            prev_main_key_states = past_key_values[0]\n             main_key_states = torch.cat((prev_main_key_states, main_key_states), dim=2)\n-            prev_main_value_states = past_key_value[1]\n+            prev_main_value_states = past_key_values[1]\n             main_value_states = torch.cat((prev_main_value_states, main_value_states), dim=2)\n \n         # Update cache\n-        past_key_value = (main_key_states, main_value_states)\n+        past_key_values = (main_key_states, main_value_states)\n \n         # get seq_length of main stream only\n         sequence_length = ngram_sequence_length // (1 + self.ngram)\n@@ -984,7 +987,7 @@ def forward(\n \n         attn_output = nn.functional.dropout(attn_output, p=self.dropout, training=self.training)\n \n-        return attn_output, main_attn_probs, predict_attn_probs, past_key_value\n+        return attn_output, main_attn_probs, predict_attn_probs, past_key_values\n \n     def get_main_relative_pos_embeddings(\n         self, hidden_states, attn_weights, position_ids, main_relative_position_buckets\n@@ -1154,6 +1157,7 @@ def __init__(self, config: XLMProphetNetConfig):\n         self.feed_forward = XLMProphetNetFeedForward(config, config.decoder_ffn_dim)\n         self.feed_forward_layer_norm = LayerNorm(config.hidden_size)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -1166,16 +1170,16 @@ def forward(\n         main_relative_position_buckets=None,\n         predict_relative_position_buckets=None,\n         position_ids=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         use_cache: bool = True,\n         output_attentions: bool = False,\n     ):\n         # 1st residual block\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n+        self_attn_past_key_value = past_key_values[:2] if past_key_values is not None else None\n         ngram_attention_output, self_attn_weights, self_attn_weights_ngram, present_key_value = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_values=self_attn_past_key_value,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             extended_predict_attention_mask=extended_predict_attention_mask,\n@@ -1186,7 +1190,7 @@ def forward(\n         hidden_states = self.self_attn_layer_norm(hidden_states + ngram_attention_output)\n \n         # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n-        cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n+        cross_attn_past_key_value = past_key_values[-2:] if past_key_values is not None else None\n         cross_attn_weights = None\n         if encoder_hidden_states is not None:\n             # 2nd residual block\n@@ -1195,7 +1199,7 @@ def forward(\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attn_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n+                past_key_values=cross_attn_past_key_value,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = self.cross_attn_layer_norm(attention_output + hidden_states)\n@@ -1544,8 +1548,6 @@ def forward(\n                 if self.config.ngram > 0:\n                     all_ngram_stream_hidden_states += (hidden_states[:, sequence_length:],)\n \n-            past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n             layer_outputs = decoder_layer(\n                 hidden_states,\n                 attention_mask=extended_attention_mask,\n@@ -1557,7 +1559,7 @@ def forward(\n                 main_relative_position_buckets=main_relative_position_buckets,\n                 predict_relative_position_buckets=predict_relative_position_buckets,\n                 position_ids=position_ids,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values[idx] if past_key_values is not None else None,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n             )"
        },
        {
            "sha": "bfcc34aea0f4490a73df07501429bc601ab5a71b",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -48,6 +48,7 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_dia import DiaConfig, DiaDecoderConfig, DiaEncoderConfig\n from .generation_dia import DiaGenerationMixin\n \n@@ -268,12 +269,13 @@ def __init__(self, config: Union[DiaEncoderConfig, DiaDecoderConfig], layer_idx:\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -287,10 +289,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":"
        },
        {
            "sha": "93bbb00824e2cef64c9cffb062c0d38934cf1917",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 18,
            "deletions": 13,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -44,6 +44,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_diffllama import DiffLlamaConfig\n \n@@ -154,13 +155,14 @@ def __init__(self, config: DiffLlamaConfig, layer_idx: Optional[int] = None):\n         self.lambda_k2 = nn.Parameter(torch.normal(0, config.lambda_std_dev, size=(self.head_dim,)))\n         self.groupnorm = nn.RMSNorm(2 * self.head_dim, eps=config.rms_norm_eps, elementwise_affine=False)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n@@ -179,10 +181,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n         value_states = repeat_kv(value_states, self.num_key_value_groups)\n@@ -232,17 +234,18 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> tuple[torch.Tensor, None]:\n-        if isinstance(past_key_value, StaticCache):\n+        if isinstance(past_key_values, StaticCache):\n             raise ValueError(\n                 \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n                 \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n@@ -273,10 +276,10 @@ def forward(\n             cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n         # to be able to avoid many of these transpose/reshape/view.\n@@ -373,13 +376,14 @@ class DiffLlamaSdpaAttention(DiffLlamaAttention):\n     \"\"\"\n \n     # Adapted from DiffLlamaAttention.forward\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n@@ -397,10 +401,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n         value_states = repeat_kv(value_states, self.num_key_value_groups)\n@@ -488,12 +492,13 @@ def __init__(self, config: DiffLlamaConfig, layer_idx: int):\n         self.input_layernorm = DiffLlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = DiffLlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -506,7 +511,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -651,7 +656,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n                 **kwargs,"
        },
        {
            "sha": "1ea0a47058a22a2038970304310d4deb33f563c3",
            "filename": "src/transformers/models/diffllama/modular_diffllama.py",
            "status": "modified",
            "additions": 14,
            "deletions": 10,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -24,6 +24,7 @@\n from ...cache_utils import Cache, StaticCache\n from ...modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask\n from ...utils import logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..gemma.modeling_gemma import GemmaForCausalLM\n from ..llama.modeling_llama import (\n     LlamaDecoderLayer,\n@@ -90,13 +91,14 @@ def __init__(self, config: DiffLlamaConfig, layer_idx: Optional[int] = None):\n         self.lambda_k2 = nn.Parameter(torch.normal(0, config.lambda_std_dev, size=(self.head_dim,)))\n         self.groupnorm = nn.RMSNorm(2 * self.head_dim, eps=config.rms_norm_eps, elementwise_affine=False)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n@@ -115,10 +117,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n         value_states = repeat_kv(value_states, self.num_key_value_groups)\n@@ -168,17 +170,18 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> tuple[torch.Tensor, None]:\n-        if isinstance(past_key_value, StaticCache):\n+        if isinstance(past_key_values, StaticCache):\n             raise ValueError(\n                 \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n                 \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n@@ -209,10 +212,10 @@ def forward(\n             cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n         # to be able to avoid many of these transpose/reshape/view.\n@@ -309,13 +312,14 @@ class DiffLlamaSdpaAttention(DiffLlamaAttention):\n     \"\"\"\n \n     # Adapted from DiffLlamaAttention.forward\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n@@ -333,10 +337,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n         value_states = repeat_kv(value_states, self.num_key_value_groups)"
        },
        {
            "sha": "1371ade4877ce47c29229f332f516c5ebb43b226",
            "filename": "src/transformers/models/doge/modeling_doge.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -40,6 +40,7 @@\n from ...modeling_utils import AttentionInterface, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_doge import DogeConfig\n \n@@ -261,12 +262,13 @@ def __init__(self, config: DogeConfig, layer_idx: Optional[int] = None):\n         self.q_norm = DogeRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n         self.k_norm = DogeRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -280,10 +282,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         # calculate dynamic mask from value_states\n         dt_states = self.dt_proj(\n@@ -441,13 +443,14 @@ def __init__(self, config: DogeConfig, layer_idx: Optional[int] = None):\n         self.mlp = DogeMLP(config) if not config.is_moe else DogeCDMoE(config)\n         self.post_attention_residual = nn.Parameter(torch.ones(config.hidden_size))\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -460,7 +463,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n@@ -578,7 +581,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,"
        },
        {
            "sha": "482b3583a0a4b85e3aaa385110a082b19e7369b8",
            "filename": "src/transformers/models/doge/modular_doge.py",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -33,6 +33,7 @@\n from ...modeling_utils import AttentionInterface\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, is_torch_flex_attn_available\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder\n from ..llama.modeling_llama import (\n     LlamaForSequenceClassification,\n@@ -357,12 +358,13 @@ def __init__(self, config: DogeConfig, layer_idx: Optional[int] = None):\n         self.q_norm = DogeRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n         self.k_norm = DogeRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -376,10 +378,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         # calculate dynamic mask from value_states\n         dt_states = self.dt_proj(\n@@ -525,13 +527,14 @@ def __init__(self, config: DogeConfig, layer_idx: Optional[int] = None):\n         self.mlp = DogeMLP(config) if not config.is_moe else DogeCDMoE(config)\n         self.post_attention_residual = nn.Parameter(torch.ones(config.hidden_size))\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -544,7 +547,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,"
        },
        {
            "sha": "2986de2afa3b162155b54f09f5817a147e65742c",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -36,6 +36,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_dots1 import Dots1Config\n \n@@ -198,12 +199,13 @@ def __init__(self, config: Dots1Config, layer_idx: int):\n         self.k_norm = Dots1RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape\n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -217,10 +219,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -375,12 +377,13 @@ def __init__(self, config: Dots1Config, layer_idx: int):\n         self.post_attention_layernorm = Dots1RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -393,7 +396,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -512,7 +515,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,"
        },
        {
            "sha": "f402614e753058f9300a17a47c2cba6531b7668e",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 21,
            "deletions": 17,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -41,6 +41,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_electra import ElectraConfig\n \n \n@@ -224,13 +225,14 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -241,19 +243,19 @@ def forward(\n         )\n \n         is_cross_attention = encoder_hidden_states is not None\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_layer = curr_past_key_value.layers[self.layer_idx].keys\n             value_layer = curr_past_key_value.layers[self.layer_idx].values\n@@ -267,22 +269,22 @@ def forward(\n                 batch_size, -1, self.num_attention_heads, self.attention_head_size\n             ).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_layer, value_layer = curr_past_key_value.update(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -377,13 +379,14 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -392,7 +395,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -448,14 +451,15 @@ def __init__(self, config, layer_idx=None):\n         self.intermediate = ElectraIntermediate(config)\n         self.output = ElectraOutput(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -464,7 +468,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n@@ -482,7 +486,7 @@ def forward(\n                 attention_mask=encoder_attention_mask,\n                 head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -557,7 +561,7 @@ def forward(\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )"
        },
        {
            "sha": "064a26df06e452865b85c97e30d0fa89f1b3b5f9",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -39,6 +39,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_emu3 import Emu3Config, Emu3TextConfig, Emu3VQVAEConfig\n \n@@ -141,12 +142,13 @@ def __init__(self, config: Emu3Config, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -160,10 +162,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -234,12 +236,13 @@ def __init__(self, config: Emu3Config, layer_idx: int):\n         self.post_attention_layernorm = Emu3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.dropout = nn.Dropout(config.attention_dropout)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n@@ -252,7 +255,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -1211,7 +1214,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n                 **kwargs,"
        },
        {
            "sha": "2933e89757ff68daa388ea4260bf8c5912a793e4",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -29,6 +29,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..chameleon.modeling_chameleon import (\n     ChameleonPreTrainedModel,\n     ChameleonVQVAEEncoderConvDownsample,\n@@ -51,12 +52,13 @@ def __init__(self, config: Emu3Config, layer_idx: int):\n         super().__init__(config, layer_idx)\n         self.dropout = nn.Dropout(config.attention_dropout)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n@@ -69,7 +71,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,"
        },
        {
            "sha": "bea39d1d8be5816de65a489a4a4989c9f7167b14",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 22,
            "deletions": 17,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -42,6 +42,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_ernie import ErnieConfig\n \n \n@@ -153,13 +154,14 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -170,19 +172,19 @@ def forward(\n         )\n \n         is_cross_attention = encoder_hidden_states is not None\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_layer = curr_past_key_value.layers[self.layer_idx].keys\n             value_layer = curr_past_key_value.layers[self.layer_idx].values\n@@ -196,22 +198,22 @@ def forward(\n                 batch_size, -1, self.num_attention_heads, self.attention_head_size\n             ).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_layer, value_layer = curr_past_key_value.update(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -306,13 +308,14 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -321,7 +324,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -377,14 +380,15 @@ def __init__(self, config, layer_idx=None):\n         self.intermediate = ErnieIntermediate(config)\n         self.output = ErnieOutput(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -393,7 +397,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n@@ -411,7 +415,7 @@ def forward(\n                 attention_mask=encoder_attention_mask,\n                 head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -422,6 +426,7 @@ def forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n         outputs = (layer_output,) + outputs\n+\n         return outputs\n \n     def feed_forward_chunk(self, attention_output):\n@@ -485,7 +490,7 @@ def forward(\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )"
        },
        {
            "sha": "1125e53727f445db5f72c12b96f67133a78899c5",
            "filename": "src/transformers/models/ernie4_5/modeling_ernie4_5.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -34,6 +34,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_ernie4_5 import Ernie4_5Config\n \n@@ -192,12 +193,13 @@ def __init__(self, config: Ernie4_5Config, layer_idx: int):\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.use_bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -211,10 +213,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -268,12 +270,13 @@ def __init__(self, config: Ernie4_5Config, layer_idx: int):\n         self.input_layernorm = Ernie4_5RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = Ernie4_5RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -286,7 +289,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -387,7 +390,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n                 **kwargs,"
        },
        {
            "sha": "2df2f0d8718e1b554a79a47d274c866bb5342d30",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -36,6 +36,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_ernie4_5_moe import Ernie4_5_MoeConfig\n \n@@ -215,12 +216,13 @@ def __init__(self, config: Ernie4_5_MoeConfig, layer_idx: int):\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.use_bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -234,10 +236,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -400,13 +402,14 @@ def __init__(self, config, layer_idx):\n         self.input_layernorm = Ernie4_5_MoeRMSNorm(config.hidden_size, config.rms_norm_eps)\n         self.post_attention_layernorm = Ernie4_5_MoeRMSNorm(config.hidden_size, config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> torch.FloatTensor:\n@@ -424,7 +427,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n@@ -444,7 +447,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -557,7 +560,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,"
        },
        {
            "sha": "2c35b7fa064b359ebd81026693c3694a22ca52e9",
            "filename": "src/transformers/models/ernie4_5_moe/modular_ernie4_5_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -297,7 +297,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,"
        },
        {
            "sha": "695372fa3a04063f4cf836203a10af7dbc70f9f5",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 11,
            "deletions": 7,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -52,6 +52,7 @@\n )\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_evolla import EvollaConfig, SaProtConfig\n \n@@ -1134,6 +1135,7 @@ def cross_attention(\n \n         return context_layer\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         query_states,\n@@ -1147,7 +1149,7 @@ def forward(\n         protein_batch_mask=None,\n         structure_batch_mask=None,\n         msa_batch_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n     ):\n         if protein_kv_states is not None:\n             bs, protein_kv_seq_len, dim = protein_kv_states.shape\n@@ -1379,12 +1381,13 @@ def __init__(self, config: EvollaConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -1398,10 +1401,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -1439,13 +1442,14 @@ def __init__(self, config: EvollaConfig, layer_idx: int):\n                 protein_encoder_dim=config.hidden_size,\n             )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         protein_kv_states: Optional[torch.Tensor] = None,\n@@ -1466,7 +1470,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -1636,7 +1640,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,"
        },
        {
            "sha": "5783f869bd2da3ef4bec23d63ca230dc8007fe88",
            "filename": "src/transformers/models/evolla/modular_evolla.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -36,6 +36,7 @@\n     can_return_tuple,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from ..esm.modeling_esm import (\n     EsmAttention,\n@@ -613,6 +614,7 @@ def cross_attention(\n \n         return context_layer\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         query_states,\n@@ -626,7 +628,7 @@ def forward(\n         protein_batch_mask=None,\n         structure_batch_mask=None,\n         msa_batch_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n     ):\n         if protein_kv_states is not None:\n             bs, protein_kv_seq_len, dim = protein_kv_states.shape\n@@ -712,13 +714,14 @@ def __init__(self, config: EvollaConfig, layer_idx: int):\n                 protein_encoder_dim=config.hidden_size,\n             )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         protein_kv_states: Optional[torch.Tensor] = None,\n@@ -739,7 +742,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -895,7 +898,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,"
        },
        {
            "sha": "bbd5d0678c7249dec164eb8fc5dbf02b5dd576f5",
            "filename": "src/transformers/models/exaone4/modeling_exaone4.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -43,6 +43,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_exaone4 import Exaone4Config\n \n \n@@ -200,12 +201,13 @@ def __init__(self, config: Exaone4Config, layer_idx: int):\n         self.q_norm = Exaone4RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n         self.k_norm = Exaone4RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -225,11 +227,11 @@ def forward(\n         if self.sliding_window is None or self.is_sliding:\n             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             cache_kwargs = {\n                 \"cache_position\": cache_position,\n             }\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -278,12 +280,13 @@ def __init__(self, config: Exaone4Config, layer_idx: int):\n         self.post_attention_layernorm = Exaone4RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Exaone4RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -294,7 +297,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -408,7 +411,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 attention_mask=causal_mask_mapping[layer_type],\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,"
        },
        {
            "sha": "fa7512a2ca9687f46cc2bc542f6c4a7c26b1fc1b",
            "filename": "src/transformers/models/exaone4/modular_exaone4.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -35,6 +35,7 @@\n     TransformersKwargs,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ..llama.modeling_llama import (\n     LlamaForCausalLM,\n     LlamaForQuestionAnswering,\n@@ -287,12 +288,13 @@ def __init__(self, config: Exaone4Config, layer_idx: int):\n         self.q_norm = Exaone4RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n         self.k_norm = Exaone4RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -312,11 +314,11 @@ def forward(\n         if self.sliding_window is None or self.is_sliding:\n             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             cache_kwargs = {\n                 \"cache_position\": cache_position,\n             }\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -422,7 +424,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 attention_mask=causal_mask_mapping[layer_type],\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,"
        },
        {
            "sha": "b9b38b4c9c98903fbe54c98be967ff761414285f",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -43,6 +43,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_falcon_h1 import FalconH1Config\n \n@@ -351,12 +352,13 @@ def __init__(self, config: FalconH1Config, layer_idx: int):\n         )\n         self.key_multiplier = config.key_multiplier\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -370,10 +372,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -1071,13 +1073,14 @@ def __init__(self, config: FalconH1Config, layer_idx: int):\n         self.input_layernorm = FalconH1RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.pre_ff_layernorm = FalconH1RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         mamba_attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[FalconHybridMambaAttentionDynamicCache] = None,\n+        past_key_values: Optional[FalconHybridMambaAttentionDynamicCache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -1089,7 +1092,7 @@ def forward(\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n             attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n                 `(batch, sequence_length)` where padding elements are indicated by 0.\n-            past_key_value (`FalconHybridMambaAttentionDynamicCache`, *optional*): cached past key and value projection states\n+            past_key_values (`FalconHybridMambaAttentionDynamicCache`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -1111,7 +1114,7 @@ def forward(\n \n         mamba_hidden_states = self.mamba(\n             hidden_states=hidden_states,\n-            cache_params=past_key_value,\n+            cache_params=past_key_values,\n             cache_position=cache_position,\n             attention_mask=mamba_attention_mask,\n         )\n@@ -1121,7 +1124,7 @@ def forward(\n             hidden_states=hidden_states * self.attention_in_multiplier,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -1309,7 +1312,7 @@ def forward(\n                 attention_mask=causal_mask,\n                 mamba_attention_mask=mamba_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "10aaadcdd1e272504ee2c452e3ba70a35d62d5f5",
            "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -52,6 +52,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_falcon_h1 import FalconH1Config\n \n@@ -204,12 +205,13 @@ def __init__(self, config: FalconH1Config, layer_idx: int):\n         super().__init__(config, layer_idx)\n         self.key_multiplier = config.key_multiplier\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -223,10 +225,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -843,13 +845,14 @@ def __init__(self, config: FalconH1Config, layer_idx: int):\n         self.input_layernorm = FalconH1RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.pre_ff_layernorm = FalconH1RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         mamba_attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[FalconHybridMambaAttentionDynamicCache] = None,\n+        past_key_values: Optional[FalconHybridMambaAttentionDynamicCache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -861,7 +864,7 @@ def forward(\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n             attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n                 `(batch, sequence_length)` where padding elements are indicated by 0.\n-            past_key_value (`FalconHybridMambaAttentionDynamicCache`, *optional*): cached past key and value projection states\n+            past_key_values (`FalconHybridMambaAttentionDynamicCache`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -883,7 +886,7 @@ def forward(\n \n         mamba_hidden_states = self.mamba(\n             hidden_states=hidden_states,\n-            cache_params=past_key_value,\n+            cache_params=past_key_values,\n             cache_position=cache_position,\n             attention_mask=mamba_attention_mask,\n         )\n@@ -893,7 +896,7 @@ def forward(\n             hidden_states=hidden_states * self.attention_in_multiplier,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -1081,7 +1084,7 @@ def forward(\n                 attention_mask=causal_mask,\n                 mamba_attention_mask=mamba_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "3d57fa99eaa14a38214da8200b0af768aef9ddaf",
            "filename": "src/transformers/models/funnel/modeling_tf_funnel.py",
            "status": "modified",
            "additions": 14,
            "deletions": 10,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_tf_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_tf_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_tf_funnel.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -1342,20 +1342,24 @@ def call(\n         **kwargs,\n     ) -> tuple[tf.Tensor] | TFFunnelForPreTrainingOutput:\n         r\"\"\"\n-        Returns:\n+                        Returns:\n \n-        Examples:\n+                        Examples:\n \n-        ```python\n-        >>> from transformers import AutoTokenizer, TFFunnelForPreTraining\n-        >>> import torch\n+                        ```python\n+                        >>> from transformers import AutoTokenizer, TFFunnelForPreTraining\n+                        >>> import torch\n+        from ...utils.deprecation import deprecate_kwarg\n+        from ...utils.deprecation import deprecate_kwarg\n+        from ...utils.deprecation import deprecate_kwarg\n+                from ...utils.deprecation import deprecate_kwarg\n \n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"funnel-transformer/small\")\n-        >>> model = TFFunnelForPreTraining.from_pretrained(\"funnel-transformer/small\")\n+                        >>> tokenizer = AutoTokenizer.from_pretrained(\"funnel-transformer/small\")\n+                        >>> model = TFFunnelForPreTraining.from_pretrained(\"funnel-transformer/small\")\n \n-        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n-        >>> logits = model(inputs).logits\n-        ```\"\"\"\n+                        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n+                        >>> logits = model(inputs).logits\n+                        ```\"\"\"\n         discriminator_hidden_states = self.funnel(\n             input_ids,\n             attention_mask,"
        },
        {
            "sha": "8710c418e65522744618826f27136e183b32fc4b",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -38,6 +38,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_gemma import GemmaConfig\n \n@@ -212,12 +213,13 @@ def __init__(self, config: GemmaConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -231,10 +233,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -267,12 +269,13 @@ def __init__(self, config: GemmaConfig, layer_idx: int):\n         self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -285,7 +288,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -395,7 +398,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,"
        },
        {
            "sha": "67aedbd55115e816036366667ea95d5acbca7ccc",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -422,7 +422,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,"
        },
        {
            "sha": "583d3de2897d741375ada4bd0c9b1b2c3b570952",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -39,6 +39,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_gemma2 import Gemma2Config\n \n@@ -191,12 +192,13 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n         self.attn_logit_softcapping = self.config.attn_logit_softcapping\n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -210,10 +212,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -251,13 +253,14 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n         self.pre_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -273,7 +276,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -456,7 +459,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "398d036caf5183572ee68b381d40e9ec86c3e300",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -29,6 +29,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..gemma.modeling_gemma import (\n     GemmaAttention,\n     GemmaForCausalLM,\n@@ -256,12 +257,13 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n         self.scaling = config.query_pre_attn_scalar**-0.5\n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -275,10 +277,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -316,13 +318,14 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n         self.pre_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -338,7 +341,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -453,7 +456,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "9e45c1f1627ed494f84c47971ba43dadbfa34497",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -39,6 +39,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from ..auto import AutoModel\n from .configuration_gemma3 import Gemma3Config, Gemma3TextConfig\n@@ -298,12 +299,13 @@ def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n         self.q_norm = Gemma3RMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)\n         self.k_norm = Gemma3RMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: torch.Tensor,\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -320,10 +322,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -360,14 +362,15 @@ def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n         self.pre_feedforward_layernorm = Gemma3RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Gemma3RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings_global: torch.Tensor,\n         position_embeddings_local: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -388,7 +391,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -555,7 +558,7 @@ def forward(\n                 position_embeddings_local=position_embeddings_local,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "af355d7b7475e7023aa6dd9a80399cf7dbbfc681",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -32,6 +32,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..gemma2.configuration_gemma2 import Gemma2Config\n from ..gemma2.modeling_gemma2 import (\n     Gemma2Attention,\n@@ -401,12 +402,13 @@ def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n         self.q_norm = Gemma3RMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)\n         self.k_norm = Gemma3RMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: torch.Tensor,\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -423,10 +425,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -463,14 +465,15 @@ def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n         self.pre_feedforward_layernorm = Gemma3RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Gemma3RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings_global: torch.Tensor,\n         position_embeddings_local: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -491,7 +494,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -633,7 +636,7 @@ def forward(\n                 position_embeddings_local=position_embeddings_local,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "72b19b639dd2b65f0aa0cdb15b10dfd68b94ec52",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -40,6 +40,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel\n from .configuration_gemma3n import Gemma3nAudioConfig, Gemma3nConfig, Gemma3nTextConfig, Gemma3nVisionConfig\n \n@@ -1302,12 +1303,13 @@ def __init__(self, config: Gemma3nTextConfig, layer_idx: int):\n             else None\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: torch.Tensor,\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -1321,9 +1323,9 @@ def forward(\n         query_states = apply_rotary_pos_emb(query_states, cos, sin, unsqueeze_dim=2)\n         query_states = query_states.transpose(1, 2)\n \n-        if self.is_kv_shared_layer and self.kv_shared_layer_index is not None and past_key_value is not None:\n+        if self.is_kv_shared_layer and self.kv_shared_layer_index is not None and past_key_values is not None:\n             # In this case we need special handling of the slice as the layer is of fixed small size (for full layers, we never go beyond)\n-            layer = past_key_value.layers[self.kv_shared_layer_index]\n+            layer = past_key_values.layers[self.kv_shared_layer_index]\n             # Device of past layer may be different from current one\n             indices = cache_position.to(layer.keys.device)\n             # Sliding window cache layers might have smaller size (for full layers, we never go beyond)\n@@ -1346,15 +1348,15 @@ def forward(\n             value_states = self.v_norm(value_states)\n             value_states = value_states.transpose(1, 2)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\n                 \"sin\": sin,\n                 \"cos\": cos,\n                 \"cache_position\": cache_position,\n                 \"sliding_window\": self.sliding_window,\n             }\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -1400,6 +1402,7 @@ def __init__(self, config: Gemma3nTextConfig, layer_idx: int):\n         self.per_layer_projection = nn.Linear(self.hidden_size_per_layer_input, self.hidden_size, bias=False)\n         self.post_per_layer_input_norm = Gemma3nRMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -1408,7 +1411,7 @@ def forward(\n         per_layer_input: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -1431,7 +1434,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -1672,7 +1675,7 @@ def forward(\n                 per_layer_input,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "e29ff12ca21a0a0909c2a26f2f704fbf40371d4d",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -32,6 +32,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel\n from ..gemma2.configuration_gemma2 import Gemma2Config\n from ..gemma2.modeling_gemma2 import (\n@@ -1749,12 +1750,13 @@ def __init__(self, config: Gemma3nTextConfig, layer_idx: int):\n             else None\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: torch.Tensor,\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -1768,9 +1770,9 @@ def forward(\n         query_states = apply_rotary_pos_emb(query_states, cos, sin, unsqueeze_dim=2)\n         query_states = query_states.transpose(1, 2)\n \n-        if self.is_kv_shared_layer and self.kv_shared_layer_index is not None and past_key_value is not None:\n+        if self.is_kv_shared_layer and self.kv_shared_layer_index is not None and past_key_values is not None:\n             # In this case we need special handling of the slice as the layer is of fixed small size (for full layers, we never go beyond)\n-            layer = past_key_value.layers[self.kv_shared_layer_index]\n+            layer = past_key_values.layers[self.kv_shared_layer_index]\n             # Device of past layer may be different from current one\n             indices = cache_position.to(layer.keys.device)\n             # Sliding window cache layers might have smaller size (for full layers, we never go beyond)\n@@ -1793,15 +1795,15 @@ def forward(\n             value_states = self.v_norm(value_states)\n             value_states = value_states.transpose(1, 2)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\n                 \"sin\": sin,\n                 \"cos\": cos,\n                 \"cache_position\": cache_position,\n                 \"sliding_window\": self.sliding_window,\n             }\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -1839,6 +1841,7 @@ def __init__(self, config: Gemma3nTextConfig, layer_idx: int):\n         self.per_layer_projection = nn.Linear(self.hidden_size_per_layer_input, self.hidden_size, bias=False)\n         self.post_per_layer_input_norm = Gemma3nRMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -1847,7 +1850,7 @@ def forward(\n         per_layer_input: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -1870,7 +1873,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -2121,7 +2124,7 @@ def forward(\n                 per_layer_input,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "53f02443f3c23e57781290aaf802c967cbfaefa1",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -43,6 +43,7 @@\n     logging,\n     torch_int,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_git import GitConfig, GitVisionConfig\n \n \n@@ -151,12 +152,13 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n             self.max_position_embeddings = config.max_position_embeddings\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         pixel_values_present: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n@@ -178,9 +180,9 @@ def forward(\n             .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n             .transpose(1, 2)\n         )\n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # NOTE: like in other caches, we store the text component. In GIT it means we discard the image component.\n-            key_layer_past, value_layer_past = past_key_value.update(\n+            key_layer_past, value_layer_past = past_key_values.update(\n                 key_layer[:, :, cutoff:, :], value_layer[:, :, cutoff:, :], self.layer_idx\n             )\n             key_layer = torch.cat([key_layer[:, :, :cutoff, :], key_layer_past], dim=2)\n@@ -191,7 +193,7 @@ def forward(\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -284,20 +286,21 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         pixel_values_present: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         attn_output, self_attn_weights = self.self(\n             hidden_states,\n             attention_mask,\n             head_mask,\n-            past_key_value,\n+            past_key_values,\n             output_attentions,\n             pixel_values_present,\n         )\n@@ -345,12 +348,13 @@ def __init__(self, config, layer_idx=None):\n         self.intermediate = GitIntermediate(config)\n         self.output = GitOutput(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         pixel_values_present: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n@@ -360,7 +364,7 @@ def forward(\n             attention_mask,\n             head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             pixel_values_present=pixel_values_present,\n         )\n "
        },
        {
            "sha": "17895e828326d4c63afa7c22c9de46ea2164f333",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -39,6 +39,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_glm import GlmConfig\n \n@@ -172,12 +173,13 @@ def __init__(self, config: GlmConfig, layer_idx: Optional[int] = None):\n         )\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -191,10 +193,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -284,12 +286,13 @@ def __init__(self, config: GlmConfig, layer_idx: int):\n         self.input_layernorm = GlmRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = GlmRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -302,7 +305,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -403,7 +406,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n                 **kwargs,"
        },
        {
            "sha": "c8febbc563e8143f4d80fc712df07427557e8abf",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -40,6 +40,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_glm4 import Glm4Config\n \n@@ -74,12 +75,13 @@ def __init__(self, config: Glm4Config, layer_idx: int):\n         self.post_self_attn_layernorm = Glm4RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_mlp_layernorm = Glm4RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -92,7 +94,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -221,12 +223,13 @@ def __init__(self, config: Glm4Config, layer_idx: Optional[int] = None):\n         )\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -240,10 +243,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -407,7 +410,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n                 **kwargs,"
        },
        {
            "sha": "6bbc9b601f591d539e1bb529456a39337589f417",
            "filename": "src/transformers/models/glm4/modular_glm4.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodular_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodular_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodular_glm4.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -23,6 +23,7 @@\n from ...modeling_outputs import CausalLMOutputWithPast\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..glm.modeling_glm import GlmAttention, GlmForCausalLM, GlmForSequenceClassification, GlmForTokenClassification\n from ..phi3.modeling_phi3 import Phi3MLP\n from .configuration_glm4 import Glm4Config\n@@ -50,12 +51,13 @@ def __init__(self, config: Glm4Config, layer_idx: int):\n         self.post_self_attn_layernorm = Glm4RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_mlp_layernorm = Glm4RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -68,7 +70,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,"
        },
        {
            "sha": "8cda3a71b5fb7bf3ee93ceecff6bf7d08d28706c",
            "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -37,6 +37,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_glm4_moe import Glm4MoeConfig\n \n@@ -152,12 +153,13 @@ def __init__(self, config: Glm4MoeConfig, layer_idx: Optional[int] = None):\n             self.q_norm = Glm4MoeRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n             self.k_norm = Glm4MoeRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -179,10 +181,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; position_ids needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -359,12 +361,13 @@ def __init__(self, config: Glm4MoeConfig, layer_idx: int):\n         self.input_layernorm = Glm4MoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = Glm4MoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -377,7 +380,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -520,7 +523,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n                 **kwargs,"
        },
        {
            "sha": "0ee6d25f9188af3388b890ece8d278fe500c5d95",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -39,6 +39,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_glm4v import Glm4vConfig, Glm4vTextConfig, Glm4vVisionConfig\n \n \n@@ -650,13 +651,14 @@ def __init__(self, config: Glm4vTextConfig, layer_idx: Optional[int] = None):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -677,9 +679,9 @@ def forward(\n             query_states, key_states, cos, sin, self.rope_scaling[\"mrope_section\"]\n         )\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -698,7 +700,7 @@ def forward(\n \n         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights, past_key_values\n \n \n class Glm4vTextMLP(nn.Module):\n@@ -730,13 +732,14 @@ def __init__(self, config: Glm4vTextConfig, layer_idx: int):\n         self.post_self_attn_layernorm = Glm4vRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_mlp_layernorm = Glm4vRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -752,7 +755,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -902,7 +905,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "414fac94cb4467807bc2b12da592026ffd0577da",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -36,6 +36,7 @@\n from ...processing_utils import ImagesKwargs, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...video_utils import VideoInput\n from ..glm4.modeling_glm4 import Glm4MLP, Glm4RMSNorm, eager_attention_forward\n from ..qwen2_5_vl.configuration_qwen2_5_vl import Qwen2_5_VLConfig\n@@ -730,13 +731,14 @@ def __init__(self, config: Glm4vTextConfig, layer_idx: Optional[int] = None):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -757,9 +759,9 @@ def forward(\n             query_states, key_states, cos, sin, self.rope_scaling[\"mrope_section\"]\n         )\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -778,7 +780,7 @@ def forward(\n \n         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights, past_key_values\n \n \n class Glm4vTextMLP(Glm4MLP):\n@@ -796,13 +798,14 @@ def __init__(self, config: Glm4vTextConfig, layer_idx: int):\n         self.post_self_attn_layernorm = Glm4vRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_mlp_layernorm = Glm4vRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -818,7 +821,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -938,7 +941,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "b08c2f718af3cc5d76fbc870964261634ce3c49d",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 17,
            "deletions": 14,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -47,6 +47,7 @@\n     auto_docstring,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.model_parallel_utils import assert_device_map, get_device_map\n from .configuration_gpt2 import GPT2Config\n \n@@ -266,10 +267,11 @@ def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, hea\n \n         return attn_output, attn_weights\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.FloatTensor]],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n@@ -279,16 +281,16 @@ def forward(\n         **kwargs,\n     ) -> tuple[Union[torch.Tensor, tuple[torch.Tensor]], ...]:\n         is_cross_attention = encoder_hidden_states is not None\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         if is_cross_attention:\n             if not hasattr(self, \"q_attn\"):\n@@ -300,7 +302,7 @@ def forward(\n             attention_mask = encoder_attention_mask\n \n             # Try to get key/value states from cache if possible\n-            if past_key_value is not None and is_updated:\n+            if past_key_values is not None and is_updated:\n                 key_states = curr_past_key_value.layers[self.layer_idx].keys\n                 value_states = curr_past_key_value.layers[self.layer_idx].values\n             else:\n@@ -317,8 +319,8 @@ def forward(\n         shape_q = (*query_states.shape[:-1], -1, self.head_dim)\n         query_states = query_states.view(shape_q).transpose(1, 2)\n \n-        if (past_key_value is not None and not is_cross_attention) or (\n-            past_key_value is not None and is_cross_attention and not is_updated\n+        if (past_key_values is not None and not is_cross_attention) or (\n+            past_key_values is not None and is_cross_attention and not is_updated\n         ):\n             # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n             cache_position = cache_position if not is_cross_attention else None\n@@ -327,7 +329,7 @@ def forward(\n             )\n             # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n             if is_cross_attention:\n-                past_key_value.is_updated[self.layer_idx] = True\n+                past_key_values.is_updated[self.layer_idx] = True\n \n         is_causal = attention_mask is None and query_states.shape[-2] > 1 and not is_cross_attention\n \n@@ -393,10 +395,11 @@ def __init__(self, config, layer_idx=None):\n \n         self.mlp = GPT2MLP(inner_dim, config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.FloatTensor]],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n@@ -410,7 +413,7 @@ def forward(\n         hidden_states = self.ln_1(hidden_states)\n         attn_output, self_attn_weights = self.attn(\n             hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n@@ -432,7 +435,7 @@ def forward(\n             hidden_states = self.ln_cross_attn(hidden_states)\n             cross_attn_output, cross_attn_weights = self.crossattention(\n                 hidden_states,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 attention_mask=attention_mask,\n                 head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,"
        },
        {
            "sha": "fd89c071a197650ab6ffd6d4e8f7fc219213ee56",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -27,6 +27,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_gpt_neox import GPTNeoXConfig\n \n@@ -321,12 +322,13 @@ def __init__(self, config: GPTNeoXConfig, layer_idx: int):\n         self.input_layernorm = GPTNeoXRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = GPTNeoXRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -339,7 +341,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,"
        },
        {
            "sha": "81af0548339e6931635b614db9cbabdd2d812716",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -34,6 +34,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_gpt_oss import GptOssConfig\n \n@@ -285,12 +286,13 @@ def __init__(self, config: GptOssConfig, layer_idx: int):\n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n         self.sinks = nn.Parameter(torch.empty(config.num_attention_heads))\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -304,9 +306,9 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             cache_kwargs = {\"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -340,12 +342,13 @@ def __init__(self, config: GptOssConfig, layer_idx: int):\n         self.post_attention_layernorm = GptOssRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -358,7 +361,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -494,7 +497,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,"
        },
        {
            "sha": "891ab3a506a1fe615f3e96e643b52302db55baf4",
            "filename": "src/transformers/models/gpt_oss/modular_gpt_oss.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -32,6 +32,7 @@\n     auto_docstring,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from ..llama.modeling_llama import (\n     LlamaDecoderLayer,\n@@ -242,12 +243,13 @@ def __init__(self, config: GptOssConfig, layer_idx: int):\n         )\n         self.sinks = nn.Parameter(torch.empty(config.num_attention_heads))\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -261,9 +263,9 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             cache_kwargs = {\"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -297,12 +299,13 @@ def __init__(self, config: GptOssConfig, layer_idx: int):\n         self.post_attention_layernorm = GptOssRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -315,7 +318,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -423,7 +426,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,"
        },
        {
            "sha": "318be2bec1f729c4c0e54c0c6c8b91c0ff139ceb",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -35,6 +35,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_granite import GraniteConfig\n \n@@ -140,12 +141,13 @@ def __init__(self, config: GraniteConfig, layer_idx: Optional[int] = None):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -159,10 +161,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -232,12 +234,13 @@ def __init__(self, config: GraniteConfig, layer_idx: int):\n         self.post_attention_layernorm = GraniteRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.residual_multiplier = config.residual_multiplier\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -256,7 +259,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n@@ -275,7 +278,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -445,7 +448,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "5e447e80aa55db5178aafc8a1306f89580db3654",
            "filename": "src/transformers/models/granite/modular_granite.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -24,6 +24,7 @@\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..llama.modeling_llama import (\n     LlamaAttention,\n     LlamaDecoderLayer,\n@@ -51,12 +52,13 @@ def __init__(self, config: GraniteConfig, layer_idx: int):\n         self.residual_multiplier = config.residual_multiplier\n         self.self_attn = GraniteAttention(config=config, layer_idx=layer_idx)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -75,7 +77,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n@@ -94,7 +96,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -200,7 +202,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "012a0581a80807e46be0c2339ab26123fb01a69f",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -28,6 +28,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_granitemoe import GraniteMoeConfig\n \n \n@@ -422,12 +423,13 @@ def __init__(self, config: GraniteMoeConfig, layer_idx: Optional[int] = None):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n         self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # None or rope embeddings\n@@ -447,10 +449,10 @@ def forward(\n         if position_embeddings is not None:\n             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -513,12 +515,13 @@ def __init__(self, config: GraniteMoeConfig, layer_idx: int):\n \n         self.residual_multiplier = config.residual_multiplier\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -538,7 +541,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             output_router_logits (`bool`, *optional*):\n@@ -560,7 +563,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -710,7 +713,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "91439bb2a3c92e463394f012b3c9d3286202ddb7",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -36,6 +36,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_granitemoehybrid import GraniteMoeHybridConfig\n \n@@ -171,12 +172,13 @@ def __init__(self, config: GraniteMoeHybridConfig, layer_idx: int):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n         self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # None or rope embeddings\n@@ -196,10 +198,10 @@ def forward(\n         if position_embeddings is not None:\n             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -1141,11 +1143,12 @@ def __init__(self, config: GraniteMoeHybridConfig, layer_idx: int):\n         # Accept 0 experts: skip MoE if num_local_experts == 0\n         self.has_experts = getattr(config, \"num_local_experts\", 0) > 0\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -1159,7 +1162,7 @@ def forward(\n             attention_mask (`torch.FloatTensor`, *optional*):\n                 attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n                 query_sequence_length, key_sequence_length)` if default attention is used.\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -1185,7 +1188,7 @@ def forward(\n             hidden_states = self.mamba(\n                 hidden_states=hidden_states,\n                 cache_position=cache_position,\n-                cache_params=past_key_value,\n+                cache_params=past_key_values,\n                 attention_mask=attention_mask,\n                 **kwargs,\n             )\n@@ -1195,7 +1198,7 @@ def forward(\n             hidden_states, self_attn_weights = self.self_attn(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -1398,7 +1401,7 @@ def forward(\n             layer_outputs = decoder_layer(\n                 hidden_states,\n                 attention_mask=layer_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "25151b6936b6c02a8e11b79d064f3d1b960695be",
            "filename": "src/transformers/models/granitemoehybrid/modular_granitemoehybrid.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -22,6 +22,7 @@\n from ...modeling_outputs import BaseModelOutputWithPast, MoeModelOutputWithPast\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..bamba.configuration_bamba import BambaConfig\n from ..bamba.modeling_bamba import BambaMixer, BambaRMSNormGated, HybridMambaAttentionDynamicCache\n from ..granitemoeshared.modeling_granitemoeshared import (\n@@ -76,11 +77,12 @@ def __init__(self, config: GraniteMoeHybridConfig, layer_idx: int):\n         # Accept 0 experts: skip MoE if num_local_experts == 0\n         self.has_experts = getattr(config, \"num_local_experts\", 0) > 0\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -94,7 +96,7 @@ def forward(\n             attention_mask (`torch.FloatTensor`, *optional*):\n                 attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n                 query_sequence_length, key_sequence_length)` if default attention is used.\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -120,7 +122,7 @@ def forward(\n             hidden_states = self.mamba(\n                 hidden_states=hidden_states,\n                 cache_position=cache_position,\n-                cache_params=past_key_value,\n+                cache_params=past_key_values,\n                 attention_mask=attention_mask,\n                 **kwargs,\n             )\n@@ -130,7 +132,7 @@ def forward(\n             hidden_states, self_attn_weights = self.self_attn(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -267,7 +269,7 @@ def forward(\n             layer_outputs = decoder_layer(\n                 hidden_states,\n                 attention_mask=layer_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "fa7577735c8f74fb5fc67f32f98bb7eb292a2e04",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -35,6 +35,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_granitemoeshared import GraniteMoeSharedConfig\n \n \n@@ -381,12 +382,13 @@ def __init__(self, config: GraniteMoeSharedConfig, layer_idx: Optional[int] = No\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n         self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # None or rope embeddings\n@@ -406,10 +408,10 @@ def forward(\n         if position_embeddings is not None:\n             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -446,12 +448,13 @@ def __init__(self, config: GraniteMoeSharedConfig, layer_idx: int):\n         self.residual_multiplier = config.residual_multiplier\n         self.shared_mlp = None if config.shared_intermediate_size == 0 else GraniteMoeSharedMLP(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -471,7 +474,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             output_router_logits (`bool`, *optional*):\n@@ -493,7 +496,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -684,7 +687,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "4170deca2e1d1f623aad4aa0265b9fbe68256e2f",
            "filename": "src/transformers/models/granitemoeshared/modular_granitemoeshared.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodular_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodular_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodular_granitemoeshared.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -22,6 +22,7 @@\n from ...cache_utils import Cache\n from ...processing_utils import Unpack\n from ...utils import logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..granitemoe.modeling_granitemoe import (\n     GraniteMoeDecoderLayer,\n     GraniteMoeForCausalLM,\n@@ -90,12 +91,13 @@ def __init__(self, config: GraniteMoeSharedConfig, layer_idx: int):\n         super().__init__(config, layer_idx)\n         self.shared_mlp = None if config.shared_intermediate_size == 0 else GraniteMoeSharedMLP(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -115,7 +117,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             output_router_logits (`bool`, *optional*):\n@@ -137,7 +139,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,"
        },
        {
            "sha": "4cca435c10adb39c34390f80d0330bb16ab88804",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -39,6 +39,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_helium import HeliumConfig\n \n@@ -214,12 +215,13 @@ def __init__(self, config: HeliumConfig, layer_idx: Optional[int] = None):\n         )\n         self.o_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -233,10 +235,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -269,12 +271,13 @@ def __init__(self, config: HeliumConfig, layer_idx: Optional[int] = None):\n         self.input_layernorm = HeliumRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = HeliumRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -287,7 +290,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -388,7 +391,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n                 **kwargs,"
        },
        {
            "sha": "4a678a97fab62e948038e336c6383ed0c0f39dce",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 16,
            "deletions": 12,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -37,6 +37,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PretrainedConfig, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_idefics import IdeficsConfig\n from .perceiver import IdeficsPerceiverResampler\n from .vision import IdeficsVisionEmbeddings, IdeficsVisionTransformer\n@@ -573,13 +574,14 @@ def __init__(\n     def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -602,18 +604,18 @@ def forward(\n             )\n \n         kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             kv_seq_len += cache_position[0]\n \n         if not is_cross_attention:\n             cos, sin = self.rotary_emb(value_states, seq_len=max(kv_seq_len, q_len))\n             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n         # [bsz, nh, t, hd]\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         if self.qk_layer_norms:\n             query_states = self.q_layer_norm(query_states)\n@@ -671,12 +673,13 @@ def __init__(self, config: IdeficsConfig, layer_idx: Optional[int] = None):\n         self.post_attention_layernorm = IdeficsRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.dropout = config.dropout\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -693,7 +696,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n         \"\"\"\n \n         residual = hidden_states\n@@ -705,7 +708,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -796,6 +799,7 @@ def __init__(self, config: IdeficsConfig, layer_idx: Optional[int] = None):\n         if not (hasattr(self, \"alpha_cross_attn\") and hasattr(self, \"alpha_dense\")):\n             raise ValueError(\"Alpha parameters not initialized correctly!\")\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -805,7 +809,7 @@ def forward(\n         cross_attention_gate: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         **kwargs,\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n@@ -823,7 +827,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n         \"\"\"\n         if image_hidden_states is None:\n             raise ValueError(\n@@ -836,7 +840,7 @@ def forward(\n                 \"`cross_attention_gate` is required for Idefics cross attention module to zero-out the cross-attention hidden_states attending to no images.\"\n             )\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             raise NotImplementedError(\"Past key value states are not implemented for Idefics cross attention module.\")\n \n         residual = hidden_states\n@@ -1158,7 +1162,7 @@ def forward(\n                     cross_attention_gate=cross_attention_gate,\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n-                    past_key_value=None,  # not implemented\n+                    past_key_values=None,  # not implemented\n                     **kwargs,\n                 )\n                 hidden_states = outputs[0]\n@@ -1167,7 +1171,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=attention_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "18d7702a3d0571ce9f4b7d17015d81e1e68127ec",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 10,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -31,6 +31,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel\n from .configuration_idefics2 import Idefics2Config, Idefics2PerceiverConfig, Idefics2VisionConfig\n \n@@ -634,13 +635,14 @@ def __init__(self, config, layer_idx: Optional[int] = None) -> None:\n \n         self.is_causal = False\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         latents: torch.Tensor,\n         context: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -652,9 +654,9 @@ def forward(\n             context (`torch.Tensor`): Tensor of shape [bsz, seq, embed_dim] representing long-form context to resample.\n             attention_mask (`torch.Tensor`, *optional*): Tensor of shape [bsz, 1, seq, n_latents] representing attention mask.\n             position_ids (`torch.LongTensor`, *optional*): Tensor of shape [bsz, seq] representing position indices of each input token.\n-            past_key_value (`tuple[torch.Tensor]`, *optional*): Tuple of tensors containing cached key and value states.\n+            past_key_values (`tuple[torch.Tensor]`, *optional*): Tuple of tensors containing cached key and value states.\n             output_attentions (`bool`, *optional*, defaults to `False`): Whether to return attention weights.\n-            use_cache (`bool`, *optional*, defaults to `False`): Whether to use past_key_value for caching.\n+            use_cache (`bool`, *optional*, defaults to `False`): Whether to use past_key_values for caching.\n         \"\"\"\n         bsz, q_len, _ = latents.size()\n         kv_seq_len = q_len + context.size()[1]\n@@ -669,10 +671,10 @@ def forward(\n         keys = keys.view(bsz, kv_seq_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         values = values.view(bsz, kv_seq_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        past_key_value = getattr(self, \"past_key_value\", past_key_value)\n+        past_key_values = getattr(self, \"past_key_values\", past_key_values)\n \n-        if past_key_value is not None:\n-            keys, values = past_key_value.update(keys, values, self.layer_idx)\n+        if past_key_values is not None:\n+            keys, values = past_key_values.update(keys, values, self.layer_idx)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -701,7 +703,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights, past_key_values\n \n \n class Idefics2PerceiverLayer(nn.Module):\n@@ -723,13 +725,14 @@ def __init__(self, config, layer_idx: int):\n             hidden_act=config.hidden_act,\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         latents: torch.Tensor,\n         context: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         **kwargs,\n@@ -746,7 +749,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n         \"\"\"\n         residual = latents\n \n@@ -834,7 +837,7 @@ def forward(\n                 context,\n                 attention_mask=attention_mask,\n                 position_ids=None,\n-                past_key_value=None,\n+                past_key_values=None,\n                 output_attentions=False,\n                 use_cache=False,\n             )"
        },
        {
            "sha": "8ac7e905a20b9a5c28a8c879a486826bbb8df472",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 29,
            "deletions": 25,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -46,6 +46,7 @@\n from ...processing_utils import Unpack\n from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_informer import InformerConfig\n \n \n@@ -433,11 +434,12 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -462,19 +464,19 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -484,15 +486,15 @@ def forward(\n             key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n             value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -556,11 +558,12 @@ def __init__(\n     def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -579,19 +582,19 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states) * self.scaling\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -601,15 +604,15 @@ def forward(\n             key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n             value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n         query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n@@ -879,6 +882,7 @@ def __init__(self, config: InformerConfig, layer_idx: Optional[int] = None):\n                 layer_idx=layer_idx,\n             )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -887,7 +891,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -905,7 +909,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -918,7 +922,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n@@ -938,7 +942,7 @@ def forward(\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -1283,7 +1287,7 @@ def forward(\n                 encoder_attention_mask=encoder_attention_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "f8fb02c1c556f741c4182599c439871823ee96f5",
            "filename": "src/transformers/models/informer/modular_informer.py",
            "status": "modified",
            "additions": 12,
            "deletions": 10,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -37,6 +37,7 @@\n     auto_docstring,\n     is_torch_flex_attn_available,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ..bart.modeling_bart import BartAttention\n from ..time_series_transformer.modeling_time_series_transformer import (\n     TimeSeriesFeatureEmbedder,\n@@ -245,11 +246,12 @@ def __init__(\n     def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -268,19 +270,19 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states) * self.scaling\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -290,15 +292,15 @@ def forward(\n             key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n             value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n         query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)"
        },
        {
            "sha": "4fe7d6cee106527512eaa93f180fa8aed6c71f60",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 26,
            "deletions": 21,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -40,6 +40,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_ssm_available\n from .configuration_jamba import JambaConfig\n \n@@ -317,12 +318,13 @@ def __init__(self, config: JambaConfig, layer_idx: Optional[int] = None):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[HybridMambaAttentionDynamicCache] = None,\n+        past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -337,8 +339,8 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx)\n+        if past_key_values is not None:\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx)\n \n         # repeat k/v heads if n_kv_heads < n_heads\n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n@@ -369,7 +371,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights, past_key_values\n \n \n # Adapted from transformers.models.mistral.modeling_mistral.MistralFlashAttention2 with Mistral->Jamba\n@@ -393,7 +395,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[HybridMambaAttentionDynamicCache] = None,\n+        past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -412,8 +414,8 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx)\n+        if past_key_values is not None:\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx)\n \n         # repeat k/v heads if n_kv_heads < n_heads\n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n@@ -470,7 +472,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights, past_key_values\n \n \n # Adapted from transformers.models.mistral.modeling_mistral.MistralSdpaAttention with Mistral->Jamba\n@@ -482,12 +484,13 @@ class JambaSdpaAttention(JambaAttention):\n     \"\"\"\n \n     # Adapted from JambaAttention.forward\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[HybridMambaAttentionDynamicCache] = None,\n+        past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -502,7 +505,7 @@ def forward(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n             )\n@@ -517,8 +520,8 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx)\n+        if past_key_values is not None:\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx)\n \n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n         value_states = repeat_kv(value_states, self.num_key_value_groups)\n@@ -553,7 +556,7 @@ def forward(\n \n         attn_output = self.o_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n+        return attn_output, None, past_key_values\n \n \n JAMBA_ATTENTION_CLASSES = {\n@@ -923,12 +926,13 @@ def __init__(self, config: JambaConfig, layer_idx: int):\n         self.input_layernorm = JambaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.pre_ff_layernorm = JambaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[HybridMambaAttentionDynamicCache] = None,\n+        past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n         output_attentions: Optional[bool] = False,\n         output_router_logits: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n@@ -939,7 +943,7 @@ def forward(\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n             attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n                 `(batch, sequence_length)` where padding elements are indicated by 0.\n-            past_key_value (`HybridMambaAttentionDynamicCache`, *optional*): cached past key and value projection states\n+            past_key_values (`HybridMambaAttentionDynamicCache`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -961,7 +965,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -1005,12 +1009,13 @@ def __init__(self, config: JambaConfig, layer_idx: int):\n         self.input_layernorm = JambaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.pre_ff_layernorm = JambaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[HybridMambaAttentionDynamicCache] = None,\n+        past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n         output_attentions: Optional[bool] = False,\n         output_router_logits: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n@@ -1021,7 +1026,7 @@ def forward(\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n             attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n                 `(batch, sequence_length)` where padding elements are indicated by 0.\n-            past_key_value (`HybridMambaAttentionDynamicCache`, *optional*): cached past key and value projection states\n+            past_key_values (`HybridMambaAttentionDynamicCache`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -1041,7 +1046,7 @@ def forward(\n \n         hidden_states = self.mamba(\n             hidden_states=hidden_states,\n-            cache_params=past_key_value,\n+            cache_params=past_key_values,\n             attention_mask=attention_mask,\n         )\n         self_attn_weights = None\n@@ -1065,7 +1070,7 @@ def forward(\n             outputs += (self_attn_weights,)\n \n         if use_cache:\n-            outputs += (past_key_value,)\n+            outputs += (past_key_values,)\n \n         if output_router_logits:\n             outputs += (router_logits,)\n@@ -1203,7 +1208,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=layer_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 output_router_logits=output_router_logits,\n                 use_cache=use_cache,"
        },
        {
            "sha": "2159761547276df0f047fe6fe9fc6767964d3d0c",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 18,
            "deletions": 13,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -35,6 +35,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_jetmoe import JetMoeConfig\n \n \n@@ -498,12 +499,13 @@ def __init__(self, config: JetMoeConfig, layer_idx: Optional[int] = None):\n \n         self.rotary_emb = JetMoeRotaryEmbedding(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -520,10 +522,10 @@ def forward(\n         cos, sin = self.rotary_emb(value_states, position_ids)\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         # repeat k/v heads for top-k attention experts\n         key_states = key_states.repeat(1, self.top_k, 1, 1)\n@@ -566,12 +568,13 @@ class JetMoeSdpaAttention(JetMoeAttention):\n     \"\"\"\n \n     # Adapted from JetMoeAttention.forward\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -586,7 +589,7 @@ def forward(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -604,10 +607,10 @@ def forward(\n         cos, sin = self.rotary_emb(value_states, position_ids)\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         # repeat k/v heads for top-k attention experts\n         key_states = key_states.repeat(1, self.top_k, 1, 1)\n@@ -655,12 +658,13 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: Optional[torch.FloatTensor],\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -696,10 +700,10 @@ def forward(\n         cos, sin = self.rotary_emb(value_states, position_ids)\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         # repeat k/v heads for top-k attention experts\n         key_states = key_states.repeat(1, self.top_k, 1, 1)\n@@ -789,11 +793,12 @@ def __init__(self, config: JetMoeConfig, layer_idx: Optional[int] = None):\n \n         self.mlp = JetMoeMoE(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: Optional[torch.FloatTensor],\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         output_router_logits: Optional[bool] = False,\n@@ -805,7 +810,7 @@ def forward(\n             hidden_states=self.input_layernorm(hidden_states),\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -961,7 +966,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 output_router_logits=output_router_logits,\n                 use_cache=use_cache,"
        },
        {
            "sha": "92aa3ace5a8e3f9da3998bc611f3b97c2d2bd2cb",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 18,
            "deletions": 15,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -36,6 +36,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging, torch_int\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_kosmos2 import Kosmos2Config, Kosmos2TextConfig, Kosmos2VisionConfig\n \n \n@@ -709,11 +710,12 @@ def __init__(\n         if add_inner_attn_layernorm:\n             self.inner_attn_ln = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -730,19 +732,19 @@ def forward(\n         query_states = self.q_proj(hidden_states)\n         query_states = query_states.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -752,15 +754,15 @@ def forward(\n             key_states = key_states.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n             value_states = value_states.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n \n@@ -848,6 +850,7 @@ def __init__(self, config: Kosmos2TextConfig, layer_idx=None):\n         self.ffn = Kosmos2TextFFN(config)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -856,7 +859,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -867,7 +870,7 @@ def forward(\n \n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n@@ -894,7 +897,7 @@ def forward(\n                 encoder_hidden_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n                 **kwargs,\n@@ -1114,7 +1117,7 @@ def forward(\n                 encoder_attention_mask=encoder_attention_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -1495,7 +1498,7 @@ def forward(self, features):\n         hidden_states, attn_weights = self.x_attn(\n             hidden_states=latent_query,\n             encoder_hidden_states=key_value_states,\n-            past_key_value=None,\n+            past_key_values=None,\n             attention_mask=None,\n             output_attentions=None,\n         )"
        },
        {
            "sha": "89f269f8a0fce7a403ca0b863f7bba78a39a77fc",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 20,
            "deletions": 17,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -37,6 +37,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel\n from .configuration_kyutai_speech_to_text import KyutaiSpeechToTextConfig\n \n@@ -428,14 +429,13 @@ def __init__(\n             self.rope_theta = config.rope_theta\n             self.rotary_emb = KyutaiSpeechToTextRotaryEmbedding(config)\n \n-    # copied from transformers.models.gemma.modeling_gemma.GemmaAttention.forward\n-    # no longer copied after attention refactors\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -454,14 +454,14 @@ def forward(\n             cos, sin = self.rotary_emb(value_states, position_ids)  # Ignore copy\n             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)  # Ignore copy\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = (\n                 {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n                 if self.rotary_emb is not None\n                 else {\"cache_position\": cache_position}\n             )  # Ignore copy\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n         value_states = repeat_kv(value_states, self.num_key_value_groups)\n@@ -511,17 +511,18 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        if isinstance(past_key_value, StaticCache):\n+        if isinstance(past_key_values, StaticCache):\n             raise ValueError(\n                 \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n                 \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n@@ -546,14 +547,14 @@ def forward(\n             cos, sin = self.rotary_emb(value_states, position_ids)  # Ignore copy\n             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)  # Ignore copy\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = (\n                 {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n                 if self.rotary_emb is not None\n                 else {\"cache_position\": cache_position}\n             )  # Ignore copy\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n         # to be able to avoid many of these transpose/reshape/view.\n@@ -626,12 +627,13 @@ class KyutaiSpeechToTextSdpaAttention(KyutaiSpeechToTextAttention):\n     \"\"\"\n \n     # Adapted from KyutaiSpeechToTextAttention.forward\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -647,7 +649,7 @@ def forward(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -667,14 +669,14 @@ def forward(\n             cos, sin = self.rotary_emb(value_states, position_ids)  # Ignore copy\n             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)  # Ignore copy\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = (\n                 {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n                 if self.rotary_emb is not None\n                 else {\"cache_position\": cache_position}\n             )  # Ignore copy\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n         value_states = repeat_kv(value_states, self.num_key_value_groups)\n@@ -735,12 +737,13 @@ def __init__(self, config: KyutaiSpeechToTextConfig, layer_idx: int, use_flexibl\n \n         self._attn_implementation = config._attn_implementation\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -758,7 +761,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             kwargs (`dict`, *optional*):\n@@ -774,7 +777,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -884,7 +887,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "9ca56d893146de1f315776a1b1dd8e3a4e9b5cc9",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 21,
            "deletions": 18,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -32,6 +32,7 @@\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_led import LEDConfig\n \n \n@@ -790,11 +791,12 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -810,19 +812,19 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states) * self.scaling\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -832,15 +834,15 @@ def forward(\n             key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n             value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n         query_states = query_states.view(bsz, tgt_len, self.num_heads, self.head_dim).transpose(1, 2)\n@@ -903,7 +905,7 @@ def forward(\n \n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights_reshaped, past_key_value\n+        return attn_output, attn_weights_reshaped, past_key_values\n \n \n class LEDEncoderLayer(GradientCheckpointingLayer):\n@@ -997,6 +999,7 @@ def __init__(self, config: LEDConfig, layer_idx=None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -1005,7 +1008,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -1023,7 +1026,7 @@ def forward(\n                 *(decoder_attention_heads,)*.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for encoder attention heads in a given layer of\n                 size *(decoder_attention_heads,)*.\n-            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n             output_attentions (`bool`): Whether the base model outputs attentions.\n                 This requires the attentions tensor to be reshaped in this function.\n         \"\"\"\n@@ -1032,7 +1035,7 @@ def forward(\n         # Self-Attention\n         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n@@ -1053,7 +1056,7 @@ def forward(\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -1076,7 +1079,7 @@ def forward(\n             outputs += (self_attn_weights, cross_attn_weights)\n \n         if use_cache:\n-            outputs += (past_key_value,)\n+            outputs += (past_key_values,)\n \n         return outputs\n \n@@ -1826,7 +1829,7 @@ def forward(\n                 encoder_attention_mask=encoder_attention_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -1980,7 +1983,7 @@ def forward(\n                 global_attentions=encoder_outputs[3] if len(encoder_outputs) > 3 else None,\n             )\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,"
        },
        {
            "sha": "7fc244cb58aea0e290b64687b9c45a4ba1ce219a",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 27,
            "deletions": 21,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -33,6 +33,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from ...utils.import_utils import is_causal_conv1d_available\n from .configuration_lfm2 import Lfm2Config\n@@ -359,12 +360,13 @@ def __init__(self, config: Lfm2Config, layer_idx: int):\n         self.q_layernorm = Lfm2RMSNorm(self.head_dim, eps=config.norm_eps)\n         self.k_layernorm = Lfm2RMSNorm(self.head_dim, eps=config.norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Lfm2HybridConvCache] = None,\n+        past_key_values: Optional[Lfm2HybridConvCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -378,9 +380,9 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -439,10 +441,11 @@ def __init__(\n         self.in_proj = nn.Linear(config.hidden_size, 3 * config.hidden_size, bias=self.bias)\n         self.out_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=self.bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def cuda_kernels_forward(\n         self,\n         x: torch.Tensor,\n-        past_key_value: Optional[Lfm2HybridConvCache] = None,\n+        past_key_values: Optional[Lfm2HybridConvCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n     ):\n@@ -453,30 +456,31 @@ def cuda_kernels_forward(\n         Bx = B * x\n \n         conv_weights = self.conv.weight.view(self.conv.weight.size(0), self.conv.weight.size(2))\n-        if past_key_value is not None and cache_position[0] > 0:\n+        if past_key_values is not None and cache_position[0] > 0:\n             conv_out = causal_conv1d_update(\n                 Bx.squeeze(-1),\n-                past_key_value.conv_cache[self.layer_idx],\n+                past_key_values.conv_cache[self.layer_idx],\n                 conv_weights,\n                 self.conv.bias,\n                 None,\n             )\n             conv_out = conv_out.unsqueeze(-1)\n         else:\n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 conv_state = nn.functional.pad(Bx, (self.L_cache - Bx.shape[-1], 0))\n-                past_key_value.conv_cache[self.layer_idx].copy_(conv_state)\n+                past_key_values.conv_cache[self.layer_idx].copy_(conv_state)\n \n             conv_out = causal_conv1d_fn(Bx, conv_weights, self.conv.bias, activation=None)\n \n         y = C * conv_out\n         y = self.out_proj(y.transpose(-1, -2).contiguous())\n         return y\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def slow_forward(\n         self,\n         x: torch.Tensor,\n-        past_key_value: Optional[Lfm2HybridConvCache] = None,\n+        past_key_values: Optional[Lfm2HybridConvCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n     ):\n@@ -488,21 +492,21 @@ def slow_forward(\n \n         Bx = B * x\n \n-        if past_key_value is not None and cache_position[0] > 0:\n-            conv_state = past_key_value.conv_cache[self.layer_idx]\n+        if past_key_values is not None and cache_position[0] > 0:\n+            conv_state = past_key_values.conv_cache[self.layer_idx]\n             cache_position = cache_position.clamp(0, self.L_cache - 1)\n             conv_state = conv_state.roll(shifts=-1, dims=-1)\n             conv_state[:, :, cache_position] = Bx.to(device=conv_state.device, dtype=conv_state.dtype)\n-            past_key_value.conv_cache[self.layer_idx].copy_(conv_state)\n+            past_key_values.conv_cache[self.layer_idx].copy_(conv_state)\n             conv_out = torch.sum(conv_state.to(Bx.device) * self.conv.weight[:, 0, :], dim=-1)\n             if self.bias:\n                 conv_out += self.conv.bias\n \n             conv_out = conv_out.unsqueeze(-1)\n         else:\n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 conv_state = nn.functional.pad(Bx, (self.L_cache - Bx.shape[-1], 0))\n-                past_key_value.conv_cache[self.layer_idx].copy_(conv_state)\n+                past_key_values.conv_cache[self.layer_idx].copy_(conv_state)\n \n             conv_out = self.conv(Bx)[..., :seqlen]\n \n@@ -511,16 +515,17 @@ def slow_forward(\n         y = self.out_proj(y)\n         return y\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        past_key_value: Optional[Lfm2HybridConvCache] = None,\n+        past_key_values: Optional[Lfm2HybridConvCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n     ):\n         if is_fast_path_available and \"cuda\" in hidden_states.device.type and not torch._dynamo.is_compiling():\n-            return self.cuda_kernels_forward(hidden_states, past_key_value, cache_position, attention_mask)\n-        return self.slow_forward(hidden_states, past_key_value, cache_position, attention_mask)\n+            return self.cuda_kernels_forward(hidden_states, past_key_values, cache_position, attention_mask)\n+        return self.slow_forward(hidden_states, past_key_values, cache_position, attention_mask)\n \n \n class Lfm2DecoderLayer(GradientCheckpointingLayer):\n@@ -536,13 +541,14 @@ def __init__(self, config: Lfm2Config, layer_idx: int):\n         self.operator_norm = Lfm2RMSNorm(config.hidden_size, eps=config.norm_eps)\n         self.ffn_norm = Lfm2RMSNorm(config.hidden_size, eps=config.norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n@@ -553,14 +559,14 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 attention_mask=attention_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 **kwargs,\n             )\n         else:\n             hidden_states = self.conv(\n                 hidden_states=self.operator_norm(hidden_states),\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 attention_mask=attention_mask,\n             )\n@@ -659,7 +665,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n                 **kwargs,"
        },
        {
            "sha": "046d79dbdd405388c27c82755552ebb03064369f",
            "filename": "src/transformers/models/lfm2/modular_lfm2.py",
            "status": "modified",
            "additions": 27,
            "deletions": 21,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -24,6 +24,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import is_causal_conv1d_available\n from ..bamba.modeling_bamba import apply_mask_to_padding_states\n from ..llama.modeling_llama import (\n@@ -240,12 +241,13 @@ def __init__(self, config: Lfm2Config, layer_idx: int):\n         del self.o_proj\n         del self.attention_dropout\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Lfm2HybridConvCache] = None,\n+        past_key_values: Optional[Lfm2HybridConvCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -259,9 +261,9 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -305,10 +307,11 @@ def __init__(\n         self.in_proj = nn.Linear(config.hidden_size, 3 * config.hidden_size, bias=self.bias)\n         self.out_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=self.bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def cuda_kernels_forward(\n         self,\n         x: torch.Tensor,\n-        past_key_value: Optional[Lfm2HybridConvCache] = None,\n+        past_key_values: Optional[Lfm2HybridConvCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n     ):\n@@ -319,30 +322,31 @@ def cuda_kernels_forward(\n         Bx = B * x\n \n         conv_weights = self.conv.weight.view(self.conv.weight.size(0), self.conv.weight.size(2))\n-        if past_key_value is not None and cache_position[0] > 0:\n+        if past_key_values is not None and cache_position[0] > 0:\n             conv_out = causal_conv1d_update(\n                 Bx.squeeze(-1),\n-                past_key_value.conv_cache[self.layer_idx],\n+                past_key_values.conv_cache[self.layer_idx],\n                 conv_weights,\n                 self.conv.bias,\n                 None,\n             )\n             conv_out = conv_out.unsqueeze(-1)\n         else:\n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 conv_state = nn.functional.pad(Bx, (self.L_cache - Bx.shape[-1], 0))\n-                past_key_value.conv_cache[self.layer_idx].copy_(conv_state)\n+                past_key_values.conv_cache[self.layer_idx].copy_(conv_state)\n \n             conv_out = causal_conv1d_fn(Bx, conv_weights, self.conv.bias, activation=None)\n \n         y = C * conv_out\n         y = self.out_proj(y.transpose(-1, -2).contiguous())\n         return y\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def slow_forward(\n         self,\n         x: torch.Tensor,\n-        past_key_value: Optional[Lfm2HybridConvCache] = None,\n+        past_key_values: Optional[Lfm2HybridConvCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n     ):\n@@ -354,21 +358,21 @@ def slow_forward(\n \n         Bx = B * x\n \n-        if past_key_value is not None and cache_position[0] > 0:\n-            conv_state = past_key_value.conv_cache[self.layer_idx]\n+        if past_key_values is not None and cache_position[0] > 0:\n+            conv_state = past_key_values.conv_cache[self.layer_idx]\n             cache_position = cache_position.clamp(0, self.L_cache - 1)\n             conv_state = conv_state.roll(shifts=-1, dims=-1)\n             conv_state[:, :, cache_position] = Bx.to(device=conv_state.device, dtype=conv_state.dtype)\n-            past_key_value.conv_cache[self.layer_idx].copy_(conv_state)\n+            past_key_values.conv_cache[self.layer_idx].copy_(conv_state)\n             conv_out = torch.sum(conv_state.to(Bx.device) * self.conv.weight[:, 0, :], dim=-1)\n             if self.bias:\n                 conv_out += self.conv.bias\n \n             conv_out = conv_out.unsqueeze(-1)\n         else:\n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 conv_state = nn.functional.pad(Bx, (self.L_cache - Bx.shape[-1], 0))\n-                past_key_value.conv_cache[self.layer_idx].copy_(conv_state)\n+                past_key_values.conv_cache[self.layer_idx].copy_(conv_state)\n \n             conv_out = self.conv(Bx)[..., :seqlen]\n \n@@ -377,16 +381,17 @@ def slow_forward(\n         y = self.out_proj(y)\n         return y\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        past_key_value: Optional[Lfm2HybridConvCache] = None,\n+        past_key_values: Optional[Lfm2HybridConvCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n     ):\n         if is_fast_path_available and \"cuda\" in hidden_states.device.type and not torch._dynamo.is_compiling():\n-            return self.cuda_kernels_forward(hidden_states, past_key_value, cache_position, attention_mask)\n-        return self.slow_forward(hidden_states, past_key_value, cache_position, attention_mask)\n+            return self.cuda_kernels_forward(hidden_states, past_key_values, cache_position, attention_mask)\n+        return self.slow_forward(hidden_states, past_key_values, cache_position, attention_mask)\n \n \n class Lfm2DecoderLayer(GradientCheckpointingLayer):\n@@ -402,13 +407,14 @@ def __init__(self, config: Lfm2Config, layer_idx: int):\n         self.operator_norm = Lfm2RMSNorm(config.hidden_size, eps=config.norm_eps)\n         self.ffn_norm = Lfm2RMSNorm(config.hidden_size, eps=config.norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n@@ -419,14 +425,14 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 attention_mask=attention_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 **kwargs,\n             )\n         else:\n             hidden_states = self.conv(\n                 hidden_states=self.operator_norm(hidden_states),\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 attention_mask=attention_mask,\n             )\n@@ -498,7 +504,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n                 **kwargs,"
        },
        {
            "sha": "ee3b39ed9e861855f54083112d41ef7f0873112c",
            "filename": "src/transformers/models/lightglue/modeling_lightglue.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -30,6 +30,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import can_return_tuple\n from ..auto.modeling_auto import AutoModelForKeypointDetection\n from .configuration_lightglue import LightGlueConfig\n@@ -199,6 +200,7 @@ def __init__(self, config: LightGlueConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "06fbeba4961fd51e3013c8f559010f7e4851b0fb",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -41,6 +41,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_llama import LlamaConfig\n \n@@ -219,12 +220,13 @@ def __init__(self, config: LlamaConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n@@ -238,10 +240,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -274,12 +276,13 @@ def __init__(self, config: LlamaConfig, layer_idx: int):\n         self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -292,7 +295,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -393,7 +396,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n                 **kwargs,"
        },
        {
            "sha": "2aeb414d19a8c49a1acfebecc7ceaefe58679a5c",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -35,6 +35,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_llama4 import Llama4Config, Llama4TextConfig\n \n@@ -308,12 +309,13 @@ def __init__(self, config: Llama4TextConfig, layer_idx):\n         if self.config.use_qk_norm and self.use_rope:\n             self.qk_norm = Llama4TextL2Norm(config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -344,10 +346,10 @@ def forward(\n         query_states = query_states.transpose(1, 2)\n         key_states = key_states.transpose(1, 2)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -384,12 +386,13 @@ def __init__(self, config, layer_idx):\n         self.input_layernorm = Llama4TextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = Llama4TextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -404,7 +407,7 @@ def forward(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n@@ -545,7 +548,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=freq_cis,\n@@ -783,7 +786,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         freqs_ci: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]"
        },
        {
            "sha": "1d89feab0567bd5229d954bc2d50ac6a5070b6f9",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 24,
            "deletions": 19,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -45,6 +45,7 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_longt5 import LongT5Config\n \n \n@@ -441,13 +442,14 @@ def compute_bias(self, query_length, key_length, device=None, cache_position=Non\n         values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, query_length, key_length)\n         return values\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         mask=None,\n         key_value_states=None,\n         position_bias=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         layer_head_mask=None,\n         query_length=None,\n         use_cache=False,\n@@ -468,18 +470,18 @@ def forward(\n         query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n         # Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`\n-        if past_key_value is not None and isinstance(past_key_value, EncoderDecoderCache):\n-            is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None and isinstance(past_key_values, EncoderDecoderCache):\n+            is_updated = past_key_values.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                curr_past_key_value = past_key_value.cross_attention_cache\n+                curr_past_key_value = past_key_values.cross_attention_cache\n             else:\n-                curr_past_key_value = past_key_value.self_attention_cache\n+                curr_past_key_value = past_key_values.self_attention_cache\n         else:\n-            curr_past_key_value = past_key_value\n+            curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -489,15 +491,15 @@ def forward(\n             key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n             value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n         scores = torch.matmul(query_states, key_states.transpose(3, 2))\n@@ -1018,13 +1020,14 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n         self.layer_norm = LongT5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         position_bias=None,\n         layer_head_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         use_cache=False,\n         output_attentions=False,\n         cache_position=None,\n@@ -1035,7 +1038,7 @@ def forward(\n             mask=attention_mask,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -1061,7 +1064,7 @@ def forward(\n         position_bias=None,\n         layer_head_mask=None,\n         output_attentions=False,\n-        **kwargs: Any,  # to accept past_key_value and use_cache kwargs\n+        **kwargs: Any,  # to accept past_key_values and use_cache kwargs\n     ):\n         normed_hidden_states = self.layer_norm(hidden_states)\n         attention_output = self.LocalSelfAttention(\n@@ -1094,7 +1097,7 @@ def forward(\n         position_bias=None,\n         layer_head_mask=None,\n         output_attentions=False,\n-        **kwargs: Any,  # to accept past_key_value and use_cache kwargs\n+        **kwargs: Any,  # to accept past_key_values and use_cache kwargs\n     ):\n         normed_hidden_states = self.layer_norm(hidden_states)\n         attention_output = self.TransientGlobalSelfAttention(\n@@ -1117,14 +1120,15 @@ def __init__(self, config, layer_idx: Optional[int] = None):\n         self.layer_norm = LongT5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         key_value_states,\n         attention_mask=None,\n         position_bias=None,\n         layer_head_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         use_cache=False,\n         query_length=None,\n         output_attentions=False,\n@@ -1137,7 +1141,7 @@ def forward(\n             key_value_states=key_value_states,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             query_length=query_length,\n             output_attentions=output_attentions,\n@@ -1172,6 +1176,7 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n \n         self.layer.append(LongT5LayerFF(config))\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -1182,7 +1187,7 @@ def forward(\n         encoder_decoder_position_bias=None,\n         layer_head_mask=None,\n         cross_attn_layer_head_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         use_cache=False,\n         output_attentions=False,\n         return_dict=True,\n@@ -1193,7 +1198,7 @@ def forward(\n             attention_mask=attention_mask,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -1214,7 +1219,7 @@ def forward(\n                 attention_mask=encoder_attention_mask,\n                 position_bias=encoder_decoder_position_bias,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 query_length=cache_position[-1] + 1,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n@@ -1497,7 +1502,7 @@ def forward(\n                 encoder_decoder_position_bias,  # as a positional argument for gradient checkpointing\n                 layer_head_mask=layer_head_mask,\n                 cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n                 return_dict=return_dict,"
        },
        {
            "sha": "fadba94f9efde4dedf90fa8e1c1d81e25b314444",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 20,
            "deletions": 16,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -44,6 +44,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, is_torch_flex_attn_available, is_torchdynamo_compiling, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_m2m_100 import M2M100Config\n \n \n@@ -251,11 +252,12 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -280,19 +282,19 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -302,15 +304,15 @@ def forward(\n             key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n             value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -432,6 +434,7 @@ def __init__(self, config: M2M100Config, layer_idx: Optional[int] = None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -440,7 +443,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -458,7 +461,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -472,7 +475,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n@@ -492,7 +495,7 @@ def forward(\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n@@ -511,6 +514,7 @@ def forward(\n \n         if output_attentions:\n             outputs += (self_attn_weights, cross_attn_weights)\n+\n         return outputs\n \n \n@@ -1133,7 +1137,7 @@ def forward(\n                     cross_attn_layer_head_mask=(\n                         cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n                     ),\n-                    past_key_value=past_key_values,\n+                    past_key_values=past_key_values,\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n@@ -1275,7 +1279,7 @@ def forward(\n                 attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n             )\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,"
        },
        {
            "sha": "20dc02213dc7335754fe2afad233746df05f7879",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 20,
            "deletions": 16,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -49,6 +49,7 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_marian import MarianConfig\n \n \n@@ -186,11 +187,12 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -215,19 +217,19 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -237,15 +239,15 @@ def forward(\n             key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n             value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -375,6 +377,7 @@ def __init__(self, config: MarianConfig, layer_idx: Optional[int] = None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -383,7 +386,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -401,7 +404,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -414,7 +417,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n@@ -434,7 +437,7 @@ def forward(\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -455,6 +458,7 @@ def forward(\n \n         if output_attentions:\n             outputs += (self_attn_weights, cross_attn_weights)\n+\n         return outputs\n \n \n@@ -1069,7 +1073,7 @@ def forward(\n                 encoder_attention_mask=encoder_attention_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -1276,7 +1280,7 @@ def forward(\n                 attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n             )\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,"
        },
        {
            "sha": "11d8ca2d269cd565d20f8490c16281960b28d9af",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 19,
            "deletions": 16,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -51,6 +51,7 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_mbart import MBartConfig\n \n \n@@ -195,11 +196,12 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -224,19 +226,19 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -246,15 +248,15 @@ def forward(\n             key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n             value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -374,6 +376,7 @@ def __init__(self, config: MBartConfig, layer_idx: Optional[int] = None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -382,7 +385,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -400,7 +403,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -414,7 +417,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n@@ -434,7 +437,7 @@ def forward(\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n@@ -1113,7 +1116,7 @@ def forward(\n                 encoder_attention_mask=encoder_attention_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -1262,7 +1265,7 @@ def forward(\n                 attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n             )\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,"
        },
        {
            "sha": "b12e97e68cf30c6b7a025d8adda48f0e3b743c7d",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 20,
            "deletions": 16,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -44,6 +44,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_megatron_bert import MegatronBertConfig\n \n \n@@ -206,13 +207,14 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -223,19 +225,19 @@ def forward(\n         )\n \n         is_cross_attention = encoder_hidden_states is not None\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_layer = curr_past_key_value.layers[self.layer_idx].keys\n             value_layer = curr_past_key_value.layers[self.layer_idx].values\n@@ -249,22 +251,22 @@ def forward(\n                 batch_size, -1, self.num_attention_heads, self.attention_head_size\n             ).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_layer, value_layer = curr_past_key_value.update(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -349,13 +351,14 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -365,7 +368,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -420,14 +423,15 @@ def __init__(self, config, layer_idx=None):\n         self.intermediate = MegatronBertIntermediate(config)\n         self.output = MegatronBertOutput(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -437,7 +441,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n@@ -455,7 +459,7 @@ def forward(\n                 attention_mask=encoder_attention_mask,\n                 head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )"
        },
        {
            "sha": "0cabf4849c1d0546d00501cff9ac7dcfcb4a7f6e",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 20,
            "deletions": 15,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -31,6 +31,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_mimi import MimiConfig\n \n \n@@ -643,12 +644,13 @@ def __init__(self, config: MimiConfig, layer_idx: Optional[int] = None):\n         self.rotary_emb = MimiRotaryEmbedding(config)\n         self.sliding_window = config.sliding_window  # Ignore copy\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -666,10 +668,10 @@ def forward(\n         cos, sin = self.rotary_emb(value_states, position_ids)\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n         value_states = repeat_kv(value_states, self.num_key_value_groups)\n@@ -719,17 +721,18 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        if isinstance(past_key_value, StaticCache):\n+        if isinstance(past_key_values, StaticCache):\n             raise ValueError(\n                 \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n                 \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n@@ -753,10 +756,10 @@ def forward(\n         cos, sin = self.rotary_emb(value_states, position_ids)\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n         # to be able to avoid many of these transpose/reshape/view.\n@@ -829,12 +832,13 @@ class MimiSdpaAttention(MimiAttention):\n     \"\"\"\n \n     # Adapted from MimiAttention.forward\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -850,7 +854,7 @@ def forward(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -869,10 +873,10 @@ def forward(\n         cos, sin = self.rotary_emb(value_states, position_ids)\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n         value_states = repeat_kv(value_states, self.num_key_value_groups)\n@@ -929,12 +933,13 @@ def __init__(self, config: MimiConfig, layer_idx: int):\n         self.self_attn_layer_scale = MimiLayerScale(config)\n         self.mlp_layer_scale = MimiLayerScale(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -952,7 +957,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             kwargs (`dict`, *optional*):\n@@ -968,7 +973,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -1130,7 +1135,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "cfd1af9cecee39440ecf35323dce48b29ca4f779",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 16,
            "deletions": 12,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -45,6 +45,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder\n from .configuration_minimax import MiniMaxConfig\n \n@@ -164,12 +165,13 @@ def decay_factors(self, slope_rate):\n \n         return query_decay, key_decay, diagonal_decay\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -187,8 +189,8 @@ def forward(\n \n         # calculated (K.T @ V) and saved as cache\n         attn_weights_inter = None\n-        if past_key_value is not None:\n-            attn_weights_inter = past_key_value.get_linear_cache(self.layer_idx)\n+        if past_key_values is not None:\n+            attn_weights_inter = past_key_values.get_linear_cache(self.layer_idx)\n \n         if attn_weights_inter is None:\n             attn_weights_inter = torch.zeros(batch_size, self.num_attention_heads, self.head_dim, self.head_dim).to(\n@@ -257,8 +259,8 @@ def forward(\n         attn_output = self.out_proj(attn_output)\n \n         # update cache\n-        if past_key_value is not None:\n-            past_key_value.set_linear_cache(self.layer_idx, attn_weights_inter)\n+        if past_key_values is not None:\n+            past_key_values.set_linear_cache(self.layer_idx, attn_weights_inter)\n \n         return attn_output, attn_weights_inter\n \n@@ -352,12 +354,13 @@ def __init__(self, config: MiniMaxConfig, layer_idx: int):\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -371,10 +374,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -507,13 +510,14 @@ def __init__(self, config: MiniMaxConfig, layer_idx: int):\n             self.attn_alpha_factor = config.full_attn_alpha_factor\n             self.attn_beta_factor = config.full_attn_beta_factor\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         output_attentions: Optional[bool] = False,\n         output_router_logits: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n@@ -528,7 +532,7 @@ def forward(\n                 with `head_dim` being the embedding dimension of each attention head.\n             attention_mask (`torch.Tensor`, *optional*): attention mask of size\n                 `(batch, sequence_length)` where padding elements are indicated by 0.\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -554,7 +558,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -706,7 +710,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 attention_mask=input_attention_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,"
        },
        {
            "sha": "9582e5f392ac8ba0a50a3e21c171cae6e6606357",
            "filename": "src/transformers/models/minimax/modular_minimax.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -30,6 +30,7 @@\n from ...modeling_outputs import MoeModelOutputWithPast\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder\n from ..mixtral.configuration_mixtral import MixtralConfig\n from ..mixtral.modeling_mixtral import (\n@@ -278,12 +279,13 @@ def decay_factors(self, slope_rate):\n \n         return query_decay, key_decay, diagonal_decay\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -301,8 +303,8 @@ def forward(\n \n         # calculated (K.T @ V) and saved as cache\n         attn_weights_inter = None\n-        if past_key_value is not None:\n-            attn_weights_inter = past_key_value.get_linear_cache(self.layer_idx)\n+        if past_key_values is not None:\n+            attn_weights_inter = past_key_values.get_linear_cache(self.layer_idx)\n \n         if attn_weights_inter is None:\n             attn_weights_inter = torch.zeros(batch_size, self.num_attention_heads, self.head_dim, self.head_dim).to(\n@@ -371,8 +373,8 @@ def forward(\n         attn_output = self.out_proj(attn_output)\n \n         # update cache\n-        if past_key_value is not None:\n-            past_key_value.set_linear_cache(self.layer_idx, attn_weights_inter)\n+        if past_key_values is not None:\n+            past_key_values.set_linear_cache(self.layer_idx, attn_weights_inter)\n \n         return attn_output, attn_weights_inter\n \n@@ -403,13 +405,14 @@ def __init__(self, config: MiniMaxConfig, layer_idx: int):\n             self.attn_alpha_factor = config.full_attn_alpha_factor\n             self.attn_beta_factor = config.full_attn_beta_factor\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         output_attentions: Optional[bool] = False,\n         output_router_logits: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n@@ -424,7 +427,7 @@ def forward(\n                 with `head_dim` being the embedding dimension of each attention head.\n             attention_mask (`torch.Tensor`, *optional*): attention mask of size\n                 `(batch, sequence_length)` where padding elements are indicated by 0.\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -450,7 +453,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -537,7 +540,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 attention_mask=input_attention_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,"
        },
        {
            "sha": "7d712eeaa2352db637d4fbe4efdf4ed9dcd4c710",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -28,6 +28,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_mistral import MistralConfig\n \n \n@@ -136,12 +137,13 @@ def __init__(self, config: MistralConfig, layer_idx: int):\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -155,10 +157,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -211,12 +213,13 @@ def __init__(self, config: MistralConfig, layer_idx: int):\n         self.input_layernorm = MistralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = MistralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -229,7 +232,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -367,7 +370,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,"
        },
        {
            "sha": "94aacded35fc22e62252fff182625f8f53fa327e",
            "filename": "src/transformers/models/mistral/modular_mistral.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -15,6 +15,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..llama.modeling_llama import (\n     LlamaAttention,\n     LlamaDecoderLayer,\n@@ -50,12 +51,13 @@ def __init__(self, config: MistralConfig, layer_idx: int):\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -69,10 +71,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -159,7 +161,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,"
        },
        {
            "sha": "41c3000c8911a44960f7e68c611c1125c1b127a3",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -49,6 +49,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder\n from .configuration_mixtral import MixtralConfig\n \n@@ -248,12 +249,13 @@ def __init__(self, config: MixtralConfig, layer_idx: int):\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -267,10 +269,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -304,13 +306,14 @@ def __init__(self, config: MixtralConfig, layer_idx: int):\n         self.input_layernorm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.FloatTensor:\n@@ -324,7 +327,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -463,7 +466,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,"
        },
        {
            "sha": "a2090d8d5a102996c635a347a5150f362c2f57d7",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -33,6 +33,7 @@\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder\n from ..mistral.modeling_mistral import (\n     MistralAttention,\n@@ -237,13 +238,14 @@ def __init__(self, config: MixtralConfig, layer_idx: int):\n         self.input_layernorm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.FloatTensor:\n@@ -257,7 +259,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -335,7 +337,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,"
        },
        {
            "sha": "c47ad38414702713ac78bac4ab734c4b67bc210d",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 19,
            "deletions": 14,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -33,6 +33,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_mllama import MllamaConfig, MllamaTextConfig, MllamaVisionConfig\n \n@@ -407,11 +408,12 @@ def __init__(\n         self.q_norm = MllamaTextRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n         self.k_norm = MllamaTextRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         cross_attention_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -430,16 +432,16 @@ def forward(\n             value_states = value_states.view(bsz, -1, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n             key_states = self.k_norm(key_states)\n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # if we have a new image + new tokens, we only computed key_states on that new image\n                 # we still update the cross key states, past_image, new_image. And use it!\n-                key_states, value_states = past_key_value.update(\n+                key_states, value_states = past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n         elif cache_position[0] != 0:\n             key_states, value_states = (\n-                past_key_value.layers[self.layer_idx].keys,\n-                past_key_value.layers[self.layer_idx].values,\n+                past_key_values.layers[self.layer_idx].keys,\n+                past_key_values.layers[self.layer_idx].values,\n             )\n         else:\n             raise ValueError(\n@@ -523,13 +525,14 @@ def __init__(self, config: MllamaTextConfig, layer_idx: int):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n         position_embeddings: torch.Tensor,\n         use_cache: bool = False,\n-        past_key_value=None,\n+        past_key_values=None,\n         cache_position=None,\n         **kwargs,\n     ):\n@@ -546,10 +549,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n \n@@ -605,6 +608,7 @@ def __init__(self, config: MllamaTextConfig, layer_idx: int):\n \n         self.layer_idx = layer_idx\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -613,7 +617,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         full_text_row_masked_out_mask: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -629,7 +633,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n@@ -648,7 +652,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -680,6 +684,7 @@ def __init__(self, config: MllamaTextConfig, layer_idx: int) -> None:\n         self.post_attention_layernorm = MllamaTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.cross_attn_mlp_gate = torch.nn.Parameter(torch.zeros(1))\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -688,7 +693,7 @@ def forward(\n         attention_mask: torch.Tensor,\n         full_text_row_masked_out_mask: tuple[torch.Tensor, torch.Tensor],\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[torch.Tensor] = None,\n@@ -701,7 +706,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=cross_attention_mask,\n             cross_attention_states=cross_attention_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -1257,7 +1262,7 @@ def forward(\n                 attention_mask=causal_mask,\n                 full_text_row_masked_out_mask=full_text_row_masked_out_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,"
        },
        {
            "sha": "ece5455e84d130a3ba2cc6bbbcc4d46874d18d9f",
            "filename": "src/transformers/models/modernbert_decoder/modeling_modernbert_decoder.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -43,6 +43,7 @@\n )\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_modernbert_decoder import ModernBertDecoderConfig\n \n@@ -113,12 +114,13 @@ def __init__(self, config: ModernBertDecoderConfig, layer_idx: Optional[int] = N\n \n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: torch.Tensor,\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -132,10 +134,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -173,13 +175,14 @@ def __init__(self, config: ModernBertDecoderConfig, layer_idx: Optional[int] = N\n         self.mlp_norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)\n         self.mlp = ModernBertMLP(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings_global: torch.Tensor,\n         position_embeddings_local: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n@@ -198,7 +201,7 @@ def forward(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -368,7 +371,7 @@ def forward(\n                 position_embeddings_global=position_embeddings_global,\n                 position_embeddings_local=position_embeddings_local,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,"
        },
        {
            "sha": "bb916f45fef56de29c907bdecaa5be8198eca833",
            "filename": "src/transformers/models/modernbert_decoder/modular_modernbert_decoder.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -38,6 +38,7 @@\n )\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n \n \n@@ -290,12 +291,13 @@ def __init__(self, config: ModernBertDecoderConfig, layer_idx: Optional[int] = N\n \n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: torch.Tensor,\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -309,10 +311,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -350,13 +352,14 @@ def __init__(self, config: ModernBertDecoderConfig, layer_idx: Optional[int] = N\n         self.mlp_norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)\n         self.mlp = ModernBertMLP(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings_global: torch.Tensor,\n         position_embeddings_local: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n@@ -375,7 +378,7 @@ def forward(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -545,7 +548,7 @@ def forward(\n                 position_embeddings_global=position_embeddings_global,\n                 position_embeddings_local=position_embeddings_local,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,"
        },
        {
            "sha": "dff967861284189c8fcbe21417db992adf6852d8",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 23,
            "deletions": 19,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -44,6 +44,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_moonshine import MoonshineConfig\n \n \n@@ -205,12 +206,13 @@ def __init__(\n         else:\n             self.head_dim_padding = 0\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         key_value_states: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n@@ -222,20 +224,20 @@ def forward(\n         )\n \n         is_cross_attention = key_value_states is not None\n-        if past_key_value is not None:\n-            is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            is_updated = past_key_values.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                past_key_value.is_updated[self.layer_idx] = True\n-                past_key_value = past_key_value.cross_attention_cache\n+                past_key_values.is_updated[self.layer_idx] = True\n+                past_key_values = past_key_values.cross_attention_cache\n             else:\n-                past_key_value = past_key_value.self_attention_cache\n+                past_key_values = past_key_values.self_attention_cache\n \n         # use key_value_states if cross attention\n         current_states = key_value_states if key_value_states is not None else hidden_states\n-        if is_cross_attention and past_key_value and is_updated:\n-            key_states = past_key_value.layers[self.layer_idx].keys\n-            value_states = past_key_value.layers[self.layer_idx].values\n+        if is_cross_attention and past_key_values and is_updated:\n+            key_states = past_key_values.layers[self.layer_idx].keys\n+            value_states = past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = (\n                 self.k_proj(current_states)\n@@ -247,18 +249,18 @@ def forward(\n                 .view(bsz, -1, self.config.num_key_value_heads, self.head_dim)\n                 .transpose(1, 2)\n             )\n-            if is_cross_attention and past_key_value is not None:\n-                key_states, value_states = past_key_value.update(\n+            if is_cross_attention and past_key_values is not None:\n+                key_states, value_states = past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n \n         if not is_cross_attention:\n             cos, sin = position_embeddings\n             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-                key_states, value_states = past_key_value.update(\n+                key_states, value_states = past_key_values.update(\n                     key_states, value_states, self.layer_idx, cache_kwargs\n                 )\n \n@@ -346,12 +348,13 @@ def __init__(self, config: MoonshineConfig, layer_idx: int):\n         self.input_layernorm = nn.LayerNorm(config.hidden_size, bias=False)\n         self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, bias=False)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -364,7 +367,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -405,6 +408,7 @@ def __init__(self, config: MoonshineConfig, layer_idx: Optional[int] = None):\n         self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, bias=False)\n         self.final_layernorm = nn.LayerNorm(config.hidden_size, bias=False)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -413,7 +417,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         encoder_position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n@@ -427,7 +431,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -442,7 +446,7 @@ def forward(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n             )\n             hidden_states = residual + hidden_states\n@@ -679,7 +683,7 @@ def forward(\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,"
        },
        {
            "sha": "cd81408283c93455a71c2ac5c80b244c0be1890d",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 20,
            "deletions": 17,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -38,6 +38,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..glm.modeling_glm import GlmAttention, GlmRotaryEmbedding, apply_rotary_pos_emb\n from ..llama.modeling_llama import LlamaDecoderLayer, LlamaModel, eager_attention_forward\n from ..whisper.modeling_whisper import WhisperModel, shift_tokens_right\n@@ -304,12 +305,13 @@ def __init__(\n         else:\n             self.head_dim_padding = 0\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         key_value_states: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n@@ -321,20 +323,20 @@ def forward(\n         )\n \n         is_cross_attention = key_value_states is not None\n-        if past_key_value is not None:\n-            is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            is_updated = past_key_values.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                past_key_value.is_updated[self.layer_idx] = True\n-                past_key_value = past_key_value.cross_attention_cache\n+                past_key_values.is_updated[self.layer_idx] = True\n+                past_key_values = past_key_values.cross_attention_cache\n             else:\n-                past_key_value = past_key_value.self_attention_cache\n+                past_key_values = past_key_values.self_attention_cache\n \n         # use key_value_states if cross attention\n         current_states = key_value_states if key_value_states is not None else hidden_states\n-        if is_cross_attention and past_key_value and is_updated:\n-            key_states = past_key_value.layers[self.layer_idx].keys\n-            value_states = past_key_value.layers[self.layer_idx].values\n+        if is_cross_attention and past_key_values and is_updated:\n+            key_states = past_key_values.layers[self.layer_idx].keys\n+            value_states = past_key_values.layers[self.layer_idx].values\n         else:\n             key_states = (\n                 self.k_proj(current_states)\n@@ -346,18 +348,18 @@ def forward(\n                 .view(bsz, -1, self.config.num_key_value_heads, self.head_dim)\n                 .transpose(1, 2)\n             )\n-            if is_cross_attention and past_key_value is not None:\n-                key_states, value_states = past_key_value.update(\n+            if is_cross_attention and past_key_values is not None:\n+                key_states, value_states = past_key_values.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n \n         if not is_cross_attention:\n             cos, sin = position_embeddings\n             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-                key_states, value_states = past_key_value.update(\n+                key_states, value_states = past_key_values.update(\n                     key_states, value_states, self.layer_idx, cache_kwargs\n                 )\n \n@@ -438,6 +440,7 @@ def __init__(self, config: MoonshineConfig, layer_idx: Optional[int] = None):\n         self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, bias=False)\n         self.final_layernorm = nn.LayerNorm(config.hidden_size, bias=False)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -446,7 +449,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         encoder_position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n@@ -460,7 +463,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -475,7 +478,7 @@ def forward(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n             )\n             hidden_states = residual + hidden_states\n@@ -702,7 +705,7 @@ def forward(\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,"
        },
        {
            "sha": "de8f8c1087ee35d179730b544936b46b4e23c1c9",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 21,
            "deletions": 18,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -33,6 +33,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..auto.modeling_auto import AutoModel\n from .configuration_moshi import MoshiConfig, MoshiDepthConfig\n \n@@ -431,14 +432,13 @@ def __init__(self, config: MoshiConfig, layer_idx: Optional[int] = None, use_fle\n             self.rope_theta = config.rope_theta\n             self.rotary_emb = MoshiRotaryEmbedding(config)\n \n-    # copied from transformers.models.gemma.modeling_gemma.GemmaAttention.forward\n-    # no longer copied after attention refactors\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -457,14 +457,14 @@ def forward(\n             cos, sin = self.rotary_emb(value_states, position_ids)  # Ignore copy\n             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)  # Ignore copy\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = (\n                 {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n                 if self.rotary_emb is not None\n                 else {\"cache_position\": cache_position}\n             )  # Ignore copy\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n         value_states = repeat_kv(value_states, self.num_key_value_groups)\n@@ -514,17 +514,18 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        if isinstance(past_key_value, StaticCache):\n+        if isinstance(past_key_values, StaticCache):\n             raise ValueError(\n                 \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n                 \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n@@ -549,14 +550,14 @@ def forward(\n             cos, sin = self.rotary_emb(value_states, position_ids)  # Ignore copy\n             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)  # Ignore copy\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = (\n                 {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n                 if self.rotary_emb is not None\n                 else {\"cache_position\": cache_position}\n             )  # Ignore copy\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n         # to be able to avoid many of these transpose/reshape/view.\n@@ -629,12 +630,13 @@ class MoshiSdpaAttention(MoshiAttention):\n     \"\"\"\n \n     # Adapted from MoshiAttention.forward\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -650,7 +652,7 @@ def forward(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -670,14 +672,14 @@ def forward(\n             cos, sin = self.rotary_emb(value_states, position_ids)  # Ignore copy\n             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)  # Ignore copy\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = (\n                 {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n                 if self.rotary_emb is not None\n                 else {\"cache_position\": cache_position}\n             )  # Ignore copy\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n         value_states = repeat_kv(value_states, self.num_key_value_groups)\n@@ -738,12 +740,13 @@ def __init__(self, config: MoshiConfig, layer_idx: int, use_flexible_linear: boo\n \n         self._attn_implementation = config._attn_implementation\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -761,7 +764,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             kwargs (`dict`, *optional*):\n@@ -777,7 +780,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -1009,7 +1012,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -1295,7 +1298,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "891602c33897f0d43d4b955d4b5c91944360491e",
            "filename": "src/transformers/models/mpt/modeling_mpt.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -36,6 +36,7 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_mpt import MptConfig\n \n \n@@ -86,11 +87,12 @@ def __init__(self, config: MptConfig, layer_idx: Optional[int] = None):\n         self.out_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n         self.layer_idx = layer_idx\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_bias: torch.Tensor,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         cache_position: Optional[torch.Tensor] = None,\n     ):\n@@ -105,12 +107,12 @@ def forward(\n         key_states = key_states.reshape(batch_size, seq_length, self.n_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.reshape(batch_size, seq_length, self.n_heads, self.head_dim).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             cache_kwargs = {\"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_scores = torch.matmul(query_states, key_states.transpose(-1, -2)) * self.softmax_scale\n-        query_length = seq_length if past_key_value is None else seq_length + past_key_value.get_seq_length()\n+        query_length = seq_length if past_key_values is None else seq_length + past_key_values.get_seq_length()\n \n         if position_bias is not None:\n             if len(position_bias.shape) != 3:\n@@ -201,7 +203,7 @@ def forward(\n             layernorm_output,\n             position_bias=position_bias,\n             attention_mask=attention_mask,\n-            past_key_value=layer_past,\n+            past_key_values=layer_past,\n             cache_position=cache_position,\n         )\n \n@@ -246,13 +248,14 @@ def _init_weights(self, module: nn.Module):\n             module.weight.data.fill_(1.0)\n \n     @staticmethod\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def _convert_to_mpt_cache(\n-        past_key_value: tuple[tuple[torch.Tensor, torch.Tensor]],\n+        past_key_values: tuple[tuple[torch.Tensor, torch.Tensor]],\n     ) -> tuple[tuple[torch.Tensor, torch.Tensor]]:\n         \"\"\"\n         Converts the cache to the format expected by Mpt, i.e. to tuple(tuple([batch_size * num_heads, ...]))\n         \"\"\"\n-        batch_size, num_heads, head_dim, seq_length = past_key_value[0][0].shape\n+        batch_size, num_heads, head_dim, seq_length = past_key_values[0][0].shape\n         batch_size_times_num_heads = batch_size * num_heads\n         # key:  [batch_size, num_heads, head_dim, seq_length] -> [batch_size * num_heads, head_dim, seq_length]\n         # value: [batch_size, num_heads, seq_length, head_dim] -> [batch_size * num_heads, seq_length, head_dim]\n@@ -261,7 +264,7 @@ def _convert_to_mpt_cache(\n                 layer_past[0].reshape(batch_size_times_num_heads, head_dim, seq_length),\n                 layer_past[1].reshape(batch_size_times_num_heads, seq_length, head_dim),\n             )\n-            for layer_past in past_key_value\n+            for layer_past in past_key_values\n         )\n \n "
        },
        {
            "sha": "8d8e052b324ac3e76a6f477214695d2bf36d6116",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 22,
            "deletions": 17,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -50,6 +50,7 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.model_parallel_utils import assert_device_map, get_device_map\n from .configuration_mt5 import MT5Config\n \n@@ -339,13 +340,14 @@ def compute_bias(self, query_length, key_length, device=None, cache_position=Non\n         values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, query_length, key_length)\n         return values\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         mask=None,\n         key_value_states=None,\n         position_bias=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         layer_head_mask=None,\n         query_length=None,\n         use_cache=False,\n@@ -366,18 +368,18 @@ def forward(\n         query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n         # Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`\n-        if past_key_value is not None and isinstance(past_key_value, EncoderDecoderCache):\n-            is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None and isinstance(past_key_values, EncoderDecoderCache):\n+            is_updated = past_key_values.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                curr_past_key_value = past_key_value.cross_attention_cache\n+                curr_past_key_value = past_key_values.cross_attention_cache\n             else:\n-                curr_past_key_value = past_key_value.self_attention_cache\n+                curr_past_key_value = past_key_values.self_attention_cache\n         else:\n-            curr_past_key_value = past_key_value\n+            curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -387,15 +389,15 @@ def forward(\n             key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n             value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n         scores = torch.matmul(query_states, key_states.transpose(3, 2))\n@@ -460,13 +462,14 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n         self.layer_norm = MT5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         position_bias=None,\n         layer_head_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         use_cache=False,\n         output_attentions=False,\n         cache_position=None,\n@@ -477,7 +480,7 @@ def forward(\n             mask=attention_mask,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -495,14 +498,15 @@ def __init__(self, config, layer_idx: Optional[int] = None):\n         self.layer_norm = MT5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         key_value_states,\n         attention_mask=None,\n         position_bias=None,\n         layer_head_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         use_cache=False,\n         query_length=None,\n         output_attentions=False,\n@@ -515,7 +519,7 @@ def forward(\n             key_value_states=key_value_states,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             query_length=query_length,\n             output_attentions=output_attentions,\n@@ -540,6 +544,7 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n \n         self.layer.append(MT5LayerFF(config))\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -550,7 +555,7 @@ def forward(\n         encoder_decoder_position_bias=None,\n         layer_head_mask=None,\n         cross_attn_layer_head_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         use_cache=False,\n         output_attentions=False,\n         return_dict=True,\n@@ -561,7 +566,7 @@ def forward(\n             attention_mask=attention_mask,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -586,7 +591,7 @@ def forward(\n                 attention_mask=encoder_attention_mask,\n                 position_bias=encoder_decoder_position_bias,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 query_length=cache_position[-1] + 1,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n@@ -1085,7 +1090,7 @@ def forward(\n                 encoder_decoder_position_bias,  # as a positional argument for gradient checkpointing\n                 layer_head_mask=layer_head_mask,\n                 cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n                 return_dict=return_dict,"
        },
        {
            "sha": "a4beb1ddf98037c008109c5a9b065319ebd1e6a2",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 19,
            "deletions": 18,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -55,6 +55,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..auto.configuration_auto import AutoConfig\n from ..auto.modeling_auto import AutoModel\n from .configuration_musicgen import MusicgenConfig, MusicgenDecoderConfig\n@@ -219,11 +220,12 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n@@ -248,35 +250,35 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n             value_states = self.v_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -335,8 +337,7 @@ def __init__(self, config: MusicgenDecoderConfig, layer_idx=None):\n         self.fc2 = nn.Linear(config.ffn_dim, self.embed_dim, bias=False)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n-    # copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer.forward\n-    # TODO: change to new cache class\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -345,7 +346,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -363,7 +364,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -374,7 +375,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n@@ -394,7 +395,7 @@ def forward(\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -619,7 +620,7 @@ def forward(\n                 encoder_attention_mask=encoder_attention_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -807,7 +808,7 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.decoder(\n             input_ids=input_ids,\n             attention_mask=attention_mask,"
        },
        {
            "sha": "f2c3d6af4b82e36d92aff2674d2c455ee9fdc398",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 18,
            "deletions": 15,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -47,6 +47,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..auto.configuration_auto import AutoConfig\n from ..auto.modeling_auto import AutoModel, AutoModelForTextEncoding\n from .configuration_musicgen_melody import MusicgenMelodyConfig, MusicgenMelodyDecoderConfig\n@@ -227,11 +228,12 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n@@ -256,35 +258,35 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n             value_states = self.v_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -334,12 +336,13 @@ def __init__(self, config: MusicgenMelodyDecoderConfig, layer_idx=None):\n         self.fc2 = nn.Linear(config.ffn_dim, self.embed_dim, bias=False)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -350,7 +353,7 @@ def forward(\n             attention_mask (`torch.FloatTensor`): attention mask of size\n                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n             layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size `(attention_heads,)`.\n-            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -361,7 +364,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n@@ -583,7 +586,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=attention_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -737,7 +740,7 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.decoder(\n             input_ids=input_ids,\n             attention_mask=attention_mask,"
        },
        {
            "sha": "c15975fd6fd4abef33c717bc55b55c450a23a257",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 19,
            "deletions": 16,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -41,6 +41,7 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_mvp import MvpConfig\n \n \n@@ -122,11 +123,12 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         attn_prompt: Optional[torch.Tensor] = None,\n@@ -144,19 +146,19 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states) * self.scaling\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -166,15 +168,15 @@ def forward(\n             key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n             value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         if attn_prompt is not None:\n             key_states = torch.cat([attn_prompt[0].expand(bsz, -1, -1, -1), key_states], dim=2)\n@@ -345,6 +347,7 @@ def __init__(self, config: MvpConfig, layer_idx=None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -355,7 +358,7 @@ def forward(\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n         self_attn_prompt: Optional[torch.Tensor] = None,\n         cross_attn_prompt: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -377,7 +380,7 @@ def forward(\n                 `(2, decoder_attention_heads, pro_len, head_dim)`.\n             cross_attn_prompt (`torch.FloatTensor`): prompt of cross attention of shape\n                 `(2, decoder_attention_heads, pro_len, head_dim)`.\n-            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -387,7 +390,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             attn_prompt=self_attn_prompt,\n@@ -409,7 +412,7 @@ def forward(\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n                 attn_prompt=cross_attn_prompt,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n@@ -920,7 +923,7 @@ def forward(\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n                 self_attn_prompt=(self_attn_prompt[idx] if self.use_prompt else None),\n                 cross_attn_prompt=(cross_attn_prompt[idx] if self.use_prompt else None),\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -1082,7 +1085,7 @@ def forward(\n                 attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n             )\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,"
        },
        {
            "sha": "f8e2afb8897086f9521d0bac056714ab2bf83b9c",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 20,
            "deletions": 17,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -41,6 +41,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_nemotron import NemotronConfig\n \n \n@@ -224,13 +225,14 @@ def __init__(self, config: NemotronConfig, layer_idx: Optional[int] = None):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n         self.o_proj = nn.Linear(self.head_dim * self.num_heads, self.hidden_size, bias=config.attention_bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -249,10 +251,10 @@ def forward(\n             cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n         value_states = repeat_kv(value_states, self.num_key_value_groups)\n@@ -297,19 +299,19 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n-    # Ignore copy\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        if isinstance(past_key_value, StaticCache):\n+        if isinstance(past_key_values, StaticCache):\n             raise ValueError(\n                 \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n                 \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n@@ -334,10 +336,10 @@ def forward(\n             cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n         # to be able to avoid many of these transpose/reshape/view.\n@@ -410,14 +412,14 @@ class NemotronSdpaAttention(NemotronAttention):\n     SDPA API.\n     \"\"\"\n \n-    # Ignore copy\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -433,7 +435,7 @@ def forward(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -454,10 +456,10 @@ def forward(\n             cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n         value_states = repeat_kv(value_states, self.num_key_value_groups)\n@@ -515,12 +517,13 @@ def __init__(self, config: NemotronConfig, layer_idx: int):\n         self.input_layernorm = NemotronLayerNorm1P(config.hidden_size, eps=config.norm_eps)\n         self.post_attention_layernorm = NemotronLayerNorm1P(config.hidden_size, eps=config.norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -539,7 +542,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n@@ -558,7 +561,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -701,7 +704,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "819ebef200ad652b372b8651b6276db00cff69db",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 19,
            "deletions": 16,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -43,6 +43,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_nllb_moe import NllbMoeConfig\n \n \n@@ -541,11 +542,12 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n@@ -570,35 +572,35 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n             value_states = self.v_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -739,6 +741,7 @@ def __init__(self, config: NllbMoeConfig, is_sparse: bool = False, layer_idx: Op\n         self.ff_layer_norm = nn.LayerNorm(config.d_model)\n         self.ff_dropout = nn.Dropout(config.activation_dropout)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -747,7 +750,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         output_router_logits: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n@@ -769,7 +772,7 @@ def forward(\n                 mask for attention heads in a given layer of size `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`):\n                 mask for cross-attention heads in a given layer of size `(decoder_attention_heads,)`.\n-            past_key_value (`Tuple(torch.FloatTensor)`):\n+            past_key_values (`Tuple(torch.FloatTensor)`):\n                 cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n@@ -781,7 +784,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n@@ -799,7 +802,7 @@ def forward(\n             hidden_states, cross_attn_weights = self.cross_attention(\n                 hidden_states=hidden_states,\n                 encoder_hidden_states=encoder_hidden_states,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n                 output_attentions=output_attentions,\n@@ -1298,7 +1301,7 @@ def forward(\n                     encoder_attention_mask=encoder_attention_mask,\n                     layer_head_mask=layer_head_mask,\n                     cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n-                    past_key_value=past_key_values,\n+                    past_key_values=past_key_values,\n                     use_cache=use_cache,\n                     output_attentions=output_attentions,\n                     output_router_logits=output_router_logits,\n@@ -1545,7 +1548,7 @@ def forward(\n                 router_probs=encoder_outputs[3] if len(encoder_outputs) > 3 else None,\n             )\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,"
        },
        {
            "sha": "d32fb0d820017f517e3277a0053e4238ae600ef2",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -20,6 +20,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_olmo import OlmoConfig\n \n@@ -153,12 +154,13 @@ def __init__(self, config: OlmoConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -181,10 +183,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -216,12 +218,13 @@ def __init__(self, config: OlmoConfig, layer_idx: int):\n         self.input_layernorm = OlmoLayerNorm(config.hidden_size)\n         self.post_attention_layernorm = OlmoLayerNorm(config.hidden_size)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -234,7 +237,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -370,7 +373,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n                 **kwargs,"
        },
        {
            "sha": "f54b9106345eccb216c79b89931455fc47f02e5b",
            "filename": "src/transformers/models/olmo/modular_olmo.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -8,6 +8,7 @@\n from ...cache_utils import Cache\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...utils import logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..llama.modeling_llama import (\n     LlamaAttention,\n     LlamaDecoderLayer,\n@@ -75,12 +76,13 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n \n \n class OlmoAttention(LlamaAttention):\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -103,10 +105,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":"
        },
        {
            "sha": "a98943a88e06879af6da5a4b0f344a963f2264e9",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -22,6 +22,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_olmo2 import Olmo2Config\n \n@@ -148,12 +149,13 @@ def __init__(self, config: Olmo2Config, layer_idx: Optional[int] = None):\n         self.q_norm = Olmo2RMSNorm(config.num_attention_heads * self.head_dim, config.rms_norm_eps)\n         self.k_norm = Olmo2RMSNorm(config.num_key_value_heads * self.head_dim, config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -171,10 +173,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -222,12 +224,13 @@ def __init__(self, config: Olmo2Config, layer_idx: int):\n         self.post_attention_layernorm = Olmo2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Olmo2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -238,7 +241,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -375,7 +378,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n                 **kwargs,"
        },
        {
            "sha": "c7e4706976cca705e81bd64ec937440d44143463",
            "filename": "src/transformers/models/olmo2/modular_olmo2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -9,6 +9,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..llama.modeling_llama import LlamaPreTrainedModel, LlamaRMSNorm, eager_attention_forward\n from ..olmo.configuration_olmo import OlmoConfig\n from ..olmo.modeling_olmo import (\n@@ -194,12 +195,13 @@ def __init__(self, config: Olmo2Config, layer_idx: Optional[int] = None):\n         self.q_norm = Olmo2RMSNorm(config.num_attention_heads * self.head_dim, config.rms_norm_eps)\n         self.k_norm = Olmo2RMSNorm(config.num_key_value_heads * self.head_dim, config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -217,10 +219,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -253,12 +255,13 @@ def __init__(self, config: Olmo2Config, layer_idx: int):\n         self.self_attn = Olmo2Attention(config=config, layer_idx=layer_idx)\n         del self.input_layernorm\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -269,7 +272,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,"
        },
        {
            "sha": "6a4b970815f3f0b9e48f1247f6d0384e78bda402",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 19,
            "deletions": 14,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -29,6 +29,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_olmoe import OlmoeConfig\n \n \n@@ -288,12 +289,13 @@ def __init__(self, config: OlmoeConfig, layer_idx: Optional[int] = None):\n             (self.hidden_size // self.num_heads) * self.num_key_value_heads, eps=config.rms_norm_eps\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -318,10 +320,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n         value_states = repeat_kv(value_states, self.num_key_value_groups)\n@@ -370,12 +372,13 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -404,10 +407,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n         # to be able to avoid many of these transpose/reshape/view.\n@@ -476,12 +479,13 @@ class OlmoeSdpaAttention(OlmoeAttention):\n     \"\"\"\n \n     # Adapted from OlmoeAttention.forward\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -497,7 +501,7 @@ def forward(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -522,10 +526,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n         value_states = repeat_kv(value_states, self.num_key_value_groups)\n@@ -629,12 +633,13 @@ def __init__(self, config: OlmoeConfig, layer_idx: int):\n         self.input_layernorm = OlmoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = OlmoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         output_router_logits: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n@@ -657,7 +662,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n@@ -676,7 +681,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -821,7 +826,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 output_router_logits=output_router_logits,\n                 use_cache=use_cache,"
        },
        {
            "sha": "738c556ace7820655e78914e6f05128364fa9edc",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -36,6 +36,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_opt import OPTConfig\n \n \n@@ -138,10 +139,11 @@ def __init__(\n         self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=self.enable_bias)\n         self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=self.enable_bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -164,9 +166,9 @@ def forward(\n         key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # save all key/value_states to cache to be re-used for fast auto-regressive generation\n-            key_states, value_states = past_key_value.update(\n+            key_states, value_states = past_key_values.update(\n                 key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n             )\n \n@@ -219,12 +221,13 @@ def __init__(self, config: OPTConfig, layer_idx: Optional[int] = None):\n         self.fc2 = nn.Linear(config.ffn_dim, self.embed_dim, bias=config.enable_bias)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim, elementwise_affine=config.layer_norm_elementwise_affine)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -244,7 +247,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence..\n         \"\"\"\n@@ -258,7 +261,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             position_ids=position_ids,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n@@ -655,7 +658,7 @@ def forward(\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -726,7 +729,7 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.decoder(\n             input_ids=input_ids,\n             attention_mask=attention_mask,"
        },
        {
            "sha": "174e9308cb92d0f1b926f6827ee371b5e105c618",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 19,
            "deletions": 16,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -49,6 +49,7 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_pegasus import PegasusConfig\n \n \n@@ -185,11 +186,12 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -214,19 +216,19 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -236,15 +238,15 @@ def forward(\n             key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n             value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -366,6 +368,7 @@ def __init__(self, config: PegasusConfig, layer_idx: Optional[int] = None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -374,7 +377,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -392,7 +395,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -406,7 +409,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n@@ -426,7 +429,7 @@ def forward(\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n@@ -1119,7 +1122,7 @@ def forward(\n                 encoder_attention_mask=encoder_attention_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -1293,7 +1296,7 @@ def forward(\n                 attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n             )\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,"
        },
        {
            "sha": "faf71a29f8a517f97d9b237bacae4dca072d0654",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 19,
            "deletions": 16,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -43,6 +43,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, is_torch_flex_attn_available, is_torchdynamo_compiling, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_pegasus_x import PegasusXConfig\n \n \n@@ -206,11 +207,12 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -235,19 +237,19 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -257,15 +259,15 @@ def forward(\n             key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n             value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -672,13 +674,14 @@ def __init__(self, config: PegasusXConfig, layer_idx: Optional[int] = None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -692,7 +695,7 @@ def forward(\n                 cross attention input to the layer of shape *(seq_len, batch, embed_dim)*\n             encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n                 *(batch, 1, tgt_len, src_len)* where padding elements are indicated by very large negative values.\n-            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -707,7 +710,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -725,7 +728,7 @@ def forward(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n@@ -1369,7 +1372,7 @@ def forward(\n                 causal_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -1537,7 +1540,7 @@ def forward(\n                 attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n             )\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,"
        },
        {
            "sha": "45c66707b09a48ada45300e24c57f89b0f3c2168",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -43,6 +43,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_persimmon import PersimmonConfig\n \n \n@@ -223,12 +224,13 @@ def _split_heads(self, fused_qkv: torch.Tensor) -> tuple[torch.Tensor, torch.Ten\n         fused_qkv = fused_qkv.view(batch_size, seq_length, self.num_heads, 3, self.head_dim)\n         return fused_qkv[..., 0, :], fused_qkv[..., 1, :], fused_qkv[..., 2, :]\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -270,15 +272,15 @@ def forward(\n         query_states = torch.cat((query_rot, query_pass), dim=-1)\n         key_states = torch.cat((key_rot, key_pass), dim=-1)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # Specific to RoPE models with partial rotation\n             cache_kwargs = {\n                 \"sin\": sin,\n                 \"cos\": cos,\n                 \"partial_rotation_size\": self.rotary_ndims,\n                 \"cache_position\": cache_position,\n             }\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -314,12 +316,13 @@ def __init__(self, config: PersimmonConfig, layer_idx: int):\n         self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -335,7 +338,7 @@ def forward(\n                 Indices of positions of each input sequence tokens in the position embeddings. Selected in the range\n                 `[0, config.n_positions - 1]`.\n                 [What are position IDs?](../glossary#position-ids)\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*):\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*):\n                 cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n@@ -359,7 +362,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -508,7 +511,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "dde76814026e0eff99bd99f985d9495fedb0c2c6",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -23,6 +23,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_phi import PhiConfig\n \n@@ -128,12 +129,13 @@ def __init__(self, config: PhiConfig, layer_idx: int):\n                 config.hidden_size // config.num_attention_heads, eps=config.layer_norm_eps, elementwise_affine=True\n             )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -165,10 +167,10 @@ def forward(\n         query_states = torch.cat((query_rot, query_pass), dim=-1)\n         key_states = torch.cat((key_rot, key_pass), dim=-1)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -213,12 +215,13 @@ def __init__(self, config: PhiConfig, layer_idx: int):\n         self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.resid_dropout = nn.Dropout(config.resid_pdrop)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -234,7 +237,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -399,7 +402,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "56eb541cb092609ea7ba864d13b22f9ddb0ede17",
            "filename": "src/transformers/models/phi/modular_phi.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -12,6 +12,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..clip.modeling_clip import CLIPMLP\n from ..llama.modeling_llama import (\n     LlamaAttention,\n@@ -50,12 +51,13 @@ def __init__(self, config: PhiConfig, layer_idx: int):\n                 config.hidden_size // config.num_attention_heads, eps=config.layer_norm_eps, elementwise_affine=True\n             )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -87,10 +89,10 @@ def forward(\n         query_states = torch.cat((query_rot, query_pass), dim=-1)\n         key_states = torch.cat((key_rot, key_pass), dim=-1)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -124,12 +126,13 @@ def __init__(self, config: PhiConfig, layer_idx: int):\n         self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.resid_dropout = nn.Dropout(config.resid_pdrop)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -145,7 +148,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -248,7 +251,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "71aef43c2c0ea51b714e1b6a3d4ab2868e04f263",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -43,6 +43,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_phi3 import Phi3Config\n \n \n@@ -159,12 +160,13 @@ def __init__(self, config: Phi3Config, layer_idx: Optional[int] = None):\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n         self.qkv_proj = nn.Linear(config.hidden_size, op_size, bias=False)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -184,10 +186,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -243,12 +245,13 @@ def __init__(self, config: Phi3Config, layer_idx: int):\n         self.resid_attn_dropout = nn.Dropout(config.resid_pdrop)\n         self.resid_mlp_dropout = nn.Dropout(config.resid_pdrop)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -261,7 +264,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -399,7 +402,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,"
        },
        {
            "sha": "03cd5ad4ae7cd771c095f0f39a04ed1196d75802",
            "filename": "src/transformers/models/phi3/modular_phi3.py",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -27,6 +27,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..mistral.modeling_mistral import (\n     MistralDecoderLayer,\n     MistralForCausalLM,\n@@ -113,12 +114,13 @@ def __init__(self, config: Phi3Config, layer_idx: Optional[int] = None):\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n         self.qkv_proj = nn.Linear(config.hidden_size, op_size, bias=False)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -138,10 +140,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -173,12 +175,13 @@ def __init__(self, config: Phi3Config, layer_idx: int):\n         self.resid_attn_dropout = nn.Dropout(config.resid_pdrop)\n         self.resid_mlp_dropout = nn.Dropout(config.resid_pdrop)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -191,7 +194,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,"
        },
        {
            "sha": "449e1d8a3d058a9be4c0fec6ab5b61221661d7cc",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -46,6 +46,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, torch_int\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import TransformersKwargs, check_model_inputs\n from .configuration_phi4_multimodal import Phi4MultimodalAudioConfig, Phi4MultimodalConfig, Phi4MultimodalVisionConfig\n \n@@ -1383,12 +1384,13 @@ def __init__(self, config: Phi4MultimodalConfig, layer_idx: Optional[int] = None\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n         self.qkv_proj = nn.Linear(config.hidden_size, op_size, bias=False)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -1408,10 +1410,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -1446,12 +1448,13 @@ def __init__(self, config: Phi4MultimodalConfig, layer_idx: int):\n         self.resid_attn_dropout = nn.Dropout(config.resid_pdrop)\n         self.resid_mlp_dropout = nn.Dropout(config.resid_pdrop)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -1464,7 +1467,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -1703,7 +1706,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,"
        },
        {
            "sha": "e77c7a0fa31e7036dbeef0db06e21daa202d5717",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -1556,7 +1556,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,"
        },
        {
            "sha": "394d9c42821bc1a72a8898f04afcf24e2ca91382",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 19,
            "deletions": 14,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -35,6 +35,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_phimoe import PhimoeConfig\n \n \n@@ -268,12 +269,13 @@ def __init__(self, config: PhimoeConfig, layer_idx: Optional[int] = None):\n     def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -292,9 +294,9 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         # repeat k/v heads if n_kv_heads < n_heads\n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n@@ -336,12 +338,13 @@ class PhimoeFlashAttention2(PhimoeAttention):\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -360,9 +363,9 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         # repeat k/v heads if n_kv_heads < n_heads\n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n@@ -431,12 +434,13 @@ class PhimoeSdpaAttention(PhimoeAttention):\n     \"\"\"\n \n     # Adapted from PhimoeAttention.forward\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -452,7 +456,7 @@ def forward(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 position_embeddings=position_embeddings,\n@@ -471,9 +475,9 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n         value_states = repeat_kv(value_states, self.num_key_value_groups)\n@@ -812,12 +816,13 @@ def __init__(self, config: PhimoeConfig, layer_idx: int):\n             config.hidden_size, eps=config.rms_norm_eps, elementwise_affine=True\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         output_attentions: Optional[bool] = False,\n         output_router_logits: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n@@ -830,7 +835,7 @@ def forward(\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n             attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n                 `(batch, sequence_length)` where padding elements are indicated by 0.\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -856,7 +861,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -1008,7 +1013,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 output_router_logits=output_router_logits,\n                 use_cache=use_cache,"
        },
        {
            "sha": "07623a02859520698728678d5cdadd940b9d8064",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 23,
            "deletions": 18,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -43,6 +43,7 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_pix2struct import Pix2StructConfig, Pix2StructTextConfig, Pix2StructVisionConfig\n \n \n@@ -163,7 +164,7 @@ def forward(\n         \"\"\"\n         # Input is (batch_size, seq_length, dim)\n         # Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)\n-        # past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)\n+        # past_key_values[0] is (batch_size, n_heads, q_len - 1, dim_per_head)\n         batch_size, seq_length = hidden_states.shape[:2]\n \n         def to_projection_shape(states):\n@@ -733,13 +734,14 @@ def compute_bias(self, query_length, key_length, device=None, cache_position=Non\n         return values\n \n     # Adapted from transformers.models.t5.modeling_t5.T5Attention.forward\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         mask=None,\n         key_value_states=None,\n         position_bias=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         layer_head_mask=None,\n         query_length=None,\n         use_cache=False,\n@@ -760,18 +762,18 @@ def forward(\n         query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n         # Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`\n-        if past_key_value is not None and isinstance(past_key_value, EncoderDecoderCache):\n-            is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None and isinstance(past_key_values, EncoderDecoderCache):\n+            is_updated = past_key_values.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                curr_past_key_value = past_key_value.cross_attention_cache\n+                curr_past_key_value = past_key_values.cross_attention_cache\n             else:\n-                curr_past_key_value = past_key_value.self_attention_cache\n+                curr_past_key_value = past_key_values.self_attention_cache\n         else:\n-            curr_past_key_value = past_key_value\n+            curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value and is_updated:\n+        if is_cross_attention and past_key_values and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -781,15 +783,15 @@ def forward(\n             key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n             value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n         scores = torch.matmul(query_states, key_states.transpose(3, 2))\n@@ -854,13 +856,14 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n         self.layer_norm = Pix2StructLayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         position_bias=None,\n         layer_head_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         use_cache=False,\n         output_attentions=False,\n         cache_position=None,\n@@ -871,7 +874,7 @@ def forward(\n             mask=attention_mask,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -889,14 +892,15 @@ def __init__(self, config, layer_idx: Optional[int] = None):\n         self.layer_norm = Pix2StructLayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         key_value_states,\n         attention_mask=None,\n         position_bias=None,\n         layer_head_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         use_cache=False,\n         query_length=None,\n         output_attentions=False,\n@@ -909,7 +913,7 @@ def forward(\n             key_value_states=key_value_states,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             query_length=query_length,\n             output_attentions=output_attentions,\n@@ -937,6 +941,7 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n \n         self.mlp = Pix2StructTextLayerFF(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -947,7 +952,7 @@ def forward(\n         encoder_decoder_position_bias=None,\n         layer_head_mask=None,\n         cross_attn_layer_head_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         use_cache=False,\n         output_attentions=False,\n         return_dict=True,\n@@ -958,7 +963,7 @@ def forward(\n             attention_mask=attention_mask,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -979,7 +984,7 @@ def forward(\n                 attention_mask=encoder_attention_mask,\n                 position_bias=encoder_decoder_position_bias,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 query_length=cache_position[-1] + 1,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n@@ -1196,7 +1201,7 @@ def forward(\n                 encoder_decoder_position_bias,  # as a positional argument for gradient checkpointing\n                 layer_head_mask=layer_head_mask,\n                 cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,"
        },
        {
            "sha": "de92fb89aa4c740bc1f71d9a77df55bfba0b19ba",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 19,
            "deletions": 16,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -47,6 +47,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, is_torch_flex_attn_available, is_torchdynamo_compiling, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_plbart import PLBartConfig\n \n \n@@ -370,11 +371,12 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -399,19 +401,19 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -421,15 +423,15 @@ def forward(\n             key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n             value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -720,6 +722,7 @@ def __init__(self, config: PLBartConfig, layer_idx: Optional[int] = None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -728,7 +731,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -746,7 +749,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -759,7 +762,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n@@ -779,7 +782,7 @@ def forward(\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -1035,7 +1038,7 @@ def forward(\n                 encoder_attention_mask=encoder_attention_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -1203,7 +1206,7 @@ def forward(\n                 attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n             )\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,"
        },
        {
            "sha": "2737039eefa34b5ec170291a453591a40d3319c1",
            "filename": "src/transformers/models/plbart/modular_plbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -381,7 +381,7 @@ def forward(\n                 attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n             )\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,"
        },
        {
            "sha": "ed5a182ca1be27e74172f4a04ab6b192a0a53152",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 22,
            "deletions": 17,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -33,6 +33,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, is_torch_flex_attn_available, is_torch_fx_proxy, is_torchdynamo_compiling, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_pop2piano import Pop2PianoConfig\n \n \n@@ -283,13 +284,14 @@ def compute_bias(self, query_length, key_length, device=None, cache_position=Non\n         values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, query_length, key_length)\n         return values\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         mask=None,\n         key_value_states=None,\n         position_bias=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         layer_head_mask=None,\n         query_length=None,\n         use_cache=False,\n@@ -310,18 +312,18 @@ def forward(\n         query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n         # Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`\n-        if past_key_value is not None and isinstance(past_key_value, EncoderDecoderCache):\n-            is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None and isinstance(past_key_values, EncoderDecoderCache):\n+            is_updated = past_key_values.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                curr_past_key_value = past_key_value.cross_attention_cache\n+                curr_past_key_value = past_key_values.cross_attention_cache\n             else:\n-                curr_past_key_value = past_key_value.self_attention_cache\n+                curr_past_key_value = past_key_values.self_attention_cache\n         else:\n-            curr_past_key_value = past_key_value\n+            curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -331,15 +333,15 @@ def forward(\n             key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n             value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n         scores = torch.matmul(query_states, key_states.transpose(3, 2))\n@@ -404,13 +406,14 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n         self.layer_norm = Pop2PianoLayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         position_bias=None,\n         layer_head_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         use_cache=False,\n         output_attentions=False,\n         cache_position=None,\n@@ -421,7 +424,7 @@ def forward(\n             mask=attention_mask,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -439,14 +442,15 @@ def __init__(self, config, layer_idx: Optional[int] = None):\n         self.layer_norm = Pop2PianoLayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         key_value_states,\n         attention_mask=None,\n         position_bias=None,\n         layer_head_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         use_cache=False,\n         query_length=None,\n         output_attentions=False,\n@@ -459,7 +463,7 @@ def forward(\n             key_value_states=key_value_states,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             query_length=query_length,\n             output_attentions=output_attentions,\n@@ -486,6 +490,7 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n \n         self.layer.append(Pop2PianoLayerFF(config))\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -496,7 +501,7 @@ def forward(\n         encoder_decoder_position_bias=None,\n         layer_head_mask=None,\n         cross_attn_layer_head_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         use_cache=False,\n         output_attentions=False,\n         return_dict=True,\n@@ -507,7 +512,7 @@ def forward(\n             attention_mask=attention_mask,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -532,7 +537,7 @@ def forward(\n                 attention_mask=encoder_attention_mask,\n                 position_bias=encoder_decoder_position_bias,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 query_length=cache_position[-1] + 1,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n@@ -811,7 +816,7 @@ def forward(\n                 encoder_decoder_position_bias,  # as a positional argument for gradient checkpointing\n                 layer_head_mask=layer_head_mask,\n                 cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,"
        },
        {
            "sha": "46fe08536e027f78b0304530ba1621e9d8fe0aa8",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 23,
            "deletions": 19,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -32,6 +32,7 @@\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_prophetnet import ProphetNetConfig\n \n \n@@ -437,13 +438,14 @@ def __init__(self, config: ProphetNetConfig, num_attn_heads: int, layer_idx: Opt\n \n         self.out_proj = nn.Linear(hidden_size, hidden_size)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         key_value_states: Optional[Tensor] = None,\n         attention_mask: Optional[Tensor] = None,\n         layer_head_mask: Optional[Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[Tensor, Optional[Tensor]]:\n@@ -461,19 +463,19 @@ def forward(\n         # previous time steps are cached - no need to recompute key and value if they are static\n         query_states = self.query_proj(hidden_states) / (self.head_dim**0.5)\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -483,15 +485,15 @@ def forward(\n             key_states = key_states.view(batch_size, -1, self.num_attn_heads, self.head_dim).transpose(1, 2)\n             value_states = value_states.view(batch_size, -1, self.num_attn_heads, self.head_dim).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         query_states = query_states.view(batch_size, tgt_len, self.num_attn_heads, self.head_dim).transpose(1, 2)\n         src_len = key_states.size(2)\n@@ -606,10 +608,11 @@ def _shape(self, tensor, seq_len, batch_size):\n     def prepare_for_onnx_export_(self):\n         self.onnx_trace = True\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n-        past_key_value: Optional[tuple[Tensor]] = None,\n+        past_key_values: Optional[tuple[Tensor]] = None,\n         attention_mask=None,\n         layer_head_mask=None,\n         extended_predict_attention_mask=None,\n@@ -655,11 +658,11 @@ def forward(\n \n         # ProphetNet has two separate attention layers, one for self and one for cross attention\n         # We need to obtain the self attention only for this module, if `EncoderDecoderCache`\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                curr_past_key_value = past_key_value.self_attention_cache\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n             main_key_states, main_value_states = curr_past_key_value.update(\n                 main_key_states, main_value_states, self.layer_idx, {\"cache_position\": cache_position}\n             )\n@@ -954,6 +957,7 @@ def __init__(self, config: ProphetNetConfig, layer_idx=None):\n         self.feed_forward = ProphetNetFeedForward(config, config.decoder_ffn_dim)\n         self.feed_forward_layer_norm = LayerNorm(config.hidden_size)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -966,15 +970,15 @@ def forward(\n         main_relative_position_buckets=None,\n         predict_relative_position_buckets=None,\n         position_ids=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         use_cache: Optional[bool] = True,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ):\n         # 1st residual block\n         ngram_attention_output, self_attn_weights, self_attn_weights_ngram = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             extended_predict_attention_mask=extended_predict_attention_mask,\n@@ -992,7 +996,7 @@ def forward(\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attn_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = self.cross_attn_layer_norm(attention_output + hidden_states)\n@@ -1334,7 +1338,7 @@ def forward(\n                 main_relative_position_buckets=main_relative_position_buckets,\n                 predict_relative_position_buckets=predict_relative_position_buckets,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,"
        },
        {
            "sha": "5d39c58a24991f0e1fb37755086f098386e62e46",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -26,6 +26,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_qwen2 import Qwen2Config\n \n@@ -136,12 +137,13 @@ def __init__(self, config: Qwen2Config, layer_idx: int):\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -155,10 +157,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -214,12 +216,13 @@ def __init__(self, config: Qwen2Config, layer_idx: int):\n         self.post_attention_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -232,7 +235,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -382,7 +385,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,"
        },
        {
            "sha": "030cf82c2c7a20798e8925e75870d4a33f2b40bb",
            "filename": "src/transformers/models/qwen2/modular_qwen2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -13,6 +13,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from ..llama.modeling_llama import (\n     LlamaAttention,\n@@ -50,12 +51,13 @@ def __init__(self, config: Qwen2Config, layer_idx: int):\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -69,10 +71,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -170,7 +172,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,"
        },
        {
            "sha": "774b0cdabcb5ad93ad2ed9293b88fc35b13376ce",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -41,6 +41,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, check_torch_load_is_safe, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.hub import cached_file\n from .configuration_qwen2_5_omni import (\n     Qwen2_5OmniAudioEncoderConfig,\n@@ -1363,12 +1364,13 @@ def __init__(self, config: Qwen2_5OmniConfig, layer_idx: Optional[int] = None):\n \n         self.rotary_emb = Qwen2_5OmniRotaryEmbedding(config=config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -1390,9 +1392,9 @@ def forward(\n             query_states, key_states, cos, sin, self.rope_scaling[\"mrope_section\"]\n         )\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -1447,12 +1449,13 @@ def __init__(self, config: Qwen2_5OmniTextConfig, layer_idx: int):\n         self.post_attention_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -1470,7 +1473,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n@@ -1490,7 +1493,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -1640,7 +1643,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_ids=text_position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -2220,7 +2223,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_ids=text_position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "50e9a5c7fad9556df00b1f3ae343562fa6f94901",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -42,6 +42,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_qwen2_5_vl import Qwen2_5_VLConfig, Qwen2_5_VLTextConfig, Qwen2_5_VLVisionConfig\n \n \n@@ -657,12 +658,13 @@ def __init__(self, config: Qwen2_5_VLTextConfig, layer_idx: Optional[int] = None\n \n         self.rotary_emb = Qwen2_5_VLRotaryEmbedding(config=config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -684,9 +686,9 @@ def forward(\n             query_states, key_states, cos, sin, self.rope_scaling[\"mrope_section\"]\n         )\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -727,12 +729,13 @@ def __init__(self, config: Qwen2_5_VLTextConfig, layer_idx: int):\n         self.post_attention_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -750,7 +753,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n@@ -770,7 +773,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -919,7 +922,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_ids=text_position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "0ad984798d68c5903abc759b8fb28b96eeeb7789",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 19,
            "deletions": 15,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -45,6 +45,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_qwen2_moe import Qwen2MoeConfig\n \n \n@@ -309,13 +310,13 @@ def __init__(self, config: Qwen2MoeConfig, layer_idx: Optional[int] = None):\n \n         self.rotary_emb = Qwen2MoeRotaryEmbedding(config=self.config)\n \n-    # Ignore copy\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -334,9 +335,9 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         # repeat k/v heads if n_kv_heads < n_heads\n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n@@ -389,12 +390,13 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -413,9 +415,9 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         # repeat k/v heads if n_kv_heads < n_heads\n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n@@ -496,12 +498,13 @@ class Qwen2MoeSdpaAttention(Qwen2MoeAttention):\n     \"\"\"\n \n     # Adapted from Qwen2MoeAttention.forward\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -517,7 +520,7 @@ def forward(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -537,9 +540,9 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n         value_states = repeat_kv(value_states, self.num_key_value_groups)\n@@ -664,12 +667,13 @@ def __init__(self, config: Qwen2MoeConfig, layer_idx: int):\n         self.input_layernorm = Qwen2MoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = Qwen2MoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         output_attentions: Optional[bool] = False,\n         output_router_logits: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n@@ -691,7 +695,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n@@ -711,7 +715,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -859,7 +863,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 output_router_logits=output_router_logits,\n                 use_cache=use_cache,"
        },
        {
            "sha": "5e422451ae3350bd2b2d453e47709c618229bf5e",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -45,6 +45,7 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_qwen2_vl import Qwen2VLConfig, Qwen2VLTextConfig, Qwen2VLVisionConfig\n \n \n@@ -518,12 +519,13 @@ def __init__(self, config: Qwen2VLTextConfig, layer_idx: Optional[int] = None):\n \n         self.rotary_emb = Qwen2VLRotaryEmbedding(config=config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -545,9 +547,9 @@ def forward(\n             query_states, key_states, cos, sin, self.rope_scaling[\"mrope_section\"]\n         )\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -588,12 +590,13 @@ def __init__(self, config: Qwen2VLTextConfig, layer_idx: int):\n         self.post_attention_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -611,7 +614,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n@@ -631,7 +634,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -891,7 +894,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_ids=text_position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "785fca6d15b85bc86b4b01d1864f426bb9370c13",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -41,6 +41,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_qwen3 import Qwen3Config\n \n@@ -183,12 +184,13 @@ def __init__(self, config: Qwen3Config, layer_idx: int):\n         self.k_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape\n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -202,10 +204,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -240,12 +242,13 @@ def __init__(self, config: Qwen3Config, layer_idx: int):\n         self.post_attention_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -258,7 +261,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -408,7 +411,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,"
        },
        {
            "sha": "f1e38841faf496d3486c5b068d219e358142f3a4",
            "filename": "src/transformers/models/qwen3/modular_qwen3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodular_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodular_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodular_qwen3.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -24,6 +24,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..gemma.modeling_gemma import GemmaMLP\n from ..llama.modeling_llama import (\n     LlamaAttention,\n@@ -63,12 +64,13 @@ def __init__(self, config: Qwen3Config, layer_idx: int):\n         self.k_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape\n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -82,10 +84,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":"
        },
        {
            "sha": "340281b7d99ec4fdb5c496a7777db47bc6d6659d",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -42,6 +42,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_qwen3_moe import Qwen3MoeConfig\n \n@@ -147,12 +148,13 @@ def __init__(self, config: Qwen3MoeConfig, layer_idx: int):\n         self.k_norm = Qwen3MoeRMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape\n         self.sliding_window = getattr(config, \"sliding_window\", None)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -166,10 +168,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -300,13 +302,14 @@ def __init__(self, config: Qwen3MoeConfig, layer_idx: int):\n         self.input_layernorm = Qwen3MoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = Qwen3MoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> torch.FloatTensor:\n@@ -324,7 +327,7 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n@@ -344,7 +347,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -486,7 +489,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,"
        },
        {
            "sha": "64f9c45725f6d30932af816294699e85ca737254",
            "filename": "src/transformers/models/qwen3_moe/modular_qwen3_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -27,6 +27,7 @@\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..llama.modeling_llama import (\n     LlamaForQuestionAnswering,\n     LlamaForSequenceClassification,\n@@ -139,13 +140,14 @@ def __init__(self, config: Qwen3MoeConfig, layer_idx: int):\n         self.input_layernorm = Qwen3MoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = Qwen3MoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> torch.FloatTensor:\n@@ -159,7 +161,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )"
        },
        {
            "sha": "15538377287124a10ba9a19055d3f84bf5897144",
            "filename": "src/transformers/models/rag/modeling_tf_rag.py",
            "status": "modified",
            "additions": 25,
            "deletions": 21,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_tf_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_tf_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_tf_rag.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -567,27 +567,31 @@ def call(\n         **kwargs,\n     ) -> TFRetrievAugLMOutput:\n         r\"\"\"\n-        Returns:\n-\n-        Example:\n-\n-        ```python\n-        >>> from transformers import AutoTokenizer, RagRetriever, TFRagModel\n-        >>> import torch\n-\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/rag-token-base\")\n-        >>> retriever = RagRetriever.from_pretrained(\n-        ...     \"facebook/rag-token-base\", index_name=\"exact\", use_dummy_dataset=True\n-        ... )\n-        >>> # initialize with RagRetriever to do everything in one forward call\n-        >>> model = TFRagModel.from_pretrained(\"facebook/rag-token-base\", retriever=retriever, from_pt=True)\n-\n-        >>> input_dict = tokenizer.prepare_seq2seq_batch(\n-        ...     \"How many people live in Paris?\", \"In Paris, there are 10 million people.\", return_tensors=\"tf\"\n-        ... )\n-        >>> input_ids = input_dict[\"input_ids\"]\n-        >>> outputs = model(input_ids)\n-        ```\"\"\"\n+                        Returns:\n+\n+                        Example:\n+\n+                        ```python\n+                        >>> from transformers import AutoTokenizer, RagRetriever, TFRagModel\n+                        >>> import torch\n+        from ...utils.deprecation import deprecate_kwarg\n+        from ...utils.deprecation import deprecate_kwarg\n+        from ...utils.deprecation import deprecate_kwarg\n+                from ...utils.deprecation import deprecate_kwarg\n+\n+                        >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/rag-token-base\")\n+                        >>> retriever = RagRetriever.from_pretrained(\n+                        ...     \"facebook/rag-token-base\", index_name=\"exact\", use_dummy_dataset=True\n+                        ... )\n+                        >>> # initialize with RagRetriever to do everything in one forward call\n+                        >>> model = TFRagModel.from_pretrained(\"facebook/rag-token-base\", retriever=retriever, from_pt=True)\n+\n+                        >>> input_dict = tokenizer.prepare_seq2seq_batch(\n+                        ...     \"How many people live in Paris?\", \"In Paris, there are 10 million people.\", return_tensors=\"tf\"\n+                        ... )\n+                        >>> input_ids = input_dict[\"input_ids\"]\n+                        >>> outputs = model(input_ids)\n+                        ```\"\"\"\n         assert \"decoder_cached_states\" not in kwargs, (\n             \"Please use past_key_values to cache intermediate outputs\"\n         )  # from modeling_tf_bart.py"
        },
        {
            "sha": "65badc59b7a9102af587b4d33acfbdeeb76837a2",
            "filename": "src/transformers/models/reformer/modeling_reformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -79,7 +79,7 @@ def __init__(self, _distributed_cache_data: Optional[Iterable] = None) -> None:\n \n     def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n-        Support for backwards-compatible `past_key_value` indexing, e.g. `past_key_value[0][0].shape[2]` to get the\n+        Support for backwards-compatible `past_key_values` indexing, e.g. `past_key_values[0][0].shape[2]` to get the\n         sequence length.\n         \"\"\"\n         if layer_idx < len(self):\n@@ -89,15 +89,15 @@ def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n \n     def __iter__(self):\n         \"\"\"\n-        Support for backwards-compatible `past_key_value` iteration, e.g. `for x in past_key_value:` to iterate over\n+        Support for backwards-compatible `past_key_values` iteration, e.g. `for x in past_key_values:` to iterate over\n         keys and values\n         \"\"\"\n         for layer_idx in range(len(self)):\n             yield (self.buckets_cache[layer_idx], self.states_cache[layer_idx])\n \n     def __len__(self):\n         \"\"\"\n-        Support for backwards-compatible `past_key_value` length, e.g. `len(past_key_value)`. This value corresponds\n+        Support for backwards-compatible `past_key_values` length, e.g. `len(past_key_values)`. This value corresponds\n         to the number of layers in the model.\n         \"\"\"\n         return len(self.states_cache)"
        },
        {
            "sha": "fc3f1862e67d54b5f1e964b9767708883f718094",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 17,
            "deletions": 15,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -40,6 +40,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_rembert import RemBertConfig\n \n \n@@ -221,13 +222,14 @@ def __init__(self, config, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple:\n@@ -239,19 +241,19 @@ def forward(\n         )\n \n         is_cross_attention = encoder_hidden_states is not None\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_layer = curr_past_key_value.layers[self.layer_idx].keys\n             value_layer = curr_past_key_value.layers[self.layer_idx].values\n@@ -267,15 +269,15 @@ def forward(\n                 .transpose(1, 2)\n             )\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_layer, value_layer = curr_past_key_value.update(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n@@ -353,7 +355,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -362,7 +364,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -425,7 +427,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -434,7 +436,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n@@ -452,7 +454,7 @@ def forward(\n                 attention_mask=encoder_attention_mask,\n                 head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )"
        },
        {
            "sha": "e1770bb4db3f7db766762c8388275bdde1a4fef8",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 33,
            "deletions": 28,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -42,6 +42,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, get_torch_version, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_roberta import RobertaConfig\n \n \n@@ -166,13 +167,14 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -183,19 +185,19 @@ def forward(\n         )\n \n         is_cross_attention = encoder_hidden_states is not None\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_layer = curr_past_key_value.layers[self.layer_idx].keys\n             value_layer = curr_past_key_value.layers[self.layer_idx].values\n@@ -209,22 +211,22 @@ def forward(\n                 batch_size, -1, self.num_attention_heads, self.attention_head_size\n             ).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_layer, value_layer = curr_past_key_value.update(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -277,13 +279,14 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n \n     # Adapted from RobertaSelfAttention\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -301,7 +304,7 @@ def forward(\n                 attention_mask,\n                 head_mask,\n                 encoder_hidden_states,\n-                past_key_value,\n+                past_key_values,\n                 output_attentions,\n                 cache_position,\n             )\n@@ -314,19 +317,19 @@ def forward(\n \n         is_cross_attention = encoder_hidden_states is not None\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_layer = curr_past_key_value.layers[self.layer_idx].keys\n             value_layer = curr_past_key_value.layers[self.layer_idx].values\n@@ -342,15 +345,15 @@ def forward(\n                 .transpose(1, 2)\n             )\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_layer, value_layer = curr_past_key_value.update(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n         # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n@@ -432,13 +435,14 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -447,7 +451,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -503,14 +507,15 @@ def __init__(self, config, layer_idx=None):\n         self.intermediate = RobertaIntermediate(config)\n         self.output = RobertaOutput(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -519,7 +524,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n@@ -537,7 +542,7 @@ def forward(\n                 attention_mask=encoder_attention_mask,\n                 head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -612,7 +617,7 @@ def forward(\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )"
        },
        {
            "sha": "30cc18801d40fa416f98cda193863edd54280a22",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 21,
            "deletions": 17,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -40,6 +40,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_roberta_prelayernorm import RobertaPreLayerNormConfig\n \n \n@@ -165,13 +166,14 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -182,19 +184,19 @@ def forward(\n         )\n \n         is_cross_attention = encoder_hidden_states is not None\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_layer = curr_past_key_value.layers[self.layer_idx].keys\n             value_layer = curr_past_key_value.layers[self.layer_idx].values\n@@ -208,22 +210,22 @@ def forward(\n                 batch_size, -1, self.num_attention_heads, self.attention_head_size\n             ).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_layer, value_layer = curr_past_key_value.update(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -310,13 +312,14 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -326,7 +329,7 @@ def forward(\n             attention_mask,\n             head_mask,\n             encoder_hidden_states,\n-            past_key_value,\n+            past_key_values,\n             output_attentions,\n             cache_position,\n         )\n@@ -383,14 +386,15 @@ def __init__(self, config, layer_idx=None):\n         self.intermediate = RobertaPreLayerNormIntermediate(config)\n         self.output = RobertaPreLayerNormOutput(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -399,7 +403,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n@@ -417,7 +421,7 @@ def forward(\n                 attention_mask=encoder_attention_mask,\n                 head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -494,7 +498,7 @@ def forward(\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )"
        },
        {
            "sha": "69f9787d22327d60ed21c776125f69e2ab108e35",
            "filename": "src/transformers/models/roc_bert/modeling_roc_bert.py",
            "status": "modified",
            "additions": 21,
            "deletions": 17,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -40,6 +40,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_roc_bert import RoCBertConfig\n \n \n@@ -280,13 +281,14 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -297,19 +299,19 @@ def forward(\n         )\n \n         is_cross_attention = encoder_hidden_states is not None\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_layer = curr_past_key_value.layers[self.layer_idx].keys\n             value_layer = curr_past_key_value.layers[self.layer_idx].values\n@@ -323,22 +325,22 @@ def forward(\n                 batch_size, -1, self.num_attention_heads, self.attention_head_size\n             ).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_layer, value_layer = curr_past_key_value.update(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -433,13 +435,14 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -448,7 +451,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -504,14 +507,15 @@ def __init__(self, config, layer_idx=None):\n         self.intermediate = RoCBertIntermediate(config)\n         self.output = RoCBertOutput(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -520,7 +524,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n@@ -538,7 +542,7 @@ def forward(\n                 attention_mask=encoder_attention_mask,\n                 head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -613,7 +617,7 @@ def forward(\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )"
        },
        {
            "sha": "2a4ddc69417dfa011ed59da875b1722ebab912fb",
            "filename": "src/transformers/models/roformer/modeling_roformer.py",
            "status": "modified",
            "additions": 19,
            "deletions": 16,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -40,6 +40,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_roformer import RoFormerConfig\n \n \n@@ -211,14 +212,15 @@ def __init__(self, config, layer_idx=None):\n         self.rotary_value = config.rotary_value\n         self.layer_idx = layer_idx\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         sinusoidal_pos=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         output_attentions=False,\n         cache_position=None,\n     ):\n@@ -233,19 +235,19 @@ def forward(\n         # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_layer = curr_past_key_value.layers[self.layer_idx].keys\n             value_layer = curr_past_key_value.layers[self.layer_idx].values\n@@ -272,15 +274,15 @@ def forward(\n                         sinusoidal_pos, query_layer, key_layer\n                     )\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_layer, value_layer = curr_past_key_value.update(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n@@ -378,15 +380,15 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    # End Copy\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         sinusoidal_pos=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         output_attentions=False,\n         cache_position=None,\n     ):\n@@ -396,7 +398,7 @@ def forward(\n             sinusoidal_pos=sinusoidal_pos,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -451,6 +453,7 @@ def __init__(self, config, layer_idx=None):\n         self.intermediate = RoFormerIntermediate(config)\n         self.output = RoFormerOutput(config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -459,7 +462,7 @@ def forward(\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         output_attentions=False,\n         cache_position=None,\n     ):\n@@ -469,7 +472,7 @@ def forward(\n             sinusoidal_pos=sinusoidal_pos,\n             head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n@@ -488,7 +491,7 @@ def forward(\n                 sinusoidal_pos=sinusoidal_pos,\n                 head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )"
        },
        {
            "sha": "9098ec776b2a780365aed264c12761b5c39e98fe",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr_resnet.py",
            "status": "modified",
            "additions": 18,
            "deletions": 13,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr_resnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr_resnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr_resnet.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -339,24 +339,29 @@ def forward(\n         self, pixel_values: Tensor, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None\n     ) -> BackboneOutput:\n         r\"\"\"\n-        Examples:\n+                        Examples:\n \n-        ```python\n-        >>> from transformers import RTDetrResNetConfig, RTDetrResNetBackbone\n-        >>> import torch\n+                        ```python\n+                        >>> from transformers import RTDetrResNetConfig, RTDetrResNetBackbone\n+                        >>> import torch\n+        from ...utils.deprecation import deprecate_kwarg\n+        from ...utils.deprecation import deprecate_kwarg\n+        from ...utils.deprecation import deprecate_kwarg\n+                from ...utils.deprecation import deprecate_kwarg\n+                from ...utils.deprecation import deprecate_kwarg\n \n-        >>> config = RTDetrResNetConfig()\n-        >>> model = RTDetrResNetBackbone(config)\n+                        >>> config = RTDetrResNetConfig()\n+                        >>> model = RTDetrResNetBackbone(config)\n \n-        >>> pixel_values = torch.randn(1, 3, 224, 224)\n+                        >>> pixel_values = torch.randn(1, 3, 224, 224)\n \n-        >>> with torch.no_grad():\n-        ...     outputs = model(pixel_values)\n+                        >>> with torch.no_grad():\n+                        ...     outputs = model(pixel_values)\n \n-        >>> feature_maps = outputs.feature_maps\n-        >>> list(feature_maps[-1].shape)\n-        [1, 2048, 7, 7]\n-        ```\"\"\"\n+                        >>> feature_maps = outputs.feature_maps\n+                        >>> list(feature_maps[-1].shape)\n+                        [1, 2048, 7, 7]\n+                        ```\"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states"
        },
        {
            "sha": "2a89449894af14c10188512efb3fa1bcc06a842b",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 24,
            "deletions": 21,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -43,6 +43,7 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_seamless_m4t import SeamlessM4TConfig\n \n \n@@ -1028,11 +1029,12 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -1048,19 +1050,19 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states) * self.scaling\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -1070,15 +1072,15 @@ def forward(\n             key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n             value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n         query_states = query_states.view(bsz, tgt_len, self.num_heads, self.head_dim).transpose(1, 2)\n@@ -1258,13 +1260,14 @@ def __init__(self, config: SeamlessM4TConfig, decoder_ffn_dim=None, decoder_atte\n         self.ffn_layer_norm = nn.LayerNorm(config.hidden_size)\n         self.ffn_dropout = nn.Dropout(config.activation_dropout)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -1281,7 +1284,7 @@ def forward(\n             encoder_attention_mask (`torch.FloatTensor`):\n                 encoder attention mask of size `(batch, 1, tgt_len, src_len)` where padding elements are indicated by\n                 very large negative values.\n-            past_key_value (`Tuple(torch.FloatTensor)`):\n+            past_key_values (`Tuple(torch.FloatTensor)`):\n                 cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n@@ -1293,7 +1296,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -1310,7 +1313,7 @@ def forward(\n             hidden_states, cross_attn_weights = self.cross_attention(\n                 hidden_states=hidden_states,\n                 encoder_hidden_states=encoder_hidden_states,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 attention_mask=encoder_attention_mask,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n@@ -1845,7 +1848,7 @@ def forward(\n                 attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -1944,7 +1947,7 @@ def forward(\n                 attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n             )\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,\n@@ -2557,7 +2560,7 @@ def forward(\n \n         encoder_attention_mask = attention_mask\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.text_decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,\n@@ -2816,7 +2819,7 @@ def forward(\n                 hidden_states=encoder_outputs[0], seq_lens=sub_sampled_lengths\n             )\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.text_decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,\n@@ -3084,7 +3087,7 @@ def forward(\n \n         encoder_attention_mask = attention_mask\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.text_decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,\n@@ -3409,7 +3412,7 @@ def forward(\n                 hidden_states=encoder_outputs[0], seq_lens=sub_sampled_lengths\n             )\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.text_decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,\n@@ -3799,7 +3802,7 @@ def forward(\n                 hidden_states=encoder_outputs[0], seq_lens=sub_sampled_lengths\n             )\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.text_decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,"
        },
        {
            "sha": "821586e1fcc70913ad50f1b6ed2862dbdfd4821f",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 23,
            "deletions": 20,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -40,6 +40,7 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_seamless_m4t_v2 import SeamlessM4Tv2Config\n \n \n@@ -900,11 +901,12 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -914,19 +916,19 @@ def forward(\n         is_cross_attention = encoder_hidden_states is not None\n         batch_size, seq_length = hidden_states.shape[:2]\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -936,15 +938,15 @@ def forward(\n             key_states = key_states.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n             value_states = value_states.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         query_states = self.q_proj(hidden_states)\n         query_states = query_states.reshape(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n@@ -1092,13 +1094,14 @@ def __init__(\n         self.ffn_layer_norm = nn.LayerNorm(config.hidden_size)\n         self.ffn_dropout = nn.Dropout(config.activation_dropout)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -1115,7 +1118,7 @@ def forward(\n             encoder_attention_mask (`torch.FloatTensor`):\n                 encoder attention mask of size `(batch, 1, tgt_len, src_len)` where padding elements are indicated by\n                 very large negative values.\n-            past_key_value (`Tuple(torch.FloatTensor)`):\n+            past_key_values (`Tuple(torch.FloatTensor)`):\n                 cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n@@ -1127,7 +1130,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -1144,7 +1147,7 @@ def forward(\n             hidden_states, cross_attn_weights = self.cross_attention(\n                 hidden_states=hidden_states,\n                 encoder_hidden_states=encoder_hidden_states,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 attention_mask=encoder_attention_mask,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n@@ -1888,7 +1891,7 @@ def forward(\n                 attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -2765,7 +2768,7 @@ def forward(\n \n         encoder_attention_mask = attention_mask\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.text_decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,\n@@ -3031,7 +3034,7 @@ def forward(\n                 hidden_states=encoder_outputs[0], seq_lens=sub_sampled_lengths\n             )\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.text_decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,\n@@ -3307,7 +3310,7 @@ def forward(\n \n         encoder_attention_mask = attention_mask\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.text_decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,\n@@ -3670,7 +3673,7 @@ def forward(\n                 hidden_states=encoder_outputs[0], seq_lens=sub_sampled_lengths\n             )\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.text_decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,\n@@ -4097,7 +4100,7 @@ def forward(\n                 hidden_states=encoder_outputs[0], seq_lens=sub_sampled_lengths\n             )\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.text_decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,"
        },
        {
            "sha": "7d3430288831d2463fb729b857e1034efe435973",
            "filename": "src/transformers/models/smollm3/modeling_smollm3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -41,6 +41,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_smollm3 import SmolLM3Config\n \n@@ -150,12 +151,13 @@ def __init__(self, config: SmolLM3Config, layer_idx: int):\n             else None\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -170,9 +172,9 @@ def forward(\n             cos, sin = position_embeddings\n             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             cache_kwargs = {\"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -244,12 +246,13 @@ def __init__(self, config: SmolLM3Config, layer_idx: int):\n         self.post_attention_layernorm = SmolLM3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -262,7 +265,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -412,7 +415,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,"
        },
        {
            "sha": "e06fd1e1b11058979854f0740f49b96ead3f5faa",
            "filename": "src/transformers/models/smollm3/modular_smollm3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -24,6 +24,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..llama.modeling_llama import (\n     LlamaAttention,\n     LlamaDecoderLayer,\n@@ -273,12 +274,13 @@ def __init__(self, config: SmolLM3Config, layer_idx: int):\n             else None\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n@@ -293,9 +295,9 @@ def forward(\n             cos, sin = position_embeddings\n             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             cache_kwargs = {\"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":"
        },
        {
            "sha": "56b9a582a0422542c04a23e5db9d0f09e985e907",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 19,
            "deletions": 16,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -45,6 +45,7 @@\n     is_torch_flex_attn_available,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_speech_to_text import Speech2TextConfig\n \n \n@@ -243,11 +244,12 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n@@ -272,35 +274,35 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n             value_states = self.v_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -423,6 +425,7 @@ def __init__(self, config: Speech2TextConfig, layer_idx=None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoderLayer.forward\n     def forward(\n         self,\n@@ -432,7 +435,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -450,7 +453,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -461,7 +464,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n@@ -481,7 +484,7 @@ def forward(\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -928,7 +931,7 @@ def forward(\n                 encoder_attention_mask=encoder_attention_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -1159,7 +1162,7 @@ def forward(\n         else:\n             encoder_attention_mask = None\n \n-        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n+        # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,"
        },
        {
            "sha": "60ddec72c37e5f6841030e644da3940e7e3da2a1",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 18,
            "deletions": 15,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -39,6 +39,7 @@\n )\n from ...modeling_utils import EmbeddingAccessMixin, PreTrainedModel\n from ...utils import auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_speecht5 import SpeechT5Config, SpeechT5HifiGanConfig\n \n \n@@ -876,11 +877,12 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         position_bias: Optional[torch.Tensor] = None,\n@@ -898,19 +900,19 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states) * self.scaling\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -920,15 +922,15 @@ def forward(\n             key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n             value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n         query_states = query_states.view(bsz, tgt_len, self.num_heads, self.head_dim).transpose(1, 2)\n@@ -1115,6 +1117,7 @@ def __init__(self, config: SpeechT5Config, layer_idx=None):\n         self.feed_forward = SpeechT5FeedForward(config, config.decoder_ffn_dim)\n         self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -1123,7 +1126,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -1141,7 +1144,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -1151,7 +1154,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n@@ -1171,7 +1174,7 @@ def forward(\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -1637,7 +1640,7 @@ def forward(\n                 encoder_attention_mask=encoder_attention_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "8e711c5891dd3a82df897369263bf1b4aa0d65b0",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 18,
            "deletions": 14,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -43,6 +43,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_stablelm import StableLmConfig\n \n \n@@ -220,12 +221,13 @@ def __init__(self, config: StableLmConfig, layer_idx: Optional[int] = None):\n         self.attention_dropout = nn.Dropout(config.attention_dropout)\n         self.rotary_emb = StableLmRotaryEmbedding(config=self.config)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -263,15 +265,15 @@ def forward(\n         query_states = torch.cat((query_rot, query_pass), dim=-1)\n         key_states = torch.cat((key_rot, key_pass), dim=-1)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # Specific to RoPE models with partial rotation\n             cache_kwargs = {\n                 \"sin\": sin,\n                 \"cos\": cos,\n                 \"partial_rotation_size\": self.rotary_ndims,\n                 \"cache_position\": cache_position,\n             }\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         # Repeat k/v heads if n_kv_heads < n_heads\n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n@@ -307,12 +309,13 @@ def forward(\n \n \n class StableLmSdpaAttention(StableLmAttention):\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -328,7 +331,7 @@ def forward(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n@@ -367,15 +370,15 @@ def forward(\n         query_states = torch.cat((query_rot, query_pass), dim=-1)\n         key_states = torch.cat((key_rot, key_pass), dim=-1)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # Specific to RoPE models with partial rotation\n             cache_kwargs = {\n                 \"sin\": sin,\n                 \"cos\": cos,\n                 \"partial_rotation_size\": self.rotary_ndims,\n                 \"cache_position\": cache_position,\n             }\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         # Repeat k/v heads if n_kv_heads < n_heads\n         key_states = repeat_kv(key_states, self.num_key_value_groups)\n@@ -429,12 +432,13 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -479,14 +483,14 @@ def forward(\n         query_states = torch.cat((query_rot, query_pass), dim=-1)\n         key_states = torch.cat((key_rot, key_pass), dim=-1)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             cache_kwargs = {\n                 \"sin\": sin,\n                 \"cos\": cos,\n                 \"partial_rotation_size\": self.rotary_ndims,\n                 \"cache_position\": cache_position,\n             }\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n         # to be able to avoid many of these transpose/reshape/view.\n@@ -542,7 +546,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -558,7 +562,7 @@ def forward(\n                 `[0, config.n_positions - 1]`.\n \n                 [What are position IDs?](../glossary#position-ids)\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*):\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*):\n                 cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n@@ -582,7 +586,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n@@ -735,7 +739,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "98d9bf415f6f040440f0a9aaff6eb5e3bc661b3a",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -46,6 +46,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_starcoder2 import Starcoder2Config\n \n \n@@ -156,12 +157,13 @@ def __init__(self, config: Starcoder2Config, layer_idx: Optional[int] = None):\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.use_bias)\n         self.residual_dropout = config.residual_dropout\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -175,10 +177,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -214,12 +216,13 @@ def __init__(self, config: Starcoder2Config, layer_idx: int):\n         self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.norm_epsilon)\n         self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.norm_epsilon)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n@@ -232,7 +235,7 @@ def forward(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -374,7 +377,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,"
        },
        {
            "sha": "81cad212b09cb6f5347c167ffa417c90a07da8e9",
            "filename": "src/transformers/models/starcoder2/modular_starcoder2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -35,6 +35,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..mistral.modeling_mistral import (\n     MistralAttention,\n     MistralDecoderLayer,\n@@ -78,12 +79,13 @@ def __init__(self, config: Starcoder2Config, layer_idx: Optional[int] = None):\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.use_bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -97,10 +99,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -201,7 +203,7 @@ def forward(\n                 hidden_states,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,"
        },
        {
            "sha": "0027a48c0d096406278c2b8b0a6d0d67ce897d3f",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 21,
            "deletions": 17,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -45,6 +45,7 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_switch_transformers import SwitchTransformersConfig\n \n \n@@ -476,13 +477,14 @@ def compute_bias(self, query_length, key_length, device=None, cache_position=Non\n         values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, query_length, key_length)\n         return values\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         mask=None,\n         key_value_states=None,\n         position_bias=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         layer_head_mask=None,\n         query_length=None,\n         use_cache=False,\n@@ -503,18 +505,18 @@ def forward(\n         query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n         # Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`\n-        if past_key_value is not None and isinstance(past_key_value, EncoderDecoderCache):\n-            is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None and isinstance(past_key_values, EncoderDecoderCache):\n+            is_updated = past_key_values.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                curr_past_key_value = past_key_value.cross_attention_cache\n+                curr_past_key_value = past_key_values.cross_attention_cache\n             else:\n-                curr_past_key_value = past_key_value.self_attention_cache\n+                curr_past_key_value = past_key_values.self_attention_cache\n         else:\n-            curr_past_key_value = past_key_value\n+            curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -524,15 +526,15 @@ def forward(\n             key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n             value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n         scores = torch.matmul(query_states, key_states.transpose(3, 2))\n@@ -597,13 +599,14 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n         self.layer_norm = SwitchTransformersLayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         position_bias=None,\n         layer_head_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         use_cache=False,\n         output_attentions=False,\n         cache_position=None,\n@@ -614,7 +617,7 @@ def forward(\n             mask=attention_mask,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -634,14 +637,15 @@ def __init__(self, config, layer_idx: Optional[int] = None):\n         self.layer_norm = SwitchTransformersLayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         key_value_states,\n         attention_mask=None,\n         position_bias=None,\n         layer_head_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         use_cache=False,\n         query_length=None,\n         output_attentions=False,\n@@ -654,7 +658,7 @@ def forward(\n             key_value_states=key_value_states,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             query_length=query_length,\n             output_attentions=output_attentions,\n@@ -691,7 +695,7 @@ def forward(\n         encoder_decoder_position_bias=None,\n         layer_head_mask=None,\n         cross_attn_layer_head_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         use_cache=False,\n         output_attentions=False,\n         output_router_logits=True,\n@@ -703,7 +707,7 @@ def forward(\n             attention_mask=attention_mask,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -724,7 +728,7 @@ def forward(\n                 attention_mask=encoder_attention_mask,\n                 position_bias=encoder_decoder_position_bias,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 query_length=cache_position[-1] + 1,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n@@ -1022,7 +1026,7 @@ def forward(\n                 encoder_decoder_position_bias,\n                 layer_head_mask=layer_head_mask,\n                 cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n                 output_router_logits=output_router_logits,"
        },
        {
            "sha": "f8f4059c20ec42cbaff3d457b70b05f50a499d3e",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 22,
            "deletions": 17,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -50,6 +50,7 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.model_parallel_utils import assert_device_map, get_device_map\n from .configuration_t5 import T5Config\n \n@@ -464,13 +465,14 @@ def compute_bias(self, query_length, key_length, device=None, cache_position=Non\n         values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, query_length, key_length)\n         return values\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         mask=None,\n         key_value_states=None,\n         position_bias=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         layer_head_mask=None,\n         query_length=None,\n         use_cache=False,\n@@ -491,18 +493,18 @@ def forward(\n         query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n         # Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`\n-        if past_key_value is not None and isinstance(past_key_value, EncoderDecoderCache):\n-            is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None and isinstance(past_key_values, EncoderDecoderCache):\n+            is_updated = past_key_values.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                curr_past_key_value = past_key_value.cross_attention_cache\n+                curr_past_key_value = past_key_values.cross_attention_cache\n             else:\n-                curr_past_key_value = past_key_value.self_attention_cache\n+                curr_past_key_value = past_key_values.self_attention_cache\n         else:\n-            curr_past_key_value = past_key_value\n+            curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -512,15 +514,15 @@ def forward(\n             key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n             value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n         scores = torch.matmul(query_states, key_states.transpose(3, 2))\n@@ -584,13 +586,14 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n         self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         position_bias=None,\n         layer_head_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         use_cache=False,\n         output_attentions=False,\n         cache_position=None,\n@@ -601,7 +604,7 @@ def forward(\n             mask=attention_mask,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -618,14 +621,15 @@ def __init__(self, config, layer_idx: Optional[int] = None):\n         self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         key_value_states,\n         attention_mask=None,\n         position_bias=None,\n         layer_head_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         use_cache=False,\n         query_length=None,\n         output_attentions=False,\n@@ -638,7 +642,7 @@ def forward(\n             key_value_states=key_value_states,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             query_length=query_length,\n             output_attentions=output_attentions,\n@@ -662,6 +666,7 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n \n         self.layer.append(T5LayerFF(config))\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -672,7 +677,7 @@ def forward(\n         encoder_decoder_position_bias=None,\n         layer_head_mask=None,\n         cross_attn_layer_head_mask=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         use_cache=False,\n         output_attentions=False,\n         return_dict=True,\n@@ -683,7 +688,7 @@ def forward(\n             attention_mask=attention_mask,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n@@ -708,7 +713,7 @@ def forward(\n                 attention_mask=encoder_attention_mask,\n                 position_bias=encoder_decoder_position_bias,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 query_length=cache_position[-1] + 1,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n@@ -1098,7 +1103,7 @@ def forward(\n                 encoder_decoder_position_bias,  # as a positional argument for gradient checkpointing\n                 layer_head_mask=layer_head_mask,\n                 cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n                 return_dict=return_dict,"
        },
        {
            "sha": "8fcb072ca8794ebf59ab93514659dd6ef0f6a3f4",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 25,
            "deletions": 17,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -44,6 +44,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_t5gemma import T5GemmaConfig, T5GemmaModuleConfig\n \n \n@@ -235,12 +236,13 @@ def __init__(self, config: T5GemmaModuleConfig, layer_idx: int):\n         self.attn_logit_softcapping = self.config.attn_logit_softcapping\n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -254,10 +256,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -312,12 +314,13 @@ def __init__(self, config: T5GemmaModuleConfig, layer_idx: int):\n         if config.cross_attention_hidden_size is None:\n             raise ValueError(\"Cross-attention needs cross_attention_hidden_size to be specified.\")\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor],\n         encoder_hidden_states: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         if encoder_hidden_states is None:\n@@ -327,19 +330,19 @@ def forward(\n         hidden_shape = (*input_shape, -1, self.head_dim)\n         query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            is_updated = past_key_value.is_updated.get(self.layer_idx)\n-            curr_past_key_value = past_key_value.cross_attention_cache\n+        if past_key_values is not None:\n+            is_updated = past_key_values.is_updated.get(self.layer_idx)\n+            curr_past_key_value = past_key_values.cross_attention_cache\n \n-        if past_key_value is None or not is_updated:\n+        if past_key_values is None or not is_updated:\n             encoder_input_shape = encoder_hidden_states.shape[:-1]\n             encoder_hidden_shape = (*encoder_input_shape, -1, self.head_dim)\n             key_states = self.k_proj(encoder_hidden_states).view(encoder_hidden_shape).transpose(1, 2)\n             value_states = self.v_proj(encoder_hidden_states).view(encoder_hidden_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 key_states, value_states = curr_past_key_value.update(key_states, value_states, self.layer_idx)\n-                past_key_value.is_updated[self.layer_idx] = True\n+                past_key_values.is_updated[self.layer_idx] = True\n         else:\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -404,7 +407,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=None,\n+            past_key_values=None,\n             **kwargs,\n         )\n         hidden_states = self.post_self_attn_layernorm(hidden_states)\n@@ -427,13 +430,14 @@ def __init__(self, config, layer_idx: int):\n         self.pre_cross_attn_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_cross_attn_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[EncoderDecoderCache] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n@@ -447,7 +451,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value.self_attention_cache if past_key_value is not None else None,\n+            past_key_values=past_key_values.self_attention_cache if past_key_values is not None else None,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n@@ -461,7 +465,7 @@ def forward(\n             hidden_states=hidden_states,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=encoder_attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             **kwargs,\n         )\n@@ -530,12 +534,13 @@ def __init__(self, config: T5GemmaConfig, layer_idx: int):\n         self.attn_logit_softcapping = self.config.attn_logit_softcapping\n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -549,10 +554,10 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -712,6 +717,9 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n+        # As we want to pass `past_key_values=None` explicitly everwhere, we need to pop them from kwargs if present\n+        kwargs.pop(\"past_key_values\", None)\n+\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n "
        },
        {
            "sha": "92f1b59ea8ffe2b285e2a4fef6bf16047497f471",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 17,
            "deletions": 11,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -43,6 +43,7 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ..gemma2.configuration_gemma2 import Gemma2Config\n from ..gemma2.modeling_gemma2 import (\n     Gemma2Attention,\n@@ -262,12 +263,13 @@ def __init__(self, config: T5GemmaModuleConfig, layer_idx: int):\n             config.cross_attention_hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n         )\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor],\n         encoder_hidden_states: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         if encoder_hidden_states is None:\n@@ -277,19 +279,19 @@ def forward(\n         hidden_shape = (*input_shape, -1, self.head_dim)\n         query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            is_updated = past_key_value.is_updated.get(self.layer_idx)\n-            curr_past_key_value = past_key_value.cross_attention_cache\n+        if past_key_values is not None:\n+            is_updated = past_key_values.is_updated.get(self.layer_idx)\n+            curr_past_key_value = past_key_values.cross_attention_cache\n \n-        if past_key_value is None or not is_updated:\n+        if past_key_values is None or not is_updated:\n             encoder_input_shape = encoder_hidden_states.shape[:-1]\n             encoder_hidden_shape = (*encoder_input_shape, -1, self.head_dim)\n             key_states = self.k_proj(encoder_hidden_states).view(encoder_hidden_shape).transpose(1, 2)\n             value_states = self.v_proj(encoder_hidden_states).view(encoder_hidden_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 key_states, value_states = curr_past_key_value.update(key_states, value_states, self.layer_idx)\n-                past_key_value.is_updated[self.layer_idx] = True\n+                past_key_values.is_updated[self.layer_idx] = True\n         else:\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -378,7 +380,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=None,\n+            past_key_values=None,\n             **kwargs,\n         )\n         hidden_states = self.post_self_attn_layernorm(hidden_states)\n@@ -401,13 +403,14 @@ def __init__(self, config, layer_idx: int):\n         self.pre_cross_attn_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_cross_attn_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[EncoderDecoderCache] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n@@ -421,7 +424,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            past_key_value=past_key_value.self_attention_cache if past_key_value is not None else None,\n+            past_key_values=past_key_values.self_attention_cache if past_key_values is not None else None,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n@@ -435,7 +438,7 @@ def forward(\n             hidden_states=hidden_states,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=encoder_attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             use_cache=use_cache,\n             **kwargs,\n         )\n@@ -577,6 +580,9 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n+        # As we want to pass `past_key_values=None` explicitly everwhere, we need to pop them from kwargs if present\n+        kwargs.pop(\"past_key_values\", None)\n+\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n "
        },
        {
            "sha": "6704c77b3b775ff605e4440bb0118576abbf3769",
            "filename": "src/transformers/models/tapas/modeling_tapas.py",
            "status": "modified",
            "additions": 19,
            "deletions": 17,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -32,6 +32,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_tapas import TapasConfig\n \n \n@@ -300,13 +301,14 @@ def __init__(self, config, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n-        past_key_value=None,\n+        past_key_values=None,\n         output_attentions=False,\n         cache_position=None,\n     ):\n@@ -318,19 +320,19 @@ def forward(\n         )\n \n         is_cross_attention = encoder_hidden_states is not None\n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_layer = curr_past_key_value.layers[self.layer_idx].keys\n             value_layer = curr_past_key_value.layers[self.layer_idx].values\n@@ -346,15 +348,15 @@ def forward(\n                 .transpose(1, 2)\n             )\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_layer, value_layer = curr_past_key_value.update(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n@@ -382,7 +384,7 @@ def forward(\n \n         outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n         if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n+            outputs = outputs + (past_key_values,)\n         return outputs\n \n \n@@ -434,7 +436,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -443,7 +445,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n@@ -506,7 +508,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n@@ -515,7 +517,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n@@ -533,7 +535,7 @@ def forward(\n                 attention_mask=encoder_attention_mask,\n                 head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -597,7 +599,7 @@ def forward(\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )"
        },
        {
            "sha": "20e6b11b68644815f25ea93ea8f99059e1623b09",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 18,
            "deletions": 15,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -42,6 +42,7 @@\n from ...processing_utils import Unpack\n from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_time_series_transformer import TimeSeriesTransformerConfig\n \n \n@@ -351,11 +352,12 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -380,19 +382,19 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -402,15 +404,15 @@ def forward(\n             key_states = key_states.view(*kv_input_shape).transpose(1, 2)\n             value_states = value_states.view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -540,6 +542,7 @@ def __init__(self, config: TimeSeriesTransformerConfig, layer_idx: Optional[int]\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -548,7 +551,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -566,7 +569,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size `(decoder_attention_heads,)`.\n-            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -579,7 +582,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n@@ -599,7 +602,7 @@ def forward(\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -1061,7 +1064,7 @@ def forward(\n                 encoder_attention_mask=encoder_attention_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "e9a12069a0cf41ffc773d48f83d69fe1df876e57",
            "filename": "src/transformers/models/trocr/modeling_trocr.py",
            "status": "modified",
            "additions": 18,
            "deletions": 15,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe",
            "patch": "@@ -32,6 +32,7 @@\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_trocr import TrOCRConfig\n \n \n@@ -178,11 +179,12 @@ def __init__(\n \n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n@@ -198,19 +200,19 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states) * self.scaling\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+        if past_key_values is not None:\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                is_updated = past_key_values.is_updated.get(self.layer_idx)\n                 if is_cross_attention:\n                     # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_value.cross_attention_cache\n+                    curr_past_key_value = past_key_values.cross_attention_cache\n                 else:\n-                    curr_past_key_value = past_key_value.self_attention_cache\n+                    curr_past_key_value = past_key_values.self_attention_cache\n             else:\n-                curr_past_key_value = past_key_value\n+                curr_past_key_value = past_key_values\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_value is not None and is_updated:\n+        if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n             value_states = curr_past_key_value.layers[self.layer_idx].values\n@@ -220,15 +222,15 @@ def forward(\n             key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n             value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n                 key_states, value_states = curr_past_key_value.update(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                 if is_cross_attention:\n-                    past_key_value.is_updated[self.layer_idx] = True\n+                    past_key_values.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n         query_states = query_states.view(bsz, tgt_len, self.num_heads, self.head_dim).transpose(1, 2)\n@@ -330,6 +332,7 @@ def __init__(self, config: TrOCRConfig, layer_idx=None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -338,7 +341,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -356,7 +359,7 @@ def forward(\n                 `(encoder_attention_heads,)`.\n             cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                 size *(decoder_attention_heads,)*.\n-            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n+            past_key_values (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -366,7 +369,7 @@ def forward(\n         # Self Attention\n         hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n@@ -387,7 +390,7 @@ def forward(\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n@@ -648,7 +651,7 @@ def forward(\n                 encoder_attention_mask=encoder_attention_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,"
        },
        {
            "sha": "5a807e283e9b53c9e7eb7f87589e0a4707ba8c48",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe"
        },
        {
            "sha": "aa1470e0067a2a5557fedd3712f1f89f3e8d85d4",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 23,
            "deletions": 18,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe"
        },
        {
            "sha": "bfeb0eb7b9f6e0d7717c528065e12a2585cfbf06",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 20,
            "deletions": 17,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe"
        },
        {
            "sha": "d9df105204e870e33b392a4ba7736cf073406b40",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 18,
            "deletions": 15,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe"
        },
        {
            "sha": "fdff73ff77fcf3c7aabf093b9959f7759f95a5e4",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 33,
            "deletions": 28,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe"
        },
        {
            "sha": "c666aef841cb1c72a2c131d3dc1ba15ae9c9c9db",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 32,
            "deletions": 27,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe"
        },
        {
            "sha": "80e16dc966ae70dacd3148417e9398c024bca2ec",
            "filename": "src/transformers/models/xmod/modeling_xmod.py",
            "status": "modified",
            "additions": 20,
            "deletions": 16,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe"
        },
        {
            "sha": "e04af25febb0b7695a953a77ed230fddce7831c8",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 20,
            "deletions": 15,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe"
        },
        {
            "sha": "2f1e1e0bc6b25d29a480562636181b5873b57f81",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 20,
            "deletions": 15,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe"
        },
        {
            "sha": "5eb00899c2a5de0e4e481f08169c6180ebe718cb",
            "filename": "src/transformers/models/zamba2/modular_zamba2.py",
            "status": "modified",
            "additions": 15,
            "deletions": 11,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe"
        },
        {
            "sha": "ba03cf9cfe196d974a90dfd586e431702a8103ab",
            "filename": "src/transformers/utils/auto_docstring.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fauto_docstring.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe"
        },
        {
            "sha": "1874631f5e879e721e5c277ebc144ff5b2aa64df",
            "filename": "tests/utils/test_auto_docstring.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c3fb7f731743609262d7c77a636b41f69d204fe/tests%2Futils%2Ftest_auto_docstring.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c3fb7f731743609262d7c77a636b41f69d204fe/tests%2Futils%2Ftest_auto_docstring.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_auto_docstring.py?ref=5c3fb7f731743609262d7c77a636b41f69d204fe"
        }
    ],
    "stats": {
        "total": 5908,
        "additions": 3167,
        "deletions": 2741
    }
}