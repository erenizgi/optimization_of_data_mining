{
    "author": "garg-amit",
    "message": "Add support for partial rotary embeddings in Phi3 model (#35947)\n\n* Added support for partial_rotary_factor\n\n* addressed comments\n\n* refactored",
    "sha": "0ae93d31ce3b2acd2ea6dab03991b126b6c80a32",
    "files": [
        {
            "sha": "ec133fbd7d504131eb0213f4d1972f85504246ac",
            "filename": "src/transformers/models/phi3/configuration_phi3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ae93d31ce3b2acd2ea6dab03991b126b6c80a32/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ae93d31ce3b2acd2ea6dab03991b126b6c80a32/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py?ref=0ae93d31ce3b2acd2ea6dab03991b126b6c80a32",
            "patch": "@@ -81,6 +81,8 @@ class Phi3Config(PretrainedConfig):\n             contain the following keys: `type`, `short_factor` and `long_factor`. The `type` must be `longrope` and\n             the `short_factor` and `long_factor` must be lists of numbers with the same length as the hidden size\n             divided by the number of attention heads divided by 2.\n+        partial_rotary_factor (`float`, *optional*, defaults to 1.0):\n+            Percentage of the query and keys which will have rotary embedding. Must be between 0.0 and 1.0.\n         bos_token_id (`int`, *optional*, defaults to 1):\n             The id of the \"beginning-of-sequence\" token.\n         eos_token_id (`int`, *optional*, defaults to 32000):\n@@ -139,6 +141,7 @@ def __init__(\n         tie_word_embeddings=False,\n         rope_theta=10000.0,\n         rope_scaling=None,\n+        partial_rotary_factor=1.0,\n         bos_token_id=1,\n         eos_token_id=32000,\n         pad_token_id=32000,\n@@ -166,6 +169,7 @@ def __init__(\n         self.use_cache = use_cache\n         self.rope_theta = rope_theta\n         self.rope_scaling = rope_scaling\n+        self.partial_rotary_factor = partial_rotary_factor\n         self._rope_scaling_adjustment()\n         self._rope_scaling_validation()\n         self.sliding_window = sliding_window\n@@ -215,9 +219,10 @@ def _rope_scaling_validation(self):\n             raise ValueError(\n                 f\"`rope_scaling`'s short_factor field must be a list of numbers, got {rope_scaling_short_factor}\"\n             )\n-        if not len(rope_scaling_short_factor) == self.hidden_size // self.num_attention_heads // 2:\n+        rotary_ndims = int(self.hidden_size // self.num_attention_heads * self.partial_rotary_factor)\n+        if not len(rope_scaling_short_factor) == rotary_ndims // 2:\n             raise ValueError(\n-                f\"`rope_scaling`'s short_factor field must have length {self.hidden_size // self.num_attention_heads // 2}, got {len(rope_scaling_short_factor)}\"\n+                f\"`rope_scaling`'s short_factor field must have length {rotary_ndims // 2}, got {len(rope_scaling_short_factor)}\"\n             )\n         if not (\n             isinstance(rope_scaling_long_factor, list)\n@@ -226,9 +231,9 @@ def _rope_scaling_validation(self):\n             raise ValueError(\n                 f\"`rope_scaling`'s long_factor field must be a list of numbers, got {rope_scaling_long_factor}\"\n             )\n-        if not len(rope_scaling_long_factor) == self.hidden_size // self.num_attention_heads // 2:\n+        if not len(rope_scaling_long_factor) == rotary_ndims // 2:\n             raise ValueError(\n-                f\"`rope_scaling`'s long_factor field must have length {self.hidden_size // self.num_attention_heads // 2}, got {len(rope_scaling_long_factor)}\"\n+                f\"`rope_scaling`'s long_factor field must have length {rotary_ndims // 2}, got {len(rope_scaling_long_factor)}\"\n             )\n \n "
        },
        {
            "sha": "a26e3a979377c7d342183bf0468000d883ccaa9b",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 32,
            "deletions": 27,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ae93d31ce3b2acd2ea6dab03991b126b6c80a32/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ae93d31ce3b2acd2ea6dab03991b126b6c80a32/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=0ae93d31ce3b2acd2ea6dab03991b126b6c80a32",
            "patch": "@@ -82,33 +82,6 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n-    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n-\n-    Args:\n-        q (`torch.Tensor`): The query tensor.\n-        k (`torch.Tensor`): The key tensor.\n-        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n-        sin (`torch.Tensor`): The sine part of the rotary embedding.\n-        position_ids (`torch.Tensor`, *optional*):\n-            Deprecated and unused.\n-        unsqueeze_dim (`int`, *optional*, defaults to 1):\n-            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n-            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n-            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n-            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n-            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n-            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n-    Returns:\n-        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n-    \"\"\"\n-    cos = cos.unsqueeze(unsqueeze_dim)\n-    sin = sin.unsqueeze(unsqueeze_dim)\n-    q_embed = (q * cos) + (rotate_half(q) * sin)\n-    k_embed = (k * cos) + (rotate_half(k) * sin)\n-    return q_embed, k_embed\n-\n-\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -147,6 +120,38 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+\n+    rotary_dim = cos.shape[-1]\n+    q_rot, q_pass = q[..., :rotary_dim], q[..., rotary_dim:]\n+    k_rot, k_pass = k[..., :rotary_dim], k[..., rotary_dim:]\n+\n+    q_embed = torch.cat([(q_rot * cos) + (rotate_half(q_rot) * sin), q_pass], dim=-1)\n+    k_embed = torch.cat([(k_rot * cos) + (rotate_half(k_rot) * sin), k_pass], dim=-1)\n+    return q_embed, k_embed\n+\n+\n class Phi3Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n "
        },
        {
            "sha": "caddb77f6fcd87ac6dc0df69cb47fc5f290b63e6",
            "filename": "src/transformers/models/phi3/modular_phi3.py",
            "status": "modified",
            "additions": 33,
            "deletions": 1,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ae93d31ce3b2acd2ea6dab03991b126b6c80a32/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ae93d31ce3b2acd2ea6dab03991b126b6c80a32/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py?ref=0ae93d31ce3b2acd2ea6dab03991b126b6c80a32",
            "patch": "@@ -34,8 +34,8 @@\n     MistralForTokenClassification,\n     MistralPreTrainedModel,\n     MistralRotaryEmbedding,\n-    apply_rotary_pos_emb,\n     eager_attention_forward,\n+    rotate_half,\n )\n from .configuration_phi3 import Phi3Config\n \n@@ -64,6 +64,38 @@ def forward(self, hidden_states: torch.FloatTensor) -> torch.FloatTensor:\n         return self.down_proj(up_states)\n \n \n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+\n+    rotary_dim = cos.shape[-1]\n+    q_rot, q_pass = q[..., :rotary_dim], q[..., rotary_dim:]\n+    k_rot, k_pass = k[..., :rotary_dim], k[..., rotary_dim:]\n+\n+    q_embed = torch.cat([(q_rot * cos) + (rotate_half(q_rot) * sin), q_pass], dim=-1)\n+    k_embed = torch.cat([(k_rot * cos) + (rotate_half(k_rot) * sin), k_pass], dim=-1)\n+    return q_embed, k_embed\n+\n+\n class Phi3Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n "
        }
    ],
    "stats": {
        "total": 106,
        "additions": 74,
        "deletions": 32
    }
}