{
    "author": "MekkCyber",
    "message": "Fixing Bitnet after use_rms_norm introduction (#38229)\n\n* fix\n\n* make style",
    "sha": "2a79471318a9b7b16706f3bb5cd833c7e81919a6",
    "files": [
        {
            "sha": "2af9447f8dcd0f7208e61c857b50654e69da65ba",
            "filename": "src/transformers/integrations/bitnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a79471318a9b7b16706f3bb5cd833c7e81919a6/src%2Ftransformers%2Fintegrations%2Fbitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a79471318a9b7b16706f3bb5cd833c7e81919a6/src%2Ftransformers%2Fintegrations%2Fbitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fbitnet.py?ref=2a79471318a9b7b16706f3bb5cd833c7e81919a6",
            "patch": "@@ -362,8 +362,8 @@ def _replace_with_bitnet_linear(\n                             bias=module.bias is not None,\n                             device=module.weight.device,\n                             dtype=module.weight.dtype,\n-                            use_rms_norm=quantization_config.use_rms_norm,\n-                            rms_norm_eps=quantization_config.rms_norm_eps,\n+                            use_rms_norm=quantization_config.use_rms_norm if quantization_config else False,\n+                            rms_norm_eps=quantization_config.rms_norm_eps if quantization_config else 1e-6,\n                         )\n                         model._modules[name].requires_grad_(False)\n                     has_been_replaced = True"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}