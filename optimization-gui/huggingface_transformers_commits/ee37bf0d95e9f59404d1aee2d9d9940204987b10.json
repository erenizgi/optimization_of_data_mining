{
    "author": "Cyrilvallez",
    "message": "Automatic compilation in generate: do not rely on inner function (#34923)\n\n* compiled forward in PreTrainedModel\r\n\r\n* update\r\n\r\n* style\r\n\r\n* update name\r\n\r\n* trigger CIs\r\n\r\n* Add way to use custom compile args\r\n\r\n* style\r\n\r\n* switch parameterization to generation_config\r\n\r\n* Add to inits\r\n\r\n* Update configuration_utils.py\r\n\r\n* inits\r\n\r\n* style\r\n\r\n* docs\r\n\r\n* style\r\n\r\n* Update configuration_utils.py\r\n\r\n* back without dataclass for repo consistency\r\n\r\n* Update configuration_utils.py\r\n\r\n* style\r\n\r\n* style\r\n\r\n* style once again\r\n\r\n* add config serialization\r\n\r\n* update\r\n\r\n* true dataclass\r\n\r\n* trigger CIs\r\n\r\n* merge compile methods + remove serialization of compile config",
    "sha": "ee37bf0d95e9f59404d1aee2d9d9940204987b10",
    "files": [
        {
            "sha": "a54ac432006a84fb28db1dc8766722a9fc261a09",
            "filename": "docs/source/en/internal/generation_utils.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee37bf0d95e9f59404d1aee2d9d9940204987b10/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee37bf0d95e9f59404d1aee2d9d9940204987b10/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md?ref=ee37bf0d95e9f59404d1aee2d9d9940204987b10",
            "patch": "@@ -436,3 +436,9 @@ A [`Constraint`] can be used to force the generation to include specific tokens\n \n [[autodoc]] SynthIDTextWatermarkDetector\n     - __call__\n+\n+## Compile Utils\n+\n+[[autodoc]] CompileConfig\n+    - __call__\n+"
        },
        {
            "sha": "e1ca195680731833f8c903084e7737701fd72d03",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee37bf0d95e9f59404d1aee2d9d9940204987b10/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee37bf0d95e9f59404d1aee2d9d9940204987b10/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=ee37bf0d95e9f59404d1aee2d9d9940204987b10",
            "patch": "@@ -122,6 +122,7 @@\n     \"feature_extraction_utils\": [\"BatchFeature\", \"FeatureExtractionMixin\"],\n     \"file_utils\": [],\n     \"generation\": [\n+        \"CompileConfig\",\n         \"GenerationConfig\",\n         \"TextIteratorStreamer\",\n         \"TextStreamer\",\n@@ -4981,7 +4982,7 @@\n     from .feature_extraction_utils import BatchFeature, FeatureExtractionMixin\n \n     # Generation\n-    from .generation import GenerationConfig, TextIteratorStreamer, TextStreamer, WatermarkingConfig\n+    from .generation import CompileConfig, GenerationConfig, TextIteratorStreamer, TextStreamer, WatermarkingConfig\n     from .hf_argparser import HfArgumentParser\n \n     # Integrations"
        },
        {
            "sha": "59d970db15416f8b2dc3e4ef251467defbc8fb03",
            "filename": "src/transformers/generation/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee37bf0d95e9f59404d1aee2d9d9940204987b10/src%2Ftransformers%2Fgeneration%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee37bf0d95e9f59404d1aee2d9d9940204987b10/src%2Ftransformers%2Fgeneration%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2F__init__.py?ref=ee37bf0d95e9f59404d1aee2d9d9940204987b10",
            "patch": "@@ -20,6 +20,7 @@\n _import_structure = {\n     \"configuration_utils\": [\n         \"BaseWatermarkingConfig\",\n+        \"CompileConfig\",\n         \"GenerationConfig\",\n         \"GenerationMode\",\n         \"SynthIDTextWatermarkingConfig\",\n@@ -192,6 +193,7 @@\n if TYPE_CHECKING:\n     from .configuration_utils import (\n         BaseWatermarkingConfig,\n+        CompileConfig,\n         GenerationConfig,\n         GenerationMode,\n         SynthIDTextWatermarkingConfig,"
        },
        {
            "sha": "486cd2336c3e35845a7063746ac31b96968e532b",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 67,
            "deletions": 2,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee37bf0d95e9f59404d1aee2d9d9940204987b10/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee37bf0d95e9f59404d1aee2d9d9940204987b10/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=ee37bf0d95e9f59404d1aee2d9d9940204987b10",
            "patch": "@@ -20,7 +20,7 @@\n import warnings\n from abc import ABC, abstractmethod\n from dataclasses import dataclass, is_dataclass\n-from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union\n+from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Union\n \n from .. import __version__\n from ..configuration_utils import PretrainedConfig\n@@ -371,6 +371,12 @@ class GenerationConfig(PushToHubMixin):\n             to correctly align tokens. Can only be used with different tokenizers in speculative decoding.\n             See this [blog](https://huggingface.co/blog/universal_assisted_generation) for more details.\n \n+        > Parameters related to performances and compilation\n+\n+        compile_config (CompileConfig, *optional*):\n+            If using a static cache, this controls how `generate` will `compile` the forward pass for performance\n+            gains.\n+\n         > Wild card\n \n         generation_kwargs:\n@@ -474,6 +480,9 @@ def __init__(self, **kwargs):\n         self.assistant_lookbehind = kwargs.pop(\"assistant_lookbehind\", 10)\n         self.target_lookbehind = kwargs.pop(\"target_lookbehind\", 10)\n \n+        # Performances\n+        self.compile_config = kwargs.pop(\"compile_config\", CompileConfig())\n+\n         # Wild card\n         self.generation_kwargs = kwargs.pop(\"generation_kwargs\", {})\n \n@@ -794,7 +803,13 @@ def validate(self, is_init=False):\n                 self.watermarking_config = WatermarkingConfig.from_dict(self.watermarking_config)\n             self.watermarking_config.validate()\n \n-        # 7. other incorrect combinations\n+        # 7. performances arguments\n+        if not isinstance(self.compile_config, CompileConfig):\n+            raise ValueError(\n+                f\"You provided `compile_config` as an instance of {type(self.compile_config)}, but it must be an instance of `CompileConfig`.\"\n+            )\n+\n+        # 8. other incorrect combinations\n         if self.return_dict_in_generate is not True:\n             for extra_output_flag in self.extra_output_flags:\n                 if getattr(self, extra_output_flag) is True:\n@@ -1175,6 +1190,8 @@ def to_dict(self) -> Dict[str, Any]:\n             del output[\"_commit_hash\"]\n         if \"_original_object_hash\" in output:\n             del output[\"_original_object_hash\"]\n+        if \"compile_config\" in output:\n+            del output[\"compile_config\"]\n \n         # Transformers version when serializing this file\n         output[\"transformers_version\"] = __version__\n@@ -1559,3 +1576,51 @@ def construct_processor(self, vocab_size: int, device) -> \"WatermarkLogitsProces\n             skip_first_ngram_calls=self.skip_first_ngram_calls,\n             debug_mode=self.debug_mode,\n         )\n+\n+\n+@dataclass\n+class CompileConfig(object):\n+    \"\"\"\n+    Class that holds arguments relative to `torch.compile` behavior, when using automatic compilation in `generate`.\n+    See [`torch.compile`](https://pytorch.org/docs/stable/generated/torch.compile.html) for more details on the arguments.\n+\n+    Args:\n+        fullgraph (`bool`, *optional*, defaults to `True`):\n+            If `True`, requires that the whole forward be capturable in a single graph.\n+        dynamic (`bool` or `None`, *optional*):\n+            Whether to try to use dynamic shape graphs.\n+        backend (`str` or `Callable`, *optional*, defaults to `\"inductor\"`):\n+            Backend to be used.\n+        mode (`str`, *optional*, defaults to `\"reduce-overhead\"`):\n+            Controls balance between performance and overhead.\n+        options (`dict`, *optional*):\n+            A dictionary of options to pass to the backend.\n+\n+    Examples:\n+    ```python\n+    >>> from transformers import AutoModelForCausalLM, AutoTokenizer, CompileConfig\n+\n+    >>> tokenizer = AutoTokenizer.from_pretrained('google/gemma-2-2b')\n+    >>> model = AutoModelForCausalLM.from_pretrained('google/gemma-2-2b').cuda()\n+\n+    >>> # Automatic compile configuration, used with static cache\n+    >>> compile_config = CompileConfig(dynamic=True)\n+\n+    >>> # Generation with static cache and compile config\n+    >>> input = tokenizer.encode(\"Hello there, how\", return_tensors=\"pt\").cuda()\n+    >>> output = model.generate(\n+    ...     input, do_sample=False, max_new_tokens=300, cache_implementation=\"static\", compile_config=compile_config\n+    ... )\n+    >>> output_text = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n+    ```\n+    \"\"\"\n+\n+    fullgraph: bool = True\n+    dynamic: Optional[bool] = None\n+    backend: Union[str, Callable] = \"inductor\"\n+    mode: str = \"reduce-overhead\"\n+    options: Optional[dict] = None\n+\n+    def to_dict(self) -> Dict[str, Any]:\n+        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n+        return copy.deepcopy(self.__dict__)"
        },
        {
            "sha": "1982841df66733c0f1634a03e1b6b2d1ad2498ce",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee37bf0d95e9f59404d1aee2d9d9940204987b10/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee37bf0d95e9f59404d1aee2d9d9940204987b10/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=ee37bf0d95e9f59404d1aee2d9d9940204987b10",
            "patch": "@@ -3230,16 +3230,14 @@ def _sample(\n         unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n         model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n \n-        def model_forward(model, *args, **kwargs):\n-            return model.forward(*args, **kwargs)\n-\n+        model_forward = self.__call__\n         if isinstance(model_kwargs.get(\"past_key_values\"), StaticCache):\n             if self.device.type == \"cuda\":\n                 logger.warning_once(\"Using `torch.compile`.\")\n                 os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n-                model_forward = torch.compile(model_forward, mode=\"reduce-overhead\", fullgraph=True)\n+                model_forward = self.get_compiled_call(generation_config.compile_config)\n \n-        i = 0\n+        is_prefill = True\n         while self._has_unfinished_sequences(\n             this_peer_finished, synced_gpus, device=input_ids.device, cur_len=cur_len, max_length=max_length\n         ):\n@@ -3250,11 +3248,11 @@ def model_forward(model, *args, **kwargs):\n             model_inputs.update({\"output_attentions\": output_attentions} if output_attentions else {})\n             model_inputs.update({\"output_hidden_states\": output_hidden_states} if output_hidden_states else {})\n \n-            if i == 0:\n+            if is_prefill:\n                 outputs = self(**model_inputs, return_dict=True)\n-                i += 1\n+                is_prefill = False\n             else:\n-                outputs = model_forward(self, return_dict=True, **model_inputs)\n+                outputs = model_forward(**model_inputs, return_dict=True)\n \n             # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\n             model_kwargs = self._update_model_kwargs_for_generation("
        },
        {
            "sha": "50622c9f55145a0d88a1080f1bd039739f6d541c",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 16,
            "deletions": 1,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee37bf0d95e9f59404d1aee2d9d9940204987b10/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee37bf0d95e9f59404d1aee2d9d9940204987b10/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=ee37bf0d95e9f59404d1aee2d9d9940204987b10",
            "patch": "@@ -43,7 +43,7 @@\n from .activations import get_activation\n from .configuration_utils import PretrainedConfig\n from .dynamic_module_utils import custom_object_save\n-from .generation import GenerationConfig, GenerationMixin\n+from .generation import CompileConfig, GenerationConfig, GenerationMixin\n from .integrations import PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n from .loss.loss_utils import LOSS_MAPPING\n from .pytorch_utils import (  # noqa: F401\n@@ -5094,6 +5094,21 @@ def loss_function(self):\n             loss_type = \"ForCausalLM\"\n         return LOSS_MAPPING[loss_type]\n \n+    def get_compiled_call(self, compile_config: CompileConfig):\n+        \"\"\"Return a `torch.compile`'d version of `self.__call__`. This is useful to dynamically choose between\n+        non-compiled/compiled `forward` during inference, especially to switch between prefill (where we don't\n+        want to use compiled version to avoid recomputing the graph with new shapes) and iterative decoding\n+        (where we want the speed-ups of compiled version with static shapes).\"\"\"\n+        # Only reset it if not present or different from previous config\n+        default_config = getattr(self.generation_config, \"compile_config\", CompileConfig())\n+        if (\n+            not hasattr(self, \"_compiled_call\")\n+            or getattr(self, \"_last_compile_config\", default_config) != compile_config\n+        ):\n+            self._last_compile_config = compile_config\n+            self._compiled_call = torch.compile(self.__call__, **compile_config.to_dict())\n+        return self._compiled_call\n+\n \n PreTrainedModel.push_to_hub = copy_func(PreTrainedModel.push_to_hub)\n if PreTrainedModel.push_to_hub.__doc__ is not None:"
        }
    ],
    "stats": {
        "total": 111,
        "additions": 99,
        "deletions": 12
    }
}