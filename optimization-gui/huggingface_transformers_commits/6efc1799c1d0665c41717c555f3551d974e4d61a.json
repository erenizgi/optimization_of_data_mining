{
    "author": "MekkCyber",
    "message": "[kernels] Fix XPU layernorm kernel (#41583)\n\n* fix\n\n* add comment\n\n* better fix\n\n* style\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "6efc1799c1d0665c41717c555f3551d974e4d61a",
    "files": [
        {
            "sha": "112ac670e9a0c77ab6e3fddeec78e4e8c1200b0f",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 20,
            "deletions": 1,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/6efc1799c1d0665c41717c555f3551d974e4d61a/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6efc1799c1d0665c41717c555f3551d974e4d61a/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=6efc1799c1d0665c41717c555f3551d974e4d61a",
            "patch": "@@ -19,6 +19,7 @@\n \n from ..modeling_flash_attention_utils import lazy_import_flash_attention\n from ..utils import logging\n+from ..utils.import_utils import is_kernels_available\n from .flash_attention import flash_attention_forward\n \n \n@@ -64,6 +65,12 @@\n                     layer_name=\"LigerRMSNorm\",\n                 )\n             },\n+            \"xpu\": {\n+                Mode.INFERENCE: LayerRepository(\n+                    repo_id=\"kernels-community/rmsnorm\",\n+                    layer_name=\"RMSNorm\",\n+                )\n+            },\n         },\n         \"MLP\": {\n             \"cuda\": LayerRepository(\n@@ -139,7 +146,18 @@\n         },\n     }\n \n-    register_kernel_mapping(_KERNEL_MAPPING)\n+    def has_key(d, key):\n+        return key in d or any(isinstance(v, dict) and has_key(v, key) for v in d.values())\n+\n+    def register_kernel_mapping_transformers(mapping=None):\n+        if mapping is None:\n+            mapping = _KERNEL_MAPPING\n+        if has_key(mapping, \"xpu\") and not is_kernels_available(MIN_VERSION=\"0.10.2\"):\n+            raise ImportError(\n+                \"kernels uses an incompatible version. Please install the latest version with `pip install -U kernels`.\"\n+            )\n+        register_kernel_mapping(mapping)\n+\n \n except ImportError:\n     _kernels_available = False\n@@ -283,6 +301,7 @@ def lazy_load_kernel(kernel_name: str, mapping: dict[str, Optional[ModuleType]]\n     \"LayerRepository\",\n     \"use_kernel_forward_from_hub\",\n     \"register_kernel_mapping\",\n+    \"register_kernel_mapping_transformers\",\n     \"replace_kernel_forward_from_hub\",\n     \"lazy_load_kernel\",\n ]"
        },
        {
            "sha": "5f5f7ba62292792cff8be889d0bca933c7390623",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6efc1799c1d0665c41717c555f3551d974e4d61a/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6efc1799c1d0665c41717c555f3551d974e4d61a/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=6efc1799c1d0665c41717c555f3551d974e4d61a",
            "patch": "@@ -4033,10 +4033,14 @@ def set_use_kernels(self, use_kernels, kernel_config):\n         if use_kernels:\n             if not is_kernels_available():\n                 raise ValueError(\n-                    \"Kernels are not available. To use kernels, please install kernels using `pip install kernels`\"\n+                    \"`use_kernels=True` requires kernels>=0.9.0. Please install the latest version with `pip install -U kernels`\"\n                 )\n             from kernels import use_kernel_mapping\n \n+            from .integrations.hub_kernels import register_kernel_mapping_transformers\n+\n+            register_kernel_mapping_transformers()\n+\n             if kernel_config is not None and isinstance(kernel_config, KernelConfig):\n                 # This will make sure the mapping is valid, and the layers are registered in the model\n                 kernel_config.sanitize_kernel_mapping(self)"
        },
        {
            "sha": "f27d998f2ac75774d31767db4584e6be253d3101",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6efc1799c1d0665c41717c555f3551d974e4d61a/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6efc1799c1d0665c41717c555f3551d974e4d61a/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=6efc1799c1d0665c41717c555f3551d974e4d61a",
            "patch": "@@ -87,6 +87,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> tuple[\n TORCHAO_MIN_VERSION = \"0.4.0\"\n AUTOROUND_MIN_VERSION = \"0.5.0\"\n TRITON_MIN_VERSION = \"1.0.0\"\n+KERNELS_MIN_VERSION = \"0.9.0\"\n \n \n @lru_cache\n@@ -513,8 +514,9 @@ def is_kenlm_available() -> bool:\n \n \n @lru_cache\n-def is_kernels_available() -> bool:\n-    return _is_package_available(\"kernels\")\n+def is_kernels_available(MIN_VERSION: str = KERNELS_MIN_VERSION) -> bool:\n+    is_available, kernels_version = _is_package_available(\"kernels\", return_version=True)\n+    return is_available and version.parse(kernels_version) >= version.parse(MIN_VERSION)\n \n \n @lru_cache"
        }
    ],
    "stats": {
        "total": 33,
        "additions": 29,
        "deletions": 4
    }
}