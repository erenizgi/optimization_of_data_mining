{
    "author": "gante",
    "message": "[generate] fix default autocompile case on gpu (#37756)",
    "sha": "4d64c3859308a79dc7d9b8cafa2be039a77b2267",
    "files": [
        {
            "sha": "2ae16408b8d2d65d0c156d40f3c3c7389d09c31c",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4d64c3859308a79dc7d9b8cafa2be039a77b2267/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4d64c3859308a79dc7d9b8cafa2be039a77b2267/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=4d64c3859308a79dc7d9b8cafa2be039a77b2267",
            "patch": "@@ -3430,7 +3430,8 @@ def _sample(\n         model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n \n         model_forward = self.__call__\n-        if self._valid_auto_compile_criteria(model_kwargs, generation_config):\n+        compile_forward = self._valid_auto_compile_criteria(model_kwargs, generation_config)\n+        if compile_forward:\n             os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n             model_forward = self.get_compiled_call(generation_config.compile_config)\n "
        },
        {
            "sha": "e73862b54ea7f89b9c39f5be568bdb48ab995242",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4d64c3859308a79dc7d9b8cafa2be039a77b2267/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4d64c3859308a79dc7d9b8cafa2be039a77b2267/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=4d64c3859308a79dc7d9b8cafa2be039a77b2267",
            "patch": "@@ -5270,6 +5270,7 @@ def get_compiled_call(self, compile_config: Optional[CompileConfig]) -> Callable\n         # Only reset it if not present or different from previous config\n         if \"llama4\" in self.config.model_type:  # TODO try to enable for FULL COMPILE HYBRID CACHE SUPPORT\n             return self.__call__\n+        compile_config = compile_config or CompileConfig()\n         default_config = getattr(self.generation_config, \"compile_config\", None) or CompileConfig()\n         if (\n             not hasattr(self, \"_compiled_call\")"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 3,
        "deletions": 1
    }
}